{"id": "2505.20309", "pdf": "https://arxiv.org/pdf/2505.20309", "abs": "https://arxiv.org/abs/2505.20309", "authors": ["Amr Hegazy", "Mostafa Elhoushi", "Amr Alanwar"], "title": "Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Controlling undesirable Large Language Model (LLM) behaviors, such as the\ngeneration of unsafe content or failing to adhere to safety guidelines, often\nrelies on costly fine-tuning. Activation steering provides an alternative for\ninference-time control, but existing methods typically lack fine-grained,\nadaptive mechanisms. We introduce a novel approach using a lightweight,\ntrainable controller network integrated during inference. This controller\nnetwork observes specific intermediate LLM activations and predicts both a\nglobal scaling factor and layer-specific weights. The predicted global scaling\nfactor and layer-specific weights then dynamically modulate the intensity of a\nsteering patch, derived from a pre-computed \"refusal direction\" vector, applied\nacross the LLM's layers during generation. Trained on activations from both\nharmful and benign prompts, our controller learns to discriminatively apply\nnuanced, layer-aware interventions, activating steering primarily for harmful\ninputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild\nJailbreak Prompts demonstrate that our weighted steering controller\nsignificantly increases refusal rates compared to the base LLM, achieving\ntargeted behavioral modification without altering the original model\nparameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show\nour approach outperforms existing methods, presenting an efficient and adaptive\nmethod for fine-grained control over LLM behavior at inference time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u8bad\u7ec3\u7684\u63a7\u5236\u5668\u7f51\u7edc\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574LLM\u7684\u884c\u4e3a\uff0c\u907f\u514d\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u6216\u8fdd\u53cd\u5b89\u5168\u6307\u5357\u7684\u95ee\u9898\uff0c\u907f\u514d\u6602\u8d35\u7684\u5fae\u8c03\u3002", "method": "\u4f7f\u7528\u63a7\u5236\u5668\u7f51\u7edc\u89c2\u5bdf\u4e2d\u95f4\u6fc0\u6d3b\uff0c\u9884\u6d4b\u5168\u5c40\u7f29\u653e\u56e0\u5b50\u548c\u5c42\u7279\u5b9a\u6743\u91cd\uff0c\u52a8\u6001\u8c03\u6574\u9884\u8ba1\u7b97\u7684\u201c\u62d2\u7edd\u65b9\u5411\u201d\u5411\u91cf\u3002", "result": "\u5728ToxicChat\u548cIn-The-Wild Jailbreak Prompts\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u62d2\u7edd\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u65b9\u5f0f\uff0c\u5728\u63a8\u7406\u65f6\u5bf9LLM\u884c\u4e3a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "relevance": 90.0}}
{"id": "2505.20315", "pdf": "https://arxiv.org/pdf/2505.20315", "abs": "https://arxiv.org/abs/2505.20315", "authors": ["Zhewei Yao", "Guoheng Sun", "Lukasz Borchmann", "Zheyu Shen", "Minghang Deng", "Bohan Zhai", "Hao Zhang", "Ang Li", "Yuxiong He"], "title": "Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 2 figures", "summary": "Translating natural language into SQL (Test2SQL) is a longstanding challenge\nat the intersection of natural language understanding and structured data\naccess. While large language models (LLMs) have significantly improved fluency\nin SQL generation, producing correct and executable SQL--particularly for\ncomplex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a\nreinforcement learning (RL) framework and model family designed to generate\naccurate, executable SQL using a lightweight reward signal based solely on\nexecution correctness. Our approach avoids brittle intermediate supervision and\ncomplex reward shaping, promoting stable training and alignment with the end\ntask. Combined with carefully curated data, strong supervised initialization,\nand effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art\nexecution accuracy across six diverse Test2SQL benchmarks, including the top\nposition on the BIRD leaderboard. Notably, our 7B model outperforms prior\n70B-class systems, highlighting the framework's scalability and efficiency. We\nfurther demonstrate inference-time robustness through simple extensions like\nvalue retrieval and majority voting. Extensive experiments and ablation studies\noffer both positive and negative insights, providing practical guidance for\nfuture Test2SQL research.", "AI": {"tldr": "Arctic-Text2SQL-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u51c6\u786e\u4e14\u53ef\u6267\u884c\u7684SQL\u67e5\u8be2\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5956\u52b1\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728SQL\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u590d\u6742\u67e5\u8be2\u7684\u6b63\u786e\u4e14\u53ef\u6267\u884c\u7684SQL\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u6267\u884c\u6b63\u786e\u6027\u7684\u8f7b\u91cf\u7ea7\u5956\u52b1\u4fe1\u53f7\uff0c\u907f\u514d\u590d\u6742\u7684\u4e2d\u95f4\u76d1\u7763\u548c\u5956\u52b1\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u548c\u5f3a\u76d1\u7763\u521d\u59cb\u5316\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684Test2SQL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c7B\u6a21\u578b\u6027\u80fd\u8d85\u8fc7\u4e4b\u524d\u768470B\u7ea7\u7cfb\u7edf\u3002", "conclusion": "Arctic-Text2SQL-R1\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728SQL\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "relevance": 85.0}}
{"id": "2505.20318", "pdf": "https://arxiv.org/pdf/2505.20318", "abs": "https://arxiv.org/abs/2505.20318", "authors": ["Wang Cai", "Hsiu-Yuan Huang", "Zhixiang Wang", "Yunfang Wu"], "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context derived Vector (ICV) methods extract task-relevant representations\nfrom large language models (LLMs) and reinject them during inference, achieving\ncomparable performance to few-shot In-Context Learning (ICL) without repeated\ndemonstration processing. However, existing ICV methods remain sensitive to\nICL-specific factors, often use coarse or semantically fragmented\nrepresentations as the source of the vector, and rely on heuristic-based\ninjection positions, limiting their applicability.\n  To address these issues, we propose Dynamic Vector (DyVec), which\nincorporates an Exhaustive Query Rotation (EQR) strategy to extract robust\nsemantically aggregated latent representations by mitigating variance\nintroduced by ICL. It then applies Dynamic Latent Segmentation and Injection to\nadaptively partition representations based on task complexity and leverages\nREINFORCE-based optimization to learn optimal injection positions for each\nsegment.\n  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior\nICV baselines. Further analysis highlights the effectiveness of dynamically\nsegmenting and injecting semantically aggregated latent representations. DyVec\nprovides a lightweight and data-efficient solution for inference-time task\nadaptation.", "AI": {"tldr": "DyVec\u662f\u4e00\u79cd\u52a8\u6001\u5411\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7EQR\u7b56\u7565\u63d0\u53d6\u8bed\u4e49\u805a\u5408\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u52a8\u6001\u5206\u5272\u548c\u6ce8\u5165\u8fd9\u4e9b\u8868\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u5c11\u6837\u672cICL\u548c\u73b0\u6709ICV\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709ICV\u65b9\u6cd5\u5bf9ICL\u7279\u5b9a\u56e0\u7d20\u654f\u611f\uff0c\u8868\u793a\u7c97\u7cd9\u6216\u8bed\u4e49\u788e\u7247\u5316\uff0c\u4e14\u4f9d\u8d56\u542f\u53d1\u5f0f\u6ce8\u5165\u4f4d\u7f6e\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faDyVec\uff0c\u7ed3\u5408EQR\u7b56\u7565\u63d0\u53d6\u9c81\u68d2\u8bed\u4e49\u8868\u793a\uff0c\u52a8\u6001\u5206\u5272\u8868\u793a\u5e76\u5229\u7528REINFORCE\u4f18\u5316\u5b66\u4e60\u6700\u4f73\u6ce8\u5165\u4f4d\u7f6e\u3002", "result": "DyVec\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5c11\u6837\u672cICL\u3001LoRA\u548c\u73b0\u6709ICV\u57fa\u7ebf\u3002", "conclusion": "DyVec\u4e3a\u63a8\u7406\u65f6\u4efb\u52a1\u9002\u5e94\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20320", "pdf": "https://arxiv.org/pdf/2505.20320", "abs": "https://arxiv.org/abs/2505.20320", "authors": ["Satya Narayana Cheetirala", "Ganesh Raut", "Dhavalkumar Patel", "Fabio Sanatana", "Robert Freeman", "Matthew A Levin", "Girish N. Nadkarni", "Omar Dawkins", "Reba Miller", "Randolph M. Steinhagen", "Eyal Klang", "Prem Timsina"], "title": "Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long text classification is challenging for Large Language Models (LLMs) due\nto token limits and high computational costs. This study explores whether a\nRetrieval Augmented Generation (RAG) approach using only the most relevant text\nsegments can match the performance of processing entire clinical notes with\nlarge context LLMs. We begin by splitting clinical documents into smaller\nchunks, converting them into vector embeddings, and storing these in a FAISS\nindex. We then retrieve the top 4,000 words most pertinent to the\nclassification query and feed these consolidated segments into an LLM. We\nevaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication\nidentification task. Metrics such as AUC ROC, precision, recall, and F1 showed\nno statistically significant differences between the RAG based approach and\nwhole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can\nsignificantly reduce token usage without sacrificing classification accuracy,\nproviding a scalable and cost effective solution for analyzing lengthy clinical\ndocuments.", "AI": {"tldr": "RAG\u65b9\u6cd5\u901a\u8fc7\u68c0\u7d22\u6700\u76f8\u5173\u6587\u672c\u7247\u6bb5\uff0c\u5728\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e0e\u5168\u6587\u672c\u5904\u7406\u6027\u80fd\u76f8\u5f53\uff0c\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u957f\u6587\u672c\u5206\u7c7b\u4e2d\u7684token\u9650\u5236\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u5c06\u4e34\u5e8a\u6587\u6863\u5206\u5757\u5e76\u5411\u91cf\u5316\uff0c\u68c0\u7d22\u6700\u76f8\u5173\u76844000\u8bcd\u7247\u6bb5\u8f93\u5165LLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "RAG\u65b9\u6cd5\u4e0e\u5168\u6587\u672c\u5904\u7406\u5728AUC ROC\u7b49\u6307\u6807\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "RAG\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20381", "pdf": "https://arxiv.org/pdf/2505.20381", "abs": "https://arxiv.org/abs/2505.20381", "authors": ["Sijia Chen", "Yanqiu Yu", "En Yu", "Wenbing Tao"], "title": "ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking", "categories": ["cs.CV"], "comment": "19 pages, 11 figures, 6 tables", "summary": "Referring Multi-object tracking (RMOT) is an important research field in\ncomputer vision. Its task form is to guide the models to track the objects that\nconform to the language instruction. However, the RMOT task commonly requires\nclear language instructions, such methods often fail to work when complex\nlanguage instructions with reasoning characteristics appear. In this work, we\npropose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT).\nReaMOT is a more challenging task that requires accurate reasoning about\nobjects that match the language instruction with reasoning characteristic and\ntracking the objects' trajectories. To advance the ReaMOT task and evaluate the\nreasoning capabilities of tracking models, we construct ReaMOT Challenge, a\nreasoning-based multi-object tracking benchmark built upon 12 datasets.\nSpecifically, it comprises 1,156 language instructions with reasoning\ncharacteristic, 423,359 image-language pairs, and 869 diverse scenes, which is\ndivided into three levels of reasoning difficulty. In addition, we propose a\nset of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose\nReaTrack, a training-free framework for reasoning-based multi-object tracking\nbased on large vision-language models (LVLM) and SAM2, as a baseline for the\nReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark\ndemonstrate the effectiveness of our ReaTrack framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1ReaMOT\uff0c\u7ed3\u5408\u63a8\u7406\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u5e76\u6784\u5efa\u4e86ReaMOT Challenge\u57fa\u51c6\u548cReaTrack\u6846\u67b6\u3002", "motivation": "\u73b0\u6709RMOT\u4efb\u52a1\u5728\u590d\u6742\u63a8\u7406\u8bed\u8a00\u6307\u4ee4\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faReaMOT\u4efb\u52a1\uff0c\u6784\u5efaReaMOT Challenge\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eLVLM\u548cSAM2\u7684ReaTrack\u6846\u67b6\u3002", "result": "ReaTrack\u5728ReaMOT Challenge\u4e0a\u8868\u73b0\u6709\u6548\u3002", "conclusion": "ReaMOT\u4efb\u52a1\u548cReaTrack\u6846\u67b6\u4e3a\u63a8\u7406\u591a\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.20300", "pdf": "https://arxiv.org/pdf/2505.20300", "abs": "https://arxiv.org/abs/2505.20300", "authors": ["Chenxi Wu", "Juan Diego Toscano", "Khemraj Shukla", "Yingjie Chen", "Ali Shahmohammadi", "Edward Raymond", "Thomas Toupy", "Neda Nazemifard", "Charles Papageorgiou", "George Em Karniadakis"], "title": "FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "We propose FMEnets, a physics-informed machine learning framework for the\ndesign and analysis of non-ideal plug flow reactors. FMEnets integrates the\nfundamental governing equations (Navier-Stokes for fluid flow, material balance\nfor reactive species transport, and energy balance for temperature\ndistribution) into a unified multi-scale network model. The framework is\ncomposed of three interconnected sub-networks with independent optimizers that\nenable both forward and inverse problem-solving. In the forward mode, FMEnets\npredicts velocity, pressure, species concentrations, and temperature profiles\nusing only inlet and outlet information. In the inverse mode, FMEnets utilizes\nsparse multi-residence-time measurements to simultaneously infer unknown\nkinetic parameters and states. FMEnets can be implemented either as FME-PINNs,\nwhich employ conventional multilayer perceptrons, or as FME-KANs, based on\nKolmogorov-Arnold Networks. Comprehensive ablation studies highlight the\ncritical role of the FMEnets architecture in achieving accurate predictions.\nSpecifically, FME-KANs are more robust to noise than FME-PINNs, although both\nrepresentations are comparable in accuracy and speed in noise-free conditions.\nThe proposed framework is applied to three different sets of reaction scenarios\nand is compared with finite element simulations. FMEnets effectively captures\nthe complex interactions, achieving relative errors less than 2.5% for the\nunknown kinetic parameters. The new network framework not only provides a\ncomputationally efficient alternative for reactor design and optimization, but\nalso opens new avenues for integrating empirical correlations, limited and\nnoisy experimental data, and fundamental physical equations to guide reactor\ndesign.", "AI": {"tldr": "FMEnets\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u5206\u6790\u975e\u7406\u60f3\u6d3b\u585e\u6d41\u53cd\u5e94\u5668\uff0c\u901a\u8fc7\u6574\u5408\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u7269\u8d28\u5e73\u8861\u548c\u80fd\u91cf\u5e73\u8861\u65b9\u7a0b\uff0c\u5b9e\u73b0\u6b63\u5411\u548c\u9006\u5411\u95ee\u9898\u6c42\u89e3\u3002", "motivation": "\u89e3\u51b3\u975e\u7406\u60f3\u6d3b\u585e\u6d41\u53cd\u5e94\u5668\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u95ee\u9898\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6574\u5408\u7269\u7406\u65b9\u7a0b\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "FMEnets\u7531\u4e09\u4e2a\u4e92\u8054\u5b50\u7f51\u7edc\u7ec4\u6210\uff0c\u652f\u6301\u6b63\u5411\u9884\u6d4b\u548c\u9006\u5411\u63a8\u65ad\uff0c\u91c7\u7528FME-PINNs\u6216FME-KANs\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u5728\u4e09\u79cd\u53cd\u5e94\u573a\u666f\u4e2d\uff0cFMEnets\u7684\u9884\u6d4b\u8bef\u5dee\u4f4e\u4e8e2.5%\uff0cFME-KANs\u5bf9\u566a\u58f0\u66f4\u9c81\u68d2\u3002", "conclusion": "FMEnets\u4e3a\u53cd\u5e94\u5668\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5f00\u8f9f\u4e86\u6574\u5408\u5b9e\u9a8c\u6570\u636e\u4e0e\u7269\u7406\u65b9\u7a0b\u7684\u65b0\u9014\u5f84\u3002", "relevance": 20.0}}
{"id": "2505.20306", "pdf": "https://arxiv.org/pdf/2505.20306", "abs": "https://arxiv.org/abs/2505.20306", "authors": ["Xueqiang Ouyang", "Jia Wei"], "title": "Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review", "categories": ["cs.AI", "eess.IV", "q-bio.QM"], "comment": null, "summary": "As a global disease, infertility has always affected human beings. The\ndevelopment of assisted reproductive technology can effectively solve this\ndisease. However, the traditional in vitro fertilization-embryo transfer\ntechnology still faces many challenges in improving the success rate of\npregnancy, such as the subjectivity of embryo grading and the inefficiency of\nintegrating multi-modal data. Therefore, the introduction of artificial\nintelligence-based technologies is particularly crucial. This article reviews\nthe application progress of multi-modal artificial intelligence in embryo\ngrading and pregnancy prediction based on different data modalities (including\nstatic images, time-lapse videos and structured table data) from a new\nperspective, and discusses the main challenges in current research, such as the\ncomplexity of multi-modal information fusion and data scarcity.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u5728\u80da\u80ce\u5206\u7ea7\u548c\u598a\u5a20\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u8ba8\u8bba\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u4f53\u5916\u53d7\u7cbe-\u80da\u80ce\u79fb\u690d\u6280\u672f\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5f15\u5165\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u4e0d\u540c\u6570\u636e\u6a21\u6001\uff08\u9759\u6001\u56fe\u50cf\u3001\u5ef6\u65f6\u89c6\u9891\u548c\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\uff09\u7684\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u6280\u672f\u3002", "result": "\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u5728\u80da\u80ce\u5206\u7ea7\u548c\u598a\u5a20\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\u3002", "conclusion": "\u5f53\u524d\u7814\u7a76\u9762\u4e34\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u590d\u6742\u6027\u548c\u6570\u636e\u7a00\u7f3a\u7b49\u6311\u6218\u3002", "relevance": 10.0}}
{"id": "2505.20321", "pdf": "https://arxiv.org/pdf/2505.20321", "abs": "https://arxiv.org/abs/2505.20321", "authors": ["Mathew J. Koretsky", "Maya Willey", "Adi Asija", "Owen Bianchi", "Chelsea X. Alvarado", "Tanay Nayak", "Nicole Kuznetsov", "Sungwon Kim", "Mike A. Nalls", "Daniel Khashabi", "Faraz Faghri"], "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.", "AI": {"tldr": "BiomedSQL\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230SQL\u751f\u6210\u4e2d\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u3002\u5b83\u5305\u542b68,000\u4e2a\u95ee\u9898/SQL\u67e5\u8be2/\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230SQL\u7cfb\u7edf\u5728\u5c06\u5b9a\u6027\u79d1\u5b66\u95ee\u9898\u6620\u5c04\u4e3a\u53ef\u6267\u884cSQL\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9690\u5f0f\u9886\u57df\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\u3002BiomedSQL\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "BiomedSQL\u57fa\u4e8e\u4e00\u4e2a\u6574\u5408\u4e86\u57fa\u56e0-\u75be\u75c5\u5173\u8054\u3001\u7ec4\u5b66\u6570\u636e\u56e0\u679c\u63a8\u65ad\u548c\u836f\u7269\u6279\u51c6\u8bb0\u5f55\u7684BigQuery\u77e5\u8bc6\u5e93\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90LLM\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u5b9a\u4e49\u591a\u6b65\u4ee3\u7406BMSQL\u3002", "result": "GPT-o3-mini\u7684\u6267\u884c\u51c6\u786e\u7387\u4e3a59.0%\uff0cBMSQL\u4e3a62.6%\uff0c\u8fdc\u4f4e\u4e8e\u4e13\u5bb6\u57fa\u51c6\u768490.0%\u3002", "conclusion": "BiomedSQL\u4e3a\u63d0\u5347\u6587\u672c\u5230SQL\u7cfb\u7edf\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.20405", "pdf": "https://arxiv.org/pdf/2505.20405", "abs": "https://arxiv.org/abs/2505.20405", "authors": ["Lorenzo Baraldi", "Davide Bucciarelli", "Federico Betti", "Marcella Cornia", "Lorenzo Baraldi", "Nicu Sebe", "Rita Cucchiara"], "title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Instruction-based image editing models offer increased personalization\nopportunities in generative tasks. However, properly evaluating their results\nis challenging, and most of the existing metrics lag in terms of alignment with\nhuman judgment and explainability. To tackle these issues, we introduce DICE\n(DIfference Coherence Estimator), a model designed to detect localized\ndifferences between the original and the edited image and to assess their\nrelevance to the given modification request. DICE consists of two key\ncomponents: a difference detector and a coherence estimator, both built on an\nautoregressive Multimodal Large Language Model (MLLM) and trained using a\nstrategy that leverages self-supervision, distillation from inpainting\nnetworks, and full supervision. Through extensive experiments, we evaluate each\nstage of our pipeline, comparing different MLLMs within the proposed framework.\nWe demonstrate that DICE effectively identifies coherent edits, effectively\nevaluating images generated by different editing models with a strong\ncorrelation with human judgment. We publicly release our source code, models,\nand data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DICE\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u7ed3\u679c\uff0c\u901a\u8fc7\u68c0\u6d4b\u5c40\u90e8\u5dee\u5f02\u548c\u8bc4\u4f30\u4fee\u6539\u8bf7\u6c42\u7684\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u6307\u6807\u5728\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "DICE\u7531\u5dee\u5f02\u68c0\u6d4b\u5668\u548c\u4e00\u81f4\u6027\u4f30\u8ba1\u5668\u7ec4\u6210\uff0c\u57fa\u4e8e\u81ea\u56de\u5f52\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u3001\u84b8\u998f\u548c\u5168\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDICE\u80fd\u6709\u6548\u8bc6\u522b\u8fde\u8d2f\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "DICE\u4e3a\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002", "relevance": 60.0}}
{"id": "2505.20330", "pdf": "https://arxiv.org/pdf/2505.20330", "abs": "https://arxiv.org/abs/2505.20330", "authors": ["Yunfu Song", "Zhijian Ou"], "title": "Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning", "categories": ["cs.LG", "stat.ML"], "comment": "ICML 2018 submission. arXiv admin note: text overlap with\n  arXiv:1808.01630, arXiv:2505.18558", "summary": "Our examination of deep generative models (DGMs) developed for\nsemi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.\nFirst, mode missing and mode covering phenomenons are observed in genertion\nwith GANs and VAEs. Second, there exists an awkward conflict between good\nclassification and good generation in SSL by employing directed generative\nmodels. To address these problems, we formally present\njoint-stochastic-approximation random fields (JRFs) -- a new family of\nalgorithms for building deep undirected generative models, with application to\nSSL. It is found through synthetic experiments that JRFs work well in balancing\nmode covering and mode missing, and match the empirical data distribution well.\nEmpirically, JRFs achieve good classification results comparable to the\nstate-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in\nSSL, and simultaneously perform good generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u65e0\u5411\u751f\u6210\u6a21\u578bJRFs\uff0c\u7528\u4e8e\u89e3\u51b3\u534a\u76d1\u7763\u5b66\u4e60\u4e2dGANs\u548cVAEs\u7684\u6a21\u5f0f\u7f3a\u5931\u548c\u6a21\u5f0f\u8986\u76d6\u95ee\u9898\uff0c\u5e76\u5728\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u534a\u76d1\u7763\u5b66\u4e60\u4e2dGANs\u548cVAEs\u7684\u6a21\u5f0f\u7f3a\u5931\u548c\u6a21\u5f0f\u8986\u76d6\u95ee\u9898\uff0c\u4ee5\u53ca\u5206\u7c7b\u4e0e\u751f\u6210\u4efb\u52a1\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u8054\u5408\u968f\u673a\u8fd1\u4f3c\u968f\u673a\u573a\uff08JRFs\uff09\uff0c\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u65e0\u5411\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u3002", "result": "JRFs\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\u5e73\u8861\u4e86\u6a21\u5f0f\u8986\u76d6\u548c\u6a21\u5f0f\u7f3a\u5931\uff0c\u5e76\u5728MNIST\u3001SVHN\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u751f\u6210\u6548\u679c\u3002", "conclusion": "JRFs\u662f\u4e00\u79cd\u6709\u6548\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u3002", "relevance": 40.0}}
{"id": "2505.20310", "pdf": "https://arxiv.org/pdf/2505.20310", "abs": "https://arxiv.org/abs/2505.20310", "authors": ["Wanghan Xu", "Wenlong Zhang", "Fenghua Ling", "Ben Fei", "Yusong Hu", "Fangxuan Ren", "Jintai Lin", "Wanli Ouyang", "Lei Bai"], "title": "Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Meta-analysis is a systematic research methodology that synthesizes data from\nmultiple existing studies to derive comprehensive conclusions. This approach\nnot only mitigates limitations inherent in individual studies but also\nfacilitates novel discoveries through integrated data analysis. Traditional\nmeta-analysis involves a complex multi-stage pipeline including literature\nretrieval, paper screening, and data extraction, which demands substantial\nhuman effort and time. However, while LLM-based methods can accelerate certain\nstages, they still face significant challenges, such as hallucinations in paper\nscreening and data extraction. In this paper, we propose a multi-agent system,\nManalyzer, which achieves end-to-end automated meta-analysis through tool\ncalls. The hybrid review, hierarchical extraction, self-proving, and feedback\nchecking strategies implemented in Manalyzer significantly alleviate these two\nhallucinations. To comprehensively evaluate the performance of meta-analysis,\nwe construct a new benchmark comprising 729 papers across 3 domains,\nencompassing text, image, and table modalities, with over 10,000 data points.\nExtensive experiments demonstrate that Manalyzer achieves significant\nperformance improvements over the LLM baseline in multi meta-analysis tasks.\nProject page: https://black-yt.github.io/meta-analysis-page/ .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edfManalyzer\uff0c\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u7aef\u5230\u7aef\u81ea\u52a8\u5143\u5206\u6790\uff0c\u663e\u8457\u7f13\u89e3\u4e86LLM\u5728\u8bba\u6587\u7b5b\u9009\u548c\u6570\u636e\u63d0\u53d6\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5143\u5206\u6790\u6d41\u7a0b\u590d\u6742\u4e14\u8017\u65f6\uff0cLLM\u65b9\u6cd5\u867d\u80fd\u52a0\u901f\u90e8\u5206\u9636\u6bb5\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u66f4\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edfManalyzer\uff0c\u7ed3\u5408\u6df7\u5408\u5ba1\u67e5\u3001\u5206\u5c42\u63d0\u53d6\u3001\u81ea\u8bc1\u660e\u548c\u53cd\u9988\u68c0\u67e5\u7b56\u7565\u3002", "result": "\u5728\u5305\u542b729\u7bc7\u8bba\u6587\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cManalyzer\u5728\u591a\u4efb\u52a1\u5143\u5206\u6790\u4e2d\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\u3002", "conclusion": "Manalyzer\u4e3a\u5143\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.20322", "pdf": "https://arxiv.org/pdf/2505.20322", "abs": "https://arxiv.org/abs/2505.20322", "authors": ["Mengru Wang", "Ziwen Xu", "Shengyu Mao", "Shumin Deng", "Zhaopeng Tu", "Huajun Chen", "Ningyu Zhang"], "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": null, "summary": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSteering Target Atoms (STA)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u64cd\u4f5c\u77e5\u8bc6\u7ec4\u4ef6\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63a7\u5236\u5bf9\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u63d0\u793a\u5de5\u7a0b\uff09\u56e0\u53c2\u6570\u4f17\u591a\u4e14\u5185\u90e8\u8868\u793a\u9ad8\u5ea6\u4ea4\u7ec7\u800c\u53d7\u9650\u3002\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u96be\u4ee5\u5b9a\u4f4d\u539f\u5b50\u77e5\u8bc6\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faSTA\u65b9\u6cd5\uff0c\u5206\u79bb\u548c\u64cd\u4f5c\u77e5\u8bc6\u7ec4\u4ef6\uff0c\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\u6a21\u578b\u751f\u6210\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u5bf9\u6297\u6027\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "result": "STA\u65b9\u6cd5\u5728\u589e\u5f3a\u5b89\u5168\u6027\u548c\u7cbe\u786e\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u6027\u573a\u666f\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "STA\u65b9\u6cd5\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u7cbe\u786e\u63a7\u5236\u3002", "relevance": 85.0}}
{"id": "2505.20414", "pdf": "https://arxiv.org/pdf/2505.20414", "abs": "https://arxiv.org/abs/2505.20414", "authors": ["Royden Wagner", "Omer Sahin Tas", "Felix Hauser", "Marlon Steiner", "Dominik Strutz", "Abhishek Vivekanandan", "Carlos Fernandez", "Christoph Stiller"], "title": "RetroMotion: Retrocausal Motion Forecasting Models are Instructable", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion forecasts of road users (i.e., agents) vary in complexity as a\nfunction of scene constraints and interactive behavior. We address this with a\nmulti-task learning method for motion forecasting that includes a retrocausal\nflow of information. The corresponding tasks are to forecast (1) marginal\ntrajectory distributions for all modeled agents and (2) joint trajectory\ndistributions for interacting agents. Using a transformer model, we generate\nthe joint distributions by re-encoding marginal distributions followed by\npairwise modeling. This incorporates a retrocausal flow of information from\nlater points in marginal trajectories to earlier points in joint trajectories.\nPer trajectory point, we model positional uncertainty using compressed\nexponential power distributions. Notably, our method achieves state-of-the-art\nresults in the Waymo Interaction Prediction dataset and generalizes well to the\nArgoverse 2 dataset. Additionally, our method provides an interface for issuing\ninstructions through trajectory modifications. Our experiments show that\nregular training of motion forecasting leads to the ability to follow\ngoal-based instructions and to adapt basic directional instructions to the\nscene context. Code: https://github.com/kit-mrt/future-motion", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd0\u52a8\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86\u9006\u5411\u56e0\u679c\u5173\u7cfb\u7684\u4fe1\u606f\u6d41\uff0c\u901a\u8fc7Transformer\u6a21\u578b\u751f\u6210\u8054\u5408\u8f68\u8ff9\u5206\u5e03\uff0c\u5e76\u5728Waymo\u548cArgoverse 2\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u8fd0\u52a8\u9884\u6d4b\u4e2d\u573a\u666f\u7ea6\u675f\u548c\u4ea4\u4e92\u884c\u4e3a\u5e26\u6765\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u9006\u5411\u56e0\u679c\u5173\u7cfb\u7684\u4fe1\u606f\u6d41\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u65b0\u7f16\u7801\u8fb9\u7f18\u5206\u5e03\u548c\u6210\u5bf9\u5efa\u6a21\u751f\u6210\u8054\u5408\u8f68\u8ff9\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u538b\u7f29\u6307\u6570\u5e42\u5206\u5e03\u5efa\u6a21\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728Waymo Interaction Prediction\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u7ed3\u679c\uff0c\u5e76\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u63d0\u5347\u8fd0\u52a8\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u80fd\u901a\u8fc7\u8f68\u8ff9\u4fee\u6539\u5b9e\u73b0\u76ee\u6807\u5bfc\u5411\u7684\u6307\u4ee4\u6267\u884c\u548c\u573a\u666f\u9002\u5e94\u6027\u8c03\u6574\u3002", "relevance": 40.0}}
{"id": "2505.20346", "pdf": "https://arxiv.org/pdf/2505.20346", "abs": "https://arxiv.org/abs/2505.20346", "authors": ["Jiahao Kuang", "Nuowei Liu", "Changzhi Sun", "Tao Ji", "Yuanbin Wu"], "title": "PDFBench: A Benchmark for De novo Protein Design from Function", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "In recent years, while natural language processing and multimodal learning\nhave seen rapid advancements, the field of de novo protein design has also\nexperienced significant growth. However, most current methods rely on\nproprietary datasets and evaluation rubrics, making fair comparisons between\ndifferent approaches challenging. Moreover, these methods often employ\nevaluation metrics that capture only a subset of the desired properties of\ndesigned proteins, lacking a comprehensive assessment framework. To address\nthese, we introduce PDFBench, the first comprehensive benchmark for evaluating\nde novo protein design from function. PDFBench supports two tasks:\ndescription-guided design and keyword-guided design. To ensure fair and\nmultifaceted evaluation, we compile 22 metrics covering sequence plausibility,\nstructural fidelity, and language-protein alignment, along with measures of\nnovelty and diversity. We evaluate five state-of-the-art baselines, revealing\ntheir respective strengths and weaknesses across tasks. Finally, we analyze\ninter-metric correlations, exploring the relationships between four categories\nof metrics, and offering guidelines for metric selection. PDFBench establishes\na unified framework to drive future advances in function-driven de novo protein\ndesign.", "AI": {"tldr": "PDFBench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4ece\u5934\u86cb\u767d\u8d28\u8bbe\u8ba1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u652f\u6301\u63cf\u8ff0\u5f15\u5bfc\u548c\u5173\u952e\u8bcd\u5f15\u5bfc\u8bbe\u8ba1\uff0c\u5305\u542b22\u4e2a\u6307\u6807\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPDFBench\u57fa\u51c6\uff0c\u6db5\u76d6\u5e8f\u5217\u5408\u7406\u6027\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u8a00-\u86cb\u767d\u8d28\u5bf9\u9f50\u7b4922\u4e2a\u6307\u6807\uff0c\u8bc4\u4f30\u4e94\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u57fa\u7ebf\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u5206\u6790\u4e86\u6307\u6807\u95f4\u7684\u76f8\u5173\u6027\u3002", "conclusion": "PDFBench\u4e3a\u529f\u80fd\u9a71\u52a8\u7684\u4ece\u5934\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u3002", "relevance": 40.0}}
{"id": "2505.20313", "pdf": "https://arxiv.org/pdf/2505.20313", "abs": "https://arxiv.org/abs/2505.20313", "authors": ["Son Tran", "Edjard Mota", "Artur d'Avila Garcez"], "title": "Reasoning in Neurosymbolic AI", "categories": ["cs.AI", "cs.LO"], "comment": "50 pages, 13 figures, 56 references. Keywords: Neurosymbolic AI,\n  Restricted Boltzmann Machines, Logical Reasoning, SAT solving, MaxSAT,\n  Energy-based Learning, Constrained Optimization, Modular Deep Learning", "summary": "Knowledge representation and reasoning in neural networks have been a\nlong-standing endeavor which has attracted much attention recently. The\nprincipled integration of reasoning and learning in neural networks is a main\nobjective of the area of neurosymbolic Artificial Intelligence (AI). In this\nchapter, a simple energy-based neurosymbolic AI system is described that can\nrepresent and reason formally about any propositional logic formula. This\ncreates a powerful combination of learning from data and knowledge and logical\nreasoning. We start by positioning neurosymbolic AI in the context of the\ncurrent AI landscape that is unsurprisingly dominated by Large Language Models\n(LLMs). We identify important challenges of data efficiency, fairness and\nsafety of LLMs that might be addressed by neurosymbolic reasoning systems with\nformal reasoning capabilities. We then discuss the representation of logic by\nthe specific energy-based system, including illustrative examples and empirical\nevaluation of the correspondence between logical reasoning and energy\nminimization using Restricted Boltzmann Machines (RBM). Learning from data and\nknowledge is also evaluated empirically and compared with a symbolic, neural\nand a neurosymbolic system. Results reported in this chapter in an accessible\nway are expected to reignite the research on the use of neural networks as\nmassively-parallel models for logical reasoning and promote the principled\nintegration of reasoning and learning in deep networks. We conclude the chapter\nwith a discussion of the importance of positioning neurosymbolic AI within a\nbroader framework of formal reasoning and accountability in AI, discussing the\nchallenges for neurosynbolic AI to tackle the various known problems of\nreliability of deep learning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\uff0c\u80fd\u591f\u8868\u793a\u548c\u63a8\u7406\u547d\u9898\u903b\u8f91\u516c\u5f0f\uff0c\u7ed3\u5408\u6570\u636e\u5b66\u4e60\u548c\u903b\u8f91\u63a8\u7406\u3002\u8ba8\u8bba\u4e86\u795e\u7ecf\u7b26\u53f7AI\u5728\u89e3\u51b3LLMs\u7684\u6570\u636e\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5c06\u903b\u8f91\u63a8\u7406\u4e0e\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u6570\u636e\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\uff0c\u5229\u7528\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a\uff08RBM\uff09\u5b9e\u73b0\u903b\u8f91\u63a8\u7406\u4e0e\u80fd\u91cf\u6700\u5c0f\u5316\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u7ed3\u5408\u6570\u636e\u5b66\u4e60\u548c\u903b\u8f91\u63a8\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u7eaf\u7b26\u53f7\u3001\u7eaf\u795e\u7ecf\u548c\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7AI\u5728\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u53ef\u9760\u6027\u95ee\u9898\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5728\u5f62\u5f0f\u63a8\u7406\u548c\u95ee\u8d23\u5236\u6846\u67b6\u4e2d\u7684\u5b9a\u4f4d\u3002", "relevance": 60.0}}
{"id": "2505.20323", "pdf": "https://arxiv.org/pdf/2505.20323", "abs": "https://arxiv.org/abs/2505.20323", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "George H. Chen", "Jeremy C. Weiss"], "title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding temporal dynamics in clinical narratives is essential for\nmodeling patient trajectories, yet large-scale temporally annotated resources\nremain limited. We present PMOA-TTS, the first openly available dataset of\n124,699 PubMed Open Access (PMOA) case reports, each converted into structured\n(event, time) timelines via a scalable LLM-based pipeline. Our approach\ncombines heuristic filtering with Llama 3.3 to identify single-patient case\nreports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,\nresulting in over 5.6 million timestamped clinical events. To assess timeline\nquality, we evaluate against a clinician-curated reference set using three\nmetrics: (i) event-level matching (80% match at a cosine similarity threshold\nof 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the\nLog-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide\ndiagnostic and demographic coverage. In a downstream survival prediction task,\nembeddings from extracted timelines achieve time-dependent concordance indices\nup to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally\nstructured narratives. PMOA-TTS provides a scalable foundation for timeline\nextraction, temporal reasoning, and longitudinal modeling in biomedical NLP.\nThe dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .", "AI": {"tldr": "PMOA-TTS\u662f\u4e00\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5305\u542b124,699\u7bc7PubMed\u5f00\u653e\u83b7\u53d6\u75c5\u4f8b\u62a5\u544a\uff0c\u901a\u8fc7LLM\uff08Llama 3.3\u548cDeepSeek R1\uff09\u63d0\u53d6\u4e86560\u4e07\u4e2a\u65f6\u95f4\u6807\u8bb0\u4e34\u5e8a\u4e8b\u4ef6\uff0c\u7528\u4e8e\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u548c\u9884\u6d4b\u4efb\u52a1\u3002", "motivation": "\u4e34\u5e8a\u53d9\u4e8b\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u9700\u8981\u5927\u89c4\u6a21\u65f6\u95f4\u6807\u8bb0\u6570\u636e\uff0c\u4f46\u73b0\u6709\u8d44\u6e90\u6709\u9650\u3002", "method": "\u7ed3\u5408\u542f\u53d1\u5f0f\u8fc7\u6ee4\u548cLLM\uff08Llama 3.3\u548cDeepSeek R1\uff09\u63d0\u53d6\u65f6\u95f4\u7ebf\uff0c\u5e76\u901a\u8fc7\u4e09\u9879\u6307\u6807\u8bc4\u4f30\u8d28\u91cf\u3002", "result": "\u6570\u636e\u96c6\u8986\u76d6\u5e7f\u6cdb\uff0c\u65f6\u95f4\u7ebf\u63d0\u53d6\u8d28\u91cf\u9ad8\uff0c\u4e0b\u6e38\u751f\u5b58\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff08\u65f6\u95f4\u4f9d\u8d56\u4e00\u81f4\u6027\u6307\u6570\u8fbe0.82\uff09\u3002", "conclusion": "PMOA-TTS\u4e3a\u751f\u7269\u533b\u5b66NLP\u4e2d\u7684\u65f6\u95f4\u7ebf\u63d0\u53d6\u548c\u65f6\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.20426", "pdf": "https://arxiv.org/pdf/2505.20426", "abs": "https://arxiv.org/abs/2505.20426", "authors": ["Yunlong Tang", "Pinxin Liu", "Mingqian Feng", "Zhangyun Tan", "Rui Mao", "Chao Huang", "Jing Bi", "Yunzhong Xiao", "Susan Liang", "Hang Hua", "Ali Vosoughi", "Luchuan Song", "Zeliang Zhang", "Chenliang Xu"], "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness", "categories": ["cs.CV"], "comment": null, "summary": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/", "AI": {"tldr": "MMPerspective\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bf9\u900f\u89c6\u51e0\u4f55\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b10\u4e2a\u4efb\u52a1\u548c5,083\u4e2a\u95ee\u7b54\u5bf9\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u8868\u9762\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u900f\u89c6\u51e0\u4f55\u7684\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86MMPerspective\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,711\u5f20\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\uff0c\u8986\u76d6\u900f\u89c6\u611f\u77e5\u3001\u63a8\u7406\u548c\u9c81\u68d2\u6027\u4e09\u4e2a\u7ef4\u5ea6\u3002\u8bc4\u4f30\u4e8643\u4e2a\u5148\u8fdbMLLMs\u3002", "result": "\u6a21\u578b\u5728\u8868\u9762\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u4e0e\u900f\u89c6\u80fd\u529b\u76f8\u5173\u3002", "conclusion": "MMPerspective\u4e3a\u8bca\u65ad\u548c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.20350", "pdf": "https://arxiv.org/pdf/2505.20350", "abs": "https://arxiv.org/abs/2505.20350", "authors": ["Jifeng Hu", "Sili Huang", "Siyuan Guo", "Zhaogeng Liu", "Li Shen", "Lichao Sun", "Hechang Chen", "Yi Chang", "Dacheng Tao"], "title": "Decision Flow Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, generative models have shown remarkable capabilities across\ndiverse fields, including images, videos, language, and decision-making. By\napplying powerful generative models such as flow-based models to reinforcement\nlearning, we can effectively model complex multi-modal action distributions and\nachieve superior robotic control in continuous action spaces, surpassing the\nlimitations of single-modal action distributions with traditional\nGaussian-based policies. Previous methods usually adopt the generative models\nas behavior models to fit state-conditioned action distributions from datasets,\nwith policy optimization conducted separately through additional policies using\nvalue-based sample weighting or gradient-based updates. However, this\nseparation prevents the simultaneous optimization of multi-modal distribution\nfitting and policy improvement, ultimately hindering the training of models and\ndegrading the performance. To address this issue, we propose Decision Flow, a\nunified framework that integrates multi-modal action distribution modeling and\npolicy optimization. Specifically, our method formulates the action generation\nprocedure of flow-based models as a flow decision-making process, where each\naction generation step corresponds to one flow decision. Consequently, our\nmethod seamlessly optimizes the flow policy while capturing multi-modal action\ndistributions. We provide rigorous proofs of Decision Flow and validate the\neffectiveness through extensive experiments across dozens of offline RL\nenvironments. Compared with established offline RL baselines, the results\ndemonstrate that our method achieves or matches the SOTA performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDecision Flow\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u5efa\u6a21\u4e0e\u7b56\u7565\u4f18\u5316\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4e24\u8005\u5206\u79bb\u7684\u95ee\u9898\uff0c\u5e76\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e2d\uff0c\u751f\u6210\u6a21\u578b\u4e0e\u7b56\u7565\u4f18\u5316\u5206\u79bb\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5206\u5e03\u62df\u5408\u4e0e\u7b56\u7565\u6539\u8fdb\u7684\u540c\u6b65\u4f18\u5316\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faDecision Flow\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u7684\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u6d41\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u5efa\u6a21\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u7edf\u4e00\u3002", "result": "\u5728\u591a\u4e2a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\uff0cDecision Flow\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "Decision Flow\u901a\u8fc7\u7edf\u4e00\u591a\u6a21\u6001\u5206\u5e03\u5efa\u6a21\u4e0e\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u590d\u6742\u52a8\u4f5c\u7a7a\u95f4\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 75.0}}
{"id": "2505.20316", "pdf": "https://arxiv.org/pdf/2505.20316", "abs": "https://arxiv.org/abs/2505.20316", "authors": ["Yingpeng Du", "Tianjun Wei", "Zhu Sun", "Jie Zhang"], "title": "Reinforcement Speculative Decoding for Fast Ranking", "categories": ["cs.AI", "H.4.0"], "comment": "21 pages, 5 figures, 5 table", "summary": "Large Language Models (LLMs) have been widely adopted in ranking systems such\nas information retrieval (IR) systems and recommender systems (RSs). To\nalleviate the latency of auto-regressive decoding, some studies explore the\nsingle (first) token decoding for ranking approximation, but they suffer from\nsevere degradation in tail positions. Although speculative decoding (SD)\nmethods can be a remedy with verification at different positions, they face\nchallenges in ranking systems due to their left-to-right decoding paradigm.\nFirstly, ranking systems require strict latency constraints, but verification\nrounds in SD methods remain agnostic; Secondly, SD methods usually discard\nlistwise ranking knowledge about unaccepted items in previous rounds, hindering\nfuture multi-token prediction, especially when candidate tokens are the\nunaccepted items. In this paper, we propose a Reinforcement Speculative\nDecoding method for fast ranking inference of LLMs. To meet the ranking\nsystems' latency requirement, we propose an up-to-down decoding paradigm that\nemploys an agent to iteratively modify the ranking sequence under a constrained\nbudget. Specifically, we design a ranking-tailored policy optimization,\nactively exploring optimal multi-round ranking modification policy verified by\nLLMs via reinforcement learning (RL). To better approximate the target LLM\nunder the constrained budget, we trigger the agent fully utilizing the listwise\nranking knowledge about all items verified by LLMs across different rounds in\nRL, enhancing the modification policy of the agent. More importantly, we\ndemonstrate the theoretical robustness and advantages of our paradigm and\nimplementation. Experiments on both IR and RS tasks show the effectiveness of\nour proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff08Reinforcement Speculative Decoding\uff09\uff0c\u7528\u4e8e\u4f18\u5316LLM\u5728\u6392\u5e8f\u7cfb\u7edf\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u6392\u5e8f\u7cfb\u7edf\u4e2d\u5b58\u5728\u5ef6\u8fdf\u4e0d\u53ef\u63a7\u548c\u5ffd\u7565\u5217\u8868\u6392\u5e8f\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e25\u683c\u5ef6\u8fdf\u8981\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u7684\u89e3\u7801\u8303\u5f0f\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u8f6e\u6392\u5e8f\u4fee\u6539\u7b56\u7565\uff0c\u5145\u5206\u5229\u7528LLM\u9a8c\u8bc1\u7684\u5217\u8868\u6392\u5e8f\u77e5\u8bc6\u3002", "result": "\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u5ef6\u8fdf\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6392\u5e8f\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.20325", "pdf": "https://arxiv.org/pdf/2505.20325", "abs": "https://arxiv.org/abs/2505.20325", "authors": ["Amirhosein Ghasemabadi", "Keith G. Mills", "Baochun Li", "Di Niu"], "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuided by Gut (GG)\u7684\u9ad8\u6548\u81ea\u5f15\u5bfc\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6811\u641c\u7d22\u548c\u5185\u90e8\u4fe1\u53f7\uff08\u5982\u4ee4\u724c\u7ea7\u7f6e\u4fe1\u5ea6\u548c\u6b65\u9aa4\u65b0\u9896\u6027\uff09\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u9a8c\u8bc1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\uff08\u5982\u4f9d\u8d56\u5916\u90e8\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6216\u91c7\u6837\u65b9\u6cd5\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002GG\u65e8\u5728\u901a\u8fc7\u81ea\u5f15\u5bfc\u65b9\u5f0f\u964d\u4f4e\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "GG\u91c7\u7528\u8f7b\u91cf\u7ea7\u6811\u641c\u7d22\uff0c\u5229\u7528LLM\u5185\u90e8\u4fe1\u53f7\uff08\u4ee4\u724c\u7ea7\u7f6e\u4fe1\u5ea6\u548c\u6b65\u9aa4\u65b0\u9896\u6027\uff09\u8fdb\u884c\u5f15\u5bfc\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGG\u4f7f\u5c0f\u6a21\u578b\uff081.5B\u53c2\u6570\uff09\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u6a21\u578b\uff0832B-70B\u53c2\u6570\uff09\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4eGPU\u5185\u5b58\u4f7f\u752810\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53478\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c114-5\u500d\u3002", "conclusion": "GG\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "relevance": 85.0}}
{"id": "2505.20460", "pdf": "https://arxiv.org/pdf/2505.20460", "abs": "https://arxiv.org/abs/2505.20460", "authors": ["Ruqi Wu", "Xinjie Wang", "Liu Liu", "Chunle Guo", "Jiaxiong Qiu", "Chongyi Li", "Lichao Huang", "Zhizhong Su", "Ming-Ming Cheng"], "title": "DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data", "categories": ["cs.CV"], "comment": null, "summary": "We present DIPO, a novel framework for the controllable generation of\narticulated 3D objects from a pair of images: one depicting the object in a\nresting state and the other in an articulated state. Compared to the\nsingle-image approach, our dual-image input imposes only a modest overhead for\ndata collection, but at the same time provides important motion information,\nwhich is a reliable guide for predicting kinematic relationships between parts.\nSpecifically, we propose a dual-image diffusion model that captures\nrelationships between the image pair to generate part layouts and joint\nparameters. In addition, we introduce a Chain-of-Thought (CoT) based graph\nreasoner that explicitly infers part connectivity relationships. To further\nimprove robustness and generalization on complex articulated objects, we\ndevelop a fully automated dataset expansion pipeline, name LEGO-Art, that\nenriches the diversity and complexity of PartNet-Mobility dataset. We propose\nPM-X, a large-scale dataset of complex articulated 3D objects, accompanied by\nrendered images, URDF annotations, and textual descriptions. Extensive\nexperiments demonstrate that DIPO significantly outperforms existing baselines\nin both the resting state and the articulated state, while the proposed PM-X\ndataset further enhances generalization to diverse and structurally complex\narticulated objects. Our code and dataset will be released to the community\nupon publication.", "AI": {"tldr": "DIPO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e00\u5bf9\u56fe\u50cf\uff08\u9759\u6001\u548c\u52a8\u6001\uff09\u751f\u6210\u53ef\u63a7\u76843D\u5173\u8282\u7269\u4f53\uff0c\u7ed3\u5408\u53cc\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548cChain-of-Thought\u56fe\u63a8\u7406\u5668\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5355\u56fe\u50cf\u65b9\u6cd5\u5728\u751f\u62103D\u5173\u8282\u7269\u4f53\u65f6\u7f3a\u4e4f\u8fd0\u52a8\u4fe1\u606f\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u53cc\u56fe\u50cf\u8f93\u5165\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u52a8\u529b\u5b66\u5173\u7cfb\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u53cc\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548cChain-of-Thought\u56fe\u63a8\u7406\u5668\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6269\u5c55\u6d41\u7a0bLEGO-Art\uff0c\u751f\u6210PM-X\u6570\u636e\u96c6\u3002", "result": "DIPO\u5728\u9759\u6001\u548c\u52a8\u6001\u72b6\u6001\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0cPM-X\u6570\u636e\u96c6\u589e\u5f3a\u4e86\u590d\u6742\u5173\u8282\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DIPO\u6846\u67b6\u548cPM-X\u6570\u636e\u96c6\u4e3a3D\u5173\u8282\u7269\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20353", "pdf": "https://arxiv.org/pdf/2505.20353", "abs": "https://arxiv.org/abs/2505.20353", "authors": ["Dong Liu", "Jiayi Zhang", "Yifan Li", "Yanxuan Yu", "Ben Lengerich", "Ying Nian Wu"], "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM", "cs.PF"], "comment": null, "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.", "AI": {"tldr": "FastCache\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u7ea7\u7f13\u5b58\u548c\u538b\u7f29\u6846\u67b6\u52a0\u901fDiffusion Transformers\uff08DiT\uff09\u63a8\u7406\uff0c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "DiT\u56e0\u5176\u8fed\u4ee3\u7ed3\u6784\u548c\u6df1\u5ea6Transformer\u5806\u6808\u5bfc\u81f4\u8ba1\u7b97\u5bc6\u96c6\uff0cFastCache\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u95ee\u9898\u3002", "method": "1. \u7a7a\u95f4\u611f\u77e5\u7684\u4ee4\u724c\u9009\u62e9\u673a\u5236\u8fc7\u6ee4\u5197\u4f59\u4ee4\u724c\uff1b2. Transformer\u7ea7\u7f13\u5b58\u8de8\u65f6\u95f4\u6b65\u91cd\u7528\u6f5c\u5728\u6fc0\u6d3b\u3002", "result": "\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u751f\u6210\u8d28\u91cf\u4f18\u4e8e\u5176\u4ed6\u7f13\u5b58\u65b9\u6cd5\uff08FID\u548ct-FID\u6307\u6807\uff09\u3002", "conclusion": "FastCache\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86DiT\u7684\u63a8\u7406\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.20339", "pdf": "https://arxiv.org/pdf/2505.20339", "abs": "https://arxiv.org/abs/2505.20339", "authors": ["Antoni Gomila", "Vincent C. M\u00fcller"], "title": "Challenges for artificial cognitive systems", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The declared goal of this paper is to fill this gap: \"... cognitive systems\nresearch needs questions or challenges that define progress. The challenges are\nnot (yet more) predictions of the future, but a guideline to what are the aims\nand what would constitute progress.\" -- the quotation being from the project\ndescription of EUCogII, the project for the European Network for Cognitive\nSystems within which this formulation of the 'challenges' was originally\ndeveloped (http://www.eucognition.org). So, we stick out our neck and formulate\nthe challenges for artificial cognitive systems. These challenges are\narticulated in terms of a definition of what a cognitive system is: a system\nthat learns from experience and uses its acquired knowledge (both declarative\nand practical) in a flexible manner to achieve its own goals.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u586b\u8865\u8ba4\u77e5\u7cfb\u7edf\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u660e\u786e\u7684\u76ee\u6807\u548c\u6311\u6218\uff0c\u4ee5\u5b9a\u4e49\u8fdb\u5c55\u3002", "motivation": "\u8ba4\u77e5\u7cfb\u7edf\u7814\u7a76\u7f3a\u4e4f\u660e\u786e\u7684\u76ee\u6807\u548c\u6311\u6218\u6765\u5b9a\u4e49\u8fdb\u5c55\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u8ba4\u77e5\u7cfb\u7edf\uff08\u5373\u80fd\u591f\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u7075\u6d3b\u8fd0\u7528\u77e5\u8bc6\u4ee5\u5b9e\u73b0\u76ee\u6807\u7684\u7cfb\u7edf\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6311\u6218\u3002", "result": "\u660e\u786e\u63d0\u51fa\u4e86\u4eba\u5de5\u8ba4\u77e5\u7cfb\u7edf\u7684\u6311\u6218\u548c\u76ee\u6807\u3002", "conclusion": "\u672c\u6587\u4e3a\u8ba4\u77e5\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u65b9\u5411\u548c\u8861\u91cf\u6807\u51c6\u3002", "relevance": 30.0}}
{"id": "2505.20333", "pdf": "https://arxiv.org/pdf/2505.20333", "abs": "https://arxiv.org/abs/2505.20333", "authors": ["Yukun Zhang", "Qi Dong"], "title": "Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have achieved strong\nperformance, yet their internal reasoning remains opaque, limiting\ninterpretability and trust in critical applications. We propose a novel\nMulti_Scale Manifold Alignment framework that decomposes the latent space into\nglobal, intermediate, and local semantic manifolds capturing themes, context,\nand word-level details. Our method introduces cross_scale mapping functions\nthat jointly enforce geometric alignment (e.g., Procrustes analysis) and\ninformation preservation (via mutual information constraints like MINE or VIB).\nWe further incorporate curvature regularization and hyperparameter tuning for\nstable optimization. Theoretical analysis shows that alignment error, measured\nby KL divergence, can be bounded under mild assumptions. This framework offers\na unified explanation of how LLMs structure multi-scale semantics, advancing\ninterpretability and enabling applications such as bias detection and\nrobustness enhancement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u6d41\u5f62\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u89e3LLM\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5c3d\u7ba1LLMs\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u901a\u8fc7\u591a\u5c3a\u5ea6\u6d41\u5f62\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u6f5c\u5728\u7a7a\u95f4\u5206\u89e3\u4e3a\u5168\u5c40\u3001\u4e2d\u95f4\u548c\u5c40\u90e8\u8bed\u4e49\u6d41\u5f62\uff0c\u5e76\u5f15\u5165\u8de8\u5c3a\u5ea6\u6620\u5c04\u51fd\u6570\uff0c\u7ed3\u5408\u51e0\u4f55\u5bf9\u9f50\u548c\u4fe1\u606f\u4fdd\u7559\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u5bf9\u9f50\u8bef\u5dee\uff08KL\u6563\u5ea6\uff09\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u53ef\u88ab\u9650\u5236\uff0c\u6846\u67b6\u80fd\u7edf\u4e00\u89e3\u91caLLMs\u7684\u591a\u5c3a\u5ea6\u8bed\u4e49\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86LLMs\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u5982\u504f\u5dee\u68c0\u6d4b\u548c\u9c81\u68d2\u6027\u589e\u5f3a\u7b49\u5e94\u7528\u3002", "relevance": 90.0}}
{"id": "2505.20469", "pdf": "https://arxiv.org/pdf/2505.20469", "abs": "https://arxiv.org/abs/2505.20469", "authors": ["Lei Tian", "Xiaomin Li", "Liqian Ma", "Hefei Huang", "Zirui Zheng", "Hao Yin", "Taiqing Li", "Huchuan Lu", "Xu Jia"], "title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in 3D reconstruction techniques and vision-language models\nhave fueled significant progress in 3D semantic understanding, a capability\ncritical to robotics, autonomous driving, and virtual/augmented reality.\nHowever, methods that rely on 2D priors are prone to a critical challenge:\ncross-view semantic inconsistencies induced by occlusion, image blur, and\nview-dependent variations. These inconsistencies, when propagated via\nprojection supervision, deteriorate the quality of 3D Gaussian semantic fields\nand introduce artifacts in the rendered outputs. To mitigate this limitation,\nwe propose CCL-LGS, a novel framework that enforces view-consistent semantic\nsupervision by integrating multi-view semantic cues. Specifically, our approach\nfirst employs a zero-shot tracker to align a set of SAM-generated 2D masks and\nreliably identify their corresponding categories. Next, we utilize CLIP to\nextract robust semantic encodings across views. Finally, our Contrastive\nCodebook Learning (CCL) module distills discriminative semantic features by\nenforcing intra-class compactness and inter-class distinctiveness. In contrast\nto previous methods that directly apply CLIP to imperfect masks, our framework\nexplicitly resolves semantic conflicts while preserving category\ndiscriminability. Extensive experiments demonstrate that CCL-LGS outperforms\nprevious state-of-the-art methods. Our project page is available at\nhttps://epsilontl.github.io/CCL-LGS/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCCL-LGS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8bed\u4e49\u7ebf\u7d22\u89e3\u51b33D\u8bed\u4e49\u7406\u89e3\u4e2d\u7684\u8de8\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u53473D\u9ad8\u65af\u8bed\u4e49\u573a\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f9d\u8d562D\u5148\u9a8c\u7684\u65b9\u6cd5\u57283D\u8bed\u4e49\u7406\u89e3\u4e2d\u56e0\u906e\u6321\u3001\u6a21\u7cca\u548c\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u8de8\u89c6\u89d2\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u96f6\u6837\u672c\u8ddf\u8e2a\u5668\u5bf9\u9f50SAM\u751f\u6210\u76842D\u63a9\u7801\uff0c\u5229\u7528CLIP\u63d0\u53d6\u8de8\u89c6\u89d2\u8bed\u4e49\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u7801\u4e66\u5b66\u4e60\u6a21\u5757\u4f18\u5316\u8bed\u4e49\u7279\u5f81\u3002", "result": "CCL-LGS\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CCL-LGS\u901a\u8fc7\u591a\u89c6\u89d2\u8bed\u4e49\u4e00\u81f4\u6027\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u89c6\u89d2\u8bed\u4e49\u51b2\u7a81\uff0c\u63d0\u5347\u4e863D\u8bed\u4e49\u573a\u7684\u8d28\u91cf\u3002", "relevance": 40.0}}
{"id": "2505.20355", "pdf": "https://arxiv.org/pdf/2505.20355", "abs": "https://arxiv.org/abs/2505.20355", "authors": ["Yeonjoon Jung", "Daehyun Ahn", "Hyungjun Kim", "Taesu Kim", "Eunhyeok Park"], "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient\nfine-tuning (PEFT) of generative models, valued for its simplicity and\neffectiveness. Despite recent enhancements, LoRA still suffers from a\nfundamental limitation: overfitting when the bottleneck is widened. It performs\nbest at ranks 32-64, yet its accuracy stagnates or declines at higher ranks,\nstill falling short of full fine-tuning (FFT) performance. We identify the root\ncause as LoRA's structural bottleneck, which introduces gradient entanglement\nto the unrelated input channels and distorts gradient propagation. To address\nthis, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA)\nthat partitions weight matrices into sub-blocks, each with its own low-rank\nadapter. With negligible computational or storage cost, GraLoRA overcomes\nLoRA's limitations, effectively increases the representational capacity, and\nmore closely approximates FFT behavior. Experiments on code generation and\ncommonsense reasoning benchmarks show that GraLoRA consistently outperforms\nLoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on\nHumanEval+. These improvements hold across model sizes and rank settings,\nmaking GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts\nare available at https://github.com/SqueezeBits/GraLoRA.git", "AI": {"tldr": "GraLoRA improves LoRA by partitioning weight matrices into sub-blocks with individual low-rank adapters, overcoming overfitting and gradient issues, achieving better performance than LoRA and baselines.", "motivation": "LoRA's structural bottleneck causes gradient entanglement and limits performance, especially at higher ranks, falling short of full fine-tuning.", "method": "GraLoRA partitions weight matrices into sub-blocks, each with its own low-rank adapter, increasing representational capacity without added cost.", "result": "GraLoRA outperforms LoRA and baselines, with up to +8.5% absolute gain in Pass@1 on HumanEval+, scalable across model sizes and ranks.", "conclusion": "GraLoRA is a robust and scalable solution for parameter-efficient fine-tuning, closely approximating full fine-tuning performance.", "relevance": 85.0}}
{"id": "2505.20342", "pdf": "https://arxiv.org/pdf/2505.20342", "abs": "https://arxiv.org/abs/2505.20342", "authors": ["Paul de Font-Reaulx"], "title": "Machine Theory of Mind and the Structure of Human Values", "categories": ["cs.AI"], "comment": "This paper was originally submitted and accepted to the 2023 NeurIPS\n  MP2 Workshop", "summary": "Value learning is a crucial aspect of safe and ethical AI. This is primarily\npursued by methods inferring human values from behaviour. However, humans care\nabout much more than we are able to demonstrate through our actions.\nConsequently, an AI must predict the rest of our seemingly complex values from\na limited sample. I call this the value generalization problem. In this paper,\nI argue that human values have a generative rational structure and that this\nallows us to solve the value generalization problem. In particular, we can use\nBayesian Theory of Mind models to infer human values not only from behaviour,\nbut also from other values. This has been obscured by the widespread use of\nsimple utility functions to represent human values. I conclude that developing\ngenerative value-to-value inference is a crucial component of achieving a\nscalable machine theory of mind.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5982\u4f55\u901a\u8fc7\u751f\u6210\u6027\u7406\u6027\u7ed3\u6784\u89e3\u51b3\u4ef7\u503c\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528\u8d1d\u53f6\u65af\u5fc3\u667a\u7406\u8bba\u6a21\u578b\u4ece\u884c\u4e3a\u548c\u4ef7\u503c\u89c2\u4e2d\u63a8\u65ad\u4eba\u7c7b\u4ef7\u503c\u89c2\u3002", "motivation": "\u4eba\u7c7b\u4ef7\u503c\u89c2\u590d\u6742\u4e14\u96be\u4ee5\u901a\u8fc7\u884c\u4e3a\u5b8c\u5168\u4f53\u73b0\uff0cAI\u9700\u4ece\u6709\u9650\u6837\u672c\u4e2d\u9884\u6d4b\u4ef7\u503c\u89c2\uff0c\u5373\u4ef7\u503c\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5229\u7528\u8d1d\u53f6\u65af\u5fc3\u667a\u7406\u8bba\u6a21\u578b\uff0c\u4ece\u884c\u4e3a\u548c\u4ef7\u503c\u89c2\u4e2d\u63a8\u65ad\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u7b80\u5355\u6548\u7528\u51fd\u6570\u3002", "result": "\u751f\u6210\u6027\u4ef7\u503c\u89c2\u63a8\u65ad\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u673a\u5668\u5fc3\u667a\u7406\u8bba\u7684\u5173\u952e\u3002", "conclusion": "\u5f00\u53d1\u751f\u6210\u6027\u4ef7\u503c\u89c2\u63a8\u65ad\u662f\u89e3\u51b3\u4ef7\u503c\u6cdb\u5316\u95ee\u9898\u7684\u91cd\u8981\u65b9\u5411\u3002", "relevance": 75.0}}
{"id": "2505.20334", "pdf": "https://arxiv.org/pdf/2505.20334", "abs": "https://arxiv.org/abs/2505.20334", "authors": ["Yixuan Wang", "Shiyu Ji", "Yijun Liu", "Yuzhuang Xu", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures", "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLookahead Q-Cache (LAQ)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4f4e\u6210\u672c\u4f2a\u524d\u77bb\u67e5\u8be2\u6765\u4f18\u5316KV\u7f13\u5b58\u6dd8\u6c70\uff0c\u63d0\u5347\u89e3\u7801\u6548\u7387\u3002", "motivation": "KV\u7f13\u5b58\u5728\u957f\u6587\u672c\u5e8f\u5217\u4e2d\u5185\u5b58\u5360\u7528\u663e\u8457\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u9884\u586b\u5145\u9636\u6bb5\u6ce8\u610f\u529b\u5206\u6570\u6dd8\u6c70\u4ee4\u724c\uff0c\u4e0e\u5b9e\u9645\u63a8\u7406\u67e5\u8be2\u4e0d\u4e00\u81f4\uff0c\u5c24\u5176\u5728\u5185\u5b58\u53d7\u9650\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "LAQ\u6846\u67b6\u751f\u6210\u4f2a\u524d\u77bb\u67e5\u8be2\u4f5c\u4e3a\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u89c2\u5bdf\u7a97\u53e3\uff0c\u66f4\u51c6\u786e\u5730\u6a21\u62df\u771f\u5b9e\u89e3\u7801\u573a\u666f\u3002", "result": "\u5728LongBench\u548cNeedle-in-a-Haystack\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAQ\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u7f13\u5b58\u9884\u7b97\u4e0b\u6027\u80fd\u63d0\u53471\u223c4\u70b9\u3002", "conclusion": "LAQ\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e92\u8865\uff0c\u53ef\u7075\u6d3b\u7ed3\u5408\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.20471", "pdf": "https://arxiv.org/pdf/2505.20471", "abs": "https://arxiv.org/abs/2505.20471", "authors": ["Chenghao Qian", "Wenjing Li", "Yuhu Guo", "Gustav Markkula"], "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG", "cs.RO"], "comment": null, "summary": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit", "AI": {"tldr": "WeatherEdit\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5929\u6c14\u7f16\u8f91\u7ba1\u9053\uff0c\u7528\u4e8e\u57283D\u573a\u666f\u4e2d\u751f\u6210\u53ef\u63a7\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u7684\u771f\u5b9e\u5929\u6c14\u6548\u679c\u3002\u5b83\u5305\u62ec\u5929\u6c14\u80cc\u666f\u7f16\u8f91\u548c\u5929\u6c14\u7c92\u5b50\u6784\u5efa\u4e24\u90e8\u5206\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c4D\u9ad8\u65af\u573a\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u63d0\u4f9b\u53ef\u63a7\u4e14\u771f\u5b9e\u7684\u5929\u6c14\u6548\u679c\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c4D\u9ad8\u65af\u573a\uff0c\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u548c\u6a21\u62df\u63a7\u5236\u5929\u6c14\u7c92\u5b50\u7684\u5c5e\u6027\u548c\u52a8\u6001\u3002", "result": "\u5728\u591a\u4e2a\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u751f\u6210\u591a\u6837\u4e14\u53ef\u63a7\u5929\u6c14\u6548\u679c\u7684\u80fd\u529b\u3002", "conclusion": "WeatherEdit\u5728\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 20.0}}
{"id": "2505.20357", "pdf": "https://arxiv.org/pdf/2505.20357", "abs": "https://arxiv.org/abs/2505.20357", "authors": ["Jun Tian", "He Wang", "Jibo He", "Yu Pan", "Shuo Cao", "Qingquan Jiang"], "title": "Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach", "categories": ["cs.LG", "gr-qc", "physics.data-an"], "comment": null, "summary": "Convolutional neural networks (CNNs) have become widely adopted in\ngravitational wave (GW) detection pipelines due to their ability to\nautomatically learn hierarchical features from raw strain data. However, the\nphysical meaning of these learned features remains underexplored, limiting the\ninterpretability of such models. In this work, we propose a hybrid architecture\nthat combines a CNN-based feature extractor with a random forest (RF)\nclassifier to improve both detection performance and interpretability. Unlike\nprior approaches that directly connect classifiers to CNN outputs, our method\nintroduces four physically interpretable metrics - variance, signal-to-noise\nratio (SNR), waveform overlap, and peak amplitude - computed from the final\nconvolutional layer. These are jointly used with the CNN output in the RF\nclassifier to enable more informed decision boundaries. Tested on long-duration\nstrain datasets, our hybrid model outperforms a baseline CNN model, achieving a\nrelative improvement of 21\\% in sensitivity at a fixed false alarm rate of 10\nevents per month. Notably, it also shows improved detection of low-SNR signals\n(SNR $\\le$ 10), which are especially vulnerable to misclassification in noisy\nenvironments. Feature attribution via the RF model reveals that both\nCNN-extracted and handcrafted features contribute significantly to\nclassification decisions, with learned variance and CNN outputs ranked among\nthe most informative. These findings suggest that physically motivated\npost-processing of CNN feature maps can serve as a valuable tool for\ninterpretable and efficient GW detection, bridging the gap between deep\nlearning and domain knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548c\u968f\u673a\u68ee\u6797\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u5f15\u529b\u6ce2\u68c0\u6d4b\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3CNN\u5728\u5f15\u529b\u6ce2\u68c0\u6d4b\u4e2d\u7279\u5f81\u7269\u7406\u610f\u4e49\u4e0d\u660e\u786e\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u7ed3\u5408CNN\u7279\u5f81\u63d0\u53d6\u5668\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5f15\u5165\u56db\u4e2a\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6307\u6807\uff08\u65b9\u5dee\u3001\u4fe1\u566a\u6bd4\u3001\u6ce2\u5f62\u91cd\u53e0\u548c\u5cf0\u503c\u5e45\u5ea6\uff09\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u56fa\u5b9a\u8bef\u62a5\u7387\u4e0b\u7075\u654f\u5ea6\u63d0\u534721%\uff0c\u4f4e\u4fe1\u566a\u6bd4\u4fe1\u53f7\u68c0\u6d4b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7269\u7406\u542f\u53d1\u7684CNN\u7279\u5f81\u540e\u5904\u7406\u6709\u52a9\u4e8e\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u5f15\u529b\u6ce2\u68c0\u6d4b\u3002", "relevance": 30.0}}
{"id": "2505.20417", "pdf": "https://arxiv.org/pdf/2505.20417", "abs": "https://arxiv.org/abs/2505.20417", "authors": ["Meng Cao", "Shuyuan Zhang", "Xiao-Wen Chang", "Doina Precup"], "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is a widely used technique\nfor aligning Large Language Models (LLMs) with human preferences, yet it often\nsuffers from sparse reward signals, making effective credit assignment\nchallenging. In typical setups, the reward model provides a single scalar score\nfor an entire generated sequence, offering little insight into which token or\nspan-level decisions were responsible for the outcome. To address this, we\npropose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages\nShapley values in cooperative game theory. SCAR distributes the total\nsequence-level reward among constituent tokens or text spans based on their\nprincipled marginal contributions. This creates dense reward signals,\ncrucially, without necessitating the training of auxiliary critique models or\nrecourse to fine-grained human annotations at intermediate generation stages.\nUnlike prior dense reward methods, SCAR offers a game-theoretic foundation for\nfair credit attribution. Theoretically, we demonstrate that SCAR preserves the\noriginal optimal policy, and empirically, across diverse tasks including\nsentiment control, text summarization, and instruction tuning, we show that\nSCAR converges significantly faster and achieves higher final reward scores\ncompared to standard RLHF and attention-based dense reward baselines. Our\nfindings suggest that SCAR provides a more effective and theoretically sound\nmethod for credit assignment in RLHF, leading to more efficient alignment of\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u4fe1\u7528\u5206\u914d\u65b9\u6cd5SCAR\uff0c\u7528\u4e8e\u89e3\u51b3RLHF\u4e2d\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u63d0\u5347LLM\u5bf9\u9f50\u6548\u7387\u3002", "motivation": "RLHF\u5728LLM\u5bf9\u9f50\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u56f0\u96be\uff0cSCAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528Shapley\u503c\u5728\u5408\u4f5c\u535a\u5f08\u4e2d\u7684\u7406\u8bba\uff0c\u5c06\u5e8f\u5217\u7ea7\u5956\u52b1\u5206\u914d\u5230\u5404token\u6216\u6587\u672c\u7247\u6bb5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u7cbe\u7ec6\u6807\u6ce8\u3002", "result": "SCAR\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6RLHF\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5bc6\u96c6\u5956\u52b1\u65b9\u6cd5\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u6700\u7ec8\u5956\u52b1\u66f4\u9ad8\u3002", "conclusion": "SCAR\u4e3aRLHF\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86LLM\u5bf9\u9f50\u6548\u7387\u3002", "relevance": 90.0}}
{"id": "2505.20335", "pdf": "https://arxiv.org/pdf/2505.20335", "abs": "https://arxiv.org/abs/2505.20335", "authors": ["Zishun Yu", "Shangzhe Li", "Xinhua Zhang"], "title": "Language Model Distillation: A Temporal Difference Imitation Learning Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have led to significant progress across many NLP tasks,\nalthough their massive sizes often incur substantial computational costs.\nDistillation has become a common practice to compress these large and highly\ncapable models into smaller, more efficient ones. Many existing language model\ndistillation methods can be viewed as behavior cloning from the perspective of\nimitation learning or inverse reinforcement learning. This viewpoint has\ninspired subsequent studies that leverage (inverse) reinforcement learning\ntechniques, including variations of behavior cloning and temporal difference\nlearning methods. Rather than proposing yet another specific temporal\ndifference method, we introduce a general framework for temporal\ndifference-based distillation by exploiting the distributional sparsity of the\nteacher model. Specifically, it is often observed that language models assign\nmost probability mass to a small subset of tokens. Motivated by this\nobservation, we design a temporal difference learning framework that operates\non a reduced action space (a subset of vocabulary), and demonstrate how\npractical algorithms can be derived and the resulting performance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u84b8\u998f\uff0c\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u5206\u5e03\u7a00\u758f\u6027\uff0c\u5728\u7f29\u51cf\u7684\u52a8\u4f5c\u7a7a\u95f4\u4e0a\u64cd\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u84b8\u998f\u6280\u672f\u53ef\u4ee5\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u3002\u672c\u6587\u4ece\u5f3a\u5316\u5b66\u4e60\u89c6\u89d2\u51fa\u53d1\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u6982\u7387\u5206\u5e03\u7a00\u758f\u7684\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5728\u7f29\u51cf\u52a8\u4f5c\u7a7a\u95f4\uff08\u8bcd\u6c47\u5b50\u96c6\uff09\u4e0a\u64cd\u4f5c\u7684\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u51fa\u5b9e\u9645\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u84b8\u998f\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u5206\u5e03\u7a00\u758f\u6027\uff0c\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u6846\u67b6\u4e3a\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.20498", "pdf": "https://arxiv.org/pdf/2505.20498", "abs": "https://arxiv.org/abs/2505.20498", "authors": ["Dongyu Luo", "Kelin Yu", "Amir-Hossein Shahidzadeh", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos"], "title": "ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "22 pages, 11 figures, 7 tables", "summary": "Vision-based tactile sensing has been widely used in perception,\nreconstruction, and robotic manipulation. However, collecting large-scale\ntactile data remains costly due to the localized nature of sensor-object\ninteractions and inconsistencies across sensor instances. Existing approaches\nto scaling tactile data, such as simulation and free-form tactile generation,\noften suffer from unrealistic output and poor transferability to downstream\ntasks.To address this, we propose ControlTac, a two-stage controllable\nframework that generates realistic tactile images conditioned on a single\nreference tactile image, contact force, and contact position. With those\nphysical priors as control input, ControlTac generates physically plausible and\nvaried tactile images that can be used for effective data augmentation. Through\nexperiments on three downstream tasks, we demonstrate that ControlTac can\neffectively augment tactile datasets and lead to consistent gains. Our three\nreal-world experiments further validate the practical utility of our approach.\nProject page: https://dongyuluo.github.io/controltac.", "AI": {"tldr": "ControlTac\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u53ef\u63a7\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u57fa\u4e8e\u5355\u5f20\u53c2\u8003\u89e6\u89c9\u56fe\u50cf\u3001\u63a5\u89e6\u529b\u548c\u63a5\u89e6\u4f4d\u7f6e\u7684\u903c\u771f\u89e6\u89c9\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u89e6\u89c9\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u89e6\u89c9\u6570\u636e\u7684\u5927\u89c4\u6a21\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u4f20\u611f\u5668\u5b9e\u4f8b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4eff\u771f\u548c\u81ea\u7531\u89e6\u89c9\u751f\u6210\uff09\u8f93\u51fa\u4e0d\u771f\u5b9e\u4e14\u8fc1\u79fb\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e86ControlTac\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u53ef\u63a7\u6846\u67b6\uff0c\u5229\u7528\u7269\u7406\u5148\u9a8c\uff08\u53c2\u8003\u89e6\u89c9\u56fe\u50cf\u3001\u63a5\u89e6\u529b\u548c\u63a5\u89e6\u4f4d\u7f6e\uff09\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u7684\u89e6\u89c9\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eControlTac\u80fd\u6709\u6548\u589e\u5f3a\u89e6\u89c9\u6570\u636e\u96c6\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ControlTac\u901a\u8fc7\u751f\u6210\u903c\u771f\u89e6\u89c9\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u89e6\u89c9\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 30.0}}
{"id": "2505.20359", "pdf": "https://arxiv.org/pdf/2505.20359", "abs": "https://arxiv.org/abs/2505.20359", "authors": ["Lijun Zhang", "Lin Li", "Yajie Qi", "Huizhong Song", "Yaodong Yang", "Jun Wang", "Wei Wei"], "title": "Risk-aware Direct Preference Optimization under Nested Risk Measure", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When fine-tuning pre-trained Large Language Models (LLMs) to align with human\nvalues and intentions, maximizing the estimated reward can lead to superior\nperformance, but it also introduces potential risks due to deviations from the\nreference model's intended behavior. Most existing methods typically introduce\nKL divergence to constrain deviations between the trained model and the\nreference model; however, this may not be sufficient in certain applications\nthat require tight risk control. In this paper, we introduce Risk-aware Direct\nPreference Optimization (Ra-DPO), a novel approach that incorporates\nrisk-awareness by employing a class of nested risk measures. This approach\nformulates a constrained risk-aware advantage function maximization problem and\nthen converts the Bradley-Terry model into a token-level representation. The\nobjective function maximizes the likelihood of the policy while suppressing the\ndeviation between a trained model and the reference model using a sequential\nrisk ratio, thereby enhancing the model's risk-awareness. Experimental results\nacross three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and\nAlpacaEval, demonstrate the proposed method's superior performance in balancing\nalignment performance and model drift. Our code is opensourced at\nhttps://github.com/zlj123-max/Ra-DPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08Ra-DPO\uff09\uff0c\u901a\u8fc7\u5d4c\u5957\u98ce\u9669\u5ea6\u91cf\u589e\u5f3a\u6a21\u578b\u7684\u98ce\u9669\u610f\u8bc6\uff0c\u5e73\u8861\u5bf9\u9f50\u6027\u80fd\u548c\u6a21\u578b\u6f02\u79fb\u3002", "motivation": "\u5728\u5fae\u8c03\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u610f\u56fe\u65f6\uff0c\u6700\u5927\u5316\u5956\u52b1\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u4e5f\u5e26\u6765\u504f\u79bb\u53c2\u8003\u6a21\u578b\u884c\u4e3a\u7684\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528KL\u6563\u5ea6\u7ea6\u675f\uff0c\u4f46\u67d0\u4e9b\u5e94\u7528\u9700\u8981\u66f4\u4e25\u683c\u7684\u98ce\u9669\u63a7\u5236\u3002", "method": "\u5f15\u5165Ra-DPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5957\u98ce\u9669\u5ea6\u91cf\u6784\u5efa\u98ce\u9669\u611f\u77e5\u4f18\u52bf\u51fd\u6570\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u5c06Bradley-Terry\u6a21\u578b\u8f6c\u6362\u4e3a\u8bcd\u7ea7\u8868\u793a\uff0c\u76ee\u6807\u51fd\u6570\u5728\u6700\u5927\u5316\u7b56\u7565\u4f3c\u7136\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5e8f\u5217\u98ce\u9669\u6bd4\u6291\u5236\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u7684\u504f\u79bb\u3002", "result": "\u5728IMDb\u3001Anthropic HH\u548cAlpacaEval\u4e09\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u5bf9\u9f50\u6027\u80fd\u548c\u6a21\u578b\u6f02\u79fb\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "Ra-DPO\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9f50\u6027\u80fd\u5e76\u63a7\u5236\u4e86\u98ce\u9669\u3002", "relevance": 90.0}}
{"id": "2505.20466", "pdf": "https://arxiv.org/pdf/2505.20466", "abs": "https://arxiv.org/abs/2505.20466", "authors": ["P. S. Kesavan", "Pontus Nordenfelt"], "title": "Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration", "categories": ["cs.AI", "cs.HC", "cs.MA", "q-bio.QM"], "comment": "34 pages, 5 figures", "summary": "Smart microscopy represents a paradigm shift in biological imaging, moving\nfrom passive observation tools to active collaborators in scientific inquiry.\nEnabled by advances in automation, computational power, and artificial\nintelligence, these systems are now capable of adaptive decision-making and\nreal-time experimental control. Here, we introduce a theoretical framework that\nreconceptualizes smart microscopy as a partner in scientific investigation.\nCentral to our framework is the concept of the 'epistemic-empirical divide' in\ncellular investigation-the gap between what is observable (empirical domain)\nand what must be understood (epistemic domain). We propose six core design\nprinciples: epistemic-empirical awareness, hierarchical context integration, an\nevolution from detection to perception, adaptive measurement frameworks,\nnarrative synthesis capabilities, and cross-contextual reasoning. Together,\nthese principles guide a multi-agent architecture designed to align empirical\nobservation with the goals of scientific understanding. Our framework provides\na roadmap for building microscopy systems that go beyond automation to actively\nsupport hypothesis generation, insight discovery, and theory development,\nredefining the role of scientific instruments in the process of knowledge\ncreation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u663e\u5fae\u955c\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5176\u91cd\u65b0\u5b9a\u4e49\u4e3a\u79d1\u5b66\u7814\u7a76\u7684\u5408\u4f5c\u4f19\u4f34\uff0c\u901a\u8fc7\u516d\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\u5b9e\u73b0\u4ece\u89c2\u5bdf\u5230\u7406\u89e3\u7684\u8de8\u8d8a\u3002", "motivation": "\u667a\u80fd\u663e\u5fae\u955c\u4ece\u88ab\u52a8\u5de5\u5177\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u5408\u4f5c\u4f19\u4f34\u7684\u9700\u6c42\uff0c\u4ee5\u5f25\u5408\u7ec6\u80de\u7814\u7a76\u4e2d\u53ef\u89c2\u5bdf\uff08\u7ecf\u9a8c\u9886\u57df\uff09\u4e0e\u9700\u7406\u89e3\uff08\u8ba4\u77e5\u9886\u57df\uff09\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u4e86\u516d\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4ee5\u5c06\u7ecf\u9a8c\u89c2\u5bdf\u4e0e\u79d1\u5b66\u7406\u89e3\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u6846\u67b6\u4e3a\u6784\u5efa\u652f\u6301\u5047\u8bbe\u751f\u6210\u3001\u6d1e\u5bdf\u53d1\u73b0\u548c\u7406\u8bba\u53d1\u5c55\u7684\u663e\u5fae\u955c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "conclusion": "\u667a\u80fd\u663e\u5fae\u955c\u53ef\u4ee5\u91cd\u65b0\u5b9a\u4e49\u79d1\u5b66\u4eea\u5668\u5728\u77e5\u8bc6\u521b\u9020\u8fc7\u7a0b\u4e2d\u7684\u89d2\u8272\u3002", "relevance": 20.0}}
{"id": "2505.20336", "pdf": "https://arxiv.org/pdf/2505.20336", "abs": "https://arxiv.org/abs/2505.20336", "authors": ["Yu Zhang", "Wanli Jiang", "Zhengyu Yang"], "title": "MOSLIM:Align with diverse preferences in prompts through reward classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The multi-objective alignment of Large Language Models (LLMs) is essential\nfor ensuring foundational models conform to diverse human preferences. Current\nresearch in this field typically involves either multiple policies or multiple\nreward models customized for various preferences, or the need to train a\npreference-specific supervised fine-tuning (SFT) model. In this work, we\nintroduce a novel multi-objective alignment method, MOSLIM, which utilizes a\nsingle reward model and policy model to address diverse objectives. MOSLIM\nprovides a flexible way to control these objectives through prompting and does\nnot require preference training during SFT phase, allowing thousands of\noff-the-shelf models to be directly utilized within this training framework.\nMOSLIM leverages a multi-head reward model that classifies question-answer\npairs instead of scoring them and then optimize policy model with a scalar\nreward derived from a mapping function that converts classification results\nfrom reward model into reward scores. We demonstrate the efficacy of our\nproposed method across several multi-objective benchmarks and conduct ablation\nstudies on various reward model sizes and policy optimization methods. The\nMOSLIM method outperforms current multi-objective approaches in most results\nwhile requiring significantly fewer GPU computing resources compared with\nexisting policy optimization methods.", "AI": {"tldr": "MOSLIM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u5956\u52b1\u6a21\u578b\u548c\u7b56\u7565\u6a21\u578b\u5904\u7406\u591a\u6837\u5316\u76ee\u6807\uff0c\u65e0\u9700\u5728SFT\u9636\u6bb5\u8fdb\u884c\u504f\u597d\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11GPU\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u591a\u76ee\u6807\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u591a\u4e2a\u7b56\u7565\u6216\u5956\u52b1\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u591a\u5934\u90e8\u5956\u52b1\u6a21\u578b\u5bf9\u95ee\u7b54\u5bf9\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5c06\u5206\u7c7b\u7ed3\u679c\u8f6c\u5316\u4e3a\u5956\u52b1\u5206\u6570\uff0c\u4f18\u5316\u7b56\u7565\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u591a\u76ee\u6807\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MOSLIM\u4e3aLLM\u591a\u76ee\u6807\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u3002", "relevance": 90.0}}
{"id": "2505.20507", "pdf": "https://arxiv.org/pdf/2505.20507", "abs": "https://arxiv.org/abs/2505.20507", "authors": ["Elias Arbash", "Ahmed Jamal Afifi", "Ymane Belahsen", "Margret Fuchs", "Pedram Ghamisi", "Paul Scheunders", "Richard Gloaguen"], "title": "Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The global challenge of sustainable recycling demands automated, fast, and\naccurate, state-of-the-art (SOTA) material detection systems that act as a\nbedrock for a circular economy. Democratizing access to these cutting-edge\nsolutions that enable real-time waste analysis is essential for scaling up\nrecycling efforts and fostering the Green Deal. In response, we introduce\n\\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to\naccelerate the recovery of critical raw materials through accurate electrolyzer\nmaterials classification. The dataset comprises 55 co-registered\nhigh-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning\nthe 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and\n424,169 labeled ones. This enables non-invasive spectral analysis of shredded\nelectrolyzer samples, supporting quantitative and qualitative material\nclassification and spectral properties investigation. We evaluate a suite of\nbaseline machine learning (ML) methods alongside SOTA transformer-based deep\nlearning (DL) architectures, including Vision Transformer, SpectralFormer, and\nthe Multimodal Fusion Transformer, to investigate architectural bottlenecks for\nfurther efficiency optimisation when deploying transformers in material\nidentification. We implement zero-shot detection techniques and majority voting\nacross pixel-level predictions to establish object-level classification\nrobustness. In adherence to the FAIR data principles, the electrolyzers-HSI\ndataset and accompanying codebase are openly available at\nhttps://github.com/hifexplo/Electrolyzers-HSI and\nhttps://rodare.hzdr.de/record/3668, supporting reproducible research and\nfacilitating the broader adoption of smart and sustainable e-waste recycling\nsolutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aElectrolyzers-HSI\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u52a0\u901f\u5173\u952e\u539f\u6750\u6599\u7684\u56de\u6536\uff0c\u5e76\u8bc4\u4f30\u4e86\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u6750\u6599\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53ef\u6301\u7eed\u56de\u6536\u7684\u5168\u7403\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u5feb\u901f\u548c\u51c6\u786e\u7684\u6750\u6599\u68c0\u6d4b\u7cfb\u7edf\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\u548c\u7eff\u8272\u534f\u8bae\u3002", "method": "\u4f7f\u752855\u4e2a\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u548cHSI\u6570\u636e\u7acb\u65b9\u4f53\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecVision Transformer\u548cSpectralFormer\u5728\u5185\u7684\u591a\u79cd\u57fa\u7ebfML\u548cSOTA Transformer\u67b6\u6784\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b420\u4e07\u50cf\u7d20\u5411\u91cf\u548c42\u4e07\u6807\u8bb0\u50cf\u7d20\uff0c\u652f\u6301\u975e\u4fb5\u5165\u6027\u5149\u8c31\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86Transformer\u5728\u6750\u6599\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5f00\u6e90\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u63a8\u52a8\u667a\u80fd\u548c\u53ef\u6301\u7eed\u7535\u5b50\u5e9f\u7269\u56de\u6536\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.20380", "pdf": "https://arxiv.org/pdf/2505.20380", "abs": "https://arxiv.org/abs/2505.20380", "authors": ["Simin Fan", "Maria Ios Glarou", "Martin Jaggi"], "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining", "categories": ["cs.LG"], "comment": null, "summary": "The performance of large language models (LLMs) across diverse downstream\napplications is fundamentally governed by the quality and composition of their\npretraining corpora. Existing domain reweighting algorithms primarily optimize\ndata mixtures for a single target task, thereby resulting in models that\noverfit to specialized objectives while exhibiting substantial performance\ndegradation on other benchmarks. This paper introduces Group Robust\nMulti-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target\ndomain reweighting framework designed to calibrate pretraining data mixtures\nfor robust performance across multiple target tasks simultaneously. GRAPE\ndynamically adjusts sampling weights across source domains (domain weights)\nwhile concurrently modulating task weights that quantify the relative\nimportance of each individual target task. This adaptive process prioritizes\ntasks based on their learning difficulty throughout training. We formulate this\ninterleaved reweighting mechanism as a minimax optimization problem: The inner\nmaximization adjusts task weights leveraging group\ndistributed-robust-optimization (DRO), where those tasks demonstrating the\nleast improvement under the current data mixture are prioritized with higher\nweights; The outer minimization then optimizes domain weights to maximize loss\nreduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama\ndatasets demonstrate that GRAPE consistently outperforms baseline methods in\nterms of reasoning performance across 6 benchmarks. Furthermore, when applied\nto multilingual targets, GRAPE effectively identifies optimal training mixtures\nfrom mainstream languages, achieving superior language modeling capabilities\nacross 8 low-resource target languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRAPE\u7684\u591a\u6e90\u591a\u76ee\u6807\u4efb\u52a1\u57df\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6e90\u57df\u548c\u76ee\u6807\u4efb\u52a1\u7684\u6743\u91cd\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57df\u91cd\u52a0\u6743\u7b97\u6cd5\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u76ee\u6807\u4efb\u52a1\u4f18\u5316\u6570\u636e\u6df7\u5408\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5176\u4ed6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0b\u964d\u3002GRAPE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u9c81\u68d2\u6027\u3002", "method": "GRAPE\u901a\u8fc7\u6700\u5c0f\u6700\u5927\u5316\u4f18\u5316\u52a8\u6001\u8c03\u6574\u6e90\u57df\u6743\u91cd\u548c\u4efb\u52a1\u6743\u91cd\uff0c\u5185\u5c42\u6700\u5927\u5316\u5229\u7528DRO\u4f18\u5148\u5b66\u4e60\u56f0\u96be\u4efb\u52a1\uff0c\u5916\u5c42\u6700\u5c0f\u5316\u4f18\u5316\u57df\u6743\u91cd\u4ee5\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5728ClimbLab\u548cSlimPajama\u6570\u636e\u96c6\u4e0a\uff0cGRAPE\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u57288\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "GRAPE\u901a\u8fc7\u52a8\u6001\u591a\u4efb\u52a1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.20521", "pdf": "https://arxiv.org/pdf/2505.20521", "abs": "https://arxiv.org/abs/2505.20521", "authors": ["Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Luis Fraz\u00e3o", "Nuno Costa", "Ant\u00f3nio Pereira"], "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.1; H.5.2"], "comment": "28 pages, 5 figures. Submitted for review to Information Fusion", "summary": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity.", "AI": {"tldr": "Project Riley\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u3001\u591a\u6a21\u578b\u5bf9\u8bddAI\u67b6\u6784\uff0c\u6a21\u62df\u53d7\u60c5\u7eea\u72b6\u6001\u5f71\u54cd\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e94\u79cd\u60c5\u7eea\u4ee3\u7406\u751f\u6210\u3001\u6279\u8bc4\u548c\u4f18\u5316\u54cd\u5e94\uff0c\u6700\u7ec8\u901a\u8fc7\u63a8\u7406\u673a\u5236\u8f93\u51fa\u7ed3\u679c\u3002", "motivation": "\u53d7\u300a\u5934\u8111\u7279\u5de5\u961f\u300b\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u60c5\u7eea\u5f71\u54cd\u63a8\u7406\u7684AI\u7cfb\u7edf\uff0c\u63d0\u5347\u5bf9\u8bddAI\u7684\u60c5\u611f\u8868\u8fbe\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u4e94\u79cd\u60c5\u7eea\u4ee3\u7406\uff08Joy\u3001Sadness\u3001Fear\u3001Anger\u3001Disgust\uff09\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9LLM\uff0c\u5e76\u901a\u8fc7\u63a8\u7406\u673a\u5236\u6574\u5408\u8f93\u51fa\u3002", "result": "\u7528\u6237\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u60c5\u611f\u5bf9\u9f50\u548c\u6c9f\u901a\u6e05\u6670\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Project Riley\u5728\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u60c5\u611f\u8868\u8fbe\u548c\u5b9e\u7528\u6027\u65b9\u9762\u3002", "relevance": 60.0}}
{"id": "2505.20338", "pdf": "https://arxiv.org/pdf/2505.20338", "abs": "https://arxiv.org/abs/2505.20338", "authors": ["Cayo Viegas", "Rohit Gheyi", "M\u00e1rcio Ribeiro"], "title": "Assessing the Capability of LLMs in Solving POSCOMP Questions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nexpanded the capabilities of artificial intelligence in natural language\nprocessing tasks. Despite this progress, their performance in specialized\ndomains such as computer science remains relatively unexplored. Understanding\nthe proficiency of LLMs in these domains is critical for evaluating their\npractical utility and guiding future developments. The POSCOMP, a prestigious\nBrazilian examination used for graduate admissions in computer science promoted\nby the Brazlian Computer Society (SBC), provides a challenging benchmark. This\nstudy investigates whether LLMs can match or surpass human performance on the\nPOSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and\nLe Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP\nexams. The assessments measured the models' proficiency in handling complex\nquestions typical of the exam. LLM performance was notably better on text-based\nquestions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led\nwith 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced\n(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were\nobserved in the 2023 exam. ChatGPT-4 achieved the highest performance,\nsurpassing all students who took the POSCOMP 2023 exam. LLMs, particularly\nChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image\ninterpretation remains a challenge. Given the rapid evolution of LLMs, we\nexpanded our analysis to include more recent models - o1, Gemini 2.5 Pro,\nClaude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.\nThese newer models demonstrate further improvements and consistently surpass\nboth the average and top-performing human participants across all three years.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5df4\u897f\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u751f\u5165\u5b66\u8003\u8bd5\uff08POSCOMP\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0ChatGPT-4\u7b49\u6a21\u578b\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u56fe\u50cf\u89e3\u91ca\u4ecd\u662f\u6311\u6218\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u8ba1\u7b97\u673a\u79d1\u5b66\uff09\u7684\u8868\u73b0\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b9e\u7528\u6027\u5e76\u6307\u5bfc\u672a\u6765\u53d1\u5c55\u3002", "method": "\u8bc4\u4f30\u4e86ChatGPT-4\u3001Gemini 1.0 Advanced\u3001Claude 3 Sonnet\u548cLe Chat Mistral Large\u7b49\u6a21\u578b\u57282022-2023\u5e74POSCOMP\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6269\u5c55\u5206\u6790\u4e86\u66f4\u65b0\u7684\u6a21\u578b\u57282022-2024\u5e74\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "result": "ChatGPT-4\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u8003\u751f\uff1b\u65b0\u6a21\u578b\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5168\u9762\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\u3002", "conclusion": "LLMs\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6587\u672c\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u56fe\u50cf\u89e3\u91ca\u4ecd\u9700\u6539\u8fdb\u3002", "relevance": 85.0}}
{"id": "2505.20510", "pdf": "https://arxiv.org/pdf/2505.20510", "abs": "https://arxiv.org/abs/2505.20510", "authors": ["Yuxuan Sun", "Yixuan Si", "Chenglu Zhu", "Kai Zhang", "Zhongyi Shui", "Bowen Ding", "Tao Lin", "Lin Yang"], "title": "CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic", "categories": ["cs.CV"], "comment": "49 pages, 33 figures", "summary": "Recent advances in computational pathology have led to the emergence of\nnumerous foundation models. However, these approaches fail to replicate the\ndiagnostic process of pathologists, as they either simply rely on\ngeneral-purpose encoders with multi-instance learning for classification or\ndirectly apply multimodal models to generate reports from images. A significant\nlimitation is their inability to emulate the diagnostic logic employed by\npathologists, who systematically examine slides at low magnification for\noverview before progressively zooming in on suspicious regions to formulate\ncomprehensive diagnoses. To address this gap, we introduce CPathAgent, an\ninnovative agent-based model that mimics pathologists' reasoning processes by\nautonomously executing zoom-in/out and navigation operations across pathology\nimages based on observed visual features. To achieve this, we develop a\nmulti-stage training strategy unifying patch-level, region-level, and\nwhole-slide capabilities within a single model, which is essential for\nmimicking pathologists, who require understanding and reasoning capabilities\nacross all three scales. This approach generates substantially more detailed\nand interpretable diagnostic reports compared to existing methods, particularly\nfor huge region understanding. Additionally, we construct an expert-validated\nPathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical\nintermediate scale between patches and whole slides, as diagnosticians\ntypically examine several key regions rather than entire slides at once.\nExtensive experiments demonstrate that CPathAgent consistently outperforms\nexisting approaches across three scales of benchmarks, validating the\neffectiveness of our agent-based diagnostic approach and highlighting a\npromising direction for the future development of computational pathology.", "AI": {"tldr": "CPathAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\uff0c\u6a21\u4eff\u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u903b\u8f91\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u7684\u7406\u89e3\u548c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u62a5\u544a\u7684\u8be6\u7ec6\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u75c5\u7406\u5b66\u65b9\u6cd5\u672a\u80fd\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5c3a\u5ea6\u56fe\u50cf\u7684\u7cfb\u7edf\u6027\u5206\u6790\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86CPathAgent\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7edf\u4e00\u4e86\u8865\u4e01\u7ea7\u3001\u533a\u57df\u7ea7\u548c\u5168\u73bb\u7247\u7ea7\u7684\u80fd\u529b\u3002", "result": "CPathAgent\u5728\u4e09\u4e2a\u5c3a\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u66f4\u8be6\u7ec6\u548c\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u62a5\u544a\u3002", "conclusion": "CPathAgent\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.20435", "pdf": "https://arxiv.org/pdf/2505.20435", "abs": "https://arxiv.org/abs/2505.20435", "authors": ["Aideen Fay", "In\u00e9s Garc\u00eda-Redondo", "Qiquan Wang", "Haim Dubossarsky", "Anthea Monod"], "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence", "categories": ["cs.LG", "cs.AI", "cs.CG", "math.AT"], "comment": null, "summary": "Understanding how adversarial conditions affect language models requires\ntechniques that capture both global structure and local detail within\nhigh-dimensional activation spaces. We propose persistent homology (PH), a tool\nfrom topological data analysis, to systematically characterize multiscale\nlatent space dynamics in LLMs under two distinct attack modes -- backdoor\nfine-tuning and indirect prompt injection. By analyzing six state-of-the-art\nLLMs, we show that adversarial conditions consistently compress latent\ntopologies, reducing structural diversity at smaller scales while amplifying\ndominant features at coarser ones. These topological signatures are\nstatistically robust across layers, architectures, model sizes, and align with\nthe emergence of adversarial effects deeper in the network. To capture\nfiner-grained mechanisms underlying these shifts, we introduce a neuron-level\nPH framework that quantifies how information flows and transforms within and\nacross layers. Together, our findings demonstrate that PH offers a principled\nand unifying approach to interpreting representational dynamics in LLMs,\nparticularly under distributional shift.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6301\u4e45\u540c\u8c03\uff08PH\uff09\u5206\u6790LLMs\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\uff0c\u63ed\u793a\u5bf9\u6297\u653b\u51fb\u5bf9\u62d3\u6251\u7ed3\u6784\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5bf9\u6297\u6761\u4ef6\uff08\u5982\u540e\u95e8\u5fae\u8c03\u548c\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\uff09\u5982\u4f55\u5f71\u54cdLLMs\u7684\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u6301\u4e45\u540c\u8c03\uff08PH\uff09\u5206\u6790\u516d\u79cd\u5148\u8fdbLLMs\u7684\u6f5c\u5728\u7a7a\u95f4\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u795e\u7ecf\u5143\u7ea7PH\u6846\u67b6\u91cf\u5316\u4fe1\u606f\u6d41\u52a8\u3002", "result": "\u5bf9\u6297\u6761\u4ef6\u538b\u7f29\u6f5c\u5728\u62d3\u6251\u7ed3\u6784\uff0c\u51cf\u5c11\u5c0f\u5c3a\u5ea6\u7ed3\u6784\u591a\u6837\u6027\uff0c\u653e\u5927\u7c97\u5c3a\u5ea6\u4e3b\u5bfc\u7279\u5f81\uff0c\u4e14\u8fd9\u4e9b\u7279\u5f81\u5728\u7f51\u7edc\u6df1\u5c42\u66f4\u663e\u8457\u3002", "conclusion": "PH\u4e3a\u89e3\u91caLLMs\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8868\u5f81\u52a8\u6001\u63d0\u4f9b\u4e86\u7edf\u4e00\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.20522", "pdf": "https://arxiv.org/pdf/2505.20522", "abs": "https://arxiv.org/abs/2505.20522", "authors": ["Jian Wang", "Boyan Zhu", "Chak Tou Leong", "Yongqi Li", "Wenjie Li"], "title": "Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Large reasoning models (LRMs) have exhibited the capacity of enhancing\nreasoning performance via internal test-time scaling. Building upon this, a\npromising direction is to further scale test-time compute to unlock even\ngreater reasoning capabilities. However, as we push these scaling boundaries,\nsystematically understanding the practical limits and achieving optimal\nresource allocation becomes a critical challenge. In this paper, we investigate\nthe scaling Pareto of test-time scaling and introduce the Test-Time Scaling\nPerformance Model (TTSPM). We theoretically analyze two fundamental paradigms\nfor such extended scaling, parallel scaling and sequential scaling, from a\nprobabilistic modeling perspective. Our primary contribution is the derivation\nof the saturation point on the scaling budget for both strategies, identifying\nthresholds beyond which additional computation yields diminishing returns.\nRemarkably, despite their distinct mechanisms, both paradigms converge to a\nunified mathematical structure in their upper bounds. We empirically validate\nour theoretical findings on challenging reasoning benchmarks, including AIME,\nMATH-500, and GPQA, demonstrating the practical utility of these bounds for\ntest-time resource allocation. We hope that this work provides insights into\nthe cost-benefit trade-offs of test-time scaling, guiding the development of\nmore resource-efficient inference strategies for large reasoning models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u4e2d\u7684\u6027\u80fd\u9971\u548c\u70b9\uff0c\u63d0\u51fa\u4e86TTSPM\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5e76\u884c\u548c\u987a\u5e8f\u6269\u5c55\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u63a2\u7d22\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u7684\u6781\u9650\u548c\u6700\u4f18\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e86\u5e76\u884c\u548c\u987a\u5e8f\u6269\u5c55\u7684\u4e24\u79cd\u8303\u5f0f\uff0c\u63a8\u5bfc\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u9971\u548c\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u786e\u5b9a\u4e86\u4e24\u79cd\u6269\u5c55\u8303\u5f0f\u7684\u6027\u80fd\u9971\u548c\u70b9\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728AIME\u3001MATH-500\u548cGPQA\u7b49\u57fa\u51c6\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6210\u672c\u6548\u76ca\u6743\u8861\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\u3002", "relevance": 85.0}}
{"id": "2505.20340", "pdf": "https://arxiv.org/pdf/2505.20340", "abs": "https://arxiv.org/abs/2505.20340", "authors": ["Yukun Zhang", "Qi Dong"], "title": "Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework\nthat models large language model generation as a controlled dynamical system\nevolving on a low_dimensional semantic manifold. By casting latent_state\nupdates as discrete time Euler approximations of continuous dynamics, we map\nintrinsic energy_driven flows and context_dependent forces onto Transformer\ncomponents (residual connections, attention, feed-forward networks). Leveraging\nLyapunov stability theory We define three empirical metrics (state continuity,\nclustering quality, topological persistence) that quantitatively link\nlatent_trajectory properties to text fluency, grammaticality, and semantic\ncoherence. Extensive experiments across decoding parameters validate DMET's\npredictions and yield principled guidelines for balancing creativity and\nconsistency in text generation.", "AI": {"tldr": "DMET\u5c06LLM\u751f\u6210\u5efa\u6a21\u4e3a\u4f4e\u7ef4\u8bed\u4e49\u6d41\u5f62\u4e0a\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u80fd\u91cf\u9a71\u52a8\u6d41\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u529b\u6620\u5c04Transformer\u7ec4\u4ef6\uff0c\u63d0\u51fa\u4e09\u4e2a\u6307\u6807\u91cf\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u7406\u89e3LLM\u751f\u6210\u8fc7\u7a0b\u7684\u52a8\u6001\u7279\u6027\uff0c\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u4ee5\u4f18\u5316\u6587\u672c\u751f\u6210\u7684\u521b\u9020\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "method": "\u5c06\u6f5c\u5728\u72b6\u6001\u66f4\u65b0\u5efa\u6a21\u4e3a\u8fde\u7eed\u52a8\u529b\u5b66\u7684\u79bb\u6563\u8fd1\u4f3c\uff0c\u5229\u7528Lyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u5b9a\u4e49\u4e09\u4e2a\u6307\u6807\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1DMET\u7684\u9884\u6d4b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e73\u8861\u751f\u6210\u521b\u9020\u6027\u4e0e\u4e00\u81f4\u6027\u7684\u6307\u5bfc\u539f\u5219\u3002", "conclusion": "DMET\u4e3aLLM\u751f\u6210\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u5e76\u4e3a\u4f18\u5316\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u5b9e\u7528\u6307\u6807\u3002", "relevance": 85.0}}
{"id": "2505.20512", "pdf": "https://arxiv.org/pdf/2505.20512", "abs": "https://arxiv.org/abs/2505.20512", "authors": ["Tangzheng Lian", "Oya Celiktutan"], "title": "A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Affective Computing", "summary": "Recent studies on fairness have shown that Facial Expression Recognition\n(FER) models exhibit biases toward certain visually perceived demographic\ngroups. However, the limited availability of human-annotated demographic labels\nin public FER datasets has constrained the scope of such bias analysis. To\novercome this limitation, some prior works have resorted to pseudo-demographic\nlabels, which may distort bias evaluation results. Alternatively, in this\npaper, we propose a feature-level bias evaluation framework for evaluating\ndemographic biases in FER models under the setting where demographic labels are\nunavailable in the test set. Extensive experiments demonstrate that our method\nmore effectively evaluates demographic biases compared to existing approaches\nthat rely on pseudo-demographic labels. Furthermore, we observe that many\nexisting studies do not include statistical testing in their bias evaluations,\nraising concerns that some reported biases may not be statistically significant\nbut rather due to randomness. To address this issue, we introduce a\nplug-and-play statistical module to ensure the statistical significance of\nbiased evaluation results. A comprehensive bias analysis based on the proposed\nmodule is then conducted across three sensitive attributes (age, gender, and\nrace), seven facial expressions, and multiple network architectures on a\nlarge-scale dataset, revealing the prominent demographic biases in FER and\nproviding insights on selecting a fairer network architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u6807\u7b7e\u7684\u7279\u5f81\u7ea7\u504f\u5dee\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u6a21\u578b\u7684\u504f\u5dee\u5206\u6790\uff0c\u5e76\u5f15\u5165\u7edf\u8ba1\u6a21\u5757\u786e\u4fdd\u7ed3\u679c\u663e\u8457\u6027\u3002", "motivation": "\u73b0\u6709FER\u6a21\u578b\u5b58\u5728\u5bf9\u67d0\u4e9b\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u4f46\u516c\u5f00\u6570\u636e\u96c6\u7f3a\u4e4f\u6807\u6ce8\u6807\u7b7e\uff0c\u4f2a\u6807\u7b7e\u53ef\u80fd\u626d\u66f2\u8bc4\u4f30\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u7279\u5f81\u7ea7\u504f\u5dee\u8bc4\u4f30\u6846\u67b6\uff0c\u65e0\u9700\u6d4b\u8bd5\u96c6\u4eba\u53e3\u7edf\u8ba1\u6807\u7b7e\uff0c\u5e76\u52a0\u5165\u7edf\u8ba1\u6a21\u5757\u9a8c\u8bc1\u7ed3\u679c\u663e\u8457\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u4f9d\u8d56\u4f2a\u6807\u7b7e\u7684\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u63ed\u793a\u4e86FER\u6a21\u578b\u5728\u5e74\u9f84\u3001\u6027\u522b\u548c\u79cd\u65cf\u4e0a\u7684\u663e\u8457\u504f\u89c1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aFER\u6a21\u578b\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u5e76\u6307\u5bfc\u9009\u62e9\u66f4\u516c\u5e73\u7684\u7f51\u7edc\u67b6\u6784\u3002", "relevance": 50.0}}
{"id": "2505.20444", "pdf": "https://arxiv.org/pdf/2505.20444", "abs": "https://arxiv.org/abs/2505.20444", "authors": ["Haoran Li", "Yingjie Qin", "Baoyuan Ou", "Lai Xu", "Ruiwen Xu"], "title": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have made significant progress in multimodal\ntasks. However, their performance often deteriorates in long-context scenarios,\nparticularly long videos. While Rotary Position Embedding (RoPE) has been\nwidely adopted for length generalization in Large Language Models (LLMs),\nextending vanilla RoPE to capture the intricate spatial-temporal dependencies\nin videos remains an unsolved challenge. Existing methods typically allocate\ndifferent frequencies within RoPE to encode 3D positional information. However,\nthese allocation strategies mainly rely on heuristics, lacking in-depth\ntheoretical analysis. In this paper, we first study how different allocation\nstrategies impact the long-context capabilities of VLMs. Our analysis reveals\nthat current multimodal RoPEs fail to reliably capture semantic similarities\nover extended contexts. To address this issue, we propose HoPE, a Hybrid of\nPosition Embedding designed to improve the long-context capabilities of VLMs.\nHoPE introduces a hybrid frequency allocation strategy for reliable semantic\nmodeling over arbitrarily long context, and a dynamic temporal scaling\nmechanism to facilitate robust learning and flexible inference across diverse\ncontext lengths. Extensive experiments across four video benchmarks on long\nvideo understanding and retrieval tasks demonstrate that HoPE consistently\noutperforms existing methods, confirming its effectiveness. Code is available\nat https://github.com/hrlics/HoPE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHoPE\u7684\u65b0\u578b\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u901a\u8fc7\u6df7\u5408\u9891\u7387\u5206\u914d\u7b56\u7565\u548c\u52a8\u6001\u65f6\u95f4\u7f29\u653e\u673a\u5236\uff0cHoPE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7b49\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u7684RoPE\u6269\u5c55\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u65e0\u6cd5\u53ef\u9760\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "method": "\u63d0\u51faHoPE\uff0c\u7ed3\u5408\u6df7\u5408\u9891\u7387\u5206\u914d\u7b56\u7565\u548c\u52a8\u6001\u65f6\u95f4\u7f29\u653e\u673a\u5236\uff0c\u4f18\u5316\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "result": "\u5728\u56db\u4e2a\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHoPE\u5728\u957f\u89c6\u9891\u7406\u89e3\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HoPE\u6709\u6548\u63d0\u5347\u4e86VLMs\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 75.0}}
{"id": "2505.20609", "pdf": "https://arxiv.org/pdf/2505.20609", "abs": "https://arxiv.org/abs/2505.20609", "authors": ["Hyungjun Park", "Chang-Yun Woo", "Seungjo Lim", "Seunghwan Lim", "Keunho Kwak", "Ju Young Jeong", "Chong Hyun Suh"], "title": "Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Objective To develop an LLM based realtime compound diagnostic medical AI\ninterface and performed a clinical trial comparing this interface and\nphysicians for common internal medicine cases based on the United States\nMedical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A\nnonrandomized clinical trial was conducted on August 20, 2024. We recruited one\ngeneral physician, two internal medicine residents (2nd and 3rd year), and five\nsimulated patients. The clinical vignettes were adapted from the USMLE Step 2\nCS style exams. We developed 10 representative internal medicine cases based on\nactual patients and included information available on initial diagnostic\nevaluation. Primary outcome was the accuracy of the first differential\ndiagnosis. Repeatability was evaluated based on the proportion of agreement.\nResults The accuracy of the physicians' first differential diagnosis ranged\nfrom 50% to 70%, whereas the realtime compound diagnostic medical AI interface\nachieved an accuracy of 80%. The proportion of agreement for the first\ndifferential diagnosis was 0.7. The accuracy of the first and second\ndifferential diagnoses ranged from 70% to 90% for physicians, whereas the AI\ninterface achieved an accuracy rate of 100%. The average time for the AI\ninterface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).\nThe AI interface ($0.08) also reduced costs by 98.1% compared to the\nphysicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3\nfor care by physicians and were 3.9 for the AI interface Conclusion An LLM\nbased realtime compound diagnostic medical AI interface demonstrated diagnostic\naccuracy and patient satisfaction comparable to those of a physician, while\nrequiring less time and lower costs. These findings suggest that AI interfaces\nmay have the potential to assist primary care consultations for common internal\nmedicine cases.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5b9e\u65f6\u590d\u5408\u8bca\u65ad\u533b\u7597AI\u754c\u9762\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u8bd5\u9a8c\u6bd4\u8f83\u5176\u4e0e\u533b\u751f\u5728\u5e38\u89c1\u5185\u79d1\u75c5\u4f8b\u4e2d\u7684\u8868\u73b0\u3002AI\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u533b\u751f\u3002", "motivation": "\u63a2\u7d22AI\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002", "method": "\u975e\u968f\u673a\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u4f7f\u7528USMLE Step 2 CS\u98ce\u683c\u7684\u75c5\u4f8b\uff0c\u6bd4\u8f83AI\u754c\u9762\u4e0e\u533b\u751f\u7684\u8868\u73b0\u3002", "result": "AI\u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0880%\uff09\u9ad8\u4e8e\u533b\u751f\uff0850%-70%\uff09\uff0c\u4e14\u65f6\u95f4\u548c\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684AI\u754c\u9762\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u8f85\u52a9\u521d\u7ea7\u533b\u7597\u54a8\u8be2\u3002", "relevance": 70.0}}
{"id": "2505.20343", "pdf": "https://arxiv.org/pdf/2505.20343", "abs": "https://arxiv.org/abs/2505.20343", "authors": ["Sonal Prabhune", "Balaji Padmanabhan", "Kaushik Dutta"], "title": "Do LLMs have a Gender (Entropy) Bias?", "categories": ["cs.CL", "cs.AI", "68T42, 68T50", "I.2.7"], "comment": "18 pages, 4 figures", "summary": "We investigate the existence and persistence of a specific type of gender\nbias in some of the popular LLMs and contribute a new benchmark dataset,\nRealWorldQuestioning (released on HuggingFace ), developed from real-world\nquestions across four key domains in business and health contexts: education,\njobs, personal financial management, and general health. We define and study\nentropy bias, which we define as a discrepancy in the amount of information\ngenerated by an LLM in response to real questions users have asked. We tested\nthis using four different LLMs and evaluated the generated responses both\nqualitatively and quantitatively by using ChatGPT-4o (as \"LLM-as-judge\"). Our\nanalyses (metric-based comparisons and \"LLM-as-judge\" evaluation) suggest that\nthere is no significant bias in LLM responses for men and women at a category\nlevel. However, at a finer granularity (the individual question level), there\nare substantial differences in LLM responses for men and women in the majority\nof cases, which \"cancel\" each other out often due to some responses being\nbetter for males and vice versa. This is still a concern since typical users of\nthese tools often ask a specific question (only) as opposed to several varied\nones in each of these common yet important areas of life. We suggest a simple\ndebiasing approach that iteratively merges the responses for the two genders to\nproduce a final result. Our approach demonstrates that a simple, prompt-based\ndebiasing strategy can effectively debias LLM outputs, thus producing responses\nwith higher information content than both gendered variants in 78% of the\ncases, and consistently achieving a balanced integration in the remaining\ncases.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6d41\u884cLLM\u4e2d\u7279\u5b9a\u6027\u522b\u504f\u89c1\u7684\u5b58\u5728\u4e0e\u6301\u7eed\u6027\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6RealWorldQuestioning\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7c7b\u522b\u5c42\u9762LLM\u54cd\u5e94\u65e0\u663e\u8457\u6027\u522b\u504f\u89c1\uff0c\u4f46\u5728\u4e2a\u4f53\u95ee\u9898\u5c42\u9762\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u7814\u7a76LLM\u5728\u6027\u522b\u504f\u89c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u4e2d\u7684\u54cd\u5e94\u5dee\u5f02\uff0c\u65e8\u5728\u63d0\u9ad8LLM\u7684\u516c\u5e73\u6027\u548c\u4fe1\u606f\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4e0d\u540cLLM\u6d4b\u8bd5\uff0c\u901a\u8fc7ChatGPT-4o\u4f5c\u4e3a\u201cLLM-as-judge\u201d\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u53bb\u504f\u89c1\u7b56\u7565\u3002", "result": "\u7c7b\u522b\u5c42\u9762\u65e0\u663e\u8457\u504f\u89c1\uff0c\u4f46\u4e2a\u4f53\u95ee\u9898\u5c42\u9762\u5b58\u5728\u5dee\u5f02\u3002\u63d0\u51fa\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\u572878%\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u4fe1\u606f\u91cf\uff0c\u5176\u4f59\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5e73\u8861\u6574\u5408\u3002", "conclusion": "\u7b80\u5355\u7684\u63d0\u793a\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u53bb\u504f\u89c1\uff0c\u63d0\u5347LLM\u54cd\u5e94\u7684\u516c\u5e73\u6027\u548c\u4fe1\u606f\u8d28\u91cf\u3002", "relevance": 85.0}}
{"id": "2505.20513", "pdf": "https://arxiv.org/pdf/2505.20513", "abs": "https://arxiv.org/abs/2505.20513", "authors": ["Wenhao Gu", "Li Gu", "Ching Yee Suen", "Yang Wang"], "title": "MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recent advancements in handwritten text recognition (HTR) have enabled the\neffective conversion of handwritten text to digital formats. However, achieving\nrobust recognition across diverse writing styles remains challenging.\nTraditional HTR methods lack writer-specific personalization at test time due\nto limitations in model architecture and training strategies. Existing attempts\nto bridge this gap, through gradient-based meta-learning, still require labeled\nexamples and suffer from parameter-inefficient fine-tuning, leading to\nsubstantial computational and memory overhead. To overcome these challenges, we\npropose an efficient framework that formulates personalization as prompt\ntuning, incorporating an auxiliary image reconstruction task with a\nself-supervised loss to guide prompt adaptation with unlabeled test-time\nexamples. To ensure self-supervised loss effectively minimizes text recognition\nerror, we leverage meta-learning to learn the optimal initialization of the\nprompts. As a result, our method allows the model to efficiently capture unique\nwriting styles by updating less than 1% of its parameters and eliminating the\nneed for time-intensive annotation processes. We validate our approach on the\nRIMES and IAM Handwriting Database benchmarks, where it consistently\noutperforms previous state-of-the-art methods while using 20x fewer parameters.\nWe believe this represents a significant advancement in personalized\nhandwritten text recognition, paving the way for more reliable and practical\ndeployment in resource-constrained scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u624b\u5199\u6587\u672c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u8c03\u4f18\u548c\u81ea\u76d1\u7763\u635f\u5931\uff0c\u4ec5\u9700\u66f4\u65b01%\u7684\u53c2\u6570\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u4e66\u5199\u98ce\u683c\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u624b\u5199\u6587\u672c\u8bc6\u522b\u65b9\u6cd5\u7f3a\u4e4f\u6d4b\u8bd5\u65f6\u7684\u4e2a\u6027\u5316\u9002\u5e94\u80fd\u529b\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\u6216\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5c06\u4e2a\u6027\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u63d0\u793a\u8c03\u4f18\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u635f\u5931\u548c\u5143\u5b66\u4e60\u4f18\u5316\u63d0\u793a\u521d\u59cb\u5316\uff0c\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\u3002", "result": "\u5728RIMES\u548cIAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u53c2\u6570\u51cf\u5c1120\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4e2a\u6027\u5316\u624b\u5199\u6587\u672c\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20446", "pdf": "https://arxiv.org/pdf/2505.20446", "abs": "https://arxiv.org/abs/2505.20446", "authors": ["Tal Gonen", "Itai Pemper", "Ilan Naiman", "Nimrod Berman", "Omri Azencot"], "title": "Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach", "categories": ["cs.LG"], "comment": "The first two authors contributed equally", "summary": "Generative modeling of time series is a central challenge in time series\nanalysis, particularly under data-scarce conditions. Despite recent advances in\ngenerative modeling, a comprehensive understanding of how state-of-the-art\ngenerative models perform under limited supervision remains lacking. In this\nwork, we conduct the first large-scale study evaluating leading generative\nmodels in data-scarce settings, revealing a substantial performance gap between\nfull-data and data-scarce regimes. To close this gap, we propose a unified\ndiffusion-based generative framework that can synthesize high-fidelity time\nseries across diverse domains using just a few examples. Our model is\npre-trained on a large, heterogeneous collection of time series datasets,\nenabling it to learn generalizable temporal representations. It further\nincorporates architectural innovations such as dynamic convolutional layers for\nflexible channel adaptation and dataset token conditioning for domain-aware\ngeneration. Without requiring abundant supervision, our unified model achieves\nstate-of-the-art performance in few-shot settings-outperforming domain-specific\nbaselines across a wide range of subset sizes. Remarkably, it also surpasses\nall baselines even when tested on full datasets benchmarks, highlighting the\nstrength of pre-training and cross-domain generalization. We hope this work\nencourages the community to revisit few-shot generative modeling as a key\nproblem in time series research and pursue unified solutions that scale\nefficiently across domains. Code is available at\nhttps://github.com/azencot-group/ImagenFew.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6269\u6563\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u8de8\u9886\u57df\u6cdb\u5316\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u6027\u80fd\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u6269\u6563\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u5377\u79ef\u5c42\u548c\u6570\u636e\u96c6\u6807\u8bb0\u6761\u4ef6\uff0c\u9884\u8bad\u7ec3\u4e8e\u591a\u6837\u5316\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u5728\u5c11\u6837\u672c\u548c\u5168\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8d8a\u9886\u57df\u4e13\u7528\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u548c\u8de8\u9886\u57df\u6cdb\u5316\u7684\u4f18\u52bf\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u91cd\u89c6\u5c11\u6837\u672c\u751f\u6210\u5efa\u6a21\uff0c\u5e76\u63a2\u7d22\u8de8\u9886\u57df\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.20642", "pdf": "https://arxiv.org/pdf/2505.20642", "abs": "https://arxiv.org/abs/2505.20642", "authors": ["Yi Zhan", "Qi Liu", "Weibo Gao", "Zheng Zhang", "Tianfu Wang", "Shuanghong Shen", "Junyu Lu", "Zhenya Huang"], "title": "CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models", "categories": ["cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Personalized programming tutoring, such as exercise recommendation, can\nenhance learners' efficiency, motivation, and outcomes, which is increasingly\nimportant in modern digital education. However, the lack of sufficient and\nhigh-quality programming data, combined with the mismatch between offline\nevaluation and real-world learning, hinders the practical deployment of such\nsystems. To address this challenge, many approaches attempt to simulate learner\npractice data, yet they often overlook the fine-grained, iterative nature of\nprogramming learning, resulting in a lack of interpretability and granularity.\nTo fill this gap, we propose a LLM-based agent, CoderAgent, to simulate\nstudents' programming processes in a fine-grained manner without relying on\nreal data. Specifically, we equip each human learner with an intelligent agent,\nthe core of which lies in capturing the cognitive states of the human\nprogramming practice process. Inspired by ACT-R, a cognitive architecture\nframework, we design the structure of CoderAgent to align with human cognitive\narchitecture by focusing on the mastery of programming knowledge and the\napplication of coding ability. Recognizing the inherent patterns in\nmulti-layered cognitive reasoning, we introduce the Programming Tree of Thought\n(PTOT), which breaks down the process into four steps: why, how, where, and\nwhat. This approach enables a detailed analysis of iterative problem-solving\nstrategies. Finally, experimental evaluations on real-world datasets\ndemonstrate that CoderAgent provides interpretable insights into learning\ntrajectories and achieves accurate simulations, paving the way for personalized\nprogramming education.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406CoderAgent\uff0c\u7528\u4e8e\u6a21\u62df\u5b66\u751f\u7684\u7f16\u7a0b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u7f16\u7a0b\u6559\u80b2\u4e2d\u6570\u636e\u4e0d\u8db3\u548c\u8bc4\u4f30\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u4e2a\u6027\u5316\u7f16\u7a0b\u6559\u80b2\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u771f\u5b9e\u5b66\u4e60\u573a\u666f\u7684\u5339\u914d\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u7f16\u7a0b\u5b66\u4e60\u7684\u7ec6\u7c92\u5ea6\u548c\u8fed\u4ee3\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eACT-R\u8ba4\u77e5\u67b6\u6784\u7684LLM\u4ee3\u7406CoderAgent\uff0c\u5f15\u5165\u7f16\u7a0b\u601d\u7ef4\u6811\uff08PTOT\uff09\u5206\u89e3\u5b66\u4e60\u8fc7\u7a0b\u4e3a\u56db\u4e2a\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoderAgent\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u8f68\u8ff9\u5206\u6790\u5e76\u5b9e\u73b0\u7cbe\u51c6\u6a21\u62df\u3002", "conclusion": "CoderAgent\u4e3a\u4e2a\u6027\u5316\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.20347", "pdf": "https://arxiv.org/pdf/2505.20347", "abs": "https://arxiv.org/abs/2505.20347", "authors": ["Wenkai Fang", "Shunyu Liu", "Yang Zhou", "Kongcheng Zhang", "Tongya Zheng", "Kaixuan Chen", "Mingli Song", "Dacheng Tao"], "title": "SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have demonstrated the effectiveness of Reinforcement Learning\n(RL) in improving the reasoning capabilities of Large Language Models (LLMs).\nHowever, existing works inevitably rely on high-quality instructions and\nverifiable rewards for effective training, both of which are often difficult to\nobtain in specialized domains. In this paper, we propose Self-play\nReinforcement Learning(SeRL) to bootstrap LLM training with limited initial\ndata. Specifically, SeRL comprises two complementary modules: self-instruction\nand self-rewarding. The former module generates additional instructions based\non the available data at each training step, employing robust online filtering\nstrategies to ensure instruction quality, diversity, and difficulty. The latter\nmodule introduces a simple yet effective majority-voting mechanism to estimate\nresponse rewards for additional instructions, eliminating the need for external\nannotations. Finally, SeRL performs conventional RL based on the generated\ndata, facilitating iterative self-play learning. Extensive experiments on\nvarious reasoning benchmarks and across different LLM backbones demonstrate\nthat the proposed SeRL yields results superior to its counterparts and achieves\nperformance on par with those obtained by high-quality data with verifiable\nrewards. Our code is available at https://github.com/wantbook-book/SeRL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5bf9\u5f08\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08SeRL\uff09\uff0c\u901a\u8fc7\u81ea\u6211\u751f\u6210\u6307\u4ee4\u548c\u5956\u52b1\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u5916\u90e8\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6307\u4ee4\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u4f46\u5728\u4e13\u4e1a\u9886\u57df\u96be\u4ee5\u83b7\u53d6\u3002SeRL\u65e8\u5728\u901a\u8fc7\u81ea\u5bf9\u5f08\u673a\u5236\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SeRL\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1) \u81ea\u6211\u6307\u4ee4\u751f\u6210\uff0c\u901a\u8fc7\u5728\u7ebf\u8fc7\u6ee4\u786e\u4fdd\u8d28\u91cf\uff1b2) \u81ea\u6211\u5956\u52b1\u673a\u5236\uff0c\u91c7\u7528\u591a\u6570\u6295\u7968\u4f30\u8ba1\u5956\u52b1\u3002\u6700\u7ec8\u57fa\u4e8e\u751f\u6210\u6570\u636e\u8fdb\u884c\u5e38\u89c4\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSeRL\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u548c\u4e0d\u540cLLM\u9aa8\u5e72\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u65b9\u6cd5\u3002", "conclusion": "SeRL\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u5bf9\u5f08\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u51cf\u5c11\u4e86\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u5916\u90e8\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "relevance": 85.0}}
{"id": "2505.20525", "pdf": "https://arxiv.org/pdf/2505.20525", "abs": "https://arxiv.org/abs/2505.20525", "authors": ["Aniket Roy", "Maitreya Suin", "Ketul Shah", "Rama Chellappa"], "title": "MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has gained prominence as a computationally\nefficient method for fine-tuning generative models, enabling distinct visual\nconcept synthesis with minimal overhead. However, current methods struggle to\neffectively merge multiple LoRA adapters without training, particularly in\ncomplex compositions involving diverse visual elements. We introduce MultLFG, a\nnovel framework for training-free multi-LoRA composition that utilizes\nfrequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike\nexisting methods that uniformly aggregate concept-specific LoRAs, MultLFG\nemploys a timestep and frequency subband adaptive fusion strategy, selectively\nactivating relevant LoRAs based on content relevance at specific timesteps and\nfrequency bands. This frequency-sensitive guidance not only improves spatial\ncoherence but also provides finer control over multi-LoRA composition, leading\nto more accurate and consistent results. Experimental evaluations on the\nComposLoRA benchmark reveal that MultLFG substantially enhances compositional\nfidelity and image quality across various styles and concept sets,\noutperforming state-of-the-art baselines in multi-concept generation tasks.\nCode will be released.", "AI": {"tldr": "MultLFG\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591aLoRA\u9002\u914d\u5668\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u6307\u5bfc\u5b9e\u73b0\u81ea\u9002\u5e94\u878d\u5408\uff0c\u63d0\u5347\u591a\u6982\u5ff5\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u6709\u6548\u878d\u5408\u591a\u4e2aLoRA\u9002\u914d\u5668\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u591a\u6837\u5316\u89c6\u89c9\u5143\u7d20\u7684\u590d\u6742\u7ec4\u5408\u4e2d\u3002", "method": "MultLFG\u91c7\u7528\u57fa\u4e8e\u65f6\u95f4\u6b65\u548c\u9891\u5e26\u7684\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\uff0c\u9009\u62e9\u6027\u6fc0\u6d3b\u76f8\u5173LoRA\u9002\u914d\u5668\u3002", "result": "\u5728ComposLoRA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMultLFG\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5408\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MultLFG\u901a\u8fc7\u9891\u57df\u6307\u5bfc\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u591aLoRA\u878d\u5408\u6548\u679c\uff0c\u4e3a\u591a\u6982\u5ff5\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 60.0}}
{"id": "2505.20452", "pdf": "https://arxiv.org/pdf/2505.20452", "abs": "https://arxiv.org/abs/2505.20452", "authors": ["Hao Zhao", "Rong Pan"], "title": "Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes", "categories": ["cs.LG"], "comment": null, "summary": "Multiple change point (MCP) detection in non-stationary time series is\nchallenging due to the variety of underlying patterns. To address these\nchallenges, we propose a novel algorithm that integrates Active Learning (AL)\nwith Deep Gaussian Processes (DGPs) for robust MCP detection. Our method\nleverages spectral analysis to identify potential changes and employs AL to\nstrategically select new sampling points for improved efficiency. By\nincorporating the modeling flexibility of DGPs with the change-identification\ncapabilities of spectral methods, our approach adapts to diverse spectral\nchange behaviors and effectively localizes multiple change points. Experiments\non both simulated and real-world data demonstrate that our method outperforms\nexisting techniques in terms of detection accuracy and sampling efficiency for\nnon-stationary time series.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u548c\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\uff08DGPs\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u591a\u53d8\u5316\u70b9\uff08MCP\uff09\u68c0\u6d4b\u3002", "motivation": "\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u591a\u53d8\u5316\u70b9\u68c0\u6d4b\u56e0\u6a21\u5f0f\u591a\u6837\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u8c31\u5206\u6790\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u5229\u7528DGPs\u7684\u5efa\u6a21\u7075\u6d3b\u6027\uff0c\u9ad8\u6548\u5b9a\u4f4d\u53d8\u5316\u70b9\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\uff0c\u65b0\u65b9\u6cd5\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u91c7\u6837\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u7684MCP\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 20.0}}
{"id": "2505.20662", "pdf": "https://arxiv.org/pdf/2505.20662", "abs": "https://arxiv.org/abs/2505.20662", "authors": ["Xuanle Zhao", "Zilin Sang", "Yuxuan Li", "Qi Shi", "Shuo Wang", "Duzhen Zhang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage", "categories": ["cs.AI"], "comment": "20 pages, preprint version", "summary": "Efficient experiment reproduction is critical to accelerating progress in\nartificial intelligence. However, the inherent complexity of method design and\ntraining procedures presents substantial challenges for automation. Notably,\nreproducing experiments often requires implicit domain-specific knowledge not\nexplicitly documented in the original papers. To address this, we introduce the\npaper lineage algorithm, which identifies and extracts implicit knowledge from\nthe relevant references cited by the target paper. Building on this idea, we\npropose AutoReproduce, a multi-agent framework capable of automatically\nreproducing experiments described in research papers in an end-to-end manner.\nAutoReproduce enhances code executability by generating unit tests alongside\nthe reproduction process. To evaluate the reproduction capability, we construct\nReproduceBench, a benchmark annotated with verified implementations, and\nintroduce novel evaluation metrics to assess both the reproduction and\nexecution fidelity. Experimental results demonstrate that AutoReproduce\noutperforms the existing strong agent baselines on all five evaluation metrics\nby a peak margin of over $70\\%$. In particular, compared to the official\nimplementations, AutoReproduce achieves an average performance gap of $22.1\\%$\non $89.74\\%$ of the executable experiment runs. The code will be available at\nhttps://github.com/AI9Stars/AutoReproduce.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAutoReproduce\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u590d\u73b0\u5b9e\u9a8c\uff0c\u5e76\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u4ee5\u63d0\u9ad8\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u89e3\u51b3AI\u7814\u7a76\u4e2d\u5b9e\u9a8c\u590d\u73b0\u7684\u590d\u6742\u6027\u53ca\u9690\u5f0f\u77e5\u8bc6\u63d0\u53d6\u95ee\u9898\u3002", "method": "\u63d0\u51fapaper lineage\u7b97\u6cd5\u63d0\u53d6\u9690\u5f0f\u77e5\u8bc6\uff0c\u5e76\u8bbe\u8ba1AutoReproduce\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "result": "AutoReproduce\u5728ReproduceBench\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e73\u5747\u6027\u80fd\u5dee\u8ddd22.1%\u3002", "conclusion": "AutoReproduce\u6709\u6548\u63d0\u5347\u5b9e\u9a8c\u590d\u73b0\u7684\u81ea\u52a8\u5316\u548c\u51c6\u786e\u6027\u3002", "relevance": 60.0}}
{"id": "2505.20354", "pdf": "https://arxiv.org/pdf/2505.20354", "abs": "https://arxiv.org/abs/2505.20354", "authors": ["Juntong Wu", "Zijing Liu", "He Cao", "Hao Li", "Bin Feng", "Zishan Shu", "Ke Yu", "Li Yuan", "Yu Li"], "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u5b9e\u4f53\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u86cb\u767d\u8d28\u6587\u672c\u6a21\u578b\u4e2d\u7684\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u5fae\u8c03LLMs\u3002", "motivation": "\u5f53\u524d\u86cb\u767d\u8d28\u6587\u672c\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u4e14\u4f20\u7edfNLP\u6307\u6807\u4e0d\u9002\u7528\u4e8e\u8be5\u9886\u57df\u3002", "method": "\u91cd\u7ec4\u6570\u636e\u96c6\u5e76\u5f15\u5165\u57fa\u4e8e\u751f\u7269\u5b9e\u4f53\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u86cb\u767d\u8d28\u5230\u6587\u672c\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u5fae\u8c03LLMs\uff0c\u4e14\u5728\u65e0\u8bad\u7ec3\u573a\u666f\u4e0b\u8868\u73b0\u9ad8\u6548\u3002", "conclusion": "\u65b0\u6846\u67b6\u548c\u65b9\u6cd5\u89e3\u51b3\u4e86\u8bc4\u4f30\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u6587\u672c\u6a21\u578b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20540", "pdf": "https://arxiv.org/pdf/2505.20540", "abs": "https://arxiv.org/abs/2505.20540", "authors": ["Md Rashidunnabi", "Kailash Hambarde", "Hugo Proen\u00e7a"], "title": "Causality and \"In-the-Wild\" Video-Based Person Re-ID: A Survey", "categories": ["cs.CV"], "comment": "30 pages, 9 figures", "summary": "Video-based person re-identification (Re-ID) remains brittle in real-world\ndeployments despite impressive benchmark performance. Most existing models rely\non superficial correlations such as clothing, background, or lighting that fail\nto generalize across domains, viewpoints, and temporal variations. This survey\nexamines the emerging role of causal reasoning as a principled alternative to\ntraditional correlation-based approaches in video-based Re-ID. We provide a\nstructured and critical analysis of methods that leverage structural causal\nmodels, interventions, and counterfactual reasoning to isolate\nidentity-specific features from confounding factors. The survey is organized\naround a novel taxonomy of causal Re-ID methods that spans generative\ndisentanglement, domain-invariant modeling, and causal transformers. We review\ncurrent evaluation metrics and introduce causal-specific robustness measures.\nIn addition, we assess practical challenges of scalability, fairness,\ninterpretability, and privacy that must be addressed for real-world adoption.\nFinally, we identify open problems and outline future research directions that\nintegrate causal modeling with efficient architectures and self-supervised\nlearning. This survey aims to establish a coherent foundation for causal\nvideo-based person Re-ID and to catalyze the next phase of research in this\nrapidly evolving domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u56e0\u679c\u63a8\u7406\u5728\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\uff08Re-ID\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u5176\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u5728\u89c6\u9891Re-ID\u4e2d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5206\u79bb\u8eab\u4efd\u7279\u5f81\u4e0e\u6df7\u6742\u56e0\u7d20\u3002", "method": "\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u751f\u6210\u89e3\u8026\u3001\u57df\u4e0d\u53d8\u5efa\u6a21\u548c\u56e0\u679cTransformer\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u56e0\u679c\u7279\u5b9a\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u56e0\u679c\u63a8\u7406\u4e3a\u89c6\u9891Re-ID\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u516c\u5e73\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9690\u79c1\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.20454", "pdf": "https://arxiv.org/pdf/2505.20454", "abs": "https://arxiv.org/abs/2505.20454", "authors": ["Reid Graves", "Anthony Zhou", "Amir Barati Farimani"], "title": "BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction", "categories": ["cs.LG"], "comment": "21 pages, 9 figures", "summary": "Accurate prediction of blast pressure fields is essential for applications in\nstructural safety, defense planning, and hazard mitigation. Traditional methods\nsuch as empirical models and computational fluid dynamics (CFD) simulations\noffer limited trade offs between speed and accuracy; empirical models fail to\ncapture complex interactions in cluttered environments, while CFD simulations\nare computationally expensive and time consuming. In this work, we introduce\nBlastOFormer, a novel Transformer based surrogate model for full field maximum\npressure prediction from arbitrary obstacle and charge configurations.\nBlastOFormer leverages a signed distance function (SDF) encoding and a grid to\ngrid attention based architecture inspired by OFormer and Vision Transformer\n(ViT) frameworks. Trained on a dataset generated using the open source\nblastFoam CFD solver, our model outperforms convolutional neural networks\n(CNNs) and Fourier Neural Operators (FNOs) across both log transformed and\nunscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score\n(0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for\ninference, more than 600,000 times faster than CFD simulations. Qualitative\nvisualizations and error analyses further confirm BlastOFormer's superior\nspatial coherence and generalization capabilities. These results highlight its\npotential as a real time alternative to conventional CFD approaches for blast\npressure estimation in complex environments.", "AI": {"tldr": "BlastOFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7206\u70b8\u538b\u529b\u573a\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb\u66f4\u51c6\u786e\u3002", "motivation": "\u4f20\u7edf\u7206\u70b8\u538b\u529b\u573a\u9884\u6d4b\u65b9\u6cd5\uff08\u5982\u7ecf\u9a8c\u6a21\u578b\u548cCFD\u6a21\u62df\uff09\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u5c40\u9650\uff0cBlastOFormer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u7ed3\u5408SDF\u7f16\u7801\u548c\u7f51\u683c\u5230\u7f51\u683c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bad\u7ec3\u6570\u636e\u6765\u81eaCFD\u6c42\u89e3\u5668\u3002", "result": "BlastOFormer\u5728R2\u5f97\u5206\u548c\u8bef\u5dee\u6307\u6807\u4e0a\u4f18\u4e8eCNN\u548cFNO\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4CFD\u5feb60\u4e07\u500d\u3002", "conclusion": "BlastOFormer\u53ef\u4f5c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7206\u70b8\u538b\u529b\u573a\u5b9e\u65f6\u9884\u6d4b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20670", "pdf": "https://arxiv.org/pdf/2505.20670", "abs": "https://arxiv.org/abs/2505.20670", "authors": ["Zikang Guo", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning", "categories": ["cs.AI"], "comment": "Accepted to 34rd International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Complex tasks involving tool integration pose significant challenges for\nLarge Language Models (LLMs), leading to the emergence of multi-agent workflows\nas a promising solution. Reflection has emerged as an effective strategy for\ncorrecting erroneous trajectories in agentic workflows. However, existing\napproaches only exploit such capability in the post-action stage, where the\nagent observes the execution outcomes. We argue that, like humans, LLMs can\nalso engage in reflection before action execution: the agent can anticipate\nundesirable outcomes from its own decisions, which not only provides a\nnecessarily complementary perspective to evaluate the decision but also\nprevents the propagation of errors throughout the trajectory. In this paper, we\npropose MIRROR, a framework that consists of both intra-reflection, which\ncritically assesses intended actions before execution, and inter-reflection,\nwhich further adjusts the trajectory based on observations. This design\nsystematically leverages LLM reflection capabilities to eliminate and rectify\nerroneous actions on a more comprehensive scope. Evaluations on both the\nStableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior\nperformance, achieving state-of-the-art results compared to existing\napproaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMIRROR\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6267\u884c\u53cd\u601d\uff08intra-reflection\uff09\u548c\u6267\u884c\u540e\u53cd\u601d\uff08inter-reflection\uff09\u63d0\u5347LLM\u5728\u591a\u4efb\u52a1\u5de5\u5177\u96c6\u6210\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5728\u6267\u884c\u540e\u5229\u7528\u53cd\u601d\u7ea0\u6b63\u9519\u8bef\uff0c\u800c\u4eba\u7c7b\u4f1a\u5728\u6267\u884c\u524d\u53cd\u601d\u4ee5\u907f\u514d\u9519\u8bef\u4f20\u64ad\u3002MIRROR\u65e8\u5728\u66f4\u5168\u9762\u5730\u5229\u7528LLM\u7684\u53cd\u601d\u80fd\u529b\u3002", "method": "MIRROR\u7ed3\u5408\u9884\u6267\u884c\u53cd\u601d\uff08\u8bc4\u4f30\u610f\u56fe\uff09\u548c\u6267\u884c\u540e\u53cd\u601d\uff08\u8c03\u6574\u8f68\u8ff9\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u51cf\u5c11\u9519\u8bef\u3002", "result": "\u5728StableToolBench\u548cTravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMIRROR\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230SOTA\u3002", "conclusion": "MIRROR\u901a\u8fc7\u5168\u9762\u53cd\u601d\u673a\u5236\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "relevance": 85.0}}
{"id": "2505.20415", "pdf": "https://arxiv.org/pdf/2505.20415", "abs": "https://arxiv.org/abs/2505.20415", "authors": ["Xingwei Tan", "Marco Valentino", "Mahmud Akhter", "Maria Liakata", "Nikolaos Aletras"], "title": "Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) have shown promising performance in mathematical\nand logical reasoning benchmarks. However, recent studies have pointed to\nmemorization, rather than generalization, as one of the leading causes for such\nperformance. LLMs, in fact, are susceptible to content variations,\ndemonstrating a lack of robust symbolic abstractions supporting their reasoning\nprocess. To improve reliability, many attempts have been made to combine LLMs\nwith symbolic methods. Nevertheless, existing approaches fail to effectively\nleverage symbolic representations due to the challenges involved in developing\nreliable and scalable verification mechanisms. In this paper, we propose to\novercome such limitations by generating symbolic reasoning trajectories and\nselect the high-quality ones using a process reward model automatically tuned\nbased on Monte Carlo estimation. The trajectories are then employed via\nfine-tuning methods to improve logical reasoning and generalization. Our\nresults on logical reasoning benchmarks such as FOLIO and LogicAsker show the\neffectiveness of the proposed method with large gains on frontier and\nopen-weight models. Moreover, additional experiments on claim verification\nreveal that fine-tuning on the generated symbolic reasoning trajectories\nenhances out-of-domain generalizability, suggesting the potential impact of\nsymbolically-guided process supervision in alleviating the effect of\nmemorization on LLM reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u7b26\u53f7\u63a8\u7406\u8f68\u8ff9\u5e76\u5229\u7528\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u4f18\u5316\u7684\u5956\u52b1\u6a21\u578b\u7b5b\u9009\u9ad8\u8d28\u91cf\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347LLM\u7684\u903b\u8f91\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u7ed3\u5408\u7b26\u53f7\u8868\u793a\uff0c\u5bfc\u81f4LLM\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u751f\u6210\u7b26\u53f7\u63a8\u7406\u8f68\u8ff9\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u4f18\u5316\u7684\u5956\u52b1\u6a21\u578b\u7b5b\u9009\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03LLM\u3002", "result": "\u5728FOLIO\u548cLogicAsker\u7b49\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5e76\u589e\u5f3a\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7b26\u53f7\u5f15\u5bfc\u7684\u8fc7\u7a0b\u76d1\u7763\u80fd\u7f13\u89e3LLM\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u6548\u5e94\uff0c\u63d0\u5347\u53ef\u9760\u6027\u3002", "relevance": 85.0}}
{"id": "2505.20569", "pdf": "https://arxiv.org/pdf/2505.20569", "abs": "https://arxiv.org/abs/2505.20569", "authors": ["Jihoon Lee", "Min Song"], "title": "Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ACL Findings camera-ready version. Code is released at\n  https://github.com/JiHoonLee9898/RVCD", "summary": "Despite significant advancements in Large Vision-Language Models, Object\nHallucination (OH) remains a persistent challenge. Building upon prior studies\non contrastive decoding that address this issue without requiring additional\nmodel training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an\nadvanced method to suppress OH. RVCD leverages both negative and positive\nimages at the logit level, explicitly referencing AI-generated images designed\nto represent a single concept. Our approach demonstrates substantial\nimprovements over existing decoding-based methods.", "AI": {"tldr": "RVCD\uff08\u68c0\u7d22\u89c6\u89c9\u5bf9\u6bd4\u89e3\u7801\uff09\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6b63\u8d1f\u56fe\u50cf\u5728logit\u7ea7\u522b\u6291\u5236\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\uff08OH\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7269\u4f53\u5e7b\u89c9\uff08OH\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u95ee\u9898\u3002RVCD\u65e8\u5728\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RVCD\u5229\u7528\u6b63\u8d1f\u56fe\u50cf\u5728logit\u7ea7\u522b\u8fdb\u884c\u5bf9\u6bd4\u89e3\u7801\uff0c\u53c2\u8003AI\u751f\u6210\u7684\u5355\u6982\u5ff5\u56fe\u50cf\uff0c\u4ee5\u6291\u5236OH\u3002", "result": "RVCD\u5728\u6291\u5236OH\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u89e3\u7801\u7684\u65b9\u6cd5\u3002", "conclusion": "RVCD\u4e3a\u6291\u5236\u7269\u4f53\u5e7b\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "relevance": 70.0}}
{"id": "2505.20485", "pdf": "https://arxiv.org/pdf/2505.20485", "abs": "https://arxiv.org/abs/2505.20485", "authors": ["Abhijit Chunduru", "Majid Morafah", "Mahdi Morafah", "Vishnu Pandi Chellapandi", "Ang Li"], "title": "Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.PF"], "comment": null, "summary": "The inevitable presence of data heterogeneity has made federated learning\nvery challenging. There are numerous methods to deal with this issue, such as\nlocal regularization, better model fusion techniques, and data sharing. Though\neffective, they lack a deep understanding of how data heterogeneity can affect\nthe global decision boundary. In this paper, we bridge this gap by performing\nan experimental analysis of the learned decision boundary using a toy example.\nOur observations are surprising: (1) we find that the existing methods suffer\nfrom forgetting and clients forget the global decision boundary and only learn\nthe perfect local one, and (2) this happens regardless of the initial weights,\nand clients forget the global decision boundary even starting from pre-trained\noptimal weights. In this paper, we present FedProj, a federated learning\nframework that robustly learns the global decision boundary and avoids its\nforgetting during local training. To achieve better ensemble knowledge fusion,\nwe design a novel server-side ensemble knowledge transfer loss to further\ncalibrate the learned global decision boundary. To alleviate the issue of\nlearned global decision boundary forgetting, we further propose leveraging an\nepisodic memory of average ensemble logits on a public unlabeled dataset to\nregulate the gradient updates at each step of local training. Experimental\nresults demonstrate that FedProj outperforms state-of-the-art methods by a\nlarge margin.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u53d1\u73b0\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u5ba2\u6237\u7aef\u9057\u5fd8\u5168\u5c40\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u63d0\u51faFedProj\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u77e5\u8bc6\u8f6c\u79fb\u635f\u5931\u548c\u516c\u5171\u672a\u6807\u8bb0\u6570\u636e\u96c6\u7684\u8bb0\u5fc6\u673a\u5236\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u5f02\u6784\u6027\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5176\u5982\u4f55\u5f71\u54cd\u5168\u5c40\u51b3\u7b56\u8fb9\u754c\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51faFedProj\u6846\u67b6\uff0c\u5305\u62ec\u670d\u52a1\u5668\u7aef\u77e5\u8bc6\u8f6c\u79fb\u635f\u5931\u548c\u5229\u7528\u516c\u5171\u672a\u6807\u8bb0\u6570\u636e\u96c6\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u907f\u514d\u5168\u5c40\u51b3\u7b56\u8fb9\u754c\u9057\u5fd8\u3002", "result": "FedProj\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedProj\u901a\u8fc7\u65b0\u9896\u7684\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5168\u5c40\u51b3\u7b56\u8fb9\u754c\u9057\u5fd8\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.20671", "pdf": "https://arxiv.org/pdf/2505.20671", "abs": "https://arxiv.org/abs/2505.20671", "authors": ["Heng Tan", "Hua Yan", "Yu Yang"], "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While reinforcement learning (RL) has achieved notable success in various\ndomains, training effective policies for complex tasks remains challenging.\nAgents often converge to local optima and fail to maximize long-term rewards.\nExisting approaches to mitigate training bottlenecks typically fall into two\ncategories: (i) Automated policy refinement, which identifies critical states\nfrom past trajectories to guide policy updates, but suffers from costly and\nuncertain model training; and (ii) Human-in-the-loop refinement, where human\nfeedback is used to correct agent behavior, but this does not scale well to\nenvironments with large or continuous action spaces. In this work, we design a\nlarge language model-guided policy modulation framework that leverages LLMs to\nimprove RL training without additional model training or human intervention. We\nfirst prompt an LLM to identify critical states from a sub-optimal agent's\ntrajectories. Based on these states, the LLM then provides action suggestions\nand assigns implicit rewards to guide policy refinement. Experiments across\nstandard RL benchmarks demonstrate that our method outperforms state-of-the-art\nbaselines, highlighting the effectiveness of LLM-based explanations in\naddressing RL training bottlenecks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u6216\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u5c40\u90e8\u6700\u4f18\u548c\u957f\u671f\u5956\u52b1\u6700\u5927\u5316\u7684\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\uff08\u81ea\u52a8\u7b56\u7565\u4f18\u5316\u548c\u4eba\u5de5\u53cd\u9988\uff09\u7684\u9ad8\u6210\u672c\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aLLM\u5f15\u5bfc\u7684\u7b56\u7565\u8c03\u5236\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8bc6\u522b\u5173\u952e\u72b6\u6001\u3001\u63d0\u4f9b\u52a8\u4f5c\u5efa\u8bae\u548c\u9690\u5f0f\u5956\u52b1\u6765\u6307\u5bfc\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86LLM\u5728\u89e3\u51b3RL\u8bad\u7ec3\u74f6\u9888\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLM\u53ef\u4ee5\u6709\u6548\u5730\u8f85\u52a9RL\u8bad\u7ec3\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20416", "pdf": "https://arxiv.org/pdf/2505.20416", "abs": "https://arxiv.org/abs/2505.20416", "authors": ["Zihong Chen", "Wanli Jiang", "Jinzhe Li", "Zhonghang Yuan", "Huanjun Kong", "Wanli Ouyang", "Nanqing Dong"], "title": "GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning for large language models (LLMs) typically requires substantial\namounts of high-quality supervised data, which is both costly and\nlabor-intensive to acquire. While synthetic data generation has emerged as a\npromising solution, existing approaches frequently suffer from factual\ninaccuracies, insufficient long-tail coverage, simplistic knowledge structures,\nand homogenized outputs. To address these challenges, we introduce GraphGen, a\nknowledge graph-guided framework designed for three key question-answering (QA)\nscenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by\nconstructing a fine-grained knowledge graph from the source text. It then\nidentifies knowledge gaps in LLMs using the expected calibration error metric,\nprioritizing the generation of QA pairs that target high-value, long-tail\nknowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling\nto capture complex relational information and employs style-controlled\ngeneration to diversify the resulting QA data. Experimental results on\nknowledge-intensive tasks under closed-book settings demonstrate that GraphGen\noutperforms conventional synthetic data methods, offering a more reliable and\ncomprehensive solution to the data scarcity challenge in supervised\nfine-tuning. The code and data are publicly available at\nhttps://github.com/open-sciencelab/GraphGen.", "AI": {"tldr": "GraphGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684QA\u6570\u636e\uff0c\u89e3\u51b3LLM\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u5fae\u8c03\u4e2d\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u7684\u7f3a\u9677\uff08\u5982\u4e8b\u5b9e\u9519\u8bef\u3001\u957f\u5c3e\u8986\u76d6\u4e0d\u8db3\u7b49\uff09\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u56fe\u8c31\uff0c\u8bc6\u522bLLM\u7684\u77e5\u8bc6\u7f3a\u53e3\uff0c\u5e76\u5229\u7528\u591a\u8df3\u90bb\u57df\u91c7\u6837\u548c\u98ce\u683c\u63a7\u5236\u751f\u6210\u591a\u6837\u5316\u7684QA\u6570\u636e\u3002", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0cGraphGen\u4f18\u4e8e\u4f20\u7edf\u5408\u6210\u6570\u636e\u65b9\u6cd5\u3002", "conclusion": "GraphGen\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u3001\u5168\u9762\u7684\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20582", "pdf": "https://arxiv.org/pdf/2505.20582", "abs": "https://arxiv.org/abs/2505.20582", "authors": ["Yizhou Zhao", "Chunjiang Liu", "Haoyu Chen", "Bhiksha Raj", "Min Xu", "Tadas Baltrusaitis", "Mitch Rundle", "HsiangTao Wu", "Kamran Ghasedi"], "title": "Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting", "categories": ["cs.CV"], "comment": null, "summary": "Face reenactment and portrait relighting are essential tasks in portrait\nediting, yet they are typically addressed independently, without much synergy.\nMost face reenactment methods prioritize motion control and multiview\nconsistency, while portrait relighting focuses on adjusting shading effects. To\ntake advantage of both geometric consistency and illumination awareness, we\nintroduce Total-Editing, a unified portrait editing framework that enables\nprecise control over appearance, motion, and lighting. Specifically, we design\na neural radiance field decoder with intrinsic decomposition capabilities. This\nallows seamless integration of lighting information from portrait images or HDR\nenvironment maps into synthesized portraits. We also incorporate a moving least\nsquares based deformation field to enhance the spatiotemporal coherence of\navatar motion and shading effects. With these innovations, our unified\nframework significantly improves the quality and realism of portrait editing\nresults. Further, the multi-source nature of Total-Editing supports more\nflexible applications, such as illumination transfer from one portrait to\nanother, or portrait animation with customized backgrounds.", "AI": {"tldr": "Total-Editing\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8096\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u5149\u7167\u611f\u77e5\uff0c\u901a\u8fc7\u795e\u7ecf\u8f90\u5c04\u573a\u89e3\u7801\u5668\u548c\u53d8\u5f62\u573a\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8096\u50cf\u5916\u89c2\u3001\u8fd0\u52a8\u548c\u5149\u7167\u7684\u7cbe\u786e\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u8096\u50cf\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406\u9762\u90e8\u91cd\u6f14\u548c\u8096\u50cf\u91cd\u5149\u7167\uff0c\u7f3a\u4e4f\u534f\u540c\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u5149\u7167\u611f\u77e5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u8096\u50cf\u7f16\u8f91\u7ed3\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709\u56fa\u6709\u5206\u89e3\u80fd\u529b\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u89e3\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u79fb\u52a8\u6700\u5c0f\u4e8c\u4e58\u7684\u53d8\u5f62\u573a\uff0c\u4ee5\u589e\u5f3a\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u5149\u7167\u6548\u679c\u3002", "result": "Total-Editing\u663e\u8457\u63d0\u5347\u4e86\u8096\u50cf\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u771f\u5b9e\u611f\uff0c\u5e76\u652f\u6301\u591a\u6e90\u5e94\u7528\uff0c\u5982\u5149\u7167\u8f6c\u79fb\u548c\u8096\u50cf\u52a8\u753b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8096\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u548c\u5149\u7167\u63a7\u5236\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.20515", "pdf": "https://arxiv.org/pdf/2505.20515", "abs": "https://arxiv.org/abs/2505.20515", "authors": ["Avik Pal", "Alan Edelman", "Christopher Rackauckas"], "title": "Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints", "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA"], "comment": null, "summary": "Despite the promise of scientific machine learning (SciML) in combining\ndata-driven techniques with mechanistic modeling, existing approaches for\nincorporating hard constraints in neural differential equations (NDEs) face\nsignificant limitations. Scalability issues and poor numerical properties\nprevent these neural models from being used for modeling physical systems with\ncomplicated conservation laws. We propose Manifold-Projected Neural ODEs\n(PNODEs), a method that explicitly enforces algebraic constraints by projecting\neach ODE step onto the constraint manifold. This framework arises naturally\nfrom semi-explicit differential-algebraic equations (DAEs), and includes both a\nrobust iterative variant and a fast approximation requiring a single Jacobian\nfactorization. We further demonstrate that prior works on relaxation methods\nare special cases of our approach. PNODEs consistently outperform baselines\nacross six benchmark problems achieving a mean constraint violation error below\n$10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to\nother methods for a given level of error tolerance. These results show that\nconstraint projection offers a simple strategy for learning physically\nconsistent long-horizon dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPNODEs\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06ODE\u6b65\u9aa4\u6295\u5f71\u5230\u7ea6\u675f\u6d41\u5f62\u4e0a\uff0c\u663e\u5f0f\u5f3a\u5236\u6267\u884c\u4ee3\u6570\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u4e2d\u786c\u7ea6\u675f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u6280\u672f\u548c\u673a\u5236\u5efa\u6a21\u65f6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u5b88\u6052\u5b9a\u5f8b\u7684\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6570\u503c\u6027\u8d28\u95ee\u9898\u3002", "method": "PNODEs\u57fa\u4e8e\u534a\u663e\u5f0f\u5fae\u5206\u4ee3\u6570\u65b9\u7a0b\uff0c\u901a\u8fc7\u6295\u5f71\u6bcf\u4e2aODE\u6b65\u9aa4\u5230\u7ea6\u675f\u6d41\u5f62\u4e0a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8fed\u4ee3\u53d8\u4f53\u548c\u5feb\u901f\u8fd1\u4f3c\u65b9\u6cd5\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cPNODEs\u7684\u5e73\u5747\u7ea6\u675f\u8fdd\u53cd\u8bef\u5dee\u4f4e\u4e8e10^-10\uff0c\u4e14\u8fd0\u884c\u65f6\u66f4\u4f4e\u3002", "conclusion": "\u7ea6\u675f\u6295\u5f71\u662f\u5b66\u4e60\u7269\u7406\u4e00\u81f4\u7684\u957f\u65f6\u7a0b\u52a8\u529b\u5b66\u7684\u7b80\u5355\u7b56\u7565\u3002", "relevance": 40.0}}
{"id": "2505.20672", "pdf": "https://arxiv.org/pdf/2505.20672", "abs": "https://arxiv.org/abs/2505.20672", "authors": ["Woochang Sim", "Hyunseok Ryu", "Kyungmin Choi", "Sungwon Han", "Sundong Kim"], "title": "GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general\nAI capabilities, requiring solvers to infer abstract patterns from only a\nhandful of examples. Despite substantial progress in deep learning,\nstate-of-the-art models still achieve accuracy rates of merely 40-55% on 2024\nARC Competition, indicative of a significant gap between their performance and\nhuman-level reasoning. In this work, we seek to bridge that gap by introducing\nan analogy-inspired ARC dataset, GIFARC. Leveraging large language models\n(LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks\nfrom a variety of GIF images that include analogies. Each new task is paired\nwith ground-truth analogy, providing an explicit mapping between visual\ntransformations and everyday concepts. By embedding robust human-intuitive\nanalogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task\nanalogically before engaging in brute-force pattern search, thus efficiently\nreducing problem complexity and build a more concise and human-understandable\nsolution. We empirically validate that guiding LLM with analogic approach with\nGIFARC affects task-solving approaches of LLMs to align with analogic approach\nof human.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGIFARC\u6570\u636e\u96c6\uff0c\u5229\u7528LLMs\u548cVLMs\u4eceGIF\u56fe\u50cf\u751f\u6210\u7c7b\u6bd4\u4efb\u52a1\uff0c\u5e2e\u52a9AI\u901a\u8fc7\u7c7b\u6bd4\u63a8\u7406\u63d0\u5347ARC\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3ARC\u4efb\u52a1\u4e2dAI\u4e0e\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u7c7b\u6bd4\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5229\u7528LLMs\u548cVLMs\u4eceGIF\u56fe\u50cf\u5408\u6210\u7c7b\u6bd4\u4efb\u52a1\uff0c\u5d4c\u5165\u4eba\u7c7b\u76f4\u89c9\u7c7b\u6bd4\uff0c\u6307\u5bfcAI\u5148\u8fdb\u884c\u7c7b\u6bd4\u63a8\u7406\u518d\u641c\u7d22\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGIFARC\u80fd\u5f15\u5bfcLLMs\u91c7\u7528\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u7c7b\u6bd4\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "GIFARC\u901a\u8fc7\u7c7b\u6bd4\u65b9\u6cd5\u6709\u6548\u63d0\u5347AI\u5728ARC\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u5dee\u8ddd\u3002", "relevance": 70.0}}
{"id": "2505.20422", "pdf": "https://arxiv.org/pdf/2505.20422", "abs": "https://arxiv.org/abs/2505.20422", "authors": ["Arvindh Arun", "Sumit Kumar", "Mojtaba Nayyeri", "Bo Xiong", "Ponnurangam Kumaraguru", "Antonio Vergari", "Steffen Staab"], "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling\nzero-shot reasoning over unseen graphs by learning transferable patterns.\nHowever, most existing KGFMs rely solely on graph structure, overlooking the\nrich semantic signals encoded in textual attributes. We introduce SEMMA, a\ndual-module KGFM that systematically integrates transferable textual semantics\nalongside structure. SEMMA leverages Large Language Models (LLMs) to enrich\nrelation identifiers, generating semantic embeddings that subsequently form a\ntextual relation graph, which is fused with the structural component. Across 54\ndiverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully\ninductive link prediction. Crucially, we show that in more challenging\ngeneralization settings, where the test-time relation vocabulary is entirely\nunseen, structural methods collapse while SEMMA is 2x more effective. Our\nfindings demonstrate that textual semantics are critical for generalization in\nsettings where structure alone fails, highlighting the need for foundation\nmodels that unify structural and linguistic signals in knowledge reasoning.", "AI": {"tldr": "SEMMA\u662f\u4e00\u79cd\u53cc\u6a21\u5757\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u56fe\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u5c5e\u6027\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u53f7\u3002", "method": "SEMMA\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u8bed\u4e49\u5d4c\u5165\uff0c\u6784\u5efa\u6587\u672c\u5173\u7cfb\u56fe\uff0c\u5e76\u4e0e\u7ed3\u6784\u56fe\u878d\u5408\u3002", "result": "\u572854\u4e2a\u77e5\u8bc6\u56fe\u8c31\u4e0a\uff0cSEMMA\u5728\u5b8c\u5168\u5f52\u7eb3\u94fe\u63a5\u9884\u6d4b\u4e2d\u4f18\u4e8e\u7eaf\u7ed3\u6784\u57fa\u7ebf\uff08\u5982ULTRA\uff09\uff0c\u5728\u672a\u89c1\u5173\u7cfb\u8bcd\u6c47\u7684\u66f4\u5177\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u6587\u672c\u8bed\u4e49\u5bf9\u4e8e\u77e5\u8bc6\u63a8\u7406\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u4fe1\u606f\u4e0d\u8db3\u65f6\uff0c\u5f3a\u8c03\u4e86\u7edf\u4e00\u7ed3\u6784\u548c\u8bed\u8a00\u4fe1\u53f7\u7684\u57fa\u7840\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002", "relevance": 75.0}}
{"id": "2505.20610", "pdf": "https://arxiv.org/pdf/2505.20610", "abs": "https://arxiv.org/abs/2505.20610", "authors": ["Xiaobao Wei", "Xiaoan Zhang", "Hao Wang", "Qingpo Wuwu", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "OmniIndoor3D: Comprehensive Indoor 3D Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose a novel framework for comprehensive indoor 3D reconstruction using\nGaussian representations, called OmniIndoor3D. This framework enables accurate\nappearance, geometry, and panoptic reconstruction of diverse indoor scenes\ncaptured by a consumer-level RGB-D camera. Since 3DGS is primarily optimized\nfor photorealistic rendering, it lacks the precise geometry critical for\nhigh-quality panoptic reconstruction. Therefore, OmniIndoor3D first combines\nmultiple RGB-D images to create a coarse 3D reconstruction, which is then used\nto initialize the 3D Gaussians and guide the 3DGS training. To decouple the\noptimization conflict between appearance and geometry, we introduce a\nlightweight MLP that adjusts the geometric properties of 3D Gaussians. The\nintroduced lightweight MLP serves as a low-pass filter for geometry\nreconstruction and significantly reduces noise in indoor scenes. To improve the\ndistribution of Gaussian primitives, we propose a densification strategy guided\nby panoptic priors to encourage smoothness on planar surfaces. Through the\njoint optimization of appearance, geometry, and panoptic reconstruction,\nOmniIndoor3D provides comprehensive 3D indoor scene understanding, which\nfacilitates accurate and robust robotic navigation. We perform thorough\nevaluations across multiple datasets, and OmniIndoor3D achieves\nstate-of-the-art results in appearance, geometry, and panoptic reconstruction.\nWe believe our work bridges a critical gap in indoor 3D reconstruction. The\ncode will be released at: https://ucwxb.github.io/OmniIndoor3D/", "AI": {"tldr": "OmniIndoor3D\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u8868\u793a\u7684\u5ba4\u51853D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RGB-D\u56fe\u50cf\u548c\u4f18\u53163D\u9ad8\u65af\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u5916\u89c2\u3001\u51e0\u4f55\u548c\u5168\u666f\u91cd\u5efa\u7684\u8054\u5408\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u9ad8\u65af\u8868\u793a\uff083DGS\uff09\u5728\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5916\u89c2\u4e0e\u51e0\u4f55\u4f18\u5316\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u3002", "method": "1. \u7ed3\u5408\u591a\u5f20RGB-D\u56fe\u50cf\u751f\u6210\u7c97\u75653D\u91cd\u5efa\uff1b2. \u5f15\u5165\u8f7b\u91cf\u7ea7MLP\u8c03\u65743D\u9ad8\u65af\u7684\u51e0\u4f55\u5c5e\u6027\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u5168\u666f\u5148\u9a8c\u7684\u9ad8\u65af\u5206\u5e03\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5916\u89c2\u3001\u51e0\u4f55\u548c\u5168\u666f\u91cd\u5efa\u7684SOTA\u6027\u80fd\u3002", "conclusion": "OmniIndoor3D\u586b\u8865\u4e86\u5ba4\u51853D\u91cd\u5efa\u7684\u5173\u952e\u7a7a\u767d\uff0c\u652f\u6301\u7cbe\u786e\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002", "relevance": 30.0}}
{"id": "2505.20524", "pdf": "https://arxiv.org/pdf/2505.20524", "abs": "https://arxiv.org/abs/2505.20524", "authors": ["Alejandro Hern\u00e1ndez-Cano", "Dhia Garbaya", "Imanol Schlag", "Martin Jaggi"], "title": "Towards Fully FP8 GEMM LLM Training at Scale", "categories": ["cs.LG"], "comment": "15 pages, 7 figures", "summary": "Despite the significant potential of FP8 data formats for large language\nmodel (LLM) pre-training, their adoption has been limited due to challenges in\nmaintaining stability at scale. Existing approaches often rely on suboptimal\nfine-grained FP8 kernels or fall back to higher-precision matrix\nmultiplications (GEMMs) in sensitive components, such as attention projections,\ncompromising potential throughput gains. We introduce a new class of LLM\narchitectures that, for the first time, support FP8 computation for all GEMMs\nwithin transformer blocks during both forward and backward passes. This enables\nunprecedented throughput gains, particularly at scale, while matching the\ndownstream performance of standard BF16 training. Our architecture design\nreduces large outlier activations, promoting stable long-term FP8 training. In\naddition, we identify key metrics to monitor low-precision training and predict\npotential future divergences.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301FP8\u8ba1\u7b97\u7684LLM\u67b6\u6784\uff0c\u89e3\u51b3\u4e86FP8\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eBF16\u8bad\u7ec3\u76f8\u5f53\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "motivation": "FP8\u6570\u636e\u683c\u5f0f\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u89c4\u6a21\u6269\u5c55\u65f6\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5176\u5e94\u7528\u53d7\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6b21\u4f18\u7684FP8\u5185\u6838\uff0c\u8981\u4e48\u5728\u654f\u611f\u7ec4\u4ef6\u4e2d\u56de\u9000\u5230\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\uff0c\u9650\u5236\u4e86\u541e\u5410\u91cf\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u67b6\u6784\uff0c\u9996\u6b21\u652f\u6301\u5728Transformer\u5757\u7684\u6240\u6709GEMM\u4e2d\u4f7f\u7528FP8\u8ba1\u7b97\uff08\u5305\u62ec\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\uff09\uff0c\u5e76\u901a\u8fc7\u51cf\u5c11\u5927\u5f02\u5e38\u6fc0\u6d3b\u503c\u6765\u7a33\u5b9aFP8\u8bad\u7ec3\u3002", "result": "\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\uff0c\u540c\u65f6\u4e0b\u6e38\u6027\u80fd\u4e0e\u6807\u51c6BF16\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "\u8be5\u67b6\u6784\u8bbe\u8ba1\u4e3aFP8\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u76d1\u63a7\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u7684\u5173\u952e\u6307\u6807\u3002", "relevance": 85.0}}
{"id": "2505.20728", "pdf": "https://arxiv.org/pdf/2505.20728", "abs": "https://arxiv.org/abs/2505.20728", "authors": ["Zesen Lyu", "Dandan Zhang", "Wei Ye", "Fangdi Li", "Zhihang Jiang", "Yao Yang"], "title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Spatial reasoning is a core component of human cognition, enabling\nindividuals to perceive, comprehend, and interact with the physical world. It\nrelies on a nuanced understanding of spatial structures and inter-object\nrelationships, serving as the foundation for complex reasoning and\ndecision-making. To investigate whether current vision-language models (VLMs)\nexhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark\nconsisting of 1,100 carefully curated real-world images with high spatial\ncomplexity. Based on this dataset, we design five tasks to rigorously evaluate\nVLMs' spatial perception, structural understanding, and reasoning capabilities,\nwhile deliberately minimizing reliance on domain-specific knowledge to better\nisolate and assess the general spatial reasoning capability. We conduct a\ncomprehensive evaluation across 24 state-of-the-art VLMs. The results show that\neven the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy\nand performs particularly poorly on the Order Generation task, with only 30.00%\naccuracy, far below the performance exceeding 90% achieved by human\nparticipants. This persistent gap underscores the need for continued progress,\npositioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for\nadvancing spatial reasoning research in VLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Jigsaw-Puzzles\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5f3a\u6a21\u578bGemini-2.5-Pro\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u63a2\u7d22\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b1,100\u5f20\u9ad8\u7a7a\u95f4\u590d\u6742\u5ea6\u56fe\u50cf\u7684Jigsaw-Puzzles\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u4e94\u9879\u4efb\u52a1\uff0c\u8bc4\u4f3024\u79cd\u5148\u8fdbVLMs\u3002", "result": "Gemini-2.5-Pro\u603b\u4f53\u51c6\u786e\u7387\u4e3a77.14%\uff0c\u5728Order Generation\u4efb\u52a1\u4e2d\u4ec530.00%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768490%\u4ee5\u4e0a\u3002", "conclusion": "\u5f53\u524dVLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\uff0cJigsaw-Puzzles\u53ef\u4f5c\u4e3a\u63a8\u52a8\u7814\u7a76\u7684\u6311\u6218\u6027\u57fa\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.20428", "pdf": "https://arxiv.org/pdf/2505.20428", "abs": "https://arxiv.org/abs/2505.20428", "authors": ["Angelina A. Aquino", "Lester James V. Miranda", "Elsie Marie T. Or"], "title": "The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project", "categories": ["cs.CL"], "comment": "Link to treebank:\n  https://huggingface.co/datasets/UD-Filipino/UD_Tagalog-NewsCrawl ; All\n  authors contributed equally in this work", "summary": "This paper presents UD-NewsCrawl, the largest Tagalog treebank to date,\ncontaining 15.6k trees manually annotated according to the Universal\nDependencies framework. We detail our treebank development process, including\ndata collection, pre-processing, manual annotation, and quality assurance\nprocedures. We provide baseline evaluations using multiple transformer-based\nmodels to assess the performance of state-of-the-art dependency parsers on\nTagalog. We also highlight challenges in the syntactic analysis of Tagalog\ngiven its distinctive grammatical properties, and discuss its implications for\nthe annotation of this treebank. We anticipate that UD-NewsCrawl and our\nbaseline model implementations will serve as valuable resources for advancing\ncomputational linguistics research in underrepresented languages like Tagalog.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86UD-NewsCrawl\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u4ed6\u52a0\u7984\u8bed\u6811\u5e93\uff0c\u5305\u542b15.6k\u68f5\u6811\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5176\u5f00\u53d1\u8fc7\u7a0b\u3001\u57fa\u7ebf\u8bc4\u4f30\u53ca\u4ed6\u52a0\u7984\u8bed\u8bed\u6cd5\u5206\u6790\u7684\u6311\u6218\u3002", "motivation": "\u63a8\u52a8\u8ba1\u7b97\u8bed\u8a00\u5b66\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\uff08\u5982\u4ed6\u52a0\u7984\u8bed\uff09\u4e2d\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u8d44\u6e90\u548c\u57fa\u7ebf\u6a21\u578b\u3002", "method": "\u6570\u636e\u6536\u96c6\u3001\u9884\u5904\u7406\u3001\u624b\u52a8\u6807\u6ce8\u3001\u8d28\u91cf\u4fdd\u8bc1\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u6700\u5927\u7684\u4ed6\u52a0\u7984\u8bed\u6811\u5e93\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "UD-NewsCrawl\u548c\u57fa\u7ebf\u6a21\u578b\u5c06\u63a8\u52a8\u4ed6\u52a0\u7984\u8bed\u7b49\u8bed\u8a00\u7684\u8ba1\u7b97\u8bed\u8a00\u5b66\u7814\u7a76\u3002", "relevance": 20.0}}
{"id": "2505.20611", "pdf": "https://arxiv.org/pdf/2505.20611", "abs": "https://arxiv.org/abs/2505.20611", "authors": ["Zenghao Zheng", "Lianping Yang", "Jinshan Pan", "Hegui Zhu"], "title": "Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based methods for 3-D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3-D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u62d3\u6251\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u9aa8\u9abc\u611f\u77e5\u6a21\u5757\u548c\u65f6\u7a7a\u7ec6\u5316\u6a21\u5757\u4f18\u5316Mamba\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "Transformer\u57283D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u56e0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u800cMamba\u6a21\u578b\u867d\u9ad8\u6548\u4f46\u7f3a\u4e4f\u5bf9\u62d3\u6251\u7ed3\u6784\u548c\u5c40\u90e8\u5173\u8282\u5173\u7cfb\u7684\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMamba-Driven Topology Fusion\u6846\u67b6\uff0c\u5305\u62ec\u9aa8\u9abc\u611f\u77e5\u6a21\u5757\uff08\u63a8\u65ad\u9aa8\u9abc\u5411\u91cf\u65b9\u5411\u4e0e\u957f\u5ea6\uff09\u3001\u589e\u5f3a\u7684\u5377\u79ef\u7ed3\u6784\uff08\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff09\u548c\u65f6\u7a7a\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u5404\u6a21\u5757\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u9aa8\u9abc\u62d3\u6251\u7ed3\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86Mamba\u6a21\u578b\u5728\u4eba\u4f53\u7ed3\u6784\u5173\u7cfb\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "relevance": 75.0}}
{"id": "2505.20532", "pdf": "https://arxiv.org/pdf/2505.20532", "abs": "https://arxiv.org/abs/2505.20532", "authors": ["Dian Jin", "Xin Bing", "Yuqian Zhang"], "title": "One-shot Robust Federated Learning of Independent Component Analysis", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper investigates a general robust one-shot aggregation framework for\ndistributed and federated Independent Component Analysis (ICA) problem. We\npropose a geometric median-based aggregation algorithm that leverages $k$-means\nclustering to resolve the permutation ambiguity in local client estimations.\nOur method first performs k-means to partition client-provided estimators into\nclusters and then aggregates estimators within each cluster using the geometric\nmedian. This approach provably remains effective even in highly heterogeneous\nscenarios where at most half of the clients can observe only a minimal number\nof samples. The key theoretical contribution lies in the combined analysis of\nthe geometric median's error bound-aided by sample quantiles-and the maximum\nmisclustering rates of the aforementioned solution of $k$-means. The\neffectiveness of the proposed approach is further supported by simulation\nstudies conducted under various heterogeneous settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u4e2d\u4f4d\u6570\u548ck\u5747\u503c\u805a\u7c7b\u7684\u5206\u5e03\u5f0fICA\u9c81\u68d2\u805a\u5408\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u573a\u666f\u4e0b\u7684\u6392\u5217\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u548c\u8054\u90a6ICA\u95ee\u9898\u4e2d\u7684\u9c81\u68d2\u805a\u5408\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u5ea6\u5f02\u6784\u73af\u5883\u4e0b\u5ba2\u6237\u7aef\u4f30\u8ba1\u7684\u6392\u5217\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u91c7\u7528k\u5747\u503c\u805a\u7c7b\u5bf9\u5ba2\u6237\u7aef\u63d0\u4f9b\u7684\u4f30\u8ba1\u5668\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u5728\u6bcf\u7ec4\u5185\u4f7f\u7528\u51e0\u4f55\u4e2d\u4f4d\u6570\u805a\u5408\uff0c\u7ed3\u5408\u6837\u672c\u5206\u4f4d\u6570\u5206\u6790\u8bef\u5dee\u754c\u9650\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u5ea6\u5f02\u6784\u573a\u666f\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u5373\u4f7f\u534a\u6570\u5ba2\u6237\u7aef\u4ec5\u89c2\u6d4b\u5230\u5c11\u91cf\u6837\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u5206\u5e03\u5f0fICA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u73af\u5883\u3002", "relevance": 30.0}}
{"id": "2505.20733", "pdf": "https://arxiv.org/pdf/2505.20733", "abs": "https://arxiv.org/abs/2505.20733", "authors": ["Cheonsu Jeong", "Seongmin Sim", "Hyoyoung Cho", "Sungsu Kim", "Byounggwan Shin"], "title": "E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents an intelligent work automation approach in the context of\ncontemporary digital transformation by integrating generative AI and\nIntelligent Document Processing (IDP) technologies with an Automation Agent to\nrealize End-to-End (E2E) automation of corporate financial expense processing\ntasks. While traditional Robotic Process Automation (RPA) has proven effective\nfor repetitive, rule-based simple task automation, it faces limitations in\nhandling unstructured data, exception management, and complex decision-making.\nThis study designs and implements a four-stage integrated process comprising\nautomatic recognition of supporting documents such as receipts via OCR/IDP,\nitem classification based on a policy-driven database, intelligent exception\nhandling supported by generative AI (large language models, LLMs), and\nhuman-in-the-loop final decision-making with continuous system learning through\nan Automation Agent. Applied to a major Korean enterprise (Company S), the\nsystem demonstrated quantitative benefits including over 80% reduction in\nprocessing time for paper receipt expense tasks, decreased error rates, and\nimproved compliance, as well as qualitative benefits such as enhanced accuracy\nand consistency, increased employee satisfaction, and data-driven decision\nsupport. Furthermore, the system embodies a virtuous cycle by learning from\nhuman judgments to progressively improve automatic exception handling\ncapabilities. Empirically, this research confirms that the organic integration\nof generative AI, IDP, and Automation Agents effectively overcomes the\nlimitations of conventional automation and enables E2E automation of complex\ncorporate processes. The study also discusses potential extensions to other\ndomains such as accounting, human resources, and procurement, and proposes\nfuture directions for AI-driven hyper-automation development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u6587\u6863\u5904\u7406\u6280\u672f\u7684\u81ea\u52a8\u5316\u4ee3\u7406\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4f01\u4e1a\u8d22\u52a1\u8d39\u7528\u5904\u7406\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfRPA\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\u548c\u590d\u6742\u51b3\u7b56\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5f0fAI\u548c\u81ea\u52a8\u5316\u4ee3\u7406\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u9636\u6bb5\u96c6\u6210\u6d41\u7a0b\uff0c\u5305\u62ecOCR/IDP\u6587\u6863\u8bc6\u522b\u3001\u57fa\u4e8e\u7b56\u7565\u7684\u5206\u7c7b\u3001\u751f\u6210\u5f0fAI\u652f\u6301\u7684\u5f02\u5e38\u5904\u7406\uff0c\u4ee5\u53ca\u4eba\u673a\u534f\u540c\u51b3\u7b56\u3002", "result": "\u5728\u97e9\u56fd\u4f01\u4e1aS\u7684\u5e94\u7528\u4e2d\uff0c\u5904\u7406\u65f6\u95f4\u51cf\u5c1180%\uff0c\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u5408\u89c4\u6027\u548c\u5458\u5de5\u6ee1\u610f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4e0e\u81ea\u52a8\u5316\u4ee3\u7406\u7684\u6709\u673a\u7ed3\u5408\u80fd\u6709\u6548\u5b9e\u73b0\u590d\u6742\u4f01\u4e1a\u6d41\u7a0b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\uff0c\u5e76\u5177\u6709\u6269\u5c55\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.20429", "pdf": "https://arxiv.org/pdf/2505.20429", "abs": "https://arxiv.org/abs/2505.20429", "authors": ["Shuhao Guan", "Moule Lin", "Cheng Xu", "Xinyi Liu", "Jinman Zhao", "Jiexin Fan", "Qi Xu", "Derek Greene"], "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 main", "summary": "This paper introduces PreP-OCR, a two-stage pipeline that combines document\nimage restoration with semantic-aware post-OCR correction to improve text\nextraction from degraded historical documents. Our key innovation lies in\njointly optimizing image clarity and linguistic consistency. First, we generate\nsynthetic image pairs with randomized text fonts, layouts, and degradations. An\nimage restoration model is trained on this synthetic data, using\nmulti-directional patch extraction and fusion to process large images. Second,\na ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,\naddresses any remaining OCR errors. Detailed experiments on 13,831 pages of\nreal historical documents in English, French, and Spanish show that PreP-OCR\npipeline reduces character error rates by 63.9-70.3\\% compared to OCR on raw\nimages. Our pipeline demonstrates the potential of integrating image\nrestoration with linguistic error correction for digitizing historical\narchives.", "AI": {"tldr": "PreP-OCR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408\u56fe\u50cf\u4fee\u590d\u548c\u8bed\u4e49\u611f\u77e5\u7684OCR\u540e\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u5386\u53f2\u6587\u6863\u7684\u6587\u672c\u63d0\u53d6\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5386\u53f2\u6587\u6863\u56e0\u9000\u5316\u5bfc\u81f4\u7684OCR\u9519\u8bef\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf\u6e05\u6670\u5ea6\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "method": "1. \u751f\u6210\u5408\u6210\u56fe\u50cf\u5bf9\u8bad\u7ec3\u56fe\u50cf\u4fee\u590d\u6a21\u578b\uff1b2. \u4f7f\u7528ByT5\u8fdb\u884cOCR\u540e\u6821\u6b63\u3002", "result": "\u572813,831\u9875\u771f\u5b9e\u5386\u53f2\u6587\u6863\u4e0a\uff0c\u5b57\u7b26\u9519\u8bef\u7387\u964d\u4f4e63.9-70.3%\u3002", "conclusion": "\u7ed3\u5408\u56fe\u50cf\u4fee\u590d\u548c\u8bed\u8a00\u6821\u6b63\u53ef\u6709\u6548\u63d0\u5347\u5386\u53f2\u6863\u6848\u6570\u5b57\u5316\u8d28\u91cf\u3002", "relevance": 30.0}}
{"id": "2505.20612", "pdf": "https://arxiv.org/pdf/2505.20612", "abs": "https://arxiv.org/abs/2505.20612", "authors": ["Peter Robicheaux", "Matvei Popov", "Anish Madan", "Isaac Robinson", "Joseph Nelson", "Deva Ramanan", "Neehar Peri"], "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The first two authors contributed equally", "summary": "Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Our code and dataset are available at\nhttps://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u63cf\u8ff0\u5bf9\u9f50\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86Roboflow100-VL\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30VLM\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u7b49\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLM\u5728\u5206\u5e03\u5916\u4efb\u52a1\u548c\u6a21\u6001\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5bf9\u9f50\u65b0\u6982\u5ff5\u3002", "method": "\u901a\u8fc7Roboflow100-VL\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u8bc4\u4f30VLM\u5728\u4e0d\u540c\u6570\u636e\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "VLM\u5728\u533b\u7597\u5f71\u50cf\u6570\u636e\u4e0a\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\u4f4e\u4e8e2%\uff0c\u8868\u660e\u5c11\u6837\u672c\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u5c11\u6837\u672c\u5bf9\u9f50\u662f\u63d0\u5347VLM\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u3002", "relevance": 60.0}}
{"id": "2505.20535", "pdf": "https://arxiv.org/pdf/2505.20535", "abs": "https://arxiv.org/abs/2505.20535", "authors": ["Uros Zivanovic", "Serafina Di Gioia", "Andre Scaffidi", "Mart\u00edn de los Rios", "Gabriella Contardo", "Roberto Trotta"], "title": "Rotary Masked Autoencoders are Versatile Learners", "categories": ["cs.LG"], "comment": "26 pages, 5 figures", "summary": "Applying Transformers to irregular time-series typically requires\nspecializations to their baseline architecture, which can result in additional\ncomputational overhead and increased method complexity. We present the Rotary\nMasked Autoencoder (RoMAE), which utilizes the popular Rotary Positional\nEmbedding (RoPE) method for continuous positions. RoMAE is an extension to the\nMasked Autoencoder (MAE) that enables representation learning with\nmultidimensional continuous positional information while avoiding any\ntime-series-specific architectural specializations. We showcase RoMAE's\nperformance on a variety of modalities including irregular and multivariate\ntime-series, images, and audio, demonstrating that RoMAE surpasses specialized\ntime-series architectures on difficult datasets such as the DESC ELAsTiCC\nChallenge while maintaining MAE's usual performance across other modalities. In\naddition, we investigate RoMAE's ability to reconstruct the embedded continuous\npositions, demonstrating that including learned embeddings in the input\nsequence breaks RoPE's relative position property.", "AI": {"tldr": "RoMAE\u6269\u5c55\u4e86MAE\uff0c\u5229\u7528RoPE\u5904\u7406\u591a\u7ef4\u8fde\u7eed\u4f4d\u7f6e\u4fe1\u606f\uff0c\u65e0\u9700\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u67b6\u6784\uff0c\u5728\u591a\u79cd\u6a21\u6001\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3Transformer\u5728\u975e\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u4e2d\u9700\u8981\u4e13\u7528\u67b6\u6784\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u65b9\u6cd5\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faRoMAE\uff0c\u7ed3\u5408RoPE\u548cMAE\uff0c\u652f\u6301\u591a\u7ef4\u8fde\u7eed\u4f4d\u7f6e\u4fe1\u606f\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\uff08\u5982\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u50cf\u3001\u97f3\u9891\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e13\u7528\u67b6\u6784\uff0c\u540c\u65f6\u4fdd\u6301MAE\u7684\u6027\u80fd\u3002", "conclusion": "RoMAE\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\uff0c\u65e0\u9700\u4e13\u7528\u67b6\u6784\u3002", "relevance": 60.0}}
{"id": "2505.20737", "pdf": "https://arxiv.org/pdf/2505.20737", "abs": "https://arxiv.org/abs/2505.20737", "authors": ["Zilong Wang", "Jingfeng Yang", "Sreyashi Nag", "Samarth Varshney", "Xianfeng Tang", "Haoming Jiang", "Jingbo Shang", "Sheikh Muhammad Sarwar"], "title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories", "categories": ["cs.AI"], "comment": "preprint", "summary": "Large language models (LLMs) have exhibited extraordinary performance in a\nvariety of tasks while it remains challenging for them to solve complex\nmulti-step tasks as agents. In practice, agents sensitive to the outcome of\ncertain key steps which makes them likely to fail the task because of a subtle\nmistake in the planning trajectory. Recent approaches resort to calibrating the\nreasoning process through reinforcement learning. They reward or penalize every\nreasoning step with process supervision, as known as Process Reward Models\n(PRMs). However, PRMs are difficult and costly to scale up with a large number\nof next action candidates since they require extensive computations to acquire\nthe training data through the per-step trajectory exploration. To mitigate this\nissue, we focus on the relative reward trend across successive reasoning steps\nand propose maintaining an increasing reward in the collected trajectories for\nprocess supervision, which we term Reward Rising Optimization (RRO).\nSpecifically, we incrementally augment the process supervision until\nidentifying a step exhibiting positive reward differentials, i.e. rising\nrewards, relative to its preceding iteration. This method dynamically expands\nthe search space for the next action candidates, efficiently capturing\nhigh-quality data. We provide mathematical groundings and empirical results on\nthe WebShop and InterCode-SQL benchmarks, showing that our proposed RRO\nachieves superior performance while requiring much less exploration cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReward Rising Optimization (RRO)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8fc7\u7a0b\u76d1\u7763\u4e2d\u7684\u5956\u52b1\u8d8b\u52bf\uff0c\u51cf\u5c11\u63a2\u7d22\u6210\u672c\uff0c\u63d0\u5347\u591a\u6b65\u4efb\u52a1\u4e2dLLM\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Process Reward Models (PRMs)\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u8fc7\u7a0b\u76d1\u7763\u3002", "method": "\u63d0\u51faRRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u5956\u52b1\u4e0a\u5347\u7684\u6b65\u9aa4\uff0c\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\uff0c\u9ad8\u6548\u6355\u83b7\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "result": "\u5728WebShop\u548cInterCode-SQL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRRO\u8868\u73b0\u4f18\u5f02\u4e14\u63a2\u7d22\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "RRO\u4e3aLLM\u4ee3\u7406\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20438", "pdf": "https://arxiv.org/pdf/2505.20438", "abs": "https://arxiv.org/abs/2505.20438", "authors": ["Jingyu Liu", "Ce Zhang"], "title": "HAMburger: Accelerating LLM Inference via Token Smashing", "categories": ["cs.CL"], "comment": null, "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.", "AI": {"tldr": "HAMburger\u662f\u4e00\u79cd\u5c42\u6b21\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316KV\u7f13\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u4e2d\u6bcf\u4e2atoken\u9700\u8981\u4e00\u6b21\u524d\u5411\u4f20\u9012\u548cKV\u7f13\u5b58\uff0c\u6548\u7387\u4f4e\u4e0b\u3002HAMburger\u65e8\u5728\u901a\u8fc7\u975e\u5747\u5300\u8ba1\u7b97\u548c\u5b58\u50a8\u5206\u914d\u63d0\u5347\u6548\u7387\u3002", "method": "\u5f15\u5165HAMburger\u6a21\u578b\uff0c\u7ed3\u5408\u7ec4\u5408\u5d4c\u5165\u5668\u548c\u5fae\u6b65\u89e3\u7801\u5668\uff0c\u5c06\u591a\u4e2atoken\u538b\u7f29\u4e3a\u5355\u4e2aKV\u7f13\u5b58\uff0c\u5e76\u652f\u6301\u6bcf\u4e2a\u6b65\u9aa4\u751f\u6210\u591a\u4e2atoken\u3002", "result": "KV\u7f13\u5b58\u8ba1\u7b97\u51cf\u5c112\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53472\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u77ed\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u8d28\u91cf\u3002", "conclusion": "HAMburger\u63a2\u7d22\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u9ad8\u6548\u4e14\u786c\u4ef6\u65e0\u5173\u7684\u63a8\u7406\u65b9\u6848\u3002", "relevance": 90.0}}
{"id": "2505.20615", "pdf": "https://arxiv.org/pdf/2505.20615", "abs": "https://arxiv.org/abs/2505.20615", "authors": ["Omid Halimi Milani", "Ahmet Enis Cetin", "Bharati Prasad"], "title": "Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at EUSIPCO 2025. Camera-ready due June 20, 2025", "summary": "Obstructive sleep apnea (OSA) is a significant risk factor for hypertension,\nprimarily due to intermittent hypoxia and sleep fragmentation. Predicting\nwhether individuals with OSA will develop hypertension within five years\nremains a complex challenge. This study introduces a novel deep learning\napproach that integrates Discrete Cosine Transform (DCT)-based transfer\nlearning to enhance prediction accuracy. We are the first to incorporate all\npolysomnography signals together for hypertension prediction, leveraging their\ncollective information to improve model performance. Features were extracted\nfrom these signals and transformed into a 2D representation to utilize\npre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet\nvariants. To further improve feature learning, we introduced a DCT layer, which\ntransforms input features into a frequency-based representation, preserving\nessential spectral information, decorrelating features, and enhancing\nrobustness to noise. This frequency-domain approach, coupled with transfer\nlearning, is especially beneficial for limited medical datasets, as it\nleverages rich representations from pre-trained networks to improve\ngeneralization. By strategically placing the DCT layer at deeper truncation\ndepths within EfficientNet, our model achieved a best area under the curve\n(AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature\nextraction and transfer learning in predicting hypertension risk in OSA\npatients over a five-year period.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u963b\u585e\u6027\u7761\u7720\u547c\u5438\u6682\u505c\uff08OSA\uff09\u60a3\u8005\u5728\u4e94\u5e74\u5185\u53d1\u5c55\u4e3a\u9ad8\u8840\u538b\u7684\u98ce\u9669\u3002", "motivation": "OSA\u662f\u9ad8\u8840\u538b\u7684\u91cd\u8981\u98ce\u9669\u56e0\u7d20\uff0c\u4f46\u9884\u6d4b\u5176\u53d1\u5c55\u4e3a\u9ad8\u8840\u538b\u7684\u51c6\u786e\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408DCT\u548c\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u5229\u7528\u6240\u6709\u591a\u5bfc\u7761\u7720\u56fe\u4fe1\u53f7\uff0c\u63d0\u53d6\u7279\u5f81\u5e76\u8f6c\u6362\u4e3a2D\u8868\u793a\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u76842D\u795e\u7ecf\u7f51\u7edc\uff08\u5982MobileNet\u3001EfficientNet\u3001ResNet\uff09\u3002\u5f15\u5165DCT\u5c42\u4ee5\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "result": "\u6a21\u578b\u5728EfficientNet\u4e2d\u6700\u4f73AUC\u4e3a72.88%\uff0c\u9a8c\u8bc1\u4e86\u9891\u7387\u57df\u7279\u5f81\u63d0\u53d6\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "conclusion": "DCT\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86OSA\u60a3\u8005\u9ad8\u8840\u538b\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "relevance": 20.0}}
{"id": "2505.20553", "pdf": "https://arxiv.org/pdf/2505.20553", "abs": "https://arxiv.org/abs/2505.20553", "authors": ["Lu\u00eds Carvalho", "Jo\u00e3o L. Costa", "Jos\u00e9 Mour\u00e3o", "Gon\u00e7alo Oliveira"], "title": "A ZeNN architecture to avoid the Gaussian trap", "categories": ["cs.LG", "math.PR", "68T07, 68T01", "I.2.0; G.0"], "comment": null, "summary": "We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order\nto overcome several shortcomings of standard multi-layer perceptrons (MLPs).\nNamely, in the large width limit, MLPs are non-parametric, they do not have a\nwell-defined pointwise limit, they lose non-Gaussian attributes and become\nunable to perform feature learning; moreover, finite width MLPs perform poorly\nin learning high frequencies. The new ZeNN architecture is inspired by three\nsimple principles from harmonic analysis:\n  i) Enumerate the perceptons and introduce a non-learnable weight to enforce\nconvergence;\n  ii) Introduce a scaling (or frequency) factor;\n  iii) Choose activation functions that lead to near orthogonal systems.\n  We will show that these ideas allow us to fix the referred shortcomings of\nMLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they\nexhibit a rich asymptotic structure beyond Gaussianity, and perform feature\nlearning. Moreover, when appropriate activation functions are chosen, (finite\nwidth) ZeNNs excel at learning high-frequency features of functions with low\ndimensional domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZeta Neural Networks (ZeNNs)\u7684\u65b0\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u591a\u5c42\u611f\u77e5\u673a(MLPs)\u5728\u5927\u5bbd\u5ea6\u9650\u5236\u4e0b\u7684\u7f3a\u9677\uff0c\u5982\u975e\u53c2\u6570\u6027\u3001\u70b9\u6001\u6781\u9650\u7f3a\u5931\u3001\u975e\u9ad8\u65af\u5c5e\u6027\u4e22\u5931\u53ca\u9ad8\u9891\u5b66\u4e60\u80fd\u529b\u5dee\u3002", "motivation": "MLPs\u5728\u5927\u5bbd\u5ea6\u9650\u5236\u4e0b\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff0c\u5982\u65e0\u6cd5\u6536\u655b\u3001\u5931\u53bb\u975e\u9ad8\u65af\u7279\u6027\u53ca\u9ad8\u9891\u5b66\u4e60\u80fd\u529b\u5dee\uff0cZeNNs\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u8c10\u6ce2\u5206\u6790\u7684\u4e09\u4e2a\u539f\u5219\u8bbe\u8ba1ZeNNs\uff1a\u679a\u4e3e\u611f\u77e5\u673a\u5e76\u5f15\u5165\u975e\u5b66\u4e60\u6743\u91cd\u3001\u5f15\u5165\u7f29\u653e\u56e0\u5b50\u3001\u9009\u62e9\u5bfc\u81f4\u8fd1\u6b63\u4ea4\u7cfb\u7edf\u7684\u6fc0\u6d3b\u51fd\u6570\u3002", "result": "ZeNNs\u5728\u65e0\u9650\u5bbd\u5ea6\u9650\u5236\u4e0b\u80fd\u70b9\u6001\u6536\u655b\u3001\u4fdd\u7559\u975e\u9ad8\u65af\u7279\u6027\u5e76\u5177\u5907\u7279\u5f81\u5b66\u4e60\u80fd\u529b\uff1b\u6709\u9650\u5bbd\u5ea6\u4e0b\u5728\u9ad8\u9891\u7279\u5f81\u5b66\u4e60\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ZeNNs\u6210\u529f\u89e3\u51b3\u4e86MLPs\u7684\u7f3a\u9677\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.20740", "pdf": "https://arxiv.org/pdf/2505.20740", "abs": "https://arxiv.org/abs/2505.20740", "authors": ["Xiangyu Zhao", "Wanghan Xu", "Bo Liu", "Yuhao Zhou", "Fenghua Ling", "Ben Fei", "Xiaoyu Yue", "Lei Bai", "Wenlong Zhang", "Xiao-Ming Wu"], "title": "MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of multimodal large language models (MLLMs) has\nunlocked new opportunities to tackle complex scientific challenges. Despite\nthis progress, their application in addressing earth science problems,\nespecially at the graduate level, remains underexplored. A significant barrier\nis the absence of benchmarks that capture the depth and contextual complexity\nof geoscientific reasoning. Current benchmarks often rely on synthetic datasets\nor simplistic figure-caption pairs, which do not adequately reflect the\nintricate reasoning and domain-specific insights required for real-world\nscientific applications. To address these gaps, we introduce MSEarth, a\nmultimodal scientific benchmark curated from high-quality, open-access\nscientific publications. MSEarth encompasses the five major spheres of Earth\nscience: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,\nfeaturing over 7K figures with refined captions. These captions are crafted\nfrom the original figure captions and enriched with discussions and reasoning\nfrom the papers, ensuring the benchmark captures the nuanced reasoning and\nknowledge-intensive content essential for advanced scientific tasks. MSEarth\nsupports a variety of tasks, including scientific figure captioning, multiple\nchoice questions, and open-ended reasoning challenges. By bridging the gap in\ngraduate-level benchmarks, MSEarth provides a scalable and high-fidelity\nresource to enhance the development and evaluation of MLLMs in scientific\nreasoning. The benchmark is publicly available to foster further research and\ninnovation in this field. Resources related to this benchmark can be found at\nhttps://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MSEarth\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u79d1\u5b66\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5730\u7403\u79d1\u5b66\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u53cd\u6620\u5730\u7403\u79d1\u5b66\u590d\u6742\u63a8\u7406\u7684\u57fa\u51c6\uff0cMSEarth\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MSEarth\u57fa\u4e8e\u9ad8\u8d28\u91cf\u79d1\u5b66\u51fa\u7248\u7269\uff0c\u5305\u542b7K\u591a\u5f20\u56fe\u7247\u53ca\u8be6\u7ec6\u6807\u6ce8\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002", "result": "MSEarth\u4e3aMLLMs\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u7684\u8bc4\u4f30\u8d44\u6e90\u3002", "conclusion": "MSEarth\u63a8\u52a8\u4e86MLLMs\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "relevance": 60.0}}
{"id": "2505.20445", "pdf": "https://arxiv.org/pdf/2505.20445", "abs": "https://arxiv.org/abs/2505.20445", "authors": ["Zhaolin Li", "Jan Niehues"], "title": "In-context Language Learning for Endangered Languages in Speech Recognition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With approximately 7,000 languages spoken worldwide, current large language\nmodels (LLMs) support only a small subset. Prior research indicates LLMs can\nlearn new languages for certain tasks without supervised data. We extend this\ninvestigation to speech recognition, investigating whether LLMs can learn\nunseen, low-resource languages through in-context learning (ICL). With\nexperiments on four diverse endangered languages that LLMs have not been\ntrained on, we find that providing more relevant text samples enhances\nperformance in both language modelling and Automatic Speech Recognition (ASR)\ntasks. Furthermore, we show that the probability-based approach outperforms the\ntraditional instruction-based approach in language learning. Lastly, we show\nICL enables LLMs to achieve ASR performance that is comparable to or even\nsurpasses dedicated language models trained specifically for these languages,\nwhile preserving the original capabilities of the LLMs.", "AI": {"tldr": "LLMs can learn low-resource languages via in-context learning (ICL), improving language modeling and ASR tasks, even surpassing dedicated models.", "motivation": "Address the limited language support of LLMs by exploring unsupervised learning for low-resource languages.", "method": "Use ICL on four endangered languages, comparing probability-based and instruction-based approaches.", "result": "ICL enhances performance, with probability-based methods outperforming instruction-based ones, achieving ASR results comparable to dedicated models.", "conclusion": "ICL enables LLMs to learn new languages effectively without losing original capabilities.", "relevance": 85.0}}
{"id": "2505.20617", "pdf": "https://arxiv.org/pdf/2505.20617", "abs": "https://arxiv.org/abs/2505.20617", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Fayao Liu", "Xulei Yang", "Jiacheng Wei", "Lemiao Qiu", "Guosheng Lin"], "title": "OccLE: Label-Efficient 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction offers an intuitive and efficient scene\nunderstanding and has attracted significant interest in autonomous driving\nperception. Existing approaches either rely on full supervision, which demands\ncostly voxel-level annotations, or on self-supervision, which provides limited\nguidance and yields suboptimal performance. To address these challenges, we\npropose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes\nimages and LiDAR as inputs and maintains high performance with limited voxel\nannotations. Our intuition is to decouple the semantic and geometric learning\ntasks and then fuse the learned feature grids from both tasks for the final\nsemantic occupancy prediction. Therefore, the semantic branch distills 2D\nfoundation model to provide aligned pseudo labels for 2D and 3D semantic\nlearning. The geometric branch integrates image and LiDAR inputs in cross-plane\nsynergy based on their inherency, employing semi-supervision to enhance\ngeometry learning. We fuse semantic-geometric feature grids through Dual Mamba\nand incorporate a scatter-accumulated projection to supervise unannotated\nprediction with aligned pseudo labels. Experiments show that OccLE achieves\ncompetitive performance with only 10% of voxel annotations, reaching a mIoU of\n16.59% on the SemanticKITTI validation set.", "AI": {"tldr": "OccLE\u662f\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u76843D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u548c\u51e0\u4f55\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u878d\u5408\u5176\u7279\u5f81\u7f51\u683c\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u9884\u6d4b\uff0c\u4ec5\u970010%\u7684\u4f53\u7d20\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u5bf9\u6602\u8d35\u4f53\u7d20\u6807\u6ce8\u7684\u4f9d\u8d56\u6216\u81ea\u76d1\u7763\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1\uff09\u89e3\u8026\u8bed\u4e49\u548c\u51e0\u4f55\u5b66\u4e60\u4efb\u52a1\uff1b2\uff09\u8bed\u4e49\u5206\u652f\u5229\u75282D\u57fa\u7840\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff1b3\uff09\u51e0\u4f55\u5206\u652f\u901a\u8fc7\u534a\u76d1\u7763\u589e\u5f3a\u5b66\u4e60\uff1b4\uff09\u901a\u8fc7Dual Mamba\u878d\u5408\u7279\u5f81\u7f51\u683c\u3002", "result": "\u5728SemanticKITTI\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523016.59%\u7684mIoU\uff0c\u4ec5\u970010%\u7684\u4f53\u7d20\u6807\u6ce8\u3002", "conclusion": "OccLE\u5728\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20556", "pdf": "https://arxiv.org/pdf/2505.20556", "abs": "https://arxiv.org/abs/2505.20556", "authors": ["Yinglun Xu", "Hangoo Kang", "Tarun Suresh", "Yuxuan Wan", "Gagandeep Singh"], "title": "Learning a Pessimistic Reward Model in RLHF", "categories": ["cs.LG"], "comment": null, "summary": "This work proposes `PET', a novel pessimistic reward fine-tuning method, to\nlearn a pessimistic reward model robust against reward hacking in offline\nreinforcement learning from human feedback (RLHF). Traditional reward modeling\ntechniques in RLHF train an imperfect reward model, on which a KL\nregularization plays a pivotal role in mitigating reward hacking when\noptimizing a policy. Such an intuition-based method still suffers from reward\nhacking, and the policies with large KL divergence from the dataset\ndistribution are excluded during learning. In contrast, we show that when\noptimizing a policy on a pessimistic reward model fine-tuned through PET,\nreward hacking can be prevented without relying on any regularization. We test\nour methods on the standard TL;DR summarization dataset. We find that one can\nlearn a high-quality policy on our pessimistic reward without using any\nregularization. Such a policy has a high KL divergence from the dataset\ndistribution while having high performance in practice. In summary, our work\nshows the feasibility of learning a pessimistic reward model against reward\nhacking. The agent can greedily search for the policy with a high pessimistic\nreward without suffering from reward hacking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPET\u7684\u60b2\u89c2\u5956\u52b1\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u79bb\u7ebfRLHF\u4e2d\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u65e0\u9700\u4f9d\u8d56KL\u6b63\u5219\u5316\u3002", "motivation": "\u4f20\u7edfRLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4f9d\u8d56KL\u6b63\u5219\u5316\u4f46\u6548\u679c\u6709\u9650\u3002", "method": "\u901a\u8fc7PET\u5fae\u8c03\u60b2\u89c2\u5956\u52b1\u6a21\u578b\uff0c\u4f18\u5316\u7b56\u7565\u65f6\u65e0\u9700\u6b63\u5219\u5316\u3002", "result": "\u5728TL;DR\u6570\u636e\u96c6\u4e0a\uff0cPET\u65b9\u6cd5\u6210\u529f\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u7b56\u7565\u6027\u80fd\u9ad8\u4e14KL\u6563\u5ea6\u5927\u3002", "conclusion": "PET\u5c55\u793a\u4e86\u5b66\u4e60\u60b2\u89c2\u5956\u52b1\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u7b56\u7565\u53ef\u8d2a\u5a6a\u4f18\u5316\u800c\u65e0\u5956\u52b1\u9ed1\u5ba2\u98ce\u9669\u3002", "relevance": 85.0}}
{"id": "2505.20749", "pdf": "https://arxiv.org/pdf/2505.20749", "abs": "https://arxiv.org/abs/2505.20749", "authors": ["Alfin Wijaya Rahardja", "Junwei Liu", "Weitong Chen", "Zhenpeng Chen", "Yiling Lou"], "title": "Can Agents Fix Agent Issues?", "categories": ["cs.AI", "cs.SE"], "comment": "18 pages, 7 figures", "summary": "LLM-based agent systems are emerging as a new software paradigm and have been\nwidely adopted across diverse domains such as medicine, robotics, and\nprogramming. However, maintaining these systems requires substantial effort, as\nthey are inevitably prone to bugs and continually evolve to meet changing\nexternal requirements. Therefore, automatically resolving agent issues (i.e.,\nbug reports or feature requests) is a crucial and challenging task. While\nrecent software engineering (SE) agents (e.g., SWE-agent) have shown promise in\naddressing issues in traditional software systems, it remains unclear how\neffectively they can resolve real-world issues in agent systems, which differ\nsignificantly from traditional software. To fill this gap, we first manually\nanalyze 201 real-world agent issues and identify common categories of agent\nissues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a\nreproducible benchmark comprising 50 agent issue resolution tasks (each with an\nexecutable environment and failure-triggering tests). We further evaluate\nstate-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited\neffectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results\nunderscore the unique challenges of maintaining agent systems compared to\ntraditional software, highlighting the need for further research to develop\nadvanced SE agents for resolving agent issues. Data and code are available at\nhttps://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM-based agent\u7cfb\u7edf\u7684\u81ea\u52a8\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u6784\u5efa\u4e86AGENTISSUE-BENCH\u57fa\u51c6\uff0c\u5e76\u53d1\u73b0\u73b0\u6709SE agent\u5728\u89e3\u51b3agent\u7cfb\u7edf\u95ee\u9898\u65f6\u7684\u6709\u6548\u6027\u6709\u9650\uff083.33%-12.67%\uff09\u3002", "motivation": "LLM-based agent\u7cfb\u7edf\u5e7f\u6cdb\u4f7f\u7528\u4f46\u7ef4\u62a4\u56f0\u96be\uff0c\u9700\u8981\u81ea\u52a8\u89e3\u51b3bug\u548c\u529f\u80fd\u8bf7\u6c42\u7684\u65b9\u6cd5\u3002", "method": "\u624b\u52a8\u5206\u6790201\u4e2a\u771f\u5b9eagent\u95ee\u9898\uff0c\u6784\u5efaAGENTISSUE-BENCH\u57fa\u51c6\uff0850\u4e2a\u4efb\u52a1\uff09\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709SE agent\u3002", "result": "SE agent\u5728AGENTISSUE-BENCH\u4e0a\u7684\u89e3\u51b3\u7387\u4ec5\u4e3a3.33%-12.67%\uff0c\u8868\u660eagent\u7cfb\u7edf\u7ef4\u62a4\u7684\u72ec\u7279\u6311\u6218\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u66f4\u5148\u8fdb\u7684SE agent\u4ee5\u89e3\u51b3agent\u7cfb\u7edf\u95ee\u9898\u3002", "relevance": 75.0}}
{"id": "2505.20451", "pdf": "https://arxiv.org/pdf/2505.20451", "abs": "https://arxiv.org/abs/2505.20451", "authors": ["Sahana Ramnath", "Anurag Mudgil", "Brihi Joshi", "Skyler Hallinan", "Xiang Ren"], "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries", "categories": ["cs.CL"], "comment": null, "summary": "Today, large language models are widely used as judges to evaluate responses\nfrom other language models. Hence, it is imperative to benchmark and improve\nthese LLM-judges on real-world language model usage: a typical human-assistant\nconversation is lengthy, and shows significant diversity in topics, intents,\nand requirements across turns, e.g. social interactions, task requests,\nfeedback. We present Amulet, a framework that leverages pertinent linguistic\nconcepts of dialog-acts and maxims to improve the accuracy of LLM-judges on\npreference data with complex, multi-turn conversational context. Amulet\npresents valuable insights about (a) the communicative structures and intents\npresent in the conversation (dialog acts), and (b) the satisfaction of\nconversational principles (maxims) by the preference responses, and uses them\nto make judgments. On four challenging datasets, Amulet shows that (a) humans\nfrequently (60 to 70 percent of the time) change their intents from one turn of\nthe conversation to the next, and (b) in 75 percent of instances, the\npreference responses can be differentiated via dialog acts and/or maxims,\nreiterating the latter's significance in judging such data. Amulet can be used\neither as a judge by applying the framework to a single LLM, or integrated into\na jury with different LLM judges; our judges and juries show strong\nimprovements on relevant baselines for all four datasets.", "AI": {"tldr": "Amulet\u6846\u67b6\u5229\u7528\u5bf9\u8bdd\u884c\u4e3a\u548c\u51c6\u5219\u6539\u8fdbLLM\u8bc4\u59d4\u5728\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u8bc4\u59d4\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u54cd\u5e94\u65f6\u5e7f\u6cdb\u4f7f\u7528\uff0c\u9700\u63d0\u5347\u5176\u5728\u771f\u5b9e\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002", "method": "Amulet\u7ed3\u5408\u5bf9\u8bdd\u884c\u4e3a\u548c\u51c6\u5219\u5206\u6790\u5bf9\u8bdd\u7ed3\u6784\u548c\u610f\u56fe\uff0c\u5e76\u7528\u4e8e\u5224\u65ad\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e2d\uff0cAmulet\u663e\u793a\u4eba\u7c7b\u610f\u56fe\u9891\u7e41\u53d8\u5316\uff0c\u4e1475%\u7684\u504f\u597d\u54cd\u5e94\u53ef\u901a\u8fc7\u5bf9\u8bdd\u884c\u4e3a\u6216\u51c6\u5219\u533a\u5206\u3002", "conclusion": "Amulet\u663e\u8457\u63d0\u5347LLM\u8bc4\u59d4\u5728\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53ef\u4f5c\u4e3a\u72ec\u7acb\u8bc4\u59d4\u6216\u96c6\u6210\u5230\u8bc4\u59d4\u56e2\u4e2d\u3002", "relevance": 85.0}}
{"id": "2505.20626", "pdf": "https://arxiv.org/pdf/2505.20626", "abs": "https://arxiv.org/abs/2505.20626", "authors": ["Yohai Mazuz", "Janna Bruner", "Lior Wolf"], "title": "ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation", "categories": ["cs.CV"], "comment": null, "summary": "In text-to-image models, consistent character generation is the task of\nachieving text alignment while maintaining the subject's appearance across\ndifferent prompts. However, since style and appearance are often entangled, the\nexisting methods struggle to preserve consistent subject characteristics while\nadhering to varying style prompts. Current approaches for consistent\ntext-to-image generation typically rely on large-scale fine-tuning on curated\nimage sets or per-subject optimization, which either fail to generalize across\nprompts or do not align well with textual descriptions. Meanwhile,\ntraining-free methods often fail to maintain subject consistency across\ndifferent styles. In this work, we introduce a training-free method that\nachieves both style alignment and subject consistency. The attention matrices\nare manipulated such that Queries and Keys are obtained from the anchor\nimage(s) that are used to define the subject, while the Values are imported\nfrom a parallel copy that is not subject-anchored. Additionally, cross-image\ncomponents are added to the self-attention mechanism by expanding the Key and\nValue matrices. To do without shifting from the target style, we align the\nstatistics of the Value matrices. As is demonstrated in a comprehensive battery\nof qualitative and quantitative experiments, our method effectively decouples\nstyle from subject appearance and enables faithful generation of text-aligned\nimages with consistent characters across diverse styles.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u64cd\u7eb5\u6ce8\u610f\u529b\u77e9\u9635\u5b9e\u73b0\u6587\u672c\u5bf9\u9f50\u548c\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u8026\u98ce\u683c\u4e0e\u4e3b\u9898\u5916\u89c2\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u98ce\u683c\u5bf9\u9f50\u4e4b\u95f4\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u4f9d\u8d56\u5927\u89c4\u6a21\u5fae\u8c03\u6216\u4f18\u5316\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u951a\u5b9a\u56fe\u50cf\u7684Query\u548cKey\uff0c\u540c\u65f6\u4ece\u975e\u951a\u5b9a\u56fe\u50cf\u5bfc\u5165Value\uff0c\u5e76\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u6dfb\u52a0\u8de8\u56fe\u50cf\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u8026\u98ce\u683c\u4e0e\u4e3b\u9898\uff0c\u751f\u6210\u591a\u6837\u98ce\u683c\u4e0b\u4e00\u81f4\u7684\u6587\u672c\u5bf9\u9f50\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u98ce\u683c\u5bf9\u9f50\u548c\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20561", "pdf": "https://arxiv.org/pdf/2505.20561", "abs": "https://arxiv.org/abs/2505.20561", "authors": ["Shenao Zhang", "Yaqing Wang", "Yinxiao Liu", "Tianqi Liu", "Peter Grabowski", "Eugene Ie", "Zhaoran Wang", "Yunxuan Li"], "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nexhibited strong reasoning capabilities and emergent reflective behaviors, such\nas backtracking and error correction. However, conventional Markovian RL\nconfines exploration to the training phase to learn an optimal deterministic\npolicy and depends on the history contexts only through the current state.\nTherefore, it remains unclear whether reflective reasoning will emerge during\nMarkovian RL training, or why they are beneficial at test time. To remedy this,\nwe recast reflective exploration within the Bayes-Adaptive RL framework, which\nexplicitly optimizes the expected return under a posterior distribution over\nMarkov decision processes. This Bayesian formulation inherently incentivizes\nboth reward-maximizing exploitation and information-gathering exploration via\nbelief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and\nswitch strategies based on the observed outcomes, offering principled guidance\non when and how the model should reflectively explore. Empirical results on\nboth synthetic and mathematical reasoning tasks demonstrate that BARL\noutperforms standard Markovian RL approaches at test time, achieving superior\ntoken efficiency with improved exploration effectiveness. Our code is available\nat https://github.com/shenao-zhang/BARL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\uff08BARL\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u5728LLM\u8bad\u7ec3\u4e2d\u65e0\u6cd5\u6fc0\u53d1\u53cd\u601d\u6027\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u63a2\u7d22\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3LLM\u65f6\u65e0\u6cd5\u660e\u786e\u6fc0\u53d1\u53cd\u601d\u6027\u63a8\u7406\u884c\u4e3a\uff08\u5982\u56de\u6eaf\u548c\u9519\u8bef\u4fee\u6b63\uff09\uff0c\u4e14\u5176\u63a2\u7d22\u4ec5\u9650\u4e8e\u8bad\u7ec3\u9636\u6bb5\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u8d1d\u53f6\u65af\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u5c06\u53cd\u601d\u6027\u63a2\u7d22\u91cd\u65b0\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u540e\u9a8c\u5206\u5e03\u4f18\u5316\u671f\u671b\u56de\u62a5\uff0c\u6fc0\u52b1\u6a21\u578b\u8fdb\u884c\u5956\u52b1\u6700\u5927\u5316\u548c\u4fe1\u606f\u6536\u96c6\u7684\u63a2\u7d22\u3002\u63d0\u51fa\u4e86BARL\u7b97\u6cd5\uff0c\u6307\u5bfcLLM\u6839\u636e\u89c2\u5bdf\u7ed3\u679c\u5207\u6362\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBARL\u5728\u6d4b\u8bd5\u65f6\u4f18\u4e8e\u4f20\u7edf\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6807\u8bb0\u6548\u7387\u548c\u63a2\u7d22\u6548\u679c\u3002", "conclusion": "BARL\u4e3aLLM\u7684\u53cd\u601d\u6027\u63a2\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "relevance": 85.0}}
{"id": "2505.20820", "pdf": "https://arxiv.org/pdf/2505.20820", "abs": "https://arxiv.org/abs/2505.20820", "authors": ["Hyomin Kim", "Yunhui Jang", "Sungsoo Ahn"], "title": "MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have large potential for molecular optimization,\nas they can gather external chemistry tools and enable collaborative\ninteractions to iteratively refine molecular candidates. However, this\npotential remains underexplored, particularly in the context of structured\nreasoning, interpretability, and comprehensive tool-grounded molecular\noptimization. To address this gap, we introduce MT-Mol, a multi-agent framework\nfor molecular optimization that leverages tool-guided reasoning and\nrole-specialized LLM agents. Our system incorporates comprehensive RDKit tools,\ncategorized into five distinct domains: structural descriptors, electronic and\ntopological features, fragment-based functional groups, molecular\nrepresentations, and miscellaneous chemical properties. Each category is\nmanaged by an expert analyst agent, responsible for extracting task-relevant\ntools and enabling interpretable, chemically grounded feedback. MT-Mol produces\nmolecules with tool-aligned and stepwise reasoning through the interaction\nbetween the analyst agents, a molecule-generating scientist, a reasoning-output\nverifier, and a reviewer agent. As a result, we show that our framework shows\nthe state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.", "AI": {"tldr": "MT-Mol\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5de5\u5177\u5f15\u5bfc\u7684\u63a8\u7406\u548c\u89d2\u8272\u4e13\u4e1a\u5316\u7684LLM\u4ee3\u7406\u8fdb\u884c\u5206\u5b50\u4f18\u5316\uff0c\u5728PMO-1K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u5206\u5b50\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5316\u63a8\u7406\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5de5\u5177\u5f15\u5bfc\u4f18\u5316\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165MT-Mol\u6846\u67b6\uff0c\u7ed3\u5408RDKit\u5de5\u5177\u548c\u89d2\u8272\u4e13\u4e1a\u5316\u7684LLM\u4ee3\u7406\uff08\u5982\u5206\u6790\u5e08\u3001\u5206\u5b50\u751f\u6210\u79d1\u5b66\u5bb6\u7b49\uff09\uff0c\u8fdb\u884c\u5de5\u5177\u5bf9\u9f50\u7684\u9010\u6b65\u63a8\u7406\u3002", "result": "\u5728PMO-1K\u57fa\u51c6\u6d4b\u8bd5\u768423\u9879\u4efb\u52a1\u4e2d\uff0c17\u9879\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MT-Mol\u5c55\u793a\u4e86LLM\u5728\u5206\u5b50\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5de5\u5177\u5f15\u5bfc\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "relevance": 70.0}}
{"id": "2505.20482", "pdf": "https://arxiv.org/pdf/2505.20482", "abs": "https://arxiv.org/abs/2505.20482", "authors": ["Vibhor Agarwal", "Arjoo Gupta", "Suparna De", "Nishanth Sastry"], "title": "Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at International AAAI Conference on Web and Social Media\n  (ICWSM) 2025", "summary": "Understanding online conversations has attracted research attention with the\ngrowth of social networks and online discussion forums. Content analysis of\nposts and replies in online conversations is difficult because each individual\nutterance is usually short and may implicitly refer to other posts within the\nsame conversation. Thus, understanding individual posts requires capturing the\nconversational context and dependencies between different parts of a\nconversation tree and then encoding the context dependencies between posts and\ncomments/replies into the language model.\n  To this end, we propose a general-purpose mechanism to discover appropriate\nconversational context for various aspects about an online post in a\nconversation, such as whether it is informative, insightful, interesting or\nfunny. Specifically, we design two families of Conversation Kernels, which\nexplore different parts of the neighborhood of a post in the tree representing\nthe conversation and through this, build relevant conversational context that\nis appropriate for each task being considered. We apply our developed method to\nconversations crawled from slashdot.org, which allows users to apply highly\ndifferent labels to posts, such as 'insightful', 'funny', etc., and therefore\nprovides an ideal experimental platform to study whether a framework such as\nConversation Kernels is general-purpose and flexible enough to be adapted to\ndisparately different conversation understanding tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u673a\u5236\uff08Conversation Kernels\uff09\u6765\u5206\u6790\u5728\u7ebf\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u8bc4\u4f30\u5e16\u5b50\u7684\u4e0d\u540c\u5c5e\u6027\uff08\u5982\u4fe1\u606f\u6027\u3001\u8da3\u5473\u6027\uff09\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u7f51\u7edc\u7684\u589e\u957f\uff0c\u7406\u89e3\u5728\u7ebf\u5bf9\u8bdd\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7531\u4e8e\u5e16\u5b50\u77ed\u4e14\u9690\u542b\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5bf9\u8bdd\u6811\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cdConversation Kernels\uff0c\u63a2\u7d22\u5bf9\u8bdd\u6811\u4e2d\u5e16\u5b50\u7684\u4e0d\u540c\u90bb\u57df\uff0c\u6784\u5efa\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728slashdot.org\u7684\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5bf9\u8bdd\u7406\u89e3\u4efb\u52a1\u3002", "conclusion": "Conversation Kernels\u662f\u4e00\u79cd\u901a\u7528\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u5bf9\u8bdd\u7406\u89e3\u4efb\u52a1\u3002", "relevance": 40.0}}
{"id": "2505.20629", "pdf": "https://arxiv.org/pdf/2505.20629", "abs": "https://arxiv.org/abs/2505.20629", "authors": ["Bolin Lai", "Sangmin Lee", "Xu Cao", "Xiang Li", "James M. Rehg"], "title": "Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 11 figures, 4 tables", "summary": "Text-image-to-video (TI2V) generation is a critical problem for controllable\nvideo generation using both semantic and visual conditions. Most existing\nmethods typically add visual conditions to text-to-video (T2V) foundation\nmodels by finetuning, which is costly in resources and only limited to a few\npredefined conditioning settings. To tackle this issue, we introduce a unified\nformulation for TI2V generation with flexible visual conditioning. Furthermore,\nwe propose an innovative training-free approach, dubbed FlexTI2V, that can\ncondition T2V foundation models on an arbitrary amount of images at arbitrary\npositions. Specifically, we firstly invert the condition images to noisy\nrepresentation in a latent space. Then, in the denoising process of T2V models,\nour method uses a novel random patch swapping strategy to incorporate visual\nfeatures into video representations through local image patches. To balance\ncreativity and fidelity, we use a dynamic control mechanism to adjust the\nstrength of visual conditioning to each video frame. Extensive experiments\nvalidate that our method surpasses previous training-free image conditioning\nmethods by a notable margin. We also show more insights of our method by\ndetailed ablation study and analysis.", "AI": {"tldr": "FlexTI2V\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684TI2V\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5757\u4ea4\u6362\u7b56\u7565\u5c06\u89c6\u89c9\u7279\u5f81\u878d\u5165\u89c6\u9891\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u56fe\u50cf\u6761\u4ef6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5fae\u8c03\u5c06\u89c6\u89c9\u6761\u4ef6\u6dfb\u52a0\u5230T2V\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4ec5\u9650\u4e8e\u9884\u5b9a\u4e49\u6761\u4ef6\u8bbe\u7f6e\u3002FlexTI2V\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6761\u4ef6\u56fe\u50cf\u53cd\u8f6c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u566a\u58f0\u8868\u793a\uff0c\u5e76\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u968f\u673a\u5757\u4ea4\u6362\u7b56\u7565\u878d\u5165\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlexTI2V\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u6761\u4ef6\u65b9\u6cd5\u3002", "conclusion": "FlexTI2V\u4e3aTI2V\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u521b\u9020\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "relevance": 40.0}}
{"id": "2505.20563", "pdf": "https://arxiv.org/pdf/2505.20563", "abs": "https://arxiv.org/abs/2505.20563", "authors": ["Jingjing Liu", "Xiansen Ju", "Xianchao Xiu", "Wanquan Liu"], "title": "Bi-Level Unsupervised Feature Selection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Unsupervised feature selection (UFS) is an important task in data\nengineering. However, most UFS methods construct models from a single\nperspective and often fail to simultaneously evaluate feature importance and\npreserve their inherent data structure, thus limiting their performance. To\naddress this challenge, we propose a novel bi-level unsupervised feature\nselection (BLUFS) method, including a clustering level and a feature level.\nSpecifically, at the clustering level, spectral clustering is used to generate\npseudo-labels for representing the data structure, while a continuous linear\nregression model is developed to learn the projection matrix. At the feature\nlevel, the $\\ell_{2,0}$-norm constraint is imposed on the projection matrix for\nmore effectively selecting features. To the best of our knowledge, this is the\nfirst work to combine a bi-level framework with the $\\ell_{2,0}$-norm. To solve\nthe proposed bi-level model, we design an efficient proximal alternating\nminimization (PAM) algorithm, whose subproblems either have explicit solutions\nor can be computed by fast solvers. Furthermore, we establish the convergence\nresult and computational complexity. Finally, extensive experiments on two\nsynthetic datasets and eight real datasets demonstrate the superiority of BLUFS\nin clustering and classification tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5c42\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff08BLUFS\uff09\uff0c\u7ed3\u5408\u805a\u7c7b\u548c\u7279\u5f81\u7ea7\u522b\uff0c\u5229\u7528\u8c31\u805a\u7c7b\u548c\u21132,0\u8303\u6570\u7ea6\u675f\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u9009\u62e9\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u901a\u5e38\u4ece\u5355\u4e00\u89c6\u89d2\u5efa\u6a21\uff0c\u96be\u4ee5\u540c\u65f6\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\u548c\u4fdd\u7559\u6570\u636e\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "BLUFS\u65b9\u6cd5\u5305\u62ec\u805a\u7c7b\u7ea7\u522b\uff08\u8c31\u805a\u7c7b\u751f\u6210\u4f2a\u6807\u7b7e\uff09\u548c\u7279\u5f81\u7ea7\u522b\uff08\u21132,0\u8303\u6570\u7ea6\u675f\u6295\u5f71\u77e9\u9635\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684PAM\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBLUFS\u5728\u805a\u7c7b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "BLUFS\u901a\u8fc7\u53cc\u5c42\u6846\u67b6\u548c\u21132,0\u8303\u6570\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.20869", "pdf": "https://arxiv.org/pdf/2505.20869", "abs": "https://arxiv.org/abs/2505.20869", "authors": ["Kuo Zhou", "Lu Zhang"], "title": "Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated formidable capabilities in\nsolving mathematical problems, yet they may still commit logical reasoning and\ncomputational errors during the problem-solving process. Thus, this paper\nproposes a framework, MATH-VF, which includes a Formalizer and a Critic, for\nformally verifying the correctness of the solutions generated by large language\nmodels. Our framework first utilizes a Formalizer which employs an LLM to\ntranslate a natural language solution into a formal context. Afterward, our\nCritic (which integrates various external tools such as a Computer Algebra\nSystem and an SMT solver) evaluates the correctness of each statement within\nthe formal context, and when a statement is incorrect, our Critic provides\ncorrective feedback. We empirically investigate the effectiveness of MATH-VF in\ntwo scenarios: 1) Verification: MATH-VF is utilized to determine the\ncorrectness of a solution to a given problem. 2) Refinement: When MATH-VF\nidentifies errors in the solution generated by an LLM-based solution generator\nfor a given problem, it submits the corrective suggestions proposed by the\nCritic to the solution generator to regenerate the solution. We evaluate our\nframework on widely used mathematical benchmarks: MATH500 and ProcessBench,\ndemonstrating the superiority of our approach over existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMATH-VF\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u5916\u90e8\u5de5\u5177\u7ed3\u5408\uff0c\u7ea0\u6b63LLM\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u903b\u8f91\u548c\u8ba1\u7b97\u9519\u8bef\u3002", "motivation": "LLM\u5728\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\u53ef\u80fd\u4ea7\u751f\u903b\u8f91\u548c\u8ba1\u7b97\u9519\u8bef\uff0c\u9700\u8981\u4e00\u79cd\u9a8c\u8bc1\u548c\u7ea0\u6b63\u673a\u5236\u3002", "method": "MATH-VF\u5305\u542bFormalizer\uff08\u5c06\u81ea\u7136\u8bed\u8a00\u89e3\u8f6c\u4e3a\u5f62\u5f0f\u5316\u4e0a\u4e0b\u6587\uff09\u548cCritic\uff08\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u9a8c\u8bc1\u5e76\u63d0\u4f9b\u53cd\u9988\uff09\u3002", "result": "\u5728MATH500\u548cProcessBench\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MATH-VF\u80fd\u6709\u6548\u9a8c\u8bc1\u548c\u7ea0\u6b63LLM\u751f\u6210\u7684\u6570\u5b66\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.20487", "pdf": "https://arxiv.org/pdf/2505.20487", "abs": "https://arxiv.org/abs/2505.20487", "authors": ["Roi Cohen", "Russa Biswas", "Gerard de Melo"], "title": "InFact: Informativeness Alignment for Improved LLM Factuality", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual completeness is a general term that captures how detailed and\ninformative a factually correct text is. For instance, the factual sentence\n``Barack Obama was born in the United States'' is factually correct, though\nless informative than the factual sentence ``Barack Obama was born in Honolulu,\nHawaii, United States''. Despite the known fact that LLMs tend to hallucinate\nand generate factually incorrect text, they might also tend to choose to\ngenerate factual text that is indeed factually correct and yet less informative\nthan other, more informative choices. In this work, we tackle this problem by\nproposing an informativeness alignment mechanism. This mechanism takes\nadvantage of recent factual benchmarks to propose an informativeness alignment\nobjective. This objective prioritizes answers that are both correct and\ninformative. A key finding of our work is that when training a model to\nmaximize this objective or optimize its preference, we can improve not just\ninformativeness but also factuality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u5bf9\u9f50\u673a\u5236\uff0c\u65e8\u5728\u63d0\u5347LLM\u751f\u6210\u6587\u672c\u7684\u4fe1\u606f\u91cf\u548c\u4e8b\u5b9e\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5bb9\u6613\u751f\u6210\u4e8b\u5b9e\u9519\u8bef\u7684\u6587\u672c\uff0c\u4f46\u5373\u4f7f\u751f\u6210\u4e8b\u5b9e\u6b63\u786e\u7684\u6587\u672c\uff0c\u5176\u4fe1\u606f\u91cf\u4e5f\u53ef\u80fd\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u73b0\u6709\u7684\u4e8b\u5b9e\u57fa\u51c6\uff0c\u63d0\u51fa\u4fe1\u606f\u5bf9\u9f50\u76ee\u6807\uff0c\u4f18\u5316\u6a21\u578b\u751f\u6210\u65e2\u6b63\u786e\u53c8\u8be6\u7ec6\u7684\u56de\u7b54\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u4fe1\u606f\u5bf9\u9f50\u76ee\u6807\u4e0d\u4ec5\u80fd\u63d0\u5347\u4fe1\u606f\u91cf\uff0c\u8fd8\u80fd\u6539\u5584\u4e8b\u5b9e\u6027\u3002", "conclusion": "\u4fe1\u606f\u5bf9\u9f50\u673a\u5236\u662f\u63d0\u5347LLM\u751f\u6210\u8d28\u91cf\u7684\u6709\u6548\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.20637", "pdf": "https://arxiv.org/pdf/2505.20637", "abs": "https://arxiv.org/abs/2505.20637", "authors": ["Ana M. Cabanas", "Alma Pedro", "Domingo Mery"], "title": "TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Understanding how facial affect analysis (FAA) systems perform across\ndifferent demographic groups requires reliable measurement of sensitive\nattributes such as ancestry, often approximated by skin tone, which itself is\nhighly influenced by lighting conditions. This study compares two objective\nskin tone classification methods: the widely used Individual Typology Angle\n(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and\nHue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness\nacross skin tone groups defined by each method. Results reveal a severe\nunderrepresentation of dark skin tones ($\\sim 2 \\%$), alongside fairness\ndisparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While\nITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$\nmethod yields more consistent subgrouping and enables clearer diagnostics\nthrough metrics such as Equal Opportunity. Grad-CAM analysis further highlights\ndifferences in model attention patterns by skin tone, suggesting variation in\nfeature encoding. To support future mitigation efforts, we also propose a\nmodular fairness-aware pipeline that integrates perceptual skin tone\nestimation, model interpretability, and fairness evaluation. These findings\nemphasize the relevance of skin tone measurement choices in fairness assessment\nand suggest that ITA-based evaluations may overlook disparities affecting\ndarker-skinned individuals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u80a4\u8272\u5206\u7c7b\u65b9\u6cd5\uff08ITA\u548c\u57fa\u4e8e$L^*$-$H^*$\u7684\u65b9\u6cd5\uff09\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u9762\u90e8\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u8868\u73b0\uff0c\u53d1\u73b0\u6df1\u80a4\u8272\u7fa4\u4f53\u4ee3\u8868\u6027\u4e0d\u8db3\u4e14\u5b58\u5728\u516c\u5e73\u6027\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u4e0d\u540c\u80a4\u8272\u5206\u7c7b\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u9762\u90e8\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u6df1\u80a4\u8272\u7fa4\u4f53\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528AffectNet\u6570\u636e\u96c6\u548cMobileNet\u6a21\u578b\uff0c\u6bd4\u8f83ITA\u548c$L^*$-$H^*$\u65b9\u6cd5\u5728\u80a4\u8272\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7F1-score\u3001TPR\u7b49\u6307\u6807\u8bc4\u4f30\u516c\u5e73\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6df1\u80a4\u8272\u7fa4\u4f53\u4ee3\u8868\u6027\u4e0d\u8db3\uff08\u7ea62%\uff09\uff0c\u4e14ITA\u65b9\u6cd5\u56e0\u5bf9\u5149\u7167\u654f\u611f\u800c\u8868\u73b0\u4e0d\u4f73\uff0c$L^*$-$H^*$\u65b9\u6cd5\u66f4\u4e00\u81f4\u4e14\u80fd\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u516c\u5e73\u6027\u8bca\u65ad\u3002", "conclusion": "\u80a4\u8272\u5206\u7c7b\u65b9\u6cd5\u7684\u9009\u62e9\u5bf9\u516c\u5e73\u6027\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0cITA\u53ef\u80fd\u4f4e\u4f30\u6df1\u80a4\u8272\u7fa4\u4f53\u7684\u5dee\u5f02\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u6d41\u7a0b\u3002", "relevance": 30.0}}
{"id": "2505.20578", "pdf": "https://arxiv.org/pdf/2505.20578", "abs": "https://arxiv.org/abs/2505.20578", "authors": ["Xingyu Chen", "Shihao Ma", "Runsheng Lin", "Jiecong Lin", "Bo Wang"], "title": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": "9 pages, 3 figures", "summary": "Designing regulatory DNA sequences that achieve precise cell-type-specific\ngene expression is crucial for advancements in synthetic biology, gene therapy\nand precision medicine. Although transformer-based language models (LMs) can\neffectively capture patterns in regulatory DNA, their generative approaches\noften struggle to produce novel sequences with reliable cell-specific activity.\nHere, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL)\nframework tailored for designing regulatory DNA sequences with controllable\ncell-type specificity. By formulating regulatory sequence design as a\nbiologically informed constrained optimization problem, we apply RL to\nautoregressive genomic LMs, enabling the models to iteratively refine sequences\nthat maximize regulatory activity in targeted cell types while constraining\noff-target effects. Our evaluation on human promoters and enhancers\ndemonstrates that Ctrl-DNA consistently outperforms existing generative and\nRL-based approaches, generating high-fitness regulatory sequences and achieving\nstate-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences\ncapture key cell-type-specific transcription factor binding sites (TFBS), short\nDNA motifs recognized by regulatory proteins that control gene expression,\ndemonstrating the biological plausibility of the generated sequences.", "AI": {"tldr": "Ctrl-DNA\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5177\u6709\u53ef\u63a7\u7ec6\u80de\u7c7b\u578b\u7279\u5f02\u6027\u7684\u8c03\u63a7DNA\u5e8f\u5217\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8bbe\u8ba1\u7cbe\u786e\u7684\u7ec6\u80de\u7c7b\u578b\u7279\u5f02\u6027\u8c03\u63a7DNA\u5e8f\u5217\u5bf9\u5408\u6210\u751f\u7269\u5b66\u3001\u57fa\u56e0\u6cbb\u7597\u548c\u7cbe\u51c6\u533b\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u53ef\u9760\u5730\u751f\u6210\u65b0\u5e8f\u5217\u3002", "method": "\u901a\u8fc7\u5c06\u8c03\u63a7\u5e8f\u5217\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u751f\u7269\u4fe1\u606f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u81ea\u56de\u5f52\u57fa\u56e0\u7ec4\u8bed\u8a00\u6a21\u578b\uff0c\u8fed\u4ee3\u751f\u6210\u9ad8\u6d3b\u6027\u5e8f\u5217\u3002", "result": "\u5728\u4eba\u7c7b\u542f\u52a8\u5b50\u548c\u589e\u5f3a\u5b50\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCtrl-DNA\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u9002\u5e94\u6027\u5e8f\u5217\u5e76\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u7ec6\u80de\u7c7b\u578b\u7279\u5f02\u6027\u3002", "conclusion": "Ctrl-DNA\u751f\u6210\u7684\u5e8f\u5217\u6355\u83b7\u4e86\u5173\u952e\u8f6c\u5f55\u56e0\u5b50\u7ed3\u5408\u4f4d\u70b9\uff0c\u8bc1\u660e\u4e86\u5176\u751f\u7269\u5408\u7406\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20889", "pdf": "https://arxiv.org/pdf/2505.20889", "abs": "https://arxiv.org/abs/2505.20889", "authors": ["Leizhen Wang", "Peibo Duan", "Cheng Lyu", "Zhenliang Ma"], "title": "Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment", "categories": ["cs.AI"], "comment": null, "summary": "Modern navigation systems and shared mobility platforms increasingly rely on\npersonalized route recommendations to improve individual travel experience and\noperational efficiency. However, a key question remains: can such sequential,\npersonalized routing decisions collectively lead to system-optimal (SO) traffic\nassignment? This paper addresses this question by proposing a learning-based\nframework that reformulates the static SO traffic assignment problem as a\nsingle-agent deep reinforcement learning (RL) task. A central agent\nsequentially recommends routes to travelers as origin-destination (OD) demands\narrive, to minimize total system travel time. To enhance learning efficiency\nand solution quality, we develop an MSA-guided deep Q-learning algorithm that\nintegrates the iterative structure of traditional traffic assignment methods\ninto the RL training process. The proposed approach is evaluated on both the\nBraess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent\nconverges to the theoretical SO solution in the Braess network and achieves\nonly a 0.35% deviation in the OW network. Further ablation studies demonstrate\nthat the route action set's design significantly impacts convergence speed and\nfinal performance, with SO-informed route sets leading to faster learning and\nbetter outcomes. This work provides a theoretically grounded and practically\nrelevant approach to bridging individual routing behavior with system-level\nefficiency through learning-based sequential assignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5c06\u9759\u6001\u7cfb\u7edf\u6700\u4f18\u4ea4\u901a\u5206\u914d\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u901a\u8fc7\u63a8\u8350\u4e2a\u6027\u5316\u8def\u7ebf\u4ee5\u6700\u5c0f\u5316\u603b\u7cfb\u7edf\u65c5\u884c\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u4e2a\u6027\u5316\u8def\u7ebf\u63a8\u8350\u662f\u5426\u80fd\u5b9e\u73b0\u7cfb\u7edf\u6700\u4f18\u4ea4\u901a\u5206\u914d\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u4ea4\u901a\u6548\u7387\u3002", "method": "\u91c7\u7528MSA\u5f15\u5bfc\u7684\u6df1\u5ea6Q\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u4ea4\u901a\u5206\u914d\u65b9\u6cd5\u7684\u8fed\u4ee3\u7ed3\u6784\u3002", "result": "\u5728Braess\u7f51\u7edc\u4e2d\u6536\u655b\u5230\u7406\u8bba\u6700\u4f18\u89e3\uff0c\u5728OW\u7f51\u7edc\u4e2d\u4ec5\u504f\u79bb0.35%\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u578b\u987a\u5e8f\u5206\u914d\uff0c\u5c06\u4e2a\u4f53\u8def\u7531\u884c\u4e3a\u4e0e\u7cfb\u7edf\u6548\u7387\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u548c\u5b9e\u8df5\u76f8\u5173\u7684\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.20496", "pdf": "https://arxiv.org/pdf/2505.20496", "abs": "https://arxiv.org/abs/2505.20496", "authors": ["Asif Shahriar", "Rifat Shahriyar", "M Saifur Rahman"], "title": "Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages", "categories": ["cs.CL"], "comment": null, "summary": "Conventional transformer models typically compress the information from all\ntokens in a sequence into a single \\texttt{[CLS]} token to represent global\ncontext-- an approach that can lead to information loss in tasks requiring\nlocalized or hierarchical cues. In this work, we introduce \\textit{Inceptive\nTransformer}, a modular and lightweight architecture that enriches\ntransformer-based token representations by integrating a multi-scale feature\nextraction module inspired by inception networks. Our model is designed to\nbalance local and global dependencies by dynamically weighting tokens based on\ntheir relevance to a particular task. Evaluation across a diverse range of\ntasks including emotion recognition (both English and Bangla), irony detection,\ndisease identification, and anti-COVID vaccine tweets classification shows that\nour models consistently outperform the baselines by 1\\% to 14\\% while\nmaintaining efficiency. These findings highlight the versatility and\ncross-lingual applicability of our method for enriching transformer-based\nrepresentations across diverse domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInceptive Transformer\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u589e\u5f3aTransformer\u7684\u8868\u793a\u80fd\u529b\uff0c\u52a8\u6001\u6743\u8861\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u5c06\u6240\u6709token\u4fe1\u606f\u538b\u7f29\u5230\u5355\u4e2a[CLS] token\u4e2d\uff0c\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5c40\u90e8\u6216\u5c42\u6b21\u5316\u4fe1\u606f\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u5f15\u5165Inceptive Transformer\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u52a8\u6001\u52a0\u6743token\u4ee5\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u3002", "result": "\u5728\u60c5\u611f\u8bc6\u522b\u3001\u53cd\u8bbd\u68c0\u6d4b\u3001\u75be\u75c5\u8bc6\u522b\u548c\u75ab\u82d7\u63a8\u6587\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u63d0\u53471%\u81f314%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u548c\u591a\u9886\u57df\u4efb\u52a1\u4e2d\u5177\u6709\u901a\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002", "relevance": 85.0}}
{"id": "2505.20639", "pdf": "https://arxiv.org/pdf/2505.20639", "abs": "https://arxiv.org/abs/2505.20639", "authors": ["Guiping Cao", "Tao Wang", "Wenjian Huang", "Xiangyuan Lan", "Jianguo Zhang", "Dongmei Jiang"], "title": "Open-Det: An Efficient Learning Framework for Open-Ended Detection", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Open-Ended object Detection (OED) is a novel and challenging task that\ndetects objects and generates their category names in a free-form manner,\nwithout requiring additional vocabularies during inference. However, the\nexisting OED models, such as GenerateU, require large-scale datasets for\ntraining, suffer from slow convergence, and exhibit limited performance. To\naddress these issues, we present a novel and efficient Open-Det framework,\nconsisting of four collaborative parts. Specifically, Open-Det accelerates\nmodel training in both the bounding box and object name generation process by\nreconstructing the Object Detector and the Object Name Generator. To bridge the\nsemantic gap between Vision and Language modalities, we propose a\nVision-Language Aligner with V-to-L and L-to-V alignment mechanisms,\nincorporating with the Prompts Distiller to transfer knowledge from the VLM\ninto VL-prompts, enabling accurate object name generation for the LLM. In\naddition, we design a Masked Alignment Loss to eliminate contradictory\nsupervision and introduce a Joint Loss to enhance classification, resulting in\nmore efficient training. Compared to GenerateU, Open-Det, using only 1.5% of\nthe training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs.\n149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher\nperformance (+1.0% in APr). The source codes are available at:\nhttps://github.com/Med-Process/Open-Det.", "AI": {"tldr": "Open-Det\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u76ee\u6807\u68c0\u6d4b\u5668\u548c\u540d\u79f0\u751f\u6210\u5668\u3001\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u673a\u5236\u548c\u63d0\u793a\u84b8\u998f\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u6536\u655b\u6162\u4e14\u6027\u80fd\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faOpen-Det\u6846\u67b6\uff0c\u5305\u62ec\u76ee\u6807\u68c0\u6d4b\u5668\u548c\u540d\u79f0\u751f\u6210\u5668\u7684\u91cd\u6784\u3001\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u673a\u5236\u3001\u63d0\u793a\u84b8\u998f\u5668\u548c\u8054\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u4ec5\u75281.5%\u7684\u6570\u636e\u548c20.8%\u7684\u8bad\u7ec3\u5468\u671f\uff0c\u6027\u80fd\u63d0\u53471.0%\uff08APr\uff09\u3002", "conclusion": "Open-Det\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.20579", "pdf": "https://arxiv.org/pdf/2505.20579", "abs": "https://arxiv.org/abs/2505.20579", "authors": ["Dane Malenfant", "Blake A. Richards"], "title": "The challenge of hidden gifts in multi-agent reinforcement learning", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "Sometimes we benefit from actions that others have taken even when we are\nunaware that they took those actions. For example, if your neighbor chooses not\nto take a parking spot in front of your house when you are not there, you can\nbenefit, even without being aware that they took this action. These \"hidden\ngifts\" represent an interesting challenge for multi-agent reinforcement\nlearning (MARL), since assigning credit when the beneficial actions of others\nare hidden is non-trivial. Here, we study the impact of hidden gifts with a\nvery simple MARL task. In this task, agents in a grid-world environment have\nindividual doors to unlock in order to obtain individual rewards. As well, if\nall the agents unlock their door the group receives a larger collective reward.\nHowever, there is only one key for all of the doors, such that the collective\nreward can only be obtained when the agents drop the key for others after they\nuse it. Notably, there is nothing to indicate to an agent that the other agents\nhave dropped the key, thus the act of dropping the key for others is a \"hidden\ngift\". We show that several different state-of-the-art RL algorithms, including\nMARL algorithms, fail to learn how to obtain the collective reward in this\nsimple task. Interestingly, we find that independent model-free policy gradient\nagents can solve the task when we provide them with information about their own\naction history, but MARL agents still cannot solve the task with action\nhistory. Finally, we derive a correction term for these independent agents,\ninspired by learning aware approaches, which reduces the variance in learning\nand helps them to converge to collective success more reliably. These results\nshow that credit assignment in multi-agent settings can be particularly\nchallenging in the presence of \"hidden gifts\", and demonstrate that learning\nawareness in independent agents can benefit these settings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u201c\u9690\u85cf\u793c\u7269\u201d\u73b0\u8c61\u5bf9\u4fe1\u7528\u5206\u914d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u683c\u4e16\u754c\u4efb\u52a1\u5c55\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u5f53\u667a\u80fd\u4f53\u7684\u6709\u76ca\u884c\u4e3a\u5bf9\u5176\u4ed6\u667a\u80fd\u4f53\u4e0d\u53ef\u89c1\u65f6\uff08\u201c\u9690\u85cf\u793c\u7269\u201d\uff09\uff0c\u5982\u4f55\u6709\u6548\u5206\u914d\u4fe1\u7528\uff0c\u8fd9\u5bf9MARL\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u683c\u4e16\u754c\u4efb\u52a1\uff0c\u667a\u80fd\u4f53\u9700\u5171\u4eab\u94a5\u5319\u89e3\u9501\u95e8\u4ee5\u83b7\u53d6\u96c6\u4f53\u5956\u52b1\uff0c\u4f46\u94a5\u5319\u7684\u4f20\u9012\u884c\u4e3a\u5bf9\u5176\u4ed6\u667a\u80fd\u4f53\u4e0d\u53ef\u89c1\u3002\u6d4b\u8bd5\u4e86\u591a\u79cdRL\u548cMARL\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b66\u4e60\u611f\u77e5\u7684\u4fee\u6b63\u9879\u3002", "result": "\u73b0\u6709MARL\u7b97\u6cd5\u65e0\u6cd5\u5b8c\u6210\u4efb\u52a1\uff0c\u800c\u72ec\u7acb\u7684\u6a21\u578b\u65e0\u5173\u7b56\u7565\u68af\u5ea6\u667a\u80fd\u4f53\u5728\u83b7\u5f97\u81ea\u8eab\u884c\u4e3a\u5386\u53f2\u4fe1\u606f\u540e\u53ef\u4ee5\u89e3\u51b3\u4efb\u52a1\u3002\u4fee\u6b63\u9879\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5176\u6536\u655b\u7a33\u5b9a\u6027\u3002", "conclusion": "\u201c\u9690\u85cf\u793c\u7269\u201d\u73b0\u8c61\u589e\u52a0\u4e86MARL\u4e2d\u4fe1\u7528\u5206\u914d\u7684\u96be\u5ea6\uff0c\u5b66\u4e60\u611f\u77e5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u72ec\u7acb\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.20948", "pdf": "https://arxiv.org/pdf/2505.20948", "abs": "https://arxiv.org/abs/2505.20948", "authors": ["Yisen Gao", "Jiaxin Bai", "Tianshi Zheng", "Qingyun Sun", "Ziwei Zhang", "Jianxin Li", "Yangqiu Song", "Xingcheng Fu"], "title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs", "categories": ["cs.AI"], "comment": "Under Review", "summary": "Abductive reasoning in knowledge graphs aims to generate plausible logical\nhypotheses from observed entities, with broad applications in areas such as\nclinical diagnosis and scientific discovery. However, due to a lack of\ncontrollability, a single observation may yield numerous plausible but\nredundant or irrelevant hypotheses on large-scale knowledge graphs. To address\nthis limitation, we introduce the task of controllable hypothesis generation to\nimprove the practical utility of abductive reasoning. This task faces two key\nchallenges when controlling for generating long and complex logical hypotheses:\nhypothesis space collapse and hypothesis oversensitivity. To address these\nchallenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation\nframework for abductive reasoning over knowledge graphs, trained in a two-stage\nparadigm including supervised learning and subsequent reinforcement learning.\nTo mitigate hypothesis space collapse, we design a dataset augmentation\nstrategy based on sub-logical decomposition, enabling the model to learn\ncomplex logical structures by leveraging semantic patterns in simpler\ncomponents. To address hypothesis oversensitivity, we incorporate smoothed\nsemantic rewards including Dice and Overlap scores, and introduce a\ncondition-adherence reward to guide the generation toward user-specified\ncontrol constraints. Extensive experiments on three benchmark datasets\ndemonstrate that our model not only better adheres to control conditions but\nalso achieves superior semantic similarity performance compared to baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u5047\u8bbe\u751f\u6210\u6846\u67b6CtrlHGen\uff0c\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u6eaf\u56e0\u63a8\u7406\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u89e3\u51b3\u5047\u8bbe\u7a7a\u95f4\u584c\u7f29\u548c\u5047\u8bbe\u8fc7\u654f\u611f\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u6eaf\u56e0\u63a8\u7406\u4e2d\u7f3a\u4e4f\u53ef\u63a7\u6027\u5bfc\u81f4\u7684\u5197\u4f59\u6216\u65e0\u5173\u5047\u8bbe\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faCtrlHGen\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5b66\u4e60+\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u5b50\u903b\u8f91\u5206\u89e3\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u8bed\u4e49\u5956\u52b1\u673a\u5236\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCtrlHGen\u5728\u63a7\u5236\u6761\u4ef6\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "CtrlHGen\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u63a7\u5047\u8bbe\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6eaf\u56e0\u63a8\u7406\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20500", "pdf": "https://arxiv.org/pdf/2505.20500", "abs": "https://arxiv.org/abs/2505.20500", "authors": ["Naba Rizvi", "Harper Strickland", "Saleha Ahmedi", "Aekta Kallepalli", "Isha Khirwadkar", "William Wu", "Imani N. S. Munyaka", "Nedjma Ousidhoum"], "title": "Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in decision-making tasks\nlike r\\'esum\\'e screening and content moderation, giving them the power to\namplify or suppress certain perspectives. While previous research has\nidentified disability-related biases in LLMs, little is known about how they\nconceptualize ableism or detect it in text. We evaluate the ability of four\nLLMs to identify nuanced ableism directed at autistic individuals. We examine\nthe gap between their understanding of relevant terminology and their\neffectiveness in recognizing ableist content in context. Our results reveal\nthat LLMs can identify autism-related language but often miss harmful or\noffensive connotations. Further, we conduct a qualitative comparison of human\nand LLM explanations. We find that LLMs tend to rely on surface-level keyword\nmatching, leading to context misinterpretations, in contrast to human\nannotators who consider context, speaker identity, and potential impact. On the\nother hand, both LLMs and humans agree on the annotation scheme, suggesting\nthat a binary classification is adequate for evaluating LLM performance, which\nis consistent with findings from prior studies involving human annotators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc6\u522b\u9488\u5bf9\u81ea\u95ed\u75c7\u60a3\u8005\u7684\u9690\u6027\u6b67\u89c6\uff08ableism\uff09\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u867d\u7136\u80fd\u8bc6\u522b\u76f8\u5173\u672f\u8bed\uff0c\u4f46\u5e38\u5ffd\u7565\u6709\u5bb3\u6216\u5192\u72af\u6027\u542b\u4e49\u3002", "motivation": "LLMs\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u9690\u6027\u6b67\u89c6\u7684\u7406\u89e3\u548c\u8bc6\u522b\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u81ea\u95ed\u75c7\u60a3\u8005\u7684\u6b67\u89c6\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cdLLMs\u5728\u8bc6\u522b\u9690\u6027\u6b67\u89c6\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4eba\u7c7b\u548cLLMs\u7684\u89e3\u91ca\u65b9\u5f0f\u3002", "result": "LLMs\u4f9d\u8d56\u5173\u952e\u8bcd\u5339\u914d\uff0c\u5e38\u8bef\u89e3\u4e0a\u4e0b\u6587\uff0c\u800c\u4eba\u7c7b\u5219\u8003\u8651\u66f4\u591a\u56e0\u7d20\u3002\u4f46\u4e24\u8005\u5728\u5206\u7c7b\u65b9\u6848\u4e0a\u4e00\u81f4\u3002", "conclusion": "LLMs\u5728\u8bc6\u522b\u9690\u6027\u6b67\u89c6\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u6539\u8fdb\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "relevance": 75.0}}
{"id": "2505.20640", "pdf": "https://arxiv.org/pdf/2505.20640", "abs": "https://arxiv.org/abs/2505.20640", "authors": ["Yifan Li", "Yuhang Chen", "Anh Dao", "Lichi Li", "Zhongyi Cai", "Zhen Tan", "Tianlong Chen", "Yu Kong"], "title": "IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios", "categories": ["cs.CV"], "comment": "v1.0", "summary": "Existing Embodied Question Answering (EQA) benchmarks primarily focus on\nhousehold environments, often overlooking safety-critical aspects and reasoning\nprocesses pertinent to industrial settings. This drawback limits the evaluation\nof agent readiness for real-world industrial applications. To bridge this, we\nintroduce IndustryEQA, the first benchmark dedicated to evaluating embodied\nagent capabilities within safety-critical warehouse scenarios. Built upon the\nNVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory\nvideos featuring diverse industrial assets, dynamic human agents, and carefully\ndesigned hazardous situations inspired by real-world safety guidelines. The\nbenchmark includes rich annotations covering six categories: equipment safety,\nhuman safety, object recognition, attribute recognition, temporal\nunderstanding, and spatial understanding. Besides, it also provides extra\nreasoning evaluation based on these categories. Specifically, it comprises 971\nquestion-answer pairs generated from small warehouse and 373 pairs from large\nones, incorporating scenarios with and without human. We further propose a\ncomprehensive evaluation framework, including various baseline models, to\nassess their general perception and reasoning abilities in industrial\nenvironments. IndustryEQA aims to steer EQA research towards developing more\nrobust, safety-aware, and practically applicable embodied agents for complex\nindustrial environments. Benchmark and codes are available.", "AI": {"tldr": "IndustryEQA\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u5de5\u4e1a\u73af\u5883\u4e2d\u5177\u8eab\u4ee3\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709EQA\u57fa\u51c6\u5728\u5de5\u4e1a\u5b89\u5168\u6027\u548c\u63a8\u7406\u8fc7\u7a0b\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709EQA\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5bb6\u5ead\u73af\u5883\uff0c\u5ffd\u89c6\u4e86\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5173\u952e\u6027\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eNVIDIA Isaac Sim\u5e73\u53f0\uff0cIndustryEQA\u63d0\u4f9b\u9ad8\u4fdd\u771f\u89c6\u9891\u548c\u4e30\u5bcc\u6ce8\u91ca\uff0c\u6db5\u76d6\u516d\u7c7b\u5de5\u4e1a\u573a\u666f\uff0c\u5e76\u751f\u62101344\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "IndustryEQA\u65e8\u5728\u63a8\u52a8EQA\u7814\u7a76\uff0c\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u5b89\u5168\u4e14\u5b9e\u7528\u7684\u5de5\u4e1a\u5177\u8eab\u4ee3\u7406\u3002", "relevance": 40.0}}
{"id": "2505.20589", "pdf": "https://arxiv.org/pdf/2505.20589", "abs": "https://arxiv.org/abs/2505.20589", "authors": ["Mahdi Pourmirzaei", "Farzaneh Esmaili", "Salhuldin Alqarghuli", "Mohammadreza Pourmirzaei", "Ye Han", "Kai Chen", "Mohsen Rezaei", "Duolin Wang", "Dong Xu"], "title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "The diverse nature of protein prediction tasks has traditionally necessitated\nspecialized models, hindering the development of broadly applicable and\ncomputationally efficient Protein Language Models (PLMs). In this work, we\nintroduce Prot2Token, a unified framework that overcomes these challenges by\nconverting a wide spectrum of protein-related predictions, from sequence-level\nproperties and residue-specific attributes to complex inter-protein\ninteractions, into a standardized next-token prediction format. At its core,\nProt2Token employs an autoregressive decoder, conditioned on embeddings from\npre-trained protein encoders and guided by learnable task tokens, to perform\ndiverse predictions. This architecture uniquely facilitates multi-task\nlearning, enabling a single model to master numerous tasks with improved\nefficiency. We present extensive experimental validation across a variety of\nbenchmarks, demonstrating Prot2Tokens strong predictive power in different\ntypes of protein-prediction tasks. Key results include significant speedups\n(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or\nexceeding specialized approaches. Beyond that, we introduce an auxiliary\nself-supervised decoder pre-training approach to improve spatially sensitive\ntask performance. Prot2Token thus offers a significant step towards a\nversatile, high-throughput paradigm for protein modeling, promising to\naccelerate biological discovery and the development of novel therapeutics. The\ncode is available at https://github.com/mahdip72/prot2token .", "AI": {"tldr": "Prot2Token\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u86cb\u767d\u8d28\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u79cd\u86cb\u767d\u8d28\u76f8\u5173\u4efb\u52a1\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u683c\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u86cb\u767d\u8d28\u9884\u6d4b\u4efb\u52a1\u9700\u8981\u4e13\u7528\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u9ad8\u6548\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u81ea\u56de\u5f52\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u7f16\u7801\u5668\u7684\u5d4c\u5165\u548c\u53ef\u5b66\u4e60\u4efb\u52a1token\uff0c\u8fdb\u884c\u591a\u4efb\u52a1\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff08\u5982\u6bd4AlphaFold2\u5feb1000\u500d\uff09\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7\u4e13\u7528\u65b9\u6cd5\u3002", "conclusion": "Prot2Token\u4e3a\u86cb\u767d\u8d28\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u3001\u9ad8\u901a\u91cf\u7684\u8303\u5f0f\uff0c\u6709\u671b\u52a0\u901f\u751f\u7269\u53d1\u73b0\u548c\u65b0\u7597\u6cd5\u5f00\u53d1\u3002", "relevance": 40.0}}
{"id": "2505.21045", "pdf": "https://arxiv.org/pdf/2505.21045", "abs": "https://arxiv.org/abs/2505.21045", "authors": ["Lingyi Cai", "Ruichen Zhang", "Changyuan Zhao", "Yu Zhang", "Jiawen Kang", "Dusit Niyato", "Tao Jiang", "Xuemin Shen"], "title": "Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking", "categories": ["cs.AI"], "comment": "7 pages, 5 figures", "summary": "Low-Altitude Economic Networking (LAENet) aims to support diverse flying\napplications below 1,000 meters by deploying various aerial vehicles for\nflexible and cost-effective aerial networking. However, complex\ndecision-making, resource constraints, and environmental uncertainty pose\nsignificant challenges to the development of the LAENet. Reinforcement learning\n(RL) offers a potential solution in response to these challenges but has\nlimitations in generalization, reward design, and model stability. The\nemergence of large language models (LLMs) offers new opportunities for RL to\nmitigate these limitations. In this paper, we first present a tutorial about\nintegrating LLMs into RL by using the capacities of generation, contextual\nunderstanding, and structured reasoning of LLMs. We then propose an\nLLM-enhanced RL framework for the LAENet in terms of serving the LLM as\ninformation processor, reward designer, decision-maker, and generator.\nMoreover, we conduct a case study by using LLMs to design a reward function to\nimprove the learning performance of RL in the LAENet. Finally, we provide a\nconclusion and discuss future work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\uff08LAENet\uff09\u4e2d\u7684\u51b3\u7b56\u6311\u6218\u3002", "motivation": "LAENet\u9762\u4e34\u590d\u6742\u51b3\u7b56\u3001\u8d44\u6e90\u9650\u5236\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u5956\u52b1\u8bbe\u8ba1\u548c\u6a21\u578b\u7a33\u5b9a\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u800cLLM\u4e3aRL\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aLLM\u589e\u5f3a\u7684RL\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u5176\u4f5c\u4e3a\u4fe1\u606f\u5904\u7406\u5668\u3001\u5956\u52b1\u8bbe\u8ba1\u5668\u3001\u51b3\u7b56\u751f\u6210\u5668\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86LLM\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u5982\u4f55\u63d0\u5347RL\u5728LAENet\u4e2d\u7684\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "LLM\u4e0eRL\u7684\u7ed3\u5408\u4e3aLAENet\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.20501", "pdf": "https://arxiv.org/pdf/2505.20501", "abs": "https://arxiv.org/abs/2505.20501", "authors": ["Rohan Balasubramanian", "Nitish Gokulakrishnan", "Syeda Jannatus Saba", "Steven Skiena"], "title": "Gatsby Without the 'E': Crafting Lipograms with LLMs", "categories": ["cs.CL"], "comment": "7.5 pages", "summary": "Lipograms are a unique form of constrained writing where all occurrences of a\nparticular letter are excluded from the text, typified by the novel Gadsby,\nwhich daringly avoids all usage of the letter 'e'. In this study, we explore\nthe power of modern large language models (LLMs) by transforming the novel F.\nScott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented\nwith a range of techniques, from baseline methods like synonym replacement to\nsophisticated generative models enhanced with beam search and named entity\nanalysis. We show that excluding up to 3.6% of the most common letters (up to\nthe letter 'u') had minimal impact on the text's meaning, although translation\nfidelity rapidly and predictably decays with stronger lipogram constraints. Our\nwork highlights the surprising flexibility of English under strict constraints,\nrevealing just how adaptable and creative language can be.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c06\u300a\u4e86\u4e0d\u8d77\u7684\u76d6\u8328\u6bd4\u300b\u8f6c\u6362\u4e3a\u65e0\u5b57\u6bcd'e'\u6587\u672c\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u82f1\u8bed\u5728\u4e25\u683c\u7ea6\u675f\u4e0b\u7684\u7075\u6d3b\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5LLMs\u5728\u4e25\u683c\u7ea6\u675f\uff08\u5982\u907f\u5b57\u6587\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u8bed\u8a00\u7684\u9002\u5e94\u6027\u548c\u521b\u9020\u6027\u3002", "method": "\u91c7\u7528\u4e86\u4ece\u540c\u4e49\u8bcd\u66ff\u6362\u5230\u751f\u6210\u6a21\u578b\uff08\u7ed3\u5408\u675f\u641c\u7d22\u548c\u547d\u540d\u5b9e\u4f53\u5206\u6790\uff09\u7b49\u591a\u79cd\u6280\u672f\u3002", "result": "\u6392\u9664\u5e38\u89c1\u5b57\u6bcd\uff08\u6700\u591a3.6%\uff09\u5bf9\u6587\u672c\u610f\u4e49\u5f71\u54cd\u8f83\u5c0f\uff0c\u4f46\u66f4\u5f3a\u7684\u7ea6\u675f\u4f1a\u5bfc\u81f4\u7ffb\u8bd1\u4fdd\u771f\u5ea6\u5feb\u901f\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u82f1\u8bed\u5728\u4e25\u683c\u7ea6\u675f\u4e0b\u7684\u60ca\u4eba\u7075\u6d3b\u6027\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u7684\u9002\u5e94\u6027\u548c\u521b\u9020\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20641", "pdf": "https://arxiv.org/pdf/2505.20641", "abs": "https://arxiv.org/abs/2505.20641", "authors": ["Yuan Wu", "Zhiqiang Yan", "Yigong Zhang", "Xiang Li", "ian Yang"], "title": "See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Occupancy prediction aims to estimate the 3D spatial distribution of occupied\nregions along with their corresponding semantic labels. Existing vision-based\nmethods perform well on daytime benchmarks but struggle in nighttime scenarios\ndue to limited visibility and challenging lighting conditions. To address these\nchallenges, we propose \\textbf{LIAR}, a novel framework that learns\nillumination-affined representations. LIAR first introduces Selective Low-light\nImage Enhancement (SLLIE), which leverages the illumination priors from daytime\nscenes to adaptively determine whether a nighttime image is genuinely dark or\nsufficiently well-lit, enabling more targeted global enhancement. Building on\nthe illumination maps generated by SLLIE, LIAR further incorporates two\nillumination-aware components: 2D Illumination-guided Sampling (2D-IGS) and 3D\nIllumination-driven Projection (3D-IDP), to respectively tackle local\nunderexposure and overexposure. Specifically, 2D-IGS modulates feature sampling\npositions according to illumination maps, assigning larger offsets to darker\nregions and smaller ones to brighter regions, thereby alleviating feature\ndegradation in underexposed areas. Subsequently, 3D-IDP enhances semantic\nunderstanding in overexposed regions by constructing illumination intensity\nfields and supplying refined residual queries to the BEV context refinement\nprocess. Extensive experiments on both real and synthetic datasets demonstrate\nthe superior performance of LIAR under challenging nighttime scenarios. The\nsource code and pretrained models are available\n\\href{https://github.com/yanzq95/LIAR}{here}.", "AI": {"tldr": "LIAR\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u548c\u5149\u7167\u611f\u77e5\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u591c\u95f4\u573a\u666f\u4e0b\u76843D\u5360\u7528\u9884\u6d4b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u767d\u5929\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591c\u95f4\u56e0\u5149\u7167\u6761\u4ef6\u5dee\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u9009\u62e9\u6027\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08SLLIE\uff09\u548c\u4e24\u4e2a\u5149\u7167\u611f\u77e5\u7ec4\u4ef6\uff082D-IGS\u548c3D-IDP\uff09\uff0c\u5206\u522b\u89e3\u51b3\u5c40\u90e8\u66dd\u5149\u4e0d\u8db3\u548c\u8fc7\u5ea6\u66dd\u5149\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0cLIAR\u5728\u591c\u95f4\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LIAR\u901a\u8fc7\u5149\u7167\u611f\u77e5\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u591c\u95f43D\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.20621", "pdf": "https://arxiv.org/pdf/2505.20621", "abs": "https://arxiv.org/abs/2505.20621", "authors": ["Shijie Liu", "Andrew C. Cullen", "Paul Montague", "Sarah Erfani", "Benjamin I. P. Rubinstein"], "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Similar to other machine learning frameworks, Offline Reinforcement Learning\n(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on\nexternally sourced datasets, a vulnerability that is exacerbated by its\nsequential nature. To mitigate the risks posed by RL poisoning, we extend\ncertified defenses to provide larger guarantees against adversarial\nmanipulation, ensuring robustness for both per-state actions, and the overall\nexpected cumulative reward. Our approach leverages properties of Differential\nPrivacy, in a manner that allows this work to span both continuous and discrete\nspaces, as well as stochastic and deterministic environments -- significantly\nexpanding the scope and applicability of achievable guarantees. Empirical\nevaluations demonstrate that our approach ensures the performance drops to no\nmore than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly\nimproving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while\nproducing certified radii that is $5$ times larger as well. This highlights the\npotential of our framework to enhance safety and reliability in offline RL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6bd2\u653b\u51fb\u7684\u8ba4\u8bc1\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u6280\u672f\u786e\u4fdd\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9632\u5fa1\u6548\u679c\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u96c6\uff0c\u6613\u53d7\u4e2d\u6bd2\u653b\u51fb\uff0c\u5176\u987a\u5e8f\u6027\u52a0\u5267\u4e86\u98ce\u9669\u3002\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u66f4\u5f3a\u7684\u9632\u5fa1\u4fdd\u8bc1\u3002", "method": "\u5229\u7528\u5dee\u5206\u9690\u79c1\u7279\u6027\uff0c\u6269\u5c55\u8ba4\u8bc1\u9632\u5fa1\u8303\u56f4\uff0c\u8986\u76d6\u8fde\u7eed\u548c\u79bb\u6563\u7a7a\u95f4\u3001\u968f\u673a\u548c\u786e\u5b9a\u6027\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u57287%\u6570\u636e\u4e2d\u6bd2\u65f6\u6027\u80fd\u4e0b\u964d\u4e0d\u8d85\u8fc750%\uff0c\u4f18\u4e8e\u4e4b\u524d0.008%\u7684\u7ed3\u679c\uff0c\u8ba4\u8bc1\u534a\u5f84\u6269\u59275\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "relevance": 70.0}}
{"id": "2505.21055", "pdf": "https://arxiv.org/pdf/2505.21055", "abs": "https://arxiv.org/abs/2505.21055", "authors": ["Kaiming Liu", "Xuanyu Lei", "Ziyue Wang", "Peng Li", "Yang Liu"], "title": "Agent-Environment Alignment via Automated Interface Generation", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) agents have shown impressive reasoning\ncapabilities in interactive decision-making tasks. These agents interact with\nenvironment through intermediate interfaces, such as predefined action spaces\nand interaction rules, which mediate the perception and action. However,\nmismatches often happen between the internal expectations of the agent\nregarding the influence of its issued actions and the actual state transitions\nin the environment, a phenomenon referred to as \\textbf{agent-environment\nmisalignment}. While prior work has invested substantially in improving agent\nstrategies and environment design, the critical role of the interface still\nremains underexplored. In this work, we empirically demonstrate that\nagent-environment misalignment poses a significant bottleneck to agent\nperformance. To mitigate this issue, we propose \\textbf{ALIGN}, an\n\\underline{A}uto-A\\underline{l}igned \\underline{I}nterface\n\\underline{G}e\\underline{n}eration framework that alleviates the misalignment\nby enriching the interface. Specifically, the ALIGN-generated interface\nenhances both the static information of the environment and the step-wise\nobservations returned to the agent. Implemented as a lightweight wrapper, this\ninterface achieves the alignment without modifying either the agent logic or\nthe environment code. Experiments across multiple domains including embodied\ntasks, web navigation and tool-use, show consistent performance improvements,\nwith up to a 45.67\\% success rate improvement observed in ALFWorld. Meanwhile,\nALIGN-generated interface can generalize across different agent architectures\nand LLM backbones without interface regeneration. Code and experimental results\nare available at https://github.com/THUNLP-MT/ALIGN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faALIGN\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5bf9\u9f50\u63a5\u53e3\u89e3\u51b3LLM\u4ee3\u7406\u4e0e\u73af\u5883\u95f4\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u9519\u4f4d\uff08agent-environment misalignment\uff09\u662f\u5f71\u54cdLLM\u4ee3\u7406\u6027\u80fd\u7684\u5173\u952e\u74f6\u9888\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4ee3\u7406\u7b56\u7565\u548c\u73af\u5883\u8bbe\u8ba1\uff0c\u63a5\u53e3\u7684\u4f5c\u7528\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faALIGN\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u63a5\u53e3\u4e30\u5bcc\u73af\u5883\u9759\u6001\u4fe1\u606f\u548c\u9010\u6b65\u89c2\u5bdf\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7406\u903b\u8f91\u6216\u73af\u5883\u4ee3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u5177\u8eab\u4efb\u52a1\u3001\u7f51\u9875\u5bfc\u822a\u548c\u5de5\u5177\u4f7f\u7528\uff09\u4e2d\uff0cALIGN\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cALFWorld\u4e2d\u6210\u529f\u7387\u6700\u9ad8\u63d0\u534745.67%\u3002", "conclusion": "ALIGN\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u4ee3\u7406\u67b6\u6784\u548cLLM\u9aa8\u5e72\u6a21\u578b\u3002", "relevance": 85.0}}
{"id": "2505.20505", "pdf": "https://arxiv.org/pdf/2505.20505", "abs": "https://arxiv.org/abs/2505.20505", "authors": ["Md Mahadi Hassan", "John Salvador", "Akond Rahman", "Santu Karmaker"], "title": "Large Language Models for IT Automation Tasks: Are We There Yet?", "categories": ["cs.CL", "cs.SE"], "comment": "8 pages", "summary": "LLMs show promise in code generation, yet their effectiveness for IT\nautomation tasks, particularly for tools like Ansible, remains understudied.\nExisting benchmarks rely primarily on synthetic tasks that fail to capture the\nneeds of practitioners who use IT automation tools, such as Ansible. We present\nITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,\nconfiguring servers, managing files) where each task accounts for state\nreconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'\nability to generate functional Ansible automation scripts via dynamic execution\nin controlled environments. We evaluate 14 open-source LLMs, none of which\naccomplish pass@10 at a rate beyond 12%. To explain these low scores, we\nanalyze 1,411 execution failures across the evaluated LLMs and identify two\nmain categories of prevalent semantic errors: failures in state reconciliation\nrelated reasoning (44.87% combined from variable (11.43%), host (11.84%),\npath(11.63%), and template (9.97%) issues) and deficiencies in module-specific\nexecution knowledge (24.37% combined from Attribute and parameter (14.44%) and\nmodule (9.93%) errors). Our findings reveal key limitations in open-source\nLLMs' ability to track state changes and apply specialized module knowledge,\nindicating that reliable IT automation will require major advances in state\nreasoning and domain-specific execution understanding.", "AI": {"tldr": "ITAB\u662f\u4e00\u4e2a\u9488\u5bf9IT\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5f00\u6e90LLM\u5728\u751f\u6210\u529f\u80fd\u6027Ansible\u811a\u672c\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6210\u529f\u7387\u4f4e\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u72b6\u6001\u63a8\u7406\u548c\u6a21\u5757\u77e5\u8bc6\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76LLM\u5728IT\u81ea\u52a8\u5316\uff08\u5982Ansible\uff09\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faITAB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b126\u4e2a\u591a\u6837\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u52a8\u6001\u6267\u884c\u8bc4\u4f3014\u4e2a\u5f00\u6e90LLM\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5931\u8d25\u6848\u4f8b\u3002", "result": "LLM\u5728pass@10\u4e2d\u7684\u6210\u529f\u7387\u4f4e\u4e8e12%\uff0c\u4e3b\u8981\u9519\u8bef\u4e3a\u72b6\u6001\u63a8\u7406\u76f8\u5173\uff0844.87%\uff09\u548c\u6a21\u5757\u77e5\u8bc6\u4e0d\u8db3\uff0824.37%\uff09\u3002", "conclusion": "\u5f00\u6e90LLM\u5728IT\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63d0\u5347\u72b6\u6001\u63a8\u7406\u548c\u9886\u57df\u77e5\u8bc6\u3002", "relevance": 70.0}}
{"id": "2505.20644", "pdf": "https://arxiv.org/pdf/2505.20644", "abs": "https://arxiv.org/abs/2505.20644", "authors": ["Haoyu Zhang", "Yisen Feng", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "HCQA-1.5 @ Ego4D EgoSchema Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The third-place solution for the Ego4D EgoSchema Challenge at the\n  CVPR EgoVis Workshop 2025", "summary": "In this report, we present the method that achieves third place for Ego4D\nEgoSchema Challenge in CVPR 2025. To improve the reliability of answer\nprediction in egocentric video question answering, we propose an effective\nextension to the previously proposed HCQA framework. Our approach introduces a\nmulti-source aggregation strategy to generate diverse predictions, followed by\na confidence-based filtering mechanism that selects high-confidence answers\ndirectly. For low-confidence cases, we incorporate a fine-grained reasoning\nmodule that performs additional visual and contextual analysis to refine the\npredictions. Evaluated on the EgoSchema blind test set, our method achieves 77%\naccuracy on over 5,000 human-curated multiple-choice questions, outperforming\nlast year's winning solution and the majority of participating teams. Our code\nwill be added at https://github.com/Hyu-Zhang/HCQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684HCQA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6e90\u805a\u5408\u548c\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u673a\u5236\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5728EgoSchema\u76f2\u6d4b\u96c6\u4e0a\u53d6\u5f9777%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u63d0\u5347\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7b54\u6848\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "method": "\u6269\u5c55HCQA\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6e90\u805a\u5408\u7b56\u7565\u751f\u6210\u591a\u6837\u5316\u9884\u6d4b\uff0c\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u673a\u5236\u9009\u62e9\u9ad8\u7f6e\u4fe1\u5ea6\u7b54\u6848\uff0c\u5e76\u5bf9\u4f4e\u7f6e\u4fe1\u5ea6\u60c5\u51b5\u4f7f\u7528\u7ec6\u7c92\u5ea6\u63a8\u7406\u6a21\u5757\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728EgoSchema\u76f2\u6d4b\u96c6\u4e0a\u8fbe\u523077%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u53bb\u5e74\u7684\u83b7\u80dc\u65b9\u6848\u548c\u591a\u6570\u53c2\u8d5b\u56e2\u961f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "relevance": 40.0}}
{"id": "2505.20628", "pdf": "https://arxiv.org/pdf/2505.20628", "abs": "https://arxiv.org/abs/2505.20628", "authors": ["Juan Ramirez", "Meraj Hashemizadeh", "Simon Lacoste-Julien"], "title": "Position: Adopt Constraints Over Penalties in Deep Learning", "categories": ["cs.LG", "math.OC"], "comment": "Code available at\n  https://github.com/merajhashemi/constraints-vs-penalties", "summary": "Recent efforts toward developing trustworthy AI systems with accountability\nguarantees have led to a growing reliance on machine learning formulations that\nincorporate external requirements, or constraints. These requirements are often\nenforced through penalization--adding fixed-weight terms to the task loss. We\nargue that this approach is ill-suited, and that tailored constrained\noptimization methods should be adopted instead. In particular, no penalty\ncoefficient may yield a solution that both satisfies the constraints and\nachieves good performance--i.e., one solving the constrained problem. Moreover,\ntuning these coefficients is costly, incurring significant time and\ncomputational overhead. In contrast, tailored constrained methods--such as the\nLagrangian approach, which optimizes the penalization \"coefficients\" (the\nLagrange multipliers) alongside the model--(i) truly solve the constrained\nproblem and add accountability, (ii) eliminate the need for extensive penalty\ntuning, and (iii) integrate seamlessly with modern deep learning pipelines.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5728\u53ef\u4fe1AI\u7cfb\u7edf\u4e2d\u91c7\u7528\u5b9a\u5236\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff09\u800c\u975e\u56fa\u5b9a\u60e9\u7f5a\u9879\uff0c\u4ee5\u63d0\u9ad8\u7ea6\u675f\u6ee1\u8db3\u6027\u548c\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8c03\u53c2\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u53ef\u4fe1AI\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u60e9\u7f5a\u9879\u6765\u6ee1\u8db3\u5916\u90e8\u7ea6\u675f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u7ea6\u675f\u548c\u6027\u80fd\u8981\u6c42\uff0c\u4e14\u8c03\u53c2\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u91c7\u7528\u5b9a\u5236\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff09\uff0c\u52a8\u6001\u4f18\u5316\u60e9\u7f5a\u7cfb\u6570\uff08\u62c9\u683c\u6717\u65e5\u4e58\u6570\uff09\u4ee5\u76f4\u63a5\u89e3\u51b3\u7ea6\u675f\u95ee\u9898\u3002", "result": "\u5b9a\u5236\u65b9\u6cd5\u80fd\u771f\u6b63\u89e3\u51b3\u7ea6\u675f\u95ee\u9898\u3001\u51cf\u5c11\u8c03\u53c2\u9700\u6c42\uff0c\u5e76\u4e0e\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "\u5b9a\u5236\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\u66f4\u9002\u5408\u53ef\u4fe1AI\u7cfb\u7edf\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u95ee\u8d23\u6027\u548c\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.21067", "pdf": "https://arxiv.org/pdf/2505.21067", "abs": "https://arxiv.org/abs/2505.21067", "authors": ["Xiao Hu", "Xingyu Lu", "Liyuan Mao", "YiFan Zhang", "Tianke Zhang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou"], "title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has played an important role in improving the\nreasoning ability of large language models (LLMs). Some studies apply RL\ndirectly to \\textit{smaller} base models (known as zero-RL) and also achieve\nnotable progress. However, in this paper, we show that using only 920 examples,\na simple distillation method based on the base model can clearly outperform\nzero-RL, which typically requires much more data and computational cost. By\nanalyzing the token frequency in model outputs, we find that the distilled\nmodel shows more flexible reasoning. It uses anthropomorphic tokens and logical\nconnectors much more often than the zero-RL model. Further analysis reveals\nthat distillation enhances the presence of two advanced cognitive behaviors:\nMulti-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent\noccurrences of these two advanced cognitive behaviors give rise to flexible\nreasoning, which is essential for solving complex reasoning problems, while\nzero-RL fails to significantly boost the frequency of these behaviors.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u4ec5\u9700920\u4e2a\u6837\u672c\u5373\u53ef\u8d85\u8d8a\u9700\u8981\u66f4\u591a\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u7684\u96f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u907f\u514d\u96f6\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u7b80\u5355\u84b8\u998f\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u6837\u672c\uff08920\u4e2a\uff09\uff0c\u5206\u6790\u6a21\u578b\u8f93\u51fa\u7684token\u9891\u7387\u3002", "result": "\u84b8\u998f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u7075\u6d3b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u7528\u66f4\u591a\u62df\u4eba\u5316token\u548c\u903b\u8f91\u8fde\u63a5\u8bcd\uff0c\u5e76\u589e\u5f3a\u4e24\u79cd\u9ad8\u7ea7\u8ba4\u77e5\u884c\u4e3a\u3002", "conclusion": "\u84b8\u998f\u65b9\u6cd5\u5728\u63d0\u5347\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u96f6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "relevance": 85.0}}
{"id": "2505.20506", "pdf": "https://arxiv.org/pdf/2505.20506", "abs": "https://arxiv.org/abs/2505.20506", "authors": ["Hawau Olamide Toyin", "Rufael Marew", "Humaid Alblooshi", "Samar M. Magdy", "Hanan Aldarmaki"], "title": "ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025 The dataset is available at\n  https://huggingface.co/datasets/MBZUAI/ArVoice", "summary": "We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech\ncorpus with diacritized transcriptions, intended for multi-speaker speech\nsynthesis, and can be useful for other tasks such as speech-based diacritic\nrestoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a\nnew professionally recorded set from six voice talents with diverse\ndemographics, (2) a modified subset of the Arabic Speech Corpus; and (3)\nhigh-quality synthetic speech from two commercial systems. The complete corpus\nconsists of a total of 83.52 hours of speech across 11 voices; around 10 hours\nconsist of human voices from 7 speakers. We train three open-source TTS and two\nvoice conversion systems to illustrate the use cases of the dataset. The corpus\nis available for research use.", "AI": {"tldr": "ArVoice\u662f\u4e00\u4e2a\u591a\u8bf4\u8bdd\u8005\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u5305\u542b\u5e26\u97f3\u6807\u7684\u8f6c\u5f55\uff0c\u9002\u7528\u4e8e\u8bed\u97f3\u5408\u6210\u7b49\u4efb\u52a1\u3002", "motivation": "\u4e3a\u591a\u8bf4\u8bdd\u8005\u8bed\u97f3\u5408\u6210\u53ca\u5176\u4ed6\u4efb\u52a1\uff08\u5982\u97f3\u6807\u6062\u590d\u3001\u8bed\u97f3\u8f6c\u6362\u548c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff09\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u6570\u636e\u3002", "method": "\u7ed3\u5408\u4e13\u4e1a\u5f55\u97f3\u3001\u4fee\u6539\u7684\u963f\u62c9\u4f2f\u8bed\u97f3\u8bed\u6599\u5e93\u5b50\u96c6\u548c\u5546\u4e1a\u7cfb\u7edf\u5408\u6210\u8bed\u97f3\uff0c\u6784\u5efa83.52\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\u96c6\u3002", "result": "\u8bad\u7ec3\u4e86\u4e09\u4e2a\u5f00\u6e90TTS\u548c\u4e24\u4e2a\u8bed\u97f3\u8f6c\u6362\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ArVoice\u8bed\u6599\u5e93\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u6570\u636e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "relevance": 20.0}}
{"id": "2505.20649", "pdf": "https://arxiv.org/pdf/2505.20649", "abs": "https://arxiv.org/abs/2505.20649", "authors": ["HsiaoYuan Hsu", "Yuxin Peng"], "title": "Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design", "categories": ["cs.CV"], "comment": "Accepted to IJCAI 2025 (AI, Arts and Creativity). Project page is at\n  https://thekinsley.github.io/Scan-and-Print/", "summary": "In AI-empowered poster design, content-aware layout generation is crucial for\nthe on-image arrangement of visual-textual elements, e.g., logo, text, and\nunderlay. To perceive the background images, existing work demanded a high\nparameter count that far exceeds the size of available training data, which has\nimpeded the model's real-time performance and generalization ability. To\naddress these challenges, we proposed a patch-level data summarization and\naugmentation approach, vividly named Scan-and-Print. Specifically, the scan\nprocedure selects only the patches suitable for placing element vertices to\nperform fine-grained perception efficiently. Then, the print procedure mixes up\nthe patches and vertices across two image-layout pairs to synthesize over 100%\nnew samples in each epoch while preserving their plausibility. Besides, to\nfacilitate the vertex-level operations, a vertex-based layout representation is\nintroduced. Extensive experimental results on widely used benchmarks\ndemonstrated that Scan-and-Print can generate visually appealing layouts with\nstate-of-the-art quality while dramatically reducing computational bottleneck\nby 95.2%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aScan-and-Print\u7684\u8865\u4e01\u7ea7\u6570\u636e\u603b\u7ed3\u548c\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8eAI\u9a71\u52a8\u7684\u6d77\u62a5\u8bbe\u8ba1\u4e2d\u7684\u5185\u5bb9\u611f\u77e5\u5e03\u5c40\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u74f6\u9888\u5e76\u63d0\u5347\u4e86\u5e03\u5c40\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u80cc\u666f\u56fe\u50cf\u611f\u77e5\u4e2d\u9700\u8981\u5927\u91cf\u53c2\u6570\uff0c\u5bfc\u81f4\u5b9e\u65f6\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "Scan-and-Print\u65b9\u6cd5\u901a\u8fc7\u626b\u63cf\u9009\u62e9\u9002\u5408\u653e\u7f6e\u5143\u7d20\u9876\u70b9\u7684\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u6253\u5370\u5408\u6210\u65b0\u6837\u672c\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u9876\u70b9\u7684\u5e03\u5c40\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5e03\u5c40\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u74f6\u9888\u964d\u4f4e\u4e8695.2%\u3002", "conclusion": "Scan-and-Print\u65b9\u6cd5\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u5185\u5bb9\u611f\u77e5\u5e03\u5c40\u751f\u6210\u4efb\u52a1\u3002", "relevance": 30.0}}
{"id": "2505.20634", "pdf": "https://arxiv.org/pdf/2505.20634", "abs": "https://arxiv.org/abs/2505.20634", "authors": ["Ruiqi Lyu", "Alistair Turcan", "Bryan Wilder"], "title": "Explaining Concept Shift with Interpretable Feature Attribution", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Regardless the amount of data a machine learning (ML) model is trained on,\nthere will inevitably be data that differs from their training set, lowering\nmodel performance. Concept shift occurs when the distribution of labels\nconditioned on the features changes, making even a well-tuned ML model to have\nlearned a fundamentally incorrect representation. Identifying these shifted\nfeatures provides unique insight into how one dataset differs from another,\nconsidering the difference may be across a scientifically relevant dimension,\nsuch as time, disease status, population, etc. In this paper, we propose\nSGShift, a model for detecting concept shift in tabular data and attributing\nreduced model performance to a sparse set of shifted features. SGShift models\nconcept shift with a Generalized Additive Model (GAM) and performs subsequent\nfeature selection to identify shifted features. We propose further extensions\nof SGShift by incorporating knockoffs to control false discoveries and an\nabsorption term to account for models with poor fit to the data. We conduct\nextensive experiments in synthetic and real data across various ML models and\nfind SGShift can identify shifted features with AUC $>0.9$ and recall $>90\\%$,\noften 2 or 3 times as high as baseline methods.", "AI": {"tldr": "SGShift\u662f\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u8868\u683c\u6570\u636e\u4e2d\u6982\u5ff5\u6f02\u79fb\u5e76\u5f52\u56e0\u4e8e\u7a00\u758f\u7279\u5f81\u96c6\u7684\u6a21\u578b\uff0c\u901a\u8fc7GAM\u5efa\u6a21\u548c\u7279\u5f81\u9009\u62e9\u5b9e\u73b0\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u7684\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u6982\u5ff5\u6f02\u79fb\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u9519\u8bef\u7684\u8868\u793a\uff0c\u8bc6\u522b\u8fd9\u4e9b\u7279\u5f81\u6709\u52a9\u4e8e\u7406\u89e3\u6570\u636e\u96c6\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5e7f\u4e49\u52a0\u6027\u6a21\u578b\uff08GAM\uff09\u5efa\u6a21\u6982\u5ff5\u6f02\u79fb\uff0c\u7ed3\u5408\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u5f15\u5165knockoffs\u548c\u5438\u6536\u9879\u4ee5\u63a7\u5236\u5047\u53d1\u73b0\u548c\u6a21\u578b\u62df\u5408\u4e0d\u8db3\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\uff0cSGShift\u7684AUC>0.9\uff0c\u53ec\u56de\u7387>90%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SGShift\u80fd\u6709\u6548\u8bc6\u522b\u6982\u5ff5\u6f02\u79fb\u7279\u5f81\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u63d0\u4f9b\u89e3\u91ca\u3002", "relevance": 60.0}}
{"id": "2505.21106", "pdf": "https://arxiv.org/pdf/2505.21106", "abs": "https://arxiv.org/abs/2505.21106", "authors": ["Zhengyang Ji", "Yifan Jia", "Shang Gao", "Yutao Yue"], "title": "Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Large Vision Language Models (LVLMs) have achieved remarkable progress in\nmultimodal tasks, yet they also exhibit notable social biases. These biases\noften manifest as unintended associations between neutral concepts and\nsensitive human attributes, leading to disparate model behaviors across\ndemographic groups. While existing studies primarily focus on detecting and\nquantifying such biases, they offer limited insight into the underlying\nmechanisms within the models. To address this gap, we propose an explanatory\nframework that combines information flow analysis with multi-round dialogue\nevaluation, aiming to understand the origin of social bias from the perspective\nof imbalanced internal information utilization. Specifically, we first identify\nhigh-contribution image tokens involved in the model's reasoning process for\nneutral questions via information flow analysis. Then, we design a multi-turn\ndialogue mechanism to evaluate the extent to which these key tokens encode\nsensitive information. Extensive experiments reveal that LVLMs exhibit\nsystematic disparities in information usage when processing images of different\ndemographic groups, suggesting that social bias is deeply rooted in the model's\ninternal reasoning dynamics. Furthermore, we complement our findings from a\ntextual modality perspective, showing that the model's semantic representations\nalready display biased proximity patterns, thereby offering a cross-modal\nexplanation of bias formation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u606f\u6d41\u5206\u6790\u548c\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\uff0c\u4ee5\u7406\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u793e\u4f1a\u504f\u89c1\u7684\u8d77\u6e90\u3002\u7814\u7a76\u53d1\u73b0\uff0cLVLMs\u5728\u5904\u7406\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u56fe\u50cf\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u4fe1\u606f\u5229\u7528\u5dee\u5f02\uff0c\u4e14\u6587\u672c\u6a21\u6001\u7684\u8bed\u4e49\u8868\u793a\u4e5f\u663e\u793a\u51fa\u504f\u89c1\u6a21\u5f0f\u3002", "motivation": "LVLMs\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u68c0\u6d4b\u548c\u91cf\u5316\u504f\u89c1\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5185\u90e8\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u8bc6\u522b\u9ad8\u8d21\u732e\u56fe\u50cf\u6807\u8bb0\uff0c\u5e76\u8bbe\u8ba1\u591a\u8f6e\u5bf9\u8bdd\u673a\u5236\u8bc4\u4f30\u8fd9\u4e9b\u6807\u8bb0\u5bf9\u654f\u611f\u4fe1\u606f\u7684\u7f16\u7801\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLVLMs\u5728\u5904\u7406\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u56fe\u50cf\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u4fe1\u606f\u5229\u7528\u5dee\u5f02\uff0c\u4e14\u6587\u672c\u6a21\u6001\u7684\u8bed\u4e49\u8868\u793a\u4e5f\u663e\u793a\u51fa\u504f\u89c1\u6a21\u5f0f\u3002", "conclusion": "\u793e\u4f1a\u504f\u89c1\u6839\u690d\u4e8e\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u52a8\u6001\uff0c\u4e14\u8de8\u6a21\u6001\u7684\u8bed\u4e49\u8868\u793a\u8fdb\u4e00\u6b65\u89e3\u91ca\u4e86\u504f\u89c1\u7684\u5f62\u6210\u3002", "relevance": 75.0}}
{"id": "2505.20511", "pdf": "https://arxiv.org/pdf/2505.20511", "abs": "https://arxiv.org/abs/2505.20511", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yang Liu", "Pengxu Zhu", "Yun Xue", "Ziwei Gong", "Julia Hirschberg", "Bolei Ma"], "title": "Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects", "categories": ["cs.CL"], "comment": null, "summary": "While text-based emotion recognition methods have achieved notable success,\nreal-world dialogue systems often demand a more nuanced emotional understanding\nthan any single modality can offer. Multimodal Emotion Recognition in\nConversations (MERC) has thus emerged as a crucial direction for enhancing the\nnaturalness and emotional understanding of human-computer interaction. Its goal\nis to accurately recognize emotions by integrating information from various\nmodalities such as text, speech, and visual signals.\n  This survey offers a systematic overview of MERC, including its motivations,\ncore tasks, representative methods, and evaluation strategies. We further\nexamine recent trends, highlight key challenges, and outline future directions.\nAs interest in emotionally intelligent systems grows, this survey provides\ntimely guidance for advancing MERC research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u591a\u6a21\u6001\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08MERC\uff09\u7684\u7cfb\u7edf\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u5176\u52a8\u673a\u3001\u6838\u5fc3\u4efb\u52a1\u3001\u65b9\u6cd5\u3001\u8bc4\u4f30\u7b56\u7565\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u60c5\u611f\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u89c9\u7b49\u591a\u6a21\u6001\u4fe1\u606f\u6765\u51c6\u786e\u8bc6\u522b\u60c5\u611f\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff0\u4e86MERC\u7684\u6838\u5fc3\u4efb\u52a1\u3001\u4ee3\u8868\u6027\u65b9\u6cd5\u548c\u8bc4\u4f30\u7b56\u7565\u3002", "result": "\u603b\u7ed3\u4e86MERC\u7684\u6700\u65b0\u8d8b\u52bf\u3001\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aMERC\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ca\u65f6\u6307\u5bfc\u3002", "relevance": 40.0}}
{"id": "2505.20653", "pdf": "https://arxiv.org/pdf/2505.20653", "abs": "https://arxiv.org/abs/2505.20653", "authors": ["Lingyu Qiu", "Ke Jiang", "Xiaoyang Tan"], "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICME2025", "summary": "Recent advancements in domain generalization for deepfake detection have\nattracted significant attention, with previous methods often incorporating\nadditional modules to prevent overfitting to domain-specific patterns. However,\nsuch regularization can hinder the optimization of the empirical risk\nminimization (ERM) objective, ultimately degrading model performance. In this\npaper, we propose a novel learning objective that aligns generalization\ngradient updates with ERM gradient updates. The key innovation is the\napplication of perturbations to model parameters, aligning the ascending points\nacross domains, which specifically enhances the robustness of deepfake\ndetection models to domain shifts. This approach effectively preserves\ndomain-invariant features while managing domain-specific characteristics,\nwithout introducing additional regularization. Experimental results on multiple\nchallenging deepfake detection datasets demonstrate that our gradient alignment\nstrategy outperforms state-of-the-art domain generalization techniques,\nconfirming the efficacy of our method. The code is available at\nhttps://github.com/Lynn0925/RoGA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u68af\u5ea6\u5bf9\u9f50\u7b56\u7565\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6b63\u5219\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u989d\u5916\u6a21\u5757\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u4f46\u53ef\u80fd\u963b\u788d\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u76ee\u6807\u7684\u4f18\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u6270\u52a8\u6a21\u578b\u53c2\u6570\uff0c\u5bf9\u9f50\u4e0d\u540c\u9886\u57df\u7684\u68af\u5ea6\u66f4\u65b0\u65b9\u5411\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u9886\u57df\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9886\u57df\u6cdb\u5316\u6280\u672f\u3002", "conclusion": "\u68af\u5ea6\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6b63\u5219\u5316\u3002", "relevance": 30.0}}
{"id": "2505.20643", "pdf": "https://arxiv.org/pdf/2505.20643", "abs": "https://arxiv.org/abs/2505.20643", "authors": ["Bo Pan", "Liang Zhao"], "title": "Can Past Experience Accelerate LLM Reasoning?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Allocating more compute to large language models (LLMs) reasoning has\ngenerally been demonstrated to improve their effectiveness, but also results in\nincreased inference time. In contrast, humans can perform tasks faster and\nbetter with increased experience and exposure. Hence, this paper aims to\ninvestigate the question: Can LLMs also become faster at reasoning through\nrecurrent exposure on relevant tasks, and if so, how can it be achieved? To\naddress these questions, we first formalize the problem setting of LLM\nreasoning speedup systematically in the dimensions of task relevancy and\ncompute budget calculation. We then propose SpeedupLLM, a theoretically\nguaranteed framework to implement and benchmark such reasoning speedup\nbehaviour based on adaptive compute allocation and memory mechanisms. We\nfurther conduct comprehensive experiments to benchmark such behaviour across\ndifferent question similarity levels, memory methods, and reasoning methods.\nResults show that LLMs can generally reason faster with past experience,\nachieving up to a 56% reduction in compute cost when equipped with appropriate\nmemory and reasoning methods.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u53cd\u590d\u63a5\u89e6\u76f8\u5173\u4efb\u52a1\u6765\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86SpeedupLLM\u6846\u67b6\uff0c\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u53ef\u51cf\u5c1156%\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u7ecf\u9a8c\u53ef\u4ee5\u66f4\u5feb\u66f4\u597d\u5730\u5b8c\u6210\u4efb\u52a1\uff0c\u800cLLMs\u901a\u5e38\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u4ee5\u63d0\u9ad8\u6548\u679c\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u662f\u5426\u4e5f\u80fd\u901a\u8fc7\u53cd\u590d\u63a5\u89e6\u4efb\u52a1\u6765\u52a0\u901f\u63a8\u7406\u3002", "method": "\u63d0\u51faSpeedupLLM\u6846\u67b6\uff0c\u57fa\u4e8e\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\u548c\u5185\u5b58\u673a\u5236\uff0c\u7cfb\u7edf\u5316\u5730\u5b9e\u73b0\u548c\u8bc4\u4f30\u63a8\u7406\u52a0\u901f\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728\u9002\u5f53\u7684\u5185\u5b58\u548c\u63a8\u7406\u65b9\u6cd5\u4e0b\uff0c\u63a8\u7406\u901f\u5ea6\u53ef\u63d0\u5347\uff0c\u8ba1\u7b97\u6210\u672c\u6700\u9ad8\u51cf\u5c1156%\u3002", "conclusion": "LLMs\u53ef\u4ee5\u901a\u8fc7\u53cd\u590d\u63a5\u89e6\u4efb\u52a1\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\uff0cSpeedupLLM\u6846\u67b6\u4e3a\u8fd9\u4e00\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u73b0\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.21212", "pdf": "https://arxiv.org/pdf/2505.21212", "abs": "https://arxiv.org/abs/2505.21212", "authors": ["Martin C. Cooper", "Imane Bousdira", "Cl\u00e9ment Carbonnel"], "title": "Interpretable DNFs", "categories": ["cs.AI", "68T27, 05C62", "F.4.1; I.2.6"], "comment": null, "summary": "A classifier is considered interpretable if each of its decisions has an\nexplanation which is small enough to be easily understood by a human user. A\nDNF formula can be seen as a binary classifier $\\kappa$ over boolean domains.\nThe size of an explanation of a positive decision taken by a DNF $\\kappa$ is\nbounded by the size of the terms in $\\kappa$, since we can explain a positive\ndecision by giving a term of $\\kappa$ that evaluates to true. Since both\npositive and negative decisions must be explained, we consider that\ninterpretable DNFs are those $\\kappa$ for which both $\\kappa$ and\n$\\overline{\\kappa}$ can be expressed as DNFs composed of terms of bounded size.\nIn this paper, we study the family of $k$-DNFs whose complements can also be\nexpressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision\ntrees and nested $k$-DNFs, a novel family of models. Experiments indicate that\nnested $k$-DNFs are an interesting alternative to decision trees in terms of\ninterpretability and accuracy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u89e3\u91ca\u7684DNF\u5206\u7c7b\u5668\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5d4c\u5957k-DNF\u6a21\u578b\uff0c\u5e76\u4e0e\u6df1\u5ea6k\u51b3\u7b56\u6811\u8fdb\u884c\u6bd4\u8f83\uff0c\u5b9e\u9a8c\u8868\u660e\u5d4c\u5957k-DNF\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u53ef\u89e3\u91ca\u6027\u5206\u7c7b\u5668\u7684\u8bbe\u8ba1\uff0c\u7279\u522b\u662fDNF\u516c\u5f0f\u5728\u6b63\u8d1f\u51b3\u7b56\u4e2d\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u4ee5\u63d0\u4f9b\u66f4\u6613\u4e8e\u4eba\u7c7b\u7406\u89e3\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u5d4c\u5957k-DNF\u6a21\u578b\uff0c\u5e76\u4e0e\u6df1\u5ea6k\u51b3\u7b56\u6811\u8fdb\u884c\u6bd4\u8f83\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u5176\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5d4c\u5957k-DNF\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6df1\u5ea6k\u51b3\u7b56\u6811\u3002", "conclusion": "\u5d4c\u5957k-DNF\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u53ef\u89e3\u91ca\u6027\u6a21\u578b\uff0c\u53ef\u4f5c\u4e3a\u51b3\u7b56\u6811\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.20538", "pdf": "https://arxiv.org/pdf/2505.20538", "abs": "https://arxiv.org/abs/2505.20538", "authors": ["Sebastian Antony Joseph", "Syed Murtaza Husain", "Stella S. R. Offner", "St\u00e9phanie Juneau", "Paul Torrey", "Adam S. Bolton", "Juan P. Farias", "Niall Gaffney", "Greg Durrett", "Junyi Jessy Li"], "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy", "categories": ["cs.CL", "astro-ph.IM", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology.", "AI": {"tldr": "AstroVisBench\u662f\u9996\u4e2a\u9488\u5bf9\u5929\u6587\u5b66\u9886\u57df\u79d1\u5b66\u8ba1\u7b97\u548c\u53ef\u89c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLMs\u5728\u6570\u636e\u5904\u7406\u548c\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5929\u6587\u5b66\u7814\u7a76\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u751f\u6210\u6b63\u786e\u79d1\u5b66\u89c1\u89e3\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5f15\u5165AstroVisBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408LLM-as-a-judge\u5de5\u4f5c\u6d41\u9a8c\u8bc1\u6a21\u578b\u5728\u5929\u6587\u5b66\u6570\u636e\u5904\u7406\u548c\u53ef\u89c6\u5316\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u5148\u8fdbLLMs\u5728\u5929\u6587\u5b66\u7814\u7a76\u4e2d\u7684\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "AstroVisBench\u4e3aAI\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u7684\u5f00\u53d1\u3002", "relevance": 85.0}}
{"id": "2505.20655", "pdf": "https://arxiv.org/pdf/2505.20655", "abs": "https://arxiv.org/abs/2505.20655", "authors": ["Lujian Yao", "Siming Zheng", "Xinbin Yuan", "Zhuoxuan Cai", "Pu Wu", "Jinwei Chen", "Bo Li", "Peng-Tao Jiang"], "title": "Photography Perspective Composition: Towards Aesthetic Perspective Recommendation", "categories": ["cs.CV"], "comment": null, "summary": "Traditional photography composition approaches are dominated by 2D\ncropping-based methods. However, these methods fall short when scenes contain\npoorly arranged subjects. Professional photographers often employ perspective\nadjustment as a form of 3D recomposition, modifying the projected 2D\nrelationships between subjects while maintaining their actual spatial positions\nto achieve better compositional balance. Inspired by this artistic practice, we\npropose photography perspective composition (PPC), extending beyond traditional\ncropping-based methods. However, implementing the PPC faces significant\nchallenges: the scarcity of perspective transformation datasets and undefined\nassessment criteria for perspective quality. To address these challenges, we\npresent three key contributions: (1) An automated framework for building PPC\ndatasets through expert photographs. (2) A video generation approach that\ndemonstrates the transformation process from suboptimal to optimal\nperspectives. (3) A perspective quality assessment (PQA) model constructed\nbased on human performance. Our approach is concise and requires no additional\nprompt instructions or camera trajectories, helping and guiding ordinary users\nto enhance their composition skills.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6444\u5f71\u89c6\u89d2\u6784\u56fe\uff08PPC\uff09\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4f20\u7edf\u76842D\u88c1\u526a\u65b9\u6cd5\uff0c\u901a\u8fc73D\u900f\u89c6\u8c03\u6574\u4f18\u5316\u6784\u56fe\u3002\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u8bc4\u4f30\u6807\u51c6\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u3001\u89c6\u9891\u751f\u6210\u548c\u89c6\u89d2\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf2D\u88c1\u526a\u65b9\u6cd5\u5728\u573a\u666f\u4e2d\u4e3b\u4f53\u6392\u5217\u4e0d\u4f73\u65f6\u6548\u679c\u6709\u9650\uff0c\u4e13\u4e1a\u6444\u5f71\u5e08\u5e38\u75283D\u900f\u89c6\u8c03\u6574\u4f18\u5316\u6784\u56fe\u3002\u8bba\u6587\u53d7\u6b64\u542f\u53d1\uff0c\u63d0\u51faPPC\u65b9\u6cd5\uff0c\u4f46\u9762\u4e34\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\u7f3a\u5931\u7684\u6311\u6218\u3002", "method": "1. \u81ea\u52a8\u5316\u6784\u5efaPPC\u6570\u636e\u96c6\uff1b2. \u89c6\u9891\u751f\u6210\u5c55\u793a\u4ece\u6b21\u4f18\u5230\u6700\u4f18\u89c6\u89d2\u7684\u8f6c\u6362\u8fc7\u7a0b\uff1b3. \u57fa\u4e8e\u4eba\u7c7b\u8868\u73b0\u7684\u89c6\u89d2\u8d28\u91cf\u8bc4\u4f30\uff08PQA\uff09\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u6d01\uff0c\u65e0\u9700\u989d\u5916\u63d0\u793a\u6216\u76f8\u673a\u8f68\u8ff9\uff0c\u5e2e\u52a9\u666e\u901a\u7528\u6237\u63d0\u5347\u6784\u56fe\u6280\u80fd\u3002", "conclusion": "PPC\u65b9\u6cd5\u901a\u8fc73D\u900f\u89c6\u8c03\u6574\u4f18\u5316\u6784\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "relevance": 20.0}}
{"id": "2505.20646", "pdf": "https://arxiv.org/pdf/2505.20646", "abs": "https://arxiv.org/abs/2505.20646", "authors": ["Eduardo Y. Sakabe", "Felipe S. Abrah\u00e3o", "Alexandre Sim\u00f5es", "Esther Colombini", "Paula Costa", "Ricardo Gudwin", "Hector Zenil"], "title": "Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "68T07, 68Q30, 68Q32", "I.2.6; F.1.1; F.1.3"], "comment": "10 pages total, 1 figure. Submitted to NeurIPS 2025", "summary": "Understanding and controlling the informational complexity of neural networks\nis a central challenge in machine learning, with implications for\ngeneralization, optimization, and model capacity. While most approaches rely on\nentropy-based loss functions and statistical metrics, these measures often fail\nto capture deeper, causally relevant algorithmic regularities embedded in\nnetwork structure. We propose a shift toward algorithmic information theory,\nusing Binarized Neural Networks (BNNs) as a first proxy. Grounded in\nalgorithmic probability (AP) and the universal distribution it defines, our\napproach characterizes learning dynamics through a formal, causally grounded\nlens. We apply the Block Decomposition Method (BDM) -- a scalable approximation\nof algorithmic complexity based on AP -- and demonstrate that it more closely\ntracks structural changes during training than entropy, consistently exhibiting\nstronger correlations with training loss across varying model sizes and\nrandomized training runs. These results support the view of training as a\nprocess of algorithmic compression, where learning corresponds to the\nprogressive internalization of structured regularities. In doing so, our work\noffers a principled estimate of learning progression and suggests a framework\nfor complexity-aware learning and regularization, grounded in first principles\nfrom information theory, complexity, and computability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u6cd5\u4fe1\u606f\u7406\u8bba\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\uff08BNNs\uff09\u548c\u5757\u5206\u89e3\u65b9\u6cd5\uff08BDM\uff09\u6765\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u7b97\u6cd5\u590d\u6742\u6027\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u5316\u89c4\u5f8b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u71b5\u7684\u5ea6\u91cf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u56e0\u679c\u76f8\u5173\u7b97\u6cd5\u89c4\u5f8b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u672c\u8d28\u7684\u590d\u6742\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7b97\u6cd5\u6982\u7387\uff08AP\uff09\u548c\u5757\u5206\u89e3\u65b9\u6cd5\uff08BDM\uff09\u6765\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u7b97\u6cd5\u590d\u6742\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4e0e\u8bad\u7ec3\u635f\u5931\u7684\u76f8\u5173\u6027\u3002", "result": "BDM\u6bd4\u71b5\u66f4\u80fd\u51c6\u786e\u53cd\u6620\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d8\u5316\uff0c\u4e14\u4e0e\u8bad\u7ec3\u635f\u5931\u7684\u76f8\u5173\u6027\u66f4\u5f3a\u3002", "conclusion": "\u8bad\u7ec3\u53ef\u89c6\u4e3a\u7b97\u6cd5\u538b\u7f29\u8fc7\u7a0b\uff0cBDM\u4e3a\u590d\u6742\u6027\u611f\u77e5\u5b66\u4e60\u548c\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "relevance": 70.0}}
{"id": "2505.21279", "pdf": "https://arxiv.org/pdf/2505.21279", "abs": "https://arxiv.org/abs/2505.21279", "authors": ["Shaoqing Zhang", "Kehai Chen", "Zhuosheng Zhang", "Rumei Li", "Rongxiang Weng", "Yang Xiang", "Liqiang Nie", "Min Zhang"], "title": "XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in vision-language models (VLMs) have spurred increased\ninterest in Device-Control Agents (DC agents), such as utilizing in-the-wild\ndevice control to manage graphical user interfaces. Conventional methods for\nassessing the capabilities of DC agents, such as computing step-wise action\naccuracy and overall task success rates, provide a macroscopic view of DC\nagents' performance; however, they fail to offer microscopic insights into\npotential errors that may occur in real-world applications. Conducting a\nfiner-grained performance evaluation of DC agents presents significant\nchallenges. This study introduces a new perspective on evaluation methods for\nDC agents by proposing the XBOUND evaluation method, which employs the\ncalculation of a novel Explore Metric to delineate the capability boundaries of\nDC agents. Compared to previous evaluation methods, XBOUND focuses on\nindividual states to assess the proficiency of DC agents in mastering these\nstates. Furthermore, we have developed a ``pseudo'' episode tree dataset\nderived from Android Control test data. Utilizing this dataset and XBOUND, we\ncomprehensively evaluate the OS-Atlas and UI-TARS series, examining both the\noverall and specific performance across five common tasks. Additionally, we\nselect representative cases to highlight the current deficiencies and\nlimitations inherent in both series. Code is available at\nhttps://github.com/sqzhang-lazy/XBOUND.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5XBOUND\uff0c\u7528\u4e8e\u66f4\u7ec6\u7c92\u5ea6\u5730\u8bc4\u4f30\u8bbe\u5907\u63a7\u5236\u4ee3\u7406\uff08DC agents\uff09\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f2a\u60c5\u8282\u6811\u6570\u636e\u96c6\u7528\u4e8e\u6d4b\u8bd5\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9bDC agents\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u5fae\u89c2\u9519\u8bef\u6d1e\u5bdf\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faXBOUND\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u63a2\u7d22\u5ea6\u91cf\u6765\u754c\u5b9aDC agents\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u5e76\u4f7f\u7528\u4f2a\u60c5\u8282\u6811\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5bf9OS-Atlas\u548cUI-TARS\u7cfb\u5217\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u4e94\u4e2a\u5e38\u89c1\u4efb\u52a1\u4e2d\u7684\u6574\u4f53\u548c\u5177\u4f53\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7684\u7f3a\u9677\u548c\u9650\u5236\u3002", "conclusion": "XBOUND\u65b9\u6cd5\u4e3aDC agents\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u6f5c\u5728\u95ee\u9898\u3002", "relevance": 50.0}}
{"id": "2505.20546", "pdf": "https://arxiv.org/pdf/2505.20546", "abs": "https://arxiv.org/abs/2505.20546", "authors": ["Meng Lu", "Ruochen Zhang", "Ellie Pavlick", "Carsten Eickhoff"], "title": "Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit factual\ninconsistencies across languages, with significantly better performance in\nfactual recall tasks in English than in other languages. The causes of these\nfailures, however, remain poorly understood. Using mechanistic analysis\ntechniques, we uncover the underlying pipeline that LLMs employ, which involves\nusing the English-centric factual recall mechanism to process multilingual\nqueries and then translating English answers back into the target language. We\nidentify two primary sources of error: insufficient engagement of the reliable\nEnglish-centric mechanism for factual recall, and incorrect translation from\nEnglish back into the target language for the final answer. To address these\nvulnerabilities, we introduce two vector interventions, both independent of\nlanguages and datasets, to redirect the model toward better internal paths for\nhigher factual consistency. Our interventions combined increase the recall\naccuracy by over 35 percent for the lowest-performing language. Our findings\ndemonstrate how mechanistic insights can be used to unlock latent multilingual\ncapabilities in LLMs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u673a\u5236\u5206\u6790\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e8b\u5b9e\u56de\u5fc6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u5411\u91cf\u5e72\u9884\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u591a\u8bed\u8a00LLMs\u5728\u4e8b\u5b9e\u56de\u5fc6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5c24\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u5176\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u5931\u8d25\u7684\u673a\u5236\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u673a\u5236\u5206\u6790\u6280\u672f\uff0c\u8bc6\u522bLLMs\u5904\u7406\u591a\u8bed\u8a00\u67e5\u8be2\u7684\u6d41\u7a0b\uff08\u82f1\u8bed\u4e2d\u5fc3\u673a\u5236+\u7ffb\u8bd1\u56de\u76ee\u6807\u8bed\u8a00\uff09\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u5411\u91cf\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u5e72\u9884\u65b9\u6cd5\u5c06\u6700\u4f4e\u8868\u73b0\u8bed\u8a00\u7684\u56de\u5fc6\u51c6\u786e\u7387\u63d0\u9ad8\u4e8635%\u4ee5\u4e0a\u3002", "conclusion": "\u673a\u5236\u5206\u6790\u53ef\u4ee5\u89e3\u9501LLMs\u7684\u6f5c\u5728\u591a\u8bed\u8a00\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.20665", "pdf": "https://arxiv.org/pdf/2505.20665", "abs": "https://arxiv.org/abs/2505.20665", "authors": ["Muxi Diao", "Lele Yang", "Hongbo Yin", "Zhexu Wang", "Yejie Wang", "Daxin Tian", "Kongming Liang", "Zhanyu Ma"], "title": "DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous driving requires real-time, robust reasoning across perception,\nprediction, planning, and behavior. However, conventional end-to-end models\nfail to generalize in complex scenarios due to the lack of structured\nreasoning. Recent vision-language models (VLMs) have been applied to driving\ntasks, but they typically rely on isolated modules and static supervision,\nlimiting their ability to support multi-stage decision-making. We present\nAutoDriveRL, a unified training framework that formulates autonomous driving as\na structured reasoning process over four core tasks. Each task is independently\nmodeled as a vision-language question-answering problem and optimized using\ntask-specific reward models, enabling fine-grained reinforcement signals at\ndifferent reasoning stages. Within this framework, we train DriveRX, a\ncross-task reasoning VLM designed for real-time decision-making. DriveRX\nachieves strong performance on a public benchmark, outperforming GPT-4o in\nbehavior reasoning and demonstrating robustness under complex or corrupted\ndriving conditions. Our analysis further highlights the impact of vision\nencoder design and reward-guided reasoning compression. We will release the\nAutoDriveRL framework and the DriveRX model to support future research.", "AI": {"tldr": "AutoDriveRL\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u81ea\u52a8\u9a7e\u9a76\u5efa\u6a21\u4e3a\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\u6a21\u578b\u4f18\u5316\u6bcf\u4e2a\u4efb\u52a1\uff0c\u8bad\u7ec3\u51fa\u5b9e\u65f6\u51b3\u7b56\u7684VLM DriveRX\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7f3a\u4e4f\u591a\u9636\u6bb5\u51b3\u7b56\u652f\u6301\u3002", "method": "\u5c06\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u56db\u4e2a\u89c6\u89c9\u8bed\u8a00\u95ee\u7b54\u95ee\u9898\uff0c\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u6a21\u578b\u4f18\u5316\uff0c\u8bad\u7ec3\u8de8\u4efb\u52a1\u63a8\u7406\u7684VLM DriveRX\u3002", "result": "DriveRX\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8eGPT-4o\uff0c\u5e76\u5728\u590d\u6742\u6216\u635f\u574f\u7684\u9a7e\u9a76\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "AutoDriveRL\u6846\u67b6\u548cDriveRX\u6a21\u578b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.20648", "pdf": "https://arxiv.org/pdf/2505.20648", "abs": "https://arxiv.org/abs/2505.20648", "authors": ["Mengmeng Chen", "Xiaohu Wu", "Qiqi Liu", "Tiantian He", "Yew-Soon Ong", "Yaochu Jin", "Qicheng Lao", "Han Yu"], "title": "Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-objective optimization (MOO) exists extensively in machine learning,\nand aims to find a set of Pareto-optimal solutions, called the Pareto front,\ne.g., it is fundamental for multiple avenues of research in federated learning\n(FL). Pareto-Front Learning (PFL) is a powerful method implemented using\nHypernetworks (PHNs) to approximate the Pareto front. This method enables the\nacquisition of a mapping function from a given preference vector to the\nsolutions on the Pareto front. However, most existing PFL approaches still face\ntwo challenges: (a) sampling rays in high-dimensional spaces; (b) failing to\ncover the entire Pareto Front which has a convex shape. Here, we introduce a\nnovel PFL framework, called as PHN-HVVS, which decomposes the design space into\nVoronoi grids and deploys a genetic algorithm (GA) for Voronoi grid\npartitioning within high-dimensional space. We put forward a new loss function,\nwhich effectively contributes to more extensive coverage of the resultant\nPareto front and maximizes the HV Indicator. Experimental results on multiple\nMOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines\nsignificantly in generating Pareto front. Also, we illustrate that PHN-HVVS\nadvances the methodologies of several recent problems in the FL field. The code\nis available at\nhttps://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Pareto-Front Learning\u6846\u67b6PHN-HVVS\uff0c\u901a\u8fc7Voronoi\u7f51\u683c\u5206\u89e3\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u91c7\u6837\uff0c\u6539\u8fdb\u4e86Pareto\u524d\u6cbf\u7684\u8986\u76d6\u8303\u56f4\u548cHV\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709PFL\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u91c7\u6837\u56f0\u96be\u4ee5\u53ca\u65e0\u6cd5\u8986\u76d6\u6574\u4e2a\u51f8\u5f62Pareto\u524d\u6cbf\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPHN-HVVS\u6846\u67b6\uff0c\u7ed3\u5408Voronoi\u7f51\u683c\u5206\u89e3\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u65b0\u7684\u635f\u5931\u51fd\u6570\u4ee5\u4f18\u5316Pareto\u524d\u6cbf\u8986\u76d6\u3002", "result": "\u5728\u591a\u4e2aMOO\u4efb\u52a1\u4e2d\uff0cPHN-HVVS\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63a8\u52a8\u4e86FL\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "PHN-HVVS\u662f\u4e00\u79cd\u9ad8\u6548\u7684PFL\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.21281", "pdf": "https://arxiv.org/pdf/2505.21281", "abs": "https://arxiv.org/abs/2505.21281", "authors": ["Yue Zhang", "Zhiliang Tian", "Shicheng Zhou", "Haiyang Wang", "Wenqing Hou", "Yuying Liu", "Xuechen Zhao", "Minlie Huang", "Ye Wang", "Bin Zhou"], "title": "RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing\nsemantic-enhanced LJP models integrate judicial precedents and legal knowledge\nfor high performance. But they neglect legal reasoning logic, a critical\ncomponent of legal judgments requiring rigorous logical analysis. Although some\napproaches utilize legal reasoning logic for high-quality predictions, their\nlogic rigidity hinders adaptation to case-specific logical frameworks,\nparticularly in complex cases that are lengthy and detailed. This paper\nproposes a rule-enhanced legal judgment prediction framework based on\nfirst-order logic (FOL) formalism and comparative learning (CL) to develop an\nadaptive adjustment mechanism for legal judgment logic and further enhance\nperformance in LJP. Inspired by the process of human exam preparation, our\nmethod follows a three-stage approach: first, we initialize judgment rules\nusing the FOL formalism to capture complex reasoning logic accurately; next, we\npropose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize\nthe judgment rules through a quiz consisting of confusable cases; finally, we\nutilize the optimized judgment rules to predict legal judgments. Experimental\nresults on two public datasets show superior performance across all metrics.\nThe code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u89c4\u5219\u589e\u5f3a\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u903b\u8f91\u63a8\u7406\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u6a21\u578b\u5ffd\u7565\u6cd5\u5f8b\u63a8\u7406\u903b\u8f91\uff0c\u4e14\u903b\u8f91\u50f5\u5316\u96be\u4ee5\u9002\u5e94\u590d\u6742\u6848\u4f8b\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u7528\u4e00\u9636\u903b\u8f91\u521d\u59cb\u5316\u5224\u51b3\u89c4\u5219\uff1b2) \u63d0\u51fa\u6df7\u6dc6\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u89c4\u5219\uff1b3) \u5e94\u7528\u4f18\u5316\u89c4\u5219\u9884\u6d4b\u5224\u51b3\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20564", "pdf": "https://arxiv.org/pdf/2505.20564", "abs": "https://arxiv.org/abs/2505.20564", "authors": ["Chris Emezue", "The NaijaVoices Community", "Busayo Awobade", "Abraham Owodunni", "Handel Emezue", "Gloria Monica Tobechukwu Emezue", "Nefertiti Nneoma Emezue", "Sewade Ogun", "Bunmi Akinremi", "David Ifeoluwa Adelani", "Chris Pal"], "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages", "categories": ["cs.CL"], "comment": "Accepted for publication at Interspeech 2025", "summary": "The development of high-performing, robust, and reliable speech technologies\ndepends on large, high-quality datasets. However, African languages --\nincluding our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to\ninsufficient data. Popular voice-enabled technologies do not support any of the\n2000+ African languages, limiting accessibility for circa one billion people.\nWhile previous dataset efforts exist for the target languages, they lack the\nscale and diversity needed for robust speech models. To bridge this gap, we\nintroduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+\nspeakers. We outline our unique data collection approach, analyze its acoustic\ndiversity, and demonstrate its impact through finetuning experiments on\nautomatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%\n(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'\npotential to advance multilingual speech processing for African languages.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86NaijaVoices\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b1800\u5c0f\u65f6\u8bed\u97f3\u6587\u672c\u7684\u975e\u6d32\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u8bed\u97f3\u6280\u672f\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u652f\u6301\u3002", "motivation": "\u975e\u6d32\u8bed\u8a00\u5728\u8bed\u97f3\u6280\u672f\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6280\u672f\u666e\u53ca\u3002", "method": "\u91c7\u7528\u72ec\u7279\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b5000+\u8bf4\u8bdd\u8005\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cWhisper\u3001MMS\u548cXLSR\u7684WER\u5206\u522b\u63d0\u5347\u4e8675.86%\u300152.06%\u548c42.33%\u3002", "conclusion": "NaijaVoices\u6570\u636e\u96c6\u6709\u671b\u63a8\u52a8\u975e\u6d32\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u53d1\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.20675", "pdf": "https://arxiv.org/pdf/2505.20675", "abs": "https://arxiv.org/abs/2505.20675", "authors": ["Lingyu Qiu", "Ke Jiang", "Xiaoyang Tan"], "title": "Contrastive Desensitization Learning for Cross Domain Face Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a new cross-domain face forgery detection method\nthat is insensitive to different and possibly unseen forgery methods while\nensuring an acceptable low false positive rate. Although existing face forgery\ndetection methods are applicable to multiple domains to some degree, they often\ncome with a high false positive rate, which can greatly disrupt the usability\nof the system. To address this issue, we propose an Contrastive Desensitization\nNetwork (CDN) based on a robust desensitization algorithm, which captures the\nessential domain characteristics through learning them from domain\ntransformation over pairs of genuine face images. One advantage of CDN lies in\nthat the learnt face representation is theoretical justified with regard to the\nits robustness against the domain changes. Extensive experiments over\nlarge-scale benchmark datasets demonstrate that our method achieves a much\nlower false alarm rate with improved detection accuracy compared to several\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u57df\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff08CDN\uff09\uff0c\u901a\u8fc7\u5bf9\u6bd4\u53bb\u654f\u7f51\u7edc\u964d\u4f4e\u8bef\u62a5\u7387\u5e76\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u57df\u9002\u7528\u6027\u4e0a\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u5bf9\u6bd4\u53bb\u654f\u7f51\u7edc\uff08CDN\uff09\u548c\u9c81\u68d2\u53bb\u654f\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u771f\u5b9e\u4eba\u8138\u56fe\u50cf\u7684\u57df\u53d8\u6362\u7279\u5f81\u6765\u6355\u83b7\u672c\u8d28\u57df\u7279\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCDN\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u5e76\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "CDN\u65b9\u6cd5\u5728\u8de8\u57df\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7406\u8bba\u652f\u6301\u7684\u9c81\u68d2\u6027\u3002", "relevance": 30.0}}
{"id": "2505.20659", "pdf": "https://arxiv.org/pdf/2505.20659", "abs": "https://arxiv.org/abs/2505.20659", "authors": ["Nathan Monette", "Alistair Letcher", "Michael Beukman", "Matthew T. Jackson", "Alexander Rutherford", "Alexander D. Goldie", "Jakob N. Foerster"], "title": "An Optimisation Framework for Unsupervised Environment Design", "categories": ["cs.LG"], "comment": "Reinforcement Learning Conference 2025", "summary": "For reinforcement learning agents to be deployed in high-risk settings, they\nmust achieve a high level of robustness to unfamiliar scenarios. One method for\nimproving robustness is unsupervised environment design (UED), a suite of\nmethods aiming to maximise an agent's generalisability across configurations of\nan environment. In this work, we study UED from an optimisation perspective,\nproviding stronger theoretical guarantees for practical settings than prior\nwork. Whereas previous methods relied on guarantees if they reach convergence,\nour framework employs a nonconvex-strongly-concave objective for which we\nprovide a provably convergent algorithm in the zero-sum setting. We empirically\nverify the efficacy of our method, outperforming prior methods in a number of\nenvironments with varying difficulties.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u51f8-\u5f3a\u51f9\u76ee\u6807\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u964c\u751f\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u7814\u7a76\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u7684\u4f18\u5316\u7406\u8bba\u3002", "method": "\u63d0\u51fa\u975e\u51f8-\u5f3a\u51f9\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u5728\u96f6\u548c\u535a\u5f08\u4e2d\u63d0\u4f9b\u53ef\u8bc1\u660e\u6536\u655b\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u4e0d\u540c\u96be\u5ea6\u7684\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u4f18\u5316\u6846\u67b6\u4e3aUED\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "relevance": 75.0}}
{"id": "2505.21291", "pdf": "https://arxiv.org/pdf/2505.21291", "abs": "https://arxiv.org/abs/2505.21291", "authors": ["Saman Marandi", "Yu-Shu Hu", "Mohammad Modarres"], "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "categories": ["cs.AI"], "comment": "22 Pages, 11 Figures", "summary": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u53ef\u9760\u6027\u7cfb\u7edf\uff08\u5982\u6838\u7535\u7ad9\uff09\u7684\u8bca\u65ad\u3002", "motivation": "\u4f20\u7edf\u8bca\u65ad\u6a21\u578b\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u529f\u80fd\u5efa\u6a21\u66f4\u5177\u5438\u5f15\u529b\u3002", "method": "\u57fa\u4e8e\u52a8\u6001\u4e3b\u903b\u8f91\uff08DML\uff09\u6a21\u578b\u7684\u529f\u80fd\u5efa\u6a21\uff0c\u7ed3\u5408\u4e24\u4e2aLLM\u7ec4\u4ef6\uff1a\u81ea\u52a8\u6784\u5efaDML\u903b\u8f91\u7684\u5de5\u4f5c\u6d41\u548c\u4ea4\u4e92\u5f0f\u8bca\u65ad\u4ee3\u7406\u3002\u751f\u6210\u7684\u903b\u8f91\u7f16\u7801\u4e3aKG-DML\uff0c\u652f\u6301\u5206\u5c42\u6545\u969c\u63a8\u7406\u3002", "result": "\u5728\u8f85\u52a9\u7ed9\u6c34\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6846\u67b6\u8868\u73b0\u51fa90%\u4ee5\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u5de5\u5177\u548c\u53c2\u6570\u63d0\u53d6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b89\u5168\u5173\u952e\u8bca\u65ad\u4e2d\u6709\u6548\uff0c\u652f\u6301\u9ad8\u53ef\u9760\u6027\u7cfb\u7edf\u7684\u590d\u6742\u8bca\u65ad\u9700\u6c42\u3002", "relevance": 75.0}}
{"id": "2505.20571", "pdf": "https://arxiv.org/pdf/2505.20571", "abs": "https://arxiv.org/abs/2505.20571", "authors": ["Bipul Thapa", "Gabriel Cofre"], "title": "Emotion Classification In-Context in Spanish", "categories": ["cs.CL", "cs.LG"], "comment": "This paper has been accepted and presented at the 4th International\n  Conference on Applied Intelligence and Informatics (AII 2024). The final\n  version will appear in the official conference proceedings. This preprint is\n  provided to ensure the timely dissemination of the research prior to formal\n  publication", "summary": "Classifying customer feedback into distinct emotion categories is essential\nfor understanding sentiment and improving customer experience. In this paper,\nwe classify customer feedback in Spanish into three emotion\ncategories--positive, neutral, and negative--using advanced NLP and ML\ntechniques. Traditional methods translate feedback from widely spoken languages\nto less common ones, resulting in a loss of semantic integrity and contextual\nnuances inherent to the original language. To address this limitation, we\npropose a hybrid approach that combines TF-IDF with BERT embeddings,\neffectively transforming Spanish text into rich numerical representations that\npreserve the semantic depth of the original language by using a Custom Stacking\nEnsemble (CSE) approach. To evaluate emotion classification, we utilize a range\nof models, including Logistic Regression, KNN, Bagging classifier with LGBM,\nand AdaBoost. The CSE model combines these classifiers as base models and uses\na one-vs-all Logistic Regression as the meta-model. Our experimental results\ndemonstrate that CSE significantly outperforms the individual and BERT model,\nachieving a test accuracy of 93.3% on the native Spanish dataset--higher than\nthe accuracy obtained from the translated version. These findings underscore\nthe challenges of emotion classification in Spanish and highlight the\nadvantages of combining vectorization techniques like TF-IDF with BERT for\nimproved accuracy. Our results provide valuable insights for businesses seeking\nto leverage emotion classification to enhance customer feedback analysis and\nservice improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TF-IDF\u548cBERT\u5d4c\u5165\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u897f\u73ed\u7259\u8bed\u5ba2\u6237\u53cd\u9988\u7684\u60c5\u611f\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49\u5806\u53e0\u96c6\u6210\uff08CSE\uff09\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\u7ffb\u8bd1\u4e3a\u8f83\u5c11\u4f7f\u7528\u7684\u8bed\u8a00\u4f1a\u5bfc\u81f4\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u7684\u4e22\u5931\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u539f\u59cb\u8bed\u8a00\u8bed\u4e49\u6df1\u5ea6\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528TF-IDF\u548cBERT\u5d4c\u5165\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5982Logistic Regression\u3001KNN\u7b49\uff09\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e00\u5bf9\u591aLogistic Regression\u4f5c\u4e3a\u5143\u6a21\u578b\u6784\u5efaCSE\u6a21\u578b\u3002", "result": "CSE\u6a21\u578b\u5728\u897f\u73ed\u7259\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523093.3%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u6a21\u578b\u548cBERT\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408TF-IDF\u548cBERT\u7684\u65b9\u6cd5\u5728\u897f\u73ed\u7259\u8bed\u60c5\u611f\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u63d0\u5347\u5ba2\u6237\u53cd\u9988\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.20676", "pdf": "https://arxiv.org/pdf/2505.20676", "abs": "https://arxiv.org/abs/2505.20676", "authors": ["Sadaf Safa", "Ali Abedi", "Shehroz S. Khan"], "title": "Supervised Contrastive Learning for Ordinal Engagement Measurement", "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 1 figure, 5 tables", "summary": "Student engagement plays a crucial role in the successful delivery of\neducational programs. Automated engagement measurement helps instructors\nmonitor student participation, identify disengagement, and adapt their teaching\nstrategies to enhance learning outcomes effectively. This paper identifies two\nkey challenges in this problem: class imbalance and incorporating order into\nengagement levels rather than treating it as mere categories. Then, a novel\napproach to video-based student engagement measurement in virtual learning\nenvironments is proposed that utilizes supervised contrastive learning for\nordinal classification of engagement. Various affective and behavioral features\nare extracted from video samples and utilized to train ordinal classifiers\nwithin a supervised contrastive learning framework (with a sequential\nclassifier as the encoder). A key step involves the application of diverse\ntime-series data augmentation techniques to these feature vectors, enhancing\nmodel training. The effectiveness of the proposed method was evaluated using a\npublicly available dataset for engagement measurement, DAiSEE, containing\nvideos of students who participated in virtual learning programs. The results\ndemonstrate the robust ability of the proposed method for the classification of\nthe engagement level. This approach promises a significant contribution to\nunderstanding and enhancing student engagement in virtual learning\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u89c6\u9891\u5b66\u751f\u53c2\u4e0e\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u987a\u5e8f\u6027\u95ee\u9898\uff0c\u5e76\u5728DAiSEE\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u6d4b\u91cf\u5b66\u751f\u53c2\u4e0e\u5ea6\u6709\u52a9\u4e8e\u6559\u5e08\u76d1\u63a7\u548c\u8c03\u6574\u6559\u5b66\u7b56\u7565\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5ffd\u7565\u53c2\u4e0e\u5ea6\u987a\u5e8f\u6027\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u5e8f\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u60c5\u611f\u548c\u884c\u4e3a\u7279\u5f81\u8fdb\u884c\u6709\u5e8f\u5206\u7c7b\u3002", "result": "\u5728DAiSEE\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u7a33\u5065\u5730\u5206\u7c7b\u53c2\u4e0e\u5ea6\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u865a\u62df\u5b66\u4e60\u73af\u5883\u4e2d\u5b66\u751f\u53c2\u4e0e\u5ea6\u7684\u7406\u89e3\u548c\u63d0\u5347\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002", "relevance": 30.0}}
{"id": "2505.20666", "pdf": "https://arxiv.org/pdf/2505.20666", "abs": "https://arxiv.org/abs/2505.20666", "authors": ["Yukun Zhang", "Xueqing Zhou"], "title": "Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel framework, Continuous_Time Attention, which infuses\npartial differential equations (PDEs) into the Transformer's attention\nmechanism to address the challenges of extremely long input sequences. Instead\nof relying solely on a static attention matrix, we allow attention weights to\nevolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion\ndynamics. This mechanism systematically smooths local noise, enhances\nlong_range dependencies, and stabilizes gradient flow. Theoretically, our\nanalysis shows that PDE_based attention leads to better optimization landscapes\nand polynomial rather than exponential decay of distant interactions.\nEmpirically, we benchmark our method on diverse experiments_demonstrating\nconsistent gains over both standard and specialized long sequence Transformer\nvariants. Our findings highlight the potential of PDE_based formulations to\nenrich attention mechanisms with continuous_time dynamics and global coherence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u8fde\u7eed\u65f6\u95f4\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3Transformer\u5904\u7406\u957f\u5e8f\u5217\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edfTransformer\u7684\u9759\u6001\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u6781\u957f\u8f93\u5165\u5e8f\u5217\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u5c40\u90e8\u566a\u58f0\u3001\u957f\u7a0b\u4f9d\u8d56\u4e0d\u8db3\u548c\u68af\u5ea6\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u6269\u6563\u3001\u6ce2\u52a8\u6216\u53cd\u5e94\u6269\u6563\u52a8\u529b\u5b66\uff0c\u5728\u4f2a\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u52a8\u6001\u6f14\u5316\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u4ece\u800c\u5e73\u6ed1\u566a\u58f0\u3001\u589e\u5f3a\u957f\u7a0b\u4f9d\u8d56\u5e76\u7a33\u5b9a\u68af\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660ePDE\u6ce8\u610f\u529b\u4f18\u5316\u4e86\u8bad\u7ec3\u666f\u89c2\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u8870\u51cf\u7684\u8fdc\u8ddd\u79bb\u4ea4\u4e92\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "PDE\u4e3a\u57fa\u7840\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e3aTransformer\u63d0\u4f9b\u4e86\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.21318", "pdf": "https://arxiv.org/pdf/2505.21318", "abs": "https://arxiv.org/abs/2505.21318", "authors": ["Hao Li", "He Cao", "Bin Feng", "Yanjun Shao", "Xiangru Tang", "Zhiyuan Yan", "Li Yuan", "Yonghong Tian", "Yu Li"], "title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations", "categories": ["cs.AI"], "comment": "22 pages, 10 figures", "summary": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ChemCoTBench\u6846\u67b6\uff0c\u5c06\u5316\u5b66\u95ee\u9898\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u64cd\u4f5c\uff0c\u4ee5\u63d0\u5347LLMs\u5728\u5316\u5b66\u9886\u57df\u7684\u7cfb\u7edf\u6027\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u5316\u5b66\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u672a\u88ab\u5145\u5206\u5f00\u53d1\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u7b80\u5355\u77e5\u8bc6\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u590d\u6742\u4efb\u52a1\uff08\u5982\u5206\u5b50\u4f18\u5316\u548c\u53cd\u5e94\u9884\u6d4b\uff09\u6240\u9700\u7684\u9010\u6b65\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u5c06\u5206\u5b50\u53d8\u6362\u89c6\u4e3a\u6a21\u5757\u5316\u201c\u5316\u5b66\u64cd\u4f5c\u201d\uff0cChemCoTBench\u6846\u67b6\u5c06\u5316\u5b66\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u900f\u660e\u3001\u9010\u6b65\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u7b97\u672f\u64cd\u4f5c\uff08\u5982\u52a0\u6cd5\u3001\u5220\u9664\u548c\u66ff\u6362\uff09\u3002", "result": "\u5728\u5206\u5b50\u5c5e\u6027\u4f18\u5316\u548c\u5316\u5b66\u53cd\u5e94\u9884\u6d4b\u4e24\u4e2a\u9ad8\u5f71\u54cd\u529b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u6807\u6ce8\u6570\u636e\u96c6\u3001\u63a8\u7406\u5206\u7c7b\u548c\u57fa\u7ebf\u8bc4\u4f30\u3002", "conclusion": "ChemCoTBench\u586b\u8865\u4e86\u62bd\u8c61\u63a8\u7406\u65b9\u6cd5\u4e0e\u5b9e\u9645\u5316\u5b66\u53d1\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3aLLMs\u4f5c\u4e3aAI\u9a71\u52a8\u79d1\u5b66\u521b\u65b0\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 75.0}}
{"id": "2505.20591", "pdf": "https://arxiv.org/pdf/2505.20591", "abs": "https://arxiv.org/abs/2505.20591", "authors": ["Sairam Gurajada", "Eser Kandogan", "Sajjadur Rahman"], "title": "Effectiveness of Prompt Optimization in NL2SQL Systems", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "NL2SQL approaches have greatly benefited from the impressive capabilities of\nlarge language models (LLMs). In particular, bootstrapping an NL2SQL system for\na specific domain can be as simple as instructing an LLM with sufficient\ncontextual information, such as schema details and translation demonstrations.\nHowever, building an accurate system still requires the rigorous task of\nselecting the right context for each query-including identifying relevant\nschema elements, cell values, and suitable exemplars that help the LLM\nunderstand domain-specific nuances. Retrieval-based methods have become the\ngo-to approach for identifying such context. While effective, these methods\nintroduce additional inference-time costs due to the retrieval process.\n  In this paper, we argue that production scenarios demand high-precision,\nhigh-performance NL2SQL systems, rather than simply high-quality SQL\ngeneration, which is the focus of most current NL2SQL approaches. In such\nscenarios, the careful selection of a static set of exemplars-capturing the\nintricacies of the query log, target database, SQL constructs, and execution\nlatencies-plays a more crucial role than exemplar selection based solely on\nsimilarity. The key challenge, however, lies in identifying a representative\nset of exemplars for a given production setting. To this end, we propose a\nprompt optimization framework that not only addresses the high-precision\nrequirement but also optimizes the performance of the generated SQL through\nmulti-objective optimization. Preliminary empirical analysis demonstrates the\neffectiveness of the proposed framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u751f\u4ea7\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6027\u80fdNL2SQL\u7cfb\u7edf\uff0c\u901a\u8fc7\u9759\u6001\u793a\u4f8b\u9009\u62e9\u548c\u63d0\u793a\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709NL2SQL\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u9ad8\u8d28\u91cfSQL\u751f\u6210\uff0c\u4f46\u751f\u4ea7\u573a\u666f\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6027\u80fd\uff0c\u5c24\u5176\u662f\u9759\u6001\u793a\u4f8b\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u9009\u62e9\u9759\u6001\u793a\u4f8b\uff0c\u4f18\u5316SQL\u751f\u6210\u6027\u80fd\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9759\u6001\u793a\u4f8b\u9009\u62e9\u548c\u63d0\u793a\u4f18\u5316\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u751f\u4ea7\u573a\u666f\u4e2dNL2SQL\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.20680", "pdf": "https://arxiv.org/pdf/2505.20680", "abs": "https://arxiv.org/abs/2505.20680", "authors": ["Haodong Lu", "Xinyu Zhang", "Kristen Moore", "Jason Xue", "Lina Yao", "Anton van den Hengel", "Dong Gong"], "title": "Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Continual learning (CL) enables deep networks to acquire new knowledge while\navoiding catastrophic forgetting. The powerful generalization ability of\npre-trained models (PTMs), such as the Contrastive Language-Image Pre-training\n(CLIP) model, has inspired a range of CL methods targeting new and specialized\ntasks, providing rich multi-modal embeddings that support lightweight,\nincremental prompt tuning. Existing methods often rely on complex designs built\nupon specific assumptions, such as intricate regularization schemes for prompt\npools, specialized routing mechanisms, or multi-stage incrementations, that\nintroduce additional-and possibly unnecessary-complexity, underutilizing CLIP's\nintrinsic capabilities. In this paper, we propose a concise CL approach for\nCLIP based on incremental prompt tuning that fully exploits its multi-modal\nstructure and the stability of textual representations. Our method, Textual\nPrototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely\nas static classifiers, as in existing methods, but as stable anchors to guide\nthe learning of visual prompts, thereby shaping the embedding space (i.e.,\nTPPT-V). We show that our bidirectional supervision strategy enables more\neffective learning of new knowledge while reducing forgetting. To further close\nthe vision-language gap during CL, we jointly optimizes visual and textual\nprompts (i.e., TPPT-VT). We also introduce a relational diversity\nregularization on the textual anchors to prevent embedding space collapse and\nmitigate correlated forgetting. Extensive experiments and analyses demonstrate\nthe effectiveness of our proposed approach, highlighting the benefits of\nleveraging CLIP's intrinsic guidance for continual adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u91cf\u63d0\u793a\u8c03\u4f18\u7684\u7b80\u6d01\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08TPPT\uff09\uff0c\u5229\u7528CLIP\u7684\u591a\u6a21\u6001\u7ed3\u6784\u548c\u6587\u672c\u8868\u793a\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u6587\u672c\u539f\u578b\u5f15\u5bfc\u89c6\u89c9\u63d0\u793a\u5b66\u4e60\uff0c\u51cf\u5c11\u9057\u5fd8\u5e76\u63d0\u5347\u65b0\u77e5\u8bc6\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528CLIP\u7684\u5185\u5728\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u7b80\u5316\u8bbe\u8ba1\u5e76\u66f4\u6709\u6548\u5730\u5229\u7528CLIP\u7684\u591a\u6a21\u6001\u7279\u6027\u3002", "method": "\u63d0\u51faTPPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u539f\u578b\u5f15\u5bfc\u89c6\u89c9\u63d0\u793a\u5b66\u4e60\uff08TPPT-V\uff09\uff0c\u5e76\u8054\u5408\u4f18\u5316\u89c6\u89c9\u548c\u6587\u672c\u63d0\u793a\uff08TPPT-VT\uff09\uff0c\u5f15\u5165\u5173\u7cfb\u591a\u6837\u6027\u6b63\u5219\u5316\u9632\u6b62\u5d4c\u5165\u7a7a\u95f4\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTPPT\u80fd\u6709\u6548\u5b66\u4e60\u65b0\u77e5\u8bc6\u5e76\u51cf\u5c11\u9057\u5fd8\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TPPT\u901a\u8fc7\u5229\u7528CLIP\u7684\u5185\u5728\u6307\u5bfc\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6301\u7eed\u9002\u5e94\u3002", "relevance": 70.0}}
{"id": "2505.20686", "pdf": "https://arxiv.org/pdf/2505.20686", "abs": "https://arxiv.org/abs/2505.20686", "authors": ["Kiant\u00e9 Brantley", "Mingyu Chen", "Zhaolin Gao", "Jason D. Lee", "Wen Sun", "Wenhao Zhan", "Xuezhou Zhang"], "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning\nlarge language models (LLMs) to improve complex reasoning abilities. However,\nstate-of-the-art policy optimization methods often suffer from high\ncomputational overhead and memory consumption, primarily due to the need for\nmultiple generations per prompt and the reliance on critic networks or\nadvantage estimates of the current policy. In this paper, we propose $A$*-PO, a\nnovel two-stage policy optimization framework that directly approximates the\noptimal advantage function and enables efficient training of LLMs for reasoning\ntasks. In the first stage, we leverage offline sampling from a reference policy\nto estimate the optimal value function $V$*, eliminating the need for costly\nonline value estimation. In the second stage, we perform on-policy updates\nusing a simple least-squares regression loss with only a single generation per\nprompt. Theoretically, we establish performance guarantees and prove that the\nKL-regularized RL objective can be optimized without requiring complex\nexploration strategies. Empirically, $A$*-PO achieves competitive performance\nacross a wide range of mathematical reasoning benchmarks, while reducing\ntraining time by up to 2$\\times$ and peak memory usage by over 30% compared to\nPPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at\nhttps://github.com/ZhaolinGao/A-PO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA*-PO\u7684\u4e24\u9636\u6bb5\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u63a8\u7406\u4efb\u52a1\uff0c\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u5fae\u8c03LLMs\u65f6\u5b58\u5728\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "A*-PO\u901a\u8fc7\u79bb\u7ebf\u91c7\u6837\u4f30\u8ba1\u6700\u4f18\u4ef7\u503c\u51fd\u6570\uff0c\u7136\u540e\u4f7f\u7528\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u51cf\u5c11\u751f\u6210\u6b21\u6570\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e30%\u3002", "conclusion": "A*-PO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684LLM\u5fae\u8c03\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.21322", "pdf": "https://arxiv.org/pdf/2505.21322", "abs": "https://arxiv.org/abs/2505.21322", "authors": ["R. Spencer Hallyburton", "Miroslav Pajic"], "title": "Assured Autonomy with Neuro-Symbolic Perception", "categories": ["cs.AI"], "comment": null, "summary": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u8303\u5f0f\uff08NeuSPaPer\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u8c61\u68c0\u6d4b\u548c\u573a\u666f\u56fe\u751f\u6210\uff08SGG\uff09\u5b9e\u73b0\u6df1\u5ea6\u573a\u666f\u7406\u89e3\uff0c\u4ee5\u63d0\u5347AI\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7f3a\u4e4f\u53ef\u9760\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u7b26\u53f7\u7ed3\u6784\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faNeuSPaPer\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u79bb\u7ebf\u77e5\u8bc6\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408\u4e13\u7528SGG\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u5173\u7cfb\u56fe\u4ee5\u786e\u4fdd\u60c5\u5883\u611f\u77e5\u7684\u5b8c\u6574\u6027\u3002", "result": "\u901a\u8fc7\u7269\u7406\u6a21\u62df\u5668\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\uff0cSGG\u80fd\u591f\u6709\u6548\u8fde\u63a5\u4f4e\u5c42\u4f20\u611f\u5668\u611f\u77e5\u4e0e\u9ad8\u5c42\u63a8\u7406\uff0c\u4e3aCPS\u4e2d\u7684\u53ef\u4fe1\u81ea\u4e3b\u6027\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u8303\u5f0f\u4e3aAI\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u53ef\u4fe1\u81ea\u4e3b\u6027\u7684\u53d1\u5c55\u3002", "relevance": 60.0}}
{"id": "2505.20606", "pdf": "https://arxiv.org/pdf/2505.20606", "abs": "https://arxiv.org/abs/2505.20606", "authors": ["Dancheng Liu", "Amir Nassereldine", "Chenhui Xu", "Jinjun Xiong"], "title": "Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation", "categories": ["cs.CL", "cs.MM"], "comment": "in submission", "summary": "Whisper's robust performance in automatic speech recognition (ASR) is often\nattributed to its massive 680k-hour training set, an impractical scale for most\nresearchers. In this work, we examine how linguistic and acoustic diversity in\ntraining data affect the robustness of the ASR model and reveal that\ntranscription generalization is primarily driven by acoustic variation rather\nthan linguistic richness. We find that targeted acoustic augmentation methods\ncould significantly improve the generalization ability of ASR models, reducing\nword-error rates by up to 19.24 percent on unseen datasets when training on the\n960-hour Librispeech dataset. These findings highlight strategic acoustically\nfocused data augmentation as a promising alternative to massive datasets for\nbuilding robust ASR models, offering a potential solution to future foundation\nASR models when massive human speech data is lacking.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cASR\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e3b\u8981\u53d7\u58f0\u5b66\u53d8\u5316\u800c\u975e\u8bed\u8a00\u591a\u6837\u6027\u9a71\u52a8\uff0c\u58f0\u5b66\u589e\u5f3a\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u8bad\u7ec3\u6570\u636e\u4e2d\u8bed\u8a00\u548c\u58f0\u5b66\u591a\u6837\u6027\u5bf9ASR\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u66ff\u4ee3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "method": "\u5206\u6790\u58f0\u5b66\u548c\u8bed\u8a00\u591a\u6837\u6027\u5bf9ASR\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u6d4b\u8bd5\u58f0\u5b66\u589e\u5f3a\u65b9\u6cd5\u5728Librispeech\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u3002", "result": "\u58f0\u5b66\u589e\u5f3a\u65b9\u6cd5\u5728960\u5c0f\u65f6\u6570\u636e\u96c6\u4e0a\u53ef\u5c06\u672a\u89c1\u6570\u636e\u96c6\u7684\u8bcd\u9519\u8bef\u7387\u964d\u4f4e19.24%\u3002", "conclusion": "\u58f0\u5b66\u589e\u5f3a\u662f\u6784\u5efa\u9c81\u68d2ASR\u6a21\u578b\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bed\u97f3\u6570\u636e\u7684\u60c5\u51b5\u3002", "relevance": 40.0}}
{"id": "2505.20687", "pdf": "https://arxiv.org/pdf/2505.20687", "abs": "https://arxiv.org/abs/2505.20687", "authors": ["Mingxuan Sun", "Juntao Jiang", "Zhiqiang Yang", "Shenao Kong", "Jiamin Qi", "Jianru Shang", "Shuangling Luo", "Wanfa Sun", "Tianyi Wang", "Yanqi Wang", "Qixuan Wang", "Tingjian Dai", "Tianxiang Chen", "Jinming Zhang", "Xuerui Zhang", "Yuepeng He", "Pengcheng Fu", "Qiu Guan", "Shizheng Zhou", "Yanbo Yu", "Qigui Jiang", "Teng Zhou", "Liuyong Shi", "Hong Yan"], "title": "VisAlgae 2023: A Dataset and Challenge for Algae Detection in Microscopy Images", "categories": ["cs.CV"], "comment": null, "summary": "Microalgae, vital for ecological balance and economic sectors, present\nchallenges in detection due to their diverse sizes and conditions. This paper\nsummarizes the second \"Vision Meets Algae\" (VisAlgae 2023) Challenge, aiming to\nenhance high-throughput microalgae cell detection. The challenge, which\nattracted 369 participating teams, includes a dataset of 1000 images across six\nclasses, featuring microalgae of varying sizes and distinct features.\nParticipants faced tasks such as detecting small targets, handling motion blur,\nand complex backgrounds. The top 10 methods, outlined here, offer insights into\novercoming these challenges and maximizing detection accuracy. This\nintersection of algae research and computer vision offers promise for\necological understanding and technological advancement. The dataset can be\naccessed at: https://github.com/juntaoJianggavin/Visalgae2023/.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u7b2c\u4e8c\u5c4a\u201cVision Meets Algae\u201d\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u63d0\u5347\u9ad8\u901a\u91cf\u5fae\u85fb\u7ec6\u80de\u68c0\u6d4b\u6280\u672f\uff0c\u5438\u5f15\u4e86369\u652f\u56e2\u961f\u53c2\u4e0e\u3002", "motivation": "\u5fae\u85fb\u56e0\u5176\u591a\u6837\u6027\u548c\u590d\u6742\u73af\u5883\u6761\u4ef6\uff0c\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u6311\u6218\u8d5b\u63d0\u4f9b\u4e861000\u5f20\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u516d\u7c7b\u5fae\u85fb\uff0c\u4efb\u52a1\u5305\u62ec\u5c0f\u76ee\u6807\u68c0\u6d4b\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u590d\u6742\u80cc\u666f\u5904\u7406\u3002", "result": "\u524d10\u540d\u65b9\u6cd5\u5c55\u793a\u4e86\u514b\u670d\u68c0\u6d4b\u6311\u6218\u548c\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u6280\u672f\u3002", "conclusion": "\u85fb\u7c7b\u7814\u7a76\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7ed3\u5408\u4e3a\u751f\u6001\u7406\u89e3\u548c\u6280\u672f\u8fdb\u6b65\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "relevance": 10.0}}
{"id": "2505.20691", "pdf": "https://arxiv.org/pdf/2505.20691", "abs": "https://arxiv.org/abs/2505.20691", "authors": ["Shenkai Zhao", "Xinao Zhang", "Lipeng Pan", "Xiaobin Xu", "Danilo Pelusi"], "title": "Evidential Deep Active Learning for Semi-Supervised Classification", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "9 pages, 4 figures", "summary": "Semi-supervised classification based on active learning has made significant\nprogress, but the existing methods often ignore the uncertainty estimation (or\nreliability) of the prediction results during the learning process, which makes\nit questionable whether the selected samples can effectively update the model.\nHence, this paper proposes an evidential deep active learning approach for\nsemi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised\nlearning framework to simultaneously quantify the uncertainty estimation of\nlabeled and unlabeled data during the learning process. The uncertainty\nestimation of the former is associated with evidential deep learning, while\nthat of the latter is modeled by combining ignorance information and conflict\ninformation of the evidence from the perspective of the T-conorm operator.\nFurthermore, this article constructs a heuristic method to dynamically balance\nthe influence of evidence and the number of classes on uncertainty estimation\nto ensure that it does not produce counter-intuitive results in EDALSSC. For\nthe sample selection strategy, EDALSSC selects the sample with the greatest\nuncertainty estimation that is calculated in the form of a sum when the\ntraining loss increases in the latter half of the learning process.\nExperimental results demonstrate that EDALSSC outperforms existing\nsemi-supervised and supervised active learning approaches on image\nclassification datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc1\u636e\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u7684\u534a\u76d1\u7763\u5206\u7c7b\u65b9\u6cd5\uff08EDALSSC\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u6539\u8fdb\u4e86\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5206\u7c7b\u65b9\u6cd5\u5728\u4e3b\u52a8\u5b66\u4e60\u4e2d\u5e38\u5ffd\u7565\u9884\u6d4b\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5bfc\u81f4\u6837\u672c\u9009\u62e9\u7684\u6709\u6548\u6027\u5b58\u7591\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "EDALSSC\u7ed3\u5408\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u548cT-conorm\u7b97\u5b50\uff0c\u91cf\u5316\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u8bbe\u8ba1\u542f\u53d1\u5f0f\u65b9\u6cd5\u52a8\u6001\u5e73\u8861\u8bc1\u636e\u548c\u7c7b\u522b\u6570\u91cf\u7684\u5f71\u54cd\u3002\u6837\u672c\u9009\u62e9\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u603b\u548c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEDALSSC\u5728\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u534a\u76d1\u7763\u548c\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "EDALSSC\u901a\u8fc7\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5206\u7c7b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21327", "pdf": "https://arxiv.org/pdf/2505.21327", "abs": "https://arxiv.org/abs/2505.21327", "authors": ["Jiakang Yuan", "Tianshuo Peng", "Yilei Jiang", "Yiting Lu", "Renrui Zhang", "Kaituo Feng", "Chaoyou Fu", "Tao Chen", "Lei Bai", "Bo Zhang", "Xiangyu Yue"], "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities.", "AI": {"tldr": "MME-Reasoning\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u903b\u8f91\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d6\u5f52\u7eb3\u3001\u6f14\u7ece\u548c\u6eaf\u56e0\u4e09\u79cd\u63a8\u7406\u7c7b\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5148\u8fdbMLLMs\u5728\u7efc\u5408\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5168\u9762\u8bc4\u4f30MLLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u7c7b\u578b\u7684\u660e\u786e\u5206\u7c7b\u548c\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u6e05\u6670\u7406\u89e3\u3002", "method": "\u8bbe\u8ba1\u4e86MME-Reasoning\u57fa\u51c6\uff0c\u6db5\u76d6\u4e09\u79cd\u63a8\u7406\u7c7b\u578b\uff0c\u786e\u4fdd\u95ee\u9898\u6709\u6548\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u800c\u975e\u611f\u77e5\u6280\u80fd\u6216\u77e5\u8bc6\u5e7f\u5ea6\uff0c\u5e76\u6269\u5c55\u8bc4\u4f30\u534f\u8bae\u4ee5\u8986\u76d6\u591a\u6837\u5316\u95ee\u9898\u3002", "result": "\u5148\u8fdbMLLMs\u5728\u7efc\u5408\u903b\u8f91\u63a8\u7406\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e14\u5728\u4e0d\u540c\u63a8\u7406\u7c7b\u578b\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0d\u5e73\u8861\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u591a\u6837\u903b\u8f91\u63a8\u7406\u573a\u666f\u4e2d\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\u548c\u6027\u80fd\u4e0d\u5e73\u8861\uff0c\u4e3a\u63a8\u7406\u80fd\u529b\u7684\u7406\u89e3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.20613", "pdf": "https://arxiv.org/pdf/2505.20613", "abs": "https://arxiv.org/abs/2505.20613", "authors": ["Ziju Shen", "Naohao Huang", "Fanyi Yang", "Yutong Wang", "Guoxiong Gao", "Tianyi Xu", "Jiedong Jiang", "Wanyi He", "Pu Yang", "Mengzhou Sun", "Haocheng Ju", "Peihao Wu", "Bryan Dai", "Bin Dong"], "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64).", "AI": {"tldr": "REAL-Prover\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u7406\u8bc1\u660e\u5668\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u548c\u68c0\u7d22\u7cfb\u7edf\u63d0\u5347\u89e3\u51b3\u5927\u5b66\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5e76\u5728ProofNet\u548cFATE-M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u5b9a\u7406\u8bc1\u660e\u5668\u5728\u9ad8\u7ea7\u6570\u5b66\u95ee\u9898\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0cREAL-Prover\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08REAL-Prover-v1\uff09\u548c\u68c0\u7d22\u7cfb\u7edf\uff08Leansearch-PS\uff09\uff0c\u7ed3\u5408\u6570\u636e\u63d0\u53d6\u7ba1\u9053\uff08HERALD-AF\uff09\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\uff08Jixia-interactive\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728ProofNet\u6570\u636e\u96c6\u4e0a\u8fbe\u523023.7%\u7684\u6210\u529f\u7387\uff08Pass@64\uff09\uff0c\u5728FATE-M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523056.7%\u7684SOTA\u6210\u529f\u7387\u3002", "conclusion": "REAL-Prover\u5c55\u793a\u4e86LLM\u5728\u9ad8\u7ea7\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 60.0}}
{"id": "2505.20694", "pdf": "https://arxiv.org/pdf/2505.20694", "abs": "https://arxiv.org/abs/2505.20694", "authors": ["Xulin Gu", "Xinhao Zhong", "Zhixing Wei", "Yimin Zhou", "Shuoyang Sun", "Bin Chen", "Hongpeng Wang", "Yuan Luo"], "title": "Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset distillation (DD) has emerged as a powerful paradigm for dataset\ncompression, enabling the synthesis of compact surrogate datasets that\napproximate the training utility of large-scale ones. While significant\nprogress has been achieved in distilling image datasets, extending DD to the\nvideo domain remains challenging due to the high dimensionality and temporal\ncomplexity inherent in video data. Existing video distillation (VD) methods\noften suffer from excessive computational costs and struggle to preserve\ntemporal dynamics, as na\\\"ive extensions of image-based approaches typically\nlead to degraded performance. In this paper, we propose a novel uni-level video\ndataset distillation framework that directly optimizes synthetic videos with\nrespect to a pre-trained model. To address temporal redundancy and enhance\nmotion preservation, we introduce a temporal saliency-guided filtering\nmechanism that leverages inter-frame differences to guide the distillation\nprocess, encouraging the retention of informative temporal cues while\nsuppressing frame-level redundancy. Extensive experiments on standard video\nbenchmarks demonstrate that our method achieves state-of-the-art performance,\nbridging the gap between real and distilled video data and offering a scalable\nsolution for video dataset compression.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u663e\u8457\u6027\u8fc7\u6ee4\u673a\u5236\u4f18\u5316\u5408\u6210\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u548c\u65f6\u95f4\u590d\u6742\u6027\u4f7f\u5f97\u73b0\u6709\u89c6\u9891\u84b8\u998f\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5355\u7ea7\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u663e\u8457\u6027\u8fc7\u6ee4\u673a\u5236\uff0c\u5229\u7528\u5e27\u95f4\u5dee\u5f02\u6307\u5bfc\u84b8\u998f\u8fc7\u7a0b\u3002", "result": "\u5728\u6807\u51c6\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u771f\u5b9e\u4e0e\u84b8\u998f\u89c6\u9891\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u9891\u6570\u636e\u96c6\u538b\u7f29\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20697", "pdf": "https://arxiv.org/pdf/2505.20697", "abs": "https://arxiv.org/abs/2505.20697", "authors": ["Zachary C. Brown", "David Carlson"], "title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "comment": null, "summary": "The field of hypothesis generation promises to reduce costs in neuroscience\nby narrowing the range of interventional studies needed to study various\nphenomena. Existing machine learning methods can generate scientific hypotheses\nfrom complex datasets, but many approaches assume causal relationships are\nstatic over time, limiting their applicability to systems with dynamic,\nstate-dependent behavior, such as the brain. While some techniques attempt\ndynamic causal discovery through factor models, they often restrict\nrelationships to linear patterns or impose other simplifying assumptions. We\npropose a novel method that models dynamic graphs as a conditionally weighted\nsuperposition of static graphs, where each static graph can capture nonlinear\nrelationships. This approach enables the detection of complex, time-varying\ninteractions between variables beyond linear limitations. Our method improves\nf1-scores of predicted dynamic causal patterns by roughly 22-28% on average\nover baselines in some of our experiments, with some improvements reaching well\nover 60%. A case study on real brain data demonstrates our method's ability to\nuncover relationships linked to specific behavioral states, offering valuable\ninsights into neural dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u52a0\u6743\u53e0\u52a0\u9759\u6001\u56fe\u6765\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u56e0\u679c\u6a21\u5f0f\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u56e0\u679c\u5173\u7cfb\u662f\u9759\u6001\u7684\u6216\u7ebf\u6027\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u7cfb\u7edf\uff08\u5982\u5927\u8111\uff09\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u52a8\u6001\u56fe\u5efa\u6a21\u4e3a\u9759\u6001\u56fe\u7684\u6761\u4ef6\u52a0\u6743\u53e0\u52a0\uff0c\u6bcf\u4e2a\u9759\u6001\u56fe\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u52a8\u6001\u56e0\u679c\u6a21\u5f0f\u7684\u9884\u6d4bF1\u5206\u6570\u5e73\u5747\u63d0\u534722-28%\uff0c\u90e8\u5206\u6539\u8fdb\u8d85\u8fc760%\u3002", "conclusion": "\u65b9\u6cd5\u5728\u771f\u5b9e\u8111\u6570\u636e\u4e2d\u6210\u529f\u63ed\u793a\u4e86\u4e0e\u884c\u4e3a\u72b6\u6001\u76f8\u5173\u7684\u52a8\u6001\u5173\u7cfb\u3002", "relevance": 40.0}}
{"id": "2505.21344", "pdf": "https://arxiv.org/pdf/2505.21344", "abs": "https://arxiv.org/abs/2505.21344", "authors": ["Aidan Peppin", "Julia Kreutzer", "Alice Schoenauer Sebag", "Kelly Marchisio", "Beyza Ermis", "John Dang", "Samuel Cahyawijaya", "Shivalika Singh", "Seraphina Goldfarb-Tarrant", "Viraat Aryabumi", "Aakanksha", "Wei-Yin Ko", "Ahmet \u00dcst\u00fcn", "Matthias Gall\u00e9", "Marzieh Fadaee", "Sara Hooker"], "title": "The Multilingual Divide and Its Impact on Global AI Safety", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Despite advances in large language model capabilities in recent years, a\nlarge gap remains in their capabilities and safety performance for many\nlanguages beyond a relatively small handful of globally dominant languages.\nThis paper provides researchers, policymakers and governance experts with an\noverview of key challenges to bridging the \"language gap\" in AI and minimizing\nsafety risks across languages. We provide an analysis of why the language gap\nin AI exists and grows, and how it creates disparities in global AI safety. We\nidentify barriers to address these challenges, and recommend how those working\nin policy and governance can help address safety concerns associated with the\nlanguage gap by supporting multilingual dataset creation, transparency, and\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u80fd\u529b\u4e0e\u5b89\u5168\u6027\u4e0a\u7684\u5dee\u8ddd\uff0c\u5206\u6790\u4e86\u539f\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00AI\u80fd\u529b\u4e0e\u5b89\u5168\u6027\u7684\u5dee\u8ddd\u53ca\u5176\u5168\u7403\u5f71\u54cd\u3002", "method": "\u5206\u6790\u8bed\u8a00\u5dee\u8ddd\u7684\u6210\u56e0\u53ca\u5176\u5bf9AI\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u653f\u7b56\u4e0e\u6cbb\u7406\u5efa\u8bae\u3002", "result": "\u8bc6\u522b\u4e86\u8bed\u8a00\u5dee\u8ddd\u7684\u6311\u6218\u4e0e\u969c\u788d\uff0c\u5efa\u8bae\u652f\u6301\u591a\u8bed\u8a00\u6570\u636e\u96c6\u521b\u5efa\u4e0e\u900f\u660e\u7814\u7a76\u3002", "conclusion": "\u547c\u5401\u653f\u7b56\u4e0e\u6cbb\u7406\u9886\u57df\u652f\u6301\u591a\u8bed\u8a00AI\u7814\u7a76\u4ee5\u51cf\u5c11\u5b89\u5168\u98ce\u9669\u3002", "relevance": 60.0}}
{"id": "2505.20622", "pdf": "https://arxiv.org/pdf/2505.20622", "abs": "https://arxiv.org/abs/2505.20622", "authors": ["Ting Xu", "Zhichao Huang", "Jiankai Sun", "Shanbo Cheng", "Wai Lam"], "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "We present Sequential Policy Optimization for Simultaneous Machine\nTranslation (SeqPO-SiMT), a new policy optimization framework that defines the\nsimultaneous machine translation (SiMT) task as a sequential decision making\nproblem, incorporating a tailored reward to enhance translation quality while\nreducing latency. In contrast to popular Reinforcement Learning from Human\nFeedback (RLHF) methods, such as PPO and DPO, which are typically applied in\nsingle-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.\nThis intuitive framework allows the SiMT LLMs to simulate and refine the SiMT\nprocess using a tailored reward. We conduct experiments on six datasets from\ndiverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that\nSeqPO-SiMT consistently achieves significantly higher translation quality with\nlower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning\n(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17\nin the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context\nthan offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly\nrival the offline translation of high-performing LLMs, including\nQwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.", "AI": {"tldr": "SeqPO-SiMT\u662f\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u673a\u5668\u7ffb\u8bd1\uff08SiMT\uff09\uff0c\u901a\u8fc7\u5b9a\u5236\u5956\u52b1\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u591a\u6b65SiMT\u4efb\u52a1\u4e2d\u73b0\u6709RLHF\u65b9\u6cd5\uff08\u5982PPO\u548cDPO\uff09\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u5c06SiMT\u4efb\u52a1\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5b9a\u5236\u5956\u52b1\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff08\u5982COMET\u63d0\u9ad81.13\u5206\uff09\u5e76\u964d\u4f4e\u5ef6\u8fdf\uff08\u5e73\u5747\u6ede\u540e\u51cf\u5c116.17\uff09\u3002", "conclusion": "SeqPO-SiMT\u57287B LLM\u4e0a\u7684\u8868\u73b0\u751a\u81f3\u5ab2\u7f8e\u9ad8\u6027\u80fd\u79bb\u7ebf\u7ffb\u8bd1\u6a21\u578b\u3002", "relevance": 70.0}}
{"id": "2505.20704", "pdf": "https://arxiv.org/pdf/2505.20704", "abs": "https://arxiv.org/abs/2505.20704", "authors": ["Zixuan Hu", "Yichun Hu", "Xiaotong Li", "Shixiang Tang", "Ling-Yu Duan"], "title": "Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to\nunseen domains under extreme data scarcity and multiple shifts. Previous\napproaches mainly focused on sample selection strategies, while overlooking the\nfundamental problem on underlying optimization. Initially, we critically\nanalyze the widely-adopted entropy minimization framework in WTTA and uncover\nits significant limitations in noisy optimization dynamics that substantially\nhinder adaptation efficiency. Through our analysis, we identify region\nconfidence as a superior alternative to traditional entropy, however, its\ndirect optimization remains computationally prohibitive for real-time\napplications. In this paper, we introduce a novel region-integrated method\nReCAP that bypasses the lengthy process. Specifically, we propose a\nprobabilistic region modeling scheme that flexibly captures semantic changes in\nembedding space. Subsequently, we develop a finite-to-infinite asymptotic\napproximation that transforms the intractable region confidence into a\ntractable and upper-bounded proxy. These innovations significantly unlock the\noverlooked potential dynamics in local region in a concise solution. Our\nextensive experiments demonstrate the consistent superiority of ReCAP over\nexisting methods across various datasets and wild scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReCAP\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6781\u7aef\u6570\u636e\u7a00\u7f3a\u548c\u591a\u57df\u504f\u79fb\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08WTTA\uff09\uff0c\u901a\u8fc7\u533a\u57df\u7f6e\u4fe1\u5ea6\u4f18\u5316\u66ff\u4ee3\u4f20\u7edf\u71b5\u6700\u5c0f\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6548\u7387\u3002", "motivation": "\u73b0\u6709WTTA\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u800c\u5ffd\u7565\u4e86\u5e95\u5c42\u4f18\u5316\u95ee\u9898\u3002\u4f5c\u8005\u5206\u6790\u4e86\u71b5\u6700\u5c0f\u5316\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u73b0\u533a\u57df\u7f6e\u4fe1\u5ea6\u662f\u66f4\u4f18\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u76f4\u63a5\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faReCAP\u65b9\u6cd5\uff0c\u5305\u62ec\u6982\u7387\u533a\u57df\u5efa\u6a21\u65b9\u6848\u548c\u6709\u9650\u5230\u65e0\u9650\u7684\u6e10\u8fdb\u903c\u8fd1\uff0c\u5c06\u96be\u89e3\u7684\u533a\u57df\u7f6e\u4fe1\u5ea6\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u4e14\u4e0a\u754c\u7684\u4ee3\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReCAP\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReCAP\u901a\u8fc7\u533a\u57df\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86WTTA\u7684\u9002\u5e94\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20698", "pdf": "https://arxiv.org/pdf/2505.20698", "abs": "https://arxiv.org/abs/2505.20698", "authors": ["Woomin Song", "Jihoon Tack", "Sangwoo Mo", "Seunghyuk Oh", "Jinwoo Shin"], "title": "Sparsified State-Space Models are Efficient Highway Networks", "categories": ["cs.LG"], "comment": "Accepted to TMLR 2025.03", "summary": "State-space models (SSMs) offer a promising architecture for sequence\nmodeling, providing an alternative to Transformers by replacing expensive\nself-attention with linear recurrences. In this paper, we propose a simple yet\neffective trick to enhance SSMs within given computational budgets by\nsparsifying them. Our intuition is that tokens in SSMs are highly redundant due\nto gradual recurrent updates, and dense recurrence operations block the\ndelivery of past information. In particular, we observe that upper layers of\nSSMs tend to be more redundant as they encode global information, while lower\nlayers encode local information. Motivated by this, we introduce Simba, a\nhierarchical sparsification method for SSMs based on token pruning. Simba\nsparsifies upper layers more than lower layers, encouraging the upper layers to\nbehave like highways. To achieve this, we propose a novel token pruning\ncriterion for SSMs, measuring the global impact of tokens on the final output\nby accumulating local recurrences. We demonstrate that Simba outperforms the\nbaseline model, Mamba, with the same FLOPS in various natural language tasks.\nMoreover, we illustrate the effect of highways, showing that Simba not only\nenhances efficiency but also improves the information flow across long\nsequences. Code is available at https://github.com/woominsong/Simba.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSimba\u7684\u5206\u5c42\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\uff0c\u901a\u8fc7\u4ee4\u724c\u526a\u679d\u63d0\u9ad8\u6548\u7387\u548c\u4fe1\u606f\u6d41\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5197\u4f59\u548c\u6548\u7387\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u5316\u4f18\u5316SSMs\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSimba\u65b9\u6cd5\uff0c\u5bf9SSMs\u8fdb\u884c\u5206\u5c42\u7a00\u758f\u5316\uff0c\u4e0a\u5c42\u7a00\u758f\u5316\u7a0b\u5ea6\u66f4\u9ad8\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4ee4\u724c\u526a\u679d\u6807\u51c6\u3002", "result": "Simba\u5728\u76f8\u540c\u8ba1\u7b97\u91cf\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bMamba\uff0c\u5e76\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Simba\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u8fd8\u6539\u5584\u4e86\u957f\u5e8f\u5217\u4e2d\u7684\u4fe1\u606f\u6d41\u3002", "relevance": 85.0}}
{"id": "2505.21398", "pdf": "https://arxiv.org/pdf/2505.21398", "abs": "https://arxiv.org/abs/2505.21398", "authors": ["Maria Cristina Carrisi", "Mirko Marras", "Sara Vergallo"], "title": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education", "categories": ["cs.AI", "cs.ET"], "comment": "Under review", "summary": "Younger generations are growing up in a world increasingly shaped by\nintelligent technologies, making early AI literacy crucial for developing the\nskills to critically understand and navigate them. However, education in this\nfield often emphasizes tool-based learning, prioritizing usage over\nunderstanding the underlying concepts. This lack of knowledge leaves\nnon-experts, especially children, prone to misconceptions, unrealistic\nexpectations, and difficulties in recognizing biases and stereotypes. In this\npaper, we propose a structured and replicable teaching approach that fosters\nfoundational AI literacy in primary students, by building upon core\nmathematical elements closely connected to and of interest in primary\ncurricula, to strengthen conceptualization, data representation, classification\nreasoning, and evaluation of AI. To assess the effectiveness of our approach,\nwe conducted an empirical study with thirty-one fifth-grade students across two\nclasses, evaluating their progress through a post-test and a satisfaction\nsurvey. Our results indicate improvements in terminology understanding and\nusage, features description, logical reasoning, and evaluative skills, with\nstudents showing a deeper comprehension of decision-making processes and their\nlimitations. Moreover, the approach proved engaging, with students particularly\nenjoying activities that linked AI concepts to real-world reasoning. Materials:\nhttps://github.com/tail-unica/ai-literacy-primary-ed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5c0f\u5b66\u751f\u7684\u7ed3\u6784\u5316AI\u7d20\u517b\u6559\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u5143\u7d20\u5f3a\u5316\u6982\u5ff5\u5316\u3001\u6570\u636e\u8868\u793a\u3001\u5206\u7c7b\u63a8\u7406\u548cAI\u8bc4\u4f30\u80fd\u529b\uff0c\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u7684\u7406\u89e3\u548c\u5174\u8da3\u3002", "motivation": "\u65e9\u671fAI\u7d20\u517b\u5bf9\u5e74\u8f7b\u4e00\u4ee3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u6559\u80b2\u8fc7\u4e8e\u5de5\u5177\u5316\uff0c\u7f3a\u4e4f\u5bf9\u6838\u5fc3\u6982\u5ff5\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u975e\u4e13\u5bb6\uff08\u5c24\u5176\u662f\u513f\u7ae5\uff09\u5bb9\u6613\u4ea7\u751f\u8bef\u89e3\u548c\u504f\u89c1\u3002", "method": "\u57fa\u4e8e\u6570\u5b66\u5143\u7d20\u8bbe\u8ba1\u7ed3\u6784\u5316\u6559\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0831\u540d\u4e94\u5e74\u7ea7\u5b66\u751f\uff09\u8bc4\u4f30\u6548\u679c\uff0c\u5305\u62ec\u540e\u6d4b\u548c\u6ee1\u610f\u5ea6\u8c03\u67e5\u3002", "result": "\u5b66\u751f\u5728\u672f\u8bed\u7406\u89e3\u3001\u7279\u5f81\u63cf\u8ff0\u3001\u903b\u8f91\u63a8\u7406\u548c\u8bc4\u4f30\u80fd\u529b\u65b9\u9762\u6709\u6240\u63d0\u5347\uff0c\u5bf9AI\u51b3\u7b56\u8fc7\u7a0b\u53ca\u5176\u5c40\u9650\u6027\u7684\u7406\u89e3\u66f4\u6df1\uff0c\u4e14\u6559\u5b66\u65b9\u6cd5\u5177\u6709\u5438\u5f15\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u5b66\u751f\u7684AI\u7d20\u517b\uff0c\u4e14\u901a\u8fc7\u8054\u7cfb\u73b0\u5b9e\u4e16\u754c\u7684\u6d3b\u52a8\u589e\u5f3a\u4e86\u5b66\u4e60\u5174\u8da3\u3002", "relevance": 40.0}}
{"id": "2505.20624", "pdf": "https://arxiv.org/pdf/2505.20624", "abs": "https://arxiv.org/abs/2505.20624", "authors": ["Usman Naseem", "Juan Ren", "Saba Anwar", "Sarah Kohail", "Rudy Alexandro Garrido Veliz", "Robert Geislinger", "Aisha Jabr", "Idris Abdulmumin", "Laiba Qureshi", "Aarushi Ajay Borkar", "Maryam Ibrahim Mukhtar", "Abinew Ali Ayele", "Ibrahim Said Ahmad", "Adem Ali", "Martin Semmann", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "title": "POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Online polarization poses a growing challenge for democratic discourse, yet\nmost computational social science research remains monolingual, culturally\nnarrow, or event-specific. We introduce POLAR, a multilingual, multicultural,\nand multievent dataset with over 23k instances in seven languages from diverse\nonline platforms and real-world events. Polarization is annotated along three\naxes: presence, type, and manifestation, using a variety of annotation\nplatforms adapted to each cultural context. We conduct two main experiments:\n(1) we fine-tune six multilingual pretrained language models in both\nmonolingual and cross-lingual setups; and (2) we evaluate a range of open and\nclosed large language models (LLMs) in few-shot and zero-shot scenarios.\nResults show that while most models perform well on binary polarization\ndetection, they achieve substantially lower scores when predicting polarization\ntypes and manifestations. These findings highlight the complex, highly\ncontextual nature of polarization and the need for robust, adaptable approaches\nin NLP and computational social science. All resources will be released to\nsupport further research and effective mitigation of digital polarization\nglobally.", "AI": {"tldr": "POLAR\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6587\u5316\u3001\u591a\u4e8b\u4ef6\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u5728\u7ebf\u6781\u5316\u73b0\u8c61\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u73b0\u6709\u6a21\u578b\u5728\u4e8c\u5143\u6781\u5316\u68c0\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9884\u6d4b\u6781\u5316\u7c7b\u578b\u548c\u8868\u73b0\u5f62\u5f0f\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u5728\u7ebf\u6781\u5316\u5bf9\u6c11\u4e3b\u8bdd\u8bed\u6784\u6210\u6311\u6218\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u4e3a\u5355\u8bed\u8a00\u3001\u6587\u5316\u72ed\u9698\u6216\u4e8b\u4ef6\u7279\u5b9a\u3002POLAR\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaPOLAR\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u6781\u5316\u7684\u4e09\u4e2a\u7ef4\u5ea6\uff1b\u5fae\u8c03\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8bc4\u4f30LLM\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u4e8c\u5143\u6781\u5316\u68c0\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9884\u6d4b\u6781\u5316\u7c7b\u578b\u548c\u8868\u73b0\u5f62\u5f0f\u4e0a\u5f97\u5206\u8f83\u4f4e\u3002", "conclusion": "\u6781\u5316\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u6027\uff0c\u9700\u8981\u66f4\u7a33\u5065\u3001\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.20710", "pdf": "https://arxiv.org/pdf/2505.20710", "abs": "https://arxiv.org/abs/2505.20710", "authors": ["Kui Wu", "Hao Chen", "Churan Wang", "Fakhri Karray", "Zhoujun Li", "Yizhou Wang", "Fangwei Zhong"], "title": "Hierarchical Instruction-aware Embodied Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for\nreinforcement learning-based models due to the substantial gap between\nhigh-level user instructions and low-level agent actions. While recent\nadvancements in language models (e.g., LLMs, VLMs, VLAs) have improved\ninstruction comprehension, these models face critical limitations in either\ninference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To\naddress these challenges, we propose \\textbf{Hierarchical Instruction-aware\nEmbodied Visual Tracking (HIEVT)} agent, which bridges instruction\ncomprehension and action generation using \\textit{spatial goals} as\nintermediaries. HIEVT first introduces \\textit{LLM-based Semantic-Spatial Goal\nAligner} to translate diverse human instructions into spatial goals that\ndirectly annotate the desired spatial position. Then the \\textit{RL-based\nAdaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to\nposition the target as specified by the spatial goal. To benchmark UC-EVT\ntasks, we collect over ten million trajectories for training and evaluate\nacross one seen environment and nine unseen challenging environments. Extensive\nexperiments and real-world deployments demonstrate the robustness and\ngeneralizability of HIEVT across diverse environments, varying target dynamics,\nand complex instruction combinations. The complete project is available at\nhttps://sites.google.com/view/hievt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIEVT\u7684\u5206\u5c42\u6307\u4ee4\u611f\u77e5\u89c6\u89c9\u8ddf\u8e2a\u4ee3\u7406\uff0c\u901a\u8fc7\u7a7a\u95f4\u76ee\u6807\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u89e3\u51b3\u4e86\u7528\u6237\u6307\u4ee4\u4e0e\u4f4e\u5c42\u52a8\u4f5c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u63a8\u7406\u901f\u5ea6\u6162\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "HIEVT\u7ed3\u5408LLM\u8bed\u4e49-\u7a7a\u95f4\u76ee\u6807\u5bf9\u9f50\u5668\u548c\u57fa\u4e8eRL\u7684\u81ea\u9002\u5e94\u76ee\u6807\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u6307\u4ee4\u8f6c\u5316\u4e3a\u7a7a\u95f4\u76ee\u6807\u5e76\u751f\u6210\u52a8\u4f5c\u3002", "result": "\u5728\u5341\u591a\u79cd\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cHIEVT\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HIEVT\u5728\u590d\u6742\u6307\u4ee4\u548c\u52a8\u6001\u76ee\u6807\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "relevance": 75.0}}
{"id": "2505.20716", "pdf": "https://arxiv.org/pdf/2505.20716", "abs": "https://arxiv.org/abs/2505.20716", "authors": ["Reza Nematirad", "Anil Pahwa", "Balasubramaniam Natarajan"], "title": "Are Data Embeddings effective in time series forecasting?", "categories": ["cs.LG"], "comment": "Code is available at:\n  https://github.com/neuripsdataembedidng/DataEmbedding", "summary": "Time series forecasting plays a crucial role in many real-world applications,\nand numerous complex forecasting models have been proposed in recent years.\nDespite their architectural innovations, most state-of-the-art models report\nonly marginal improvements -- typically just a few thousandths in standard\nerror metrics. These models often incorporate complex data embedding layers to\ntransform raw inputs into higher-dimensional representations to enhance\naccuracy. But are data embedding techniques actually effective in time series\nforecasting? Through extensive ablation studies across fifteen state-of-the-art\nmodels and four benchmark datasets, we find that removing data embedding layers\nfrom many state-of-the-art models does not degrade forecasting performance. In\nmany cases, it improves both accuracy and computational efficiency. The gains\nfrom removing embedding layers often exceed the performance differences\ntypically reported between competing models. Code available at:\nhttps://github.com/neuripsdataembedidng/DataEmbedding", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u79fb\u9664\u6570\u636e\u5d4c\u5165\u5c42\u4e0d\u4ec5\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u53cd\u800c\u53ef\u80fd\u63d0\u5347\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u63a2\u8ba8\u6570\u636e\u5d4c\u5165\u6280\u672f\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5bf915\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u548c4\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u3002", "result": "\u79fb\u9664\u6570\u636e\u5d4c\u5165\u5c42\u901a\u5e38\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5176\u589e\u76ca\u751a\u81f3\u8d85\u8fc7\u6a21\u578b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u6570\u636e\u5d4c\u5165\u5c42\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e76\u975e\u5fc5\u8981\uff0c\u79fb\u9664\u5b83\u4eec\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c\u3002", "relevance": 30.0}}
{"id": "2505.21410", "pdf": "https://arxiv.org/pdf/2505.21410", "abs": "https://arxiv.org/abs/2505.21410", "authors": ["Shashank Sharma", "Janina Hoffmann", "Vinay Namboodiri"], "title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Hierarchical reinforcement learning (HRL) relies on abstract skills to solve\nlong-horizon tasks efficiently. While existing skill discovery methods learns\nthese skills automatically, they are limited to a single skill per task. In\ncontrast, humans learn and use both fine-grained and coarse motor skills\nsimultaneously. Inspired by human motor control, we propose Multi-Resolution\nSkill Discovery (MRSD), an HRL framework that learns multiple skill encoders at\ndifferent temporal resolutions in parallel. A high-level manager dynamically\nselects among these skills, enabling adaptive control strategies over time. We\nevaluate MRSD on tasks from the DeepMind Control Suite and show that it\noutperforms prior state-of-the-art skill discovery and HRL methods, achieving\nfaster convergence and higher final performance. Our findings highlight the\nbenefits of integrating multi-resolution skills in HRL, paving the way for more\nversatile and efficient agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5206\u8fa8\u7387\u6280\u80fd\u53d1\u73b0\uff08MRSD\uff09\u7684\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6280\u80fd\u7f16\u7801\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6280\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53d7\u4eba\u7c7b\u591a\u7c92\u5ea6\u8fd0\u52a8\u6280\u80fd\u542f\u53d1\uff0c\u89e3\u51b3\u73b0\u6709\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u4ec5\u652f\u6301\u5355\u4e00\u6280\u80fd\u7684\u5c40\u9650\u6027\u3002", "method": "MRSD\u6846\u67b6\u5e76\u884c\u5b66\u4e60\u591a\u5206\u8fa8\u7387\u6280\u80fd\u7f16\u7801\u5668\uff0c\u9ad8\u5c42\u7ba1\u7406\u5668\u52a8\u6001\u9009\u62e9\u6280\u80fd\u3002", "result": "\u5728DeepMind Control Suite\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u66f4\u9ad8\u3002", "conclusion": "\u591a\u5206\u8fa8\u7387\u6280\u80fd\u96c6\u6210\u63d0\u5347\u4e86HRL\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "relevance": 70.0}}
{"id": "2505.20625", "pdf": "https://arxiv.org/pdf/2505.20625", "abs": "https://arxiv.org/abs/2505.20625", "authors": ["Sibo Xiao", "Zixin Lin", "Wenyang Gao", "Yue Zhang"], "title": "Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration", "categories": ["cs.CL"], "comment": null, "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). Existing works leverage agent-based divide-and-conquer\nmethods for processing long contexts. But these methods face crucial\nlimitations, including prohibitive accumulated latency and amplified\ninformation loss from excessive agent invocations, and the disruption of\ninherent textual dependencies by immoderate partitioning. In this paper, we\npropose a novel multi-agent framework XpandA (Expand-Agent) coupled with\nquestion-driven workflow and dynamic partitioning for robust long-context\nprocessing. XpandA overcomes these limitations through: 1) dynamic partitioning\nof long texts, which adaptively modulates the filling rate of context windows\nfor input sequences of vastly varying lengths; 2) question-guided protocol to\nupdate flat information ensembles within centralized shared memory,\nconstructing consistent inter-agent knowledge across partitions; and 3)\nselectively replaying specific partitions based on the state-tracking of\nquestion-information couples to promote the resolution of inverted-order\nstructures across partitions (e.g., flashbacks). We perform a comprehensive\nevaluation of XpandA on multiple long-context benchmarks with length varying\nfrom 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long\nsequences and its significant effectiveness in enhancing the long-context\ncapabilities of various LLMs by achieving 20\\% improvements and 1.5x inference\nspeedup over baselines of full-context, RAG and previous agent-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aXpandA\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u95ee\u9898\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e8620%\u7684\u6027\u80fd\u63d0\u5347\u548c1.5\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ee3\u7406\u7684\u5206\u6cbb\u65b9\u6cd5\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u5ef6\u8fdf\u7d2f\u79ef\u3001\u4fe1\u606f\u4e22\u5931\u548c\u6587\u672c\u4f9d\u8d56\u7834\u574f\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "XpandA\u91c7\u7528\u52a8\u6001\u5206\u533a\u3001\u95ee\u9898\u5f15\u5bfc\u534f\u8bae\u548c\u9009\u62e9\u6027\u91cd\u653e\u6280\u672f\uff0c\u4f18\u5316\u957f\u6587\u672c\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u57281k\u81f31M\u957f\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cXpandA\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u6027\u80fd\u63d0\u534720%\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u5feb1.5\u500d\u3002", "conclusion": "XpandA\u4e3aLLM\u5904\u7406\u8d85\u957f\u5e8f\u5217\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20715", "pdf": "https://arxiv.org/pdf/2505.20715", "abs": "https://arxiv.org/abs/2505.20715", "authors": ["Fuwen Luo", "Shengfeng Lou", "Chi Chen", "Ziyue Wang", "Chenliang Li", "Weizhou Shen", "Jiyue Guo", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.", "AI": {"tldr": "MUSEG\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u6233\u611f\u77e5\u7684\u591a\u6bb5\u5b9a\u4f4d\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u7ec6\u7c92\u5ea6\u65f6\u95f4\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u63d0\u51faMUSEG\u4ee5\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MUSEG\u901a\u8fc7\u591a\u6bb5\u5b9a\u4f4d\u548c\u65f6\u95f4\u6233\u611f\u77e5\uff0c\u7ed3\u5408\u5b9a\u5236\u7684\u5206\u9636\u6bb5\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u65f6\u95f4\u654f\u611f\u89c6\u9891QA\u4efb\u52a1\u4e0a\uff0cMUSEG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MUSEG\u4e3aMLLMs\u7684\u65f6\u95f4\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.20721", "pdf": "https://arxiv.org/pdf/2505.20721", "abs": "https://arxiv.org/abs/2505.20721", "authors": ["Zaijun Ye", "Chen-Song Zhang", "Wansheng Wang"], "title": "Recurrent Neural Operators: Stable Long-Term PDE Prediction", "categories": ["cs.LG", "cs.NA", "math.NA", "65M70, 68T07, 68U20"], "comment": null, "summary": "Neural operators have emerged as powerful tools for learning solution\noperators of partial differential equations. However, in time-dependent\nproblems, standard training strategies such as teacher forcing introduce a\nmismatch between training and inference, leading to compounding errors in\nlong-term autoregressive predictions. To address this issue, we propose\nRecurrent Neural Operators (RNOs)-a novel framework that integrates recurrent\ntraining into neural operator architectures. Instead of conditioning each\ntraining step on ground-truth inputs, RNOs recursively apply the operator to\ntheir own predictions over a temporal window, effectively simulating\ninference-time dynamics during training. This alignment mitigates exposure bias\nand enhances robustness to error accumulation. Theoretically, we show that\nrecurrent training can reduce the worst-case exponential error growth typical\nof teacher forcing to linear growth. Empirically, we demonstrate that\nrecurrently trained Multigrid Neural Operators significantly outperform their\nteacher-forced counterparts in long-term accuracy and stability on standard\nbenchmarks. Our results underscore the importance of aligning training with\ninference dynamics for robust temporal generalization in neural operator\nlearning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRecurrent Neural Operators (RNOs)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5faa\u73af\u8bad\u7ec3\u6574\u5408\u5230\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\u4e2d\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u4f9d\u8d56\u95ee\u9898\u4e2d\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u8bad\u7ec3\u7b56\u7565\uff08\u5982teacher forcing\uff09\u5728\u65f6\u95f4\u4f9d\u8d56\u95ee\u9898\u4e2d\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u5339\u914d\uff0c\u4ea7\u751f\u957f\u671f\u81ea\u56de\u5f52\u9884\u6d4b\u4e2d\u7684\u590d\u5408\u8bef\u5dee\u3002", "method": "RNOs\u901a\u8fc7\u9012\u5f52\u5730\u5c06\u7b97\u5b50\u5e94\u7528\u4e8e\u81ea\u8eab\u9884\u6d4b\uff0c\u6a21\u62df\u63a8\u7406\u65f6\u52a8\u6001\uff0c\u4ece\u800c\u51cf\u5c11\u66b4\u9732\u504f\u5dee\u548c\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5faa\u73af\u8bad\u7ec3\u53ef\u5c06\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6307\u6570\u8bef\u5dee\u589e\u957f\u964d\u4e3a\u7ebf\u6027\u589e\u957f\uff1b\u5b9e\u9a8c\u8868\u660eRNOs\u5728\u957f\u671f\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u663e\u8457\u4f18\u4e8eteacher-forced\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bad\u7ec3\u4e0e\u63a8\u7406\u52a8\u6001\u5bf9\u9f50\u5bf9\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u4e2d\u7a33\u5065\u65f6\u95f4\u6cdb\u5316\u7684\u91cd\u8981\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21419", "pdf": "https://arxiv.org/pdf/2505.21419", "abs": "https://arxiv.org/abs/2505.21419", "authors": ["Yifan Wang", "Kenneth P. Birman"], "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs", "categories": ["cs.AI", "cs.OS"], "comment": "Published in EuroMLSys2025", "summary": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.", "AI": {"tldr": "ARCA\u662f\u4e00\u4e2a\u591a\u6a21\u6001RAG LLM\u7cfb\u7edf\uff0c\u65e8\u5728\u7b80\u5316\u590d\u6742\u4e91\u6258\u7ba1\u5e94\u7528\u4e2d\u95ee\u9898\u7684\u8bc6\u522b\u548c\u89e3\u51b3\uff0c\u7ed3\u5408AI\u6a21\u5f0f\u5339\u914d\u80fd\u529b\u548c\u81ea\u7136\u754c\u9762\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4e91\u6258\u7ba1\u5e94\u7528\u4e2d\u6027\u80fd\u6216\u529f\u80fd\u4e0d\u7a33\u5b9a\u7684\u591a\u6839\u56e0\u95ee\u9898\uff0c\u5229\u7528AI\u6a21\u5f0f\u5339\u914d\u548c\u81ea\u7136LLM\u754c\u9762\u7b80\u5316\u95ee\u9898\u8bc6\u522b\u4e0e\u89e3\u51b3\u3002", "method": "\u5f00\u53d1\u591a\u6a21\u6001RAG LLM\u7cfb\u7edfARCA\uff0c\u7ed3\u5408AI\u6a21\u5f0f\u5339\u914d\u548c\u81ea\u7136\u754c\u9762\uff0c\u9010\u6b65\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "ARCA\u5728\u9010\u6b65\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ARCA\u901a\u8fc7\u591a\u6a21\u6001RAG LLM\u548cAI\u6a21\u5f0f\u5339\u914d\uff0c\u6709\u6548\u7b80\u5316\u590d\u6742\u95ee\u9898\u7684\u89e3\u51b3\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "relevance": 50.0}}
{"id": "2505.20633", "pdf": "https://arxiv.org/pdf/2505.20633", "abs": "https://arxiv.org/abs/2505.20633", "authors": ["Jinwu Hu", "Zhitian Zhang", "Guohao Chen", "Xutao Wen", "Chao Shuai", "Wei Luo", "Bin Xiao", "Yuanqing Li", "Mingkui Tan"], "title": "Test-Time Learning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML2025", "summary": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTLM\u7684\u6d4b\u8bd5\u65f6\u5b66\u4e60\uff08TTL\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u672a\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u6765\u63d0\u5347LLM\u5728\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4e13\u4e1a\u9886\u57df\u548c\u8bed\u8a00\u53d8\u4f53\u4e0a\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5373\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "1. \u901a\u8fc7\u6700\u5c0f\u5316\u8f93\u5165\u56f0\u60d1\u5ea6\u5b9e\u73b0\u81ea\u76d1\u7763\u6027\u80fd\u63d0\u5347\uff1b2. \u5f15\u5165\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u7b56\u7565\uff0c\u4f18\u5148\u4f18\u5316\u9ad8\u56f0\u60d1\u5ea6\u6837\u672c\uff1b3. \u4f7f\u7528LoRA\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728AdaptEval\u57fa\u51c6\u4e0a\uff0cTLM\u6bd4\u539f\u59cbLLM\u6027\u80fd\u63d0\u5347\u81f3\u5c1120%\u3002", "conclusion": "TLM\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u9886\u57df\u9002\u5e94\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.20718", "pdf": "https://arxiv.org/pdf/2505.20718", "abs": "https://arxiv.org/abs/2505.20718", "authors": ["Kui Wu", "Shuhang Xu", "Hao Chen", "Churan Wang", "Zhoujun Li", "Yizhou Wang", "Fangwei Zhong"], "title": "VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce a novel self-improving framework that enhances Embodied Visual\nTracking (EVT) with Visual-Language Models (VLMs) to address the limitations of\ncurrent active visual tracking systems in recovering from tracking failure. Our\napproach combines the off-the-shelf active tracking methods with VLMs'\nreasoning capabilities, deploying a fast visual policy for normal tracking and\nactivating VLM reasoning only upon failure detection. The framework features a\nmemory-augmented self-reflection mechanism that enables the VLM to\nprogressively improve by learning from past experiences, effectively addressing\nVLMs' limitations in 3D spatial reasoning. Experimental results demonstrate\nsignificant performance improvements, with our framework boosting success rates\nby $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based\nmethods in challenging environments. This work represents the first integration\nof VLM-based reasoning to assist EVT agents in proactive failure recovery,\noffering substantial advances for real-world robotic applications that require\ncontinuous target monitoring in dynamic, unstructured environments. Project\nwebsite: https://sites.google.com/view/evt-recovery-assistant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5177\u8eab\u89c6\u89c9\u8ddf\u8e2a\uff08EVT\uff09\u7cfb\u7edf\u5728\u8ddf\u8e2a\u5931\u8d25\u65f6\u7684\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4e3b\u52a8\u89c6\u89c9\u8ddf\u8e2a\u7cfb\u7edf\u5728\u8ddf\u8e2a\u5931\u8d25\u65f6\u6062\u590d\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u7ed3\u5408VLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u73b0\u6210\u7684\u4e3b\u52a8\u8ddf\u8e2a\u65b9\u6cd5\u548cVLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5feb\u901f\u89c6\u89c9\u7b56\u7565\u7528\u4e8e\u6b63\u5e38\u8ddf\u8e2a\uff0c\u5931\u8d25\u65f6\u6fc0\u6d3bVLM\u63a8\u7406\u3002\u6846\u67b6\u5305\u542b\u8bb0\u5fc6\u589e\u5f3a\u7684\u81ea\u53cd\u601d\u673a\u5236\uff0c\u4f7fVLM\u80fd\u4ece\u8fc7\u53bb\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6210\u529f\u7387\u6bd4RL\u65b9\u6cd5\u63d0\u9ad872%\uff0c\u6bd4PID\u65b9\u6cd5\u63d0\u9ad8220%\u3002", "conclusion": "\u9996\u6b21\u5c06VLM\u63a8\u7406\u96c6\u6210\u5230EVT\u4e2d\uff0c\u4e3a\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.20725", "pdf": "https://arxiv.org/pdf/2505.20725", "abs": "https://arxiv.org/abs/2505.20725", "authors": ["Alberto Pliego Marug\u00e1n", "Jes\u00fas M. Pinar-P\u00e9rez", "Fausto Pedro Garc\u00eda M\u00e1rquez"], "title": "A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs", "categories": ["cs.LG", "math.OC"], "comment": "Cite as: Marug\\'an, A. P., Pinar-P\\'erez, J. M., & M\\'arquez, F. P.\n  G. (2024). A reinforcement learning agent for maintenance of deteriorating\n  systems with increasingly imperfect repairs. Reliability Engineering & System\n  Safety, 252, 110466", "summary": "Efficient maintenance has always been essential for the successful\napplication of engineering systems. However, the challenges to be overcome in\nthe implementation of Industry 4.0 necessitate new paradigms of maintenance\noptimization. Machine learning techniques are becoming increasingly used in\nengineering and maintenance, with reinforcement learning being one of the most\npromising. In this paper, we propose a gamma degradation process together with\na novel maintenance model in which repairs are increasingly imperfect, i.e.,\nthe beneficial effect of system repairs decreases as more repairs are\nperformed, reflecting the degradational behavior of real-world systems. To\ngenerate maintenance policies for this system, we developed a\nreinforcement-learning-based agent using a Double Deep Q-Network architecture.\nThis agent presents two important advantages: it works without a predefined\npreventive threshold, and it can operate in a continuous degradation state\nspace. Our agent learns to behave in different scenarios, showing great\nflexibility. In addition, we performed an analysis of how changes in the main\nparameters of the environment affect the maintenance policy proposed by the\nagent. The proposed approach is demonstrated to be appropriate and to\nsignificatively improve long-run cost as compared with other common maintenance\nstrategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7ef4\u62a4\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408Gamma\u9000\u5316\u8fc7\u7a0b\u548c\u65b0\u578b\u7ef4\u62a4\u6a21\u578b\uff0c\u4f7f\u7528Double Deep Q-Network\u67b6\u6784\u751f\u6210\u7ef4\u62a4\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u957f\u671f\u6210\u672c\u3002", "motivation": "\u5de5\u4e1a4.0\u7684\u5b9e\u65bd\u9700\u8981\u65b0\u7684\u7ef4\u62a4\u4f18\u5316\u8303\u5f0f\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u7a0b\u548c\u7ef4\u62a4\u4e2d\u7684\u5e94\u7528\u5177\u6709\u6f5c\u529b\u3002", "method": "\u63d0\u51faGamma\u9000\u5316\u8fc7\u7a0b\u548c\u4e0d\u5b8c\u7f8e\u4fee\u590d\u6a21\u578b\uff0c\u5f00\u53d1\u57fa\u4e8eDouble Deep Q-Network\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u3002", "result": "\u4ee3\u7406\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7075\u6d3b\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u5e38\u89c1\u7ef4\u62a4\u7b56\u7565\uff0c\u964d\u4f4e\u4e86\u957f\u671f\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u73b0\u5b9e\u7cfb\u7edf\u9000\u5316\u884c\u4e3a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 40.0}}
{"id": "2505.21426", "pdf": "https://arxiv.org/pdf/2505.21426", "abs": "https://arxiv.org/abs/2505.21426", "authors": ["Francesco Cozzi", "Marco Pangallo", "Alan Perotti", "Andr\u00e9 Panisson", "Corrado Monti"], "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks", "categories": ["cs.AI", "cs.LG", "cs.MA", "econ.EM", "physics.soc-ph"], "comment": null, "summary": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties\nin complex systems. In ABMs, agent behaviors are governed by local interactions\nand stochastic rules. However, these rules are, in general, non-differentiable,\nlimiting the use of gradient-based methods for optimization, and thus\nintegration with real-world data. We propose a novel framework to learn a\ndifferentiable surrogate of any ABM by observing its generated data. Our method\ncombines diffusion models to capture behavioral stochasticity and graph neural\nnetworks to model agent interactions. Distinct from prior surrogate approaches,\nour method introduces a fundamental shift: rather than approximating\nsystem-level outputs, it models individual agent behavior directly, preserving\nthe decentralized, bottom-up dynamics that define ABMs. We validate our\napproach on two ABMs (Schelling's segregation model and a Predator-Prey\necosystem) showing that it replicates individual-level patterns and accurately\nforecasts emergent dynamics beyond training. Our results demonstrate the\npotential of combining diffusion models and graph learning for data-driven ABM\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u53ef\u5fae\u5206\u4ee3\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\uff08ABM\uff09\u7684\u4e2a\u4f53\u884c\u4e3a\uff0c\u4fdd\u7559\u5176\u81ea\u4e0b\u800c\u4e0a\u7684\u52a8\u6001\u7279\u6027\u3002", "motivation": "\u89e3\u51b3ABM\u4e2d\u89c4\u5219\u4e0d\u53ef\u5fae\u5206\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u4e0e\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u7ed3\u5408\uff0c\u66f4\u597d\u5730\u5229\u7528\u771f\u5b9e\u6570\u636e\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u6355\u6349\u884c\u4e3a\u968f\u673a\u6027\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u4ee3\u7406\u4ea4\u4e92\uff0c\u76f4\u63a5\u6a21\u62df\u4e2a\u4f53\u884c\u4e3a\u800c\u975e\u7cfb\u7edf\u7ea7\u8f93\u51fa\u3002", "result": "\u5728Schelling\u9694\u79bb\u6a21\u578b\u548c\u6355\u98df\u8005-\u730e\u7269\u751f\u6001\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u80fd\u590d\u5236\u4e2a\u4f53\u6a21\u5f0f\u5e76\u51c6\u786e\u9884\u6d4b\u8bad\u7ec3\u5916\u7684\u6d8c\u73b0\u52a8\u6001\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e0e\u56fe\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u6570\u636e\u9a71\u52a8\u7684ABM\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.20645", "pdf": "https://arxiv.org/pdf/2505.20645", "abs": "https://arxiv.org/abs/2505.20645", "authors": ["Kai Chen", "Zihao He", "Taiwei Shi", "Kristina Lerman"], "title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Steerability, or the ability of large language models (LLMs) to adapt outputs\nto align with diverse community-specific norms, perspectives, and communication\nstyles, is critical for real-world applications but remains under-evaluated. We\nintroduce Steer-Bench, a benchmark for assessing population-specific steering\nusing contrasting Reddit communities. Covering 30 contrasting subreddit pairs\nacross 19 domains, Steer-Bench includes over 10,000 instruction-response pairs\nand validated 5,500 multiple-choice question with corresponding silver labels\nto test alignment with diverse community norms. Our evaluation of 13 popular\nLLMs using Steer-Bench reveals that while human experts achieve an accuracy of\n81% with silver labels, the best-performing models reach only around 65%\naccuracy depending on the domain and configuration. Some models lag behind\nhuman-level alignment by over 15 percentage points, highlighting significant\ngaps in community-sensitive steerability. Steer-Bench is a benchmark to\nsystematically assess how effectively LLMs understand community-specific\ninstructions, their resilience to adversarial steering attempts, and their\nability to accurately represent diverse cultural and ideological perspectives.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Steer-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u793e\u533a\u89c4\u8303\u4e0b\u8f93\u51fa\u9002\u5e94\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u4e8630\u4e2a\u5bf9\u6bd4\u5b50Reddit\u793e\u533a\uff0c\u6d4b\u8bd5\u4e8613\u4e2a\u6d41\u884cLLMs\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLMs\u5728\u9002\u5e94\u591a\u6837\u793e\u533a\u89c4\u8303\u3001\u89c2\u70b9\u548c\u6c9f\u901a\u98ce\u683c\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8fd9\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u6784\u5efaSteer-Bench\u57fa\u51c6\uff0c\u5305\u542b30\u4e2a\u5bf9\u6bd4\u5b50Reddit\u793e\u533a\u768410,000+\u6307\u4ee4-\u54cd\u5e94\u5bf9\u548c5,500+\u591a\u9009\u95ee\u9898\uff0c\u6d4b\u8bd5LLMs\u7684\u793e\u533a\u654f\u611f\u6027\u548c\u5bf9\u6297\u6027\u5f15\u5bfc\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4eba\u7c7b\u4e13\u5bb6\u51c6\u786e\u7387\u4e3a81%\uff0c\u800c\u6700\u4f73LLM\u6a21\u578b\u4ec5\u8fbe\u523065%\uff0c\u67d0\u4e9b\u6a21\u578b\u843d\u540e\u4eba\u7c7b\u6c34\u5e7315\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5f53\u524dLLMs\u5728\u793e\u533a\u654f\u611f\u5f15\u5bfc\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0cSteer-Bench\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.20723", "pdf": "https://arxiv.org/pdf/2505.20723", "abs": "https://arxiv.org/abs/2505.20723", "authors": ["Pascal Zwick", "Nils Friederich", "Maximilian Beichter", "Lennart Hilbert", "Ralf Mikut", "Oliver Bringmann"], "title": "LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation", "categories": ["cs.CV", "cs.LG", "I.4; I.2"], "comment": null, "summary": "Enhancing the efficiency of high-quality image generation using Diffusion\nModels (DMs) is a significant challenge due to the iterative nature of the\nprocess. Flow Matching (FM) is emerging as a powerful generative modeling\nparadigm based on a simulation-free training objective instead of a score-based\none used in DMs. Typical FM approaches rely on a Gaussian distribution prior,\nwhich induces curved, conditional probability paths between the prior and\ntarget data distribution. These curved paths pose a challenge for the Ordinary\nDifferential Equation (ODE) solver, requiring a large number of inference calls\nto the flow prediction network. To address this issue, we present Learned\nDistribution-guided Flow Matching (LeDiFlow), a novel scalable method for\ntraining FM-based image generation models using a better-suited prior\ndistribution learned via a regression-based auxiliary model. By initializing\nthe ODE solver with a prior closer to the target data distribution, LeDiFlow\nenables the learning of more computationally tractable probability paths. These\npaths directly translate to fewer solver steps needed for high-quality image\ngeneration at inference time. Our method utilizes a State-Of-The-Art (SOTA)\ntransformer architecture combined with latent space sampling and can be trained\non a consumer workstation. We empirically demonstrate that LeDiFlow remarkably\noutperforms the respective FM baselines. For instance, when operating directly\non pixels, our model accelerates inference by up to 3.75x compared to the\ncorresponding pixel-space baseline. Simultaneously, our latent FM model\nenhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy\n(CMMD) metric against its respective baseline.", "AI": {"tldr": "LeDiFlow\u662f\u4e00\u79cd\u57fa\u4e8eFlow Matching\uff08FM\uff09\u7684\u65b0\u578b\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u66f4\u9002\u5408\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u56e0\u5176\u8fed\u4ee3\u6027\u8d28\u5bfc\u81f4\u751f\u6210\u6548\u7387\u4f4e\u4e0b\uff0c\u800cFM\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5176\u57fa\u4e8e\u9ad8\u65af\u5148\u9a8c\u7684\u8def\u5f84\u5bf9ODE\u6c42\u89e3\u5668\u63d0\u51fa\u4e86\u6311\u6218\u3002LeDiFlow\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5148\u9a8c\u5206\u5e03\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LeDiFlow\u901a\u8fc7\u8f85\u52a9\u56de\u5f52\u6a21\u578b\u5b66\u4e60\u66f4\u9002\u5408\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u521d\u59cb\u5316ODE\u6c42\u89e3\u5668\uff0c\u4ece\u800c\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u3002\u7ed3\u5408SOTA Transformer\u67b6\u6784\u548c\u6f5c\u5728\u7a7a\u95f4\u91c7\u6837\uff0c\u53ef\u5728\u6d88\u8d39\u7ea7\u5de5\u4f5c\u7ad9\u4e0a\u8bad\u7ec3\u3002", "result": "LeDiFlow\u5728\u50cf\u7d20\u7a7a\u95f4\u63a8\u7406\u901f\u5ea6\u63d0\u53473.75\u500d\uff0c\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u5728CMMD\u6307\u6807\u4e0a\u56fe\u50cf\u8d28\u91cf\u63d0\u53471.32\u500d\u3002", "conclusion": "LeDiFlow\u901a\u8fc7\u4f18\u5316\u5148\u9a8c\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86FM\u6a21\u578b\u7684\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002", "relevance": 40.0}}
{"id": "2505.20734", "pdf": "https://arxiv.org/pdf/2505.20734", "abs": "https://arxiv.org/abs/2505.20734", "authors": ["Zhuoyu Cheng", "Kohei Hatano", "Eiji Takimoto"], "title": "Adversarial bandit optimization for approximately linear functions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u51f8\u975e\u5149\u6ed1\u51fd\u6570\u7684bandit\u4f18\u5316\u95ee\u9898\uff0c\u635f\u5931\u51fd\u6570\u7531\u7ebf\u6027\u90e8\u5206\u548c\u6270\u52a8\u7ec4\u6210\uff0c\u63d0\u51fa\u4e86\u671f\u671b\u548c\u9ad8\u6982\u7387\u7684\u9057\u61be\u754c\uff0c\u5e76\u6539\u8fdb\u4e86bandit\u7ebf\u6027\u4f18\u5316\u7684\u9ad8\u6982\u7387\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u975e\u51f8\u975e\u5149\u6ed1\u51fd\u6570\u7684bandit\u4f18\u5316\u95ee\u9898\uff0c\u63a2\u7d22\u6270\u52a8\u5bf9\u4f18\u5316\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u635f\u5931\u51fd\u6570\u7684\u7ed3\u6784\uff08\u7ebf\u6027\u90e8\u5206\u52a0\u6270\u52a8\uff09\uff0c\u63d0\u51fa\u671f\u671b\u548c\u9ad8\u6982\u7387\u7684\u9057\u61be\u754c\u3002", "result": "\u7ed9\u51fa\u4e86\u671f\u671b\u548c\u9ad8\u6982\u7387\u7684\u9057\u61be\u754c\uff0c\u5e76\u6539\u8fdb\u4e86bandit\u7ebf\u6027\u4f18\u5316\u7684\u9ad8\u6982\u7387\u9057\u61be\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u51f8\u975e\u5149\u6ed1\u51fd\u6570\u7684bandit\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u6269\u5c55\u4e86\u5bf9\u6270\u52a8\u7684\u7406\u89e3\u3002", "relevance": 30.0}}
{"id": "2505.21427", "pdf": "https://arxiv.org/pdf/2505.21427", "abs": "https://arxiv.org/abs/2505.21427", "authors": ["Xianling Mu", "Joseph Ternasky", "Fuat Alican", "Yigit Ihlamur"], "title": "Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u900f\u660e\u4e14\u6570\u636e\u9ad8\u6548\u7684\u521d\u521b\u4f01\u4e1a\u6295\u8d44\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u521d\u521b\u4f01\u4e1a\u6295\u8d44\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u89e3\u91ca\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7b56\u7565\u5d4c\u5165LLM\u63d0\u793a\uff0c\u7ed3\u5408\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5faa\u73af\uff0c\u5b9e\u73b0\u65e0\u9700\u68af\u5ea6\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u3002", "result": "\u9884\u6d4b\u521d\u521b\u4f01\u4e1a\u6210\u529f\u7684\u51c6\u786e\u7387\u8fdc\u8d85\u73b0\u6709\u57fa\u51c6\uff0c\u6bd4\u968f\u673a\u673a\u4f1a\u9ad820\u500d\uff0c\u6bd4\u9876\u7ea7\u98ce\u6295\u516c\u53f8\u9ad87.1\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u98ce\u9669\u6295\u8d44\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002", "relevance": 60.0}}
{"id": "2505.20650", "pdf": "https://arxiv.org/pdf/2505.20650", "abs": "https://arxiv.org/abs/2505.20650", "authors": ["Yan Wang", "Yang Ren", "Lingfei Qian", "Xueqing Peng", "Keyi Wang", "Yi Han", "Dongji Feng", "Xiao-Yang Liu", "Jimin Huang", "Qianqian Xie"], "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.", "AI": {"tldr": "FinTagging\u662f\u9996\u4e2a\u5168\u8303\u56f4\u3001\u8868\u683c\u611f\u77e5\u7684XBRL\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728XBRL\u8d22\u52a1\u62a5\u544a\u4e2d\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u548c\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5c06XBRL\u6807\u8bb0\u7b80\u5316\u4e3a\u6241\u5e73\u591a\u7c7b\u5206\u7c7b\uff0c\u4ec5\u5173\u6ce8\u53d9\u8ff0\u6587\u672c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002FinTagging\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u3002", "method": "\u5c06XBRL\u6807\u8bb0\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1aFinNI\uff08\u91d1\u878d\u5b9e\u4f53\u63d0\u53d6\uff09\u548cFinCL\uff08\u5206\u7c7b\u9a71\u52a8\u7684\u6982\u5ff5\u5bf9\u9f50\uff09\uff0c\u8bc4\u4f30LLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u4fe1\u606f\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u5bf9\u9f50\uff08\u5c24\u5176\u662f\u533a\u5206\u76f8\u8fd1\u5206\u7c7b\u6761\u76ee\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u73b0\u6709LLM\u65e0\u6cd5\u5b8c\u5168\u81ea\u52a8\u5316XBRL\u6807\u8bb0\uff0c\u9700\u6539\u8fdb\u8bed\u4e49\u63a8\u7406\u548c\u6a21\u5f0f\u611f\u77e5\u5efa\u6a21\u3002", "relevance": 70.0}}
{"id": "2505.20729", "pdf": "https://arxiv.org/pdf/2505.20729", "abs": "https://arxiv.org/abs/2505.20729", "authors": ["Xiangyu Sun", "Runnan Chen", "Mingming Gong", "Dong Xu", "Tongliang Liu"], "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view scene reconstruction often faces significant challenges due to\nthe constraints imposed by limited observational data. These limitations result\nin incomplete information, leading to suboptimal reconstructions using existing\nmethodologies. To address this, we present Intern-GS, a novel approach that\neffectively leverages rich prior knowledge from vision foundation models to\nenhance the process of sparse-view Gaussian Splatting, thereby enabling\nhigh-quality scene reconstruction. Specifically, Intern-GS utilizes vision\nfoundation models to guide both the initialization and the optimization process\nof 3D Gaussian splatting, effectively addressing the limitations of sparse\ninputs. In the initialization process, our method employs DUSt3R to generate a\ndense and non-redundant gaussian point cloud. This approach significantly\nalleviates the limitations encountered by traditional structure-from-motion\n(SfM) methods, which often struggle under sparse-view constraints. During the\noptimization process, vision foundation models predict depth and appearance for\nunobserved views, refining the 3D Gaussians to compensate for missing\ninformation in unseen regions. Extensive experiments demonstrate that Intern-GS\nachieves state-of-the-art rendering quality across diverse datasets, including\nboth forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and\nTemples.", "AI": {"tldr": "Intern-GS\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u56e0\u6570\u636e\u6709\u9650\u5bfc\u81f4\u4fe1\u606f\u4e0d\u5b8c\u6574\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408DUSt3R\u751f\u6210\u5bc6\u96c6\u9ad8\u65af\u70b9\u4e91\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u672a\u89c2\u5bdf\u89c6\u56fe\u7684\u6df1\u5ea6\u548c\u5916\u89c2\uff0c\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u3002", "result": "\u5728LLFF\u3001DTU\u7b49\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "Intern-GS\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "relevance": 40.0}}
{"id": "2505.20739", "pdf": "https://arxiv.org/pdf/2505.20739", "abs": "https://arxiv.org/abs/2505.20739", "authors": ["Kunpeng Zhao", "Asahi Miyazaki", "Tsuyoshi Okita"], "title": "Detecting Informative Channels: ActionFormer", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Human Activity Recognition (HAR) has recently witnessed advancements with\nTransformer-based models. Especially, ActionFormer shows us a new perspectives\nfor HAR in the sense that this approach gives us additional outputs which\ndetect the border of the activities as well as the activity labels.\nActionFormer was originally proposed with its input as image/video. However,\nthis was converted to with its input as sensor signals as well. We analyze this\nextensively in terms of deep learning architectures. Based on the report of\nhigh temporal dynamics which limits the model's ability to capture subtle\nchanges effectively and of the interdependencies between the spatial and\ntemporal features. We propose the modified ActionFormer which will decrease\nthese defects for sensor signals. The key to our approach lies in accordance\nwith the Sequence-and-Excitation strategy to minimize the increase in\nadditional parameters and opt for the swish activation function to retain the\ninformation about direction in the negative range. Experiments on the WEAR\ndataset show that our method achieves substantial improvement of a 16.01\\% in\nterms of average mAP for inertial data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ActionFormer\u6a21\u578b\uff0c\u7528\u4e8e\u4f20\u611f\u5668\u4fe1\u53f7\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\uff0c\u901a\u8fc7Sequence-and-Excitation\u7b56\u7565\u548cswish\u6fc0\u6d3b\u51fd\u6570\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u611f\u5668\u4fe1\u53f7\u5728HAR\u4e2d\u56e0\u9ad8\u65f6\u95f4\u52a8\u6001\u6027\u548c\u65f6\u7a7a\u7279\u5f81\u4f9d\u8d56\u6027\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\u3002", "method": "\u6539\u8fdbActionFormer\uff0c\u91c7\u7528Sequence-and-Excitation\u7b56\u7565\u548cswish\u6fc0\u6d3b\u51fd\u6570\u3002", "result": "\u5728WEAR\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747mAP\u63d0\u9ad8\u4e8616.01%\u3002", "conclusion": "\u6539\u8fdb\u7684ActionFormer\u5728\u4f20\u611f\u5668\u4fe1\u53f7HAR\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u53c2\u6570\u589e\u52a0\u8f83\u5c11\u3002", "relevance": 40.0}}
{"id": "2505.21486", "pdf": "https://arxiv.org/pdf/2505.21486", "abs": "https://arxiv.org/abs/2505.21486", "authors": ["Yang Yang", "Jiemin Wu", "Yutao Yue"], "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming", "categories": ["cs.AI"], "comment": null, "summary": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u667a\u80fd\u4f53LLM\u548c\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\uff08ILP\uff09\u7684\u65b0\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u7b26\u53f7\u5316\u8bcd\u6c47\u548c\u5173\u7cfb\u6a21\u677f\uff0c\u514b\u670d\u4f20\u7edfILP\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b26\u53f7\u7ed3\u6784\u548c\u7eafLLM\u65b9\u6cd5\u7684\u566a\u58f0\u654f\u611f\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u5728\u5f00\u653e\u73af\u5883\u4e2d\u751f\u6210\u9c81\u68d2\u5047\u8bbe\u5bf9AI\u8ba4\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfILP\u4f9d\u8d56\u4e13\u5bb6\u9884\u5b9a\u4e49\u7b26\u53f7\u7ed3\u6784\uff0c\u800c\u7eafLLM\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\u3002", "method": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53LLM\u548cILP\uff0cLLM\u667a\u80fd\u4f53\u4ece\u539f\u59cb\u6587\u672c\u6570\u636e\u81ea\u52a8\u5b9a\u4e49\u7b26\u53f7\u5316\u8bcd\u6c47\u548c\u5173\u7cfb\u6a21\u677f\uff0c\u6307\u5bfcILP\u6c42\u89e3\u5668\u751f\u6210\u53ef\u89e3\u91ca\u89c4\u5219\u3002", "result": "\u5728\u591a\u6837\u5316\u6311\u6218\u6027\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u9a8c\u8bc1\u7684\u5047\u8bbe\u751f\u6210\u5f00\u8f9f\u65b0\u8def\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u514b\u670d\u4e86\u4f20\u7edfILP\u548c\u7eafLLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u5047\u8bbe\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.20654", "pdf": "https://arxiv.org/pdf/2505.20654", "abs": "https://arxiv.org/abs/2505.20654", "authors": ["Yi Zhu", "Xin Zou", "Xindong Wu"], "title": "Chinese Cyberbullying Detection: Dataset, Method, and Validation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing cyberbullying detection benchmarks were organized by the polarity of\nspeech, such as \"offensive\" and \"non-offensive\", which were essentially hate\nspeech detection. However, in the real world, cyberbullying often attracted\nwidespread social attention through incidents. To address this problem, we\npropose a novel annotation method to construct a cyberbullying dataset that\norganized by incidents. The constructed CHNCI is the first Chinese\ncyberbullying incident detection dataset, which consists of 220,676 comments in\n91 incidents. Specifically, we first combine three cyberbullying detection\nmethods based on explanations generation as an ensemble method to generate the\npseudo labels, and then let human annotators judge these labels. Then we\npropose the evaluation criteria for validating whether it constitutes a\ncyberbullying incident. Experimental results demonstrate that the constructed\ndataset can be a benchmark for the tasks of cyberbullying detection and\nincident prediction. To the best of our knowledge, this is the first study for\nthe Chinese cyberbullying incident detection task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u4e2d\u6587\u7f51\u7edc\u6b3a\u51cc\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u4e2d\u6587\u7f51\u7edc\u6b3a\u51cc\u4e8b\u4ef6\u68c0\u6d4b\u6570\u636e\u96c6CHNCI\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u57fa\u51c6\u591a\u57fa\u4e8e\u8a00\u8bba\u6781\u6027\uff08\u5982\u2018\u653b\u51fb\u6027\u2019\u548c\u2018\u975e\u653b\u51fb\u6027\u2019\uff09\uff0c\u800c\u73b0\u5b9e\u4e2d\u7f51\u7edc\u6b3a\u51cc\u5e38\u901a\u8fc7\u4e8b\u4ef6\u5f15\u53d1\u5e7f\u6cdb\u5173\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u57fa\u4e8e\u89e3\u91ca\u751f\u6210\u7684\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u65b9\u6cd5\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4eba\u5de5\u6807\u6ce8\u540e\u63d0\u51fa\u4e8b\u4ef6\u6784\u6210\u6807\u51c6\uff0c\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u7684CHNCI\u6570\u636e\u96c6\u5305\u542b220,676\u6761\u8bc4\u8bba\u548c91\u4e2a\u4e8b\u4ef6\uff0c\u53ef\u4f5c\u4e3a\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u548c\u4e8b\u4ef6\u9884\u6d4b\u7684\u57fa\u51c6\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u6587\u7f51\u7edc\u6b3a\u51cc\u4e8b\u4ef6\u68c0\u6d4b\u7684\u7814\u7a76\uff0c\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "relevance": 30.0}}
{"id": "2505.20744", "pdf": "https://arxiv.org/pdf/2505.20744", "abs": "https://arxiv.org/abs/2505.20744", "authors": ["Hao Zhang", "Zhan Zhuang", "Xuehao Wang", "Xiaodong Yang", "Yu Zhang"], "title": "MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Human Activity Recognition (HAR) with wearable sensors is challenged by\nlimited interpretability, which significantly impacts cross-dataset\ngeneralization. To address this challenge, we propose Motion-Primitive\nTransformer (MoPFormer), a novel self-supervised framework that enhances\ninterpretability by tokenizing inertial measurement unit signals into\nsemantically meaningful motion primitives and leverages a Transformer\narchitecture to learn rich temporal representations. MoPFormer comprises\ntwo-stages. first stage is to partition multi-channel sensor streams into short\nsegments and quantizing them into discrete \"motion primitive\" codewords, while\nthe second stage enriches those tokenized sequences through a context-aware\nembedding module and then processes them with a Transformer encoder. The\nproposed MoPFormer can be pre-trained using a masked motion-modeling objective\nthat reconstructs missing primitives, enabling it to develop robust\nrepresentations across diverse sensor configurations. Experiments on six HAR\nbenchmarks demonstrate that MoPFormer not only outperforms state-of-the-art\nmethods but also successfully generalizes across multiple datasets. Most\nimportantly, the learned motion primitives significantly enhance both\ninterpretability and cross-dataset performance by capturing fundamental\nmovement patterns that remain consistent across similar activities regardless\nof dataset origin.", "AI": {"tldr": "MoPFormer\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4f20\u611f\u5668\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8bed\u4e49\u5316\u8fd0\u52a8\u57fa\u5143\u5e76\u5229\u7528Transformer\u5b66\u4e60\u65f6\u95f4\u8868\u793a\uff0c\u63d0\u5347\u4e86\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3HAR\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5c06\u4f20\u611f\u5668\u4fe1\u53f7\u5206\u5272\u5e76\u91cf\u5316\u4e3a\u8fd0\u52a8\u57fa\u5143\uff1b2\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5d4c\u5165\u6a21\u5757\u548cTransformer\u7f16\u7801\u5668\u5904\u7406\u3002", "result": "\u5728\u516d\u4e2aHAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u8de8\u6570\u636e\u96c6\u6027\u80fd\u3002", "conclusion": "MoPFormer\u901a\u8fc7\u6355\u6349\u57fa\u7840\u8fd0\u52a8\u6a21\u5f0f\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 30.0}}
{"id": "2505.20742", "pdf": "https://arxiv.org/pdf/2505.20742", "abs": "https://arxiv.org/abs/2505.20742", "authors": ["Sunwoo Kim", "Soo Yong Lee", "Jaemin Yoo", "Kijung Shin"], "title": "'Hello, World!': Making GNNs Talk with LLMs", "categories": ["cs.LG"], "comment": "Code and datasets are in https://github.com/kswoo97/GLN-Code", "summary": "While graph neural networks (GNNs) have shown remarkable performance across\ndiverse graph-related tasks, their high-dimensional hidden representations\nrender them black boxes. In this work, we propose Graph Lingual Network (GLN),\na GNN built on large language models (LLMs), with hidden representations in the\nform of human-readable text. Through careful prompt design, GLN incorporates\nnot only the message passing module of GNNs but also advanced GNN techniques,\nincluding graph attention and initial residual connection. The\ncomprehensibility of GLN's hidden representations enables an intuitive analysis\nof how node representations change (1) across layers and (2) under advanced GNN\ntechniques, shedding light on the inner workings of GNNs. Furthermore, we\ndemonstrate that GLN achieves strong zero-shot performance on node\nclassification and link prediction, outperforming existing LLM-based baseline\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GLN\uff09\uff0c\u5176\u9690\u85cf\u8868\u793a\u4ee5\u4eba\u7c7b\u53ef\u8bfb\u7684\u6587\u672c\u5f62\u5f0f\u5448\u73b0\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u56e0\u9ad8\u7ef4\u9690\u85cf\u8868\u793a\u800c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u5c06GNN\u7684\u6d88\u606f\u4f20\u9012\u6a21\u5757\u4e0e\u9ad8\u7ea7\u6280\u672f\uff08\u5982\u56fe\u6ce8\u610f\u529b\u548c\u521d\u59cb\u6b8b\u5dee\u8fde\u63a5\uff09\u7ed3\u5408\u5230GLN\u4e2d\u3002", "result": "GLN\u5728\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GLN\u4e0d\u4ec5\u63d0\u5347\u4e86GNN\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2212.13462", "pdf": "https://arxiv.org/pdf/2212.13462", "abs": "https://arxiv.org/abs/2212.13462", "authors": ["Abdullah Hamdi", "Faisal AlZahrani", "Silvio Giancola", "Bernard Ghanem"], "title": "MVTN: Learning Multi-View Transformations for 3D Understanding", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "under review journal extension for the ICCV 2021 paper\n  arXiv:2011.13244", "summary": "Multi-view projection techniques have shown themselves to be highly effective\nin achieving top-performing results in the recognition of 3D shapes. These\nmethods involve learning how to combine information from multiple view-points.\nHowever, the camera view-points from which these views are obtained are often\nfixed for all shapes. To overcome the static nature of current multi-view\ntechniques, we propose learning these view-points. Specifically, we introduce\nthe Multi-View Transformation Network (MVTN), which uses differentiable\nrendering to determine optimal view-points for 3D shape recognition. As a\nresult, MVTN can be trained end-to-end with any multi-view network for 3D shape\nclassification. We integrate MVTN into a novel adaptive multi-view pipeline\nthat is capable of rendering both 3D meshes and point clouds. Our approach\ndemonstrates state-of-the-art performance in 3D classification and shape\nretrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).\nFurther analysis indicates that our approach exhibits improved robustness to\nocclusion compared to other methods. We also investigate additional aspects of\nMVTN, such as 2D pretraining and its use for segmentation. To support further\nresearch in this area, we have released MVTorch, a PyTorch library for 3D\nunderstanding and generation using multi-view projections.", "AI": {"tldr": "MVTN\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u5b66\u4e60\u6700\u4f18\u89c6\u89d2\uff0c\u63d0\u53473D\u5f62\u72b6\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u591a\u89c6\u89d2\u6280\u672f\u89c6\u89d2\u56fa\u5b9a\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faMVTN\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u81ea\u9002\u5e94\u591a\u89c6\u89d2\u6d41\u7a0b\uff0c\u652f\u6301\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728ModelNet40\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6297\u906e\u6321\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "MVTN\u4e3a3D\u7406\u89e3\u63d0\u4f9b\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4e86MVTorch\u5e93\u3002", "relevance": 40.0}}
{"id": "2505.20658", "pdf": "https://arxiv.org/pdf/2505.20658", "abs": "https://arxiv.org/abs/2505.20658", "authors": ["Yue Fang", "Zhi Jin", "Jie An", "Hongshen Chen", "Xiaohong Chen", "Naijun Zhan"], "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge", "categories": ["cs.CL"], "comment": "13 pages, 5 figures, published to ACL", "summary": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSTL-DivEn\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b16,000\u4e2a\u591a\u6837\u5316\u7684NL-STL\u6837\u672c\uff0c\u5e76\u63d0\u51fa\u4e86KGST\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5230STL\u7684\u8f6c\u6362\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u7684\u8017\u65f6\u548c\u6613\u9519\u95ee\u9898\uff0c\u540c\u65f6\u586b\u8865\u81ea\u52a8\u8f6c\u6362\u9886\u57df\u7f3a\u4e4f\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "1. \u521b\u5efa\u5c0f\u89c4\u6a21\u79cd\u5b50\u96c6\uff1b2. \u901a\u8fc7\u805a\u7c7b\u548cLLM\u751f\u6210\u591a\u6837\u5316\u6837\u672c\uff1b3. \u5f15\u5165KGST\u6846\u67b6\uff0c\u57fa\u4e8e\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u751f\u6210-\u4f18\u5316\u8f6c\u6362\u3002", "result": "STL-DivEn\u6570\u636e\u96c6\u591a\u6837\u6027\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\uff0cKGST\u6846\u67b6\u5728\u8f6c\u6362\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "STL-DivEn\u548cKGST\u4e3aNL-STL\u8f6c\u6362\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u548c\u65b9\u6cd5\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.20753", "pdf": "https://arxiv.org/pdf/2505.20753", "abs": "https://arxiv.org/abs/2505.20753", "authors": ["Yufei Zhan", "Hongyin Zhao", "Yousong Zhu", "Shurong Zheng", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "title": "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": "Tech report", "summary": "Large Multimodal Models (LMMs) have recently demonstrated remarkable visual\nunderstanding performance on both vision-language and vision-centric tasks.\nHowever, they often fall short in integrating advanced, task-specific\ncapabilities for compositional reasoning, which hinders their progress toward\ntruly competent general vision models. To address this, we present a unified\nvisual reasoning mechanism that enables LMMs to solve complicated compositional\nproblems by leveraging their intrinsic capabilities (e.g. grounding and visual\nunderstanding capabilities). Different from the previous shortcut learning\nmechanism, our approach introduces a human-like\nunderstanding-thinking-answering process, allowing the model to complete all\nsteps in a single pass forwarding without the need for multiple inferences or\nexternal tools. This design bridges the gap between foundational visual\ncapabilities and general question answering, encouraging LMMs to generate\nfaithful and traceable responses for complex visual reasoning. Meanwhile, we\ncurate 334K visual instruction samples covering both general scenes and\ntext-rich scenes and involving multiple foundational visual capabilities. Our\ntrained model, Griffon-R, has the ability of end-to-end automatic\nunderstanding, self-thinking, and reasoning answers. Comprehensive experiments\nshow that Griffon-R not only achieves advancing performance on complex visual\nreasoning benchmarks including VSR and CLEVR, but also enhances multimodal\ncapabilities across various benchmarks like MMBench and ScienceQA. Data,\nmodels, and codes will be release at\nhttps://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u5408LMMs\u7684\u5185\u5728\u80fd\u529b\uff08\u5982\u57fa\u7840\u89c6\u89c9\u7406\u89e3\uff09\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002\u6a21\u578bGriffon-R\u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u5176\u901a\u7528\u89c6\u89c9\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u4ea4\u4e92\u5f0f\u7684\u7406\u89e3-\u601d\u8003-\u56de\u7b54\u8fc7\u7a0b\uff0c\u652f\u6301\u5355\u6b21\u524d\u5411\u63a8\u7406\uff0c\u65e0\u9700\u591a\u6b21\u63a8\u65ad\u6216\u5916\u90e8\u5de5\u5177\u3002", "result": "Griffon-R\u5728VSR\u3001CLEVR\u7b49\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728MMBench\u548cScienceQA\u7b49\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e5f\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u57fa\u7840\u89c6\u89c9\u80fd\u529b\u4e0e\u901a\u7528\u95ee\u7b54\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86LMMs\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "relevance": 70.0}}
{"id": "2505.20755", "pdf": "https://arxiv.org/pdf/2505.20755", "abs": "https://arxiv.org/abs/2505.20755", "authors": ["Yifei Wang", "Weimin Bai", "Colin Zhang", "Debing Zhang", "Weijian Luo", "He Sun"], "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a\ntheory-driven framework which we name the \\textbf{\\emph{Uni-Instruct}}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\n$f$-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded $f$-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded $f$-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\textbf{\\emph{1.46}} for unconditional\ngeneration and \\textbf{\\emph{1.38}} for conditional generation. On the\nImageNet-$64\\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\textbf{\\emph{1.02}}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.", "AI": {"tldr": "Uni-Instruct\u7edf\u4e00\u4e8610\u591a\u79cd\u4e00\u6b65\u6269\u6563\u84b8\u998f\u65b9\u6cd5\uff0c\u63d0\u51fa\u7406\u8bba\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55f-\u6563\u5ea6\u7406\u8bba\u89e3\u51b3\u4e0d\u53ef\u884c\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u6846\u67b6\u7edf\u4e00\u73b0\u6709\u4e00\u6b65\u6269\u6563\u84b8\u998f\u65b9\u6cd5\uff0c\u89e3\u51b3f-\u6563\u5ea6\u6269\u5c55\u7684\u4e0d\u53ef\u884c\u6027\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u6027\u80fd\u3002", "method": "\u63d0\u51faUni-Instruct\u6846\u67b6\uff0c\u57fa\u4e8e\u6269\u5c55f-\u6563\u5ea6\u7406\u8bba\uff0c\u8bbe\u8ba1\u7b49\u6548\u4e14\u53ef\u884c\u7684\u635f\u5931\u51fd\u6570\uff0c\u8bad\u7ec3\u4e00\u6b65\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728CIFAR10\u548cImageNet-64\u00d764\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18FID\u503c\uff081.46\u548c1.02\uff09\uff0c\u5728\u6587\u672c\u52303D\u751f\u6210\u4efb\u52a1\u4e2d\u7565\u4f18\u4e8eSDS\u548cVSD\u3002", "conclusion": "Uni-Instruct\u4e3a\u4e00\u6b65\u6269\u6563\u84b8\u998f\u548c\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u57fa\u7840\uff0c\u5177\u6709\u6f5c\u5728\u7684\u7814\u7a76\u4ef7\u503c\u3002", "relevance": 30.0}}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039", "abs": "https://arxiv.org/abs/2505.00039", "authors": ["Hudson de Martim"], "title": "Graph RAG for Legal Norms: A Hierarchical and Temporal Approach", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nadvance the field of Artificial Intelligence applied to Law, creating\nopportunities for more effective systems in legal research, legislative\nanalysis, and decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6cd5\u5f8b\u89c4\u8303\u5206\u6790\u7684Graph RAG\u65b9\u6cd5\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u672c\u7247\u6bb5\uff0c\u4ee5\u5904\u7406\u6cd5\u5f8b\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u3002", "motivation": "\u6cd5\u5f8b\u89c4\u8303\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u3001\u5f15\u7528\u7f51\u7edc\u548c\u591a\u7248\u672c\u7279\u70b9\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5176\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6587\u672c\u7247\u6bb5\uff0c\u6784\u5efa\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u95f4\u6f14\u5316\u7684\u6cd5\u5f8b\u77e5\u8bc6\u8868\u793a\u3002", "result": "Graph RAG\u80fd\u591f\u66f4\u6709\u6548\u5730\u8868\u793a\u6cd5\u5f8b\u77e5\u8bc6\uff0c\u652f\u6301\u6cd5\u5f8b\u7814\u7a76\u548c\u51b3\u7b56\u3002", "conclusion": "Graph RAG\u4e3a\u6cd5\u5f8b\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.20660", "pdf": "https://arxiv.org/pdf/2505.20660", "abs": "https://arxiv.org/abs/2505.20660", "authors": ["Qinzhuo Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents have gained substantial attention due\nto their impressive capabilities to complete tasks through multiple\ninteractions within GUI environments. However, existing agents primarily focus\non enhancing the accuracy of individual actions and often lack effective\nmechanisms for detecting and recovering from errors. To address these\nshortcomings, we propose the BacktrackAgent, a robust framework that\nincorporates a backtracking mechanism to improve task completion efficiency.\nBacktrackAgent includes verifier, judger, and reflector components as modules\nfor error detection and recovery, while also applying judgment rewards to\nfurther enhance the agent's performance. Additionally, we develop a training\ndataset specifically designed for the backtracking mechanism, which considers\nthe outcome pages after action executions. Experimental results show that\nBacktrackAgent has achieved performance improvements in both task success rate\nand step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be\nreleased upon acceptance.", "AI": {"tldr": "BacktrackAgent\u662f\u4e00\u4e2aGUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u6eaf\u673a\u5236\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u6548\u7387\uff0c\u5305\u542b\u9a8c\u8bc1\u5668\u3001\u5224\u65ad\u5668\u548c\u53cd\u5c04\u5668\u6a21\u5757\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u6b65\u9aa4\u51c6\u786e\u6027\u4e0a\u6709\u6240\u63d0\u5347\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u52a8\u4f5c\u7684\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u6062\u590d\u673a\u5236\uff0cBacktrackAgent\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faBacktrackAgent\u6846\u67b6\uff0c\u5305\u542b\u9a8c\u8bc1\u5668\u3001\u5224\u65ad\u5668\u548c\u53cd\u5c04\u5668\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7528\u4e8e\u56de\u6eaf\u673a\u5236\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728Mobile3M\u548cAuto-UI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBacktrackAgent\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u6b65\u9aa4\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "BacktrackAgent\u901a\u8fc7\u56de\u6eaf\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "relevance": 40.0}}
{"id": "2505.20759", "pdf": "https://arxiv.org/pdf/2505.20759", "abs": "https://arxiv.org/abs/2505.20759", "authors": ["Ansel Blume", "Jeonghwan Kim", "Hyeonjeong Ha", "Elen Chatikyan", "Xiaomeng Jin", "Khanh Duy Nguyen", "Nanyun Peng", "Kai-Wei Chang", "Derek Hoiem", "Heng Ji"], "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "Real-world objects are composed of distinctive, object-specific parts.\nIdentifying these parts is key to performing fine-grained, compositional\nreasoning-yet, large multimodal models (LMMs) struggle to perform this\nseemingly straightforward task. In this work, we introduce PARTONOMY, an LMM\nbenchmark designed for pixel-level part grounding. We construct PARTONOMY from\nexisting part datasets and our own rigorously annotated set of images,\nencompassing 862 part labels and 534 object labels for evaluation. Unlike\nexisting datasets that simply ask models to identify generic parts, PARTONOMY\nuses specialized concepts (e.g., agricultural airplane), and challenges models\nto compare objects' parts, consider part-whole relationships, and justify\ntextual predictions with visual segmentations. Our experiments demonstrate\nsignificant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only\n5.9% gIoU), highlighting a critical gap in their part grounding abilities. We\nnote that existing segmentation-enabled LMMs (segmenting LMMs) have two key\narchitectural shortcomings: they use special [SEG] tokens not seen during\npretraining which induce distribution shift, and they discard predicted\nsegmentations instead of using past predictions to guide future ones. To\naddress these deficiencies, we train several part-centric LMMs and propose\nPLUM, a novel segmenting LMM that uses span tagging instead of segmentation\ntokens and that conditions on prior predictions in a feedback loop. We find\nthat pretrained PLUM outperforms existing segmenting LMMs on reasoning\nsegmentation, VQA, and visual hallucination benchmarks. In addition, PLUM\nfinetuned on our proposed Explanatory Part Segmentation task is competitive\nwith segmenting LMMs trained on significantly more segmentation data. Our work\nopens up new avenues towards enabling fine-grained, grounded visual\nunderstanding in LMMs.", "AI": {"tldr": "PARTONOMY\u662f\u4e00\u4e2a\u9488\u5bf9\u50cf\u7d20\u7ea7\u90e8\u5206\u5b9a\u4f4d\u7684LMM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709LMM\u5728\u90e8\u5206\u5b9a\u4f4d\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51faPLUM\u6a21\u578b\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709LMM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u90e8\u5206\u5b9a\u4f4d\u548c\u7ec4\u5408\u63a8\u7406\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u548c\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efaPARTONOMY\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51faPLUM\u6a21\u578b\uff0c\u91c7\u7528span\u6807\u8bb0\u548c\u53cd\u9988\u5faa\u73af\u673a\u5236\u6539\u8fdb\u73b0\u6709LMM\u7684\u7f3a\u9677\u3002", "result": "PLUM\u5728\u63a8\u7406\u5206\u5272\u3001VQA\u548c\u89c6\u89c9\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LMM\uff0c\u4e14\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u4e5f\u80fd\u7ade\u4e89\u3002", "conclusion": "PLUM\u4e3aLMM\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u5b9a\u4f4d\u7684\u5173\u952e\u95ee\u9898\u3002", "relevance": 75.0}}
{"id": "2505.20761", "pdf": "https://arxiv.org/pdf/2505.20761", "abs": "https://arxiv.org/abs/2505.20761", "authors": ["Ryota Ushio", "Takashi Ishida", "Masashi Sugiyama"], "title": "Practical estimation of the optimal classification error with soft labels and calibration", "categories": ["cs.LG", "stat.ML"], "comment": "36 pages, 24 figures; GitHub:\n  https://github.com/RyotaUshio/bayes-error-estimation", "summary": "While the performance of machine learning systems has experienced significant\nimprovement in recent years, relatively little attention has been paid to the\nfundamental question: to what extent can we improve our models? This paper\nprovides a means of answering this question in the setting of binary\nclassification, which is practical and theoretically supported. We extend a\nprevious work that utilizes soft labels for estimating the Bayes error, the\noptimal error rate, in two important ways. First, we theoretically investigate\nthe properties of the bias of the hard-label-based estimator discussed in the\noriginal work. We reveal that the decay rate of the bias is adaptive to how\nwell the two class-conditional distributions are separated, and it can decay\nsignificantly faster than the previous result suggested as the number of hard\nlabels per instance grows. Second, we tackle a more challenging problem\nsetting: estimation with corrupted soft labels. One might be tempted to use\ncalibrated soft labels instead of clean ones. However, we reveal that\ncalibration guarantee is not enough, that is, even perfectly calibrated soft\nlabels can result in a substantially inaccurate estimate. Then, we show that\nisotonic calibration can provide a statistically consistent estimator under an\nassumption weaker than that of the previous work. Our method is instance-free,\ni.e., we do not assume access to any input instances. This feature allows it to\nbe adopted in practical scenarios where the instances are not available due to\nprivacy issues. Experiments with synthetic and real-world datasets show the\nvalidity of our methods and theory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u4f30\u8ba1\u8d1d\u53f6\u65af\u8bef\u5dee\u7684\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u4e4b\u524d\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u6781\u9650\uff0c\u7279\u522b\u662f\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u5982\u4f55\u4f30\u8ba1\u8d1d\u53f6\u65af\u8bef\u5dee\u3002", "method": "\u6269\u5c55\u4e86\u4e4b\u524d\u57fa\u4e8e\u8f6f\u6807\u7b7e\u4f30\u8ba1\u8d1d\u53f6\u65af\u8bef\u5dee\u7684\u5de5\u4f5c\uff0c\u7406\u8bba\u5206\u6790\u4e86\u786c\u6807\u7b7e\u4f30\u8ba1\u5668\u7684\u504f\u5dee\u6027\u8d28\uff0c\u5e76\u89e3\u51b3\u4e86\u5e26\u6709\u566a\u58f0\u8f6f\u6807\u7b7e\u7684\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "result": "\u63ed\u793a\u4e86\u504f\u5dee\u8870\u51cf\u7387\u4e0e\u7c7b\u522b\u5206\u5e03\u5206\u79bb\u7a0b\u5ea6\u7684\u5173\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u5373\u4f7f\u5b8c\u7f8e\u6821\u51c6\u7684\u8f6f\u6807\u7b7e\u4e5f\u53ef\u80fd\u5bfc\u81f4\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u6709\u6548\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u573a\u666f\u3002", "relevance": 50.0}}
{"id": "2505.12703", "pdf": "https://arxiv.org/pdf/2505.12703", "abs": "https://arxiv.org/abs/2505.12703", "authors": ["Jiabin Chen", "Haiping Wang", "Jinpeng Li", "Yuan Liu", "Zhen Dong", "Bisheng Yang"], "title": "SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose SpatialLLM, a novel approach advancing spatial intelligence tasks\nin complex urban scenes. Unlike previous methods requiring geographic analysis\ntools or domain expertise, SpatialLLM is a unified language model directly\naddressing various spatial intelligence tasks without any training,\nfine-tuning, or expert intervention. The core of SpatialLLM lies in\nconstructing detailed and structured scene descriptions from raw spatial data\nto prompt pre-trained LLMs for scene-based analysis. Extensive experiments show\nthat, with our designs, pretrained LLMs can accurately perceive spatial\ndistribution information and enable zero-shot execution of advanced spatial\nintelligence tasks, including urban planning, ecological analysis, traffic\nmanagement, etc. We argue that multi-field knowledge, context length, and\nreasoning ability are key factors influencing LLM performances in urban\nanalysis. We hope that SpatialLLM will provide a novel viable perspective for\nurban intelligent analysis and management. The code and dataset are available\nat https://github.com/WHU-USI3DV/SpatialLLM.", "AI": {"tldr": "SpatialLLM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u7edf\u4e00\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u5b9e\u73b0\u96f6\u6837\u672c\u7a7a\u95f4\u667a\u80fd\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u7a7a\u95f4\u667a\u80fd\u4efb\u52a1\u5bf9\u5730\u7406\u5206\u6790\u5de5\u5177\u6216\u9886\u57df\u4e13\u5bb6\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u8be6\u7ec6\u7684\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3LLM\u8fdb\u884c\u573a\u666f\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u8bad\u7ec3LLM\u80fd\u51c6\u786e\u611f\u77e5\u7a7a\u95f4\u5206\u5e03\u4fe1\u606f\uff0c\u96f6\u6837\u672c\u6267\u884c\u9ad8\u7ea7\u7a7a\u95f4\u667a\u80fd\u4efb\u52a1\u3002", "conclusion": "\u591a\u9886\u57df\u77e5\u8bc6\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u63a8\u7406\u80fd\u529b\u662f\u5f71\u54cdLLM\u5728\u57ce\u5e02\u5206\u6790\u4e2d\u8868\u73b0\u7684\u5173\u952e\u56e0\u7d20\u3002", "relevance": 60.0}}
{"id": "2505.20664", "pdf": "https://arxiv.org/pdf/2505.20664", "abs": "https://arxiv.org/abs/2505.20664", "authors": ["Yang He", "Xiao Ding", "Bibo Cai", "Yufei Zhang", "Kai Xiong", "Zhouhao Sun", "Bing Qin", "Ting Liu"], "title": "Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reasoning-augmented large language models (RLLMs) significantly enhance\ncomplex task performance through extended reasoning chains, they inevitably\nintroduce substantial unnecessary token consumption, particularly for simpler\nproblems where Short Chain-of-Thought (Short CoT) suffices. This overthinking\nphenomenon leads to inefficient resource usage without proportional accuracy\ngains. To address this issue, we propose Self-Route, a dynamic reasoning\nframework that automatically selects between general and reasoning modes based\non model capability estimation. Our approach introduces a lightweight\npre-inference stage to extract capability-aware embeddings from hidden layer\nrepresentations, enabling real-time evaluation of the model's ability to solve\nproblems. We further construct Gradient-10K, a model difficulty\nestimation-based dataset with dense complexity sampling, to train the router\nfor precise capability boundary detection. Extensive experiments demonstrate\nthat Self-Route achieves comparable accuracy to reasoning models while reducing\ntoken consumption by 30-55\\% across diverse benchmarks. The proposed framework\ndemonstrates consistent effectiveness across models with different parameter\nscales and reasoning paradigms, highlighting its general applicability and\npractical value.", "AI": {"tldr": "Self-Route\u662f\u4e00\u4e2a\u52a8\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9884\u63a8\u7406\u9636\u6bb5\u548c\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\uff0c\u81ea\u52a8\u9009\u62e9\u63a8\u7406\u6a21\u5f0f\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684token\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u63a8\u7406\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08RLLMs\uff09\u5728\u5904\u7406\u7b80\u5355\u95ee\u9898\u65f6\u8fc7\u5ea6\u6d88\u8017token\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u3002", "method": "\u63d0\u51faSelf-Route\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u9884\u63a8\u7406\u9636\u6bb5\u63d0\u53d6\u80fd\u529b\u611f\u77e5\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408Gradient-10K\u6570\u636e\u96c6\u8bad\u7ec3\u8def\u7531\u5668\u3002", "result": "\u5728\u4fdd\u6301\u63a8\u7406\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0ctoken\u6d88\u8017\u51cf\u5c1130-55%\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "Self-Route\u5177\u6709\u901a\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.20764", "pdf": "https://arxiv.org/pdf/2505.20764", "abs": "https://arxiv.org/abs/2505.20764", "authors": ["Eric Xing", "Pranavi Kolouju", "Robert Pless", "Abby Stylianou", "Nathan Jacobs"], "title": "ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval", "categories": ["cs.CV", "cs.LG"], "comment": "15 pages, 8 figures, 6 tables. CVPR 2025", "summary": "Composed image retrieval (CIR) is the task of retrieving a target image\nspecified by a query image and a relative text that describes a semantic\nmodification to the query image. Existing methods in CIR struggle to accurately\nrepresent the image and the text modification, resulting in subpar performance.\nTo address this limitation, we introduce a CIR framework, ConText-CIR, trained\nwith a Text Concept-Consistency loss that encourages the representations of\nnoun phrases in the text modification to better attend to the relevant parts of\nthe query image. To support training with this loss function, we also propose a\nsynthetic data generation pipeline that creates training data from existing CIR\ndatasets or unlabeled images. We show that these components together enable\nstronger performance on CIR tasks, setting a new state-of-the-art in composed\nimage retrieval in both the supervised and zero-shot settings on multiple\nbenchmark datasets, including CIRR and CIRCO. Source code, model checkpoints,\nand our new datasets are available at https://github.com/mvrl/ConText-CIR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u6846\u67b6ConText-CIR\uff0c\u901a\u8fc7\u5f15\u5165\u6587\u672c\u6982\u5ff5\u4e00\u81f4\u6027\u635f\u5931\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CIR\u65b9\u6cd5\u5728\u51c6\u786e\u8868\u793a\u56fe\u50cf\u548c\u6587\u672c\u4fee\u6539\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u68c0\u7d22\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51faConText-CIR\u6846\u67b6\uff0c\u4f7f\u7528\u6587\u672c\u6982\u5ff5\u4e00\u81f4\u6027\u635f\u5931\u4f18\u5316\u6587\u672c\u4fee\u6539\u4e2d\u7684\u540d\u8bcd\u77ed\u8bed\u4e0e\u56fe\u50cf\u76f8\u5173\u90e8\u5206\u7684\u6ce8\u610f\u529b\uff0c\u5e76\u8bbe\u8ba1\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982CIRR\u548cCIRCO\uff09\u4e0a\u5b9e\u73b0\u4e86\u76d1\u7763\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u7684SOTA\u6027\u80fd\u3002", "conclusion": "ConText-CIR\u901a\u8fc7\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u548c\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86CIR\u4efb\u52a1\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20765", "pdf": "https://arxiv.org/pdf/2505.20765", "abs": "https://arxiv.org/abs/2505.20765", "authors": ["Kohei Obata", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies", "categories": ["cs.LG"], "comment": "Accepted by KDD 2025", "summary": "Unsupervised anomaly detection in time series has been a pivotal research\narea for decades. Current mainstream approaches focus on learning normality, on\nthe assumption that all or most of the samples in the training set are normal.\nHowever, anomalies in the training set (i.e., anomaly contamination) can be\nmisleading. Recent studies employ data augmentation to generate\npseudo-anomalies and learn the boundary separating the training samples from\nthe augmented samples. Although this approach mitigates anomaly contamination\nif augmented samples mimic unseen real anomalies, it suffers from several\nlimitations. (1) Covering a wide range of time series anomalies is challenging.\n(2) It disregards augmented samples that resemble normal samples (i.e., false\nanomalies). (3) It places too much trust in the labels of training and\naugmented samples. In response, we propose RedLamp, which employs diverse data\naugmentations to generate multiclass pseudo-anomalies and learns the multiclass\nboundary. Such multiclass pseudo-anomalies cover a wide variety of time series\nanomalies. We conduct multiclass classification using soft labels, which\nprevents the model from being overconfident and ensures its robustness against\ncontaminated/false anomalies. The learned latent space is inherently\nexplainable as it is trained to separate pseudo-anomalies into multiclasses.\nExtensive experiments demonstrate the effectiveness of RedLamp in anomaly\ndetection and its robustness against anomaly contamination.", "AI": {"tldr": "\u63d0\u51faRedLamp\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c7b\u4f2a\u5f02\u5e38\u6570\u636e\u589e\u5f3a\u548c\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5f02\u5e38\u6c61\u67d3\u548c\u8986\u76d6\u8303\u56f4\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u6613\u53d7\u8bad\u7ec3\u96c6\u4e2d\u5f02\u5e38\u6c61\u67d3\u7684\u5f71\u54cd\uff0c\u4e14\u6570\u636e\u589e\u5f3a\u751f\u6210\u7684\u4f2a\u5f02\u5e38\u8986\u76d6\u8303\u56f4\u6709\u9650\u3002", "method": "\u4f7f\u7528\u591a\u6837\u6570\u636e\u589e\u5f3a\u751f\u6210\u591a\u7c7b\u4f2a\u5f02\u5e38\uff0c\u901a\u8fc7\u591a\u7c7b\u5206\u7c7b\u548c\u8f6f\u6807\u7b7e\u5b66\u4e60\u8fb9\u754c\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRedLamp\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6709\u6548\uff0c\u4e14\u5bf9\u5f02\u5e38\u6c61\u67d3\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "RedLamp\u901a\u8fc7\u591a\u7c7b\u4f2a\u5f02\u5e38\u548c\u8f6f\u6807\u7b7e\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "relevance": 30.0}}
{"id": "2505.20674", "pdf": "https://arxiv.org/pdf/2505.20674", "abs": "https://arxiv.org/abs/2505.20674", "authors": ["Boyi Zeng", "Shixiang Song", "Siyuan Huang", "Yixuan Wang", "He Li", "Ziwei He", "Xinbing Wang", "Zhiyu Li", "Zhouhan Lin"], "title": "Pretraining Language Models to Ponder in Continuous Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u201c\u601d\u8003\u201d\u8fc7\u7a0b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u590d\u524d\u5411\u4f20\u9012\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53d7\u4eba\u7c7b\u5728\u8868\u8fbe\u590d\u6742\u53e5\u5b50\u524d\u4f1a\u601d\u8003\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u5c06\u7c7b\u4f3c\u8fc7\u7a0b\u5f15\u5165\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u5728\u5355\u4e2atoken\u751f\u6210\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u901a\u8fc7\u91cd\u590d\u524d\u5411\u4f20\u9012\u8fdb\u884c\u201c\u601d\u8003\u201d\uff0c\u751f\u6210\u52a0\u6743token\u5d4c\u5165\u5e76\u53cd\u9988\u4e3a\u8f93\u5165\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u601d\u8003\u6a21\u578b\u6027\u80fd\u4e0e\u53c2\u6570\u7ffb\u500d\u7684\u666e\u901a\u6a21\u578b\u76f8\u5f53\uff1b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u601d\u8003\u589e\u5f3a\u7684Pythia\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u539f\u7248\u3002", "conclusion": "\u601d\u8003\u8fc7\u7a0b\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u67b6\u6784\u4e2d\u3002", "relevance": 85.0}}
{"id": "2505.20772", "pdf": "https://arxiv.org/pdf/2505.20772", "abs": "https://arxiv.org/abs/2505.20772", "authors": ["Hongjia Liu", "Rongzhen Zhao", "Haohan Chen", "Joni Pajarinen"], "title": "MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning object-level, structured representations is widely regarded as a key\nto better generalization in vision and underpins the design of next-generation\nPre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)\nmethods adopt Slot Attention or its variants to iteratively aggregate objects'\nsuper-pixels into a fixed set of query feature vectors, termed slots. However,\ntheir reliance on a static slot count leads to an object being represented as\nmultiple parts when the number of objects varies. We introduce MetaSlot, a\nplug-and-play Slot Attention variant that adapts to variable object counts.\nMetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset\nby vector-quantizing the resulting slot representations; (ii) removes duplicate\nslots from the traditionally aggregated slots by quantizing them with the\ncodebook; and (iii) injects progressively weaker noise into the Slot Attention\niterations to accelerate and stabilize the aggregation. MetaSlot is a general\nSlot Attention variant that can be seamlessly integrated into existing OCL\narchitectures. Across multiple public datasets and tasks--including object\ndiscovery and recognition--models equipped with MetaSlot achieve significant\nperformance gains and markedly interpretable slot representations, compared\nwith existing Slot Attention variants.", "AI": {"tldr": "MetaSlot\u662f\u4e00\u79cd\u6539\u8fdb\u7684Slot Attention\u53d8\u4f53\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5bf9\u8c61\u6570\u91cf\uff0c\u63d0\u5347\u89c6\u89c9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e3b\u6d41\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u65b9\u6cd5\u56e0\u56fa\u5b9a\u69fd\u6570\u5bfc\u81f4\u5bf9\u8c61\u88ab\u5206\u5272\u4e3a\u591a\u4e2a\u90e8\u5206\u7684\u95ee\u9898\u3002", "method": "MetaSlot\u901a\u8fc7\u7ef4\u62a4\u5bf9\u8c61\u539f\u578b\u4ee3\u7801\u5e93\u3001\u53bb\u9664\u91cd\u590d\u69fd\u548c\u6ce8\u5165\u6e10\u8fdb\u566a\u58f0\u6765\u4f18\u5316Slot Attention\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\uff0cMetaSlot\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u751f\u6210\u66f4\u53ef\u89e3\u91ca\u7684\u69fd\u8868\u793a\u3002", "conclusion": "MetaSlot\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684Slot Attention\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u67b6\u6784\u3002", "relevance": 60.0}}
{"id": "2505.20774", "pdf": "https://arxiv.org/pdf/2505.20774", "abs": "https://arxiv.org/abs/2505.20774", "authors": ["Xiaowen Ma", "Zhenliang Ni", "Shuai Xiao", "Xinghao Chen"], "title": "TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "In long-term time series forecasting, different variables often influence the\ntarget variable over distinct time intervals, a challenge known as the\nmulti-delay issue. Traditional models typically process all variables or time\npoints uniformly, which limits their ability to capture complex variable\nrelationships and obtain non-trivial time representations. To address this\nissue, we propose TimePro, an innovative Mamba-based model that constructs\nvariate- and time-aware hyper-states. Unlike conventional approaches that\nmerely transfer plain states across variable or time dimensions, TimePro\npreserves the fine-grained temporal features of each variate token and\nadaptively selects the focused time points to tune the plain state. The\nreconstructed hyper-state can perceive both variable relationships and salient\ntemporal information, which helps the model make accurate forecasting. In\nexperiments, TimePro performs competitively on eight real-world long-term\nforecasting benchmarks with satisfactory linear complexity. Code is available\nat https://github.com/xwmaxwma/TimePro.", "AI": {"tldr": "TimePro\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u521b\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u5ef6\u8fdf\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u53d8\u91cf\u548c\u65f6\u95f4\u611f\u77e5\u7684\u8d85\u72b6\u6001\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u65f6\u65e0\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u53d8\u91cf\u5173\u7cfb\u548c\u975e\u5e73\u51e1\u65f6\u95f4\u8868\u793a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u611f\u77e5\u53d8\u91cf\u5173\u7cfb\u548c\u663e\u8457\u65f6\u95f4\u4fe1\u606f\u3002", "method": "TimePro\u901a\u8fc7\u4fdd\u7559\u6bcf\u4e2a\u53d8\u91cf\u6807\u8bb0\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u81ea\u9002\u5e94\u9009\u62e9\u5173\u6ce8\u7684\u65f6\u95f4\u70b9\u6765\u8c03\u6574\u666e\u901a\u72b6\u6001\uff0c\u6784\u5efa\u53d8\u91cf\u548c\u65f6\u95f4\u611f\u77e5\u7684\u8d85\u72b6\u6001\u3002", "result": "\u5728\u516b\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u957f\u671f\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTimePro\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u4ee4\u4eba\u6ee1\u610f\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "conclusion": "TimePro\u901a\u8fc7\u611f\u77e5\u53d8\u91cf\u5173\u7cfb\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20299", "pdf": "https://arxiv.org/pdf/2505.20299", "abs": "https://arxiv.org/abs/2505.20299", "authors": ["Jianpeng Chen", "Wangzhi Zhan", "Haohui Wang", "Zian Jia", "Jingru Gan", "Junkai Zhang", "Jingyuan Qi", "Tingwei Chen", "Lifu Huang", "Muhao Chen", "Ling Li", "Wei Wang", "Dawei Zhou"], "title": "MetamatBench: Integrating Heterogeneous Data, Computational Tools, and Visual Interface for Metamaterial Discovery", "categories": ["physics.optics", "cs.AI", "I.2.0; H.5; J.2; E.0"], "comment": "15 pages", "summary": "Metamaterials, engineered materials with architected structures across\nmultiple length scales, offer unprecedented and tunable mechanical properties\nthat surpass those of conventional materials. However, leveraging advanced\nmachine learning (ML) for metamaterial discovery is hindered by three\nfundamental challenges: (C1) Data Heterogeneity Challenge arises from\nheterogeneous data sources, heterogeneous composition scales, and heterogeneous\nstructure categories; (C2) Model Complexity Challenge stems from the intricate\ngeometric constraints of ML models, which complicate their adaptation to\nmetamaterial structures; and (C3) Human-AI Collaboration Challenge comes from\nthe \"dual black-box'' nature of sophisticated ML models and the need for\nintuitive user interfaces. To tackle these challenges, we introduce a unified\nframework, named MetamatBench, that operates on three levels. (1) At the data\nlevel, we integrate and standardize 5 heterogeneous, multi-modal metamaterial\ndatasets. (2) The ML level provides a comprehensive toolkit that adapts 17\nstate-of-the-art ML methods for metamaterial discovery. It also includes a\ncomprehensive evaluation suite with 12 novel performance metrics with finite\nelement-based assessments to ensure accurate and reliable model validation. (3)\nThe user level features a visual-interactive interface that bridges the gap\nbetween complex ML techniques and non-ML researchers, advancing property\nprediction and inverse design of metamaterials for research and applications.\nMetamatBench offers a unified platform deployed at\nhttp://zhoulab-1.cs.vt.edu:5550 that enables machine learning researchers and\npractitioners to develop and evaluate new methodologies in metamaterial\ndiscovery. For accessibility and reproducibility, we open-source our benchmark\nand the codebase at https://github.com/cjpcool/Metamaterial-Benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMetamatBench\u6846\u67b6\uff0c\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5728\u8d85\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u6570\u636e\u5f02\u6784\u6027\u3001\u6a21\u578b\u590d\u6742\u6027\u548c\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u52a0\u901f\u8d85\u6750\u6599\u53d1\u73b0\uff0c\u4f46\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u3001\u6a21\u578b\u590d\u6742\u6027\u548c\u4eba\u673a\u534f\u4f5c\u7684\u6311\u6218\u3002", "method": "MetamatBench\u6846\u67b6\u5206\u4e3a\u6570\u636e\u5c42\uff08\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u96c6\uff09\u3001ML\u5c42\uff08\u9002\u914d17\u79cd\u5148\u8fdb\u65b9\u6cd5\u5e76\u63d0\u4f9b\u8bc4\u4f30\u5957\u4ef6\uff09\u548c\u7528\u6237\u5c42\uff08\u53ef\u89c6\u5316\u4ea4\u4e92\u754c\u9762\uff09\u3002", "result": "\u5f00\u53d1\u4e86\u7edf\u4e00\u5e73\u53f0\u548c\u5f00\u6e90\u4ee3\u7801\uff0c\u652f\u6301\u8d85\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u65b9\u6cd5\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "conclusion": "MetamatBench\u4e3a\u8d85\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\u548c\u5e73\u53f0\uff0c\u4fc3\u8fdb\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.20679", "pdf": "https://arxiv.org/pdf/2505.20679", "abs": "https://arxiv.org/abs/2505.20679", "authors": ["Danush Khanna", "Pratinav Seth", "Sidhaarth Sredharan Murali", "Aditya Kumar Guru", "Siddharth Shukla", "Tanuj Tyagi", "Sandeep Chaurasia", "Kripabandhu Ghosh"], "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal\ncommunication, making its detection critical for safeguarding potential\nvictims. However, due to manipulation's nuanced and context-specific nature,\nidentifying manipulative language in complex, multi-turn, and multi-person\nconversations remains a significant challenge for large language models (LLMs).\nTo address this gap, we introduce the MultiManip dataset, comprising 220\nmulti-turn, multi-person dialogues balanced between manipulative and\nnon-manipulative interactions, all drawn from reality shows that mimic\nreal-world scenarios. For manipulative interactions, it includes 11 distinct\nmanipulations depicting real-life scenarios. We conduct extensive evaluations\nof state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various\nprompting strategies. Despite their capabilities, these models often struggle\nto detect manipulation effectively. To overcome this limitation, we propose\nSELF-PERCEPT, a novel, two-stage prompting framework inspired by\nSelf-Perception Theory, demonstrating strong performance in detecting\nmulti-person, multi-turn mental manipulation. Our code and data are publicly\navailable at https://github.com/danushkhanna/self-percept .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MultiManip\u6570\u636e\u96c6\u548cSELF-PERCEPT\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u8f6e\u591a\u4eba\u5bf9\u8bdd\u4e2d\u7684\u5fc3\u7406\u64cd\u7eb5\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709LLMs\u7684\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u5fc3\u7406\u64cd\u7eb5\u5728\u4eba\u9645\u4ea4\u6d41\u4e2d\u666e\u904d\u5b58\u5728\u4e14\u96be\u4ee5\u68c0\u6d4b\uff0c\u73b0\u6709LLMs\u5728\u590d\u6742\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efaMultiManip\u6570\u636e\u96c6\uff0c\u8bc4\u4f30GPT-4o\u548cLlama-3.1-8B\u7b49LLMs\uff0c\u63d0\u51fa\u57fa\u4e8e\u81ea\u6211\u8ba4\u77e5\u7406\u8bba\u7684\u4e24\u9636\u6bb5\u63d0\u793a\u6846\u67b6SELF-PERCEPT\u3002", "result": "\u73b0\u6709LLMs\u5728\u68c0\u6d4b\u64cd\u7eb5\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cSELF-PERCEPT\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "SELF-PERCEPT\u6846\u67b6\u5728\u591a\u8f6e\u591a\u4eba\u5fc3\u7406\u64cd\u7eb5\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aLLMs\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 70.0}}
{"id": "2505.20777", "pdf": "https://arxiv.org/pdf/2505.20777", "abs": "https://arxiv.org/abs/2505.20777", "authors": ["Zhehan Kan", "Yanlin Liu", "Kun Yin", "Xinghua Jiang", "Xin Li", "Haoyu Cao", "Yinsong Liu", "Deqiang Jiang", "Xing Sun", "Qingmin Liao", "Wenming Yang"], "title": "TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "DeepSeek R1 has significantly advanced complex reasoning for large language\nmodels (LLMs). While recent methods have attempted to replicate R1's reasoning\ncapabilities in multimodal settings, they face limitations, including\ninconsistencies between reasoning and final answers, model instability and\ncrashes during long-chain exploration, and low data learning efficiency. To\naddress these challenges, we propose TACO, a novel reinforcement learning\nalgorithm for visual reasoning. Building on Generalized Reinforcement Policy\nOptimization (GRPO), TACO introduces Think-Answer Consistency, which tightly\ncouples reasoning with answer consistency to ensure answers are grounded in\nthoughtful reasoning. We also introduce the Rollback Resample Strategy, which\nadaptively removes problematic samples and reintroduces them to the sampler,\nenabling stable long-chain exploration and future learning opportunities.\nAdditionally, TACO employs an adaptive learning schedule that focuses on\nmoderate difficulty samples to optimize data efficiency. Furthermore, we\npropose the Test-Time-Resolution-Scaling scheme to address performance\ndegradation due to varying resolutions during reasoning while balancing\ncomputational overhead. Extensive experiments on in-distribution and\nout-of-distribution benchmarks for REC and VQA tasks show that fine-tuning\nLVLMs leads to significant performance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TACO\uff0c\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u95ee\u9898\uff0c\u5982\u63a8\u7406\u4e0e\u7b54\u6848\u4e0d\u4e00\u81f4\u3001\u6a21\u578b\u4e0d\u7a33\u5b9a\u7b49\u3002\u901a\u8fc7\u5f15\u5165Think-Answer Consistency\u548cRollback Resample Strategy\uff0cTACO\u63d0\u5347\u4e86\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u4e2dLLMs\u7684\u5c40\u9650\u6027\uff0c\u5982\u63a8\u7406\u4e0e\u7b54\u6848\u4e0d\u4e00\u81f4\u3001\u6a21\u578b\u4e0d\u7a33\u5b9a\u548c\u4f4e\u6570\u636e\u6548\u7387\u3002", "method": "\u63d0\u51faTACO\u7b97\u6cd5\uff0c\u7ed3\u5408Think-Answer Consistency\u3001Rollback Resample Strategy\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u8ba1\u5212\uff0c\u4f18\u5316\u63a8\u7406\u548c\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728REC\u548cVQA\u4efb\u52a1\u4e2d\uff0cTACO\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "TACO\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "relevance": 85.0}}
{"id": "2505.20775", "pdf": "https://arxiv.org/pdf/2505.20775", "abs": "https://arxiv.org/abs/2505.20775", "authors": ["Fabian Scheurer", "Alexander Hammer", "Mario Schubert", "Robert-Patrick Steiner", "Oliver Gamm", "Kaomei Guan", "Frank Sonntag", "Hagen Malberg", "Martin Schmidt"], "title": "Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI", "categories": ["cs.LG", "eess.SP", "q-bio.CB"], "comment": null, "summary": "Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an\nimportant resource for the identification of new therapeutic targets and\ncardioprotective drugs. After differentiation iPSC-CMs show an immature,\nfetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation\nmedium (MM) strongly enhances their structural, metabolic and functional\nphenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging\nas most methods are time consuming and go in line with cell damage or loss of\nthe sample. To address this issue, we developed a non-invasive approach for\nautomated classification of iPSC-CM maturity through interpretable artificial\nintelligence (AI)-based analysis of beat characteristics derived from\nvideo-based motion analysis. In a prospective study, we evaluated 230 video\nrecordings of early-state, immature iPSC-CMs on day 21 after differentiation\n(d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10\nfeatures were extracted using Maia motion analysis software and entered into a\nsupport vector machine (SVM). The hyperparameters of the SVM were optimized in\na grid search on 80 % of the data using 5-fold cross-validation. The optimized\nmodel achieved an accuracy of 99.5 $\\pm$ 1.1 % on a hold-out test set. Shapley\nAdditive Explanations (SHAP) identified displacement, relaxation-rise time and\nbeating duration as the most relevant features for assessing maturity level.\nOur results suggest the use of non-invasive, optical motion analysis combined\nwith AI-based methods as a tool to assess iPSC-CMs maturity and could be\napplied before performing functional readouts or drug testing. This may\npotentially reduce the variability and improve the reproducibility of\nexperimental studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u9891\u8fd0\u52a8\u5206\u6790\u81ea\u52a8\u5206\u7c7biPSC-CMs\u7684\u6210\u719f\u5ea6\uff0c\u51c6\u786e\u7387\u8fbe99.5%\u3002", "motivation": "iPSC-CMs\u7684\u6210\u719f\u5ea6\u8bc4\u4f30\u901a\u5e38\u8017\u65f6\u4e14\u53ef\u80fd\u635f\u4f24\u7ec6\u80de\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u5206\u6790\u89c6\u9891\u8fd0\u52a8\u7279\u5f81\uff0c\u5e76\u901a\u8fc7SHAP\u89e3\u91ca\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523099.5%\u7684\u51c6\u786e\u7387\uff0c\u4f4d\u79fb\u3001\u677e\u5f1b\u4e0a\u5347\u65f6\u95f4\u548c\u8df3\u52a8\u6301\u7eed\u65f6\u95f4\u662f\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c11\u5b9e\u9a8c\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3002", "relevance": 10.0}}
{"id": "2505.20302", "pdf": "https://arxiv.org/pdf/2505.20302", "abs": "https://arxiv.org/abs/2505.20302", "authors": ["Patrick Yubeaton", "Andre Nakkab", "Weihua Xiao", "Luca Collini", "Ramesh Karri", "Chinmay Hegde", "Siddharth Garg"], "title": "VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification", "categories": ["cs.PL", "cs.AI", "cs.LO"], "comment": null, "summary": "This paper introduces VeriThoughts, a novel dataset designed for\nreasoning-based Verilog code generation. We establish a new benchmark framework\ngrounded in formal verification methods to evaluate the quality and correctness\nof generated hardware descriptions. Additionally, we present a suite of\nspecialized small-scale models optimized specifically for Verilog generation.\nOur work addresses the growing need for automated hardware design tools that\ncan produce verifiably correct implementations from high-level specifications,\npotentially accelerating the hardware development process while maintaining\nrigorous correctness guarantees. Our code and data are available at\n\\href{https://github.com/wilyub/VeriThoughts}{this URL}.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86VeriThoughts\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u57fa\u4e8e\u63a8\u7406\u7684Verilog\u4ee3\u7801\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f62\u5f0f\u9a8c\u8bc1\u65b9\u6cd5\u7684\u57fa\u51c6\u6846\u67b6\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u5957\u4e13\u4e3aVerilog\u751f\u6210\u4f18\u5316\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u5316\u786c\u4ef6\u8bbe\u8ba1\u5de5\u5177\u7684\u9700\u6c42\uff0c\u4ee5\u4ece\u9ad8\u7ea7\u89c4\u8303\u751f\u6210\u53ef\u9a8c\u8bc1\u6b63\u786e\u7684\u786c\u4ef6\u63cf\u8ff0\uff0c\u52a0\u901f\u5f00\u53d1\u8fc7\u7a0b\u5e76\u786e\u4fdd\u6b63\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86VeriThoughts\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e13\u4e3aVerilog\u751f\u6210\u4f18\u5316\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u5316\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u5728Verilog\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "VeriThoughts\u4e3a\u81ea\u52a8\u5316\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u671b\u52a0\u901f\u5f00\u53d1\u5e76\u786e\u4fdd\u6b63\u786e\u6027\u3002", "relevance": 30.0}}
{"id": "2505.20693", "pdf": "https://arxiv.org/pdf/2505.20693", "abs": "https://arxiv.org/abs/2505.20693", "authors": ["Praveen Srinivasa Varadhan", "Srija Anand", "Soma Siddhartha", "Mitesh M. Khapra"], "title": "Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "What happens when an English Fairytaler is fine-tuned on Indian languages? We\nevaluate how the English F5-TTS model adapts to 11 Indian languages, measuring\npolyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:\n(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and\n(iii) fine-tuning on both Indian and English data to prevent forgetting.\nFine-tuning with only Indian data proves most effective and the resultant IN-F5\nis a near-human polyglot; that enables speakers of one language (e.g., Odia) to\nfluently speak in another (e.g., Hindi). Our results show English pretraining\naids low-resource TTS in reaching human parity. To aid progress in other\nlow-resource languages, we study data-constrained setups and arrive at a\ncompute optimal strategy. Finally, we show IN-F5 can synthesize unseen\nlanguages like Bhojpuri and Tulu using a human-in-the-loop approach for\nzero-resource TTS via synthetic data generation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5c06\u82f1\u8bedF5-TTS\u6a21\u578b\u5fae\u8c03\u523011\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u6548\u679c\uff0c\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u6d41\u7545\u6027\u3001\u8bed\u97f3\u514b\u9686\u3001\u98ce\u683c\u514b\u9686\u548c\u4ee3\u7801\u6df7\u5408\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u5370\u5ea6\u6570\u636e\u5fae\u8c03\u6548\u679c\u6700\u4f73\uff0c\u4e14\u82f1\u8bed\u9884\u8bad\u7ec3\u6709\u52a9\u4e8e\u4f4e\u8d44\u6e90TTS\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u63a2\u7d22\u82f1\u8bed\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u4e0a\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u591a\u8bed\u8a00TTS\u7684\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a(i)\u4ece\u5934\u8bad\u7ec3\uff0c(ii)\u4ec5\u7528\u5370\u5ea6\u6570\u636e\u5fae\u8c03\uff0c(iii)\u540c\u65f6\u7528\u5370\u5ea6\u548c\u82f1\u8bed\u6570\u636e\u5fae\u8c03\u4ee5\u9632\u6b62\u9057\u5fd8\u3002", "result": "\u4ec5\u7528\u5370\u5ea6\u6570\u636e\u5fae\u8c03\u7684IN-F5\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u63a5\u8fd1\u4eba\u7c7b\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u96f6\u8d44\u6e90\u60c5\u51b5\u4e0b\u5408\u6210\u672a\u89c1\u8fc7\u7684\u8bed\u8a00\u3002", "conclusion": "\u82f1\u8bed\u9884\u8bad\u7ec3\u5bf9\u4f4e\u8d44\u6e90TTS\u6709\u76ca\uff0c\u4e14IN-F5\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u96f6\u8d44\u6e90TTS\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "relevance": 40.0}}
{"id": "2505.20782", "pdf": "https://arxiv.org/pdf/2505.20782", "abs": "https://arxiv.org/abs/2505.20782", "authors": ["Ta\u00efga Gon\u00e7alves", "Tomo Miyazaki", "Shinichiro Omachi"], "title": "Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks", "categories": ["cs.CV"], "comment": null, "summary": "We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for\ngenerating adversarial examples that mislead image classifiers toward any\ntarget class, including those not seen during training. Traditional targeted\nattacks are limited to one class per model, requiring expensive retraining for\neach target. Multi-targeted attacks address this by introducing a perturbation\ngenerator with a conditional input to specify the target class. However,\nexisting methods are constrained to classes observed during training and\nrequire access to the black-box model's training data--introducing a form of\ndata leakage that undermines realistic evaluation in practical black-box\nscenarios. We identify overreliance on class embeddings as a key limitation,\nleading to overfitting and poor generalization to unseen classes. To address\nthis, CD-MTA replaces class-level supervision with an image-based conditional\ninput and introduces class-agnostic losses that align the perturbed and target\nimages in the feature space. This design removes dependence on class semantics,\nthereby enabling generalization to unseen classes across datasets. Experiments\non ImageNet and seven other datasets show that CD-MTA outperforms prior\nmulti-targeted attacks in both standard and cross-domain settings--without\naccessing the black-box model's training data.", "AI": {"tldr": "CD-MTA\u662f\u4e00\u79cd\u751f\u6210\u5bf9\u6297\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u53ef\u8bef\u5bfc\u56fe\u50cf\u5206\u7c7b\u5668\u81f3\u4efb\u610f\u76ee\u6807\u7c7b\u522b\uff0c\u5305\u62ec\u672a\u89c1\u7c7b\u522b\uff0c\u65e0\u9700\u8bbf\u95ee\u9ed1\u76d2\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u653b\u51fb\u65b9\u6cd5\u5c40\u9650\u4e8e\u6bcf\u4e2a\u6a21\u578b\u4e00\u4e2a\u7c7b\u522b\uff0c\u4e14\u9700\u8bbf\u95ee\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u9ed1\u76d2\u573a\u666f\u7684\u8bc4\u4f30\u3002CD-MTA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CD-MTA\u91c7\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u8f93\u5165\u6761\u4ef6\u548c\u7c7b\u522b\u65e0\u5173\u7684\u635f\u5931\u51fd\u6570\uff0c\u51cf\u5c11\u5bf9\u7c7b\u522b\u5d4c\u5165\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728ImageNet\u7b49\u6570\u636e\u96c6\u4e0a\uff0cCD-MTA\u5728\u6807\u51c6\u548c\u8de8\u57df\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CD-MTA\u901a\u8fc7\u53bb\u9664\u5bf9\u7c7b\u522b\u8bed\u4e49\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "relevance": 40.0}}
{"id": "2505.20797", "pdf": "https://arxiv.org/pdf/2505.20797", "abs": "https://arxiv.org/abs/2505.20797", "authors": ["Antonio Tudisco", "Deborah Volpe", "Giovanna Turvani"], "title": "Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "Accurate and reliable diagnosis of diseases is crucial in enabling timely\nmedical treatment and enhancing patient survival rates. In recent years,\nMachine Learning has revolutionized diagnostic practices by creating\nclassification models capable of identifying diseases. However, these\nclassification problems often suffer from significant class imbalances, which\ncan inhibit the effectiveness of traditional models. Therefore, the interest in\nQuantum models has arisen, driven by the captivating promise of overcoming the\nlimitations of the classical counterpart thanks to their ability to express\ncomplex patterns by mapping data in a higher-dimensional computational space.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u6a21\u578b\u5728\u89e3\u51b3\u533b\u5b66\u8bca\u65ad\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u8868\u73b0\u53d7\u9650\uff0c\u91cf\u5b50\u6a21\u578b\u56e0\u5176\u9ad8\u7ef4\u8ba1\u7b97\u7a7a\u95f4\u7684\u4f18\u52bf\u53ef\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5229\u7528\u91cf\u5b50\u6a21\u578b\u5904\u7406\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u91cf\u5b50\u6a21\u578b\u5c55\u73b0\u51fa\u514b\u670d\u4f20\u7edf\u6a21\u578b\u9650\u5236\u7684\u6f5c\u529b\u3002", "conclusion": "\u91cf\u5b50\u6a21\u578b\u4e3a\u89e3\u51b3\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 30.0}}
{"id": "2505.20303", "pdf": "https://arxiv.org/pdf/2505.20303", "abs": "https://arxiv.org/abs/2505.20303", "authors": ["David Hanson"], "title": "Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As artificial intelligence becomes increasingly integrated into software\ndevelopment processes, the prevalence and sophistication of AI-generated code\ncontinue to expand rapidly. This study addresses the critical need for\ntransparency and safety in AI generated code by examining the current\nlandscape, identifying potential risks, and exploring future implications. We\nanalyze market opportunities for detecting AI-generated code, discuss the\nchallenges associated with managing increasing complexity, and propose\nsolutions to enhance transparency and functionality analysis. Furthermore, this\nstudy investigates the longterm implications of AI generated code, including\nits potential role in the development of artificial general intelligence and\nits impact on human AI interaction. In conclusion, we emphasize the importance\nof proactive measures for ensuring the responsible development and deployment\nof AI in software engineering.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86AI\u751f\u6210\u4ee3\u7801\u7684\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\uff0c\u5206\u6790\u4e86\u5e02\u573a\u673a\u4f1a\u3001\u6311\u6218\u53ca\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u957f\u671f\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u666e\u53ca\uff0cAI\u751f\u6210\u4ee3\u7801\u7684\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u5f53\u524dAI\u751f\u6210\u4ee3\u7801\u7684\u73b0\u72b6\u3001\u6f5c\u5728\u98ce\u9669\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u5f71\u54cd\u3002\u63d0\u51fa\u4e86\u589e\u5f3a\u900f\u660e\u6027\u548c\u529f\u80fd\u5206\u6790\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9700\u8981\u91c7\u53d6\u4e3b\u52a8\u63aa\u65bd\u786e\u4fddAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u90e8\u7f72\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u786e\u4fddAI\u751f\u6210\u4ee3\u7801\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20700", "pdf": "https://arxiv.org/pdf/2505.20700", "abs": "https://arxiv.org/abs/2505.20700", "authors": ["Yong Wu", "Weihang Pan", "Ke Li", "Chen Binhui", "Ping Li", "Binbin Lin"], "title": "Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nyet aligning such abilities to small language models (SLMs) remains a challenge\ndue to distributional mismatches and limited model capacity. Existing reasoning\ndatasets, typically designed for powerful LLMs, often lead to degraded\nperformance when directly applied to weaker models. In this work, we introduce\nDynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation\nframework that bridges the capability gap between expert reasoning trajectories\nand diverse SLMs. Instead of uniformly imitating expert steps, DART employs a\nselective imitation strategy guided by step-wise adaptability estimation via\nsolution simulation. When expert steps surpass the student's capacity --\nsignaled by an Imitation Gap -- the student autonomously explores alternative\nreasoning paths, constrained by outcome consistency. We validate DART across\nmultiple reasoning benchmarks and model scales, demonstrating that it\nsignificantly improves generalization and data efficiency over static\nfine-tuning. Our method enhances supervision quality by aligning training\nsignals with the student's reasoning capabilities, offering a scalable solution\nfor reasoning alignment in resource-constrained models.", "AI": {"tldr": "DART\u662f\u4e00\u79cd\u52a8\u6001\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6a21\u4eff\u4e13\u5bb6\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u80fd\u529b\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5206\u5e03\u4e0d\u5339\u914d\u548c\u6a21\u578b\u5bb9\u91cf\u9650\u5236\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u9f50\u5230\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u63a8\u7406\u6570\u636e\u96c6\u901a\u5e38\u9488\u5bf9\u5f3a\u5927\u7684LLMs\u8bbe\u8ba1\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u8f83\u5f31\u6a21\u578b\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "DART\u91c7\u7528\u9009\u62e9\u6027\u6a21\u4eff\u7b56\u7565\uff0c\u901a\u8fc7\u89e3\u51b3\u65b9\u6848\u6a21\u62df\u4f30\u8ba1\u6b65\u9aa4\u9002\u5e94\u6027\u3002\u5f53\u4e13\u5bb6\u6b65\u9aa4\u8d85\u51fa\u5b66\u751f\u6a21\u578b\u80fd\u529b\u65f6\uff08\u6a21\u4eff\u5dee\u8ddd\uff09\uff0c\u5b66\u751f\u81ea\u4e3b\u63a2\u7d22\u66ff\u4ee3\u63a8\u7406\u8def\u5f84\uff0c\u53d7\u7ed3\u679c\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u9a8c\u8bc1\uff0cDART\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\uff0c\u4f18\u4e8e\u9759\u6001\u5fae\u8c03\u3002", "conclusion": "DART\u901a\u8fc7\u5c06\u8bad\u7ec3\u4fe1\u53f7\u4e0e\u5b66\u751f\u63a8\u7406\u80fd\u529b\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u76d1\u7763\u8d28\u91cf\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u63a8\u7406\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20789", "pdf": "https://arxiv.org/pdf/2505.20789", "abs": "https://arxiv.org/abs/2505.20789", "authors": ["Yang Zheng", "Wen Li", "Zhaoqiang Liu"], "title": "Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Inverse problems (IPs) involve reconstructing signals from noisy\nobservations. Traditional approaches often rely on handcrafted priors, which\ncan fail to capture the complexity of real-world data. The advent of\npre-trained generative models has introduced new paradigms, offering improved\nreconstructions by learning rich priors from data. Among these, diffusion\nmodels (DMs) have emerged as a powerful framework, achieving remarkable\nreconstruction performance across numerous IPs. However, existing DM-based\nmethods frequently encounter issues such as heavy computational demands and\nsuboptimal convergence. In this work, building upon the idea of the recent work\nDMPlug~\\cite{wang2024dmplug}, we propose two novel methods, DMILO and\nDMILO-PGD, to address these challenges. Our first method, DMILO, employs\nintermediate layer optimization (ILO) to alleviate the memory burden inherent\nin DMPlug. Additionally, by introducing sparse deviations, we expand the range\nof DMs, enabling the exploration of underlying signals that may lie outside the\nrange of the diffusion model. We further propose DMILO-PGD, which integrates\nILO with projected gradient descent (PGD), thereby reducing the risk of\nsuboptimal convergence. We provide an intuitive theoretical analysis of our\napproach under appropriate conditions and validate its superiority through\nextensive experiments on diverse image datasets, encompassing both linear and\nnonlinear IPs. Our results demonstrate significant performance gains over\nstate-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD\nin addressing common challenges in DM-based IP solvers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\uff08DMILO\u548cDMILO-PGD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u6536\u655b\u95ee\u9898\u3002DMILO\u901a\u8fc7\u4e2d\u95f4\u5c42\u4f18\u5316\u51cf\u8f7b\u5185\u5b58\u8d1f\u62c5\uff0cDMILO-PGD\u7ed3\u5408\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u8fdb\u4e00\u6b65\u4f18\u5316\u6536\u655b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u5148\u9a8c\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u6570\u636e\u7684\u590d\u6742\u6027\u3002\u6269\u6563\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u8d1f\u62c5\u91cd\u548c\u6536\u655b\u4e0d\u4f18\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDMILO\uff08\u4e2d\u95f4\u5c42\u4f18\u5316\uff09\u548cDMILO-PGD\uff08\u7ed3\u5408\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff09\uff0c\u901a\u8fc7\u7a00\u758f\u504f\u5dee\u6269\u5c55\u6269\u6563\u6a21\u578b\u8303\u56f4\uff0c\u4f18\u5316\u6536\u655b\u3002", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DMILO\u548cDMILO-PGD\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u5e38\u89c1\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.20802", "pdf": "https://arxiv.org/pdf/2505.20802", "abs": "https://arxiv.org/abs/2505.20802", "authors": ["Hemanth Saratchandran", "Damien Teney", "Simon Lucey"], "title": "Leaner Transformers: More Heads, Less Depth", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Transformers have reshaped machine learning by utilizing attention mechanisms\nto capture complex patterns in large datasets, leading to significant\nimprovements in performance. This success has contributed to the belief that\n\"bigger means better\", leading to ever-increasing model sizes. This paper\nchallenge this ideology by showing that many existing transformers might be\nunnecessarily oversized. We discover a theoretical principle that redefines the\nrole of multi-head attention. An important benefit of the multiple heads is in\nimproving the conditioning of the attention block. We exploit this theoretical\ninsight and redesign popular architectures with an increased number of heads.\nThe improvement in the conditioning proves so significant in practice that\nmodel depth can be decreased, reducing the parameter count by up to 30-50%\nwhile maintaining accuracy. We obtain consistent benefits across a variety of\ntransformer-based architectures of various scales, on tasks in computer vision\n(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,\nTinyStories, and the Long-Range Arena benchmark).", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u201c\u8d8a\u5927\u8d8a\u597d\u201d\u7684Transformer\u6a21\u578b\u8bbe\u8ba1\u7406\u5ff5\uff0c\u63d0\u51fa\u901a\u8fc7\u589e\u52a0\u6ce8\u610f\u529b\u5934\u6570\u91cf\u6765\u6539\u5584\u6ce8\u610f\u529b\u5757\u7684\u6027\u80fd\uff0c\u4ece\u800c\u51cf\u5c11\u6a21\u578b\u6df1\u5ea6\u548c\u53c2\u6570\u6570\u91cf\uff0830-50%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u53ef\u80fd\u8fc7\u4e8e\u5e9e\u5927\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u91cd\u65b0\u5b9a\u4e49\u591a\u5934\u6ce8\u610f\u529b\u7684\u4f5c\u7528\uff0c\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7406\u8bba\u539f\u5219\uff0c\u5229\u7528\u591a\u5934\u6ce8\u610f\u529b\u6539\u5584\u6ce8\u610f\u529b\u5757\u7684\u6027\u80fd\uff0c\u5e76\u57fa\u4e8e\u6b64\u91cd\u65b0\u8bbe\u8ba1\u6d41\u884c\u67b6\u6784\uff0c\u589e\u52a0\u6ce8\u610f\u529b\u5934\u6570\u91cf\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\uff08\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u5e8f\u5217\u5efa\u6a21\uff09\u4e2d\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c1130-50%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6\u800c\u4e0d\u635f\u5931\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.20308", "pdf": "https://arxiv.org/pdf/2505.20308", "abs": "https://arxiv.org/abs/2505.20308", "authors": ["Muhammad Tayyab Khan", "Lequn Chen", "Wenhe Feng", "Seung Ki Moon"], "title": "Large Language Model-Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Metal additive manufacturing (AM) involves complex interdependencies among\nprocesses, materials, feedstock, and post-processing steps. However, the\nunderlying relationships and domain knowledge remain fragmented across\nliterature and static databases that often demand expert-level queries,\nlimiting their applicability in design and planning. To address these gaps, we\ndevelop a novel and queryable knowledge graph (KG) in Neo4j, encoding 53\ndistinct metals and alloys across seven material families, nine AM processes,\nfour feedstock types, and associated post-processing requirements. A large\nlanguage model (LLM) interface, guided by a few-shot prompting strategy,\nenables natural language querying without the need for formal query syntax. The\nsystem supports a range of tasks, including compatibility checks,\nmulti-constraint filtering, and design for AM (DfAM) guidance. User natural\nlanguage queries are normalized, translated into Cypher, and executed over the\nKG, with results reformatted into structured responses. This work presents the\nfirst real-time, interactive system that integrates a domain-specific metal AM\nKG with an LLM interface, offering accessible, explainable decision support for\nengineers and advancing human-centric tools in manufacturing intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u7528\u4e8e\u91d1\u5c5e\u589e\u6750\u5236\u9020\uff08AM\uff09\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u91d1\u5c5e\u589e\u6750\u5236\u9020\u6d89\u53ca\u590d\u6742\u7684\u591a\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u8d44\u6e90\u5206\u6563\u4e14\u96be\u4ee5\u67e5\u8be2\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bbe\u8ba1\u548c\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aNeo4j\u77e5\u8bc6\u56fe\u8c31\uff0c\u7f16\u7801\u4e8653\u79cd\u91d1\u5c5e\u548c\u5408\u91d1\u7684\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7LLM\u63a5\u53e3\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\uff0c\u63d0\u4f9b\u517c\u5bb9\u6027\u68c0\u67e5\u3001\u591a\u7ea6\u675f\u8fc7\u6ee4\u7b49\u529f\u80fd\uff0c\u63d0\u5347\u4e86\u5de5\u7a0b\u5e08\u7684\u51b3\u7b56\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u91d1\u5c5e\u589e\u6750\u5236\u9020\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u5236\u9020\u667a\u80fd\u5316\u7684\u53d1\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.20707", "pdf": "https://arxiv.org/pdf/2505.20707", "abs": "https://arxiv.org/abs/2505.20707", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Deepak Subramani"], "title": "Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective", "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "comment": null, "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, making them promising for educational applications. However,\ntheir capacity for complex reasoning, particularly in domains such as physics,\nremains underexplored. This study investigates the high school physics\nreasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),\nincluding instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.\nWe developed a comprehensive physics dataset from the OpenStax High School\nPhysics textbook, annotated according to Bloom's Taxonomy, with LaTeX and\nplaintext mathematical notations. A novel cultural contextualization approach\nwas applied to a subset, creating culturally adapted problems for Asian,\nAfrican, and South American/Australian contexts while preserving core physics\nprinciples. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,\nwe evaluated answer and reasoning chain correctness, along with calculation\naccuracy. The results reveal significant differences between the SLMs. Qwen 3\n1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was\nsubstantially low (38%). The format of the mathematical notation had a\nnegligible impact on performance. SLMs exhibited varied performance across the\nphysics topics and showed a decline in reasoning quality with increasing\ncognitive and knowledge complexity. In particular, the consistency of reasoning\nwas largely maintained in diverse cultural contexts, especially by better\nperforming models. These findings indicate that, while SLMs can often find\ncorrect answers, their underlying reasoning is frequently flawed, suggesting an\noverreliance on pattern recognition. For SLMs to become reliable educational\ntools in physics, future development must prioritize enhancing genuine\nunderstanding and the generation of sound, verifiable reasoning chains over\nmere answer accuracy.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u9ad8\u4e2d\u7269\u7406\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u867d\u7136\u67d0\u4e9b\u6a21\u578b\u7b54\u6848\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u63a8\u7406\u94fe\u7684\u6b63\u786e\u6027\u8f83\u4f4e\uff0c\u8868\u660e\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6a21\u5f0f\u8bc6\u522b\u800c\u975e\u771f\u6b63\u7406\u89e3\u3002", "motivation": "\u63a2\u7d22SLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u7269\u7406\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u5176\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528OpenStax\u9ad8\u4e2d\u7269\u7406\u6559\u6750\u6784\u5efa\u6570\u636e\u96c6\uff0c\u7ed3\u5408Bloom\u5206\u7c7b\u6cd5\u548c\u6587\u5316\u60c5\u5883\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7LLM-as-a-judge\u6846\u67b6\u8bc4\u4f30SLMs\u7684\u7b54\u6848\u548c\u63a8\u7406\u94fe\u6b63\u786e\u6027\u3002", "result": "Qwen 3 1.7B\u7b54\u6848\u51c6\u786e\u738785%\uff0c\u4f46\u5b8c\u5168\u6b63\u786e\u63a8\u7406\u4ec538%\uff1b\u6570\u5b66\u7b26\u53f7\u683c\u5f0f\u5bf9\u6027\u80fd\u5f71\u54cd\u5c0f\uff1b\u6587\u5316\u60c5\u5883\u5316\u5bf9\u63a8\u7406\u4e00\u81f4\u6027\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "SLMs\u9700\u589e\u5f3a\u771f\u5b9e\u7406\u89e3\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u94fe\u751f\u6210\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u624d\u80fd\u6210\u4e3a\u53ef\u9760\u6559\u80b2\u5de5\u5177\u3002", "relevance": 60.0}}
{"id": "2505.20793", "pdf": "https://arxiv.org/pdf/2505.20793", "abs": "https://arxiv.org/abs/2505.20793", "authors": ["Juan A. Rodriguez", "Haotian Zhang", "Abhay Puri", "Aarash Feizi", "Rishav Pramanik", "Pascal Wichmann", "Arnab Mondal", "Mohammad Reza Samsami", "Rabiul Awal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar", "David Vazquez", "Christopher Pal", "Marco Pedersoli"], "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Scalable Vector Graphics (SVG) offer a powerful format for representing\nvisual designs as interpretable code. Recent advances in vision-language models\n(VLMs) have enabled high-quality SVG generation by framing the problem as a\ncode generation task and leveraging large-scale pretraining. VLMs are\nparticularly suitable for this task as they capture both global semantics and\nfine-grained visual patterns, while transferring knowledge across vision,\nnatural language, and code domains. However, existing VLM approaches often\nstruggle to produce faithful and efficient SVGs because they never observe the\nrendered images during training. Although differentiable rendering for\nautoregressive SVG code generation remains unavailable, rendered outputs can\nstill be compared to original inputs, enabling evaluative feedback suitable for\nreinforcement learning (RL). We introduce RLRF(Reinforcement Learning from\nRendering Feedback), an RL method that enhances SVG generation in\nautoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an\ninput image, the model generates SVG roll-outs that are rendered and compared\nto the original image to compute a reward. This visual fidelity feedback guides\nthe model toward producing more accurate, efficient, and semantically coherent\nSVGs. RLRF significantly outperforms supervised fine-tuning, addressing common\nfailure modes and enabling precise, high-quality SVG generation with strong\nstructural understanding and generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLRF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5229\u7528\u6e32\u67d3\u53cd\u9988\u63d0\u5347SVG\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728SVG\u751f\u6210\u4e2d\u56e0\u7f3a\u4e4f\u8bad\u7ec3\u65f6\u7684\u6e32\u67d3\u89c2\u5bdf\u800c\u96be\u4ee5\u751f\u6210\u9ad8\u6548\u4e14\u51c6\u786e\u7684SVG\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5RLRF\uff0c\u901a\u8fc7\u6bd4\u8f83\u6e32\u67d3\u7684SVG\u8f93\u51fa\u4e0e\u539f\u59cb\u56fe\u50cf\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u4f18\u5316\u6a21\u578b\u751f\u6210\u6548\u679c\u3002", "result": "RLRF\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u80fd\u751f\u6210\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u8bed\u4e49\u4e00\u81f4\u7684SVG\u3002", "conclusion": "RLRF\u901a\u8fc7\u6e32\u67d3\u53cd\u9988\u6709\u6548\u89e3\u51b3\u4e86SVG\u751f\u6210\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 60.0}}
{"id": "2505.20804", "pdf": "https://arxiv.org/pdf/2505.20804", "abs": "https://arxiv.org/abs/2505.20804", "authors": ["Antonio Tudisco", "Deborah Volpe", "Giovanna Turvani"], "title": "Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "Effective and accurate diagnosis of diseases such as cancer, diabetes, and\nheart failure is crucial for timely medical intervention and improving patient\nsurvival rates. Machine learning has revolutionized diagnostic methods in\nrecent years by developing classification models that detect diseases based on\nselected features. However, these classification tasks are often highly\nimbalanced, limiting the performance of classical models. Quantum models offer\na promising alternative, exploiting their ability to express complex patterns\nby operating in a higher-dimensional computational space through superposition\nand entanglement. These unique properties make quantum models potentially more\neffective in addressing the challenges of imbalanced datasets. This work\nevaluates the potential of quantum classifiers in healthcare, focusing on\nQuantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs),\ncomparing them with popular classical models. The study is based on three\nwell-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes.\nThe results indicate that QSVMs outperform QNNs across all datasets due to\ntheir susceptibility to overfitting. Furthermore, quantum models prove the\nability to overcome classical models in scenarios with high dataset imbalance.\nAlthough preliminary, these findings highlight the potential of quantum models\nin healthcare classification tasks and lead the way for further research in\nthis domain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u6a21\u578b\uff08\u5982QNNs\u548cQSVMs\uff09\u5728\u533b\u7597\u8bca\u65ad\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0QSVMs\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u3002", "motivation": "\u533b\u7597\u8bca\u65ad\u5206\u7c7b\u4efb\u52a1\u5e38\u9762\u4e34\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7ecf\u5178\u6a21\u578b\u6027\u80fd\u53d7\u9650\uff0c\u91cf\u5b50\u6a21\u578b\u56e0\u5176\u9ad8\u7ef4\u8ba1\u7b97\u7279\u6027\u53ef\u80fd\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u91cf\u5b50\u6a21\u578b\uff08QNNs\u548cQSVMs\uff09\u4e0e\u7ecf\u5178\u6a21\u578b\u5728\u4e09\u4e2a\u533b\u7597\u6570\u636e\u96c6\uff08\u524d\u5217\u817a\u764c\u3001\u5fc3\u8870\u3001\u7cd6\u5c3f\u75c5\uff09\u4e0a\u7684\u8868\u73b0\u3002", "result": "QSVMs\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eQNNs\uff0c\u4e14\u91cf\u5b50\u6a21\u578b\u5728\u9ad8\u6570\u636e\u4e0d\u5e73\u8861\u573a\u666f\u4e0b\u8d85\u8d8a\u7ecf\u5178\u6a21\u578b\u3002", "conclusion": "\u91cf\u5b50\u6a21\u578b\u5728\u533b\u7597\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "relevance": 30.0}}
{"id": "2505.20312", "pdf": "https://arxiv.org/pdf/2505.20312", "abs": "https://arxiv.org/abs/2505.20312", "authors": ["Aditya Bhattacharya", "Katrien Verbert"], "title": "Let's Get You Hired: A Job Seeker's Perspective on Multi-Agent Recruitment Systems for Explaining Hiring Decisions", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": "Pre-print version only. Please check the published version for any\n  reference or citation", "summary": "During job recruitment, traditional applicant selection methods often lack\ntransparency. Candidates are rarely given sufficient justifications for\nrecruiting decisions, whether they are made manually by human recruiters or\nthrough the use of black-box Applicant Tracking Systems (ATS). To address this\nproblem, our work introduces a multi-agent AI system that uses Large Language\nModels (LLMs) to guide job seekers during the recruitment process. Using an\niterative user-centric design approach, we first conducted a two-phased\nexploratory study with four active job seekers to inform the design and\ndevelopment of the system. Subsequently, we conducted an in-depth, qualitative\nuser study with 20 active job seekers through individual one-to-one interviews\nto evaluate the developed prototype. The results of our evaluation demonstrate\nthat participants perceived our multi-agent recruitment system as significantly\nmore actionable, trustworthy, and fair compared to traditional methods. Our\nstudy further helped us uncover in-depth insights into factors contributing to\nthese perceived user experiences. Drawing from these insights, we offer broader\ndesign implications for building user-aligned, multi-agent explainable AI\nsystems across diverse domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff08\u4f7f\u7528LLMs\uff09\u7684\u62db\u8058\u8f85\u52a9\u5de5\u5177\uff0c\u65e8\u5728\u63d0\u9ad8\u62db\u8058\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u62db\u8058\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\uff0c\u4e3a\u6c42\u804c\u8005\u63d0\u4f9b\u66f4\u53ef\u4fe1\u548c\u516c\u5e73\u7684\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u7684\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\u63a2\u7d22\u6027\u7814\u7a76\u548c20\u540d\u6c42\u804c\u8005\u7684\u5b9a\u6027\u7528\u6237\u7814\u7a76\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u4e3a\u8be5\u7cfb\u7edf\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u64cd\u4f5c\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u516c\u5e73\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u7528\u6237\u5bf9\u9f50\u7684\u591a\u667a\u80fd\u4f53\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\u3002", "relevance": 60.0}}
{"id": "2505.20732", "pdf": "https://arxiv.org/pdf/2505.20732", "abs": "https://arxiv.org/abs/2505.20732", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) holds significant promise for training LLM agents\nto handle complex, goal-oriented tasks that require multi-step interactions\nwith external environments. However, a critical challenge when applying RL to\nthese agentic tasks arises from delayed rewards: feedback signals are typically\navailable only after the entire task is completed. This makes it non-trivial to\nassign delayed rewards to earlier actions, providing insufficient guidance\nregarding environmental constraints and hindering agent training. In this work,\nwe draw on the insight that the ultimate completion of a task emerges from the\ncumulative progress an agent makes across individual steps. We propose Stepwise\nProgress Attribution (SPA), a general reward redistribution framework that\ndecomposes the final reward into stepwise contributions, each reflecting its\nincremental progress toward overall task completion. To achieve this, we train\na progress estimator that accumulates stepwise contributions over a trajectory\nto match the task completion. During policy optimization, we combine the\nestimated per-step contribution with a grounding signal for actions executed in\nthe environment as the fine-grained, intermediate reward for effective agent\ntraining. Extensive experiments on common agent benchmarks (including Webshop,\nALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the\nstate-of-the-art method in both success rate (+2.5\\% on average) and grounding\naccuracy (+1.9\\% on average). Further analyses demonstrate that our method\nremarkably provides more effective intermediate rewards for RL training. Our\ncode is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPA\u7684\u5956\u52b1\u518d\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u5ef6\u8fdf\u5956\u52b1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u6700\u7ec8\u5956\u52b1\u4e3a\u9010\u6b65\u8d21\u732e\uff0c\u63d0\u5347LLM\u4ee3\u7406\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u5ef6\u8fdf\u5956\u52b1\u5bf9LLM\u4ee3\u7406\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e2d\u95f4\u5956\u52b1\u4ee5\u4f18\u5316\u8bad\u7ec3\u3002", "method": "\u63d0\u51faStepwise Progress Attribution (SPA)\u6846\u67b6\uff0c\u8bad\u7ec3\u8fdb\u5ea6\u4f30\u8ba1\u5668\u5206\u89e3\u6700\u7ec8\u5956\u52b1\u4e3a\u9010\u6b65\u8d21\u732e\uff0c\u5e76\u7ed3\u5408\u73af\u5883\u52a8\u4f5c\u4fe1\u53f7\u4f5c\u4e3a\u4e2d\u95f4\u5956\u52b1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPA\u5728\u6210\u529f\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5e73\u5747\u63d0\u53472.5%\u548c1.9%\uff09\u3002", "conclusion": "SPA\u901a\u8fc7\u6709\u6548\u7684\u4e2d\u95f4\u5956\u52b1\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u679c\u3002", "relevance": 85.0}}
{"id": "2505.20808", "pdf": "https://arxiv.org/pdf/2505.20808", "abs": "https://arxiv.org/abs/2505.20808", "authors": ["Bo-Kai Ruan", "Zi-Xiang Ni", "Bo-Lun Huang", "Teng-Fang Hsiao", "Hong-Han Shuai"], "title": "Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown strong capabilities in high-fidelity image\ngeneration but often falter when synthesizing rare concepts, i.e., prompts that\nare infrequently observed in the training distribution. In this paper, we\nintroduce RAP, a principled framework that treats rare concept generation as\nnavigating a latent causal path: a progressive, model-aligned trajectory\nthrough the generative space from frequent concepts to rare targets. Rather\nthan relying on heuristic prompt alternation, we theoretically justify that\nrare prompt guidance can be approximated by semantically related frequent\nprompts. We then formulate prompt switching as a dynamic process based on score\nsimilarity, enabling adaptive stage transitions. Furthermore, we reinterpret\nprompt alternation as a second-order denoising mechanism, promoting smooth\nsemantic progression and coherent visual synthesis. Through this causal lens,\nwe align input scheduling with the model's internal generative dynamics.\nExperiments across diverse diffusion backbones demonstrate that RAP\nconsistently enhances rare concept generation, outperforming strong baselines\nin both automated evaluations and human studies.", "AI": {"tldr": "RAP\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u56e0\u679c\u8def\u5f84\u6539\u8fdb\u6269\u6563\u6a21\u578b\u5728\u7a00\u6709\u6982\u5ff5\u751f\u6210\u4e0a\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u65f6\u5bf9\u7a00\u6709\u6982\u5ff5\u8868\u73b0\u4e0d\u4f73\uff0cRAP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u7a00\u6709\u6982\u5ff5\u751f\u6210\u89c6\u4e3a\u6f5c\u5728\u56e0\u679c\u8def\u5f84\u5bfc\u822a\uff0c\u5229\u7528\u8bed\u4e49\u76f8\u5173\u7684\u9891\u7e41\u63d0\u793a\u8fd1\u4f3c\u7a00\u6709\u63d0\u793a\uff0c\u52a8\u6001\u5207\u6362\u63d0\u793a\u5e76\u91cd\u65b0\u89e3\u91ca\u4e3a\u4e8c\u9636\u53bb\u566a\u673a\u5236\u3002", "result": "RAP\u5728\u591a\u79cd\u6269\u6563\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u7a00\u6709\u6982\u5ff5\u751f\u6210\u6548\u679c\uff0c\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u7814\u7a76\u5747\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "RAP\u901a\u8fc7\u56e0\u679c\u8def\u5f84\u548c\u52a8\u6001\u63d0\u793a\u5207\u6362\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u7a00\u6709\u6982\u5ff5\u7684\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.20807", "pdf": "https://arxiv.org/pdf/2505.20807", "abs": "https://arxiv.org/abs/2505.20807", "authors": ["Yurui Lai", "Taiyan Zhang", "Renchi Yang"], "title": "Simple yet Effective Graph Distillation via Clustering", "categories": ["cs.LG"], "comment": "This is the technical report of the paper \"Simple yet Effective Graph\n  Distillation via Clustering\" accepted by KDD 2025", "summary": "Despite plentiful successes achieved by graph representation learning in\nvarious domains, the training of graph neural networks (GNNs) still remains\ntenaciously challenging due to the tremendous computational overhead needed for\nsizable graphs in practice. Recently, graph data distillation (GDD), which\nseeks to distill large graphs into compact and informative ones, has emerged as\na promising technique to enable efficient GNN training. However, most existing\nGDD works rely on heuristics that align model gradients or representation\ndistributions on condensed and original graphs, leading to compromised result\nquality, expensive training for distilling large graphs, or both. Motivated by\nthis, this paper presents an efficient and effective GDD approach, ClustGDD.\nUnder the hood, ClustGDD resorts to synthesizing the condensed graph and node\nattributes through fast and theoretically-grounded clustering that minimizes\nthe within-cluster sum of squares and maximizes the homophily on the original\ngraph. The fundamental idea is inspired by our empirical and theoretical\nfindings unveiling the connection between clustering and empirical condensation\nquality using Fr\\'echet Inception Distance, a well-known quality metric for\nsynthetic images. Furthermore, to mitigate the adverse effects caused by the\nhomophily-based clustering, ClustGDD refines the nodal attributes of the\ncondensed graph with a small augmentation learned via class-aware graph\nsampling and consistency loss. Our extensive experiments exhibit that GNNs\ntrained over condensed graphs output by ClustGDD consistently achieve superior\nor comparable performance to state-of-the-art GDD methods in terms of node\nclassification on five benchmark datasets, while being orders of magnitude\nfaster.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u6570\u636e\u84b8\u998f\u65b9\u6cd5ClustGDD\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u540c\u8d28\u6027\u4f18\u5316\u6765\u538b\u7f29\u5927\u56fe\uff0c\u63d0\u5347GNN\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u56fe\u6570\u636e\u84b8\u998f\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u5bf9\u9f50\u68af\u5ea6\u6216\u8868\u793a\u5206\u5e03\uff0c\u5bfc\u81f4\u7ed3\u679c\u8d28\u91cf\u4f4e\u6216\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "ClustGDD\u901a\u8fc7\u5feb\u901f\u805a\u7c7b\u548c\u540c\u8d28\u6027\u6700\u5927\u5316\u5408\u6210\u538b\u7f29\u56fe\uff0c\u5e76\u7ed3\u5408\u7c7b\u611f\u77e5\u56fe\u91c7\u6837\u548c\u4e00\u81f4\u6027\u635f\u5931\u4f18\u5316\u8282\u70b9\u5c5e\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cClustGDD\u751f\u6210\u7684\u538b\u7f29\u56fe\u8bad\u7ec3\u7684GNN\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u5feb\u5f97\u591a\u3002", "conclusion": "ClustGDD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u56fe\u6570\u636e\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u8bad\u7ec3\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.20738", "pdf": "https://arxiv.org/pdf/2505.20738", "abs": "https://arxiv.org/abs/2505.20738", "authors": ["Peiwen Yuan", "Yiwei Li", "Shaoxiong Feng", "Xinglin Wang", "Yueqi Zhang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-Benchmark-Generator methods have been widely studied as a supplement\nto human annotators for scalable evaluation, while the potential biases within\nthis paradigm remain underexplored. In this work, we systematically define and\nvalidate the phenomenon of inflated performance in models evaluated on their\nself-generated benchmarks, referred to as self-bias, and attribute it to\nsub-biases arising from question domain, language style, and wrong labels. On\nthis basis, we propose Silencer, a general framework that leverages the\nheterogeneity between multiple generators at both the sample and benchmark\nlevels to neutralize bias and generate high-quality, self-bias-silenced\nbenchmark. Experimental results across various settings demonstrate that\nSilencer can suppress self-bias to near zero, significantly improve evaluation\neffectiveness of the generated benchmark (with an average improvement from\n0.655 to 0.833 in Pearson correlation with high-quality human-annotated\nbenchmark), while also exhibiting strong generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSilencer\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u751f\u6210\u5668\u7684\u5f02\u8d28\u6027\u4e2d\u548cLLM\u81ea\u751f\u6210\u8bc4\u6d4b\u57fa\u51c6\u4e2d\u7684\u81ea\u504f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u6d4b\u6548\u679c\u3002", "motivation": "\u7814\u7a76LLM\u81ea\u751f\u6210\u8bc4\u6d4b\u57fa\u51c6\u4e2d\u7684\u6f5c\u5728\u504f\u7f6e\uff08\u81ea\u504f\uff09\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u8bc4\u6d4b\u8d28\u91cf\u3002", "method": "\u5b9a\u4e49\u5e76\u9a8c\u8bc1\u81ea\u504f\u73b0\u8c61\uff0c\u63d0\u51faSilencer\u6846\u67b6\uff0c\u5229\u7528\u591a\u751f\u6210\u5668\u7684\u5f02\u8d28\u6027\u5728\u6837\u672c\u548c\u57fa\u51c6\u5c42\u9762\u4e2d\u548c\u504f\u7f6e\u3002", "result": "Silencer\u80fd\u5c06\u81ea\u504f\u964d\u81f3\u63a5\u8fd1\u96f6\uff0c\u8bc4\u6d4b\u6548\u679c\u663e\u8457\u63d0\u5347\uff08Pearson\u76f8\u5173\u7cfb\u6570\u4ece0.655\u63d0\u5347\u81f30.833\uff09\uff0c\u4e14\u5177\u6709\u5f3a\u6cdb\u5316\u6027\u3002", "conclusion": "Silencer\u662f\u4e00\u79cd\u6709\u6548\u4e2d\u548c\u81ea\u504f\u7684\u901a\u7528\u6846\u67b6\uff0c\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u6d4b\u57fa\u51c6\u3002", "relevance": 85.0}}
{"id": "2505.20827", "pdf": "https://arxiv.org/pdf/2505.20827", "abs": "https://arxiv.org/abs/2505.20827", "authors": ["Guangcong Zheng", "Jianlong Yuan", "Bo Wang", "Haoyang Huang", "Guoqing Ma", "Nan Duan"], "title": "Frame-Level Captions for Long Video Generation with Complex Multi Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Generating long videos that can show complex stories, like movie scenes from\nscripts, has great promise and offers much more than short clips. However,\ncurrent methods that use autoregression with diffusion models often struggle\nbecause their step-by-step process naturally leads to a serious error\naccumulation (drift). Also, many existing ways to make long videos focus on\nsingle, continuous scenes, making them less useful for stories with many events\nand changes. This paper introduces a new approach to solve these problems.\nFirst, we propose a novel way to annotate datasets at the frame-level,\nproviding detailed text guidance needed for making complex, multi-scene long\nvideos. This detailed guidance works with a Frame-Level Attention Mechanism to\nmake sure text and video match precisely. A key feature is that each part\n(frame) within these windows can be guided by its own distinct text prompt. Our\ntraining uses Diffusion Forcing to provide the model with the ability to handle\ntime flexibly. We tested our approach on difficult VBench 2.0 benchmarks\n(\"Complex Plots\" and \"Complex Landscapes\") based on the WanX2.1-T2V-1.3B model.\nThe results show our method is better at following instructions in complex,\nchanging scenes and creates high-quality long videos. We plan to share our\ndataset annotation methods and trained models with the research community.\nProject page: https://zgctroy.github.io/frame-level-captions .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e27\u7ea7\u6807\u6ce8\u548c\u5e27\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u591a\u573a\u666f\u5207\u6362\u95ee\u9898\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u591a\u573a\u666f\u5207\u6362\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e27\u7ea7\u6807\u6ce8\u548c\u5e27\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408Diffusion Forcing\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6587\u672c\u4e0e\u89c6\u9891\u7684\u7cbe\u786e\u5339\u914d\u3002", "result": "\u5728VBench 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u3001\u591a\u53d8\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u591a\u573a\u666f\u5904\u7406\u80fd\u529b\uff0c\u8ba1\u5212\u516c\u5f00\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "relevance": 60.0}}
{"id": "2505.20815", "pdf": "https://arxiv.org/pdf/2505.20815", "abs": "https://arxiv.org/abs/2505.20815", "authors": ["Shiqi Yang", "Ziyi Huang", "Wengran Xiao", "Xinyu Shen"], "title": "Interpretable Credit Default Prediction with Ensemble Learning and SHAP", "categories": ["cs.LG"], "comment": null, "summary": "This study focuses on the problem of credit default prediction, builds a\nmodeling framework based on machine learning, and conducts comparative\nexperiments on a variety of mainstream classification algorithms. Through\npreprocessing, feature engineering, and model training of the Home Credit\ndataset, the performance of multiple models including logistic regression,\nrandom forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and\nrecall is evaluated. The results show that the ensemble learning method has\nobvious advantages in predictive performance, especially in dealing with\ncomplex nonlinear relationships between features and data imbalance problems.\nIt shows strong robustness. At the same time, the SHAP method is used to\nanalyze the importance and dependency of features, and it is found that the\nexternal credit score variable plays a dominant role in model decision making,\nwhich helps to improve the model's interpretability and practical application\nvalue. The research results provide effective reference and technical support\nfor the intelligent development of credit risk control systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6bd4\u8f83\u591a\u79cd\u5206\u7c7b\u7b97\u6cd5\u5728\u4fe1\u7528\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u901a\u8fc7SHAP\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e3a\u4fe1\u7528\u98ce\u9669\u63a7\u5236\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u53d1\u5c55\u63d0\u4f9b\u6280\u672f\u652f\u6301\u548c\u53c2\u8003\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5bfb\u627e\u6700\u4f18\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u4f7f\u7528Home Credit\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u5de5\u7a0b\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u6bd4\u8f83\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001LightGBM\u7b49\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5229\u7528SHAP\u65b9\u6cd5\u5206\u6790\u7279\u5f81\u91cd\u8981\u6027\u3002", "result": "\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff08\u5982XGBoost\u3001LightGBM\uff09\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002SHAP\u5206\u6790\u663e\u793a\u5916\u90e8\u4fe1\u7528\u8bc4\u5206\u53d8\u91cf\u5bf9\u6a21\u578b\u51b3\u7b56\u8d77\u4e3b\u5bfc\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4fe1\u7528\u98ce\u9669\u63a7\u5236\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u53c2\u8003\uff0c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u548cSHAP\u5206\u6790\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 20.0}}
{"id": "2505.20767", "pdf": "https://arxiv.org/pdf/2505.20767", "abs": "https://arxiv.org/abs/2505.20767", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Faithfulness hallucination are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandard, existing benchmarks only contain \"factual statements\" that rephrase\nsource materials without marking \"cognitive statements\" that make inference\nfrom the given context, making the consistency evaluation and optimization of\ncognitive statements difficult. Inspired by how an evidence is assessed in the\nlegislative domain, we design a rigorous framework to assess different levels\nof faithfulness of cognitive statements and create a benchmark dataset where we\nreveal insightful statistics. We design an annotation pipeline to create larger\nbenchmarks for different LLMs automatically, and the resulting larger-scale\nCogniBench-L dataset can be used to train accurate cognitive hallucination\ndetection model. We release our model and dataset at:\nhttps://github.com/FUTUREEEEEE/CogniBench", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u8ba4\u77e5\u9648\u8ff0\u5fe0\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86CogniBench-L\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u5305\u542b\u91cd\u8ff0\u6e90\u6750\u6599\u7684\u201c\u4e8b\u5b9e\u9648\u8ff0\u201d\uff0c\u7f3a\u4e4f\u5bf9\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u7684\u201c\u8ba4\u77e5\u9648\u8ff0\u201d\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5bfc\u81f4\u5176\u5fe0\u5b9e\u6027\u96be\u4ee5\u8bc4\u4f30\u548c\u4f18\u5316\u3002", "method": "\u53d7\u7acb\u6cd5\u9886\u57df\u8bc1\u636e\u8bc4\u4f30\u7684\u542f\u53d1\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u8ba4\u77e5\u9648\u8ff0\u7684\u4e0d\u540c\u5fe0\u5b9e\u6027\u7ea7\u522b\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7edf\u8ba1\u6570\u636e\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u4ee5\u751f\u6210\u66f4\u5927\u89c4\u6a21\u7684CogniBench-L\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u8bad\u7ec3\u51c6\u786e\u7684\u8ba4\u77e5\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7CogniBench-L\u6570\u636e\u96c6\u548c\u68c0\u6d4b\u6a21\u578b\uff0c\u4e3aLLM\u751f\u6210\u7684\u8ba4\u77e5\u9648\u8ff0\u7684\u5fe0\u5b9e\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.20830", "pdf": "https://arxiv.org/pdf/2505.20830", "abs": "https://arxiv.org/abs/2505.20830", "authors": ["Linli Ma", "Suzhen Lin", "Jianchao Zeng", "Zanxia Jin", "Yanbo Wang", "Fengyuan Li", "Yubing Luo"], "title": "Causality-Driven Infrared and Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion aims to combine complementary information from multiple source\nimages to generate more comprehensive scene representations. Existing methods\nprimarily rely on the stacking and design of network architectures to enhance\nthe fusion performance, often ignoring the impact of dataset scene bias on\nmodel training. This oversight leads the model to learn spurious correlations\nbetween specific scenes and fusion weights under conventional likelihood\nestimation framework, thereby limiting fusion performance. To solve the above\nproblems, this paper first re-examines the image fusion task from the causality\nperspective, and disentangles the model from the impact of bias by constructing\na tailored causal graph to clarify the causalities among the variables in image\nfusion task. Then, the Back-door Adjustment based Feature Fusion Module (BAFFM)\nis proposed to eliminate confounder interference and enable the model to learn\nthe true causal effect. Finally, Extensive experiments on three standard\ndatasets prove that the proposed method significantly surpasses\nstate-of-the-art methods in infrared and visible image fusion.", "AI": {"tldr": "\u672c\u6587\u4ece\u56e0\u679c\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u56fe\u50cf\u878d\u5408\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u95e8\u8c03\u6574\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08BAFFM\uff09\uff0c\u4ee5\u6d88\u9664\u6df7\u6742\u56e0\u7d20\u5e72\u6270\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u5e38\u5ffd\u7565\u6570\u636e\u96c6\u573a\u666f\u504f\u5dee\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u5230\u865a\u5047\u76f8\u5173\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u5206\u6790\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efa\u56e0\u679c\u56fe\u4ee5\u660e\u786e\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u63d0\u51faBAFFM\u6a21\u5757\u6d88\u9664\u6df7\u6742\u56e0\u7d20\u5e72\u6270\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u548cBAFFM\u6a21\u5757\uff0c\u672c\u6587\u6210\u529f\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.20836", "pdf": "https://arxiv.org/pdf/2505.20836", "abs": "https://arxiv.org/abs/2505.20836", "authors": ["Hexiong Yang", "Mingrui Chen", "Huaibo Huang", "Junxian Duan", "Jie Cao", "Zhen Zhou", "Ran He"], "title": "HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Inspired by the great success of Masked Language Modeling (MLM) in the\nnatural language domain, the paradigm of self-supervised pre-training and\nfine-tuning has also achieved remarkable progress in the field of DNA sequence\nmodeling. However, previous methods often relied on massive pre-training data\nor large-scale base models with huge parameters, imposing a significant\ncomputational burden. To address this, many works attempted to use more compact\nmodels to achieve similar outcomes but still fell short by a considerable\nmargin. In this work, we propose a Hybrid Architecture Distillation (HAD)\napproach, leveraging both distillation and reconstruction tasks for more\nefficient and effective pre-training. Specifically, we employ the NTv2-500M as\nthe teacher model and devise a grouping masking strategy to align the feature\nembeddings of visible tokens while concurrently reconstructing the invisible\ntokens during MLM pre-training. To validate the effectiveness of our proposed\nmethod, we conducted comprehensive experiments on the Nucleotide Transformer\nBenchmark and Genomic Benchmark. Compared to models with similar parameters,\nour model achieved excellent performance. More surprisingly, it even surpassed\nthe distillation ceiling-teacher model on some sub-tasks, which is more than\n500 $\\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization,\nwhich shows that our model can gain a sophisticated understanding of the\nintrinsic representation pattern in genomic sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\u84b8\u998f\uff08HAD\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u84b8\u998f\u548c\u91cd\u5efa\u4efb\u52a1\uff0c\u7528\u4e8e\u66f4\u9ad8\u6548\u7684DNA\u5e8f\u5217\u5efa\u6a21\u9884\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u6216\u5927\u53c2\u6570\u6a21\u578b\u5e26\u6765\u7684\u8ba1\u7b97\u8d1f\u62c5\u95ee\u9898\u3002", "method": "\u4f7f\u7528NTv2-500M\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u8bbe\u8ba1\u5206\u7ec4\u63a9\u7801\u7b56\u7565\uff0c\u5bf9\u9f50\u53ef\u89c1\u4ee4\u724c\u7684\u7279\u5f81\u5d4c\u5165\u5e76\u91cd\u5efa\u4e0d\u53ef\u89c1\u4ee4\u724c\u3002", "result": "\u5728Nucleotide Transformer Benchmark\u548cGenomic Benchmark\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u53c2\u6570\u5927500\u500d\u7684\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "HAD\u65b9\u6cd5\u5728DNA\u5e8f\u5217\u5efa\u6a21\u4e2d\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u80fd\u6df1\u5165\u7406\u89e3\u57fa\u56e0\u7ec4\u5e8f\u5217\u7684\u5185\u5728\u8868\u793a\u6a21\u5f0f\u3002", "relevance": 70.0}}
{"id": "2505.20776", "pdf": "https://arxiv.org/pdf/2505.20776", "abs": "https://arxiv.org/abs/2505.20776", "authors": ["Jungyoub Cha", "Hyunjong Kim", "Sungzoon Cho"], "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; C.4"], "comment": "8 pages, 3 figures. Under review at EMNLP 2025", "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .", "AI": {"tldr": "SpecExtend\u901a\u8fc7\u96c6\u6210\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u548c\u8de8\u6a21\u578b\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u4e0a\u7684\u63a8\u6d4b\u89e3\u7801\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u63a8\u6d4b\u89e3\u7801\u5728\u957f\u8f93\u5165\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6ce8\u610f\u529b\u6210\u672c\u589e\u52a0\u548c\u8349\u7a3f\u51c6\u786e\u6027\u964d\u4f4e\u3002", "method": "\u7ed3\u5408FlashAttention\u548cHybrid Tree Attention\uff0c\u63d0\u51fa\u8de8\u6a21\u578b\u68c0\u7d22\u7b56\u7565\u52a8\u6001\u66f4\u65b0KV\u7f13\u5b58\u3002", "result": "\u572816K tokens\u7684\u8f93\u5165\u4e0a\uff0c\u52a0\u901f\u8fbe2.22\u500d\u3002", "conclusion": "SpecExtend\u4e3a\u957f\u5e8f\u5217\u63a8\u6d4b\u89e3\u7801\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20834", "pdf": "https://arxiv.org/pdf/2505.20834", "abs": "https://arxiv.org/abs/2505.20834", "authors": ["Jingjun Yang", "Liangwei Fan", "Jinpu Zhang", "Xiangkai Lian", "Hui Shen", "Dewen Hu"], "title": "Fully Spiking Neural Networks for Unified Frame-Event Object Tracking", "categories": ["cs.CV", "cs.NE"], "comment": "13 pages,6 figures,4 tables", "summary": "The integration of image and event streams offers a promising approach for\nachieving robust visual object tracking in complex environments. However,\ncurrent fusion methods achieve high performance at the cost of significant\ncomputational overhead and struggle to efficiently extract the sparse,\nasynchronous information from event streams, failing to leverage the\nenergy-efficient advantages of event-driven spiking paradigms. To address this\nchallenge, we propose the first fully Spiking Frame-Event Tracking framework\ncalled SpikeFET. This network achieves synergistic integration of convolutional\nlocal feature extraction and Transformer-based global modeling within the\nspiking paradigm, effectively fusing frame and event data. To overcome the\ndegradation of translation invariance caused by convolutional padding, we\nintroduce a Random Patchwork Module (RPM) that eliminates positional bias\nthrough randomized spatial reorganization and learnable type encoding while\npreserving residual structures. Furthermore, we propose a Spatial-Temporal\nRegularization (STR) strategy that overcomes similarity metric degradation from\nasymmetric features by enforcing spatio-temporal consistency among temporal\ntemplate features in latent space. Extensive experiments across multiple\nbenchmarks demonstrate that the proposed framework achieves superior tracking\naccuracy over existing methods while significantly reducing power consumption,\nattaining an optimal balance between performance and efficiency. The code will\nbe released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpikeFET\u7684\u5168\u8109\u51b2\u5e27\u4e8b\u4ef6\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8eTransformer\u7684\u5168\u5c40\u5efa\u6a21\uff0c\u9ad8\u6548\u878d\u5408\u5e27\u548c\u4e8b\u4ef6\u6570\u636e\uff0c\u540c\u65f6\u964d\u4f4e\u529f\u8017\u3002", "motivation": "\u5f53\u524d\u878d\u5408\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e14\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u4e8b\u4ef6\u6d41\u4e2d\u7684\u7a00\u758f\u5f02\u6b65\u4fe1\u606f\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u8109\u51b2\u8303\u5f0f\u7684\u80fd\u6548\u4f18\u52bf\u3002", "method": "\u63d0\u51faSpikeFET\u6846\u67b6\uff0c\u7ed3\u5408\u5377\u79ef\u548cTransformer\uff0c\u5f15\u5165\u968f\u673a\u62fc\u56fe\u6a21\u5757\uff08RPM\uff09\u6d88\u9664\u4f4d\u7f6e\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u65f6\u7a7a\u6b63\u5219\u5316\uff08STR\uff09\u7b56\u7565\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpikeFET\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u529f\u8017\u3002", "conclusion": "SpikeFET\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4e3a\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20839", "pdf": "https://arxiv.org/pdf/2505.20839", "abs": "https://arxiv.org/abs/2505.20839", "authors": ["Daehyeon Baek", "Jieun Choi", "Jimyoung Son", "Kyungmin Bin", "Seungbeom Choi", "Kihyo Moon", "Minsung Jang", "Hyojung Lee"], "title": "FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration", "categories": ["cs.LG"], "comment": null, "summary": "As large language models become increasingly prevalent, memory bandwidth\nconstraints significantly limit inference throughput, motivating post-training\nquantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ\nframework and an INT4-FP8 matrix multiplication kernel that accelerates LLM\ninference across all linear layers. Specifically, FireQ quantizes linear layer\nweights and key-values to INT4, and activations and queries to FP8,\nsignificantly enhancing throughput. Additionally, we introduce a three-stage\npipelining for the prefill phase, which modifies the FlashAttention-3 kernel,\neffectively reducing time-to-first-token in the prefill phase. To minimize\naccuracy loss from quantization, we develop novel outlier smoothing techniques\ntailored separately for linear and attention layers. In linear layers, we\nexplicitly use per-tensor scaling to prevent underflow caused by the FP8\nquantization scaling factor of INT4 quantization, and channel-wise scaling to\ncompensate for coarse granularity of INT4. In attention layers, we address\nquantization challenges posed by rotary positional embeddings (RoPE) by\ncombining pre-RoPE and post-RoPE scaling strategies. FireQ significantly\noutperforms state-of-the-art methods, achieving 1.68x faster inference in\nfeed-forward network layers on Llama2-7B and 1.26x faster prefill phase\nperformance on Llama3-8B compared to QServe, with negligible accuracy loss.", "AI": {"tldr": "FireQ\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7INT4-FP8\u6df7\u5408\u91cf\u5316\u548c\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u541e\u5410\u91cf\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53d7\u5185\u5b58\u5e26\u5bbd\u9650\u5236\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6210\u4e3a\u63d0\u5347\u541e\u5410\u91cf\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faFireQ\u6846\u67b6\uff0c\u7ed3\u5408INT4-FP8\u6df7\u5408\u91cf\u5316\u3001\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u8bbe\u8ba1\u548c\u9488\u5bf9\u7ebf\u6027\u5c42\u4e0e\u6ce8\u610f\u529b\u5c42\u7684\u5f02\u5e38\u5e73\u6ed1\u6280\u672f\u3002", "result": "\u5728Llama2-7B\u548cLlama3-8B\u4e0a\u5206\u522b\u5b9e\u73b01.68\u500d\u548c1.26\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "FireQ\u901a\u8fc7\u91cf\u5316\u4f18\u5316\u548c\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "relevance": 85.0}}
{"id": "2505.20779", "pdf": "https://arxiv.org/pdf/2505.20779", "abs": "https://arxiv.org/abs/2505.20779", "authors": ["Noy Sternlicht", "Tom Hope"], "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature", "categories": ["cs.CL"], "comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web", "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCHIMERA\uff0c\u4e00\u4e2a\u901a\u8fc7\u81ea\u52a8\u6316\u6398\u79d1\u5b66\u6587\u732e\u6784\u5efa\u7684\u5927\u89c4\u6a21\u77e5\u8bc6\u5e93\uff0c\u7528\u4e8e\u63a2\u7d22\u548c\u9884\u6d4b\u8de8\u9886\u57df\u6982\u5ff5\u91cd\u7ec4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u79d1\u5b66\u5bb6\u5982\u4f55\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709\u6982\u5ff5\u521b\u65b0\uff0c\u5e76\u6784\u5efa\u5de5\u5177\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4ece\u8bba\u6587\u6458\u8981\u4e2d\u63d0\u53d6\u91cd\u7ec4\u4fe1\u606f\uff0c\u8bad\u7ec3\u57fa\u4e8eLLM\u7684\u63d0\u53d6\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u5305\u542b28K\u91cd\u7ec4\u793a\u4f8b\u7684\u77e5\u8bc6\u5e93\u3002", "result": "\u7ed3\u679c\u662f\u4e00\u4e2a\u53ef\u7528\u4e8e\u5206\u6790AI\u5b50\u9886\u57df\u91cd\u7ec4\u7279\u6027\u7684\u77e5\u8bc6\u5e93\uff0c\u4ee5\u53ca\u4e00\u4e2a\u80fd\u9884\u6d4b\u65b0\u91cd\u7ec4\u65b9\u5411\u7684\u6a21\u578b\u3002", "conclusion": "\u7ed3\u8bba\u662fCHIMERA\u4e3a\u79d1\u5b66\u521b\u65b0\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u9884\u6d4b\u65b0\u65b9\u5411\u7684\u6f5c\u529b\u3002", "relevance": 70.0}}
{"id": "2505.20858", "pdf": "https://arxiv.org/pdf/2505.20858", "abs": "https://arxiv.org/abs/2505.20858", "authors": ["Jason Chui", "Daniel Cremers"], "title": "ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient", "categories": ["cs.CV"], "comment": "15 pages, 14 figures, 5 tables", "summary": "Classical Bundle Adjustment (BA) methods require accurate initial estimates\nfor convergence and typically assume known camera intrinsics, which limits\ntheir applicability when such information is uncertain or unavailable. We\npropose a novel probabilistic formulation of BA (ProBA) that explicitly models\nand propagates uncertainty in both the 2D observations and the 3D scene\nstructure, enabling optimization without any prior knowledge of camera poses or\nfocal length. Our method uses 3D Gaussians instead of point-like landmarks and\nwe introduce uncertainty-aware reprojection losses by projecting the 3D\nGaussians onto the 2D image space, and enforce geometric consistency across\nmultiple 3D Gaussians using the Bhattacharyya coefficient to encourage overlap\nbetween their corresponding Gaussian distributions. This probabilistic\nframework leads to more robust and reliable optimization, even in the presence\nof outliers in the correspondence set, reducing the likelihood of converging to\npoor local minima. Experimental results show that \\textit{ProBA} outperforms\ntraditional methods in challenging real-world conditions. By removing the need\nfor strong initialization and known intrinsics, ProBA enhances the practicality\nof SLAM systems deployed in unstructured environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u5316\u7684Bundle Adjustment\u65b9\u6cd5\uff08ProBA\uff09\uff0c\u901a\u8fc7\u5efa\u6a21\u548c\u4f20\u64ad2D\u89c2\u6d4b\u4e0e3D\u573a\u666f\u7ed3\u6784\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u76f8\u673a\u4f4d\u59ff\u6216\u7126\u8ddd\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u9ad8\u4e86SLAM\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edfBundle Adjustment\u65b9\u6cd5\u4f9d\u8d56\u51c6\u786e\u7684\u521d\u59cb\u4f30\u8ba1\u548c\u5df2\u77e5\u76f8\u673a\u5185\u53c2\uff0c\u9650\u5236\u4e86\u5176\u5728\u4fe1\u606f\u4e0d\u786e\u5b9a\u6216\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6a21\u578b\u66ff\u4ee3\u70b9\u72b6\u5730\u6807\uff0c\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u91cd\u6295\u5f71\u635f\u5931\uff0c\u5e76\u901a\u8fc7Bhattacharyya\u7cfb\u6570\u5f3a\u5236\u591a3D\u9ad8\u65af\u4e4b\u95f4\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProBA\u5728\u6311\u6218\u6027\u73b0\u5b9e\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "ProBA\u901a\u8fc7\u6d88\u9664\u5bf9\u5f3a\u521d\u59cb\u5316\u548c\u5df2\u77e5\u5185\u53c2\u7684\u9700\u6c42\uff0c\u63d0\u5347\u4e86SLAM\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "relevance": 20.0}}
{"id": "2505.20840", "pdf": "https://arxiv.org/pdf/2505.20840", "abs": "https://arxiv.org/abs/2505.20840", "authors": ["Dooho Lee", "Myeong Kong", "Sagad Hamid", "Cheonwoo Lee", "Jaemin Yoo"], "title": "Aggregation Buffer: Revisiting DropEdge with a New Parameter Block", "categories": ["cs.LG"], "comment": null, "summary": "We revisit DropEdge, a data augmentation technique for GNNs which randomly\nremoves edges to expose diverse graph structures during training. While being a\npromising approach to effectively reduce overfitting on specific connections in\nthe graph, we observe that its potential performance gain in supervised\nlearning tasks is significantly limited. To understand why, we provide a\ntheoretical analysis showing that the limited performance of DropEdge comes\nfrom the fundamental limitation that exists in many GNN architectures. Based on\nthis analysis, we propose Aggregation Buffer, a parameter block specifically\ndesigned to improve the robustness of GNNs by addressing the limitation of\nDropEdge. Our method is compatible with any GNN model, and shows consistent\nperformance improvements on multiple datasets. Moreover, our method effectively\naddresses well-known problems such as degree bias or structural disparity as a\nunifying solution. Code and datasets are available at\nhttps://github.com/dooho00/agg-buffer.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86DropEdge\uff08\u4e00\u79cdGNN\u6570\u636e\u589e\u5f3a\u6280\u672f\uff09\uff0c\u53d1\u73b0\u5176\u5728\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAggregation Buffer\u7684\u65b0\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "motivation": "\u7814\u7a76DropEdge\u5728GNN\u4e2d\u7684\u6027\u80fd\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u589e\u5f3aGNN\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790DropEdge\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faAggregation Buffer\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u517c\u5bb9\u4efb\u4f55GNN\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6027\u80fd\u7684\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u89e3\u51b3\u4e86\u5ea6\u504f\u5dee\u548c\u7ed3\u6784\u5dee\u5f02\u7b49\u95ee\u9898\u3002", "conclusion": "Aggregation Buffer\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u5347GNN\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20809", "pdf": "https://arxiv.org/pdf/2505.20809", "abs": "https://arxiv.org/abs/2505.20809", "authors": ["Zhengxuan Wu", "Qinan Yu", "Aryaman Arora", "Christopher D. Manning", "Christopher Potts"], "title": "Improved Representation Steering for Language Models", "categories": ["cs.CL"], "comment": "46 pages, 23 figures, preprint", "summary": "Steering methods for language models (LMs) seek to provide fine-grained and\ninterpretable control over model generations by variously changing model\ninputs, weights, or representations to adjust behavior. Recent work has shown\nthat adjusting weights or representations is often less effective than steering\nby prompting, for instance when wanting to introduce or suppress a particular\nconcept. We demonstrate how to improve representation steering via our new\nReference-free Preference Steering (RePS), a bidirectional\npreference-optimization objective that jointly does concept steering and\nsuppression. We train three parameterizations of RePS and evaluate them on\nAxBench, a large-scale model steering benchmark. On Gemma models with sizes\nranging from 2B to 27B, RePS outperforms all existing steering methods trained\nwith a language modeling objective and substantially narrows the gap with\nprompting -- while promoting interpretability and minimizing parameter count.\nIn suppression, RePS matches the language-modeling objective on Gemma-2 and\noutperforms it on the larger Gemma-3 variants while remaining resilient to\nprompt-based jailbreaking attacks that defeat prompting. Overall, our results\nsuggest that RePS provides an interpretable and robust alternative to prompting\nfor both steering and suppression.", "AI": {"tldr": "RePS\u662f\u4e00\u79cd\u65b0\u7684\u53cc\u5411\u504f\u597d\u4f18\u5316\u76ee\u6807\uff0c\u7528\u4e8e\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u8868\u793a\u5bfc\u5411\uff0c\u540c\u65f6\u5728\u6982\u5ff5\u5bfc\u5411\u548c\u6291\u5236\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u6743\u91cd\u6216\u8868\u793a\u6765\u66f4\u6709\u6548\u5730\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u6982\u5ff5\u5bfc\u5411\u548c\u6291\u5236\u65b9\u9762\u3002", "method": "\u63d0\u51faReference-free Preference Steering (RePS)\uff0c\u901a\u8fc7\u53cc\u5411\u504f\u597d\u4f18\u5316\u76ee\u6807\u8bad\u7ec3\u4e09\u79cd\u53c2\u6570\u5316\u6a21\u578b\uff0c\u5e76\u5728AxBench\u57fa\u51c6\u4e0a\u8bc4\u4f30\u3002", "result": "RePS\u5728Gemma\u6a21\u578b\uff082B-27B\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u5bfc\u5411\u65b9\u6cd5\uff0c\u63a5\u8fd1\u63d0\u793a\u6cd5\u7684\u6548\u679c\uff0c\u4e14\u5728\u6291\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u4e14\u6297\u653b\u51fb\u3002", "conclusion": "RePS\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u5bfc\u5411\u548c\u6291\u5236\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20861", "pdf": "https://arxiv.org/pdf/2505.20861", "abs": "https://arxiv.org/abs/2505.20861", "authors": ["Yifeng Ma", "Jinwei Qi", "Chaonan Ji", "Peng Zhang", "Bang Zhang", "Zhidong Deng", "Liefeng Bo"], "title": "Exploring Timeline Control for Facial Motion Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025, Project Page:\n  https://humanaigc.github.io/facial-motion-timeline-control/", "summary": "This paper introduces a new control signal for facial motion generation:\ntimeline control. Compared to audio and text signals, timelines provide more\nfine-grained control, such as generating specific facial motions with precise\ntiming. Users can specify a multi-track timeline of facial actions arranged in\ntemporal intervals, allowing precise control over the timing of each action. To\nmodel the timeline control capability, We first annotate the time intervals of\nfacial actions in natural facial motion sequences at a frame-level granularity.\nThis process is facilitated by Toeplitz Inverse Covariance-based Clustering to\nminimize human labor. Based on the annotations, we propose a diffusion-based\ngeneration model capable of generating facial motions that are natural and\naccurately aligned with input timelines. Our method supports text-guided motion\ngeneration by using ChatGPT to convert text into timelines. Experimental\nresults show that our method can annotate facial action intervals with\nsatisfactory accuracy, and produces natural facial motions accurately aligned\nwith timelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u63a7\u5236\u4fe1\u53f7\u2014\u2014\u65f6\u95f4\u7ebf\u63a7\u5236\uff0c\u652f\u6301\u591a\u8f68\u9053\u65f6\u95f4\u7ebf\u8f93\u5165\uff0c\u5b9e\u73b0\u7cbe\u786e\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u548c\u6587\u672c\u4fe1\u53f7\u5bf9\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u7684\u63a7\u5236\u7c92\u5ea6\u8f83\u7c97\uff0c\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u52a8\u4f5c\u7684\u65f6\u5e8f\u3002\u65f6\u95f4\u7ebf\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u63a7\u5236\u80fd\u529b\u3002", "method": "1. \u4f7f\u7528Toeplitz\u9006\u534f\u65b9\u5dee\u805a\u7c7b\u6807\u6ce8\u9762\u90e8\u52a8\u4f5c\u65f6\u95f4\u533a\u95f4\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u65f6\u95f4\u7ebf\u8f93\u5165\uff1b3. \u7ed3\u5408ChatGPT\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u65f6\u95f4\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u80fd\u51c6\u786e\u6807\u6ce8\u9762\u90e8\u52a8\u4f5c\u533a\u95f4\uff0c\u5e76\u751f\u6210\u4e0e\u65f6\u95f4\u7ebf\u7cbe\u786e\u5bf9\u9f50\u7684\u81ea\u7136\u9762\u90e8\u52a8\u4f5c\u3002", "conclusion": "\u65f6\u95f4\u7ebf\u63a7\u5236\u662f\u4e00\u79cd\u6709\u6548\u7684\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u6587\u672c\u5f15\u5bfc\u3002", "relevance": 30.0}}
{"id": "2505.20853", "pdf": "https://arxiv.org/pdf/2505.20853", "abs": "https://arxiv.org/abs/2505.20853", "authors": ["Shuo Wang", "Shunyang Huang", "Jinghui Yuan", "Zhixiang Shen", "Zhao Kang"], "title": "Cooperation of Experts: Fusing Heterogeneous Information with Large Margin", "categories": ["cs.LG", "cs.AI"], "comment": "Published in ICML 2025", "summary": "Fusing heterogeneous information remains a persistent challenge in modern\ndata analysis. While significant progress has been made, existing approaches\noften fail to account for the inherent heterogeneity of object patterns across\ndifferent semantic spaces. To address this limitation, we propose the\nCooperation of Experts (CoE) framework, which encodes multi-typed information\ninto unified heterogeneous multiplex networks. By overcoming modality and\nconnection differences, CoE provides a powerful and flexible model for\ncapturing the intricate structures of real-world complex data. In our\nframework, dedicated encoders act as domain-specific experts, each specializing\nin learning distinct relational patterns in specific semantic spaces. To\nenhance robustness and extract complementary knowledge, these experts\ncollaborate through a novel large margin mechanism supported by a tailored\noptimization strategy. Rigorous theoretical analyses guarantee the framework's\nfeasibility and stability, while extensive experiments across diverse\nbenchmarks demonstrate its superior performance and broad applicability. Our\ncode is available at https://github.com/strangeAlan/CoE.", "AI": {"tldr": "\u63d0\u51fa\u4e86CoE\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u5f02\u6784\u591a\u8def\u7f51\u7edc\u7f16\u7801\u591a\u7c7b\u578b\u4fe1\u606f\uff0c\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5904\u7406\u4e0d\u540c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bf9\u8c61\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\u3002", "method": "CoE\u6846\u67b6\u4f7f\u7528\u4e13\u7528\u7f16\u7801\u5668\u4f5c\u4e3a\u9886\u57df\u4e13\u5bb6\uff0c\u901a\u8fc7\u5927\u95f4\u9694\u673a\u5236\u548c\u4f18\u5316\u7b56\u7565\u534f\u4f5c\u3002", "result": "\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u80fd\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "CoE\u4e3a\u590d\u6742\u6570\u636e\u63d0\u4f9b\u5f3a\u5927\u7075\u6d3b\u7684\u5efa\u6a21\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.20813", "pdf": "https://arxiv.org/pdf/2505.20813", "abs": "https://arxiv.org/abs/2505.20813", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "summary": "In knowledge graph embedding, leveraging relation-specific\nentity-transformation has markedly enhanced performance. However, the\nconsistency of embedding differences before and after transformation remains\nunaddressed, risking the loss of valuable inductive bias inherent in the\nembeddings. This inconsistency stems from two problems. First, transformation\nrepresentations are specified for relations in a disconnected manner, allowing\ndissimilar transformations and corresponding entity-embeddings for similar\nrelations. Second, a generalized plug-in approach as a SFBR (Semantic Filter\nBased on Relations) disrupts this consistency through excessive concentration\nof entity embeddings under entity-based regularization, generating\nindistinguishable score distributions among relations. In this paper, we\nintroduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),\ncontaining more consistent entity-transformation characterized by three\nfeatures: 1) shared affine transformation of relation embeddings across all\nrelations, 2) rooted entity-transformation that adds an entity embedding to its\nchange represented by the transformed vector, and 3) normalization of the\nchange to prevent scale reduction. To amplify the advantages of consistency\nthat preserve semantics on embeddings, RSCF adds relation transformation and\nprediction modules for enhancing the semantics. In knowledge graph completion\ntasks with distance-based and tensor decomposition models, RSCF significantly\noutperforms state-of-the-art KGE methods, showing robustness across all\nrelations and their frequencies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRSCF\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u6301\u5b9e\u4f53\u8f6c\u6362\u7684\u4e00\u81f4\u6027\u6765\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u4e2d\uff0c\u5b9e\u4f53\u8f6c\u6362\u7684\u4e0d\u4e00\u81f4\u6027\u53ef\u80fd\u5bfc\u81f4\u6709\u4ef7\u503c\u7684\u5f52\u7eb3\u504f\u7f6e\u4e22\u5931\uff0c\u5f71\u54cd\u5d4c\u5165\u8d28\u91cf\u3002", "method": "RSCF\u901a\u8fc7\u5171\u4eab\u4eff\u5c04\u53d8\u6362\u3001\u6839\u690d\u5b9e\u4f53\u8f6c\u6362\u548c\u5f52\u4e00\u5316\u53d8\u5316\u6765\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u6dfb\u52a0\u5173\u7cfb\u8f6c\u6362\u548c\u9884\u6d4b\u6a21\u5757\u589e\u5f3a\u8bed\u4e49\u3002", "result": "\u5728\u57fa\u4e8e\u8ddd\u79bb\u548c\u5f20\u91cf\u5206\u89e3\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\uff0cRSCF\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5bf9\u6240\u6709\u5173\u7cfb\u53ca\u5176\u9891\u7387\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RSCF\u901a\u8fc7\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.20862", "pdf": "https://arxiv.org/pdf/2505.20862", "abs": "https://arxiv.org/abs/2505.20862", "authors": ["Chaeyoung Jung", "Youngjoon Jang", "Joon Son Chung"], "title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Hallucination remains a major challenge in multimodal large language models\n(MLLMs). To address this, various contrastive decoding (CD) methods have been\nproposed that contrasts original logits with hallucinated logits generated from\nperturbed inputs. While CD has shown promise in vision-language models (VLMs),\nit is not well-suited for AV-LLMs, where hallucinations often emerge from both\nunimodal and cross-modal combinations involving audio, video, and language.\nThese intricate interactions call for a more adaptive and modality-aware\ndecoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding\n(AVCD)-a novel, training-free decoding framework designed to model trimodal\ninteractions and suppress modality-induced hallucinations in AV-LLMs. Unlike\nprevious CD methods in VLMs that corrupt a fixed modality, AVCD leverages\nattention distributions to dynamically identify less dominant modalities and\napplies attentive masking to generate perturbed output logits. To support CD in\na trimodal setting, we also reformulate the original CD framework to jointly\nhandle audio, visual, and textual inputs. Finally, to improve efficiency, we\nintroduce entropy-guided adaptive decoding, which selectively skips unnecessary\ndecoding steps based on the model's confidence in its predictions. Extensive\nexperiments demonstrate that AVCD consistently outperforms existing decoding\nmethods. Especially, on the AVHBench dataset, it improves accuracy by 6% for\nVideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and\ngeneralizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAVCD\u7684\u65b0\u578b\u89e3\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u6291\u5236\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u548c\u63a9\u853d\u4e3b\u5bfc\u6a21\u6001\uff0c\u5e76\u5728\u4e09\u6a21\u6001\u8bbe\u7f6e\u4e0b\u4f18\u5316\u5bf9\u6bd4\u89e3\u7801\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08AV-LLMs\uff09\u4e2d\u56e0\u97f3\u9891\u3001\u89c6\u9891\u548c\u8bed\u8a00\u590d\u6742\u4ea4\u4e92\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faAudio-Visual Contrastive Decoding (AVCD)\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5206\u5e03\u52a8\u6001\u8bc6\u522b\u4e3b\u5bfc\u6a21\u6001\u5e76\u5e94\u7528\u63a9\u853d\u751f\u6210\u6270\u52a8\u8f93\u51fa\u5bf9\u6570\uff0c\u540c\u65f6\u5f15\u5165\u71b5\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u89e3\u7801\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728AVHBench\u6570\u636e\u96c6\u4e0a\uff0cAVCD\u663e\u8457\u63d0\u5347\u4e86VideoLLaMA2\u548cvideo-SALMONN\u7684\u51c6\u786e\u6027\uff08\u5206\u522b\u63d0\u9ad86%\u548c11%\uff09\u3002", "conclusion": "AVCD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u7801\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u4ea4\u4e92\u573a\u666f\u3002", "relevance": 85.0}}
{"id": "2505.20881", "pdf": "https://arxiv.org/pdf/2505.20881", "abs": "https://arxiv.org/abs/2505.20881", "authors": ["Yiding Shi", "Jianan Zhou", "Wen Song", "Jieyi Bi", "Yaoxin Wu", "Jie Zhang"], "title": "Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Heuristic design with large language models (LLMs) has emerged as a promising\napproach for tackling combinatorial optimization problems (COPs). However,\nexisting approaches often rely on manually predefined evolutionary computation\n(EC) optimizers and single-task training schemes, which may constrain the\nexploration of diverse heuristic algorithms and hinder the generalization of\nthe resulting heuristics. To address these issues, we propose Meta-Optimization\nof Heuristics (MoH), a novel framework that operates at the optimizer level,\ndiscovering effective optimizers through the principle of meta-learning.\nSpecifically, MoH leverages LLMs to iteratively refine a meta-optimizer that\nautonomously constructs diverse optimizers through (self-)invocation, thereby\neliminating the reliance on a predefined EC optimizer. These constructed\noptimizers subsequently evolve heuristics for downstream tasks, enabling\nbroader heuristic exploration. Moreover, MoH employs a multi-task training\nscheme to promote its generalization capability. Experiments on classic COPs\ndemonstrate that MoH constructs an effective and interpretable meta-optimizer,\nachieving state-of-the-art performance across various downstream tasks,\nparticularly in cross-size settings.", "AI": {"tldr": "MoH\u6846\u67b6\u5229\u7528LLMs\u901a\u8fc7\u5143\u5b66\u4e60\u81ea\u52a8\u6784\u5efa\u591a\u6837\u5316\u7684\u4f18\u5316\u5668\uff0c\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u8fdb\u5316\u8ba1\u7b97\u4f18\u5316\u5668\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u8fdb\u5316\u8ba1\u7b97\u4f18\u5316\u5668\u548c\u5355\u4efb\u52a1\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "MoH\u5229\u7528LLMs\u8fed\u4ee3\u4f18\u5316\u5143\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u81ea\u8c03\u7528\u6784\u5efa\u591a\u6837\u5316\u4f18\u5316\u5668\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "MoH\u5728\u7ecf\u5178\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8de8\u5c3a\u5bf8\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MoH\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u5143\u4f18\u5316\u5668\u6784\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 70.0}}
{"id": "2505.20324", "pdf": "https://arxiv.org/pdf/2505.20324", "abs": "https://arxiv.org/abs/2505.20324", "authors": ["Md Arman Islam", "Devi Varaprasad Jonnala", "Ritika Rekhi", "Pratik Pokharel", "Siddharth Cilamkoti", "Asif Imran", "Tevfik Kosar", "Bekir Turkkan"], "title": "Evaluating the Energy-Efficiency of the Code Generated by LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As the quality of code generated by Large Language Models (LLMs) improves,\ntheir adoption in the software industry for automated code generation continues\nto grow. Researchers primarily focus on enhancing the functional correctness of\nthe generated code while commonly overlooking its energy efficiency and\nenvironmental impact. This paper investigates the energy efficiency of the code\ngenerated by 20 popular LLMs for 878 programming problems of varying difficulty\nlevels and diverse algorithmic categories selected from the LeetCode platform\nby comparing them against canonical human-written solutions. Although LLMs can\nproduce functionally correct results in most cases, our findings show that the\nperformance and energy efficiency of LLM-produced solutions are often far below\nthose of human-written solutions. Among the studied LLMs, DeepSeek-v3 and\nGPT-4o generate the most energy-efficient code, whereas Grok-2 and\nGemini-1.5-Pro are among the least energy-efficient models. On average,\nhuman-generated canonical solutions are approximately 1.17 times more energy\nefficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and\nover 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific\nalgorithmic groups such as dynamic programming, backtracking, and bit\nmanipulation, LLM-generated code can consume up to 450 times more energy than\nhuman-generated canonical solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e8620\u79cd\u6d41\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u80fd\u6e90\u6548\u7387\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u666e\u904d\u4f4e\u4e8e\u4eba\u7c7b\u7f16\u5199\u7684\u4ee3\u7801\u3002DeepSeek-v3\u548cGPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u4e0d\u5982\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7814\u7a76\u5176\u751f\u6210\u7684\u4ee3\u7801\u7684\u80fd\u6e90\u6548\u7387\u548c\u73af\u5883\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83878\u4e2aLeetCode\u7f16\u7a0b\u95ee\u9898\u7684LLM\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u7c7b\u7f16\u5199\u7684\u89c4\u8303\u89e3\u51b3\u65b9\u6848\uff0c\u8bc4\u4f30\u5176\u80fd\u6e90\u6548\u7387\u3002", "result": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u80fd\u6e90\u6548\u7387\u4e0a\u666e\u904d\u4f4e\u4e8e\u4eba\u7c7b\u4ee3\u7801\uff0c\u67d0\u4e9b\u7b97\u6cd5\u7c7b\u522b\u5dee\u8ddd\u53ef\u8fbe450\u500d\u3002DeepSeek-v3\u548cGPT-4o\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\u3002", "conclusion": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u80fd\u6e90\u6548\u7387\u4e0a\u4ecd\u9700\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u89c4\u5212\u7b49\u7b97\u6cd5\u7c7b\u522b\u4e2d\u3002", "relevance": 65.0}}
{"id": "2505.20816", "pdf": "https://arxiv.org/pdf/2505.20816", "abs": "https://arxiv.org/abs/2505.20816", "authors": ["Krishna Singh Rajput", "Tejas Anvekar", "Chitta Baral", "Vivek Gupta"], "title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in multimodal question answering have primarily focused on\ncombining heterogeneous modalities or fine-tuning multimodal large language\nmodels. While these approaches have shown strong performance, they often rely\non a single, generalized reasoning strategy, overlooking the unique\ncharacteristics of each modality ultimately limiting both accuracy and\ninterpretability. To address these limitations, we propose MAMMQA, a\nmulti-agent QA framework for multimodal inputs spanning text, tables, and\nimages. Our system includes two Visual Language Model (VLM) agents and one\ntext-based Large Language Model (LLM) agent. The first VLM decomposes the user\nquery into sub-questions and sequentially retrieves partial answers from each\nmodality. The second VLM synthesizes and refines these results through\ncross-modal reasoning. Finally, the LLM integrates the insights into a cohesive\nanswer. This modular design enhances interpretability by making the reasoning\nprocess transparent and allows each agent to operate within its domain of\nexpertise. Experiments on diverse multimodal QA benchmarks demonstrate that our\ncooperative, multi-agent framework consistently outperforms existing baselines\nin both accuracy and robustness.", "AI": {"tldr": "MAMMQA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u3001\u8de8\u6a21\u6001\u63a8\u7406\u548c\u6574\u5408\u7b54\u6848\uff0c\u63d0\u5347\u591a\u6a21\u6001\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u63a8\u7406\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u6a21\u6001\u7279\u6027\uff0c\u9650\u5236\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u4e00\u4e2a\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\uff0c\u5206\u89e3\u95ee\u9898\u3001\u8de8\u6a21\u6001\u63a8\u7406\u5e76\u6574\u5408\u7b54\u6848\u3002", "result": "\u5728\u591a\u6a21\u6001\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u900f\u660e\u63a8\u7406\u548c\u5206\u5de5\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u95ee\u7b54\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.20872", "pdf": "https://arxiv.org/pdf/2505.20872", "abs": "https://arxiv.org/abs/2505.20872", "authors": ["Antony Zhao", "Alex Proshkin", "Fergal Hennessy", "Francesco Crivelli"], "title": "In Context Learning with Vision Transformers: Case Study", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2.10; I.4.8"], "comment": "12 pages, 16 figures. UC Berkeley research project", "summary": "Large transformer models have been shown to be capable of performing\nin-context learning. By using examples in a prompt as well as a query, they are\ncapable of performing tasks such as few-shot, one-shot, or zero-shot learning\nto output the corresponding answer to this query. One area of interest to us is\nthat these transformer models have been shown to be capable of learning the\ngeneral class of certain functions, such as linear functions and small 2-layer\nneural networks, on random data (Garg et al, 2023). We aim to extend this to\nthe image space to analyze their capability to in-context learn more complex\nfunctions on the image space, such as convolutional neural networks and other\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578bTransformer\u6a21\u578b\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5b66\u4e60\u590d\u6742\u51fd\u6570\uff08\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u5176\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1Transformer\u6a21\u578b\u662f\u5426\u80fd\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5b66\u4e60\u66f4\u590d\u6742\u7684\u51fd\u6570\uff0c\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4ece\u800c\u6269\u5c55\u5176\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790Transformer\u6a21\u578b\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5b66\u4e60\u590d\u6742\u51fd\u6570\u7684\u80fd\u529b\uff0c\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7b49\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0Transformer\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u590d\u6742\u51fd\u6570\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u8868\u660eTransformer\u6a21\u578b\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "relevance": 85.0}}
{"id": "2505.20882", "pdf": "https://arxiv.org/pdf/2505.20882", "abs": "https://arxiv.org/abs/2505.20882", "authors": ["Marc Damie", "Edwige Cyffers"], "title": "Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Decentralized machine learning - where each client keeps its own data locally\nand uses its own computational resources to collaboratively train a model by\nexchanging peer-to-peer messages - is increasingly popular, as it enables\nbetter scalability and control over the data. A major challenge in this setting\nis that learning dynamics depend on the topology of the communication graph,\nwhich motivates the use of real graph datasets for benchmarking decentralized\nalgorithms. Unfortunately, existing graph datasets are largely limited to\nfor-profit social networks crawled at a fixed point in time and often collected\nat the user scale, where links are heavily influenced by the platform and its\nrecommendation algorithms. The Fediverse, which includes several free and\nopen-source decentralized social media platforms such as Mastodon, Misskey, and\nLemmy, offers an interesting real-world alternative. We introduce Fedivertex, a\nnew dataset of 182 graphs, covering seven social networks from the Fediverse,\ncrawled weekly over 14 weeks. We release the dataset along with a Python\npackage to facilitate its use, and illustrate its utility on several tasks,\nincluding a new defederation task, which captures a process of link deletion\nobserved on these networks.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Fedivertex\u6570\u636e\u96c6\uff0c\u5305\u542b182\u4e2a\u6765\u81eaFediverse\u7684\u793e\u4ea4\u7f51\u7edc\u56fe\uff0c\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u56fe\u6570\u636e\u96c6\u591a\u4e3a\u5546\u4e1a\u793e\u4ea4\u7f51\u7edc\uff0c\u53d7\u5e73\u53f0\u548c\u63a8\u8350\u7b97\u6cd5\u5f71\u54cd\u8f83\u5927\uff0c\u800cFediverse\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u53bb\u4e2d\u5fc3\u5316\u793e\u4ea4\u7f51\u7edc\u6570\u636e\u3002", "method": "\u6536\u96c6\u4e867\u4e2aFediverse\u793e\u4ea4\u7f51\u7edc\u7684182\u4e2a\u56fe\u6570\u636e\uff0c\u6bcf\u5468\u722c\u53d6\u4e00\u6b21\uff0c\u6301\u7eed14\u5468\uff0c\u5e76\u5f00\u53d1\u4e86Python\u5de5\u5177\u5305\u3002", "result": "\u53d1\u5e03\u4e86Fedivertex\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5305\u62ec\u65b0\u7684defederation\u4efb\u52a1\u3002", "conclusion": "Fedivertex\u4e3a\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u3002", "relevance": 40.0}}
{"id": "2505.20819", "pdf": "https://arxiv.org/pdf/2505.20819", "abs": "https://arxiv.org/abs/2505.20819", "authors": ["Paul Youssef", "Zhixue Zhao", "Christin Seifert", "J\u00f6rg Schl\u00f6tterer"], "title": "Tracing and Reversing Rank-One Model Edits", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing methods (KEs) are a cost-effective way to update the\nfactual content of large language models (LLMs), but they pose a dual-use risk.\nWhile KEs are beneficial for updating outdated or incorrect information, they\ncan be exploited maliciously to implant misinformation or bias. In order to\ndefend against these types of malicious manipulation, we need robust techniques\nthat can reliably detect, interpret, and mitigate adversarial edits. This work\ninvestigates the traceability and reversibility of knowledge edits, focusing on\nthe widely used Rank-One Model Editing (ROME) method. We first show that ROME\nintroduces distinctive distributional patterns in the edited weight matrices,\nwhich can serve as effective signals for locating the edited weights. Second,\nwe show that these altered weights can reliably be used to predict the edited\nfactual relation, enabling partial reconstruction of the modified fact.\nBuilding on this, we propose a method to infer the edited object entity\ndirectly from the modified weights, without access to the editing prompt,\nachieving over 95% accuracy. Finally, we demonstrate that ROME edits can be\nreversed, recovering the model's original outputs with $\\geq$ 80% accuracy. Our\nfindings highlight the feasibility of detecting, tracing, and reversing edits\nbased on the edited weights, offering a robust framework for safeguarding LLMs\nagainst adversarial manipulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff08KEs\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u9006\u6027\uff0c\u91cd\u70b9\u5173\u6ce8ROME\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u3001\u8ffd\u8e2a\u548c\u9006\u8f6c\u6076\u610f\u7f16\u8f91\u7684\u6846\u67b6\u3002", "motivation": "\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u867d\u7136\u80fd\u9ad8\u6548\u66f4\u65b0LLMs\u4e2d\u7684\u4e8b\u5b9e\u5185\u5bb9\uff0c\u4f46\u4e5f\u53ef\u80fd\u88ab\u6076\u610f\u5229\u7528\u690d\u5165\u9519\u8bef\u4fe1\u606f\u6216\u504f\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u9632\u5fa1\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5206\u6790ROME\u7f16\u8f91\u540e\u7684\u6743\u91cd\u77e9\u9635\u5206\u5e03\u6a21\u5f0f\uff0c\u5b9a\u4f4d\u7f16\u8f91\u6743\u91cd\u5e76\u9884\u6d4b\u7f16\u8f91\u7684\u4e8b\u5b9e\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u65ad\u7f16\u8f91\u5bf9\u8c61\u5b9e\u4f53\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u7f16\u8f91\u7684\u53ef\u9006\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0ROME\u7f16\u8f91\u5728\u6743\u91cd\u77e9\u9635\u4e2d\u7559\u4e0b\u72ec\u7279\u6a21\u5f0f\uff0c\u53ef\u9ad8\u7cbe\u5ea6\uff08\u226595%\uff09\u63a8\u65ad\u7f16\u8f91\u5bf9\u8c61\uff0c\u5e76\u80fd\u4ee5\u226580%\u7684\u51c6\u786e\u7387\u9006\u8f6c\u7f16\u8f91\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u7f16\u8f91\u6743\u91cd\u68c0\u6d4b\u3001\u8ffd\u8e2a\u548c\u9006\u8f6c\u7f16\u8f91\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4fdd\u62a4LLMs\u514d\u53d7\u6076\u610f\u64cd\u7eb5\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "relevance": 85.0}}
{"id": "2505.20873", "pdf": "https://arxiv.org/pdf/2505.20873", "abs": "https://arxiv.org/abs/2505.20873", "authors": ["Chaeyoung Jung", "Youngjoon Jang", "Jongmin Choi", "Joon Son Chung"], "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.", "AI": {"tldr": "\u63d0\u51faFork-Merge Decoding\uff08FMD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u9636\u6bb5\u7684\u5206\u652f-\u5408\u5e76\u7b56\u7565\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08AV-LLMs\uff09\u4e2d\u7684\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524dAV-LLMs\u4e2d\u97f3\u9891\u548c\u89c6\u9891\u7279\u5f81\u8054\u5408\u5904\u7406\u53ef\u80fd\u5bfc\u81f4\u6a21\u6001\u504f\u5dee\uff0c\u5373\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u67d0\u4e00\u6a21\u6001\u3002", "method": "FMD\u5728\u63a8\u7406\u9636\u6bb5\u5148\u5206\u522b\u5904\u7406\u97f3\u9891\u548c\u89c6\u9891\u8f93\u5165\uff08\u5206\u652f\u9636\u6bb5\uff09\uff0c\u518d\u5408\u5e76\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8054\u5408\u63a8\u7406\uff08\u5408\u5e76\u9636\u6bb5\uff09\u3002", "result": "\u5728VideoLLaMA2\u548cvideo-SALMONN\u4e0a\u5b9e\u9a8c\uff0c\u4efb\u52a1\u6027\u80fd\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "FMD\u901a\u8fc7\u63a8\u7406\u9636\u6bb5\u5e72\u9884\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u7684\u9c81\u68d2\u6027\u3002", "relevance": 70.0}}
{"id": "2505.20885", "pdf": "https://arxiv.org/pdf/2505.20885", "abs": "https://arxiv.org/abs/2505.20885", "authors": ["Haipeng Luo", "Spandan Senapati", "Vatsal Sharan"], "title": "Improved Bounds for Swap Multicalibration and Swap Omniprediction", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this paper, we consider the related problems of multicalibration -- a\nmultigroup fairness notion and omniprediction -- a simultaneous loss\nminimization paradigm, both in the distributional and online settings. The\nrecent work of Garg et al. (2024) raised the open problem of whether it is\npossible to efficiently achieve $O(\\sqrt{T})$ $\\ell_{2}$-multicalibration error\nagainst bounded linear functions. In this paper, we answer this question in a\nstrongly affirmative sense. We propose an efficient algorithm that achieves\n$O(T^{\\frac{1}{3}})$ $\\ell_{2}$-swap multicalibration error (both in high\nprobability and expectation). On propagating this bound onward, we obtain\nsignificantly improved rates for $\\ell_{1}$-swap multicalibration and swap\nomniprediction for a loss class of convex Lipschitz functions. In particular,\nwe show that our algorithm achieves $O(T^{\\frac{2}{3}})$ $\\ell_{1}$-swap\nmulticalibration and swap omniprediction errors, thereby improving upon the\nprevious best-known bound of $O(T^{\\frac{7}{8}})$. As a consequence of our\nimproved online results, we further obtain several improved sample complexity\nrates in the distributional setting. In particular, we establish a\n$O(\\varepsilon ^ {-3})$ sample complexity of efficiently learning an\n$\\varepsilon$-swap omnipredictor for the class of convex and Lipschitz\nfunctions, $O(\\varepsilon ^{-2.5})$ sample complexity of efficiently learning\nan $\\varepsilon$-swap agnostic learner for the squared loss, and $O(\\varepsilon\n^ {-5}), O(\\varepsilon ^ {-2.5})$ sample complexities of learning $\\ell_{1},\n\\ell_{2}$-swap multicalibrated predictors against linear functions, all of\nwhich significantly improve on the previous best-known bounds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u591a\u6821\u51c6\u548c\u5168\u9884\u6d4b\u7684\u8bef\u5dee\u7387\uff0c\u5e76\u5728\u5206\u5e03\u548c\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3Garg\u7b49\u4eba\uff082024\uff09\u63d0\u51fa\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5373\u662f\u5426\u80fd\u591f\u9ad8\u6548\u5b9e\u73b0$O(\\sqrt{T})$\u7684$\\ell_{2}$-\u591a\u6821\u51c6\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86$O(T^{\\frac{1}{3}})$\u7684$\\ell_{2}$-\u4ea4\u6362\u591a\u6821\u51c6\u8bef\u5dee\uff0c\u5e76\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e86$\\ell_{1}$-\u4ea4\u6362\u591a\u6821\u51c6\u548c\u4ea4\u6362\u5168\u9884\u6d4b\u7684\u8bef\u5dee\u7387\u3002", "result": "\u7b97\u6cd5\u5728\u5728\u7ebf\u548c\u5206\u5e03\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8bef\u5dee\u7387\u548c\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4f8b\u5982$O(\\varepsilon^{-3})$\u7684\u6837\u672c\u590d\u6742\u5ea6\u5b66\u4e60$\\varepsilon$-\u4ea4\u6362\u5168\u9884\u6d4b\u5668\u3002", "conclusion": "\u672c\u6587\u7684\u7b97\u6cd5\u5728\u591a\u6821\u51c6\u548c\u5168\u9884\u6d4b\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.20326", "pdf": "https://arxiv.org/pdf/2505.20326", "abs": "https://arxiv.org/abs/2505.20326", "authors": ["Avinash Madasu", "Vasudev Lal", "Phillip Howard"], "title": "Cultural Awareness in Vision-Language Models: A Cross-Country Exploration", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in diverse cultural\ncontexts, yet their internal biases remain poorly understood. In this work, we\npropose a novel framework to systematically evaluate how VLMs encode cultural\ndifferences and biases related to race, gender, and physical traits across\ncountries. We introduce three retrieval-based tasks: (1) Race to Country\nretrieval, which examines the association between individuals from specific\nracial groups (East Asian, White, Middle Eastern, Latino, South Asian, and\nBlack) and different countries; (2) Personal Traits to Country retrieval, where\nimages are paired with trait-based prompts (e.g., Smart, Honest, Criminal,\nViolent) to investigate potential stereotypical associations; and (3) Physical\nCharacteristics to Country retrieval, focusing on visual attributes like\nskinny, young, obese, and old to explore how physical appearances are\nculturally linked to nations. Our findings reveal persistent biases in VLMs,\nhighlighting how visual representations may inadvertently reinforce societal\nstereotypes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u79cd\u65cf\u3001\u6027\u522b\u548c\u8eab\u4f53\u7279\u5f81\u65b9\u9762\u6587\u5316\u504f\u89c1\u7684\u65b0\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e2d\u7684\u6301\u7eed\u6027\u504f\u89c1\u3002", "motivation": "\u7814\u7a76VLM\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u5185\u90e8\u504f\u89c1\uff0c\u586b\u8865\u4e86\u5bf9\u5176\u6587\u5316\u5dee\u5f02\u7f16\u7801\u7406\u89e3\u7684\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u68c0\u7d22\u4efb\u52a1\uff08\u79cd\u65cf\u4e0e\u56fd\u5bb6\u3001\u4e2a\u4eba\u7279\u5f81\u4e0e\u56fd\u5bb6\u3001\u8eab\u4f53\u7279\u5f81\u4e0e\u56fd\u5bb6\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30VLM\u7684\u504f\u89c1\u3002", "result": "\u53d1\u73b0VLM\u4e2d\u5b58\u5728\u6301\u7eed\u6027\u504f\u89c1\uff0c\u89c6\u89c9\u8868\u5f81\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u793e\u4f1a\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u7406\u89e3\u5e76\u89e3\u51b3VLM\u4e2d\u6587\u5316\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002", "relevance": 60.0}}
{"id": "2505.20825", "pdf": "https://arxiv.org/pdf/2505.20825", "abs": "https://arxiv.org/abs/2505.20825", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "title": "Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Long-form question answering (LFQA) presents unique challenges for large\nlanguage models, requiring the synthesis of coherent, paragraph-length answers.\nWhile retrieval-augmented generation (RAG) systems have emerged as a promising\nsolution, existing research struggles with key limitations: the scarcity of\nhigh-quality training data for long-form generation, the compounding risk of\nhallucination in extended outputs, and the absence of reliable evaluation\nmetrics for factual completeness. In this paper, we propose RioRAG, a novel\nreinforcement learning (RL) framework that advances long-form RAG through\nreinforced informativeness optimization. Our approach introduces two\nfundamental innovations to address the core challenges. First, we develop an RL\ntraining paradigm of reinforced informativeness optimization that directly\noptimizes informativeness and effectively addresses the slow-thinking deficit\nin conventional RAG systems, bypassing the need for expensive supervised data.\nSecond, we propose a nugget-centric hierarchical reward modeling approach that\nenables precise assessment of long-form answers through a three-stage process:\nextracting the nugget from every source webpage, constructing a nugget claim\nchecklist, and computing rewards based on factual alignment. Extensive\nexperiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the\neffectiveness of the proposed method. Our codes are available at\nhttps://github.com/RUCAIBox/RioRAG.", "AI": {"tldr": "RioRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u4e30\u5bcc\u6027\u6539\u8fdb\u957f\u5f62\u5f0f\u95ee\u7b54\uff08LFQA\uff09\u4e2d\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u957f\u5f62\u5f0f\u95ee\u7b54\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u3001\u5e7b\u89c9\u98ce\u9669\u589e\u52a0\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u9760\u8bc4\u4f30\u6307\u6807\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RioRAG\uff0c\u5305\u62ec\u4fe1\u606f\u4e30\u5bcc\u6027\u4f18\u5316\u548c\u57fa\u4e8e\u6838\u5fc3\u4fe1\u606f\u7684\u5c42\u6b21\u5316\u5956\u52b1\u5efa\u6a21\u3002", "result": "\u5728LongFact\u548cRAGChecker\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RioRAG\u6709\u6548\u63d0\u5347\u4e86\u957f\u5f62\u5f0f\u95ee\u7b54\u7684\u751f\u6210\u8d28\u91cf\u548c\u4e8b\u5b9e\u6027\u3002", "relevance": 85.0}}
{"id": "2505.20876", "pdf": "https://arxiv.org/pdf/2505.20876", "abs": "https://arxiv.org/abs/2505.20876", "authors": ["Tatsuya Sasayama", "Shintaro Ito", "Koichi Ito", "Takafumi Aoki"], "title": "Stereo Radargrammetry Using Deep Learning from Airborne SAR Images", "categories": ["cs.CV", "eess.IV"], "comment": "5 pages, 5 figures, conference IGARSS2025", "summary": "In this paper, we propose a stereo radargrammetry method using deep learning\nfrom airborne Synthetic Aperture Radar (SAR) images.Deep learning-based methods\nare considered to suffer less from geometric image modulation, while there is\nno public SAR image dataset used to train such methods.We create a SAR image\ndataset and perform fine-tuning of a deep learning-based image correspondence\nmethod.The proposed method suppresses the degradation of image quality by pixel\ninterpolation without ground projection of the SAR image and divides the SAR\nimage into patches for processing, which makes it possible to apply deep\nlearning.Through a set of experiments, we demonstrate that the proposed method\nexhibits a wider range and more accurate elevation measurements compared to\nconventional methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7acb\u4f53\u96f7\u8fbe\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u673a\u8f7d\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u56fe\u50cf\uff0c\u901a\u8fc7\u5fae\u8c03\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e76\u5206\u5272\u56fe\u50cf\u4e3a\u5c0f\u5757\u5904\u7406\uff0c\u63d0\u9ad8\u4e86\u6d4b\u91cf\u8303\u56f4\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728SAR\u56fe\u50cf\u5904\u7406\u4e2d\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u53d7\u51e0\u4f55\u56fe\u50cf\u8c03\u5236\u5f71\u54cd\u8f83\u5927\uff0c\u56e0\u6b64\u63d0\u51fa\u65b0\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u521b\u5efaSAR\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u50cf\u7d20\u63d2\u503c\u548c\u56fe\u50cf\u5206\u5757\u5904\u7406\u907f\u514d\u8d28\u91cf\u4e0b\u964d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u5e7f\u7684\u6d4b\u91cf\u8303\u56f4\u548c\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86SAR\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u51e0\u4f55\u8c03\u5236\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6d4b\u91cf\u6027\u80fd\u3002", "relevance": 20.0}}
{"id": "2505.20892", "pdf": "https://arxiv.org/pdf/2505.20892", "abs": "https://arxiv.org/abs/2505.20892", "authors": ["Jeonghwan Cheon", "Jaehyuk Bae", "Se-Bum Paik"], "title": "One-Time Soft Alignment Enables Resilient Learning without Weight Transport", "categories": ["cs.LG"], "comment": "28 pages", "summary": "Backpropagation is the cornerstone of deep learning, but its reliance on\nsymmetric weight transport and global synchronization makes it computationally\nexpensive and biologically implausible. Feedback alignment offers a promising\nalternative by approximating error gradients through fixed random feedback,\nthereby avoiding symmetric weight transport. However, this approach often\nstruggles with poor learning performance and instability, especially in deep\nnetworks. Here, we show that a one-time soft alignment between forward and\nfeedback weights at initialization enables deep networks to achieve performance\ncomparable to backpropagation, without requiring weight transport during\nlearning. This simple initialization condition guides stable error minimization\nin the loss landscape, improving network trainability. Spectral analyses\nfurther reveal that initial alignment promotes smoother gradient flow and\nconvergence to flatter minima, resulting in better generalization and\nrobustness. Notably, we also find that allowing moderate deviations from exact\nweight symmetry can improve adversarial robustness compared to standard\nbackpropagation. These findings demonstrate that a simple initialization\nstrategy can enable effective learning in deep networks in a biologically\nplausible and resource-efficient manner.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u521d\u59cb\u5316\u65f6\u8f6f\u5bf9\u9f50\u524d\u5411\u548c\u53cd\u9988\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u4f7f\u6df1\u5ea6\u7f51\u7edc\u5728\u4e0d\u4f9d\u8d56\u5bf9\u79f0\u6743\u91cd\u4f20\u8f93\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u53cd\u5411\u4f20\u64ad\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\u4f9d\u8d56\u5bf9\u79f0\u6743\u91cd\u4f20\u8f93\u548c\u5168\u5c40\u540c\u6b65\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u751f\u7269\u5b66\u4e0a\u4e0d\u73b0\u5b9e\uff1b\u53cd\u9988\u5bf9\u9f50\u867d\u907f\u514d\u5bf9\u79f0\u6743\u91cd\u4f20\u8f93\uff0c\u4f46\u5b66\u4e60\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u8f83\u5dee\u3002", "method": "\u5728\u521d\u59cb\u5316\u65f6\u8fdb\u884c\u4e00\u6b21\u8f6f\u5bf9\u9f50\u524d\u5411\u548c\u53cd\u9988\u6743\u91cd\uff0c\u907f\u514d\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6743\u91cd\u4f20\u8f93\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6027\uff0c\u4fc3\u8fdb\u68af\u5ea6\u5e73\u6ed1\u6d41\u52a8\u548c\u6536\u655b\u5230\u66f4\u5e73\u5766\u7684\u6781\u5c0f\u503c\uff0c\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4f18\u4e8e\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u3002", "conclusion": "\u7b80\u5355\u7684\u521d\u59cb\u5316\u7b56\u7565\u53ef\u5b9e\u73b0\u751f\u7269\u5b66\u5408\u7406\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6df1\u5ea6\u7f51\u7edc\u5b66\u4e60\u3002", "relevance": 60.0}}
{"id": "2505.20327", "pdf": "https://arxiv.org/pdf/2505.20327", "abs": "https://arxiv.org/abs/2505.20327", "authors": ["Aurora Poggi", "Giuseppe Alessio D'Inverno", "Hjalmar Brismar", "Ozan \u00d6ktem", "Matthieu Barreau", "Kateryna Morozovska"], "title": "Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Data-driven discovery of dynamics in biological systems allows for better\nobservation and characterization of processes, such as calcium signaling in\ncell culture. Recent advancements in techniques allow the exploration of\npreviously unattainable insights of dynamical systems, such as the Sparse\nIdentification of Non-Linear Dynamics (SINDy), overcoming the limitations of\nmore classic methodologies. The latter requires some prior knowledge of an\neffective library of candidate terms, which is not realistic for a real case\nstudy. Using inspiration from fields like traffic density estimation and\ncontrol theory, we propose a methodology for characterization and performance\nanalysis of calcium delivery in a family of cells. In this work, we compare the\nperformance of the Constrained Regularized Least-Squares Method (CRLSM) and\nPhysics-Informed Neural Networks (PINN) for system identification and parameter\ndiscovery for governing ordinary differential equations (ODEs). The CRLSM\nachieves a fairly good parameter estimate and a good data fit when using the\nlearned parameters in the Consensus problem. On the other hand, despite the\ninitial hypothesis, PINNs fail to match the CRLSM performance and, under the\ncurrent configuration, do not provide fair parameter estimation. However, we\nhave only studied a limited number of PINN architectures, and it is expected\nthat additional hyperparameter tuning, as well as uncertainty quantification,\ncould significantly improve the performance in future works.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86CRLSM\u548cPINN\u5728\u751f\u7269\u7cfb\u7edf\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0CRLSM\u8868\u73b0\u66f4\u4f18\uff0c\u800cPINN\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u7cfb\u7edf\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u6bd4\u8f83\u4e86CRLSM\u548cPINN\u5728ODE\u53c2\u6570\u4f30\u8ba1\u548c\u6570\u636e\u62df\u5408\u4e2d\u7684\u8868\u73b0\u3002", "result": "CRLSM\u5728\u53c2\u6570\u4f30\u8ba1\u548c\u6570\u636e\u62df\u5408\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u800cPINN\u5f53\u524d\u914d\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "PINN\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u8d85\u53c2\u6570\u8c03\u6574\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.20826", "pdf": "https://arxiv.org/pdf/2505.20826", "abs": "https://arxiv.org/abs/2505.20826", "authors": ["Soichiro Murakami", "Peinan Zhang", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "title": "AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset", "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "Identifying factors that make ad text attractive is essential for advertising\nsuccess. This study proposes AdParaphrase v2.0, a dataset for ad text\nparaphrasing, containing human preference data, to enable the analysis of the\nlinguistic factors and to support the development of methods for generating\nattractive ad texts. Compared with v1.0, this dataset is 20 times larger,\ncomprising 16,460 ad text paraphrase pairs, each annotated with preference data\nfrom ten evaluators, thereby enabling a more comprehensive and reliable\nanalysis. Through the experiments, we identified multiple linguistic features\nof engaging ad texts that were not observed in v1.0 and explored various\nmethods for generating attractive ad texts. Furthermore, our analysis\ndemonstrated the relationships between human preference and ad performance, and\nhighlighted the potential of reference-free metrics based on large language\nmodels for evaluating ad text attractiveness. The dataset is publicly available\nat: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AdParaphrase v2.0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5e7f\u544a\u6587\u672c\u6539\u5199\u5206\u6790\uff0c\u5305\u542b16,460\u5bf9\u6539\u5199\u6587\u672c\u53ca\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u652f\u6301\u751f\u6210\u5438\u5f15\u4eba\u7684\u5e7f\u544a\u6587\u672c\u65b9\u6cd5\u5f00\u53d1\u3002", "motivation": "\u7814\u7a76\u5e7f\u544a\u6587\u672c\u5438\u5f15\u529b\u7684\u8bed\u8a00\u56e0\u7d20\uff0c\u652f\u6301\u751f\u6210\u5438\u5f15\u4eba\u7684\u5e7f\u544a\u6587\u672c\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6AdParaphrase v2.0\uff0c\u5206\u6790\u8bed\u8a00\u7279\u5f81\uff0c\u63a2\u7d22\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u65b0\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u9a8c\u8bc1\u4eba\u7c7b\u504f\u597d\u4e0e\u5e7f\u544a\u8868\u73b0\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "AdParaphrase v2.0\u4e3a\u5e7f\u544a\u6587\u672c\u751f\u6210\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u8d44\u6e90\u3002", "relevance": 30.0}}
{"id": "2505.20884", "pdf": "https://arxiv.org/pdf/2505.20884", "abs": "https://arxiv.org/abs/2505.20884", "authors": ["Weichao Pan", "Bohan Xu", "Xu Wang", "Chengze Lv", "Shuoyang Wang", "Zhenke Duan"], "title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation", "categories": ["cs.CV"], "comment": null, "summary": "Fire detection in dynamic environments faces continuous challenges, including\nthe interference of illumination changes, many false detections or missed\ndetections, and it is difficult to achieve both efficiency and accuracy. To\naddress the problem of feature extraction limitation and information loss in\nthe existing YOLO-based models, this study propose You Only Look Once for Fire\nDetection with Attention-guided Inverted Residual and Dual-pooling Downscale\nFusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted\nResidual Block (AIR) integrates hybrid channel-spatial attention with inverted\nresiduals to adaptively enhance fire features and suppress environmental noise;\n(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns\nthrough learnable fusion of max-average pooling outputs, mitigating small-fire\ndetection failures. Extensive evaluation on two public datasets shows the\nefficient performance of our model. Our proposed model keeps the sum amount of\nparameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),\nand mAP75 is higher than the mainstream real-time object detection models\nYOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants\n1.3-5.5%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aYOLO-FireAD\u7684\u706b\u707e\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5012\u7f6e\u6b8b\u5dee\u5757\u548c\u53cc\u6c60\u5316\u4e0b\u91c7\u6837\u878d\u5408\u5757\uff0c\u63d0\u5347\u4e86\u706b\u707e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709YOLO\u6a21\u578b\u5728\u706b\u707e\u68c0\u6d4b\u4e2d\u5b58\u5728\u7279\u5f81\u63d0\u53d6\u53d7\u9650\u548c\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u53c8\u80fd\u51c6\u786e\u68c0\u6d4b\u706b\u707e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faYOLO-FireAD\u6a21\u578b\uff0c\u5305\u542b\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5012\u7f6e\u6b8b\u5dee\u5757\uff08AIR\uff09\u548c\u53cc\u6c60\u5316\u4e0b\u91c7\u6837\u878d\u5408\u5757\uff08DPDF\uff09\uff0c\u4ee5\u589e\u5f3a\u706b\u707e\u7279\u5f81\u5e76\u6291\u5236\u73af\u5883\u566a\u58f0\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c1151.8%\uff0c\u8ba1\u7b97\u91cf\u964d\u4f4e43.2%\uff0cmAP75\u6bd4\u4e3b\u6d41\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u9ad81.3-5.5%\u3002", "conclusion": "YOLO-FireAD\u5728\u706b\u707e\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709YOLO\u53d8\u4f53\u3002", "relevance": 20.0}}
{"id": "2505.20894", "pdf": "https://arxiv.org/pdf/2505.20894", "abs": "https://arxiv.org/abs/2505.20894", "authors": ["Marius Bock", "Michael Moeller", "Kristof Van Laerhoven"], "title": "DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition", "categories": ["cs.LG", "cs.HC", "eess.IV"], "comment": "7 pages, 3 figures", "summary": "Despite recognized limitations in modeling long-range temporal dependencies,\nHuman Activity Recognition (HAR) has traditionally relied on a sliding window\napproach to segment labeled datasets. Deep learning models like the\nDeepConvLSTM typically classify each window independently, thereby restricting\nlearnable temporal context to within-window information. To address this\nconstraint, we propose DeepConvContext, a multi-scale time series\nclassification framework for HAR. Drawing inspiration from the vision-based\nTemporal Action Localization community, DeepConvContext models both intra- and\ninter-window temporal patterns by processing sequences of time-ordered windows.\nUnlike recent HAR models that incorporate attention mechanisms, DeepConvContext\nrelies solely on LSTMs -- with ablation studies demonstrating the superior\nperformance of LSTMs over attention-based variants for modeling inertial sensor\ndata. Across six widely-used HAR benchmarks, DeepConvContext achieves an\naverage 10% improvement in F1-score over the classic DeepConvLSTM, with gains\nof up to 21%. Code to reproduce our experiments is publicly available via\ngithub.com/mariusbock/context_har.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepConvContext\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3HAR\u4e2d\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5904\u7406\u65f6\u95f4\u6709\u5e8f\u7a97\u53e3\u5e8f\u5217\u6765\u5efa\u6a21\u7a97\u53e3\u5185\u548c\u7a97\u53e3\u95f4\u7684\u65f6\u95f4\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edfHAR\u65b9\u6cd5\u4f9d\u8d56\u6ed1\u52a8\u7a97\u53e3\u5206\u5272\u6570\u636e\uff0c\u9650\u5236\u4e86\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u7a97\u53e3\u5185\u548c\u7a97\u53e3\u95f4\u7684\u65f6\u95f4\u6a21\u5f0f\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faDeepConvContext\u6846\u67b6\uff0c\u57fa\u4e8eLSTM\u5efa\u6a21\u591a\u5c3a\u5ea6\u65f6\u95f4\u6a21\u5f0f\uff0c\u907f\u514d\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684HAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747F1\u5206\u6570\u6bd4\u7ecf\u5178DeepConvLSTM\u63d0\u9ad810%\uff0c\u6700\u9ad8\u63d0\u534721%\u3002", "conclusion": "LSTM\u5728\u5efa\u6a21\u60ef\u6027\u4f20\u611f\u5668\u6570\u636e\u65b9\u9762\u4f18\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff0cDeepConvContext\u663e\u8457\u63d0\u5347\u4e86HAR\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20841", "pdf": "https://arxiv.org/pdf/2505.20841", "abs": "https://arxiv.org/abs/2505.20841", "authors": ["Xinbo Wu", "Abhishek Umrawal", "Lav R. Varshney"], "title": "Concealment of Intent: A Game-Theoretic Analysis", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) grow more capable, concerns about their safe\ndeployment have also grown. Although alignment mechanisms have been introduced\nto deter misuse, they remain vulnerable to carefully designed adversarial\nprompts. In this work, we present a scalable attack strategy: intent-hiding\nadversarial prompting, which conceals malicious intent through the composition\nof skills. We develop a game-theoretic framework to model the interaction\nbetween such attacks and defense systems that apply both prompt and response\nfiltering. Our analysis identifies equilibrium points and reveals structural\nadvantages for the attacker. To counter these threats, we propose and analyze a\ndefense mechanism tailored to intent-hiding attacks. Empirically, we validate\nthe attack's effectiveness on multiple real-world LLMs across a range of\nmalicious behaviors, demonstrating clear advantages over existing adversarial\nprompting techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u653b\u51fb\u7b56\u7565\u2014\u2014\u610f\u56fe\u9690\u85cf\u5bf9\u6297\u63d0\u793a\uff0c\u901a\u8fc7\u6280\u80fd\u7ec4\u5408\u9690\u85cf\u6076\u610f\u610f\u56fe\uff0c\u5e76\u5f00\u53d1\u4e86\u535a\u5f08\u8bba\u6846\u67b6\u5206\u6790\u653b\u51fb\u4e0e\u9632\u5fa1\u7cfb\u7edf\u7684\u4ea4\u4e92\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9eLLM\u4e0a\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u5176\u5b89\u5168\u90e8\u7f72\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u5c3d\u7ba1\u5df2\u6709\u5bf9\u9f50\u673a\u5236\u9632\u6b62\u6ee5\u7528\uff0c\u4f46\u4ecd\u6613\u53d7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u63d0\u793a\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u610f\u56fe\u9690\u85cf\u5bf9\u6297\u63d0\u793a\u653b\u51fb\u7b56\u7565\uff0c\u5f00\u53d1\u535a\u5f08\u8bba\u6846\u67b6\u5206\u6790\u653b\u51fb\u4e0e\u9632\u5fa1\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\u3002", "result": "\u653b\u51fb\u5728\u591a\u4e2a\u771f\u5b9eLLM\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4f18\u4e8e\u73b0\u6709\u5bf9\u6297\u63d0\u793a\u6280\u672f\u3002\u535a\u5f08\u8bba\u5206\u6790\u63ed\u793a\u4e86\u653b\u51fb\u8005\u7684\u7ed3\u6784\u6027\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u610f\u56fe\u9690\u85cf\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u673a\u5236\uff0c\u4e3aLLM\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.20890", "pdf": "https://arxiv.org/pdf/2505.20890", "abs": "https://arxiv.org/abs/2505.20890", "authors": ["Yoojin Kwon", "Hongjun Suh", "Wooseok Lee", "Taesik Gong", "Songyi Han", "Hyung-Sin Kim"], "title": "Frequency Composition for Compressed and Domain-Adaptive Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Modern on-device neural network applications must operate under resource\nconstraints while adapting to unpredictable domain shifts. However, this\ncombined challenge-model compression and domain adaptation-remains largely\nunaddressed, as prior work has tackled each issue in isolation: compressed\nnetworks prioritize efficiency within a fixed domain, whereas large, capable\nmodels focus on handling domain shifts. In this work, we propose CoDA, a\nfrequency composition-based framework that unifies compression and domain\nadaptation. During training, CoDA employs quantization-aware training (QAT)\nwith low-frequency components, enabling a compressed model to selectively learn\nrobust, generalizable features. At test time, it refines the compact model in a\nsource-free manner (i.e., test-time adaptation, TTA), leveraging the\nfull-frequency information from incoming data to adapt to target domains while\ntreating high-frequency components as domain-specific cues. LFC are aligned\nwith the trained distribution, while HFC unique to the target distribution are\nsolely utilized for batch normalization. CoDA can be integrated synergistically\ninto existing QAT and TTA methods. CoDA is evaluated on widely used\ndomain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various\nmodel architectures. With significant compression, it achieves accuracy\nimprovements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the\nfull-precision TTA baseline.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a\u57fa\u4e8e\u9891\u7387\u7ec4\u5408\u7684\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u6a21\u578b\u538b\u7f29\u548c\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6a21\u578b\u5728\u57df\u8f6c\u79fb\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u8bbe\u5907\u4e0a\u7684\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u548c\u57df\u8f6c\u79fb\u4e0d\u53ef\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5355\u72ec\u5904\u7406\u538b\u7f29\u548c\u57df\u9002\u5e94\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "CoDA\u7ed3\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u548c\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\uff0c\u5229\u7528\u4f4e\u9891\u5206\u91cf\u5b66\u4e60\u901a\u7528\u7279\u5f81\uff0c\u9ad8\u9891\u5206\u91cf\u4f5c\u4e3a\u57df\u7279\u5b9a\u7ebf\u7d22\u3002", "result": "\u5728CIFAR10-C\u548cImageNet-C\u4e0a\uff0cCoDA\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5206\u522b\u6bd4\u5168\u7cbe\u5ea6TTA\u57fa\u7ebf\u63d0\u9ad8\u4e867.96%\u548c5.37%\u3002", "conclusion": "CoDA\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20896", "pdf": "https://arxiv.org/pdf/2505.20896", "abs": "https://arxiv.org/abs/2505.20896", "authors": ["Yiwei Wu", "Atticus Geiger", "Rapha\u00ebl Milli\u00e8re"], "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 10 figures, 1 table. To appear in the Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Variable binding -- the ability to associate variables with values -- is\nfundamental to symbolic computation and cognition. Although classical\narchitectures typically implement variable binding via addressable memory, it\nis not well understood how modern neural networks lacking built-in binding\noperations may acquire this capacity. We investigate this by training a\nTransformer to dereference queried variables in symbolic programs where\nvariables are assigned either numerical constants or other variables. Each\nprogram requires following chains of variable assignments up to four steps deep\nto find the queried value, and also contains irrelevant chains of assignments\nacting as distractors. Our analysis reveals a developmental trajectory with\nthree distinct phases during training: (1) random prediction of numerical\nconstants, (2) a shallow heuristic prioritizing early variable assignments, and\n(3) the emergence of a systematic mechanism for dereferencing assignment\nchains. Using causal interventions, we find that the model learns to exploit\nthe residual stream as an addressable memory space, with specialized attention\nheads routing information across token positions. This mechanism allows the\nmodel to dynamically track variable bindings across layers, resulting in\naccurate dereferencing. Our results show how Transformer models can learn to\nimplement systematic variable binding without explicit architectural support,\nbridging connectionist and symbolic approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5982\u4f55\u5728\u6ca1\u6709\u5185\u7f6e\u7ed1\u5b9a\u64cd\u4f5c\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u53d8\u91cf\u7ed1\u5b9a\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5229\u7528\u6b8b\u5dee\u6d41\u4f5c\u4e3a\u53ef\u5bfb\u5740\u5185\u5b58\u7a7a\u95f4\u3002", "motivation": "\u63a2\u7d22\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\uff08\u5982Transformer\uff09\u5982\u4f55\u5728\u6ca1\u6709\u663e\u5f0f\u7ed1\u5b9a\u64cd\u4f5c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53d8\u91cf\u7ed1\u5b9a\uff0c\u4ee5\u5f25\u5408\u8fde\u63a5\u4e3b\u4e49\u548c\u7b26\u53f7\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2aTransformer\u6a21\u578b\u5728\u7b26\u53f7\u7a0b\u5e8f\u4e2d\u89e3\u5f15\u7528\u53d8\u91cf\uff0c\u5206\u6790\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u63ed\u793a\u5176\u5de5\u4f5c\u673a\u5236\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ecf\u5386\u4e86\u4e09\u4e2a\u9636\u6bb5\uff0c\u6700\u7ec8\u5b66\u4f1a\u5229\u7528\u6b8b\u5dee\u6d41\u4f5c\u4e3a\u53ef\u5bfb\u5740\u5185\u5b58\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u52a8\u6001\u8ddf\u8e2a\u53d8\u91cf\u7ed1\u5b9a\u3002", "conclusion": "Transformer\u6a21\u578b\u53ef\u4ee5\u5728\u6ca1\u6709\u663e\u5f0f\u67b6\u6784\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u7cfb\u7edf\u5316\u7684\u53d8\u91cf\u7ed1\u5b9a\uff0c\u4e3a\u8fde\u63a5\u4e3b\u4e49\u548c\u7b26\u53f7\u65b9\u6cd5\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.20871", "pdf": "https://arxiv.org/pdf/2505.20871", "abs": "https://arxiv.org/abs/2505.20871", "authors": ["Xin Sun", "Jianan Xie", "Zhongqi Chen", "Qiang Liu", "Shu Wu", "Yuehe Chen", "Bowen Song", "Weiqiang Wang", "Zilei Wang", "Liang Wang"], "title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG", "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Large language models (LLMs) augmented with retrieval systems have\nsignificantly advanced natural language processing tasks by integrating\nexternal knowledge sources, enabling more accurate and contextually rich\nresponses. To improve the robustness of such systems against noisy retrievals,\nRetrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.\nHowever, RAFT conditions models to generate answers even in the absence of\nreliable knowledge. This behavior undermines their reliability in high-stakes\ndomains, where acknowledging uncertainty is critical. To address this issue, we\npropose Divide-Then-Align (DTA), a post-training approach designed to endow RAG\nsystems with the ability to respond with \"I don't know\" when the query is out\nof the knowledge boundary of both the retrieved passages and the model's\ninternal knowledge. DTA divides data samples into four knowledge quadrants and\nconstructs tailored preference data for each quadrant, resulting in a curated\ndataset for Direct Preference Optimization (DPO). Experimental results on three\nbenchmark datasets demonstrate that DTA effectively balances accuracy with\nappropriate abstention, enhancing the reliability and trustworthiness of\nretrieval-augmented systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDivide-Then-Align (DTA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u589e\u5f3aRAG\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u4f7f\u5176\u5728\u77e5\u8bc6\u8fb9\u754c\u5916\u65f6\u80fd\u56de\u7b54\u201c\u6211\u4e0d\u77e5\u9053\u201d\u3002", "motivation": "RAFT\u65b9\u6cd5\u5728\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u7f3a\u4e4f\u53ef\u9760\u77e5\u8bc6\u65f6\u4ecd\u751f\u6210\u7b54\u6848\uff0c\u5f71\u54cd\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u6027\u3002DTA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DTA\u5c06\u6570\u636e\u6837\u672c\u5206\u4e3a\u56db\u4e2a\u77e5\u8bc6\u8c61\u9650\uff0c\u4e3a\u6bcf\u4e2a\u8c61\u9650\u6784\u5efa\u5b9a\u5236\u504f\u597d\u6570\u636e\uff0c\u7528\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDTA\u6709\u6548\u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u9002\u5f53\u5f03\u6743\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "DTA\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u6027\u3002", "relevance": 85.0}}
{"id": "2505.20897", "pdf": "https://arxiv.org/pdf/2505.20897", "abs": "https://arxiv.org/abs/2505.20897", "authors": ["Pingrui Zhang", "Yifei Su", "Pengyuan Wu", "Dong An", "Li Zhang", "Zhigang Wang", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires the agent to navigate by\nfollowing natural instructions under partial observability, making it difficult\nto align perception with language. Recent methods mitigate this by imagining\nfuture scenes, yet they rely on vision-based synthesis, leading to high\ncomputational cost and redundant details. To this end, we propose to adaptively\nimagine key environmental semantics via \\textit{language} form, enabling a more\nreliable and efficient strategy. Specifically, we introduce a novel Adaptive\nText Dreamer (ATD), a dual-branch self-guided imagination policy built upon a\nlarge language model (LLM). ATD is designed with a human-like left-right brain\narchitecture, where the left brain focuses on logical integration, and the\nright brain is responsible for imaginative prediction of future scenes. To\nachieve this, we fine-tune only the Q-former within both brains to efficiently\nactivate domain-specific knowledge in the LLM, enabling dynamic updates of\nlogical reasoning and imagination during navigation. Furthermore, we introduce\na cross-interaction mechanism to regularize the imagined outputs and inject\nthem into a navigation expert module, allowing ATD to jointly exploit both the\nreasoning capacity of the LLM and the expertise of the navigation model. We\nconduct extensive experiments on the R2R benchmark, where ATD achieves\nstate-of-the-art performance with fewer parameters. The code is\n\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u9002\u5e94\u6587\u672c\u60f3\u8c61\u5668\uff08ATD\uff09\uff0c\u901a\u8fc7\u8bed\u8a00\u5f62\u5f0f\u60f3\u8c61\u5173\u952e\u73af\u5883\u8bed\u4e49\uff0c\u63d0\u5347\u5bfc\u822a\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4efb\u52a1\u4e2d\uff0c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u5bfc\u81f4\u611f\u77e5\u4e0e\u8bed\u8a00\u5bf9\u9f50\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u5408\u6210\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5197\u4f59\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u81ea\u5f15\u5bfc\u60f3\u8c61\u7b56\u7565ATD\uff0c\u91c7\u7528\u7c7b\u4f3c\u4eba\u8111\u7684\u5de6\u53f3\u8111\u67b6\u6784\uff0c\u5de6\u8111\u903b\u8f91\u6574\u5408\uff0c\u53f3\u8111\u60f3\u8c61\u9884\u6d4b\u3002\u4ec5\u5fae\u8c03Q-former\u6fc0\u6d3bLLM\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u52a8\u6001\u66f4\u65b0\u63a8\u7406\u4e0e\u60f3\u8c61\u3002", "result": "\u5728R2R\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cATD\u4ee5\u8f83\u5c11\u53c2\u6570\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "ATD\u901a\u8fc7\u8bed\u8a00\u5f62\u5f0f\u9ad8\u6548\u60f3\u8c61\u5173\u952e\u8bed\u4e49\uff0c\u7ed3\u5408LLM\u63a8\u7406\u80fd\u529b\u548c\u5bfc\u822a\u6a21\u578b\u4e13\u957f\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.20918", "pdf": "https://arxiv.org/pdf/2505.20918", "abs": "https://arxiv.org/abs/2505.20918", "authors": ["Rahul Nair", "Inge Vejsbjerg", "Elizabeth Daly", "Christos Varytimidis", "Bran Knowles"], "title": "Humble AI in the real-world: the case of algorithmic hiring", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": "CHIWORK '25, Symposium on Human-Computer Interaction for Work, June\n  23--25, 2025, Amsterdam, Netherlands Late Breaking Work", "summary": "Humble AI (Knowles et al., 2023) argues for cautiousness in AI development\nand deployments through scepticism (accounting for limitations of statistical\nlearning), curiosity (accounting for unexpected outcomes), and commitment\n(accounting for multifaceted values beyond performance). We present a\nreal-world case study for humble AI in the domain of algorithmic hiring.\nSpecifically, we evaluate virtual screening algorithms in a widely used hiring\nplatform that matches candidates to job openings. There are several challenges\nin misrecognition and stereotyping in such contexts that are difficult to\nassess through standard fairness and trust frameworks; e.g., someone with a\nnon-traditional background is less likely to rank highly. We demonstrate\ntechnical feasibility of how humble AI principles can be translated to practice\nthrough uncertainty quantification of ranks, entropy estimates, and a user\nexperience that highlights algorithmic unknowns. We describe preliminary\ndiscussions with focus groups made up of recruiters. Future user studies seek\nto evaluate whether the higher cognitive load of a humble AI system fosters a\nclimate of trust in its outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8c26\u900aAI\u7684\u7406\u5ff5\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u5728\u7b97\u6cd5\u62db\u8058\u4e2d\u5e94\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\u6765\u63d0\u5347\u4fe1\u4efb\u3002", "motivation": "\u63a2\u8ba8AI\u5f00\u53d1\u4e2d\u7684\u8c28\u614e\u6001\u5ea6\uff0c\u7279\u522b\u662f\u5728\u7b97\u6cd5\u62db\u8058\u4e2d\u5b58\u5728\u7684\u8bef\u8bc6\u522b\u548c\u523b\u677f\u5370\u8c61\u95ee\u9898\uff0c\u6807\u51c6\u516c\u5e73\u6027\u6846\u67b6\u96be\u4ee5\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u71b5\u4f30\u8ba1\u548c\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\uff0c\u5c06\u8c26\u900aAI\u539f\u5219\u8f6c\u5316\u4e3a\u5b9e\u8df5\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u8c26\u900aAI\u7cfb\u7edf\u53ef\u80fd\u901a\u8fc7\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\u6765\u63d0\u5347\u7528\u6237\u5bf9\u5176\u7ed3\u679c\u7684\u4fe1\u4efb\u3002", "conclusion": "\u8c26\u900aAI\u5728\u7b97\u6cd5\u62db\u8058\u4e2d\u5177\u6709\u6280\u672f\u53ef\u884c\u6027\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "relevance": 60.0}}
{"id": "2505.20874", "pdf": "https://arxiv.org/pdf/2505.20874", "abs": "https://arxiv.org/abs/2505.20874", "authors": ["Sirui Xia", "Aili Chen", "Xintao Wang", "Tinghui Zhu", "Yikai Zhang", "Jiangjie Chen", "Yanghua Xiao"], "title": "Can LLMs Learn to Map the World from Local Descriptions?", "categories": ["cs.CL"], "comment": "19 pages, 11 figures", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\ncapabilities in tasks such as code and mathematics. However, their potential to\ninternalize structured spatial knowledge remains underexplored. This study\ninvestigates whether LLMs, grounded in locally relative human observations, can\nconstruct coherent global spatial cognition by integrating fragmented\nrelational descriptions. We focus on two core aspects of spatial cognition:\nspatial perception, where models infer consistent global layouts from local\npositional relationships, and spatial navigation, where models learn road\nconnectivity from trajectory data and plan optimal paths between unconnected\nlocations. Experiments conducted in a simulated urban environment demonstrate\nthat LLMs not only generalize to unseen spatial relationships between points of\ninterest (POIs) but also exhibit latent representations aligned with real-world\nspatial distributions. Furthermore, LLMs can learn road connectivity from\ntrajectory descriptions, enabling accurate path planning and dynamic spatial\nawareness during navigation.", "AI": {"tldr": "LLMs can construct global spatial cognition from local observations, showing potential in spatial perception and navigation tasks.", "motivation": "Explore LLMs' ability to internalize structured spatial knowledge, which is underexplored despite their success in other tasks.", "method": "Focus on spatial perception (inferring global layouts from local relationships) and spatial navigation (learning road connectivity from trajectory data). Experiments in a simulated urban environment.", "result": "LLMs generalize to unseen spatial relationships and exhibit latent representations aligned with real-world distributions. They also learn road connectivity for accurate path planning.", "conclusion": "LLMs demonstrate strong potential in spatial cognition tasks, bridging local observations to global understanding.", "relevance": 75.0}}
{"id": "2505.20904", "pdf": "https://arxiv.org/pdf/2505.20904", "abs": "https://arxiv.org/abs/2505.20904", "authors": ["Guanghu Xie", "Yonglong Zhang", "Zhiduo Jiang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion", "categories": ["cs.CV"], "comment": null, "summary": "Transparent and reflective objects pose significant challenges for depth\nsensors, resulting in incomplete depth information that adversely affects\ndownstream robotic perception and manipulation tasks. To address this issue, we\npropose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba\narchitectures. The encoder is constructed based on a dual-branch\nTransformer-CNN framework, while the multi-scale fusion module leverages a\nTransformer-Mamba architecture, which also serves as the foundation for the\ndecoder design. We introduce a novel multimodal fusion module grounded in\nself-attention mechanisms and state space models, marking the first application\nof the Mamba architecture in the field of transparent object depth completion\nand revealing its promising potential. Additionally, we design an innovative\nmulti-scale fusion module for the decoder that combines channel attention,\nspatial attention, and multi-scale feature extraction techniques to effectively\nintegrate multi-scale features through a down-fusion strategy. Extensive\nevaluations on multiple public datasets demonstrate that our model achieves\nstate-of-the-art(SOTA) performance, validating the effectiveness of our\napproach.", "AI": {"tldr": "HTMNet\u662f\u4e00\u4e2a\u7ed3\u5408Transformer\u3001CNN\u548cMamba\u67b6\u6784\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u900f\u660e\u548c\u53cd\u5c04\u7269\u4f53\u6df1\u5ea6\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u900f\u660e\u548c\u53cd\u5c04\u7269\u4f53\u5bf9\u6df1\u5ea6\u4f20\u611f\u5668\u9020\u6210\u6311\u6218\uff0c\u5bfc\u81f4\u6df1\u5ea6\u4fe1\u606f\u4e0d\u5b8c\u6574\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u611f\u77e5\u548c\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u63d0\u51faHTMNet\uff0c\u91c7\u7528\u53cc\u5206\u652fTransformer-CNN\u7f16\u7801\u5668\u548cTransformer-Mamba\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "HTMNet\u5c55\u793a\u4e86Mamba\u67b6\u6784\u5728\u900f\u660e\u7269\u4f53\u6df1\u5ea6\u8865\u5168\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u6709\u6548\u6574\u5408\u7279\u5f81\u3002", "relevance": 40.0}}
{"id": "2505.20924", "pdf": "https://arxiv.org/pdf/2505.20924", "abs": "https://arxiv.org/abs/2505.20924", "authors": ["Marius Bock", "Maximilian Hopp", "Kristof Van Laerhoven", "Michael Moeller"], "title": "Label Leakage in Federated Inertial-based Human Activity Recognition", "categories": ["cs.LG", "cs.HC"], "comment": "7 pages, 4 figures", "summary": "While prior work has shown that Federated Learning updates can leak sensitive\ninformation, label reconstruction attacks, which aim to recover input labels\nfrom shared gradients, have not yet been examined in the context of Human\nActivity Recognition (HAR). Given the sensitive nature of activity labels, this\nstudy evaluates the effectiveness of state-of-the-art gradient-based label\nleakage attacks on HAR benchmark datasets. Our findings show that the number of\nactivity classes, sampling strategy, and class imbalance are critical factors\ninfluencing the extent of label leakage, with reconstruction accuracies\nreaching up to 90% on two benchmark datasets, even for trained models.\nMoreover, we find that Local Differential Privacy techniques such as gradient\nnoise and clipping offer only limited protection, as certain attacks still\nreliably infer both majority and minority class labels. We conclude by offering\npractical recommendations for the privacy-aware deployment of federated HAR\nsystems and identify open challenges for future research. Code to reproduce our\nexperiments is publicly available via github.com/mariusbock/leakage_har.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6807\u7b7e\u91cd\u5efa\u653b\u51fb\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u6d3b\u52a8\u7c7b\u522b\u6570\u91cf\u3001\u91c7\u6837\u7b56\u7565\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u662f\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u91cd\u5efa\u51c6\u786e\u7387\u9ad8\u8fbe90%\u3002\u540c\u65f6\uff0c\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u6280\u672f\u4fdd\u62a4\u6709\u9650\uff0c\u63d0\u51fa\u4e86\u9690\u79c1\u4fdd\u62a4\u90e8\u7f72\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u4e2d\u6807\u7b7e\u91cd\u5efa\u653b\u51fb\u5bf9HAR\u654f\u611f\u6570\u636e\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728HAR\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u6807\u7b7e\u6cc4\u6f0f\u653b\u51fb\uff0c\u5206\u6790\u4e86\u6d3b\u52a8\u7c7b\u522b\u6570\u91cf\u3001\u91c7\u6837\u7b56\u7565\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u5f71\u54cd\u3002", "result": "\u91cd\u5efa\u51c6\u786e\u7387\u9ad8\u8fbe90%\uff0c\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u6280\u672f\u4fdd\u62a4\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9690\u79c1\u4fdd\u62a4\u90e8\u7f72\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.20875", "pdf": "https://arxiv.org/pdf/2505.20875", "abs": "https://arxiv.org/abs/2505.20875", "authors": ["Jiyoung Lee", "Seungho Kim", "Jieun Han", "Jun-Min Lee", "Kitaek Kim", "Alice Oh", "Edward Choi"], "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 6 figures, 16 tables", "summary": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Trans-EnV\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u591a\u79cd\u975e\u6807\u51c6\u82f1\u8bed\u53d8\u4f53\u4e0a\u7684\u8bed\u8a00\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLMs\u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8e\u6807\u51c6\u7f8e\u5f0f\u82f1\u8bed\uff08SAE\uff09\uff0c\u5ffd\u89c6\u4e86\u5168\u7403\u82f1\u8bed\u591a\u6837\u6027\uff0c\u53ef\u80fd\u5f15\u53d1\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u5b66\u4e13\u5bb6\u77e5\u8bc6\u548cLLM\u9a71\u52a8\u7684\u8f6c\u6362\uff0c\u5c06SAE\u6570\u636e\u96c6\u8f6c\u5316\u4e3a38\u79cd\u82f1\u8bed\u53d8\u4f53\uff0c\u8bc4\u4f307\u79cd\u5148\u8fdbLLMs\u3002", "result": "\u975e\u6807\u51c6\u82f1\u8bed\u53d8\u4f53\u4e0aLLMs\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe46.3%\uff0c\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5f3a\u8c03\u8de8\u591a\u6837\u82f1\u8bed\u53d8\u4f53\u7684\u5168\u9762\u8bed\u8a00\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "relevance": 85.0}}
{"id": "2505.20909", "pdf": "https://arxiv.org/pdf/2505.20909", "abs": "https://arxiv.org/abs/2505.20909", "authors": ["Wei Li", "Hebei Li", "Yansong Peng", "Siying Wu", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "Diffusion models have significantly advanced text-to-image generation, laying\nthe foundation for the development of personalized generative frameworks.\nHowever, existing methods lack precise layout controllability and overlook the\npotential of dynamic features of reference subjects in improving fidelity. In\nthis work, we propose Layout-Controllable Personalized Diffusion\n(LCP-Diffusion) model, a novel framework that integrates subject identity\npreservation with flexible layout guidance in a tuning-free approach. Our model\nemploys a Dynamic-Static Complementary Visual Refining module to\ncomprehensively capture the intricate details of reference subjects, and\nintroduces a Dual Layout Control mechanism to enforce robust spatial control\nacross both training and inference stages. Extensive experiments validate that\nLCP-Diffusion excels in both identity preservation and layout controllability.\nTo the best of our knowledge, this is a pioneering work enabling users to\n\"create anything anywhere\".", "AI": {"tldr": "\u63d0\u51fa\u4e86LCP-Diffusion\u6a21\u578b\uff0c\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u4e2a\u6027\u5316\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u8eab\u4efd\u4fdd\u7559\u4e0e\u7075\u6d3b\u5e03\u5c40\u63a7\u5236\uff0c\u901a\u8fc7\u52a8\u6001-\u9759\u6001\u4e92\u8865\u89c6\u89c9\u7ec6\u5316\u6a21\u5757\u548c\u53cc\u91cd\u5e03\u5c40\u63a7\u5236\u673a\u5236\u5b9e\u73b0\u9ad8\u4fdd\u771f\u4e0e\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7f3a\u4e4f\u7cbe\u786e\u7684\u5e03\u5c40\u63a7\u5236\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u53c2\u8003\u4e3b\u9898\u7684\u52a8\u6001\u7279\u5f81\u4ee5\u63d0\u9ad8\u4fdd\u771f\u5ea6\u3002", "method": "\u91c7\u7528\u52a8\u6001-\u9759\u6001\u4e92\u8865\u89c6\u89c9\u7ec6\u5316\u6a21\u5757\u6355\u83b7\u53c2\u8003\u4e3b\u9898\u7ec6\u8282\uff0c\u5f15\u5165\u53cc\u91cd\u5e03\u5c40\u63a7\u5236\u673a\u5236\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5b9e\u73b0\u7a7a\u95f4\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1LCP-Diffusion\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u5e03\u5c40\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LCP-Diffusion\u662f\u9996\u4e2a\u5b9e\u73b0\u201c\u5728\u4efb\u4f55\u5730\u65b9\u521b\u5efa\u4efb\u4f55\u5185\u5bb9\u201d\u7684\u6846\u67b6\u3002", "relevance": 40.0}}
{"id": "2505.20930", "pdf": "https://arxiv.org/pdf/2505.20930", "abs": "https://arxiv.org/abs/2505.20930", "authors": ["Ruiqi Zhang", "Simon H. Tindemans"], "title": "MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models", "categories": ["cs.LG"], "comment": "5 pages, 3 figures, 1 table", "summary": "Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction\ntechnique for accelerating reliability assessments of complex power system.\nRecently, data-driven surrogate models have been proposed as lower-level models\nin the MLMC framework due to their high correlation and negligible execution\ntime once trained. However, in resource adequacy assessments, pre-labeled\ndatasets are typically unavailable. For large-scale systems, the efficiency\ngains from surrogate models are often offset by the substantial time required\nfor labeling training data. Therefore, this paper introduces a speed metric\nthat accounts for training time in evaluating MLMC efficiency. Considering the\ntotal time budget is limited, a vote-by-committee active learning approach is\nproposed to reduce the required labeling calls. A case study demonstrates that,\nwithin practical variance thresholds, active learning enables significantly\nimproved MLMC efficiency with reduced training effort, compared to regular\nsurrogate modelling approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u8bad\u7ec3\u65f6\u95f4\u7684\u901f\u5ea6\u6307\u6807\uff0c\u5e76\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u4ee5\u63d0\u9ad8Multilevel Monte Carlo\uff08MLMC\uff09\u5728\u7535\u529b\u7cfb\u7edf\u53ef\u9760\u6027\u8bc4\u4f30\u4e2d\u7684\u6548\u7387\u3002", "motivation": "\u5728\u8d44\u6e90\u5145\u8db3\u6027\u8bc4\u4f30\u4e2d\uff0c\u9884\u6807\u6ce8\u6570\u636e\u96c6\u901a\u5e38\u4e0d\u53ef\u7528\uff0c\u4e14\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u8bad\u7ec3\u65f6\u95f4\u53ef\u80fd\u62b5\u6d88\u4ee3\u7406\u6a21\u578b\u7684\u6548\u7387\u589e\u76ca\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u6709\u9650\u65f6\u95f4\u9884\u7b97\u5185\u4f18\u5316MLMC\u6548\u7387\u3002", "method": "\u5f15\u5165\u901f\u5ea6\u6307\u6807\u8bc4\u4f30MLMC\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u59d4\u5458\u4f1a\u6295\u7968\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u65b9\u5dee\u9608\u503c\u5185\u663e\u8457\u63d0\u9ad8\u4e86MLMC\u6548\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u4e3b\u52a8\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u4f18\u5316MLMC\u6846\u67b6\u4e2d\u7684\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u5347\u7535\u529b\u7cfb\u7edf\u53ef\u9760\u6027\u8bc4\u4f30\u7684\u6548\u7387\u3002", "relevance": 30.0}}
{"id": "2505.20880", "pdf": "https://arxiv.org/pdf/2505.20880", "abs": "https://arxiv.org/abs/2505.20880", "authors": ["Baraa Hikal", "Ahmed Nasreldin", "Ali Hamdi"], "title": "MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection", "categories": ["cs.CL"], "comment": null, "summary": "This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes. The task involves detecting hallucinated spans in text\ngenerated by instruction-tuned Large Language Models (LLMs) across multiple\nlanguages. Our approach combines task-specific prompt engineering with an LLM\nensemble verification mechanism, where a primary model extracts hallucination\nspans and three independent LLMs adjudicate their validity through\nprobability-based voting. This framework simulates the human annotation\nworkflow used in the shared task validation and test data. Additionally, fuzzy\nmatching refines span alignment. Our system ranked 1st in Arabic and Basque,\n2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SemEval-2025\u4efb\u52a13\u7684\u63d0\u4ea4\u6210\u679cMu-SHROOM\uff0c\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u591a\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18LLM\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u7247\u6bb5\u3002\u65b9\u6cd5\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5de5\u7a0b\u548cLLM\u96c6\u6210\u9a8c\u8bc1\u673a\u5236\uff0c\u901a\u8fc7\u6982\u7387\u6295\u7968\u9a8c\u8bc1\u7247\u6bb5\u6709\u6548\u6027\uff0c\u6a21\u62df\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\u3002\u7cfb\u7edf\u5728\u591a\u4e2a\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLM\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5de5\u7a0b\u548cLLM\u96c6\u6210\u9a8c\u8bc1\u673a\u5236\uff0c\u901a\u8fc7\u6982\u7387\u6295\u7968\u9a8c\u8bc1\u5e7b\u89c9\u7247\u6bb5\u6709\u6548\u6027\uff0c\u5e76\u91c7\u7528\u6a21\u7cca\u5339\u914d\u4f18\u5316\u7247\u6bb5\u5bf9\u9f50\u3002", "result": "\u7cfb\u7edf\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u5df4\u65af\u514b\u8bed\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5728\u5fb7\u8bed\u3001\u745e\u5178\u8bed\u548c\u82ac\u5170\u8bed\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u5728\u6377\u514b\u8bed\u3001\u6ce2\u65af\u8bed\u548c\u6cd5\u8bed\u4e2d\u6392\u540d\u7b2c\u4e09\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u6a21\u62df\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\uff0c\u5728\u591a\u8bed\u8a00\u5e7b\u89c9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aLLM\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 75.0}}
{"id": "2505.20914", "pdf": "https://arxiv.org/pdf/2505.20914", "abs": "https://arxiv.org/abs/2505.20914", "authors": ["Jianman Lin", "Haojie Li", "Chunmei Qing", "Zhijing Yang", "Liang Lin", "Tianshui Chen"], "title": "Geometry-Editable and Appearance-Preserving Object Compositon", "categories": ["cs.CV"], "comment": null, "summary": "General object composition (GOC) aims to seamlessly integrate a target object\ninto a background scene with desired geometric properties, while simultaneously\npreserving its fine-grained appearance details. Recent approaches derive\nsemantic embeddings and integrate them into advanced diffusion models to enable\ngeometry-editable generation. However, these highly compact embeddings encode\nonly high-level semantic cues and inevitably discard fine-grained appearance\ndetails. We introduce a Disentangled Geometry-editable and\nAppearance-preserving Diffusion (DGAD) model that first leverages semantic\nembeddings to implicitly capture the desired geometric transformations and then\nemploys a cross-attention retrieval mechanism to align fine-grained appearance\nfeatures with the geometry-edited representation, facilitating both precise\ngeometry editing and faithful appearance preservation in object composition.\nSpecifically, DGAD builds on CLIP/DINO-derived and reference networks to\nextract semantic embeddings and appearance-preserving representations, which\nare then seamlessly integrated into the encoding and decoding pipelines in a\ndisentangled manner. We first integrate the semantic embeddings into\npre-trained diffusion models that exhibit strong spatial reasoning capabilities\nto implicitly capture object geometry, thereby facilitating flexible object\nmanipulation and ensuring effective editability. Then, we design a dense\ncross-attention mechanism that leverages the implicitly learned object geometry\nto retrieve and spatially align appearance features with their corresponding\nregions, ensuring faithful appearance consistency. Extensive experiments on\npublic benchmarks demonstrate the effectiveness of the proposed DGAD framework.", "AI": {"tldr": "DGAD\u6a21\u578b\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u7f16\u8f91\u548c\u5916\u89c2\u4fdd\u7559\uff0c\u7ed3\u5408CLIP/DINO\u548c\u53c2\u8003\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u51e0\u4f55\u7f16\u8f91\u548c\u5916\u89c2\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u5b9e\u73b0\u51e0\u4f55\u7f16\u8f91\uff0c\u4f46\u4e22\u5931\u4e86\u7ec6\u7c92\u5ea6\u5916\u89c2\u7ec6\u8282\u3002DGAD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528CLIP/DINO\u63d0\u53d6\u8bed\u4e49\u5d4c\u5165\u548c\u5916\u89c2\u7279\u5f81\uff0c\u901a\u8fc7\u89e3\u8026\u65b9\u5f0f\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5e76\u4f7f\u7528\u5bc6\u96c6\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u5916\u89c2\u7279\u5f81\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u51e0\u4f55\u7f16\u8f91\u548c\u5916\u89c2\u4e00\u81f4\u6027\u3002", "conclusion": "DGAD\u6846\u67b6\u5728\u51e0\u4f55\u7f16\u8f91\u548c\u5916\u89c2\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.20934", "pdf": "https://arxiv.org/pdf/2505.20934", "abs": "https://arxiv.org/abs/2505.20934", "authors": ["Max Collins", "Jordan Vice", "Tim French", "Ajmal Mian"], "title": "NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion", "categories": ["cs.LG", "I.2, I.4"], "comment": "10 pages, 3 figures, 2 tables", "summary": "Adversarial samples exploit irregularities in the manifold ``learned'' by\ndeep learning models to cause misclassifications. The study of these\nadversarial samples provides insight into the features a model uses to classify\ninputs, which can be leveraged to improve robustness against future attacks.\nHowever, much of the existing literature focuses on constrained adversarial\nsamples, which do not accurately reflect test-time errors encountered in\nreal-world settings. To address this, we propose `NatADiff', an adversarial\nsampling scheme that leverages denoising diffusion to generate natural\nadversarial samples. Our approach is based on the observation that natural\nadversarial samples frequently contain structural elements from the adversarial\nclass. Deep learning models can exploit these structural elements to shortcut\nthe classification process, rather than learning to genuinely distinguish\nbetween classes. To leverage this behavior, we guide the diffusion trajectory\ntowards the intersection of the true and adversarial classes, combining\ntime-travel sampling with augmented classifier guidance to enhance attack\ntransferability while preserving image fidelity. Our method achieves comparable\nattack success rates to current state-of-the-art techniques, while exhibiting\nsignificantly higher transferability across model architectures and better\nalignment with natural test-time errors as measured by FID. These results\ndemonstrate that NatADiff produces adversarial samples that not only transfer\nmore effectively across models, but more faithfully resemble naturally\noccurring test-time errors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNatADiff\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u53bb\u566a\u6269\u6563\u751f\u6210\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6297\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u6837\u672c\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u53d7\u9650\u6837\u672c\uff0c\u672a\u80fd\u771f\u5b9e\u53cd\u6620\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6d4b\u8bd5\u9519\u8bef\u3002NatADiff\u65e8\u5728\u751f\u6210\u66f4\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u53bb\u566a\u6269\u6563\u548c\u65f6\u95f4\u65c5\u884c\u91c7\u6837\uff0c\u5f15\u5bfc\u6269\u6563\u8f68\u8ff9\u81f3\u771f\u5b9e\u7c7b\u522b\u4e0e\u5bf9\u6297\u7c7b\u522b\u7684\u4ea4\u96c6\uff0c\u540c\u65f6\u4f7f\u7528\u589e\u5f3a\u5206\u7c7b\u5668\u6307\u5bfc\u3002", "result": "NatADiff\u5728\u653b\u51fb\u6210\u529f\u7387\u4e0a\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u4f46\u8fc1\u79fb\u6027\u66f4\u5f3a\uff0c\u4e14\u751f\u6210\u7684\u6837\u672c\u66f4\u63a5\u8fd1\u81ea\u7136\u6d4b\u8bd5\u9519\u8bef\uff08FID\u6307\u6807\u66f4\u9ad8\uff09\u3002", "conclusion": "NatADiff\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u4e0d\u4ec5\u8fc1\u79fb\u6027\u66f4\u597d\uff0c\u8fd8\u80fd\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u81ea\u7136\u6d4b\u8bd5\u9519\u8bef\uff0c\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.20888", "pdf": "https://arxiv.org/pdf/2505.20888", "abs": "https://arxiv.org/abs/2505.20888", "authors": ["Chengyu Wang", "Junbing Yan", "Wenrui Cai", "Yuanhao Yue", "Jun Huang"], "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.", "AI": {"tldr": "EasyDistill\u662f\u4e00\u4e2a\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u7684\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5de5\u5177\u5305\uff0c\u652f\u6301\u9ed1\u76d2\u548c\u767d\u76d2\u84b8\u998f\uff0c\u63d0\u4f9b\u6570\u636e\u5408\u6210\u3001\u76d1\u7763\u5fae\u8c03\u3001\u6392\u5e8f\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u529f\u80fd\u3002", "motivation": "\u7b80\u5316\u5e76\u63d0\u5347LLMs\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u7387\uff0c\u652f\u6301\u591a\u79cd\u84b8\u998f\u573a\u666f\uff0c\u4fc3\u8fdbNLP\u793e\u533a\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301System 1\u548cSystem 2\u6a21\u578b\u7684\u84b8\u998f\uff0c\u63d0\u4f9b\u6570\u636e\u5408\u6210\u3001\u76d1\u7763\u5fae\u8c03\u3001\u6392\u5e8f\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u9c81\u68d2\u7684\u84b8\u998f\u6a21\u578b\u548c\u5de5\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3002", "conclusion": "EasyDistill\u4f7fLLMs\u7684\u9ad8\u7ea7KD\u6280\u672f\u66f4\u6613\u7528\uff0c\u5bf9NLP\u793e\u533a\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002", "relevance": 70.0}}
{"id": "2505.20920", "pdf": "https://arxiv.org/pdf/2505.20920", "abs": "https://arxiv.org/abs/2505.20920", "authors": ["Qihang Fang", "Chengcheng Tang", "Bugra Tekin", "Shugao Ma", "Yanchao Yang"], "title": "HuMoCon: Concept Discovery for Human Motion Understanding", "categories": ["cs.CV", "68T07", "I.2.10; I.2.7"], "comment": "18 pages, 10 figures", "summary": "We present HuMoCon, a novel motion-video understanding framework designed for\nadvanced human behavior analysis. The core of our method is a human motion\nconcept discovery framework that efficiently trains multi-modal encoders to\nextract semantically meaningful and generalizable features. HuMoCon addresses\nkey challenges in motion concept discovery for understanding and reasoning,\nincluding the lack of explicit multi-modality feature alignment and the loss of\nhigh-frequency information in masked autoencoding frameworks. Our approach\nintegrates a feature alignment strategy that leverages video for contextual\nunderstanding and motion for fine-grained interaction modeling, further with a\nvelocity reconstruction mechanism to enhance high-frequency feature expression\nand mitigate temporal over-smoothing. Comprehensive experiments on standard\nbenchmarks demonstrate that HuMoCon enables effective motion concept discovery\nand significantly outperforms state-of-the-art methods in training large models\nfor human motion understanding. We will open-source the associated code with\nour paper.", "AI": {"tldr": "HuMoCon\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u7ea7\u4eba\u7c7b\u884c\u4e3a\u5206\u6790\u7684\u8fd0\u52a8\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7f16\u7801\u5668\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u6982\u5ff5\u53d1\u73b0\u4e2d\u7684\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8fd0\u52a8\u6982\u5ff5\u53d1\u73b0\u4e2d\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u4e0d\u8db3\u548c\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u7684\u6311\u6218\uff0c\u63d0\u5347\u4eba\u7c7b\u884c\u4e3a\u5206\u6790\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u8fd0\u52a8\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u5efa\u6a21\u7684\u7279\u5f81\u5bf9\u9f50\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u901f\u5ea6\u91cd\u5efa\u673a\u5236\u4ee5\u51cf\u5c11\u65f6\u95f4\u5e73\u6ed1\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8fd0\u52a8\u6982\u5ff5\u53d1\u73b0\u3002", "conclusion": "HuMoCon\u4e3a\u4eba\u7c7b\u8fd0\u52a8\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u6846\u67b6\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002", "relevance": 30.0}}
{"id": "2505.20938", "pdf": "https://arxiv.org/pdf/2505.20938", "abs": "https://arxiv.org/abs/2505.20938", "authors": ["Chongjie Si", "Yidan Cui", "Fuchao Yang", "Xiaokang Yang", "Wei Shen"], "title": "Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning", "categories": ["cs.LG"], "comment": null, "summary": "Partial Multi-Label Learning (PML) extends the multi-label learning paradigm\nto scenarios where each sample is associated with a candidate label set\ncontaining both ground-truth labels and noisy labels. Existing PML methods\ncommonly rely on two assumptions: sparsity of the noise label matrix and\nlow-rankness of the ground-truth label matrix. However, these assumptions are\ninherently conflicting and impractical for real-world scenarios, where the true\nlabel matrix is typically full-rank or close to full-rank. To address these\nlimitations, we demonstrate that the sparsity constraint contributes to the\nhigh-rank property of the predicted label matrix. Based on this, we propose a\nnovel method Schirn, which introduces a sparsity constraint on the noise label\nmatrix while enforcing a high-rank property on the predicted label matrix.\nExtensive experiments demonstrate the superior performance of Schirn compared\nto state-of-the-art methods, validating its effectiveness in tackling\nreal-world PML challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5Schirn\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\uff08PML\uff09\u4e2d\u771f\u5b9e\u6807\u7b7e\u77e9\u9635\u9ad8\u79e9\u4e0e\u566a\u58f0\u6807\u7b7e\u77e9\u9635\u7a00\u758f\u6027\u5047\u8bbe\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PML\u65b9\u6cd5\u4f9d\u8d56\u566a\u58f0\u6807\u7b7e\u77e9\u9635\u7a00\u758f\u6027\u548c\u771f\u5b9e\u6807\u7b7e\u77e9\u9635\u4f4e\u79e9\u6027\u5047\u8bbe\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u6210\u7acb\u3002", "method": "\u63d0\u51faSchirn\u65b9\u6cd5\uff0c\u5728\u566a\u58f0\u6807\u7b7e\u77e9\u9635\u4e0a\u65bd\u52a0\u7a00\u758f\u7ea6\u675f\uff0c\u540c\u65f6\u5728\u9884\u6d4b\u6807\u7b7e\u77e9\u9635\u4e0a\u5f3a\u5236\u9ad8\u79e9\u6027\u8d28\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSchirn\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u5b9ePML\u95ee\u9898\u3002", "conclusion": "Schirn\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u6027\u548c\u9ad8\u79e9\u6027\uff0c\u4e3aPML\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20341", "pdf": "https://arxiv.org/pdf/2505.20341", "abs": "https://arxiv.org/abs/2505.20341", "authors": ["Rui Liu", "Pu Gao", "Jiatian Xi", "Berrak Sisman", "Carlos Busso", "Haizhou Li"], "title": "Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "INTERSPEECH2025. Code and audio examples:\n  https://github.com/AI-S2-Lab/EmoCorrector", "summary": "Text-based speech editing (TSE) modifies speech using only text, eliminating\nre-recording. However, existing TSE methods, mainly focus on the content\naccuracy and acoustic consistency of synthetic speech segments, and often\noverlook the emotional shifts or inconsistency issues introduced by text\nchanges. To address this issue, we propose EmoCorrector, a novel\npost-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented\nGeneration (RAG) by extracting the edited text's emotional features, retrieving\nspeech samples with matching emotions, and synthesizing speech that aligns with\nthe desired emotion while preserving the speaker's identity and quality. To\nsupport the training and evaluation of emotional consistency modeling in TSE,\nwe pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The\nprominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data\nfeaturing diverse text variations and a range of emotional expressions.\nSubjective and objective experiments and comprehensive analysis on ECD-TSE\nconfirm that EmoCorrector significantly enhances the expression of intended\nemotion while addressing emotion inconsistency limitations in current TSE\nmethods. Code and audio examples are available at\nhttps://github.com/AI-S2-Lab/EmoCorrector.", "AI": {"tldr": "EmoCorrector\u662f\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6587\u672c\u8bed\u97f3\u7f16\u8f91\uff08TSE\uff09\u540e\u6821\u6b63\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3\u60c5\u611f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u60c5\u611f\u6821\u6b63\u6570\u636e\u96c6ECD-TSE\u3002", "motivation": "\u73b0\u6709TSE\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u51c6\u786e\u6027\u548c\u58f0\u5b66\u4e00\u81f4\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u60c5\u611f\u53d8\u5316\u6216\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "EmoCorrector\u901a\u8fc7\u63d0\u53d6\u7f16\u8f91\u6587\u672c\u7684\u60c5\u611f\u7279\u5f81\uff0c\u68c0\u7d22\u5339\u914d\u60c5\u611f\u7684\u8bed\u97f3\u6837\u672c\uff0c\u5e76\u5408\u6210\u7b26\u5408\u76ee\u6807\u60c5\u611f\u4e14\u4fdd\u7559\u8bf4\u8bdd\u8005\u8eab\u4efd\u548c\u8d28\u91cf\u7684\u8bed\u97f3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eEmoCorrector\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u60c5\u611f\u7684\u8868\u8fbe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709TSE\u65b9\u6cd5\u7684\u60c5\u611f\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "conclusion": "EmoCorrector\u4e3aTSE\u4e2d\u7684\u60c5\u611f\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.20899", "pdf": "https://arxiv.org/pdf/2505.20899", "abs": "https://arxiv.org/abs/2505.20899", "authors": ["Jeongsoo Choi", "Jaehun Kim", "Joon Son Chung"], "title": "Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper introduces a cross-lingual dubbing system that translates speech\nfrom one language to another while preserving key characteristics such as\nduration, speaker identity, and speaking speed. Despite the strong translation\nquality of existing speech translation approaches, they often overlook the\ntransfer of speech patterns, leading to mismatches with source speech and\nlimiting their suitability for dubbing applications. To address this, we\npropose a discrete diffusion-based speech-to-unit translation model with\nexplicit duration control, enabling time-aligned translation. We then\nsynthesize speech based on the predicted units and source identity with a\nconditional flow matching model. Additionally, we introduce a unit-based speed\nadaptation mechanism that guides the translation model to produce speech at a\nrate consistent with the source, without relying on any text. Extensive\nexperiments demonstrate that our framework generates natural and fluent\ntranslations that align with the original speech's duration and speaking pace,\nwhile achieving competitive translation performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u8bed\u8a00\u914d\u97f3\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u6563\u6269\u6563\u6a21\u578b\u548c\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\u5b9e\u73b0\u65f6\u95f4\u5bf9\u9f50\u7684\u8bed\u97f3\u7ffb\u8bd1\uff0c\u4fdd\u7559\u8bf4\u8bdd\u4eba\u7279\u5f81\u548c\u8bed\u901f\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u7ffb\u8bd1\u65b9\u6cd5\u5ffd\u7565\u8bed\u97f3\u6a21\u5f0f\u7684\u4f20\u9012\uff0c\u5bfc\u81f4\u4e0e\u6e90\u8bed\u97f3\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u5176\u5728\u914d\u97f3\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u79bb\u6563\u6269\u6563\u57fa\u7840\u7684\u8bed\u97f3\u5230\u5355\u5143\u7ffb\u8bd1\u6a21\u578b\uff0c\u7ed3\u5408\u663e\u5f0f\u65f6\u957f\u63a7\u5236\u548c\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\u5408\u6210\u8bed\u97f3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u751f\u6210\u7684\u7ffb\u8bd1\u81ea\u7136\u6d41\u7545\uff0c\u4e0e\u6e90\u8bed\u97f3\u7684\u65f6\u957f\u548c\u8bed\u901f\u4e00\u81f4\uff0c\u4e14\u7ffb\u8bd1\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u8bed\u8a00\u914d\u97f3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8bed\u97f3\u7684\u5173\u952e\u7279\u5f81\u3002", "relevance": 40.0}}
{"id": "2505.20928", "pdf": "https://arxiv.org/pdf/2505.20928", "abs": "https://arxiv.org/abs/2505.20928", "authors": ["Alexander Jaus", "Zdravko Marinov", "Constantin Seibold", "Simon Rei\u00df", "Jens Kleesiek", "Rainer Stiefelhagen"], "title": "Good Enough: Is it Worth Improving your Label Quality?", "categories": ["cs.CV"], "comment": null, "summary": "Improving label quality in medical image segmentation is costly, but its\nbenefits remain unclear. We systematically evaluate its impact using multiple\npseudo-labeled versions of CT datasets, generated by models like nnU-Net,\nTotalSegmentator, and MedSAM. Our results show that while higher-quality labels\nimprove in-domain performance, gains remain unclear if below a small threshold.\nFor pre-training, label quality has minimal impact, suggesting that models\nrather transfer general concepts than detailed annotations. These findings\nprovide guidance on when improving label quality is worth the effort.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u8d28\u91cf\u6807\u7b7e\u5bf9\u57df\u5185\u6027\u80fd\u6709\u63d0\u5347\uff0c\u4f46\u5bf9\u9884\u8bad\u7ec3\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u63a2\u8ba8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u8d28\u91cf\u6539\u8fdb\u7684\u4ef7\u503c\uff0c\u4ee5\u6307\u5bfc\u8d44\u6e90\u5206\u914d\u3002", "method": "\u4f7f\u7528nnU-Net\u3001TotalSegmentator\u548cMedSAM\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6807\u7b7e\u8d28\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u8d28\u91cf\u6807\u7b7e\u63d0\u5347\u57df\u5185\u6027\u80fd\uff0c\u4f46\u5bf9\u9884\u8bad\u7ec3\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "\u6539\u8fdb\u6807\u7b7e\u8d28\u91cf\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u503c\u5f97\u6295\u5165\uff0c\u4f46\u9700\u6743\u8861\u6210\u672c\u4e0e\u6536\u76ca\u3002", "relevance": 40.0}}
{"id": "2505.20943", "pdf": "https://arxiv.org/pdf/2505.20943", "abs": "https://arxiv.org/abs/2505.20943", "authors": ["Anand Brahmbhatt", "Gon Buzaglo", "Sofiia Druchyna", "Elad Hazan"], "title": "Efficient Spectral Control of Partially Observed Linear Dynamical Systems", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "comment": null, "summary": "We propose a new method for the problem of controlling linear dynamical\nsystems under partial observation and adversarial disturbances. Our new\nalgorithm, Double Spectral Control (DSC), matches the best known regret\nguarantees while exponentially improving runtime complexity over previous\napproaches in its dependence on the system's stability margin. Our key\ninnovation is a two-level spectral approximation strategy, leveraging double\nconvolution with a universal basis of spectral filters, enabling efficient and\naccurate learning of the best linear dynamical controllers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSC\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u90e8\u5206\u89c2\u6d4b\u548c\u5bf9\u6297\u6027\u5e72\u6270\u4e0b\u7684\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u6700\u4f73\u5df2\u77e5\u9057\u61be\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u90e8\u5206\u89c2\u6d4b\u548c\u5bf9\u6297\u6027\u5e72\u6270\u4e0b\u63a7\u5236\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u7cfb\u7edf\u7a33\u5b9a\u6027\u8fb9\u754c\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u7b97\u6cd5\u7684\u8fd0\u884c\u6548\u7387\u3002", "method": "\u91c7\u7528\u53cc\u7ea7\u8c31\u8fd1\u4f3c\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u5377\u79ef\u4e0e\u901a\u7528\u8c31\u6ee4\u6ce2\u5668\u57fa\u7684\u7ed3\u5408\uff0c\u9ad8\u6548\u51c6\u786e\u5730\u5b66\u4e60\u6700\u4f73\u7ebf\u6027\u52a8\u6001\u63a7\u5236\u5668\u3002", "result": "DSC\u7b97\u6cd5\u5728\u4fdd\u6301\u6700\u4f73\u9057\u61be\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fd0\u884c\u65f6\u95f4\u590d\u6742\u6027\u3002", "conclusion": "DSC\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u63a7\u5236\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u3002", "relevance": 30.0}}
{"id": "2505.20901", "pdf": "https://arxiv.org/pdf/2505.20901", "abs": "https://arxiv.org/abs/2505.20901", "authors": ["Junhyuk Choi", "Minju Kim", "Yeseon Hong", "Bugeun Kim"], "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "As large vision language models(LVLMs) rapidly advance, concerns about their\npotential to learn and generate social biases and stereotypes are increasing.\nPrevious studies on LVLM's stereotypes face two primary limitations: metrics\nthat overlooked the importance of content words, and datasets that overlooked\nthe effect of color. To address these limitations, this study introduces new\nevaluation metrics based on the Stereotype Content Model (SCM). We also propose\nBASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM\nmetrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.\nAs a result, we found three findings. (1) The SCM-based evaluation is effective\nin capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output\nalong with gender and race ones. (3) Interaction between model architecture and\nparameter sizes seems to affect stereotypes. We release BASIC publicly on\n[anonymized for review].", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSCM\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6BASIC\uff0c\u7528\u4e8e\u68c0\u6d4bLVLMs\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\uff0c\u53d1\u73b0\u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\u5927\u5c0f\u4f1a\u5f71\u54cd\u523b\u677f\u5370\u8c61\u3002", "motivation": "\u968f\u7740LVLMs\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u53ef\u80fd\u5b66\u4e60\u5e76\u751f\u6210\u793e\u4f1a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u7684\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u6307\u6807\u548c\u6570\u636e\u96c6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u57fa\u4e8eSCM\u7684\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51faBASIC\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6027\u522b\u3001\u79cd\u65cf\u548c\u989c\u8272\u523b\u677f\u5370\u8c61\u3002\u5bf98\u4e2aLVLMs\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) SCM\u8bc4\u4f30\u80fd\u6709\u6548\u6355\u6349\u523b\u677f\u5370\u8c61\uff1b(2) LVLMs\u8f93\u51fa\u4e2d\u5b58\u5728\u989c\u8272\u3001\u6027\u522b\u548c\u79cd\u65cf\u523b\u677f\u5370\u8c61\uff1b(3) \u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\u5927\u5c0f\u4f1a\u5f71\u54cd\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684SCM\u6307\u6807\u548cBASIC\u57fa\u51c6\u4e3a\u68c0\u6d4b\u548c\u51cf\u5c11LVLMs\u4e2d\u7684\u523b\u677f\u5370\u8c61\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.20932", "pdf": "https://arxiv.org/pdf/2505.20932", "abs": "https://arxiv.org/abs/2505.20932", "authors": ["Ningyuan Tang", "Minghao Fu", "Hao Yu", "Jianxin Wu"], "title": "QwT-v2: Practical, Effective and Efficient Post-Training Quantization", "categories": ["cs.CV"], "comment": null, "summary": "Network quantization is arguably one of the most practical network\ncompression approaches for reducing the enormous resource consumption of modern\ndeep neural networks. They usually require diverse and subtle design choices\nfor specific architecture and tasks. Instead, the QwT method is a simple and\ngeneral approach which introduces lightweight additional structures to improve\nquantization. But QwT incurs extra parameters and latency. More importantly,\nQwT is not compatible with many hardware platforms. In this paper, we propose\nQwT-v2, which not only enjoys all advantages of but also resolves major defects\nof QwT. By adopting a very lightweight channel-wise affine compensation (CWAC)\nmodule, QwT-v2 introduces significantly less extra parameters and computations\ncompared to QwT, and at the same time matches or even outperforms QwT in\naccuracy. The compensation module of QwT-v2 can be integrated into quantization\ninference engines with little effort, which not only effectively removes the\nextra costs but also makes it compatible with most existing hardware platforms.", "AI": {"tldr": "QwT-v2\u6539\u8fdb\u7f51\u7edc\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u9053\u4eff\u5c04\u8865\u507f\u6a21\u5757\u51cf\u5c11\u989d\u5916\u53c2\u6570\u548c\u8ba1\u7b97\uff0c\u63d0\u5347\u517c\u5bb9\u6027\u3002", "motivation": "\u89e3\u51b3QwT\u65b9\u6cd5\u7684\u989d\u5916\u53c2\u6570\u3001\u5ef6\u8fdf\u548c\u786c\u4ef6\u517c\u5bb9\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u901a\u9053\u4eff\u5c04\u8865\u507f\uff08CWAC\uff09\u6a21\u5757\uff0c\u51cf\u5c11\u989d\u5916\u5f00\u9500\u3002", "result": "QwT-v2\u5728\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6216\u8d85\u8d8aQwT\u7684\u7cbe\u5ea6\uff0c\u5e76\u517c\u5bb9\u591a\u6570\u786c\u4ef6\u5e73\u53f0\u3002", "conclusion": "QwT-v2\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u7f51\u7edc\u91cf\u5316\u6539\u8fdb\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.20964", "pdf": "https://arxiv.org/pdf/2505.20964", "abs": "https://arxiv.org/abs/2505.20964", "authors": ["Mehdi Bennis", "Salem Lahlou"], "title": "Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "The trajectories of 6G and AI are set for a creative collision. However,\ncurrent visions for 6G remain largely incremental evolutions of 5G, while\nprogress in AI is hampered by brittle, data-hungry models that lack robust\nreasoning capabilities. This paper argues for a foundational paradigm shift,\nmoving beyond the purely technical level of communication toward systems\ncapable of semantic understanding and effective, goal-oriented interaction. We\npropose a unified research vision rooted in the principles of System-2\ncognition, built upon three pillars: Abstraction, enabling agents to learn\nmeaningful world models from raw sensorimotor data; Compositionality, providing\nthe algebraic tools to combine learned concepts and subsystems; and Emergent\nCommunication, allowing intelligent agents to create their own adaptive and\ngrounded languages. By integrating these principles, we lay the groundwork for\ntruly intelligent systems that can reason, adapt, and collaborate, unifying\nadvances in wireless communications, machine learning, and robotics under a\nsingle coherent framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf2\u8ba4\u77e5\u539f\u5219\u7684\u7814\u7a76\u613f\u666f\uff0c\u65e8\u5728\u901a\u8fc7\u62bd\u8c61\u3001\u7ec4\u5408\u6027\u548c\u6d8c\u73b0\u901a\u4fe1\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u5f53\u524dAI\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d6G\u548cAI\u7684\u53d1\u5c55\u5b58\u5728\u5c40\u9650\u6027\uff0c6G\u4ecd\u662f\u5bf95G\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb\uff0c\u800cAI\u6a21\u578b\u5219\u7f3a\u4e4f\u9c81\u68d2\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a8\u52a8\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff0c\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u548c\u76ee\u6807\u5bfc\u5411\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7cfb\u7edf2\u8ba4\u77e5\u539f\u5219\u7684\u4e09\u652f\u67f1\u6846\u67b6\uff1a\u62bd\u8c61\uff08\u4ece\u539f\u59cb\u6570\u636e\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff09\u3001\u7ec4\u5408\u6027\uff08\u7ec4\u5408\u5b66\u4e60\u7684\u6982\u5ff5\u548c\u5b50\u7cfb\u7edf\uff09\u3001\u6d8c\u73b0\u901a\u4fe1\uff08\u521b\u5efa\u81ea\u9002\u5e94\u8bed\u8a00\uff09\u3002", "result": "\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b\u539f\u5219\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u4f7f\u5176\u80fd\u591f\u63a8\u7406\u3001\u9002\u5e94\u548c\u534f\u4f5c\uff0c\u7edf\u4e00\u65e0\u7ebf\u901a\u4fe1\u3001\u673a\u5668\u5b66\u4e60\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u5c55\u3002", "conclusion": "\u672c\u6587\u4e3a\u771f\u6b63\u667a\u80fd\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a86G\u548cAI\u7684\u6df1\u5ea6\u878d\u5408\u3002", "relevance": 40.0}}
{"id": "2505.20903", "pdf": "https://arxiv.org/pdf/2505.20903", "abs": "https://arxiv.org/abs/2505.20903", "authors": ["Ziming Wang", "Zeyu Shi", "Haoyi Zhou", "Shiqi Gao", "Qingyun Sun", "Jianxin Li"], "title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?", "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main; The code will be released soon", "summary": "Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,\nwith their confidence scores misaligned with actual performance. While\ncalibration has been extensively studied in models trained from scratch, the\nimpact of LLMs' prior knowledge on calibration during fine-tuning remains\nunderstudied. Our research reveals that LLMs' prior knowledge causes potential\npoor calibration due to the ubiquitous presence of known data in real-world\nfine-tuning, which appears harmful for calibration. Specifically, data aligned\nwith LLMs' prior knowledge would induce overconfidence, while new knowledge\nimproves calibration. Our findings expose a tension: LLMs' encyclopedic\nknowledge, while enabling task versatility, undermines calibration through\nunavoidable knowledge overlaps. To address this, we propose CogCalib, a\ncognition-aware framework that applies targeted learning strategies according\nto the model's prior knowledge. Experiments across 7 tasks using 3 LLM families\nprove that CogCalib significantly improves calibration while maintaining\nperformance, achieving an average 57\\% reduction in ECE compared to standard\nfine-tuning in Llama3-8B. These improvements generalize well to out-of-domain\ntasks, enhancing the objectivity and reliability of domain-specific LLMs, and\nmaking them more trustworthy for critical human-AI interaction applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5fae\u8c03\u540e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6821\u51c6\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u5148\u9a8c\u77e5\u8bc6\u4f1a\u5bfc\u81f4\u6821\u51c6\u4e0d\u4f73\uff0c\u5e76\u63d0\u51faCogCalib\u6846\u67b6\u4ee5\u6539\u5584\u6821\u51c6\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5148\u9a8c\u77e5\u8bc6\u5bf9\u6821\u51c6\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faCogCalib\u6846\u67b6\uff0c\u6839\u636e\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u5e94\u7528\u9488\u5bf9\u6027\u5b66\u4e60\u7b56\u7565\u3002", "result": "CogCalib\u663e\u8457\u6539\u5584\u4e86\u6821\u51c6\uff0cECE\u5e73\u5747\u964d\u4f4e57%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "CogCalib\u63d0\u5347\u4e86LLMs\u7684\u53ef\u9760\u6027\u548c\u5ba2\u89c2\u6027\uff0c\u9002\u7528\u4e8e\u5173\u952e\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u3002", "relevance": 90.0}}
{"id": "2505.20935", "pdf": "https://arxiv.org/pdf/2505.20935", "abs": "https://arxiv.org/abs/2505.20935", "authors": ["Sanghyun Jo", "Wooyeol Lee", "Ziseok Lee", "Kyungsu Kim"], "title": "ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation", "categories": ["cs.CV"], "comment": "34 pages", "summary": "Text-to-image diffusion models excel at generating single-instance scenes but\nstruggle with multi-instance scenarios, often merging or omitting objects.\nUnlike previous training-free approaches that rely solely on semantic-level\nguidance without addressing instance individuation, our training-free method,\nInstance-to-Semantic Attention Control (ISAC), explicitly resolves incomplete\ninstance formation and semantic entanglement through an instance-first modeling\napproach. This enables ISAC to effectively leverage a hierarchical,\ntree-structured prompt mechanism, disentangling multiple object instances and\nindividually aligning them with their corresponding semantic labels. Without\nemploying any external models, ISAC achieves up to 52% average multi-class\naccuracy and 83% average multi-instance accuracy by effectively forming\ndisentangled instances. The code will be made available upon publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5ISAC\uff0c\u901a\u8fc7\u5b9e\u4f8b\u4f18\u5148\u5efa\u6a21\u89e3\u51b3\u591a\u5b9e\u4f8b\u573a\u666f\u4e2d\u5bf9\u8c61\u5408\u5e76\u6216\u9057\u6f0f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u7c7b\u548c\u591a\u5b9e\u4f8b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u573a\u666f\u4e2d\u5bf9\u8c61\u5408\u5e76\u6216\u9057\u6f0f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u4f8b\u4e2a\u4f53\u5316\u548c\u8bed\u4e49\u89e3\u8026\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5b9e\u4f8b\u4f18\u5148\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u6811\u72b6\u7ed3\u6784\u63d0\u793a\u673a\u5236\uff0c\u5b9e\u73b0\u5b9e\u4f8b\u4e0e\u8bed\u4e49\u6807\u7b7e\u7684\u4e2a\u4f53\u5bf9\u9f50\u3002", "result": "ISAC\u5728\u591a\u7c7b\u548c\u591a\u5b9e\u4f8b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523052%\u548c83%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "ISAC\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u591a\u5b9e\u4f8b\u573a\u666f\u7684\u751f\u6210\u8d28\u91cf\u3002", "relevance": 40.0}}
{"id": "2505.20970", "pdf": "https://arxiv.org/pdf/2505.20970", "abs": "https://arxiv.org/abs/2505.20970", "authors": ["Joonkyu Kim", "Yejin Kim", "Jy-yong Sohn"], "title": "Understanding the behavior of representation forgetting in continual learning", "categories": ["cs.LG"], "comment": null, "summary": "In continual learning scenarios, catastrophic forgetting of previously\nlearned tasks is a critical issue, making it essential to effectively measure\nsuch forgetting. Recently, there has been growing interest in focusing on\nrepresentation forgetting, the forgetting measured at the hidden layer. In this\npaper, we provide the first theoretical analysis of representation forgetting\nand use this analysis to better understand the behavior of continual learning.\nFirst, we introduce a new metric called representation discrepancy, which\nmeasures the difference between representation spaces constructed by two\nsnapshots of a model trained through continual learning. We demonstrate that\nour proposed metric serves as an effective surrogate for the representation\nforgetting while remaining analytically tractable. Second, through mathematical\nanalysis of our metric, we derive several key findings about the dynamics of\nrepresentation forgetting: the forgetting occurs more rapidly to a higher\ndegree as the layer index increases, while increasing the width of the network\nslows down the forgetting process. Third, we support our theoretical findings\nthrough experiments on real image datasets, including Split-CIFAR100 and\nImageNet1K.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u8868\u793a\u5dee\u5f02\u5ea6\u91cf\u6765\u5206\u6790\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u793a\u9057\u5fd8\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u662f\u5173\u952e\u95ee\u9898\uff0c\u7814\u7a76\u8868\u793a\u9057\u5fd8\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u884c\u4e3a\u3002", "method": "\u5f15\u5165\u8868\u793a\u5dee\u5f02\u5ea6\u91cf\uff0c\u5206\u6790\u5176\u52a8\u6001\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u8868\u793a\u9057\u5fd8\u968f\u5c42\u6570\u589e\u52a0\u800c\u52a0\u5267\uff0c\u7f51\u7edc\u5bbd\u5ea6\u589e\u52a0\u53ef\u51cf\u7f13\u9057\u5fd8\u3002", "conclusion": "\u8868\u793a\u5dee\u5f02\u662f\u7814\u7a76\u6301\u7eed\u5b66\u4e60\u4e2d\u8868\u793a\u9057\u5fd8\u7684\u6709\u6548\u5de5\u5177\u3002", "relevance": 60.0}}
{"id": "2505.20910", "pdf": "https://arxiv.org/pdf/2505.20910", "abs": "https://arxiv.org/abs/2505.20910", "authors": ["Hang Zeng", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Shaojie Tang", "Guihai Chen"], "title": "Automated Privacy Information Annotation in Large Language Model Interactions", "categories": ["cs.CL"], "comment": "9 content pages", "summary": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application scenarios, typically tagging personally identifiable\ninformation (PII) in anonymous content. In this work, to support the\ndevelopment and evaluation of privacy detection models for LLM interactions\nthat are deployable on local user devices, we construct a large-scale\nmultilingual dataset with 249K user queries and 154K annotated privacy phrases.\nIn particular, we build an automated privacy annotation pipeline with\ncloud-based strong LLMs to automatically extract privacy phrases from dialogue\ndatasets and annotate leaked information. We also design evaluation metrics at\nthe levels of privacy leakage, extracted privacy phrase, and privacy\ninformation. We further establish baseline methods using light-weight LLMs with\nboth tuning-free and tuning-based methods, and report a comprehensive\nevaluation of their performance. Evaluation results reveal a gap between\ncurrent performance and the requirements of real-world LLM applications,\nmotivating future research into more effective local privacy detection methods\ngrounded in our dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ea4\u4e92\u4e2d\u9690\u79c1\u6cc4\u9732\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7528\u6237\u5728\u4e0eLLM\u4ea4\u4e92\u65f6\u53ef\u80fd\u65e0\u610f\u6cc4\u9732\u9690\u79c1\u4fe1\u606f\uff0c\u73b0\u6709\u9690\u79c1\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eLLM\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u672c\u5730\u53ef\u90e8\u7f72\u7684\u9690\u79c1\u68c0\u6d4b\u6a21\u578b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b249K\u7528\u6237\u67e5\u8be2\u548c154K\u6807\u6ce8\u9690\u79c1\u77ed\u8bed\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e91\u7aef\u5f3aLLM\u81ea\u52a8\u6807\u6ce8\u9690\u79c1\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7LLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08\u514d\u8c03\u4f18\u548c\u8c03\u4f18\u65b9\u6cd5\uff09\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6027\u80fd\u4e0e\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u6709\u6548\u7684\u672c\u5730\u9690\u79c1\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u4e3aLLM\u4ea4\u4e92\u4e2d\u7684\u9690\u79c1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "relevance": 85.0}}
{"id": "2505.20941", "pdf": "https://arxiv.org/pdf/2505.20941", "abs": "https://arxiv.org/abs/2505.20941", "authors": ["Yaohua Zha", "Yanzi Wang", "Hang Guo", "Jinpeng Wang", "Tao Dai", "Bin Chen", "Zhihao Ouyang", "Xue Yuerong", "Ke Chen", "Shu-Tao Xia"], "title": "PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Applying pre-trained models to assist point cloud understanding has recently\nbecome a mainstream paradigm in 3D perception. However, existing application\nstrategies are straightforward, utilizing only the final output of the\npre-trained model for various task heads. It neglects the rich complementary\ninformation in the intermediate layer, thereby failing to fully unlock the\npotential of pre-trained models. To overcome this limitation, we propose an\northogonal solution: Point Mamba Adapter (PMA), which constructs an ordered\nfeature sequence from all layers of the pre-trained model and leverages Mamba\nto fuse all complementary semantics, thereby promoting comprehensive point\ncloud understanding. Constructing this ordered sequence is non-trivial due to\nthe inherent isotropy of 3D space. Therefore, we further propose a\ngeometry-constrained gate prompt generator (G2PG) shared across different\nlayers, which applies shared geometric constraints to the output gates of the\nMamba and dynamically optimizes the spatial order, thus enabling more effective\nintegration of multi-layer information. Extensive experiments conducted on\nchallenging point cloud datasets across various tasks demonstrate that our PMA\nelevates the capability for point cloud understanding to a new level by fusing\ndiverse complementary intermediate features. Code is available at\nhttps://github.com/zyh16143998882/PMA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoint Mamba Adapter (PMA)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\uff0c\u7ed3\u5408Mamba\u6a21\u578b\u878d\u5408\u591a\u5c42\u6b21\u7684\u4e92\u8865\u8bed\u4e49\uff0c\u63d0\u5347\u4e86\u70b9\u4e91\u7406\u89e3\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u7ec8\u8f93\u51fa\uff0c\u5ffd\u7565\u4e86\u4e2d\u95f4\u5c42\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faPMA\uff0c\u6784\u5efa\u9884\u8bad\u7ec3\u6a21\u578b\u6240\u6709\u5c42\u7684\u7279\u5f81\u5e8f\u5217\uff0c\u5229\u7528Mamba\u878d\u5408\u4e92\u8865\u8bed\u4e49\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faG2PG\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u52a8\u6001\u4f18\u5316\u7a7a\u95f4\u987a\u5e8f\u3002", "result": "\u5728\u591a\u4e2a\u70b9\u4e91\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPMA\u901a\u8fc7\u878d\u5408\u4e2d\u95f4\u5c42\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "PMA\u901a\u8fc7\u591a\u5c42\u7ea7\u4fe1\u606f\u878d\u5408\uff0c\u4e3a\u70b9\u4e91\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20972", "pdf": "https://arxiv.org/pdf/2505.20972", "abs": "https://arxiv.org/abs/2505.20972", "authors": ["Sen Bai", "Chunqi Yang", "Xin Bai", "Xin Zhang", "Zhengang Jiang"], "title": "Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Along with AI computing shining in scientific discovery, its potential in the\ncombinatorial optimization (CO) domain has also emerged in recent years. Yet,\nexisting unsupervised neural network solvers struggle to solve $k$-grouping\nproblems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,\ndue to limited computational frameworks. In this work, we propose Deep\n$k$-grouping, an unsupervised learning-based CO framework. Specifically, we\ncontribute: Novel one-hot encoded polynomial unconstrained binary optimization\n(OH-PUBO), a formulation for modeling k-grouping problems on graphs and\nhypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated\nalgorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs\nthe relaxation of large-scale OH-PUBO objectives as differentiable loss\nfunctions and trains to optimize them in an unsupervised manner. To ensure\nscalability, it leverages GPU-accelerated algorithms to unify the training\npipeline; A Gini coefficient-based continuous relaxation annealing strategy to\nenforce discreteness of solutions while preventing convergence to local optima.\nExperimental results demonstrate that Deep $k$-grouping outperforms existing\nneural network solvers and classical heuristics such as SCIP and Tabu.", "AI": {"tldr": "Deep $k$-grouping \u662f\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u7ec4\u5408\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7 GPU \u52a0\u901f\u7b97\u6cd5\u548c Gini \u7cfb\u6570\u677e\u5f1b\u9000\u706b\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u56fe\u548c\u9ad8\u9636\u56fe\u7684 $k$-\u5206\u7ec4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u5728\u5927\u89c4\u6a21\u56fe\u548c\u9ad8\u9636\u56fe\u7684 $k$-\u5206\u7ec4\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u63d0\u51fa OH-PUBO \u516c\u5f0f\u5efa\u6a21 $k$-\u5206\u7ec4\u95ee\u9898\uff0c\u5229\u7528 GPU \u52a0\u901f\u7b97\u6cd5\u548c Gini \u7cfb\u6570\u677e\u5f1b\u9000\u706b\u7b56\u7565\u8fdb\u884c\u65e0\u76d1\u7763\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeep $k$-grouping \u4f18\u4e8e\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u548c\u7ecf\u5178\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08\u5982 SCIP \u548c Tabu\uff09\u3002", "conclusion": "Deep $k$-grouping \u4e3a\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20921", "pdf": "https://arxiv.org/pdf/2505.20921", "abs": "https://arxiv.org/abs/2505.20921", "authors": ["Injae Na", "Keonwoong Noh", "Woohwan Jung"], "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "LLM providers typically offer multiple LLM tiers, varying in performance and\nprice. As NLP tasks become more complex and modularized, selecting the suitable\nLLM tier for each subtask is a key challenge to balance between cost and\nperformance. To address the problem, we introduce LLM Automatic Transmission\n(LLM-AT) framework that automatically selects LLM tiers without training.\nLLM-AT consists of Starter, Generator, and Judge. The starter selects the\ninitial LLM tier expected to solve the given question, the generator produces a\nresponse using the LLM of the selected tier, and the judge evaluates the\nvalidity of the response. If the response is invalid, LLM-AT iteratively\nupgrades to a higher-tier model, generates a new response, and re-evaluates\nuntil a valid response is obtained. Additionally, we propose accuracy\nestimator, which enables the suitable initial LLM tier selection without\ntraining. Given an input question, accuracy estimator estimates the expected\naccuracy of each LLM tier by computing the valid response rate across top-k\nsimilar queries from past inference records. Experiments demonstrate that\nLLM-AT achieves superior performance while reducing costs, making it a\npractical solution for real-world applications.", "AI": {"tldr": "LLM-AT\u6846\u67b6\u81ea\u52a8\u9009\u62e9LLM\u5c42\u7ea7\u4ee5\u5e73\u8861\u6210\u672c\u4e0e\u6027\u80fd\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u590d\u6742NLP\u4efb\u52a1\u4e2d\u5982\u4f55\u9009\u62e9\u5408\u9002\u7684LLM\u5c42\u7ea7\u4ee5\u4f18\u5316\u6210\u672c\u4e0e\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "LLM-AT\u7531Starter\u3001Generator\u548cJudge\u7ec4\u6210\uff0c\u901a\u8fc7\u8fed\u4ee3\u5347\u7ea7LLM\u5c42\u7ea7\u76f4\u81f3\u83b7\u5f97\u6709\u6548\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM-AT\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "LLM-AT\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u5e73\u8861\u6210\u672c\u4e0e\u6027\u80fd\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20951", "pdf": "https://arxiv.org/pdf/2505.20951", "abs": "https://arxiv.org/abs/2505.20951", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Kang Wang", "Ruibo Li", "Lemiao Qiu", "Shuyou Zhang", "Zhe Wang", "Guosheng Lin"], "title": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Camera-based 3D semantic occupancy prediction offers an efficient and\ncost-effective solution for perceiving surrounding scenes in autonomous\ndriving. However, existing works rely on explicit occupancy state inference,\nleading to numerous incorrect feature assignments, and insufficient samples\nrestrict the learning of occupancy class inference. To address these\nchallenges, we propose leveraging Depth awareness and Semantic aid to boost\ncamera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform\noccupancy state and occupancy class inference, where soft occupancy confidence\nis calculated through non-learning method and multiplied with image features to\nmake the voxel representation aware of depth, enabling adaptive implicit\noccupancy state inference. Rather than focusing on improving feature learning,\nwe directly utilize well-trained image semantic segmentation and fuse multiple\nframes with their occupancy probabilities to aid occupancy class inference,\nthereby enhancing robustness. Experimental results demonstrate that DSOcc\nachieves state-of-the-art performance on the SemanticKITTI dataset among\ncamera-based methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u611f\u77e5\u548c\u8bed\u4e49\u8f85\u52a9\u7684\u76f8\u673a3D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\uff08DSOcc\uff09\uff0c\u901a\u8fc7\u8054\u5408\u63a8\u65ad\u5360\u636e\u72b6\u6001\u548c\u7c7b\u522b\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5360\u636e\u72b6\u6001\u63a8\u65ad\uff0c\u5bfc\u81f4\u7279\u5f81\u5206\u914d\u9519\u8bef\u4e14\u6837\u672c\u4e0d\u8db3\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faDSOcc\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u8f6f\u5360\u636e\u7f6e\u4fe1\u5ea6\uff0c\u7ed3\u5408\u56fe\u50cf\u7279\u5f81\u5b9e\u73b0\u6df1\u5ea6\u611f\u77e5\uff0c\u5e76\u878d\u5408\u591a\u5e27\u8bed\u4e49\u5206\u5272\u7ed3\u679c\u8f85\u52a9\u7c7b\u522b\u63a8\u65ad\u3002", "result": "\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\uff0cDSOcc\u5728\u57fa\u4e8e\u76f8\u673a\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "DSOcc\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u548c\u8bed\u4e49\u8f85\u52a9\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20992", "pdf": "https://arxiv.org/pdf/2505.20992", "abs": "https://arxiv.org/abs/2505.20992", "authors": ["Meng Qin", "Jiahong Liu", "Irwin King"], "title": "Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation", "categories": ["cs.LG", "cs.SI"], "comment": "Accepted by ACM SIGKDD 2025", "summary": "Graph neural networks (GNNs), which capture graph structures via a feature\naggregation mechanism following the graph embedding framework, have\ndemonstrated a powerful ability to support various tasks. According to the\ntopology properties (e.g., structural roles or community memberships of nodes)\nto be preserved, graph embedding can be categorized into identity and position\nembedding. However, it is unclear for most GNN-based methods which property\nthey can capture. Some of them may also suffer from low efficiency and\nscalability caused by several time- and space-consuming procedures (e.g.,\nfeature extraction and training). From a perspective of graph signal\nprocessing, we find that high- and low-frequency information in the graph\nspectral domain may characterize node identities and positions, respectively.\nBased on this investigation, we propose random feature aggregation (RFA) for\nefficient identity and position embedding, serving as an extreme ablation study\nregarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without\nlearnable parameters as its backbone, (ii) only uses random noises as inputs,\nand (iii) derives embeddings via just one feed-forward propagation (FFP).\nInspired by degree-corrected spectral clustering, we further introduce a degree\ncorrection mechanism to the GNN backbone. Surprisingly, our experiments\ndemonstrate that two variants of RFA with high- and low-pass filters can\nrespectively derive informative identity and position embeddings via just one\nFFP (i.e., without any training). As a result, RFA can achieve a better\ntrade-off between quality and efficiency for both identity and position\nembedding over various baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u4fe1\u53f7\u5904\u7406\u7684\u968f\u673a\u7279\u5f81\u805a\u5408\uff08RFA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u8eab\u4efd\u548c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u6709\u610f\u4e49\u7684\u5d4c\u5165\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u6355\u6349\u56fe\u7ed3\u6784\u5c5e\u6027\uff08\u5982\u8282\u70b9\u8eab\u4efd\u6216\u4f4d\u7f6e\uff09\u65f6\u6548\u7387\u4f4e\u4e14\u4e0d\u660e\u786e\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u9ad8\u9891\u548c\u4f4e\u9891\u4fe1\u606f\u5206\u522b\u8868\u5f81\u8282\u70b9\u8eab\u4efd\u548c\u4f4d\u7f6e\uff0c\u91c7\u7528\u65e0\u53c2\u6570\u7684\u8c31\u57faGNN\uff0c\u4ec5\u4f7f\u7528\u968f\u673a\u566a\u58f0\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRFA\u7684\u4e24\u79cd\u53d8\u4f53\uff08\u9ad8/\u4f4e\u901a\u6ee4\u6ce2\u5668\uff09\u80fd\u5206\u522b\u751f\u6210\u6709\u6548\u7684\u8eab\u4efd\u548c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4e14\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RFA\u5728\u8eab\u4efd\u548c\u4f4d\u7f6e\u5d4c\u5165\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u66f4\u597d\u5e73\u8861\u3002", "relevance": 40.0}}
{"id": "2505.20925", "pdf": "https://arxiv.org/pdf/2505.20925", "abs": "https://arxiv.org/abs/2505.20925", "authors": ["Zhuo Li", "Guodong Du", "Weiyang Guo", "Yigeng Zhou", "Xiucheng Li", "Wenya Wang", "Fangming Liu", "Yequan Wang", "Deheng Ye", "Min Zhang", "Jing Li"], "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models (LLMs) to simultaneously satisfy multiple\nobjectives remains a significant challenge, especially given the diverse and\noften conflicting nature of human preferences. Existing alignment methods\nstruggle to balance trade-offs effectively, often requiring costly retraining\nor yielding suboptimal results across the Pareto frontier of preferences. In\nthis paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a\n\\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play}\napproach that eliminates the need for model training, while enabling LLMs to\nadapt across the entire Pareto frontier and accommodate diverse user\npreferences. In particular, \\textit{HoE} consists of three hierarchical\ncomponents: LoRA Experts, Router Experts and Preference Routing, reaching\noptimal Pareto frontiers and achieving a trade-off between parameter size,\ntraining cost, and performance. We evaluate \\textit{HoE} across various tasks\non 14 objectives and 200 different preferences among 6 benchmarks,\ndemonstrating superior performance over 15 recent baselines. Code is available\nin the supplementary materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHoE\u7684\u8f7b\u91cf\u7ea7\u3001\u53c2\u6570\u9ad8\u6548\u4e14\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u76ee\u6807\u5bf9\u9f50\u4e2d\u7684\u6311\u6218\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e73\u8861\u591a\u76ee\u6807\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u6216\u5728\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "HoE\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u5c42\u6b21\u7ec4\u4ef6\uff1aLoRA\u4e13\u5bb6\u3001\u8def\u7531\u4e13\u5bb6\u548c\u504f\u597d\u8def\u7531\uff0c\u901a\u8fc7\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u5b9e\u73b0\u53c2\u6570\u89c4\u6a21\u3001\u8bad\u7ec3\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "result": "\u572814\u4e2a\u76ee\u6807\u548c6\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684200\u79cd\u4e0d\u540c\u504f\u597d\u4e0a\uff0cHoE\u8868\u73b0\u4f18\u4e8e15\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HoE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u591a\u76ee\u6807\u5bf9\u9f50\u3002", "relevance": 90.0}}
{"id": "2505.20958", "pdf": "https://arxiv.org/pdf/2505.20958", "abs": "https://arxiv.org/abs/2505.20958", "authors": ["Shubham Singh Paliwal", "Arushi Jain", "Monika Sharma", "Vikram Jamwal", "Lovekesh Vig"], "title": "OrienText: Surface Oriented Textual Image Generation", "categories": ["cs.CV"], "comment": "4 pages, SIGGRAPH Asia 2024 Technical Communications", "summary": "Textual content in images is crucial in e-commerce sectors, particularly in\nmarketing campaigns, product imaging, advertising, and the entertainment\nindustry. Current text-to-image (T2I) generation diffusion models, though\nproficient at producing high-quality images, often struggle to incorporate text\naccurately onto complex surfaces with varied perspectives, such as angled views\nof architectural elements like buildings, banners, or walls. In this paper, we\nintroduce the Surface Oriented Textual Image Generation (OrienText) method,\nwhich leverages region-specific surface normals as conditional input to T2I\ngeneration diffusion model. Our approach ensures accurate rendering and correct\norientation of the text within the image context. We demonstrate the\neffectiveness of the OrienText method on a self-curated dataset of images and\ncompare it against the existing textual image generation methods.", "AI": {"tldr": "OrienText\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u533a\u57df\u7279\u5b9a\u7684\u8868\u9762\u6cd5\u7ebf\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u6539\u8fdb\u4e86T2I\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u5728\u590d\u6742\u8868\u9762\u4e0a\u51c6\u786e\u6e32\u67d3\u548c\u5b9a\u5411\u6587\u672c\u3002", "motivation": "\u5f53\u524dT2I\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u8868\u9762\u4e0a\u51c6\u786e\u5d4c\u5165\u6587\u672c\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u5728\u7535\u5b50\u5546\u52a1\u7b49\u9886\u57df\u4e2d\u6587\u672c\u7684\u51c6\u786e\u5448\u73b0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faOrienText\u65b9\u6cd5\uff0c\u5229\u7528\u8868\u9762\u6cd5\u7ebf\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u4f18\u5316T2I\u751f\u6210\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86OrienText\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6587\u672c\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "OrienText\u5728\u590d\u6742\u8868\u9762\u4e0a\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u6587\u672c\u5d4c\u5165\uff0c\u4e3aT2I\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.20997", "pdf": "https://arxiv.org/pdf/2505.20997", "abs": "https://arxiv.org/abs/2505.20997", "authors": ["Sen Bai", "Chunqi Yang", "Xin Bai", "Xin Zhang", "Zhengang Jiang"], "title": "BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Binary (0-1) integer programming (BIP) is pivotal in scientific domains\nrequiring discrete decision-making. As the advance of AI computing, recent\nworks explore neural network-based solvers for integer linear programming (ILP)\nproblems. Yet, they lack scalability for tackling nonlinear challenges. To\nhandle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear\nrelaxations, leading to exponential growth in auxiliary variables and severe\ncomputation limitations. To overcome these limitations, we propose BIPNN\n(Binary Integer Programming Neural Network), an unsupervised learning framework\nto solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).\nSpecifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear\n(sin, log, exp) optimization problems-into unconstrained, differentiable, and\npolynomial loss functions. The reformulation stems from the observation of a\nprecise one-to-one mapping between polynomial BIP objectives and hypergraph\nstructures, enabling the unsupervised training of HyperGNN to optimize BIP\nproblems in an end-to-end manner. On this basis, we propose a GPU-accelerated\nand continuous-annealing-enhanced training pipeline for BIPNN. The pipeline\nenables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel\nvia straightforward gradient descent, thus significantly reducing the training\ncost while ensuring the generation of discrete, high-quality solutions.\nExtensive experiments on synthetic and real-world datasets highlight the\nsuperiority of our approach.", "AI": {"tldr": "BIPNN\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u7ebf\u6027\u4e8c\u8fdb\u5236\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u5728\u5904\u7406\u975e\u7ebf\u6027\u4e8c\u8fdb\u5236\u6574\u6570\u89c4\u5212\u95ee\u9898\u65f6\u7f3a\u4e4f\u6269\u5c55\u6027\uff0c\u800c\u4f20\u7edf\u7684\u5206\u652f\u5207\u5272\u65b9\u6cd5\u56e0\u7ebf\u6027\u677e\u5f1b\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002", "method": "BIPNN\u5c06\u975e\u7ebf\u6027BIP\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5fae\u7684\u591a\u9879\u5f0f\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7GPU\u52a0\u901f\u548c\u8fde\u7eed\u9000\u706b\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBIPNN\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u79bb\u6563\u89e3\u3002", "conclusion": "BIPNN\u4e3a\u975e\u7ebf\u6027BIP\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20933", "pdf": "https://arxiv.org/pdf/2505.20933", "abs": "https://arxiv.org/abs/2505.20933", "authors": ["Duzhen Zhang", "Yong Ren", "Chenxing Li", "Dong Yu", "Tielin Zhang"], "title": "Information-Theoretic Complementary Prompts for Improved Continual Text Classification", "categories": ["cs.CL"], "comment": "Accepted by Neural Networks", "summary": "Continual Text Classification (CTC) aims to continuously classify new text\ndata over time while minimizing catastrophic forgetting of previously acquired\nknowledge. However, existing methods often focus on task-specific knowledge,\noverlooking the importance of shared, task-agnostic knowledge. Inspired by the\ncomplementary learning systems theory, which posits that humans learn\ncontinually through the interaction of two systems -- the hippocampus,\nresponsible for forming distinct representations of specific experiences, and\nthe neocortex, which extracts more general and transferable representations\nfrom past experiences -- we introduce Information-Theoretic Complementary\nPrompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two\ndistinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These\nrespectively encode task-specific and task-invariant knowledge, enabling models\nto sequentially learn classification tasks without relying on data replay. To\npromote more informative prompt learning, InfoComp uses an\ninformation-theoretic framework that maximizes mutual information between\ndifferent parameters (or encoded representations). Within this framework, we\ndesign two novel loss functions: (1) to strengthen the accumulation of\ntask-specific knowledge in P-Prompt, effectively mitigating catastrophic\nforgetting, and (2) to enhance the retention of task-invariant knowledge in\nS-Prompt, improving forward knowledge transfer. Extensive experiments on\ndiverse CTC benchmarks show that our approach outperforms previous\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInfoComp\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u7406\u8bba\u6846\u67b6\u5b66\u4e60\u4e24\u4e2a\u4e0d\u540c\u7684\u63d0\u793a\u7a7a\u95f4\uff08P-Prompt\u548cS-Prompt\uff09\uff0c\u5206\u522b\u7f16\u7801\u4efb\u52a1\u7279\u5b9a\u548c\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\uff0c\u4ee5\u89e3\u51b3\u6301\u7eed\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u800c\u5ffd\u89c6\u4e86\u5171\u4eab\u7684\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\u3002\u53d7\u4eba\u7c7b\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\u7406\u8bba\u542f\u53d1\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u533a\u5206\u4efb\u52a1\u7279\u5b9a\u548c\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\u6765\u6539\u8fdb\u6301\u7eed\u5b66\u4e60\u3002", "method": "InfoComp\u65b9\u6cd5\u901a\u8fc7\u4fe1\u606f\u7406\u8bba\u6846\u67b6\u8bbe\u8ba1\u4e24\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u5206\u522b\u4f18\u5316P-Prompt\u548cS-Prompt\u7684\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u7684\u79ef\u7d2f\u548c\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\u7684\u4fdd\u7559\u3002", "result": "\u5728\u591a\u4e2a\u6301\u7eed\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfoComp\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "InfoComp\u901a\u8fc7\u533a\u5206\u4efb\u52a1\u7279\u5b9a\u548c\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u3002", "relevance": 70.0}}
{"id": "2505.20967", "pdf": "https://arxiv.org/pdf/2505.20967", "abs": "https://arxiv.org/abs/2505.20967", "authors": ["Jiarui Zhang", "Zhihao Li", "Chong Wang", "Bihan Wen"], "title": "RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Neural fields (NFs) have demonstrated remarkable performance in scene\nreconstruction, powering various tasks such as novel view synthesis. However,\nexisting NF methods relying on RGB or LiDAR inputs often exhibit severe\nfragility to adverse weather, particularly when applied in outdoor scenarios\nlike autonomous driving. In contrast, millimeter-wave radar is inherently\nrobust to environmental changes, while unfortunately, its integration with NFs\nremains largely underexplored. Besides, as outdoor driving scenarios frequently\ninvolve moving objects, making spatiotemporal modeling essential for temporally\nconsistent novel view synthesis. To this end, we introduce RF4D, a radar-based\nneural field framework specifically designed for novel view synthesis in\noutdoor dynamic scenes. RF4D explicitly incorporates temporal information into\nits representation, significantly enhancing its capability to model moving\nobjects. We further introduce a feature-level flow module that predicts latent\ntemporal offsets between adjacent frames, enforcing temporal coherence in\ndynamic scene modeling. Moreover, we propose a radar-specific power rendering\nformulation closely aligned with radar sensing physics, improving synthesis\naccuracy and interoperability. Extensive experiments on public radar datasets\ndemonstrate the superior performance of RF4D in terms of radar measurement\nsynthesis quality and occupancy estimation accuracy, achieving especially\npronounced improvements in dynamic outdoor scenarios.", "AI": {"tldr": "RF4D\u662f\u4e00\u4e2a\u57fa\u4e8e\u96f7\u8fbe\u7684\u795e\u7ecf\u573a\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u6237\u5916\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u4fe1\u606f\u548c\u7279\u5f81\u7ea7\u6d41\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRGB\u6216LiDAR\u7684\u795e\u7ecf\u573a\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u800c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5bf9\u73af\u5883\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u4e0e\u795e\u7ecf\u573a\u7684\u7ed3\u5408\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u52a8\u6001\u573a\u666f\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u5efa\u6a21\u4e5f\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "RF4D\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u65f6\u95f4\u4fe1\u606f\u548c\u7279\u5f81\u7ea7\u6d41\u6a21\u5757\u589e\u5f3a\u52a8\u6001\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e0e\u96f7\u8fbe\u7269\u7406\u7279\u6027\u5339\u914d\u7684\u529f\u7387\u6e32\u67d3\u516c\u5f0f\u3002", "result": "\u5728\u516c\u5f00\u96f7\u8fbe\u6570\u636e\u96c6\u4e0a\uff0cRF4D\u5728\u96f7\u8fbe\u6d4b\u91cf\u5408\u6210\u8d28\u91cf\u548c\u5360\u7528\u4f30\u8ba1\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u52a8\u6001\u6237\u5916\u573a\u666f\u4e2d\u6539\u8fdb\u663e\u8457\u3002", "conclusion": "RF4D\u4e3a\u96f7\u8fbe\u4e0e\u795e\u7ecf\u573a\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u5efa\u6a21\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.21005", "pdf": "https://arxiv.org/pdf/2505.21005", "abs": "https://arxiv.org/abs/2505.21005", "authors": ["Fengzhe Zhang", "Laurence I. Midgley", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "title": "Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Score-based diffusion models (SBDMs) are powerful amortized samplers for\nBoltzmann distributions; however, imperfect score estimates bias downstream\nMonte Carlo estimates. Classical importance sampling (IS) can correct this\nbias, but computing exact likelihoods requires solving the probability-flow\nordinary differential equation (PF-ODE), a procedure that is prohibitively\ncostly and scales poorly with dimensionality. We introduce Variance-Tuned\nDiffusion Importance Sampling (VT-DIS), a lightweight post-training method that\nadapts the per-step noise covariance of a pretrained SBDM by minimizing the\n$\\alpha$-divergence ($\\alpha=2$) between its forward diffusion and reverse\ndenoising trajectories. VT-DIS assigns a single trajectory-wise importance\nweight to the joint forward-reverse process, yielding unbiased expectation\nestimates at test time with negligible overhead compared to standard sampling.\nOn the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective\nsample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using\nonly a fraction of the computational budget required by vanilla diffusion + IS\nor PF-ODE-based IS.", "AI": {"tldr": "VT-DIS\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u9884\u8bad\u7ec3SBDM\u7684\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347Boltzmann\u5206\u5e03\u91c7\u6837\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\uff08SBDMs\uff09\u5728Boltzmann\u5206\u5e03\u91c7\u6837\u4e2d\u56e0\u4e0d\u5b8c\u7f8e\u5206\u6570\u4f30\u8ba1\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faVT-DIS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u03b1-\u6563\u5ea6\uff08\u03b1=2\uff09\u8c03\u6574\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u4f7f\u7528\u5355\u4e00\u8def\u5f84\u91cd\u8981\u6027\u6743\u91cd\u5b9e\u73b0\u65e0\u504f\u4f30\u8ba1\u3002", "result": "\u5728DW-4\u3001LJ-13\u548calanine-dipeptide\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u8fbe\u5230\u7ea680%\u300135%\u548c3.5%\u7684\u6709\u6548\u6837\u672c\u91cf\uff0c\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "VT-DIS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eBoltzmann\u5206\u5e03\u7684\u65e0\u504f\u91c7\u6837\u3002", "relevance": 40.0}}
{"id": "2505.20937", "pdf": "https://arxiv.org/pdf/2505.20937", "abs": "https://arxiv.org/abs/2505.20937", "authors": ["Deepesh Gavit", "Debajyoti Mazumder", "Samiran Das", "Jasabanta Patro"], "title": "On VLMs for Diverse Tasks in Multimodal Meme Classification", "categories": ["cs.CL"], "comment": "16 pages", "summary": "In this paper, we present a comprehensive and systematic analysis of\nvision-language models (VLMs) for disparate meme classification tasks. We\nintroduced a novel approach that generates a VLM-based understanding of meme\nimages and fine-tunes the LLMs on textual understanding of the embedded meme\ntext for improving the performance. Our contributions are threefold: (1)\nBenchmarking VLMs with diverse prompting strategies purposely to each sub-task;\n(2) Evaluating LoRA fine-tuning across all VLM components to assess performance\ngains; and (3) Proposing a novel approach where detailed meme interpretations\ngenerated by VLMs are used to train smaller language models (LLMs),\nsignificantly improving classification. The strategy of combining VLMs with\nLLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,\noffensive and sentiment classification, respectively. Our results reveal the\nstrengths and limitations of VLMs and present a novel strategy for meme\nunderstanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u8868\u60c5\u5305\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u3001LoRA\u5fae\u8c03\u548c\u8be6\u7ec6\u89e3\u91ca\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u5229\u7528VLMs\u548cLLMs\u7684\u7ed3\u5408\u6765\u6539\u8fdb\u8868\u60c5\u5305\u7684\u591a\u4efb\u52a1\u5206\u7c7b\uff08\u5982\u8bbd\u523a\u3001\u653b\u51fb\u6027\u548c\u60c5\u611f\u5206\u7c7b\uff09\uff0c\u5e76\u63ed\u793aVLMs\u7684\u4f18\u52a3\u52bf\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u9488\u5bf9\u5b50\u4efb\u52a1\u7684\u591a\u6837\u5316\u63d0\u793a\u7b56\u7565\u57fa\u51c6\u6d4b\u8bd5\uff1b2\uff09\u5bf9VLM\u7ec4\u4ef6\u8fdb\u884cLoRA\u5fae\u8c03\uff1b3\uff09\u5229\u7528VLM\u751f\u6210\u7684\u8be6\u7ec6\u89e3\u91ca\u8bad\u7ec3\u5c0f\u578bLLMs\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408VLMs\u548cLLMs\u7684\u65b9\u6cd5\u5728\u8bbd\u523a\u3001\u653b\u51fb\u6027\u548c\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u5347\u4e868.34%\u30013.52%\u548c26.24%\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0cVLMs\u4e0eLLMs\u7684\u7ed3\u5408\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u540c\u65f6\u63ed\u793a\u4e86VLMs\u7684\u5c40\u9650\u6027\u3002", "relevance": 60.0}}
{"id": "2505.20975", "pdf": "https://arxiv.org/pdf/2505.20975", "abs": "https://arxiv.org/abs/2505.20975", "authors": ["Shamil Ayupov", "Maksim Nakhodnov", "Anastasia Yaschenko", "Andrey Kuznetsov", "Aibek Alanov"], "title": "DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization", "categories": ["cs.CV"], "comment": "The first two authors contributed equally. The source code can be\n  found at https://github.com/ControlGenAI/DreamBoothDPO", "summary": "Personalized diffusion models have shown remarkable success in Text-to-Image\n(T2I) generation by enabling the injection of user-defined concepts into\ndiverse contexts. However, balancing concept fidelity with contextual alignment\nremains a challenging open problem. In this work, we propose an RL-based\napproach that leverages the diverse outputs of T2I models to address this\nissue. Our method eliminates the need for human-annotated scores by generating\na synthetic paired dataset for DPO-like training using external quality\nmetrics. These better-worse pairs are specifically constructed to improve both\nconcept fidelity and prompt adherence. Moreover, our approach supports flexible\nadjustment of the trade-off between image fidelity and textual alignment.\nThrough multi-step training, our approach outperforms a naive baseline in\nconvergence speed and output quality. We conduct extensive qualitative and\nquantitative analysis, demonstrating the effectiveness of our method across\nvarious architectures and fine-tuning techniques. The source code can be found\nat https://github.com/ControlGenAI/DreamBoothDPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u914d\u5bf9\u6570\u636e\u96c6\u4f18\u5316\u6982\u5ff5\u4fdd\u771f\u5ea6\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u6982\u5ff5\u4fdd\u771f\u5ea6\u4e0e\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5229\u7528\u5916\u90e8\u8d28\u91cf\u6307\u6807\u751f\u6210\u5408\u6210\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u7c7b\u4f3cDPO\u7684\u8bad\u7ec3\uff0c\u652f\u6301\u7075\u6d3b\u8c03\u6574\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u6743\u8861\u3002", "result": "\u5728\u591a\u6b65\u8bad\u7ec3\u4e2d\uff0c\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u8f93\u51fa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u67b6\u6784\u548c\u5fae\u8c03\u6280\u672f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.21012", "pdf": "https://arxiv.org/pdf/2505.21012", "abs": "https://arxiv.org/abs/2505.21012", "authors": ["Geetika", "Somya Tyagi", "Bapi Chatterjee"], "title": "Federated Instrumental Variable Analysis via Federated Generalized Method of Moments", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": "28 pages, 3 figures, 1 table", "summary": "Instrumental variables (IV) analysis is an important applied tool for areas\nsuch as healthcare and consumer economics. For IV analysis in high-dimensional\nsettings, the Generalized Method of Moments (GMM) using deep neural networks\noffers an efficient approach. With non-i.i.d. data sourced from scattered\ndecentralized clients, federated learning is a popular paradigm for training\nthe models while promising data privacy. However, to our knowledge, no\nfederated algorithm for either GMM or IV analysis exists to date. In this work,\nwe introduce federated instrumental variables analysis (FedIV) via federated\ngeneralized method of moments (FedGMM). We formulate FedGMM as a federated\nzero-sum game defined by a federated non-convex non-concave minimax\noptimization problem, which is solved using federated gradient descent ascent\n(FedGDA) algorithm. One key challenge arises in theoretically characterizing\nthe federated local optimality. To address this, we present properties and\nexistence results of clients' local equilibria via FedGDA limit points.\nThereby, we show that the federated solution consistently estimates the local\nmoment conditions of every participating client. The proposed algorithm is\nbacked by extensive experiments to demonstrate the efficacy of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8054\u90a6\u5de5\u5177\u53d8\u91cf\u5206\u6790\uff08FedIV\uff09\u548c\u8054\u90a6\u5e7f\u4e49\u77e9\u65b9\u6cd5\uff08FedGMM\uff09\uff0c\u901a\u8fc7\u8054\u90a6\u68af\u5ea6\u4e0b\u964d\u4e0a\u5347\u7b97\u6cd5\uff08FedGDA\uff09\u89e3\u51b3\u975e\u51f8\u975e\u51f9\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5ba2\u6237\u7aef\u5c40\u90e8\u5747\u8861\u7684\u5b58\u5728\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\uff0c\u5de5\u5177\u53d8\u91cf\uff08IV\uff09\u5206\u6790\u5728\u533b\u7597\u548c\u6d88\u8d39\u7ecf\u6d4e\u5b66\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u3002\u76ee\u524d\u5c1a\u65e0\u9488\u5bf9GMM\u6216IV\u5206\u6790\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4e86FedIV\u548cFedGMM\u3002", "method": "\u901a\u8fc7\u8054\u90a6\u96f6\u548c\u535a\u5f08\u5b9a\u4e49FedGMM\uff0c\u4f7f\u7528FedGDA\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u975e\u51f9\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86\u5ba2\u6237\u7aef\u5c40\u90e8\u5747\u8861\u7684\u5b58\u5728\u6027\u3002", "result": "\u8bc1\u660e\u4e86FedGDA\u7684\u6781\u9650\u70b9\u662f\u5ba2\u6237\u7aef\u7684\u5c40\u90e8\u5747\u8861\u70b9\uff0c\u4e14\u8054\u90a6\u89e3\u80fd\u4e00\u81f4\u4f30\u8ba1\u6bcf\u4e2a\u53c2\u4e0e\u5ba2\u6237\u7aef\u7684\u5c40\u90e8\u77e9\u6761\u4ef6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedIV\u548cFedGMM\u4e3a\u9ad8\u7ef4IV\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\u3002", "relevance": 30.0}}
{"id": "2505.20356", "pdf": "https://arxiv.org/pdf/2505.20356", "abs": "https://arxiv.org/abs/2505.20356", "authors": ["Shuoming Zhang", "Jiacheng Zhao", "Chunwei Xia", "Zheng Wang", "Yunji Chen", "Xiaobing Feng", "Huimin Cui"], "title": "LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability", "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": "30 pages, 8 figures, 4 tables. Preprint. Under review", "summary": "Large language models (LLMs) have the potential to revolutionize how we\ndesign and implement compilers and code translation tools. However, existing\nLLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,\na novel neural compilation system that leverages LLMs to translate high-level\nlanguages into assembly code. Our approach centers on three key innovations:\nLEGO translation, which decomposes the input program into manageable blocks;\nbreaking down the complex compilation process into smaller, simpler verifiable\nsteps by organizing it as a verifiable LLM workflow by external tests; and a\nfeedback mechanism for self-correction. Supported by formal proofs of\ntranslation composability, LEGO-Compiler demonstrates high accuracy on multiple\ndatasets, including over 99% on ExeBench and 97.9% on industrial-grade\nAnsiBench. Additionally, LEGO-Compiler has also acheived near one\norder-of-magnitude improvement on compilable code size scalability. This work\nopens new avenues for applying LLMs to system-level tasks, complementing\ntraditional compiler technologies.", "AI": {"tldr": "LEGO-Compiler\u5229\u7528LLMs\u5c06\u9ad8\u7ea7\u8bed\u8a00\u7ffb\u8bd1\u4e3a\u6c47\u7f16\u4ee3\u7801\uff0c\u901a\u8fc7\u5206\u89e3\u7a0b\u5e8f\u5757\u3001\u9a8c\u8bc1\u6b65\u9aa4\u548c\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8bd1\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLMs\u96be\u4ee5\u5904\u7406\u957f\u800c\u590d\u6742\u7684\u7a0b\u5e8f\uff0c\u963b\u788d\u4e86\u5176\u5728\u7f16\u8bd1\u5668\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002LEGO-Compiler\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528LEGO\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5c06\u7a0b\u5e8f\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u5757\uff0c\u5e76\u901a\u8fc7\u5916\u90e8\u6d4b\u8bd5\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u7ed3\u5408\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u81ea\u6821\u6b63\u3002", "result": "\u5728ExeBench\u548cAnsiBench\u4e0a\u5206\u522b\u8fbe\u523099%\u548c97.9%\u7684\u51c6\u786e\u7387\uff0c\u7f16\u8bd1\u4ee3\u7801\u89c4\u6a21\u6269\u5c55\u6027\u63d0\u5347\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "LEGO-Compiler\u4e3aLLMs\u5728\u7cfb\u7edf\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 75.0}}
{"id": "2505.20959", "pdf": "https://arxiv.org/pdf/2505.20959", "abs": "https://arxiv.org/abs/2505.20959", "authors": ["Bertram H\u00f8jer", "Terne Sasha Thorn Jakobsen", "Anna Rogers", "Stefan Heinrich"], "title": "Research Community Perspectives on \"Intelligence\" and Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": "ACL Findings 2025", "summary": "Despite the widespread use of ''artificial intelligence'' (AI) framing in\nNatural Language Processing (NLP) research, it is not clear what researchers\nmean by ''intelligence''. To that end, we present the results of a survey on\nthe notion of ''intelligence'' among researchers and its role in the research\nagenda. The survey elicited complete responses from 303 researchers from a\nvariety of fields including NLP, Machine Learning (ML), Cognitive Science,\nLinguistics, and Neuroscience. We identify 3 criteria of intelligence that the\ncommunity agrees on the most: generalization, adaptability, & reasoning. Our\nresults suggests that the perception of the current NLP systems as\n''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the\nrespondents see developing intelligent systems as a research goal, and these\nrespondents are more likely to consider the current systems intelligent.", "AI": {"tldr": "\u8bba\u6587\u8c03\u67e5\u4e86\u7814\u7a76\u8005\u5bf9\u201c\u667a\u80fd\u201d\u7684\u5b9a\u4e49\uff0c\u53d1\u73b0\u666e\u904d\u8ba4\u540c\u7684\u4e09\u5927\u6807\u51c6\u662f\u6cdb\u5316\u6027\u3001\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002\u591a\u6570\u4eba\u4e0d\u8ba4\u4e3a\u5f53\u524dNLP\u7cfb\u7edf\u201c\u667a\u80fd\u201d\uff0c\u4e14\u5c11\u6570\u4eba\u5c06\u5f00\u53d1\u667a\u80fd\u7cfb\u7edf\u4f5c\u4e3a\u7814\u7a76\u76ee\u6807\u3002", "motivation": "\u63a2\u8ba8NLP\u7814\u7a76\u4e2d\u201c\u667a\u80fd\u201d\u4e00\u8bcd\u7684\u542b\u4e49\u53ca\u5176\u5728\u7814\u7a76\u8bae\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u5bf9303\u540d\u6765\u81eaNLP\u3001ML\u3001\u8ba4\u77e5\u79d1\u5b66\u7b49\u9886\u57df\u7684\u7814\u7a76\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\u3002", "result": "\u4e09\u5927\u667a\u80fd\u6807\u51c6\u4e3a\u6cdb\u5316\u6027\u3001\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\uff1b\u4ec529%\u8ba4\u4e3a\u5f53\u524dNLP\u7cfb\u7edf\u201c\u667a\u80fd\u201d\uff0c16.2%\u5c06\u5f00\u53d1\u667a\u80fd\u7cfb\u7edf\u4f5c\u4e3a\u76ee\u6807\u3002", "conclusion": "\u5f53\u524dNLP\u7cfb\u7edf\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u4e0d\u591f\u667a\u80fd\uff0c\u4e14\u5f00\u53d1\u667a\u80fd\u7cfb\u7edf\u5e76\u975e\u4e3b\u6d41\u7814\u7a76\u76ee\u6807\u3002", "relevance": 40.0}}
{"id": "2505.20981", "pdf": "https://arxiv.org/pdf/2505.20981", "abs": "https://arxiv.org/abs/2505.20981", "authors": ["Cainan Davidson", "Deva Ramanan", "Neehar Peri"], "title": "RefAV: Towards Planning-Centric Scenario Mining", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": null, "summary": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal\ndata localized to HD maps during normal fleet testing. However, identifying\ninteresting and safety-critical scenarios from uncurated driving logs remains a\nsignificant challenge. Traditional scenario mining techniques are error-prone\nand prohibitively time-consuming, often relying on hand-crafted structured\nqueries. In this work, we revisit spatio-temporal scenario mining through the\nlens of recent vision-language models (VLMs) to detect whether a described\nscenario occurs in a driving log and, if so, precisely localize it in both time\nand space. To address this problem, we introduce RefAV, a large-scale dataset\nof 10,000 diverse natural language queries that describe complex multi-agent\ninteractions relevant to motion planning derived from 1000 driving logs in the\nArgoverse 2 Sensor dataset. We evaluate several referential multi-object\ntrackers and present an empirical analysis of our baselines. Notably, we find\nthat naively repurposing off-the-shelf VLMs yields poor performance, suggesting\nthat scenario mining presents unique challenges. Our code and dataset are\navailable at https://github.com/CainanD/RefAV/ and\nhttps://argoverse.github.io/user-guide/tasks/scenario_mining.html", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u65f6\u7a7a\u573a\u666f\u6316\u6398\u65b9\u6cd5RefAV\uff0c\u7528\u4e8e\u4ece\u81ea\u52a8\u9a7e\u9a76\u65e5\u5fd7\u4e2d\u68c0\u6d4b\u548c\u5b9a\u4f4d\u590d\u6742\u573a\u666f\uff0c\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u573a\u666f\u6316\u6398\u6280\u672f\u4f9d\u8d56\u624b\u5de5\u67e5\u8be2\u4e14\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u4ece\u6d77\u91cf\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u4e2d\u8bc6\u522b\u5b89\u5168\u548c\u5173\u952e\u573a\u666f\u3002", "method": "\u5229\u7528VLMs\u68c0\u6d4b\u548c\u5b9a\u4f4d\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u63cf\u8ff0\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u5e76\u5f15\u5165RefAV\u6570\u636e\u96c6\uff0810,000\u4e2a\u67e5\u8be2\uff0c\u57fa\u4e8eArgoverse 2\u6570\u636e\u96c6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f4\u63a5\u4f7f\u7528\u73b0\u6210VLMs\u6548\u679c\u4e0d\u4f73\uff0c\u573a\u666f\u6316\u6398\u5177\u6709\u72ec\u7279\u6311\u6218\u3002", "conclusion": "RefAV\u4e3a\u573a\u666f\u6316\u6398\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316VLMs\u3002", "relevance": 40.0}}
{"id": "2505.21020", "pdf": "https://arxiv.org/pdf/2505.21020", "abs": "https://arxiv.org/abs/2505.21020", "authors": ["Yuan Gao", "Ruiqi Shu", "Hao Wu", "Fan Xu", "Yanfei Xiang", "Ruijian Gou", "Qingsong Wen", "Xian Wu", "Xiaomeng Huang"], "title": "NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically\nimportant for marine research, yet remains challenging due to its substantial\nthermal inertia and extended time delay. Machine learning (ML)-based models\nhave demonstrated significant advancements in simulation accuracy and\ncomputational efficiency compared to traditional numerical methods.\nNevertheless, a significant limitation of current ML models for S2S ocean\nsimulation is their inadequate incorporation of physical consistency and the\nslow-changing properties of the ocean system. In this work, we propose a neural\nocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive\ngraph neural network to emulate diverse physical phenomena associated with\nocean systems effectively. Specifically, we propose a multi-stage framework\ntailored to model the ocean's slowly changing nature. Additionally, we\nintroduce a multi-scale interactive messaging module to capture complex\ndynamical behaviors, such as gradient changes and multiplicative coupling\nrelationships inherent in ocean dynamics. Extensive experimental evaluations\nconfirm that our proposed NeuralOM outperforms state-of-the-art models in S2S\nand extreme event simulation. The codes are available at\nhttps://github.com/YuanGao-YG/NeuralOM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e9a\u5b63\u8282\u5230\u5b63\u8282\uff08S2S\uff09\u6d77\u6d0b\u6a21\u62df\u7684\u795e\u7ecf\u6d77\u6d0b\u6a21\u578b\uff08NeuralOM\uff09\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4ea4\u4e92\u56fe\u795e\u7ecf\u7f51\u7edc\u6709\u6548\u6a21\u62df\u6d77\u6d0b\u7cfb\u7edf\u7684\u7269\u7406\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684S2S\u6d77\u6d0b\u6a21\u62df\u6a21\u578b\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u6d77\u6d0b\u7cfb\u7edf\u6162\u53d8\u7279\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u6846\u67b6\u548c\u591a\u5c3a\u5ea6\u4ea4\u4e92\u6d88\u606f\u6a21\u5757\uff0c\u4ee5\u6a21\u62df\u6d77\u6d0b\u7684\u6162\u53d8\u7279\u6027\u548c\u590d\u6742\u52a8\u529b\u5b66\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNeuralOM\u5728S2S\u548c\u6781\u7aef\u4e8b\u4ef6\u6a21\u62df\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "NeuralOM\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u4e00\u81f4\u6027\u548c\u6162\u53d8\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86S2S\u6d77\u6d0b\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002", "relevance": 30.0}}
{"id": "2505.20963", "pdf": "https://arxiv.org/pdf/2505.20963", "abs": "https://arxiv.org/abs/2505.20963", "authors": ["Felix Krejca", "Tobias Kietreiber", "Alexander Buchelt", "Sebastian Neumaier"], "title": "Context-Aware Content Moderation for German Newspaper Comments", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing volume of online discussions requires advanced automatic\ncontent moderation to maintain responsible discourse. While hate speech\ndetection on social media is well-studied, research on German-language\nnewspaper forums remains limited. Existing studies often neglect\nplatform-specific context, such as user history and article themes. This paper\naddresses this gap by developing and evaluating binary classification models\nfor automatic content moderation in German newspaper forums, incorporating\ncontextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging\nthe One Million Posts Corpus from the Austrian newspaper Der Standard, we\nassess the impact of context-aware models. Results show that CNN and LSTM\nmodels benefit from contextual information and perform competitively with\nstate-of-the-art approaches. In contrast, ChatGPT's zero-shot classification\ndoes not improve with added context and underperforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5fb7\u8bed\u62a5\u7eb8\u8bba\u575b\u7684\u5185\u5bb9\u5ba1\u6838\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u8bc4\u4f30\u4e86LSTM\u3001CNN\u548cChatGPT-3.5 Turbo\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793aCNN\u548cLSTM\u8868\u73b0\u4f18\u4e8eChatGPT\u3002", "motivation": "\u586b\u8865\u5fb7\u8bed\u62a5\u7eb8\u8bba\u575b\u5185\u5bb9\u5ba1\u6838\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63a2\u7d22\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528LSTM\u3001CNN\u548cChatGPT-3.5 Turbo\u6a21\u578b\uff0c\u7ed3\u5408One Million Posts Corpus\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4f5c\u7528\u3002", "result": "CNN\u548cLSTM\u6a21\u578b\u5728\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u540e\u8868\u73b0\u4f18\u5f02\uff0c\u800cChatGPT\u7684\u96f6\u6837\u672c\u5206\u7c7b\u672a\u53d7\u76ca\u4e8e\u4e0a\u4e0b\u6587\u4e14\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4f20\u7edf\u6a21\u578b\uff08\u5982CNN\u548cLSTM\uff09\u6709\u76ca\uff0c\u4f46\u5bf9ChatGPT\u65e0\u6548\uff0c\u5efa\u8bae\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u4f18\u5316\u6a21\u578b\u3002", "relevance": 30.0}}
{"id": "2505.20985", "pdf": "https://arxiv.org/pdf/2505.20985", "abs": "https://arxiv.org/abs/2505.20985", "authors": ["Mustafa \u0130zzet Mu\u015ftu", "Haz\u0131m Kemal Ekenel"], "title": "Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos", "categories": ["cs.CV"], "comment": "Accepted to the 2025 25th International Conference on Digital Signal\n  Processing (DSP 2025)", "summary": "The increasing demand for large-scale visual data, coupled with strict\nprivacy regulations, has driven research into anonymization methods that hide\npersonal identities without seriously degrading data quality. In this paper, we\nexplore the potential of face swapping methods to preserve privacy in video\ndata. Through extensive evaluations focusing on temporal consistency, anonymity\nstrength, and visual fidelity, we find that face swapping techniques can\nproduce consistent facial transitions and effectively hide identities. These\nresults underscore the suitability of face swapping for privacy-preserving\nvideo applications and lay the groundwork for future advancements in\nanonymization focused face-swapping models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u4eba\u8138\u4ea4\u6362\u6280\u672f\u4fdd\u62a4\u89c6\u9891\u6570\u636e\u9690\u79c1\u7684\u6f5c\u529b\uff0c\u8bc4\u4f30\u4e86\u5176\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u533f\u540d\u5f3a\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6280\u672f\u9002\u5408\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u9700\u6c42\u4e0e\u9690\u79c1\u6cd5\u89c4\u7684\u51b2\u7a81\u4fc3\u4f7f\u7814\u7a76\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u4ee5\u5728\u4e0d\u4e25\u91cd\u964d\u4f4e\u6570\u636e\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u9690\u85cf\u4e2a\u4eba\u8eab\u4efd\u3002", "method": "\u901a\u8fc7\u4eba\u8138\u4ea4\u6362\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728\u89c6\u9891\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u533f\u540d\u5f3a\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u4eba\u8138\u4ea4\u6362\u6280\u672f\u80fd\u4ea7\u751f\u4e00\u81f4\u7684\u9762\u90e8\u8fc7\u6e21\u5e76\u6709\u6548\u9690\u85cf\u8eab\u4efd\u3002", "conclusion": "\u4eba\u8138\u4ea4\u6362\u6280\u672f\u9002\u5408\u9690\u79c1\u4fdd\u62a4\u89c6\u9891\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u533f\u540d\u5316\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 20.0}}
{"id": "2505.21024", "pdf": "https://arxiv.org/pdf/2505.21024", "abs": "https://arxiv.org/abs/2505.21024", "authors": ["Charles London", "Varun Kanade"], "title": "Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Pause tokens, simple filler symbols such as \"...\", consistently improve\nTransformer performance on both language and mathematical tasks, yet their\ntheoretical effect remains unexplained. We provide the first formal separation\nresult, proving that adding pause tokens to constant-depth, logarithmic-width\nTransformers strictly increases their computational expressivity. With\nbounded-precision activations, Transformers without pause tokens compute only a\nstrict subset of $\\mathsf{AC}^0$ functions, while adding a polynomial number of\npause tokens allows them to express the entire class. For logarithmic-precision\nTransformers, we show that adding pause tokens achieves expressivity equivalent\nto $\\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate\nthat two-layer causally masked Transformers can learn parity when supplied with\npause tokens, a function that they appear unable to learn without them. Our\nresults provide a rigorous theoretical explanation for prior empirical\nfindings, clarify how pause tokens interact with width, depth, and numeric\nprecision, and position them as a distinct mechanism, complementary to\nchain-of-thought prompting, for enhancing Transformer reasoning.", "AI": {"tldr": "Pause tokens enhance Transformer expressivity, enabling them to compute more complex functions (e.g., parity) and matching theoretical bounds like TC\u2070.", "motivation": "To theoretically explain the empirical benefits of pause tokens in Transformers and clarify their role in computational expressivity.", "method": "Formal separation proofs and empirical validation with bounded-precision and logarithmic-precision Transformers.", "result": "Pause tokens strictly increase expressivity, enabling Transformers to compute AC\u2070 and TC\u2070 functions, and learn parity in practice.", "conclusion": "Pause tokens are a distinct mechanism for enhancing Transformer reasoning, complementary to chain-of-thought prompting.", "relevance": 85.0}}
{"id": "2505.20362", "pdf": "https://arxiv.org/pdf/2505.20362", "abs": "https://arxiv.org/abs/2505.20362", "authors": ["Jiahui Geng", "Qing Li", "Zongxiong Chen", "Yuxia Wang", "Derui Zhu", "Zhuohan Xie", "Chenyang Lyu", "Xiuying Chen", "Preslav Nakov", "Fakhri Karray"], "title": "VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The rapid advancement of vision-language models (VLMs) has brought a lot of\nattention to their safety alignment. However, existing methods have primarily\nfocused on model undersafety, where the model responds to hazardous queries,\nwhile neglecting oversafety, where the model refuses to answer safe queries. In\nthis paper, we introduce the concept of $\\textit{safety calibration}$, which\nsystematically addresses both undersafety and oversafety. Specifically, we\npresent $\\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are\nvisually or textually similar but differ in terms of safety, which is designed\nto evaluate safety calibration across image-centric and text-centric scenarios.\nBased on our benchmark, we evaluate safety calibration across eleven widely\nused VLMs. Our extensive experiments revealed major issues with both\nundersafety and oversafety. We further investigated four approaches to improve\nthe model's safety calibration. We found that even though some methods\neffectively calibrated the models' safety problems, these methods also lead to\nthe degradation of models' utility. This trade-off underscores the urgent need\nfor advanced calibration methods, and our benchmark provides a valuable tool\nfor evaluating future approaches. Our code and data are available at\nhttps://github.com/jiahuigeng/VSCBench.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5b89\u5168\u6821\u51c6\u6982\u5ff5\uff0c\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u6b20\u5b89\u5168\u548c\u8fc7\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5f15\u5165VSCBench\u6570\u636e\u96c6\u8bc4\u4f3011\u79cdVLMs\u7684\u5b89\u5168\u6821\u51c6\u8868\u73b0\u3002\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u4e14\u6539\u8fdb\u65b9\u6cd5\u4f1a\u964d\u4f4e\u6a21\u578b\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u6b20\u5b89\u5168\uff08\u56de\u7b54\u5371\u9669\u67e5\u8be2\uff09\uff0c\u800c\u5ffd\u7565\u8fc7\u5b89\u5168\uff08\u62d2\u7edd\u5b89\u5168\u67e5\u8be2\uff09\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u4e24\u8005\u3002", "method": "\u63d0\u51fa\u5b89\u5168\u6821\u51c6\u6982\u5ff5\uff0c\u6784\u5efaVSCBench\u6570\u636e\u96c6\uff083600\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff09\uff0c\u8bc4\u4f3011\u79cdVLMs\uff0c\u5e76\u5c1d\u8bd5\u56db\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aVLMs\u5728\u6b20\u5b89\u5168\u548c\u8fc7\u5b89\u5168\u65b9\u9762\u5747\u5b58\u5728\u95ee\u9898\uff0c\u6539\u8fdb\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u4f1a\u964d\u4f4e\u6a21\u578b\u5b9e\u7528\u6027\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u5b89\u5168\u6821\u51c6\u65b9\u6cd5\uff0cVSCBench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.20966", "pdf": "https://arxiv.org/pdf/2505.20966", "abs": "https://arxiv.org/abs/2505.20966", "authors": ["Zhibo Wang", "Xiaoze Jiang", "Zhiheng Qin", "Enyun Yu", "Han Li"], "title": "Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation", "categories": ["cs.CL", "cs.IR"], "comment": "KDD 2025", "summary": "Query auto-completion (QAC) plays a crucial role in modern search systems.\nHowever, in real-world applications, there are two pressing challenges that\nstill need to be addressed. First, there is a need for hierarchical\npersonalized representations for users. Previous approaches have typically used\nusers' search behavior as a single, overall representation, which proves\ninadequate in more nuanced generative scenarios. Additionally, query prefixes\nare typically short and may contain typos or sensitive information, increasing\nthe likelihood of generating toxic content compared to traditional text\ngeneration tasks. Such toxic content can degrade user experience and lead to\npublic relations issues. Therefore, the second critical challenge is\ndetoxifying QAC systems.\n  To address these two limitations, we propose a novel model (LaD) that\ncaptures personalized information from both long-term and short-term interests,\nincorporating adaptive detoxification. In LaD, personalized information is\ncaptured hierarchically at both coarse-grained and fine-grained levels. This\napproach preserves as much personalized information as possible while enabling\nonline generation within time constraints. To move a futher step, we propose an\nonline training method based on Reject Preference Optimization (RPO). By\nincorporating a special token [Reject] during both the training and inference\nprocesses, the model achieves adaptive detoxification. Consequently, the\ngenerated text presented to users is both non-toxic and relevant to the given\nprefix. We conduct comprehensive experiments on industrial-scale datasets and\nperform online A/B tests, delivering the largest single-experiment metric\nimprovement in nearly two years of our product. Our model has been deployed on\nKuaishou search, driving the primary traffic for hundreds of millions of active\nusers. The code is available at https://github.com/JXZe/LaD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLaD\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u4e2a\u6027\u5316\u8868\u793a\u548c\u81ea\u9002\u5e94\u53bb\u6bd2\u6280\u672f\u89e3\u51b3\u67e5\u8be2\u81ea\u52a8\u8865\u5168\uff08QAC\uff09\u4e2d\u7684\u4e24\u5927\u6311\u6218\u3002", "motivation": "\u89e3\u51b3QAC\u7cfb\u7edf\u4e2d\u4e2a\u6027\u5316\u8868\u793a\u4e0d\u8db3\u548c\u751f\u6210\u6709\u6bd2\u5185\u5bb9\u7684\u95ee\u9898\u3002", "method": "LaD\u6a21\u578b\u7ed3\u5408\u957f\u671f\u548c\u77ed\u671f\u5174\u8da3\u7684\u5206\u5c42\u4e2a\u6027\u5316\u8868\u793a\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u62d2\u7edd\u504f\u597d\u4f18\u5316\uff08RPO\uff09\u7684\u5728\u7ebf\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u81ea\u9002\u5e94\u53bb\u6bd2\u3002", "result": "\u5728\u5de5\u4e1a\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\uff0c\u6a21\u578b\u5df2\u5728\u5feb\u624b\u641c\u7d22\u4e2d\u90e8\u7f72\u3002", "conclusion": "LaD\u6a21\u578b\u5728\u4fdd\u6301\u4e2a\u6027\u5316\u4fe1\u606f\u7684\u540c\u65f6\u6709\u6548\u53bb\u6bd2\uff0c\u663e\u8457\u63d0\u5347\u4e86QAC\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "relevance": 50.0}}
{"id": "2505.21002", "pdf": "https://arxiv.org/pdf/2505.21002", "abs": "https://arxiv.org/abs/2505.21002", "authors": ["Mustafa \u0130zzet Mu\u015ftu", "Haz\u0131m Kemal Ekenel"], "title": "Facial Attribute Based Text Guided Face Anonymization", "categories": ["cs.CV", "I.4.9; I.2.10; I.4.8"], "comment": "6 pages, 5 figures, published in the Proceedings of the Joint\n  visuAAL-GoodBrother Conference on Trustworthy Video- and Audio-Based\n  Assistive Technologies", "summary": "The increasing prevalence of computer vision applications necessitates\nhandling vast amounts of visual data, often containing personal information.\nWhile this technology offers significant benefits, it should not compromise\nprivacy. Data privacy regulations emphasize the need for individual consent for\nprocessing personal data, hindering researchers' ability to collect\nhigh-quality datasets containing the faces of the individuals. This paper\npresents a deep learning-based face anonymization pipeline to overcome this\nchallenge. Unlike most of the existing methods, our method leverages recent\nadvancements in diffusion-based inpainting models, eliminating the need for\ntraining Generative Adversarial Networks. The pipeline employs a three-stage\napproach: face detection with RetinaNet, feature extraction with VGG-Face, and\nrealistic face generation using the state-of-the-art BrushNet diffusion model.\nBrushNet utilizes the entire image, face masks, and text prompts specifying\ndesired facial attributes like age, ethnicity, gender, and expression. This\nenables the generation of natural-looking images with unrecognizable\nindividuals, facilitating the creation of privacy-compliant datasets for\ncomputer vision research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9762\u90e8\u533f\u540d\u5316\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u6269\u6563\u4fee\u590d\u6a21\u578b\u751f\u6210\u81ea\u7136\u4f46\u4e0d\u53ef\u8bc6\u522b\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u9690\u79c1\u5408\u89c4\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u5904\u7406\u5927\u91cf\u5305\u542b\u4e2a\u4eba\u4fe1\u606f\u7684\u89c6\u89c9\u6570\u636e\u65f6\uff0c\u9690\u79c1\u4fdd\u62a4\u6210\u4e3a\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bad\u7ec3GAN\uff0c\u800c\u672c\u6587\u63d0\u51fa\u65e0\u9700GAN\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528RetinaNet\u8fdb\u884c\u9762\u90e8\u68c0\u6d4b\uff1b2) \u4f7f\u7528VGG-Face\u63d0\u53d6\u7279\u5f81\uff1b3) \u5229\u7528BrushNet\u6269\u6563\u6a21\u578b\u751f\u6210\u81ea\u7136\u4f46\u4e0d\u53ef\u8bc6\u522b\u7684\u9762\u90e8\u56fe\u50cf\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u81ea\u7136\u4e14\u9690\u79c1\u5408\u89c4\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9690\u79c1\u5408\u89c4\u6570\u636e\u96c6\u521b\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56GAN\u8bad\u7ec3\u3002", "relevance": 30.0}}
{"id": "2505.21027", "pdf": "https://arxiv.org/pdf/2505.21027", "abs": "https://arxiv.org/abs/2505.21027", "authors": ["Zhipeng He", "Chun Ouyang", "Lijie Wen", "Cong Liu", "Catarina Moreira"], "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data", "categories": ["cs.LG", "cs.AI"], "comment": "63 pages, 22 figures, 6 tables", "summary": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks have been extensively studied in unstructured data\nlike images, their application to tabular data presents new challenges. These\nchallenges arise from the inherent heterogeneity and complex feature\ninterdependencies in tabular data, which differ significantly from those in\nimage data. To address these differences, it is crucial to consider\nimperceptibility as a key criterion specific to tabular data. Most current\nresearch focuses primarily on achieving effective adversarial attacks, often\noverlooking the importance of maintaining imperceptibility. To address this\ngap, we propose a new benchmark for adversarial attacks on tabular data that\nevaluates both effectiveness and imperceptibility. In this study, we assess the\neffectiveness and imperceptibility of five adversarial attacks across four\nmodels using eleven tabular datasets, including both mixed and numerical-only\ndatasets. Our analysis explores how these factors interact and influence the\noverall performance of the attacks. We also compare the results across\ndifferent dataset types to understand the broader implications of these\nfindings. The findings from this benchmark provide valuable insights for\nimproving the design of adversarial attack algorithms, thereby advancing the\nfield of adversarial machine learning on tabular data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u5bf9\u6297\u653b\u51fb\u65b0\u57fa\u51c6\uff0c\u8bc4\u4f30\u653b\u51fb\u7684\u6709\u6548\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u5bf9\u6297\u653b\u51fb\u5728\u56fe\u50cf\u6570\u636e\u4e2d\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u5728\u8868\u683c\u6570\u636e\u4e2d\u56e0\u7279\u5f81\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u5e26\u6765\u65b0\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u89c6\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "method": "\u8bc4\u4f30\u4e94\u79cd\u5bf9\u6297\u653b\u51fb\u5728\u56db\u79cd\u6a21\u578b\u548c\u5341\u4e00\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u6709\u6548\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u7684\u4ea4\u4e92\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6539\u8fdb\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u8868\u683c\u6570\u636e\u5bf9\u6297\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "conclusion": "\u65b0\u57fa\u51c6\u4e3a\u8868\u683c\u6570\u636e\u5bf9\u6297\u653b\u51fb\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "relevance": 60.0}}
{"id": "2505.20368", "pdf": "https://arxiv.org/pdf/2505.20368", "abs": "https://arxiv.org/abs/2505.20368", "authors": ["Jaeyoung Choe", "Jihoon Kim", "Woohwan Jung"], "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86HiREC\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u68c0\u7d22\u548c\u8bc1\u636e\u6574\u7406\u89e3\u51b3\u91d1\u878d\u9886\u57dfRAG\u6a21\u578b\u5728\u6807\u51c6\u5316\u6587\u6863\u4e2d\u91cd\u590d\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86LOFin\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u6807\u51c6\u5316\u91d1\u878d\u6587\u6863\uff08\u5982SEC\u6587\u4ef6\uff09\u7684\u683c\u5f0f\u76f8\u4f3c\u6027\u5bfc\u81f4\u4f20\u7edfRAG\u65b9\u6cd5\u8bef\u8bc6\u522b\u91cd\u590d\u6587\u672c\uff0c\u5f71\u54cd\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "method": "HiREC\u6846\u67b6\u91c7\u7528\u5206\u5c42\u68c0\u7d22\uff08\u5148\u68c0\u7d22\u76f8\u5173\u6587\u6863\uff0c\u518d\u9009\u62e9\u6700\u76f8\u5173\u6bb5\u843d\uff09\u548c\u8bc1\u636e\u6574\u7406\uff08\u53bb\u9664\u65e0\u5173\u6bb5\u843d\uff0c\u5fc5\u8981\u65f6\u751f\u6210\u8865\u5145\u67e5\u8be2\uff09\u3002", "result": "\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u5305\u542b145,897\u4efdSEC\u6587\u6863\u548c1,595\u4e2a\u95ee\u7b54\u5bf9\u7684LOFin\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "HiREC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u878d\u9886\u57dfRAG\u6a21\u578b\u7684\u91cd\u590d\u68c0\u7d22\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.20971", "pdf": "https://arxiv.org/pdf/2505.20971", "abs": "https://arxiv.org/abs/2505.20971", "authors": ["Xiangqing Shen", "Fanfan Wang", "Rui Xia"], "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks,\nyet they often suffer from hallucinations and lack reliable factual grounding.\nMeanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack\nthe flexible reasoning abilities of LLMs. In this paper, we present\nReason-Align-Respond (RAR), a novel framework that systematically integrates\nLLM reasoning with knowledge graphs for KGQA. Our approach consists of three\nkey components: a Reasoner that generates human-like reasoning chains, an\nAligner that maps these chains to valid KG paths, and a Responser that\nsynthesizes the final answer. We formulate this process as a probabilistic\nmodel and optimize it using the Expectation-Maximization algorithm, which\niteratively refines the reasoning chains and knowledge paths. Extensive\nexperiments on multiple benchmarks demonstrate the effectiveness of RAR,\nachieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on\nWebQSP and CWQ respectively. Human evaluation confirms that RAR generates\nhigh-quality, interpretable reasoning chains well-aligned with KG paths.\nFurthermore, RAR exhibits strong zero-shot generalization capabilities and\nmaintains computational efficiency during inference.", "AI": {"tldr": "RAR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u63a8\u7406\u4e0e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63d0\u5347KGQA\u4efb\u52a1\u6027\u80fd\uff0c\u5305\u62ec\u63a8\u7406\u94fe\u751f\u6210\u3001KG\u8def\u5f84\u5bf9\u9f50\u548c\u7b54\u6848\u5408\u6210\uff0c\u5b9e\u73b0\u4e86SOTA\u6548\u679c\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u548cKG\u7f3a\u4e4f\u7075\u6d3b\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faRAR\u6846\u67b6\uff0c\u5305\u542bReasoner\u3001Aligner\u548cResponser\uff0c\u901a\u8fc7EM\u7b97\u6cd5\u4f18\u5316\u63a8\u7406\u94fe\u4e0eKG\u8def\u5f84\u7684\u5bf9\u9f50\u3002", "result": "\u5728WebQSP\u548cCWQ\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523093.3%\u548c91.0%\u7684Hit@1\u5206\u6570\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u63a8\u7406\u9ad8\u6548\u3002", "conclusion": "RAR\u6709\u6548\u6574\u5408LLM\u4e0eKG\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u9002\u7528\u4e8eKGQA\u4efb\u52a1\u3002", "relevance": 85.0}}
{"id": "2505.21010", "pdf": "https://arxiv.org/pdf/2505.21010", "abs": "https://arxiv.org/abs/2505.21010", "authors": ["Sabbir Ahmed", "Mamshad Nayeem Rizve", "Abdullah Al Arafat", "Jacqueline Liu", "Rahim Hossain", "Mohaiminul Al Nahian", "Adnan Siraj Rakin"], "title": "Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Semi-Supervised Federated Learning (SSFL) is gaining popularity over\nconventional Federated Learning in many real-world applications. Due to the\npractical limitation of limited labeled data on the client side, SSFL considers\nthat participating clients train with unlabeled data, and only the central\nserver has the necessary resources to access limited labeled data, making it an\nideal fit for real-world applications (e.g., healthcare). However, traditional\nSSFL assumes that the data distributions in the training phase and testing\nphase are the same. In practice, however, domain shifts frequently occur,\nmaking it essential for SSFL to incorporate generalization capabilities and\nenhance their practicality. The core challenge is improving model\ngeneralization to new, unseen domains while the client participate in SSFL.\nHowever, the decentralized setup of SSFL and unsupervised client training\nnecessitates innovation to achieve improved generalization across domains. To\nachieve this, we propose a novel framework called the Unified Alignment\nProtocol (UAP), which consists of an alternating two-stage training process.\nThe first stage involves training the server model to learn and align the\nfeatures with a parametric distribution, which is subsequently communicated to\nclients without additional communication overhead. The second stage proposes a\nnovel training algorithm that utilizes the server feature distribution to align\nclient features accordingly. Our extensive experiments on standard domain\ngeneralization benchmark datasets across multiple model architectures reveal\nthat proposed UAP successfully achieves SOTA generalization performance in SSFL\nsetting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7edf\u4e00\u5bf9\u9f50\u534f\u8bae\uff08UAP\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\uff08SSFL\uff09\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316\u3002", "motivation": "SSFL\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u4e00\u81f4\uff0c\u4f46\u5b9e\u9645\u4e2d\u9886\u57df\u504f\u79fb\u5e38\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "UAP\u6846\u67b6\u5305\u62ec\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u670d\u52a1\u5668\u6a21\u578b\u5b66\u4e60\u5e76\u5bf9\u9f50\u7279\u5f81\u5206\u5e03\uff0c\u968f\u540e\u5ba2\u6237\u7aef\u5229\u7528\u670d\u52a1\u5668\u7279\u5f81\u5206\u5e03\u8fdb\u884c\u5bf9\u9f50\u8bad\u7ec3\u3002", "result": "\u5728\u6807\u51c6\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUAP\u5728\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "UAP\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86SSFL\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21034", "pdf": "https://arxiv.org/pdf/2505.21034", "abs": "https://arxiv.org/abs/2505.21034", "authors": ["Wenhu Li", "Niki van Stein", "Thomas B\u00e4ck", "Elena Raponi"], "title": "LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Bayesian optimization (BO) is a powerful class of algorithms for optimizing\nexpensive black-box functions, but designing effective BO algorithms remains a\nmanual, expertise-driven task. Recent advancements in Large Language Models\n(LLMs) have opened new avenues for automating scientific discovery, including\nthe automatic design of optimization algorithms. While prior work has used LLMs\nwithin optimization loops or to generate non-BO algorithms, we tackle a new\nchallenge: Using LLMs to automatically generate full BO algorithm code. Our\nframework uses an evolution strategy to guide an LLM in generating Python code\nthat preserves the key components of BO algorithms: An initial design, a\nsurrogate model, and an acquisition function. The LLM is prompted to produce\nmultiple candidate algorithms, which are evaluated on the established Black-Box\nOptimization Benchmarking (BBOB) test suite from the COmparing Continuous\nOptimizers (COCO) platform. Based on their performance, top candidates are\nselected, combined, and mutated via controlled prompt variations, enabling\niterative refinement. Despite no additional fine-tuning, the LLM-generated\nalgorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB\nfunctions in dimension 5 and generalize well to higher dimensions, and\ndifferent tasks (from the Bayesmark framework). This work demonstrates that\nLLMs can serve as algorithmic co-designers, offering a new paradigm for\nautomating BO development and accelerating the discovery of novel algorithmic\ncombinations. The source code is provided at\nhttps://github.com/Ewendawi/LLaMEA-BO.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u7b97\u6cd5\u4ee3\u7801\uff0c\u901a\u8fc7\u8fdb\u5316\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\uff0c\u5728BBOB\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u5316\u8bbe\u8ba1BO\u7b97\u6cd5\u4ee5\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63a2\u7d22LLM\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b56\u7565\u5f15\u5bfcLLM\u751f\u6210BO\u7b97\u6cd5\u4ee3\u7801\uff0c\u5305\u62ec\u521d\u59cb\u8bbe\u8ba1\u3001\u4ee3\u7406\u6a21\u578b\u548c\u83b7\u53d6\u51fd\u6570\uff0c\u5e76\u901a\u8fc7BBOB\u6d4b\u8bd5\u8bc4\u4f30\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u572824\u4e2aBBOB\u51fd\u6570\u4e2d\u768419\u4e2a\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709BO\u7b97\u6cd5\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u66f4\u9ad8\u7ef4\u5ea6\u548c\u4e0d\u540c\u4efb\u52a1\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5de5\u5177\uff0c\u4e3aBO\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002", "relevance": 75.0}}
{"id": "2505.20377", "pdf": "https://arxiv.org/pdf/2505.20377", "abs": "https://arxiv.org/abs/2505.20377", "authors": ["Lennart Ullner", "Alona Zharova", "Felix Creutzig"], "title": "Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Efficient energy management in prosumer households is key to alleviating grid\nstress in an energy transition marked by electric vehicles (EV), renewable\nenergies and battery storage. However, it is unclear how households optimize\nprosumer EV charging. Here we study real-world data from 90 households on\nfixed-rate electricity tariffs in German-speaking countries to investigate the\npotential of Deep Reinforcement Learning (DRL) and other control approaches\n(Rule-Based, Model Predictive Control) to manage the dynamic and uncertain\nenvironment of Home Energy Management (HEM) and optimize household charging\npatterns. The DRL agent efficiently aligns charging of EV and battery storage\nwith photovoltaic (PV) surplus. We find that frequent EV charging transactions,\nearly EV connections and PV surplus increase optimization potential. A detailed\nanalysis of nine households (1 hour resolution, 1 year) demonstrates that high\nbattery capacity facilitates self optimization; in this case further\nalgorithmic control shows little value. In cases with relatively low battery\ncapacity, algorithmic control with DRL improves energy management and cost\nsavings by a relevant margin. This result is further corroborated by our\nsimulation of a synthetic household. We conclude that prosumer households with\noptimization potential would profit from DRL, thus benefiting also the full\nelectricity system and its decarbonization.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\uff08HEM\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0DRL\u80fd\u6709\u6548\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u5145\u7535\u548c\u5149\u4f0f\uff08PV\uff09\u5269\u4f59\u5229\u7528\uff0c\u5c24\u5176\u5728\u7535\u6c60\u5bb9\u91cf\u8f83\u4f4e\u65f6\u6548\u679c\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u7f13\u89e3\u7535\u7f51\u538b\u529b\u5e76\u4fc3\u8fdb\u80fd\u6e90\u8f6c\u578b\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u6570\u636e\uff0890\u6237\u5bb6\u5ead\uff09\u6bd4\u8f83DRL\u3001\u89c4\u5219\u63a7\u5236\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5728HEM\u4e2d\u7684\u8868\u73b0\u3002", "result": "DRL\u5728\u7535\u6c60\u5bb9\u91cf\u8f83\u4f4e\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6e90\u7ba1\u7406\u548c\u6210\u672c\u8282\u7ea6\uff0c\u800c\u9ad8\u7535\u6c60\u5bb9\u91cf\u65f6\u7b97\u6cd5\u63a7\u5236\u4ef7\u503c\u6709\u9650\u3002", "conclusion": "DRL\u5bf9\u6709\u4f18\u5316\u6f5c\u529b\u7684\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u7535\u7f51\u8131\u78b3\u3002", "relevance": 30.0}}
{"id": "2505.20976", "pdf": "https://arxiv.org/pdf/2505.20976", "abs": "https://arxiv.org/abs/2505.20976", "authors": ["Peiming Guo", "Meishan Zhang", "Jianling Li", "Min Zhang", "Yue Zhang"], "title": "Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main conference", "summary": "Cross-domain constituency parsing is still an unsolved challenge in\ncomputational linguistics since the available multi-domain constituency\ntreebank is limited. We investigate automatic treebank generation by large\nlanguage models (LLMs) in this paper. The performance of LLMs on constituency\nparsing is poor, therefore we propose a novel treebank generation method, LLM\nback generation, which is similar to the reverse process of constituency\nparsing. LLM back generation takes the incomplete cross-domain constituency\ntree with only domain keyword leaf nodes as input and fills the missing words\nto generate the cross-domain constituency treebank. Besides, we also introduce\na span-level contrastive learning pre-training strategy to make full use of the\nLLM back generation treebank for cross-domain constituency parsing. We verify\nthe effectiveness of our LLM back generation treebank coupled with contrastive\nlearning pre-training on five target domains of MCTB. Experimental results show\nthat our approach achieves state-of-the-art performance on average results\ncompared with various baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6811\u5e93\u751f\u6210\u65b9\u6cd5\uff08LLM\u53cd\u5411\u751f\u6210\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u9886\u57df\u9009\u533a\u89e3\u6790\u7684\u6311\u6218\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8de8\u9886\u57df\u9009\u533a\u89e3\u6790\u5728\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u73b0\u6709\u591a\u9886\u57df\u6811\u5e93\u6709\u9650\uff0c\u56e0\u6b64\u63a2\u7d22\u5229\u7528LLM\u81ea\u52a8\u751f\u6210\u6811\u5e93\u3002", "method": "\u63d0\u51faLLM\u53cd\u5411\u751f\u6210\u65b9\u6cd5\uff0c\u8f93\u5165\u4ec5\u542b\u9886\u57df\u5173\u952e\u8bcd\u53f6\u8282\u70b9\u7684\u4e0d\u5b8c\u6574\u6811\uff0c\u586b\u5145\u7f3a\u5931\u8bcd\u751f\u6210\u8de8\u9886\u57df\u6811\u5e93\uff1b\u7ed3\u5408\u8de8\u5ea6\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728MCTB\u7684\u4e94\u4e2a\u76ee\u6807\u9886\u57df\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "LLM\u53cd\u5411\u751f\u6210\u6811\u5e93\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u80fd\u6709\u6548\u63d0\u5347\u8de8\u9886\u57df\u9009\u533a\u89e3\u6790\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.21032", "pdf": "https://arxiv.org/pdf/2505.21032", "abs": "https://arxiv.org/abs/2505.21032", "authors": ["Nils Neukirch", "Johanna Vielhaben", "Nils Strodthoff"], "title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 10 figures, code is available at\n  https://github.com/AI4HealthUOL/FeatInv", "summary": "Internal representations are crucial for understanding deep neural networks,\nsuch as their properties and reasoning patterns, but remain difficult to\ninterpret. While mapping from feature space to input space aids in interpreting\nthe former, existing approaches often rely on crude approximations. We propose\nusing a conditional diffusion model - a pretrained high-fidelity diffusion\nmodel conditioned on spatially resolved feature maps - to learn such a mapping\nin a probabilistic manner. We demonstrate the feasibility of this approach\nacross various pretrained image classifiers from CNNs to ViTs, showing\nexcellent reconstruction capabilities. Through qualitative comparisons and\nrobustness analysis, we validate our method and showcase possible applications,\nsuch as the visualization of concept steering in input space or investigations\nof the composite nature of the feature space. This approach has broad potential\nfor improving feature space understanding in computer vision models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7279\u5f81\u7a7a\u95f4\u5230\u8f93\u5165\u7a7a\u95f4\u7684\u6620\u5c04\uff0c\u4ee5\u63d0\u5347\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u8868\u793a\u7684\u7406\u89e3\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u8868\u793a\u96be\u4ee5\u89e3\u91ca\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7c97\u7565\u8fd1\u4f3c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9ad8\u4fdd\u771f\u6269\u6563\u6a21\u578b\u6539\u5584\u7279\u5f81\u7a7a\u95f4\u5230\u8f93\u5165\u7a7a\u95f4\u7684\u6620\u5c04\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u9ad8\u4fdd\u771f\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u7a7a\u95f4\u5206\u8fa8\u7279\u5f81\u56fe\u4e3a\u6761\u4ef6\uff0c\u5b66\u4e60\u4ece\u7279\u5f81\u7a7a\u95f4\u5230\u8f93\u5165\u7a7a\u95f4\u7684\u6982\u7387\u6620\u5c04\u3002", "result": "\u5728\u591a\u79cd\u9884\u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668\uff08\u4eceCNN\u5230ViT\uff09\u4e0a\u5c55\u793a\u4e86\u51fa\u8272\u7684\u91cd\u5efa\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u6bd4\u8f83\u548c\u9c81\u68d2\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u7279\u5f81\u7a7a\u95f4\u7684\u53ef\u89c6\u5316\u548c\u5206\u6790\u3002", "relevance": 75.0}}
{"id": "2505.21039", "pdf": "https://arxiv.org/pdf/2505.21039", "abs": "https://arxiv.org/abs/2505.21039", "authors": ["Louis Allain", "S\u00e9bastien da Veiga", "Brian Staber"], "title": "Scalable and adaptive prediction bands with kernel sum-of-squares", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Conformal Prediction (CP) is a popular framework for constructing prediction\nbands with valid coverage in finite samples, while being free of any\ndistributional assumption. A well-known limitation of conformal prediction is\nthe lack of adaptivity, although several works introduced practically efficient\nalternate procedures. In this work, we build upon recent ideas that rely on\nrecasting the CP problem as a statistical learning problem, directly targeting\ncoverage and adaptivity. This statistical learning problem is based on\nreproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS)\nmethods. First, we extend previous results with a general representer theorem\nand exhibit the dual formulation of the learning problem. Crucially, such dual\nformulation can be solved efficiently by accelerated gradient methods with\nseveral hundreds or thousands of samples, unlike previous strategies based on\noff-the-shelf semidefinite programming algorithms. Second, we introduce a new\nhyperparameter tuning strategy tailored specifically to target adaptivity\nthrough bounds on test-conditional coverage. This strategy, based on the\nHilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune\nkernel lengthscales in our framework, but has broader applicability since it\ncould be used in any CP algorithm where the score function is learned. Finally,\nextensive experiments are conducted to show how our method compares to related\nwork. All figures can be reproduced with the accompanying code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u5b66\u4e60\u7684\u9002\u5e94\u6027\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528RKHS\u548c\u6838SoS\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7HSIC\u8c03\u4f18\u7b56\u7565\u63d0\u5347\u4e86\u6d4b\u8bd5\u6761\u4ef6\u8986\u76d6\u3002", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u8986\u76d6\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u57fa\u4e8eRKHS\u548c\u6838SoS\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u52a0\u901f\u68af\u5ea6\u6cd5\u9ad8\u6548\u6c42\u89e3\uff1b\u5f15\u5165HSIC\u8c03\u4f18\u7b56\u7565\u4f18\u5316\u6838\u957f\u5ea6\u5c3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u8986\u76d6\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5171\u5f62\u9884\u6d4b\u7684\u9002\u5e94\u6027\u548c\u8986\u76d6\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.20977", "pdf": "https://arxiv.org/pdf/2505.20977", "abs": "https://arxiv.org/abs/2505.20977", "authors": ["Yu Zhang", "Jinlong Ma", "Yongshuai Hou", "Xuefeng Bai", "Kehai Chen", "Yang Xiang", "Jun Yu", "Min Zhang"], "title": "Evaluating and Steering Modality Preferences in Multimodal Large Language Model", "categories": ["cs.CL"], "comment": "Modality Preference", "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a \\textbf{MC\\textsuperscript{2}}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6a21\u6001\u504f\u597d\u4e0a\u7684\u8868\u73b0\uff0c\u6784\u5efa\u4e86MC\u00b2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5747\u5b58\u5728\u6a21\u6001\u504f\u597d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u793a\u5de5\u7a0b\u7684\u8c03\u63a7\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u6001\u504f\u597d\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u63ed\u793a\u5176\u51b3\u7b56\u884c\u4e3a\u4e2d\u7684\u6f5c\u5728\u504f\u5dee\u3002", "method": "\u6784\u5efaMC\u00b2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f3018\u79cdMLLMs\u7684\u6a21\u6001\u504f\u597d\uff1b\u63d0\u51fa\u57fa\u4e8e\u8868\u793a\u5de5\u7a0b\u7684\u8c03\u63a7\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5747\u8868\u73b0\u51fa\u6a21\u6001\u504f\u597d\uff0c\u8c03\u63a7\u65b9\u6cd5\u80fd\u6709\u6548\u63a7\u5236\u504f\u597d\u65b9\u5411\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u6a21\u6001\u504f\u597d\u662fMLLMs\u7684\u666e\u904d\u73b0\u8c61\uff0c\u8868\u793a\u5de5\u7a0b\u65b9\u6cd5\u53ef\u6709\u6548\u8c03\u63a7\u504f\u597d\uff0c\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "relevance": 85.0}}
{"id": "2505.21036", "pdf": "https://arxiv.org/pdf/2505.21036", "abs": "https://arxiv.org/abs/2505.21036", "authors": ["Aiyue Chen", "Bin Dong", "Jingru Li", "Jing Lin", "Yiwu Yao", "Gongyi Wang"], "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video generation using diffusion models is highly computationally intensive,\nwith 3D attention in Diffusion Transformer (DiT) models accounting for over\n80\\% of the total computational resources. In this work, we introduce {\\bf\nRainFusion}, a novel training-free sparse attention method that exploits\ninherent sparsity nature in visual data to accelerate attention computation\nwhile preserving video quality. Specifically, we identify three unique sparse\npatterns in video generation attention calculations--Spatial Pattern, Temporal\nPattern and Textural Pattern. The sparse pattern for each attention head is\ndetermined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our\nproposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed\n{\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated\ninto state-of-the-art 3D-attention video generation models without additional\ntraining or calibration. We evaluate our method on leading open-sourced models\nincluding HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its\nbroad applicability and effectiveness. Experimental results show that\nRainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation\nwhile maintaining video quality, with only a minimal impact on VBench scores\n(-0.2\\%).", "AI": {"tldr": "RainFusion\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u578b\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u7a00\u758f\u6027\u52a0\u901f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f3D\u6ce8\u610f\u529b\u673a\u5236\u5360\u7528\u4e86\u5927\u91cf\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u63d0\u51faRainFusion\u65b9\u6cd5\uff0c\u8bc6\u522b\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e09\u79cd\u7a00\u758f\u6a21\u5f0f\uff08\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u7eb9\u7406\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u8bc6\u522b\u6a21\u5757\uff08ARM\uff09\u5728\u7ebf\u786e\u5b9a\u7a00\u758f\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cRainFusion\u5b9e\u73b0\u4e862\u500d\u4ee5\u4e0a\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u52a0\u901f\uff0c\u4e14\u89c6\u9891\u8d28\u91cf\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\uff08VBench\u5206\u6570\u4ec5\u4e0b\u964d0.2%\uff09\u3002", "conclusion": "RainFusion\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73b0\u67093D\u6ce8\u610f\u529b\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "relevance": 60.0}}
{"id": "2505.21046", "pdf": "https://arxiv.org/pdf/2505.21046", "abs": "https://arxiv.org/abs/2505.21046", "authors": ["Zhenling Chen", "Haiwei Fu", "Zhiguo Zeng"], "title": "A domain adaptation neural network for digital twin-supported fault diagnosis", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": "Preprint accepted by ICCAD 2025 at Barcelona", "summary": "Digital twins offer a promising solution to the lack of sufficient labeled\ndata in deep learning-based fault diagnosis by generating simulated data for\nmodel training. However, discrepancies between simulation and real-world\nsystems can lead to a significant drop in performance when models are applied\nin real scenarios. To address this issue, we propose a fault diagnosis\nframework based on Domain-Adversarial Neural Networks (DANN), which enables\nknowledge transfer from simulated (source domain) to real-world (target domain)\ndata. We evaluate the proposed framework using a publicly available robotics\nfault diagnosis dataset, which includes 3,600 sequences generated by a digital\ntwin model and 90 real sequences collected from physical systems. The DANN\nmethod is compared with commonly used lightweight deep learning models such as\nCNN, TCN, Transformer, and LSTM. Experimental results show that incorporating\ndomain adaptation significantly improves the diagnostic performance. For\nexample, applying DANN to a baseline CNN model improves its accuracy from\n70.00% to 80.22% on real-world test data, demonstrating the effectiveness of\ndomain adaptation in bridging the sim-to-real gap.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u7684\u6545\u969c\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u62df\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6a21\u62df\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ece\u6a21\u62df\u6570\u636e\uff08\u6e90\u57df\uff09\u5230\u771f\u5b9e\u6570\u636e\uff08\u76ee\u6807\u57df\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDANN\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\uff0c\u4f8b\u5982\u5c06CNN\u7684\u51c6\u786e\u7387\u4ece70.00%\u63d0\u5347\u81f380.22%\u3002", "conclusion": "\u57df\u9002\u5e94\u65b9\u6cd5\u80fd\u6709\u6548\u5f25\u5408\u6a21\u62df\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "relevance": 40.0}}
{"id": "2505.20993", "pdf": "https://arxiv.org/pdf/2505.20993", "abs": "https://arxiv.org/abs/2505.20993", "authors": ["Jie Shao", "Jianxin Wu"], "title": "Who Reasons in the Large Language Models?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), the\nprocess of endowing them with new capabilities--such as mathematical\nreasoning--remains largely empirical and opaque. A critical open question is\nwhether reasoning abilities stem from the entire model, specific modules, or\nare merely artifacts of overfitting. In this work, we hypothesize that the\nreasoning capabilities in well-trained LLMs are primarily attributed to the\noutput projection module (oproj) in the Transformer's multi-head self-attention\n(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for\nNetworks (SfN), a suite of diagnostic tools designed to probe and analyze the\ninternal behaviors of LLMs. Using SfN, we provide both circumstantial and\nempirical evidence suggesting that oproj plays a central role in enabling\nreasoning, whereas other modules contribute more to fluent dialogue. These\nfindings offer a new perspective on LLM interpretability and open avenues for\nmore targeted training strategies, potentially enabling more efficient and\nspecialized LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5047\u8bbe\uff0c\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u4e3b\u8981\u6e90\u4e8eTransformer\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u8f93\u51fa\u6295\u5f71\u6a21\u5757\uff08oproj\uff09\uff0c\u5e76\u901a\u8fc7\u8bca\u65ad\u5de5\u5177SfN\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u5047\u8bbe\u3002", "motivation": "\u63a2\u7d22LLMs\u63a8\u7406\u80fd\u529b\u7684\u6765\u6e90\uff0c\u4ee5\u89e3\u51b3\u5176\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4e0d\u900f\u660e\u6027\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u5de5\u5177SfN\uff0c\u5206\u6790LLMs\u5185\u90e8\u884c\u4e3a\uff0c\u9a8c\u8bc1oproj\u5728\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0oproj\u5728\u63a8\u7406\u4e2d\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u5176\u4ed6\u6a21\u5757\u66f4\u591a\u5f71\u54cd\u6d41\u7545\u5bf9\u8bdd\u3002", "conclusion": "\u4e3aLLMs\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u9488\u5bf9\u6027\u8bad\u7ec3\u7b56\u7565\u5f00\u8f9f\u9014\u5f84\u3002", "relevance": 85.0}}
{"id": "2505.21049", "pdf": "https://arxiv.org/pdf/2505.21049", "abs": "https://arxiv.org/abs/2505.21049", "authors": ["Dehao Wang", "Haohang Zhu", "Yiwen Xu", "Kaiqi Liu"], "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing", "categories": ["cs.CV"], "comment": null, "summary": "Road potholes pose a serious threat to driving safety and comfort, making\ntheir detection and assessment a critical task in fields such as autonomous\ndriving. When driving vehicles, the operators usually avoid large potholes and\napproach smaller ones at reduced speeds to ensure safety. Therefore, accurately\nestimating pothole area is of vital importance. Most existing vision-based\nmethods rely on distance priors to construct geometric models. However, their\nperformance is susceptible to variations in camera angles and typically relies\non the assumption of a flat road surface, potentially leading to significant\nerrors in complex real-world environments. To address these problems, a robust\npothole area estimation framework that integrates object detection and\nmonocular depth estimation in a video stream is proposed in this paper. First,\nto enhance pothole feature extraction and improve the detection of small\npotholes, ACSH-YOLOv8 is proposed with ACmix module and the small object\ndetection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,\nwhile DepthAnything V2 generates depth maps for each frame. With the obtained\ndepth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel\n(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter\nbased on Confidence and Distance (CDKF) is developed to maintain consistency of\nestimation results across consecutive frames. The results show that ACSH-YOLOv8\nmodel achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.\nThrough CDKF optimization across consecutive frames, pothole predictions become\nmore robust, thereby enhancing the method's practical applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u5751\u6d1e\u9762\u79ef\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684ACSH-YOLOv8\u6a21\u578b\u548cMBTP\u65b9\u6cd5\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5751\u6d1e\u5bf9\u9a7e\u9a76\u5b89\u5168\u548c\u8212\u9002\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u6613\u53d7\u76f8\u673a\u89d2\u5ea6\u548c\u8def\u9762\u5e73\u5766\u5047\u8bbe\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bef\u5dee\u3002", "method": "\u63d0\u51faACSH-YOLOv8\u6a21\u578b\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408BoT-SORT\u8ddf\u8e2a\u548cDepthAnything V2\u751f\u6210\u6df1\u5ea6\u56fe\uff0c\u4f7f\u7528MBTP\u65b9\u6cd5\u4f30\u8ba1\u9762\u79ef\uff0c\u5e76\u901a\u8fc7CDKF\u4f18\u5316\u7ed3\u679c\u3002", "result": "ACSH-YOLOv8\u7684AP(50)\u8fbe\u523076.6%\uff0c\u6bd4YOLOv8\u63d0\u53477.6%\uff1bCDKF\u4f18\u5316\u540e\u9884\u6d4b\u66f4\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u5751\u6d1e\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002", "relevance": 10.0}}
{"id": "2505.21073", "pdf": "https://arxiv.org/pdf/2505.21073", "abs": "https://arxiv.org/abs/2505.21073", "authors": ["Pierre Houedry", "Nicolas Courty", "Florestan Martin-Baillon", "Laetitia Chapel", "Titouan Vayer"], "title": "Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Trees and the associated shortest-path tree metrics provide a powerful\nframework for representing hierarchical and combinatorial structures in data.\nGiven an arbitrary metric space, its deviation from a tree metric can be\nquantified by Gromov's $\\delta$-hyperbolicity. Nonetheless, designing\nalgorithms that bridge an arbitrary metric to its closest tree metric is still\na vivid subject of interest, as most common approaches are either heuristical\nand lack guarantees, or perform moderately well. In this work, we introduce a\nnovel differentiable optimization framework, coined DeltaZero, that solves this\nproblem. Our method leverages a smooth surrogate for Gromov's\n$\\delta$-hyperbolicity which enables a gradient-based optimization, with a\ntractable complexity. The corresponding optimization procedure is derived from\na problem with better worst case guarantees than existing bounds, and is\njustified statistically. Experiments on synthetic and real-world datasets\ndemonstrate that our method consistently achieves state-of-the-art distortion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeltaZero\u7684\u53ef\u5fae\u5206\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u4efb\u610f\u5ea6\u91cf\u7a7a\u95f4\u903c\u8fd1\u5230\u6700\u8fd1\u7684\u6811\u5ea6\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u91cf\u5316\u4efb\u610f\u5ea6\u91cf\u7a7a\u95f4\u4e0e\u6811\u5ea6\u91cf\u7684\u504f\u5dee\uff0c\u5e76\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5e73\u6ed1\u7684\u66ff\u4ee3\u51fd\u6570\uff08Gromov\u7684\u03b4-\u53cc\u66f2\u6027\uff09\u6765\u5b9e\u73b0\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff0c\u5e76\u63a8\u5bfc\u51fa\u5177\u6709\u66f4\u597d\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeltaZero\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5931\u771f\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u662fDeltaZero\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6301\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5ea6\u91cf\u7a7a\u95f4\u5230\u6811\u5ea6\u91cf\u7684\u903c\u8fd1\u6548\u679c\u3002", "relevance": 40.0}}
{"id": "2505.20995", "pdf": "https://arxiv.org/pdf/2505.20995", "abs": "https://arxiv.org/abs/2505.20995", "authors": ["Justin J. H. Lo", "Patrycja Strycharczuk", "Sam Kirkham"], "title": "Articulatory strategy in vowel production as a basis for speaker discrimination", "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "The way speakers articulate is well known to be variable across individuals\nwhile at the same time subject to anatomical and biomechanical constraints. In\nthis study, we ask whether articulatory strategy in vowel production can be\nsufficiently speaker-specific to form the basis for speaker discrimination. We\nconducted Generalised Procrustes Analyses of tongue shape data from 40 English\nspeakers from the North West of England, and assessed the\nspeaker-discriminatory potential of orthogonal tongue shape features within the\nframework of likelihood ratios. Tongue size emerged as the individual dimension\nwith the strongest discriminatory power, while tongue shape variation in the\nmore anterior part of the tongue generally outperformed tongue shape variation\nin the posterior part. When considered in combination, shape-only information\nmay offer comparable levels of speaker specificity to size-and-shape\ninformation, but only when features do not exhibit speaker-level co-variation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bf4\u8bdd\u8005\u5728\u5143\u97f3\u53d1\u97f3\u4e2d\u7684\u820c\u5f62\u7279\u5f81\u662f\u5426\u8db3\u4ee5\u7528\u4e8e\u8bf4\u8bdd\u8005\u8bc6\u522b\uff0c\u53d1\u73b0\u820c\u5f62\u5927\u5c0f\u662f\u6700\u5177\u533a\u5206\u529b\u7684\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u4e2a\u4f53\u53d1\u97f3\u7b56\u7565\u7684\u5dee\u5f02\u6027\u662f\u5426\u53ef\u7528\u4e8e\u8bf4\u8bdd\u8005\u8bc6\u522b\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u548c\u751f\u7269\u529b\u5b66\u9650\u5236\u3002", "method": "\u5bf940\u540d\u82f1\u8bed\u8bf4\u8bdd\u8005\u7684\u820c\u5f62\u6570\u636e\u8fdb\u884c\u5e7f\u4e49Procrustes\u5206\u6790\uff0c\u5229\u7528\u4f3c\u7136\u6bd4\u6846\u67b6\u8bc4\u4f30\u820c\u5f62\u7279\u5f81\u7684\u533a\u5206\u80fd\u529b\u3002", "result": "\u820c\u5f62\u5927\u5c0f\u662f\u6700\u5177\u533a\u5206\u529b\u7684\u7279\u5f81\uff0c\u524d\u90e8\u820c\u5f62\u53d8\u5316\u4f18\u4e8e\u540e\u90e8\u3002\u5f62\u72b6\u4fe1\u606f\u5728\u65e0\u534f\u53d8\u65f6\u53ef\u4e0e\u5927\u5c0f\u548c\u5f62\u72b6\u4fe1\u606f\u5ab2\u7f8e\u3002", "conclusion": "\u820c\u5f62\u7279\u5f81\u53ef\u7528\u4e8e\u8bf4\u8bdd\u8005\u8bc6\u522b\uff0c\u4f46\u9700\u6ce8\u610f\u7279\u5f81\u95f4\u7684\u534f\u53d8\u5f71\u54cd\u3002", "relevance": 30.0}}
{"id": "2505.21050", "pdf": "https://arxiv.org/pdf/2505.21050", "abs": "https://arxiv.org/abs/2505.21050", "authors": ["Xin Yang", "Jiantao Lin", "Yingjie Xu", "Haodong Li", "Yingcong Chen"], "title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents", "categories": ["cs.CV"], "comment": null, "summary": "Despite the availability of large-scale 3D datasets and advancements in 3D\ngenerative models, the complexity and uneven quality of 3D geometry and texture\ndata continue to hinder the performance of 3D generation techniques. In most\nexisting approaches, 3D geometry and texture are generated in separate stages\nusing different models and non-unified representations, frequently leading to\nunsatisfactory coherence between geometry and texture. To address these\nchallenges, we propose a novel framework for joint generation of 3D geometry\nand texture. Specifically, we focus in generate a versatile 2.5D\nrepresentations that can be seamlessly transformed between 2D and 3D. Our\napproach begins by integrating multiview RGB, normal, and coordinate images\ninto a unified representation, termed as 2.5D latents. Next, we adapt\npre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing\nboth text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D\nrefiner-decoder framework that efficiently generates detailed 3D\nrepresentations from 2.5D images. Extensive experiments demonstrate that our\nmodel not only excels in generating high-quality 3D objects with coherent\nstructure and color from text and image inputs but also significantly\noutperforms existing methods in geometry-conditioned texture generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u751f\u62103D\u51e0\u4f55\u548c\u7eb9\u7406\u7684\u65b0\u6846\u67b6\uff0c\u5229\u75282.5D\u6f5c\u5728\u8868\u793a\u548c\u9884\u8bad\u7ec3\u76842D\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d3D\u51e0\u4f55\u548c\u7eb9\u7406\u751f\u6210\u5206\u79bb\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u8fde\u8d2f\uff0c\u9700\u8981\u7edf\u4e00\u8868\u793a\u548c\u9ad8\u6548\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u6574\u5408\u591a\u89c6\u56feRGB\u3001\u6cd5\u7ebf\u548c\u5750\u6807\u56fe\u50cf\u4e3a2.5D\u6f5c\u5728\u8868\u793a\uff0c\u5229\u7528\u9884\u8bad\u7ec32D\u57fa\u7840\u6a21\u578b\u751f\u62102.5D\u56fe\u50cf\uff0c\u518d\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u751f\u62103D\u8868\u793a\u3002", "result": "\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf3D\u5bf9\u8c61\u548c\u51e0\u4f55\u6761\u4ef6\u7eb9\u7406\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u57283D\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u51e0\u4f55\u548c\u7eb9\u7406\u7684\u8fde\u8d2f\u6027\uff0c\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "relevance": 30.0}}
{"id": "2505.21074", "pdf": "https://arxiv.org/pdf/2505.21074", "abs": "https://arxiv.org/abs/2505.21074", "authors": ["Yichuan Cao", "Yibo Miao", "Xiao-Shan Gao", "Yinpeng Dong"], "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "stat.ML"], "comment": null, "summary": "Text-to-image (T2I) models raise ethical and safety concerns due to their\npotential to generate inappropriate or harmful images. Evaluating these models'\nsecurity through red-teaming is vital, yet white-box approaches are limited by\ntheir need for internal access, complicating their use with closed-source\nmodels. Moreover, existing black-box methods often assume knowledge about the\nmodel's specific defense mechanisms, limiting their utility in real-world\ncommercial API scenarios. A significant challenge is how to evade unknown and\ndiverse defense mechanisms. To overcome this difficulty, we propose a novel\nRule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively\nemploys LLM to modify prompts to query and leverages feedback from T2I systems\nfor fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a\nprior, enabling the LLM to dynamically adapt to unknown defense mechanisms.\nGiven that the feedback is often labeled and coarse-grained, making it\ndifficult to utilize directly, we further propose rule-based preference\nmodeling, which employs a set of rules to evaluate desired or undesired\nfeedback, facilitating finer-grained control over the LLM's dynamic adaptation\nprocess. Extensive experiments on nineteen T2I systems with varied safety\nmechanisms, three online commercial API services, and T2V models verify the\nsuperiority and practicality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u504f\u597d\u5efa\u6a21\u7684\u5f15\u5bfc\u7ea2\u961f\u65b9\u6cd5\uff08RPG-RT\uff09\uff0c\u5229\u7528LLM\u52a8\u6001\u9002\u5e94\u672a\u77e5\u9632\u5fa1\u673a\u5236\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5728\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b89\u5168\u6027\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u672a\u77e5\u9632\u5fa1\u673a\u5236\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u4f7f\u7528LLM\u4fee\u6539\u63d0\u793a\u5e76\u5229\u7528\u53cd\u9988\u8fdb\u884c\u5fae\u8c03\uff0c\u7ed3\u5408\u89c4\u5219\u504f\u597d\u5efa\u6a21\u7ec6\u5316\u63a7\u5236\u3002", "result": "\u572819\u4e2aT2I\u7cfb\u7edf\u30013\u4e2a\u5546\u4e1aAPI\u548cT2V\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "RPG-RT\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347T2I\u6a21\u578b\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.21003", "pdf": "https://arxiv.org/pdf/2505.21003", "abs": "https://arxiv.org/abs/2505.21003", "authors": ["Yifei Wang", "Yu Sheng", "Linjing Li", "Daniel Zeng"], "title": "Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?", "categories": ["cs.CL"], "comment": "Camera-ready versions for ACL 2025 Findings", "summary": "Recent advances in handling long sequences have facilitated the exploration\nof long-context in-context learning (ICL). While much of the existing research\nemphasizes performance improvements driven by additional in-context examples,\nthe influence on the trustworthiness of generated responses remains\nunderexplored. This paper addresses this gap by investigating how increased\nexamples influence predictive uncertainty, an essential aspect in\ntrustworthiness. We begin by systematically quantifying the uncertainty of ICL\nwith varying shot counts, analyzing the impact of example quantity. Through\nuncertainty decomposition, we introduce a novel perspective on performance\nenhancement, with a focus on epistemic uncertainty (EU). Our results reveal\nthat additional examples reduce total uncertainty in both simple and complex\ntasks by injecting task-specific knowledge, thereby diminishing EU and\nenhancing performance. For complex tasks, these advantages emerge only after\naddressing the increased noise and uncertainty associated with longer inputs.\nFinally, we explore the evolution of internal confidence across layers,\nunveiling the mechanisms driving the reduction in uncertainty.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u957f\u4e0a\u4e0b\u6587\u60c5\u5883\u5b66\u4e60\uff08ICL\uff09\u4e2d\u793a\u4f8b\u6570\u91cf\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u751f\u6210\u54cd\u5e94\u53ef\u4fe1\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u589e\u52a0\u793a\u4f8b\u80fd\u901a\u8fc7\u51cf\u5c11\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8ICL\u6027\u80fd\u63d0\u5347\uff0c\u800c\u793a\u4f8b\u6570\u91cf\u5bf9\u751f\u6210\u54cd\u5e94\u53ef\u4fe1\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u91cf\u5316\u4e0d\u540c\u793a\u4f8b\u6570\u91cf\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5206\u89e3\u4e0d\u786e\u5b9a\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff08EU\uff09\uff0c\u5e76\u5206\u6790\u5185\u90e8\u4fe1\u5fc3\u7684\u6f14\u53d8\u3002", "result": "\u589e\u52a0\u793a\u4f8b\u901a\u8fc7\u6ce8\u5165\u4efb\u52a1\u77e5\u8bc6\u51cf\u5c11\u603b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u6027\u80fd\uff1b\u590d\u6742\u4efb\u52a1\u9700\u5148\u89e3\u51b3\u8f93\u5165\u566a\u58f0\u95ee\u9898\u3002", "conclusion": "\u793a\u4f8b\u6570\u91cf\u901a\u8fc7\u51cf\u5c11\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u63d0\u5347ICL\u7684\u53ef\u4fe1\u5ea6\u548c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9700\u6ce8\u610f\u8f93\u5165\u566a\u58f0\u3002", "relevance": 85.0}}
{"id": "2505.21060", "pdf": "https://arxiv.org/pdf/2505.21060", "abs": "https://arxiv.org/abs/2505.21060", "authors": ["Peng Wang", "Xiang Liu", "Peidong Liu"], "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles", "categories": ["cs.CV"], "comment": "Project page: https://nickisdope.github.io/Styl3R", "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f3D\u573a\u666f\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u652f\u67b6\u6784\u5206\u79bb\u7ed3\u6784\u548c\u5916\u89c2\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u98ce\u683c\u5316\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u98ce\u683c\u5316\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u3001\u4f9d\u8d56\u5bc6\u96c6\u8f93\u5165\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u76843D\u98ce\u683c\u5316\u3002", "method": "\u4f7f\u7528\u5206\u652f\u67b6\u6784\u5206\u79bb\u7ed3\u6784\u548c\u5916\u89c2\u5efa\u6a21\uff0c\u7ed3\u5408\u8eab\u4efd\u635f\u5931\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u652f\u6301\u7a00\u758f\u8f93\u5165\u548c\u4efb\u610f\u98ce\u683c\u56fe\u50cf\u3002", "result": "\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u573a\u666f\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\uff0c\u9002\u7528\u4e8e3D\u573a\u666f\u98ce\u683c\u5316\u4efb\u52a1\u3002", "relevance": 30.0}}
{"id": "2505.21077", "pdf": "https://arxiv.org/pdf/2505.21077", "abs": "https://arxiv.org/abs/2505.21077", "authors": ["Mete Erdogan", "Francesco Tonin", "Volkan Cevher"], "title": "Efficient Large Language Model Inference with Neural Block Linearization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The high inference demands of transformer-based Large Language Models (LLMs)\npose substantial challenges in their deployment. To this end, we introduce\nNeural Block Linearization (NBL), a novel framework for accelerating\ntransformer model inference by replacing self-attention layers with linear\napproximations derived from Linear Minimum Mean Squared Error estimators. NBL\nleverages Canonical Correlation Analysis to compute a theoretical upper bound\non the approximation error. Then, we use this bound as a criterion for\nsubstitution, selecting the LLM layers with the lowest linearization error. NBL\ncan be efficiently applied to pre-trained LLMs without the need for\nfine-tuning. In experiments, NBL achieves notable computational speed-ups while\npreserving competitive accuracy on multiple reasoning benchmarks. For instance,\napplying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B\nincreases the inference speed by 32% with less than 1% accuracy trade-off,\nmaking it a flexible and promising solution to improve the inference efficiency\nof LLMs.", "AI": {"tldr": "NBL\u6846\u67b6\u901a\u8fc7\u7ebf\u6027\u8fd1\u4f3c\u66ff\u6362\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u52a0\u901fTransformer\u63a8\u7406\uff0c\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3Transformer LLMs\u63a8\u7406\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u5668\u8fd1\u4f3c\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u901a\u8fc7CCA\u8ba1\u7b97\u8bef\u5dee\u4e0a\u9650\u5e76\u9009\u62e9\u66ff\u6362\u5c42\u3002", "result": "\u63a8\u7406\u901f\u5ea6\u63d0\u534732%\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1%\u3002", "conclusion": "NBL\u662f\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u7684\u6709\u6548\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.20423", "pdf": "https://arxiv.org/pdf/2505.20423", "abs": "https://arxiv.org/abs/2505.20423", "authors": ["Julio de la Torre-Vanegas", "Miguel Soriano-Garcia", "Israel Becerra", "Diego Mercado-Ravell"], "title": "Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Landing safely in crowded urban environments remains an essential yet\nchallenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in\nemergency situations. In this work, we propose a risk-aware approach that\nharnesses semantic segmentation to continuously evaluate potential hazards in\nthe drone's field of view. By using a specialized deep neural network to assign\npixel-level risk values and applying an algorithm based on risk maps, our\nmethod adaptively identifies a stable Safe Landing Zone (SLZ) despite moving\ncritical obstacles such as vehicles, people, etc., and other visual challenges\nlike shifting illumination. A control system then guides the UAV toward this\nlow-risk region, employing altitude-dependent safety thresholds and temporal\nlanding point stabilization to ensure robust descent trajectories. Experimental\nvalidation in diverse urban environments demonstrates the effectiveness of our\napproach, achieving over 90% landing success rates in very challenging real\nscenarios, showing significant improvements in various risk metrics. Our\nfindings suggest that risk-oriented vision methods can effectively help reduce\nthe risk of accidents in emergency landing situations, particularly in complex,\nunstructured, urban scenarios, densely populated with moving risky obstacles,\nwhile potentiating the true capabilities of UAVs in complex urban operations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u98ce\u9669\u611f\u77e5\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5b89\u5168\u7740\u9646\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u98ce\u9669\u5e76\u5f15\u5bfc\u65e0\u4eba\u673a\u81f3\u5b89\u5168\u533a\u57df\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u62e5\u6324\u57ce\u5e02\u73af\u5883\u4e2d\u7d27\u6025\u7740\u9646\u65f6\u7684\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u9762\u5bf9\u79fb\u52a8\u969c\u788d\u7269\u548c\u89c6\u89c9\u5e72\u6270\u65f6\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u5206\u5272\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e3a\u50cf\u7d20\u7ea7\u98ce\u9669\u8d4b\u503c\uff0c\u7ed3\u5408\u98ce\u9669\u5730\u56fe\u7b97\u6cd5\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u52a8\u6001\u8bc6\u522b\u5b89\u5168\u7740\u9646\u533a\u5e76\u5f15\u5bfc\u65e0\u4eba\u673a\u3002", "result": "\u5728\u591a\u6837\u5316\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7740\u9646\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u98ce\u9669\u6307\u6807\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u98ce\u9669\u5bfc\u5411\u7684\u89c6\u89c9\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u7d27\u6025\u7740\u9646\u4e8b\u6545\u98ce\u9669\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "relevance": 30.0}}
{"id": "2505.21011", "pdf": "https://arxiv.org/pdf/2505.21011", "abs": "https://arxiv.org/abs/2505.21011", "authors": ["Liang Cheng", "Zhaowei Wang", "Mark Steedman"], "title": "LLMs are Frequency Pattern Learners in Natural Language Inference", "categories": ["cs.CL"], "comment": "9 pages", "summary": "While fine-tuning LLMs on NLI corpora improves their inferential performance,\nthe underlying mechanisms driving this improvement remain largely opaque. In\nthis work, we conduct a series of experiments to investigate what LLMs actually\nlearn during fine-tuning. We begin by analyzing predicate frequencies in\npremises and hypotheses across NLI datasets and identify a consistent frequency\nbias, where predicates in hypotheses occur more frequently than those in\npremises for positive instances. To assess the impact of this bias, we evaluate\nboth standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial\ncases. We find that LLMs exploit frequency bias for inference and perform\npoorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit\nsignificantly increased reliance on this bias, suggesting that they are\nlearning these frequency patterns from datasets. Finally, we compute the\nfrequencies of hyponyms and their corresponding hypernyms from WordNet,\nrevealing a correlation between frequency bias and textual entailment. These\nfindings help explain why learning frequency patterns can enhance model\nperformance on inference tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728NLI\u4efb\u52a1\u5fae\u8c03\u4e2d\u5b66\u4e60\u5230\u7684\u9891\u7387\u504f\u5dee\uff0c\u53d1\u73b0\u6a21\u578b\u4f9d\u8d56\u8fd9\u79cd\u504f\u5dee\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u5728\u5bf9\u6297\u6027\u6848\u4f8b\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7a76LLMs\u5728NLI\u4efb\u52a1\u5fae\u8c03\u4e2d\u5b9e\u9645\u5b66\u4e60\u5230\u7684\u673a\u5236\uff0c\u5c24\u5176\u662f\u9891\u7387\u504f\u5dee\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790NLI\u6570\u636e\u96c6\u4e2d\u8c13\u8bcd\u9891\u7387\uff0c\u8bc4\u4f30\u6807\u51c6\u4e0e\u5fae\u8c03LLMs\u5728\u504f\u5dee\u4e00\u81f4\u548c\u5bf9\u6297\u6027\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5229\u7528WordNet\u5206\u6790\u9891\u7387\u504f\u5dee\u4e0e\u6587\u672c\u8574\u542b\u7684\u5173\u7cfb\u3002", "result": "LLMs\u4f9d\u8d56\u9891\u7387\u504f\u5dee\u8fdb\u884c\u63a8\u7406\uff0c\u5fae\u8c03\u540e\u66f4\u663e\u8457\uff1b\u9891\u7387\u504f\u5dee\u4e0e\u6587\u672c\u8574\u542b\u76f8\u5173\u3002", "conclusion": "\u9891\u7387\u504f\u5dee\u7684\u5b66\u4e60\u89e3\u91ca\u4e86LLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u7684\u539f\u56e0\u3002", "relevance": 85.0}}
{"id": "2505.21061", "pdf": "https://arxiv.org/pdf/2505.21061", "abs": "https://arxiv.org/abs/2505.21061", "authors": ["Fatemeh Pesaran Zadeh", "Yoojin Oh", "Gunhee Kim"], "title": "LPOI: Listwise Preference Optimization for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "ACL 2025 Main. Code is released at\n  https://github.com/fatemehpesaran310/lpoi", "summary": "Aligning large VLMs with human preferences is a challenging task, as methods\nlike RLHF and DPO often overfit to textual information or exacerbate\nhallucinations. Although augmenting negative image samples partially addresses\nthese pitfalls, no prior work has employed listwise preference optimization for\nVLMs, due to the complexity and cost of constructing listwise image samples. In\nthis work, we propose LPOI, the first object-aware listwise preference\noptimization developed for reducing hallucinations in VLMs. LPOI identifies and\nmasks a critical object in the image, and then interpolates the masked region\nbetween the positive and negative images to form a sequence of incrementally\nmore complete images. The model is trained to rank these images in ascending\norder of object visibility, effectively reducing hallucinations while retaining\nvisual fidelity. LPOI requires no extra annotations beyond standard pairwise\npreference data, as it automatically constructs the ranked lists through object\nmasking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and\nObject HalBench confirm that LPOI outperforms existing preference optimization\nmethods in reducing hallucinations and enhancing VLM performance. We make the\ncode available at https://github.com/fatemehpesaran310/lpoi.", "AI": {"tldr": "LPOI\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u5bf9\u8c61\u611f\u77e5\u5217\u8868\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u548c\u63d2\u503c\u6280\u672f\u51cf\u5c11\u5e7b\u89c9\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982RLHF\u548cDPO\u5728VLMs\u4e2d\u5bb9\u6613\u8fc7\u62df\u5408\u6216\u52a0\u5267\u5e7b\u89c9\uff0c\u800cLPOI\u65e8\u5728\u901a\u8fc7\u5217\u8868\u504f\u597d\u4f18\u5316\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "LPOI\u901a\u8fc7\u63a9\u7801\u5173\u952e\u5bf9\u8c61\u5e76\u5728\u6b63\u8d1f\u6837\u672c\u95f4\u63d2\u503c\u751f\u6210\u6e10\u8fdb\u5b8c\u6574\u7684\u56fe\u50cf\u5e8f\u5217\uff0c\u8bad\u7ec3\u6a21\u578b\u6309\u5bf9\u8c61\u53ef\u89c1\u6027\u6392\u5e8f\u3002", "result": "\u5728MMHalBench\u3001AMBER\u548cObject HalBench\u4e0a\uff0cLPOI\u5728\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u5347VLM\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LPOI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u6807\u6ce8\u7684\u5217\u8868\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11VLMs\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "relevance": 85.0}}
{"id": "2505.21095", "pdf": "https://arxiv.org/pdf/2505.21095", "abs": "https://arxiv.org/abs/2505.21095", "authors": ["Kei Takemura", "Ryuta Matsuno", "Keita Sakuma"], "title": "Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations", "categories": ["cs.LG"], "comment": null, "summary": "A central goal in online learning is to achieve adaptivity to unknown problem\ncharacteristics, such as environmental changes captured by gradient variation\n(GV), function curvature (universal online learning, UOL), and gradient scales\n(Lipschitz adaptivity, LA). Simultaneously achieving these with optimal\nperformance is a major challenge, partly due to limitations in algorithms for\nprediction with expert advice. These algorithms often serve as meta-algorithms\nin online ensemble frameworks, and their sub-optimality hinders overall UOL\nperformance. Specifically, existing algorithms addressing the ``impossible\ntuning'' issue incur an excess $\\sqrt{\\log T}$ factor in their regret bound\ncompared to the lower bound. To solve this problem, we propose a novel\noptimistic online mirror descent algorithm with an auxiliary initial round\nusing large learning rates. This design enables a refined analysis where a\ngenerated negative term cancels the gap-related factor, resolving the\nimpossible tuning issue up to $\\log\\log T$ factors. Leveraging our improved\nalgorithm as a meta-algorithm, we develop the first UOL algorithm that\nsimultaneously achieves state-of-the-art GV bounds and LA under standard\nassumptions. Our UOL result overcomes key limitations of prior works, notably\nresolving the conflict between LA mechanisms and regret analysis for GV bounds\n-- an open problem highlighted by Xie et al.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e50\u89c2\u5728\u7ebf\u955c\u50cf\u4e0b\u964d\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u5b66\u4e60\u4e2d\u540c\u65f6\u9002\u5e94\u68af\u5ea6\u53d8\u5316\u3001\u51fd\u6570\u66f2\u7387\u548c\u68af\u5ea6\u5c3a\u5ea6\u7684\u6311\u6218\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u9700\u8981\u9002\u5e94\u672a\u77e5\u95ee\u9898\u7279\u6027\uff08\u5982\u68af\u5ea6\u53d8\u5316\u3001\u51fd\u6570\u66f2\u7387\u548c\u68af\u5ea6\u5c3a\u5ea6\uff09\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u56e0\u6b21\u4f18\u6027\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e50\u89c2\u5728\u7ebf\u955c\u50cf\u4e0b\u964d\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u521d\u59cb\u8f6e\u6b21\u548c\u5927\u5b66\u4e60\u7387\uff0c\u4f18\u5316\u5206\u6790\u5e76\u6d88\u9664\u989d\u5916\u56e0\u5b50\u3002", "result": "\u65b0\u7b97\u6cd5\u9996\u6b21\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u4f18\u68af\u5ea6\u53d8\u5316\u548cLipschitz\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u5173\u952e\u9650\u5236\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u89e3\u51b3\u4e86\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u4e0d\u53ef\u80fd\u8c03\u4f18\u95ee\u9898\uff0c\u4e3a\u901a\u7528\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.20424", "pdf": "https://arxiv.org/pdf/2505.20424", "abs": "https://arxiv.org/abs/2505.20424", "authors": ["Jian Zhang", "Hanbo Zhang", "Anxing Xiao", "David Hsu"], "title": "Robot Operation of Home Appliances by Reading User Manuals", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Operating home appliances, among the most common tools in every household, is\na critical capability for assistive home robots. This paper presents ApBot, a\nrobot system that operates novel household appliances by \"reading\" their user\nmanuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual\ndocument, (ii) ground the policies to the appliance in the physical world, and\n(iii) execute the policies reliably over potentially many steps, despite\ncompounding errors. To tackle these challenges, ApBot constructs a structured,\nsymbolic model of an appliance from its manual, with the help of a large\nvision-language model (VLM). It grounds the symbolic actions visually to\ncontrol panel elements. Finally, ApBot closes the loop by updating the model\nbased on visual feedback. Our experiments show that across a wide range of\nsimulated and real-world appliances, ApBot achieves consistent and\nstatistically significant improvements in task success rate, compared with\nstate-of-the-art large VLMs used directly as control policies. These results\nsuggest that a structured internal representations plays an important role in\nrobust robot operation of home appliances, especially, complex ones.", "AI": {"tldr": "ApBot\u662f\u4e00\u4e2a\u901a\u8fc7\u9605\u8bfb\u7528\u6237\u624b\u518c\u64cd\u4f5c\u5bb6\u7535\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6784\u5efa\u7ed3\u6784\u5316\u7b26\u53f7\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u66f4\u65b0\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5bb6\u5ead\u673a\u5668\u4eba\u9700\u8981\u80fd\u591f\u64cd\u4f5c\u5404\u79cd\u5bb6\u7535\uff0c\u4f46\u76f4\u63a5\u4ece\u7528\u6237\u624b\u518c\u4e2d\u63d0\u53d6\u7b56\u7565\u5e76\u6267\u884c\u5b58\u5728\u6311\u6218\u3002", "method": "ApBot\u901a\u8fc7VLM\u4ece\u7528\u6237\u624b\u518c\u4e2d\u6784\u5efa\u7ed3\u6784\u5316\u7b26\u53f7\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cApBot\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5bb6\u7535\u4e0a\u7684\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528VLM\u7684\u7b56\u7565\u3002", "conclusion": "\u7ed3\u6784\u5316\u5185\u90e8\u8868\u793a\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u590d\u6742\u5bb6\u7535\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "relevance": 40.0}}
{"id": "2505.21033", "pdf": "https://arxiv.org/pdf/2505.21033", "abs": "https://arxiv.org/abs/2505.21033", "authors": ["Seungmin Lee", "Yongsang Yoo", "Minhwa Jung", "Min Song"], "title": "Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation", "categories": ["cs.CL"], "comment": "19 pages, 3 figures, Accepted to Findings of the ACL 2025", "summary": "Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent\nsegments. DTS plays a crucial role in various NLP downstream tasks, but suffers\nfrom chronic problems: data shortage, labeling ambiguity, and incremental\ncomplexity of recently proposed solutions. On the other hand, Despite advances\nin Large Language Models (LLMs) and reasoning strategies, these have rarely\nbeen applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for\nOpen-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step\ndeductive reasoning to enhance DTS performance and enable case study using\nintermediate result. Our method employs a structured prompting approach for\nbidirectional context summarization, utterance intent classification, and\ndeductive topic shift detection. In the intent classification process, we\npropose the generalizable intent list for domain-agnostic dialogue intent\nclassification. Experiments in various dialogue settings demonstrate that\nDef-DTS consistently outperforms traditional and state-of-the-art approaches,\nwith each subtask contributing to improved performance, particularly in\nreducing type 2 error. We also explore the potential for autolabeling,\nemphasizing the importance of LLM reasoning techniques in DTS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDef-DTS\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u591a\u6b65\u6f14\u7ece\u63a8\u7406\u63d0\u5347\u5bf9\u8bdd\u4e3b\u9898\u5206\u5272\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5b9e\u73b0\u4e0a\u4e0b\u6587\u6458\u8981\u3001\u610f\u56fe\u5206\u7c7b\u548c\u4e3b\u9898\u8f6c\u79fb\u68c0\u6d4b\u3002", "motivation": "\u5bf9\u8bdd\u4e3b\u9898\u5206\u5272\uff08DTS\uff09\u5728NLP\u4e0b\u6e38\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u6570\u636e\u77ed\u7f3a\u3001\u6807\u6ce8\u6a21\u7cca\u548c\u89e3\u51b3\u65b9\u6848\u590d\u6742\u6027\u7b49\u95ee\u9898\u3002\u5c3d\u7ba1LLM\u548c\u63a8\u7406\u7b56\u7565\u6709\u8fdb\u5c55\uff0c\u4f46\u5f88\u5c11\u5e94\u7528\u4e8eDTS\u3002", "method": "\u91c7\u7528LLM\u7684\u591a\u6b65\u6f14\u7ece\u63a8\u7406\uff0c\u5305\u62ec\u53cc\u5411\u4e0a\u4e0b\u6587\u6458\u8981\u3001\u610f\u56fe\u5206\u7c7b\u548c\u4e3b\u9898\u8f6c\u79fb\u68c0\u6d4b\uff0c\u5e76\u63d0\u51fa\u901a\u7528\u610f\u56fe\u5217\u8868\u4ee5\u5b9e\u73b0\u9886\u57df\u65e0\u5173\u7684\u610f\u56fe\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDef-DTS\u4f18\u4e8e\u4f20\u7edf\u548c\u6700\u65b0\u65b9\u6cd5\uff0c\u5404\u5b50\u4efb\u52a1\u5747\u63d0\u5347\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u7b2c\u4e8c\u7c7b\u9519\u8bef\uff0c\u5e76\u63a2\u7d22\u4e86\u81ea\u52a8\u6807\u6ce8\u7684\u6f5c\u529b\u3002", "conclusion": "LLM\u63a8\u7406\u6280\u672f\u5728DTS\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cDef-DTS\u4e3a\u6027\u80fd\u63d0\u5347\u548c\u81ea\u52a8\u6807\u6ce8\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 60.0}}
{"id": "2505.21062", "pdf": "https://arxiv.org/pdf/2505.21062", "abs": "https://arxiv.org/abs/2505.21062", "authors": ["Davide Lobba", "Fulvio Sanguigni", "Bin Ren", "Marcella Cornia", "Rita Cucchiara", "Nicu Sebe"], "title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals", "categories": ["cs.CV"], "comment": null, "summary": "While virtual try-on (VTON) systems aim to render a garment onto a target\nperson image, this paper tackles the novel task of virtual try-off (VTOFF),\nwhich addresses the inverse problem: generating standardized product images of\ngarments from real-world photos of clothed individuals. Unlike VTON, which must\nresolve diverse pose and style variations, VTOFF benefits from a consistent and\nwell-defined output format -- typically a flat, lay-down-style representation\nof the garment -- making it a promising tool for data generation and dataset\nenhancement. However, existing VTOFF approaches face two major limitations: (i)\ndifficulty in disentangling garment features from occlusions and complex poses,\noften leading to visual artifacts, and (ii) restricted applicability to\nsingle-category garments (e.g., upper-body clothes only), limiting\ngeneralization. To address these challenges, we present Text-Enhanced\nMUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a\ndual DiT-based backbone with a modified multimodal attention mechanism for\nrobust garment feature extraction. Our architecture is designed to receive\ngarment information from multiple modalities like images, text, and masks to\nwork in a multi-category setting. Finally, we propose an additional alignment\nmodule to further refine the generated visual details. Experiments on VITON-HD\nand Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the\nVTOFF task, significantly improving both visual quality and fidelity to the\ntarget garments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEMU-VTOFF\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u865a\u62df\u8bd5\u8131\uff08VTOFF\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u548c\u53ccDiT\u9aa8\u5e72\u7f51\u7edc\u63d0\u5347\u670d\u88c5\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u865a\u62df\u8bd5\u8131\uff08VTOFF\uff09\u4efb\u52a1\u65e8\u5728\u4ece\u7a7f\u7740\u670d\u88c5\u7684\u4e2a\u4f53\u7167\u7247\u4e2d\u751f\u6210\u6807\u51c6\u5316\u7684\u670d\u88c5\u4ea7\u54c1\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u79bb\u670d\u88c5\u7279\u5f81\u548c\u9002\u7528\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faTEMU-VTOFF\u67b6\u6784\uff0c\u91c7\u7528\u53ccDiT\u9aa8\u5e72\u7f51\u7edc\u548c\u6539\u8fdb\u7684\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u652f\u6301\u56fe\u50cf\u3001\u6587\u672c\u548c\u63a9\u7801\u7b49\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5e76\u5f15\u5165\u5bf9\u9f50\u6a21\u5757\u4f18\u5316\u751f\u6210\u7ec6\u8282\u3002", "result": "\u5728VITON-HD\u548cDress Code\u6570\u636e\u96c6\u4e0a\uff0cTEMU-VTOFF\u5728VTOFF\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002", "conclusion": "TEMU-VTOFF\u901a\u8fc7\u591a\u6a21\u6001\u548c\u4f18\u5316\u67b6\u6784\u89e3\u51b3\u4e86VTOFF\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u6570\u636e\u751f\u6210\u548c\u6570\u636e\u96c6\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.21101", "pdf": "https://arxiv.org/pdf/2505.21101", "abs": "https://arxiv.org/abs/2505.21101", "authors": ["Badr Moufad", "Yazid Janati", "Alain Durmus", "Ahmed Ghorbel", "Eric Moulines", "Jimmy Olsson"], "title": "Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance", "categories": ["cs.LG", "stat.ME"], "comment": "preprint", "summary": "Classifier-Free Guidance (CFG) is a widely used technique for improving\nconditional diffusion models by linearly combining the outputs of conditional\nand unconditional denoisers. While CFG enhances visual quality and improves\nalignment with prompts, it often reduces sample diversity, leading to a\nchallenging trade-off between quality and diversity. To address this issue, we\nmake two key contributions. First, CFG generally does not correspond to a\nwell-defined denoising diffusion model (DDM). In particular, contrary to common\nintuition, CFG does not yield samples from the target distribution associated\nwith the limiting CFG score as the noise level approaches zero -- where the\ndata distribution is tilted by a power $w \\gt 1$ of the conditional\ndistribution. We identify the missing component: a R\\'enyi divergence term that\nacts as a repulsive force and is required to correct CFG and render it\nconsistent with a proper DDM. Our analysis shows that this correction term\nvanishes in the low-noise limit. Second, motivated by this insight, we propose\na Gibbs-like sampling procedure to draw samples from the desired tilted\ndistribution. This method starts with an initial sample from the conditional\ndiffusion model without CFG and iteratively refines it, preserving diversity\nwhile progressively enhancing sample quality. We evaluate our approach on both\nimage and text-to-audio generation tasks, demonstrating substantial\nimprovements over CFG across all considered metrics. The code is available at\nhttps://github.com/yazidjanati/cfgig", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u6b63Classifier-Free Guidance (CFG)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165R\u00e9nyi\u6563\u5ea6\u9879\u548cGibbs\u91c7\u6837\uff0c\u89e3\u51b3\u4e86CFG\u5728\u63d0\u5347\u6837\u672c\u8d28\u91cf\u65f6\u727a\u7272\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "CFG\u5728\u63d0\u5347\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6027\u80fd\u65f6\uff0c\u5e38\u5bfc\u81f4\u6837\u672c\u591a\u6837\u6027\u4e0b\u964d\uff0c\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u5206\u6790CFG\u7684\u6570\u5b66\u6027\u8d28\uff0c\u53d1\u73b0\u5176\u7f3a\u5c11R\u00e9nyi\u6563\u5ea6\u9879\uff0c\u63d0\u51faGibbs\u91c7\u6837\u65b9\u6cd5\u4ee5\u4fee\u6b63CFG\uff0c\u4fdd\u6301\u591a\u6837\u6027\u540c\u65f6\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u5728\u56fe\u50cf\u548c\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCFG\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CFG\u7684\u5c40\u9650\u6027\u3002", "relevance": 60.0}}
{"id": "2505.21040", "pdf": "https://arxiv.org/pdf/2505.21040", "abs": "https://arxiv.org/abs/2505.21040", "authors": ["Wei Chen", "Zhao Zhang", "Meng Yuan", "Kepeng Xu", "Fuzhen Zhuang"], "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 6 figures", "summary": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u8de8\u4efb\u52a1\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6FCKT\uff0c\u7528\u4e8e\u76ee\u6807\u60c5\u611f\u5206\u6790\uff08TSA\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u65b9\u9762\u7ea7\u4fe1\u606f\u6539\u8fdb\u60c5\u611f\u9884\u6d4b\uff0c\u51cf\u5c11\u8d1f\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u60c5\u611f\u5206\u6790\u4e2d\u591a\u91c7\u7528\u7c97\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5ffd\u7565\u4e86\u65b9\u9762-\u60c5\u611f\u5173\u7cfb\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faFCKT\u6846\u67b6\uff0c\u663e\u5f0f\u7ed3\u5408\u65b9\u9762\u7ea7\u4fe1\u606f\u8fdb\u884c\u60c5\u611f\u9884\u6d4b\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFCKT\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "conclusion": "FCKT\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u6709\u6548\u63d0\u5347\u4e86\u76ee\u6807\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21070", "pdf": "https://arxiv.org/pdf/2505.21070", "abs": "https://arxiv.org/abs/2505.21070", "authors": ["Zeqing Wang", "Bowen Zheng", "Xingyi Yang", "Yuecong Xu", "Xinchao Wang"], "title": "Minute-Long Videos with Dual Parallelisms", "categories": ["cs.CV"], "comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal", "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDualParal\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8eDiffusion Transformer\uff08DiT\uff09\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7684\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002\u901a\u8fc7\u5e76\u884c\u5316\u65f6\u95f4\u5e27\u548c\u6a21\u578b\u5c42\uff0c\u7ed3\u5408\u5757\u5f0f\u53bb\u566a\u548c\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684DiT\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u9762\u4e34\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faDualParal\u7b56\u7565\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u65f6\u95f4\u5e27\u548c\u6a21\u578b\u5c42\uff0c\u7ed3\u5408\u5757\u5f0f\u53bb\u566a\u3001\u7279\u5f81\u7f13\u5b58\u548c\u534f\u8c03\u566a\u58f0\u521d\u59cb\u5316\u6280\u672f\uff0c\u4f18\u5316\u5206\u5e03\u5f0f\u63a8\u7406\u3002", "result": "\u57288\u5757RTX 4090 GPU\u4e0a\uff0c\u5b9e\u73b0\u4e861,025\u5e27\u89c6\u9891\u751f\u6210\uff0c\u5ef6\u8fdf\u964d\u4f4e6.54\u500d\uff0c\u5185\u5b58\u6210\u672c\u964d\u4f4e1.48\u500d\u3002", "conclusion": "DualParal\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "relevance": 70.0}}
{"id": "2505.21119", "pdf": "https://arxiv.org/pdf/2505.21119", "abs": "https://arxiv.org/abs/2505.21119", "authors": ["Moritz A. Zanger", "Max Weltevrede", "Yaniv Oren", "Pascal R. Van der Vaart", "Caroline Horsch", "Wendelin B\u00f6hmer", "Matthijs T. J. Spaan"], "title": "Universal Value-Function Uncertainties", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Estimating epistemic uncertainty in value functions is a crucial challenge\nfor many aspects of reinforcement learning (RL), including efficient\nexploration, safe decision-making, and offline RL. While deep ensembles provide\na robust method for quantifying value uncertainty, they come with significant\ncomputational overhead. Single-model methods, while computationally favorable,\noften rely on heuristics and typically require additional propagation\nmechanisms for myopic uncertainty estimates. In this work we introduce\nuniversal value-function uncertainties (UVU), which, similar in spirit to\nrandom network distillation (RND), quantify uncertainty as squared prediction\nerrors between an online learner and a fixed, randomly initialized target\nnetwork. Unlike RND, UVU errors reflect policy-conditional value uncertainty,\nincorporating the future uncertainties any given policy may encounter. This is\ndue to the training procedure employed in UVU: the online network is trained\nusing temporal difference learning with a synthetic reward derived from the\nfixed, randomly initialized target network. We provide an extensive theoretical\nanalysis of our approach using neural tangent kernel (NTK) theory and show that\nin the limit of infinite network width, UVU errors are exactly equivalent to\nthe variance of an ensemble of independent universal value functions.\nEmpirically, we show that UVU achieves equal performance to large ensembles on\nchallenging multi-task offline RL settings, while offering simplicity and\nsubstantial computational savings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUVU\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u51fd\u6570\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u907f\u514d\u4e86\u6df1\u5ea6\u96c6\u6210\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u51fd\u6570\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6548\u63a2\u7d22\u3001\u5b89\u5168\u51b3\u7b56\u548c\u79bb\u7ebfRL\u4e2d\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5f15\u5165UVU\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u5668\u548c\u56fa\u5b9a\u968f\u673a\u521d\u59cb\u5316\u76ee\u6807\u7f51\u7edc\u4e4b\u95f4\u7684\u9884\u6d4b\u8bef\u5dee\u5e73\u65b9\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u7b56\u7565\u6761\u4ef6\u4ef7\u503c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eUVU\u5728\u65e0\u9650\u7f51\u7edc\u5bbd\u5ea6\u4e0b\u7b49\u540c\u4e8e\u72ec\u7acb\u901a\u7528\u4ef7\u503c\u51fd\u6570\u96c6\u6210\u7684\u65b9\u5dee\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4e0e\u5927\u578b\u96c6\u6210\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u66f4\u9ad8\u6548\u3002", "conclusion": "UVU\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u79bb\u7ebfRL\uff0c\u6027\u80fd\u4e0e\u6df1\u5ea6\u96c6\u6210\u76f8\u5f53\u3002", "relevance": 85.0}}
{"id": "2505.21043", "pdf": "https://arxiv.org/pdf/2505.21043", "abs": "https://arxiv.org/abs/2505.21043", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "title": "Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction", "categories": ["cs.CL", "cs.RO"], "comment": null, "summary": "Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)\nfacilitate naturalistic human-robot interaction, yet most rely solely on\nspeech. We introduce MM-VAP, a multimodal PTTM which combines speech with\nvisual cues including facial expression, head pose and gaze. We find that it\noutperforms the state-of-the-art audio-only in videoconferencing interactions\n(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which\naggregates all holds and shifts, we group by duration of silence between turns.\nThis reveals that through the inclusion of visual features, MM-VAP outperforms\na state-of-the-art audio-only turn-taking model across all durations of speaker\ntransitions. We conduct a detailed ablation study, which reveals that facial\nexpression features contribute the most to model performance. Thus, our working\nhypothesis is that when interlocutors can see one another, visual cues are\nvital for turn-taking and must therefore be included for accurate turn-taking\nprediction. We additionally validate the suitability of automatic speech\nalignment for PTTM training using telephone speech. This work represents the\nfirst comprehensive analysis of multimodal PTTMs. We discuss implications for\nfuture work and make all code publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u9884\u6d4b\u8f6e\u6362\u6a21\u578b\uff08MM-VAP\uff09\uff0c\u7ed3\u5408\u8bed\u97f3\u548c\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u9762\u90e8\u8868\u60c5\u3001\u5934\u90e8\u59ff\u52bf\u548c\u6ce8\u89c6\uff09\uff0c\u5728\u89c6\u9891\u4f1a\u8bae\u4ea4\u4e92\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ec5\u97f3\u9891\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u8f6e\u6362\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8bed\u97f3\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u7ebf\u7d22\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u9762\u4ea4\u4e92\u4e2d\u3002", "method": "MM-VAP\u7ed3\u5408\u8bed\u97f3\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6309\u6c89\u9ed8\u6301\u7eed\u65f6\u95f4\u5206\u7ec4\u5206\u6790\u8f6e\u6362\u884c\u4e3a\u3002", "result": "MM-VAP\u5728\u8f6e\u6362\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u4ec5\u97f3\u9891\u6a21\u578b\uff0884% vs. 79%\uff09\uff0c\u4e14\u89c6\u89c9\u7279\u5f81\u4e2d\u9762\u90e8\u8868\u60c5\u8d21\u732e\u6700\u5927\u3002", "conclusion": "\u89c6\u89c9\u7ebf\u7d22\u5bf9\u8f6e\u6362\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89c1\u4ea4\u4e92\u4e2d\u3002", "relevance": 40.0}}
{"id": "2505.21076", "pdf": "https://arxiv.org/pdf/2505.21076", "abs": "https://arxiv.org/abs/2505.21076", "authors": ["Weihao Xuan", "Junjue Wang", "Heli Qi", "Zihang Chen", "Zhuo Zheng", "Yanfei Zhong", "Junshi Xia", "Naoto Yokoya"], "title": "DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models have demonstrated remarkable capabilities in\nvisual understanding, but their application to long-term Earth observation\nanalysis remains limited, primarily focusing on single-temporal or bi-temporal\nimagery. To address this gap, we introduce DVL-Suite, a comprehensive framework\nfor analyzing long-term urban dynamics through remote sensing imagery. Our\nsuite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42\nmegacities in the U.S. from 2005 to 2023, organized into two components:\nDVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding\ntasks, from fundamental change detection (pixel-level) to quantitative analyses\n(regional-level) and comprehensive urban narratives (scene-level), capturing\ndiverse urban dynamics including expansion/transformation patterns, disaster\nassessment, and environmental challenges. We evaluate 17 state-of-the-art\nmultimodal large language models and reveal their limitations in long-term\ntemporal understanding and quantitative analysis. These challenges motivate the\ncreation of DVL-Instruct, a specialized instruction-tuning dataset designed to\nenhance models' capabilities in multi-temporal Earth observation. Building upon\nthis dataset, we develop DVLChat, a baseline model capable of both image-level\nquestion-answering and pixel-level segmentation, facilitating a comprehensive\nunderstanding of city dynamics through language interactions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DVL-Suite\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u9065\u611f\u5f71\u50cf\u5206\u6790\u957f\u671f\u57ce\u5e02\u52a8\u6001\uff0c\u5305\u62ecDVL-Bench\u548cDVL-Instruct\u4e24\u90e8\u5206\uff0c\u5e76\u5f00\u53d1\u4e86DVLChat\u6a21\u578b\u4ee5\u63d0\u5347\u591a\u65f6\u95f4\u5730\u7403\u89c2\u6d4b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u5730\u7403\u89c2\u6d4b\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5355\u65f6\u95f4\u6216\u53cc\u65f6\u95f4\u5f71\u50cf\u7684\u5e94\u7528\u95ee\u9898\u3002", "method": "\u63d0\u51faDVL-Suite\u6846\u67b6\uff0c\u5305\u542b15,063\u5f20\u9ad8\u5206\u8fa8\u7387\u591a\u65f6\u95f4\u5f71\u50cf\uff0c\u6db5\u76d67\u4e2a\u57ce\u5e02\u7406\u89e3\u4efb\u52a1\uff0c\u8bc4\u4f3017\u4e2a\u5148\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5f00\u53d1DVL-Instruct\u6570\u636e\u96c6\u548cDVLChat\u6a21\u578b\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u957f\u671f\u65f6\u95f4\u7406\u89e3\u548c\u5b9a\u91cf\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7DVLChat\u6a21\u578b\u5b9e\u73b0\u4e86\u56fe\u50cf\u7ea7\u95ee\u7b54\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u3002", "conclusion": "DVL-Suite\u548cDVLChat\u4e3a\u957f\u671f\u57ce\u5e02\u52a8\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7403\u89c2\u6d4b\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.21133", "pdf": "https://arxiv.org/pdf/2505.21133", "abs": "https://arxiv.org/abs/2505.21133", "authors": ["Marshal Arijona Sinaga", "Julien Martinelli", "Samuel Kaski"], "title": "Robust and Computation-Aware Gaussian Processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Gaussian processes (GPs) are widely used for regression and optimization\ntasks such as Bayesian optimization (BO) due to their expressiveness and\nprincipled uncertainty estimates. However, in settings with large datasets\ncorrupted by outliers, standard GPs and their sparse approximations struggle\nwith computational tractability and robustness. We introduce Robust\nComputation-aware Gaussian Process (RCaGP), a novel GP model that jointly\naddresses these challenges by combining a principled treatment of\napproximation-induced uncertainty with robust generalized Bayesian updating.\nThe key insight is that robustness and approximation-awareness are not\northogonal but intertwined: approximations can exacerbate the impact of\noutliers, and mitigating one without the other is insufficient. Unlike previous\nwork that focuses narrowly on either robustness or approximation quality, RCaGP\ncombines both in a principled and scalable framework, thus effectively managing\nboth outliers and computational uncertainties introduced by approximations such\nas low-rank matrix multiplications. Our model ensures more conservative and\nreliable uncertainty estimates, a property we rigorously demonstrate.\nAdditionally, we establish a robustness property and show that the mean\nfunction is key to preserving it, motivating a tailored model selection scheme\nfor robust mean functions. Empirical results confirm that solving these\nchallenges jointly leads to superior performance across both clean and\noutlier-contaminated settings, both on regression and high-throughput Bayesian\noptimization benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRCaGP\u7684\u65b0\u578b\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\uff0c\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u5927\u6570\u636e\u96c6\u4e2d\u5f02\u5e38\u503c\u548c\u8ba1\u7b97\u8fd1\u4f3c\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u9c81\u68d2\u6027\u548c\u8fd1\u4f3c\u611f\u77e5\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f30\u8ba1\u3002", "motivation": "\u6807\u51c6\u9ad8\u65af\u8fc7\u7a0b\u53ca\u5176\u7a00\u758f\u8fd1\u4f3c\u5728\u5927\u6570\u636e\u96c6\u548c\u5f02\u5e38\u503c\u5b58\u5728\u65f6\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0cRCaGP\u65e8\u5728\u8054\u5408\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "RCaGP\u7ed3\u5408\u4e86\u9c81\u68d2\u5e7f\u4e49\u8d1d\u53f6\u65af\u66f4\u65b0\u548c\u8fd1\u4f3c\u8bf1\u5bfc\u4e0d\u786e\u5b9a\u6027\u7684\u5904\u7406\uff0c\u91cd\u70b9\u5173\u6ce8\u9c81\u68d2\u6027\u548c\u8fd1\u4f3c\u8d28\u91cf\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "RCaGP\u5728\u5e72\u51c0\u548c\u5f02\u5e38\u503c\u6c61\u67d3\u7684\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u66f4\u4fdd\u5b88\u548c\u53ef\u9760\u7684\u4f30\u8ba1\u3002", "conclusion": "\u8054\u5408\u89e3\u51b3\u9c81\u68d2\u6027\u548c\u8fd1\u4f3c\u95ee\u9898\u4f18\u4e8e\u5355\u72ec\u5904\u7406\uff0cRCaGP\u4e3a\u56de\u5f52\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.21068", "pdf": "https://arxiv.org/pdf/2505.21068", "abs": "https://arxiv.org/abs/2505.21068", "authors": ["Anil Batra", "Laura Sevilla-Lara", "Marcus Rohrbach", "Frank Keller"], "title": "Predicting Implicit Arguments in Procedural Video Instructions", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Implicit-VidSRL\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u70f9\u996a\u8fc7\u7a0b\u4e2d\u5bf9\u9690\u5f0f\u53c2\u6570\uff08\u5982what\u548cwhere/with\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86iSRL-Qwen2-VL\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8eGPT-4o\u3002", "motivation": "\u73b0\u6709SRL\u57fa\u51c6\u6d4b\u8bd5\u5e38\u5ffd\u7565\u9690\u5f0f\u53c2\u6570\uff0c\u5bfc\u81f4\u5bf9\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e0d\u5b8c\u6574\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u6a21\u578b\u5bf9\u9690\u5f0f\u53c2\u6570\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaImplicit-VidSRL\u6570\u636e\u96c6\uff0c\u5305\u542b\u70f9\u996a\u8fc7\u7a0b\u4e2d\u7684\u9690\u5f0f\u548c\u663e\u5f0f\u53c2\u6570\uff1b\u63d0\u51faiSRL-Qwen2-VL\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\u3002", "result": "iSRL-Qwen2-VL\u5728\u9690\u5f0f\u53c2\u6570\u63a8\u7406\u4e0a\u6bd4GPT-4o\u63d0\u5347\u4e8617%\uff08what\uff09\u548c14.7%\uff08where/with\uff09\u3002", "conclusion": "\u591a\u6a21\u6001\u6570\u636e\u5bf9\u9690\u5f0f\u53c2\u6570\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0ciSRL-Qwen2-VL\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.21079", "pdf": "https://arxiv.org/pdf/2505.21079", "abs": "https://arxiv.org/abs/2505.21079", "authors": ["Yue Zhang", "Yingzhao Jian", "Hehe Fan", "Yi Yang", "Roger Zimmermann"], "title": "Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated considerable potential for comprehensive 3D scene understanding.\nHowever, existing approaches typically utilize only one or a limited subset of\n3D modalities, resulting in incomplete representations of 3D scenes and reduced\ninterpretive accuracy. Furthermore, different types of queries inherently\ndepend on distinct modalities, indicating that uniform processing of all\nmodality tokens may fail to effectively capture query-specific context. To\naddress these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts\n(MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion.\nSpecifically, Uni3D-MoE integrates a comprehensive set of 3D modalities,\nincluding multi-view RGB and depth images, bird's-eye-view (BEV) maps, point\nclouds, and voxel representations. At its core, our framework employs a\nlearnable routing mechanism within the sparse MoE-based large language model,\ndynamically selecting appropriate experts at the token level. Each expert\nspecializes in processing multimodal tokens based on learned modality\npreferences, thus facilitating flexible collaboration tailored to diverse\ntask-specific requirements. Extensive evaluations on standard 3D scene\nunderstanding benchmarks and specialized datasets demonstrate the efficacy of\nUni3D-MoE.", "AI": {"tldr": "Uni3D-MoE\u662f\u4e00\u79cd\u57fa\u4e8e\u7a00\u758fMixture-of-Experts\uff08MoE\uff09\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5b9e\u73b0\u81ea\u9002\u5e943D\u591a\u6a21\u6001\u878d\u5408\uff0c\u63d0\u53473D\u573a\u666f\u7406\u89e3\u7684\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f7f\u7528\u6709\u9650\u76843D\u6a21\u6001\uff0c\u5bfc\u81f4\u573a\u666f\u8868\u793a\u4e0d\u5b8c\u6574\u4e14\u89e3\u91ca\u51c6\u786e\u6027\u964d\u4f4e\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u67e5\u8be2\u4f9d\u8d56\u4e0d\u540c\u6a21\u6001\uff0c\u7edf\u4e00\u5904\u7406\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u6355\u6349\u67e5\u8be2\u7279\u5b9a\u4e0a\u4e0b\u6587\u3002", "method": "Uni3D-MoE\u6574\u5408\u4e86\u591a\u79cd3D\u6a21\u6001\uff08\u5982RGB\u3001\u6df1\u5ea6\u56fe\u50cf\u3001BEV\u5730\u56fe\u3001\u70b9\u4e91\u548c\u4f53\u7d20\u8868\u793a\uff09\uff0c\u5e76\u5728\u7a00\u758fMoE\u6846\u67b6\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u8def\u7531\u673a\u5236\uff0c\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5904\u7406\u591a\u6a21\u6001\u4ee4\u724c\u3002", "result": "\u5728\u6807\u51c63D\u573a\u666f\u7406\u89e3\u57fa\u51c6\u548c\u4e13\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u9a8c\u8bc1\u4e86Uni3D-MoE\u7684\u6709\u6548\u6027\u3002", "conclusion": "Uni3D-MoE\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.21135", "pdf": "https://arxiv.org/pdf/2505.21135", "abs": "https://arxiv.org/abs/2505.21135", "authors": ["Anqi Tang", "Youming Chen", "Shuchen Xue", "Zhaoqiang Liu"], "title": "Learning Single Index Models with Diffusion Priors", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "ICML 2025", "summary": "Diffusion models (DMs) have demonstrated remarkable ability to generate\ndiverse and high-quality images by efficiently modeling complex data\ndistributions. They have also been explored as powerful generative priors for\nsignal recovery, resulting in a substantial improvement in the quality of\nreconstructed signals. However, existing research on signal recovery with\ndiffusion models either focuses on specific reconstruction problems or is\nunable to handle nonlinear measurement models with discontinuous or unknown\nlink functions. In this work, we focus on using DMs to achieve accurate\nrecovery from semi-parametric single index models, which encompass a variety of\npopular nonlinear models that may have {\\em discontinuous} and {\\em unknown}\nlink functions. We propose an efficient reconstruction method that only\nrequires one round of unconditional sampling and (partial) inversion of DMs.\nTheoretical analysis on the effectiveness of the proposed methods has been\nestablished under appropriate conditions. We perform numerical experiments on\nimage datasets for different nonlinear measurement models. We observe that\ncompared to competing methods, our approach can yield more accurate\nreconstructions while utilizing significantly fewer neural function\nevaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u4fe1\u53f7\u6062\u590d\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u534a\u53c2\u6570\u5355\u6307\u6807\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u975e\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u4e2d\u7684\u4e0d\u8fde\u7eed\u6216\u672a\u77e5\u94fe\u63a5\u51fd\u6570\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u4fe1\u53f7\u6062\u590d\u4e2d\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u91cd\u5efa\u95ee\u9898\uff0c\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u975e\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u4e2d\u7684\u4e0d\u8fde\u7eed\u6216\u672a\u77e5\u94fe\u63a5\u51fd\u6570\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u53f7\u6062\u590d\u65b9\u6cd5\uff0c\u4ec5\u9700\u4e00\u8f6e\u65e0\u6761\u4ef6\u91c7\u6837\u548c\u90e8\u5206\u6269\u6563\u6a21\u578b\u7684\u53cd\u8f6c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u6bd4\u7ade\u4e89\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u4e14\u795e\u7ecf\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u4e2d\u7684\u4fe1\u53f7\u6062\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.21072", "pdf": "https://arxiv.org/pdf/2505.21072", "abs": "https://arxiv.org/abs/2505.21072", "authors": ["Ekaterina Fadeeva", "Aleksandr Rubashevskii", "Roman Vashurin", "Shehzaad Dhuliawala", "Artem Shelmanov", "Timothy Baldwin", "Preslav Nakov", "Mrinmaya Sachan", "Maxim Panov"], "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFRANQ\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bRAG\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u533a\u5206\u4e8b\u5b9e\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RAG\u7cfb\u7edf\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6df7\u6dc6\u4e86\u4e8b\u5b9e\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u5bfc\u81f4\u8bef\u5224\u3002", "method": "\u63d0\u51faFRANQ\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u5206\u522b\u8bc4\u4f30\u4e8b\u5b9e\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u5e76\u6784\u5efa\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFRANQ\u5728\u591a\u6570\u636e\u96c6\u548cLLMs\u4e0a\u80fd\u66f4\u51c6\u786e\u5730\u68c0\u6d4bRAG\u751f\u6210\u54cd\u5e94\u4e2d\u7684\u4e8b\u5b9e\u9519\u8bef\u3002", "conclusion": "FRANQ\u4e3aRAG\u7cfb\u7edf\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u533a\u5206\u4e8b\u5b9e\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "relevance": 85.0}}
{"id": "2505.21089", "pdf": "https://arxiv.org/pdf/2505.21089", "abs": "https://arxiv.org/abs/2505.21089", "authors": ["Junjue Wang", "Weihao Xuan", "Heli Qi", "Zhihao Liu", "Kunyi Liu", "Yuhan Wu", "Hongruixuan Chen", "Jian Song", "Junshi Xia", "Zhuo Zheng", "Naoto Yokoya"], "title": "DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response", "categories": ["cs.CV", "I.4.9"], "comment": "A multi-hazard, multi-sensor, and multi-task vision-language dataset\n  for global-scale disaster assessment and response", "summary": "Large vision-language models (VLMs) have made great achievements in Earth\nvision. However, complex disaster scenes with diverse disaster types,\ngeographic regions, and satellite sensors have posed new challenges for VLM\napplications. To fill this gap, we curate a remote sensing vision-language\ndataset (DisasterM3) for global-scale disaster assessment and response.\nDisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction\npairs across 5 continents, with three characteristics: 1) Multi-hazard:\nDisasterM3 involves 36 historical disaster events with significant impacts,\nwhich are categorized into 10 common natural and man-made disasters.\n2)Multi-sensor: Extreme weather during disasters often hinders optical sensor\nimaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery\nfor post-disaster scenes. 3) Multi-task: Based on real-world scenarios,\nDisasterM3 includes 9 disaster-related visual perception and reasoning tasks,\nharnessing the full potential of VLM's reasoning ability with progressing from\ndisaster-bearing body recognition to structural damage assessment and object\nrelational reasoning, culminating in the generation of long-form disaster\nreports. We extensively evaluated 14 generic and remote sensing VLMs on our\nbenchmark, revealing that state-of-the-art models struggle with the disaster\ntasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap,\nand damage object counting insensitivity. Focusing on these issues, we\nfine-tune four VLMs using our dataset and achieve stable improvements across\nall tasks, with robust cross-sensor and cross-disaster generalization\ncapabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u7403\u5c3a\u5ea6\u7684\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6DisasterM3\uff0c\u7528\u4e8e\u707e\u5bb3\u8bc4\u4f30\u4e0e\u54cd\u5e94\uff0c\u5305\u542b\u591a\u707e\u5bb3\u3001\u591a\u4f20\u611f\u5668\u548c\u591a\u4efb\u52a1\u7279\u6027\uff0c\u5e76\u8bc4\u4f30\u4e8614\u79cd\u901a\u7528\u548c\u9065\u611fVLMs\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u707e\u5bb3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u586b\u8865\u590d\u6742\u707e\u5bb3\u573a\u666f\u4e0bVLM\u5e94\u7528\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u5168\u7403\u5c3a\u5ea6\u7684\u707e\u5bb3\u8bc4\u4f30\u4e0e\u54cd\u5e94\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efaDisasterM3\u6570\u636e\u96c6\uff0c\u5305\u542b26,988\u5f20\u53cc\u65f6\u76f8\u536b\u661f\u56fe\u50cf\u548c123k\u6307\u4ee4\u5bf9\uff0c\u8986\u76d65\u5927\u6d3236\u79cd\u707e\u5bb3\u4e8b\u4ef6\uff0c\u6d89\u53ca\u591a\u707e\u5bb3\u3001\u591a\u4f20\u611f\u5668\u548c\u591a\u4efb\u52a1\u7279\u6027\u3002\u8bc4\u4f3014\u79cdVLMs\uff0c\u5e76\u9488\u5bf9\u95ee\u9898\u5fae\u8c034\u79cd\u6a21\u578b\u3002", "result": "\u73b0\u6709VLMs\u5728\u707e\u5bb3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u7a33\u5b9a\u63d0\u5347\uff0c\u5177\u5907\u8de8\u4f20\u611f\u5668\u548c\u8de8\u707e\u5bb3\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DisasterM3\u6570\u636e\u96c6\u4e3a\u707e\u5bb3\u573a\u666f\u4e0b\u7684VLM\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.21136", "pdf": "https://arxiv.org/pdf/2505.21136", "abs": "https://arxiv.org/abs/2505.21136", "authors": ["Jintao Zhang", "Xiaoming Xu", "Jia Wei", "Haofeng Huang", "Pengle Zhang", "Chendong Xiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention2++: A More Efficient Implementation of SageAttention2", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV"], "comment": null, "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.", "AI": {"tldr": "SageAttention2++\u901a\u8fc7\u91cf\u5316\u52a0\u901f\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u5229\u7528FP8\u6307\u4ee4\u8fdb\u4e00\u6b65\u63d0\u901f\uff0c\u6bd4FlashAttention\u5feb3.9\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u6ce8\u610f\u529b\u673a\u5236\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u968f\u5e8f\u5217\u957f\u5ea6\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u6548\u7387\u662f\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u91cf\u5316\u52a0\u901f\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u4f7f\u7528FP8\u6307\u4ee4\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "SageAttention2++\u6bd4FlashAttention\u5feb3.9\u500d\uff0c\u4e14\u51c6\u786e\u6027\u4e0d\u53d8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u52a0\u901f\u4e86\u8bed\u8a00\u3001\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "relevance": 85.0}}
{"id": "2505.20481", "pdf": "https://arxiv.org/pdf/2505.20481", "abs": "https://arxiv.org/abs/2505.20481", "authors": ["Berat Kutay U\u011fra\u015f", "\u00d6mer Nezih Gerek", "\u0130brahim Talha Sayg\u0131"], "title": "CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate ECG interpretation is vital, yet complex cardiac data and\n\"black-box\" AI models limit clinical utility. Inspired by Transformer\narchitectures' success in NLP for understanding sequential data, we frame ECG\nas the heart's unique \"language\" of temporal patterns. We present\nCardioPatternFormer, a novel Transformer-based model for interpretable ECG\nclassification. It employs a sophisticated attention mechanism to precisely\nidentify and classify diverse cardiac patterns, excelling at discerning subtle\nanomalies and distinguishing multiple co-occurring conditions. This\npattern-guided attention provides clear insights by highlighting influential\nsignal regions, effectively allowing the \"heart to talk\" through transparent\ninterpretations. CardioPatternFormer demonstrates robust performance on\nchallenging ECGs, including complex multi-pathology cases. Its interpretability\nvia attention maps enables clinicians to understand the model's rationale,\nfostering trust and aiding informed diagnostic decisions. This work offers a\npowerful, transparent solution for advanced ECG analysis, paving the way for\nmore reliable and clinically actionable AI in cardiology.", "AI": {"tldr": "CardioPatternFormer\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u5fc3\u7535\u56fe\u5206\u7c7b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u5fc3\u810f\u6a21\u5f0f\u5e76\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u3002", "motivation": "\u5fc3\u7535\u56fe\u89e3\u91ca\u590d\u6742\u4e14AI\u6a21\u578b\u901a\u5e38\u4e3a\u9ed1\u76d2\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002\u53d7Transformer\u5728NLP\u4e2d\u5904\u7406\u5e8f\u5217\u6570\u636e\u7684\u542f\u53d1\uff0c\u5c06ECG\u89c6\u4e3a\u5fc3\u810f\u7684\u2018\u8bed\u8a00\u2019\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u7cbe\u786e\u8bc6\u522b\u548c\u5206\u7c7b\u5fc3\u810f\u6a21\u5f0f\uff0c\u5e76\u751f\u6210\u6ce8\u610f\u529b\u56fe\u4ee5\u63d0\u4f9b\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u5728\u590d\u6742ECG\uff08\u5305\u62ec\u591a\u75c5\u7406\u60c5\u51b5\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u3002", "conclusion": "CardioPatternFormer\u4e3a\u5fc3\u7535\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u3002", "relevance": 60.0}}
{"id": "2505.21082", "pdf": "https://arxiv.org/pdf/2505.21082", "abs": "https://arxiv.org/abs/2505.21082", "authors": ["Jieyong Kim", "Tongyoung Kim", "Soonjin Yoon", "Jaehyung Kim", "Dongha Lee"], "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RPM\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u7ea7\u4e2a\u6027\u5316\uff08reasoning-level personalization\uff09\u6765\u4f18\u5316\u9ed1\u76d2LLM\u7684\u8f93\u51fa\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u7528\u6237\u7684\u4e2a\u6027\u5316\u903b\u8f91\u3002", "motivation": "\u9ed1\u76d2LLM\u901a\u5e38\u751f\u6210\u901a\u7528\u54cd\u5e94\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u7684\u4e2a\u6027\u5316\u504f\u597d\u548c\u63a8\u7406\u98ce\u683c\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u54cd\u5e94\u7ea7\u4e2a\u6027\u5316\uff0c\u672a\u80fd\u5efa\u6a21\u7528\u6237\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "RPM\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u7528\u6237\u5386\u53f2\u4e2d\u7684\u7279\u5f81\u6784\u5efa\u4e2a\u6027\u5316\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u57fa\u4e8e\u7279\u5f81\u76f8\u4f3c\u6027\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u56e0\u7d20\u751f\u6210\u7528\u6237\u7279\u5b9a\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRPM\u5728\u591a\u6837\u4efb\u52a1\u4e2d\u4f18\u4e8e\u54cd\u5e94\u7ea7\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63a8\u7406\u7ea7\u4e2a\u6027\u5316\u662f\u4f18\u5316\u9ed1\u76d2LLM\u8f93\u51fa\u7684\u6709\u6548\u65b9\u6cd5\uff0cRPM\u6846\u67b6\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.21099", "pdf": "https://arxiv.org/pdf/2505.21099", "abs": "https://arxiv.org/abs/2505.21099", "authors": ["Tianhao Peng", "Ho Man Kwan", "Yuxuan Jiang", "Ge Gao", "Fan Zhang", "Xiaozhong Xu", "Shan Liu", "David Bull"], "title": "Instance Data Condensation for Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning based image Super-Resolution (ISR) relies on large training\ndatasets to optimize model generalization; this requires substantial\ncomputational and storage resources during training. While dataset condensation\nhas shown potential in improving data efficiency and privacy for high-level\ncomputer vision tasks, it has not yet been fully exploited for ISR. In this\npaper, we propose a novel Instance Data Condensation (IDC) framework\nspecifically for ISR, which achieves instance-level data condensation through\nRandom Local Fourier Feature Extraction and Multi-level Feature Distribution\nMatching. This aims to optimize feature distributions at both global and local\nlevels and obtain high-quality synthesized training content with fine detail.\nThis framework has been utilized to condense the most commonly used training\ndataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic\ndataset offers comparable or (in certain cases) even better performance\ncompared to the original full dataset and excellent training stability when\nused to train various popular ISR models. To the best of our knowledge, this is\nthe first time that a condensed/synthetic dataset (with a 10% data volume) has\ndemonstrated such performance. The source code and the synthetic dataset have\nbeen made available at https://github.com/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08ISR\uff09\u7684\u5b9e\u4f8b\u6570\u636e\u538b\u7f29\uff08IDC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u5c40\u90e8\u5085\u91cc\u53f6\u7279\u5f81\u63d0\u53d6\u548c\u591a\u7ea7\u7279\u5f81\u5206\u5e03\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u5408\u6210\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u96c6\u538b\u7f29\u5728\u9ad8\u5c42\u6b21\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728ISR\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u538b\u7f29\u63d0\u9ad8ISR\u7684\u6570\u636e\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u63d0\u51faIDC\u6846\u67b6\uff0c\u7ed3\u5408\u968f\u673a\u5c40\u90e8\u5085\u91cc\u53f6\u7279\u5f81\u63d0\u53d6\u548c\u591a\u7ea7\u7279\u5f81\u5206\u5e03\u5339\u914d\uff0c\u4f18\u5316\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5206\u5e03\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u3002", "result": "\u5728DIV2K\u6570\u636e\u96c6\u4e0a\u4ee510%\u7684\u538b\u7f29\u7387\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u8bad\u7ec3\u6548\u679c\u4e0e\u539f\u6570\u636e\u96c6\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4e14\u8bad\u7ec3\u7a33\u5b9a\u6027\u4f18\u5f02\u3002", "conclusion": "IDC\u6846\u67b6\u9996\u6b21\u5c55\u793a\u4e86\u572810%\u6570\u636e\u91cf\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684ISR\u8bad\u7ec3\u6570\u636e\u538b\u7f29\u3002", "relevance": 40.0}}
{"id": "2505.21140", "pdf": "https://arxiv.org/pdf/2505.21140", "abs": "https://arxiv.org/abs/2505.21140", "authors": ["Honglin Gao", "Xiang Li", "Lan Zhao", "Gaoxi Xiao"], "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5f02\u6784\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff08HeteroBA\uff09\uff0c\u901a\u8fc7\u63d2\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u89e6\u53d1\u8282\u70b9\uff0c\u5b9e\u73b0\u5728\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u4f7f\u6a21\u578b\u5bf9\u7279\u5b9a\u8282\u70b9\u8fdb\u884c\u9519\u8bef\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNNs\uff09\u7684\u9884\u6d4b\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5176\u5728\u5bf9\u6297\u6027\u653b\u51fb\uff08\u5982\u540e\u95e8\u653b\u51fb\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faHeteroBA\u6846\u67b6\uff0c\u901a\u8fc7\u63d2\u5165\u5177\u6709\u771f\u5b9e\u7279\u5f81\u7684\u89e6\u53d1\u8282\u70b9\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u548c\u805a\u7c7b\u7684\u7b56\u7565\u9009\u62e9\u8f85\u52a9\u8282\u70b9\uff0c\u5b9e\u73b0\u89e6\u53d1\u4f20\u64ad\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cdHGNN\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHeteroBA\u80fd\u4ee5\u9ad8\u653b\u51fb\u6210\u529f\u7387\u5b9e\u73b0\u76ee\u6807\uff0c\u540c\u65f6\u5bf9\u5e72\u51c0\u6570\u636e\u7684\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86HGNNs\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u547c\u5401\u5728\u591a\u5173\u7cfb\u56fe\u573a\u666f\u4e2d\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002", "relevance": 40.0}}
{"id": "2505.21092", "pdf": "https://arxiv.org/pdf/2505.21092", "abs": "https://arxiv.org/abs/2505.21092", "authors": ["Daeen Kabir", "Minhajur Rahman Chowdhury Mahim", "Sheikh Shafayat", "Adnan Sadik", "Arian Ahmed", "Eunsu Kim", "Alice Oh"], "title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we introduce BLUCK, a new dataset designed to measure the\nperformance of Large Language Models (LLMs) in Bengali linguistic understanding\nand cultural knowledge. Our dataset comprises 2366 multiple-choice questions\n(MCQs) carefully curated from compiled collections of several college and job\nlevel examinations and spans 23 categories covering knowledge on Bangladesh's\nculture and history and Bengali linguistics. We benchmarked BLUCK using 6\nproprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,\nGemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that\nwhile these models perform reasonably well overall, they, however, struggles in\nsome areas of Bengali phonetics. Although current LLMs' performance on Bengali\ncultural and linguistic contexts is still not comparable to that of mainstream\nlanguages like English, our results indicate Bengali's status as a mid-resource\nlanguage. Importantly, BLUCK is also the first MCQ-based evaluation benchmark\nthat is centered around native Bengali culture, history, and linguistics.", "AI": {"tldr": "BLUCK\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b5f\u52a0\u62c9\u8bed\u8bed\u8a00\u7406\u89e3\u548c\u6587\u5316\u77e5\u8bc6\u65b9\u9762\u7684\u8868\u73b0\u3002\u6570\u636e\u96c6\u5305\u542b2366\u4e2a\u591a\u9009\u9898\uff0c\u6db5\u76d623\u4e2a\u7c7b\u522b\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u97f3\u5b66\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u6574\u4f53\u8868\u73b0\u5c1a\u53ef\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u5b5f\u52a0\u62c9\u8bed\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efaBLUCK\u6570\u636e\u96c6\uff0c\u5305\u542b2366\u4e2a\u591a\u9009\u9898\uff0c\u6d4b\u8bd56\u4e2a\u4e13\u6709\u548c3\u4e2a\u5f00\u6e90LLMs\u3002", "result": "\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u97f3\u5b66\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u6574\u4f53\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4e0d\u53ca\u4e3b\u6d41\u8bed\u8a00\u5982\u82f1\u8bed\u3002", "conclusion": "BLUCK\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u5b5f\u52a0\u62c9\u6587\u5316\u548c\u8bed\u8a00\u7684\u591a\u9009\u9898\u8bc4\u6d4b\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u4e2d\u7b49\u8d44\u6e90\u8bed\u8a00\u7684\u5730\u4f4d\u3002", "relevance": 60.0}}
{"id": "2505.21114", "pdf": "https://arxiv.org/pdf/2505.21114", "abs": "https://arxiv.org/abs/2505.21114", "authors": ["Shuai Wang", "Zexian Li", "Qipeng zhang", "Tianhui Song", "Xubin Li", "Tiezheng Ge", "Bo Zheng", "Limin Wang"], "title": "Differentiable Solver Search for Fast Diffusion Sampling", "categories": ["cs.CV"], "comment": "accpeted on ICML25", "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u6c42\u89e3\u5668\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684ODE\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684ODE\u6c42\u89e3\u5668\u4f9d\u8d56\u4f20\u7edf\u7684t\u76f8\u5173\u62c9\u683c\u6717\u65e5\u63d2\u503c\uff0c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65f6\u95f4\u6b65\u957f\u548c\u6c42\u89e3\u5668\u7cfb\u6570\u6784\u5efa\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u51fa\u53ef\u5fae\u5206\u6c42\u89e3\u5668\u641c\u7d22\u7b97\u6cd5\uff0c\u627e\u5230\u66f4\u4f18\u7684\u6c42\u89e3\u5668\u3002", "result": "\u5728ImageNet256\u4e0a\uff0c\u4f7f\u752810\u6b65\u7684SiT-XL/2\u548cFlowDCN-XL/2\u5206\u522b\u8fbe\u5230FID 2.40\u548c2.35\uff0c\u4f18\u4e8e\u4f20\u7edf\u6c42\u89e3\u5668\u3002", "conclusion": "\u641c\u7d22\u5230\u7684\u6c42\u89e3\u5668\u5177\u6709\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u5206\u8fa8\u7387\u3002", "relevance": 40.0}}
{"id": "2505.21141", "pdf": "https://arxiv.org/pdf/2505.21141", "abs": "https://arxiv.org/abs/2505.21141", "authors": ["Nancy C. Woods", "Virtue Ene Agada", "Adebola K. Ojo"], "title": "A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques", "categories": ["cs.LG", "68T05"], "comment": "12 pages", "summary": "Phishing is a semantic attack which targets the user rather than the\ncomputer. It is a new Internet crime in comparison with other forms such as\nvirus and hacking. Considering the damage phishing websites has caused to\nvarious economies by collapsing organizations, stealing information and\nfinancial diversion, various researchers have embarked on different ways of\ndetecting phishing websites but there has been no agreement about the best\nalgorithm to be used for prediction. This study is interested in integrating\nthe strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class\nClassification Rules based on Association Rules (MCAR) to establish a strong\nand better means of predicting phishing websites. A total of 11,056 websites\nwere used from both PhishTank and yahoo directory to verify the effectiveness\nof this approach. Feature extraction and rules generation were done by the MCAR\ntechnique; classification and prediction were done by SVM technique. The result\nshowed that the technique achieved 98.30% classification accuracy with a\ncomputation time of 2205.33s with minimum error rate. It showed a total of 98%\nArea under the Curve (AUC) which showed the proportion of accuracy in\nclassifying phishing websites. The model showed 82.84% variance in the\nprediction of phishing websites based on the coefficient of determination. The\nuse of two techniques together in detecting phishing websites produced a more\naccurate result as it combined the strength of both techniques respectively.\nThis research work centralized on this advantage by building a hybrid of two\ntechniques to help produce a more accurate result.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u548c\u591a\u7c7b\u5206\u7c7b\u5173\u8054\u89c4\u5219\uff08MCAR\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u9493\u9c7c\u7f51\u7ad9\uff0c\u8fbe\u5230\u4e8698.30%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u9493\u9c7c\u7f51\u7ad9\u5bf9\u7ecf\u6d4e\u548c\u4fe1\u606f\u5b89\u5168\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u7b97\u6cd5\u5c1a\u672a\u8fbe\u6210\u4e00\u81f4\u3002\u7814\u7a76\u65e8\u5728\u6574\u5408SVM\u548cMCAR\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528MCAR\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u89c4\u5219\u751f\u6210\uff0cSVM\u8fdb\u884c\u5206\u7c7b\u548c\u9884\u6d4b\u3002\u5b9e\u9a8c\u57fa\u4e8e11,056\u4e2a\u7f51\u7ad9\u6570\u636e\u3002", "result": "\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8fbe98.30%\uff0cAUC\u4e3a98%\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e3a2205.33\u79d2\u3002", "conclusion": "\u7ed3\u5408SVM\u548cMCAR\u7684\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9493\u9c7c\u7f51\u7ad9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "relevance": 20.0}}
{"id": "2505.21097", "pdf": "https://arxiv.org/pdf/2505.21097", "abs": "https://arxiv.org/abs/2505.21097", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "title": "Thinker: Learning to Think Fast and Slow", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "comment": "21 pages", "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u7406\u8bba\u7684\u56db\u9636\u6bb5QA\u4efb\u52a1\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u601d\u8003\u3001\u9a8c\u8bc1\u3001\u6162\u901f\u601d\u8003\u548c\u603b\u7ed3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u9488\u5bf9LLM\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u7684\u641c\u7d22\u884c\u4e3a\u4e0d\u7cbe\u786e\u548c\u7f3a\u4e4f\u4fe1\u5fc3\u7684\u95ee\u9898\uff0c\u53d7\u5fc3\u7406\u5b66\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u9636\u6bb5QA\u4efb\u52a1\uff1a\u5feb\u901f\u601d\u8003\uff08\u4e25\u683ctoken\u9650\u5236\uff09\u3001\u9a8c\u8bc1\u3001\u6162\u901f\u601d\u8003\u548c\u603b\u7ed3\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002", "result": "Qwen2.5-1.5B\u7684\u5e73\u5747\u51c6\u786e\u7387\u4ece24.9%\u63d0\u5347\u81f327.9%\uff0cDeepSeek-R1-Qwen-1.5B\u4ece45.9%\u63d0\u5347\u81f349.8%\u3002\u5feb\u901f\u601d\u8003\u6a21\u5f0f\u4ec5\u75281000token\u5373\u8fbe\u523026.8%\u51c6\u786e\u7387\u3002", "conclusion": "\u76f4\u89c9\u548c\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u662f\u4e92\u8865\u7cfb\u7edf\uff0c\u9488\u5bf9\u6027\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.21117", "pdf": "https://arxiv.org/pdf/2505.21117", "abs": "https://arxiv.org/abs/2505.21117", "authors": ["Adeela Islam", "Stefano Fiorini", "Stuart James", "Pietro Morerio", "Alessio Del Bue"], "title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The task of reassembly is a significant challenge across multiple domains,\nincluding archaeology, genomics, and molecular docking, requiring the precise\nplacement and orientation of elements to reconstruct an original structure. In\nthis work, we address key limitations in state-of-the-art Deep Learning methods\nfor reassembly, namely i) scalability; ii) multimodality; and iii) real-world\napplicability: beyond square or simple geometric shapes, realistic and complex\nerosion, or other real-world problems. We propose ReassembleNet, a method that\nreduces complexity by representing each input piece as a set of contour\nkeypoints and learning to select the most informative ones by Graph Neural\nNetworks pooling inspired techniques. ReassembleNet effectively lowers\ncomputational complexity while enabling the integration of features from\nmultiple modalities, including both geometric and texture data. Further\nenhanced through pretraining on a semi-synthetic dataset. We then apply\ndiffusion-based pose estimation to recover the original structure. We improve\non prior methods by 55% and 86% for RMSE Rotation and Translation,\nrespectively.", "AI": {"tldr": "ReassembleNet\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6269\u6563\u59ff\u6001\u4f30\u8ba1\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u9886\u57df\u4e2d\u7684\u91cd\u7ec4\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u91cd\u7ec4\u4efb\u52a1\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u3001\u591a\u6a21\u6001\u548c\u5b9e\u9645\u9002\u7528\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u5f62\u72b6\u548c\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faReassembleNet\uff0c\u901a\u8fc7\u8f6e\u5ed3\u5173\u952e\u70b9\u8868\u793a\u8f93\u5165\u7247\u6bb5\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6c60\u5316\u6280\u672f\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5173\u952e\u70b9\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u59ff\u6001\u4f30\u8ba1\u6062\u590d\u539f\u59cb\u7ed3\u6784\u3002", "result": "\u5728\u65cb\u8f6c\u548c\u5e73\u79fb\u7684RMSE\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8655%\u548c86%\u3002", "conclusion": "ReassembleNet\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u590d\u6742\u548c\u771f\u5b9e\u4e16\u754c\u7684\u91cd\u7ec4\u4efb\u52a1\u3002", "relevance": 40.0}}
{"id": "2505.21147", "pdf": "https://arxiv.org/pdf/2505.21147", "abs": "https://arxiv.org/abs/2505.21147", "authors": ["Xuanning Zhou", "Hao Zeng", "Xiaobo Xia", "Bingyi Jing", "Hongxin Wei"], "title": "Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score", "categories": ["cs.LG"], "comment": null, "summary": "Conformal prediction (CP) is a powerful framework for uncertainty\nquantification, providing prediction sets with coverage guarantees when\ncalibrated on sufficient labeled data. However, in real-world applications\nwhere labeled data is often limited, standard CP can lead to coverage deviation\nand output overly large prediction sets. In this paper, we extend CP to the\nsemi-supervised setting and propose SemiCP, leveraging both labeled data and\nunlabeled data for calibration. Specifically, we introduce a novel\nnonconformity score function, NNM, designed for unlabeled data. This function\nselects labeled data with similar pseudo-label scores to estimate nonconformity\nscores, integrating them into the calibration process to overcome sample size\nlimitations. We theoretically demonstrate that, under mild assumptions, SemiCP\nprovide asymptotically coverage guarantee for prediction sets. Extensive\nexperiments further validate that our approach effectively reduces instability\nand inefficiency under limited calibration data, can be adapted to conditional\ncoverage settings, and integrates seamlessly with existing CP methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSemiCP\uff0c\u4e00\u79cd\u534a\u76d1\u7763\u73af\u5883\u4e0b\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u6539\u8fdb\u6821\u51c6\uff0c\u89e3\u51b3\u6709\u9650\u6807\u8bb0\u6570\u636e\u5bfc\u81f4\u7684\u8986\u76d6\u504f\u5dee\u548c\u9884\u6d4b\u96c6\u8fc7\u5927\u95ee\u9898\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6807\u8bb0\u6570\u636e\u6709\u9650\uff0c\u6807\u51c6\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u53ef\u80fd\u5bfc\u81f4\u8986\u76d6\u504f\u5dee\u548c\u9884\u6d4b\u96c6\u8fc7\u5927\u3002\u8bba\u6587\u65e8\u5728\u6269\u5c55CP\u5230\u534a\u76d1\u7763\u8bbe\u7f6e\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u6539\u8fdb\u6821\u51c6\u3002", "method": "\u63d0\u51faSemiCP\uff0c\u5f15\u5165\u65b0\u7684\u975e\u5171\u5f62\u6027\u8bc4\u5206\u51fd\u6570NNM\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u4f2a\u6807\u7b7e\u8bc4\u5206\u9009\u62e9\u76f8\u4f3c\u6807\u8bb0\u6570\u636e\uff0c\u6574\u5408\u5230\u6821\u51c6\u8fc7\u7a0b\u4e2d\u3002", "result": "\u7406\u8bba\u8bc1\u660eSemiCP\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u63d0\u4f9b\u6e10\u8fd1\u8986\u76d6\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u6709\u6548\u51cf\u5c11\u6709\u9650\u6821\u51c6\u6570\u636e\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u4f4e\u6548\u6027\uff0c\u9002\u5e94\u6761\u4ef6\u8986\u76d6\u8bbe\u7f6e\uff0c\u5e76\u4e0e\u73b0\u6709CP\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "SemiCP\u901a\u8fc7\u534a\u76d1\u7763\u6821\u51c6\u6539\u8fdb\u4e86CP\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9a8c\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.21109", "pdf": "https://arxiv.org/pdf/2505.21109", "abs": "https://arxiv.org/abs/2505.21109", "authors": ["Bogdan Bogachov", "Yaoyao Fiona Zhao"], "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG", "I.2.7; I.2.1; I.5.1; I.2.6; H.3.1"], "comment": "10 pages, 4 Figures, 6 Tables. This paper has been accepted to be\n  published in the proceedings of IDETC-CIE 2025", "summary": "Despite recent advancements in domain adaptation techniques for large\nlanguage models, these methods remain computationally intensive, and the\nresulting models can still exhibit hallucination issues. Most existing\nadaptation methods do not prioritize reducing the computational resources\nrequired for fine-tuning and inference of language models. Hallucination issues\nhave gradually decreased with each new model release. However, they remain\nprevalent in engineering contexts, where generating well-structured text with\nminimal errors and inconsistencies is critical. This work introduces a novel\napproach called the Small Language Graph (SLG), which is a lightweight\nadaptation solution designed to address the two key challenges outlined above.\nThe system is structured in the form of a graph, where each node represents a\nlightweight expert - a small language model fine-tuned on specific and concise\ntexts. The results of this study have shown that SLG was able to surpass\nconventional fine-tuning methods on the Exact Match metric by 3 times.\nAdditionally, the fine-tuning process was 1.7 times faster compared to that of\na larger stand-alone language model. These findings introduce a potential for\nsmall to medium-sized engineering companies to confidently use generative AI\ntechnologies, such as LLMs, without the necessity to invest in expensive\ncomputational resources. Also, the graph architecture and the small size of\nexpert nodes offer a possible opportunity for distributed AI systems, thus\npotentially diverting the global need for expensive centralized compute\nclusters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9886\u57df\u9002\u5e94\u65b9\u6cd5Small Language Graph (SLG)\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u5c0f\u578b\u4e13\u5bb6\u8282\u70b9\u89e3\u51b3\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5de5\u7a0b\u573a\u666f\u4e2d\u9700\u8981\u9ad8\u7cbe\u5ea6\u6587\u672c\u751f\u6210\u3002", "method": "\u91c7\u7528\u56fe\u7ed3\u6784\uff0c\u6bcf\u4e2a\u8282\u70b9\u4e3a\u5c0f\u578b\u4e13\u5bb6\u6a21\u578b\uff0c\u9488\u5bf9\u7279\u5b9a\u6587\u672c\u5fae\u8c03\uff0c\u5b9e\u73b0\u8f7b\u91cf\u5316\u548c\u9ad8\u6548\u6027\u3002", "result": "SLG\u5728Exact Match\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd53\u500d\uff0c\u5fae\u8c03\u901f\u5ea6\u5feb1.7\u500d\u3002", "conclusion": "SLG\u4e3a\u4e2d\u5c0f\u578b\u516c\u53f8\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u9014\u5f84\uff0c\u5e76\u53ef\u80fd\u63a8\u52a8\u5206\u5e03\u5f0fAI\u7cfb\u7edf\u53d1\u5c55\u3002", "relevance": 85.0}}
{"id": "2505.21144", "pdf": "https://arxiv.org/pdf/2505.21144", "abs": "https://arxiv.org/abs/2505.21144", "authors": ["Sergey Karpukhin", "Vadim Titov", "Andrey Kuznetsov", "Aibek Alanov"], "title": "FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention", "categories": ["cs.CV"], "comment": "code available at https://github.com/shredder67/fastface", "summary": "In latest years plethora of identity-preserving adapters for a personalized\ngeneration with diffusion models have been released. Their main disadvantage is\nthat they are dominantly trained jointly with base diffusion models, which\nsuffer from slow multi-step inference. This work aims to tackle the challenge\nof training-free adaptation of pretrained ID-adapters to diffusion models\naccelerated via distillation - through careful re-design of classifier-free\nguidance for few-step stylistic generation and attention manipulation\nmechanisms in decoupled blocks to improve identity similarity and fidelity, we\npropose universal FastFace framework. Additionally, we develop a disentangled\npublic evaluation protocol for id-preserving adapters.", "AI": {"tldr": "\u63d0\u51fa\u4e86FastFace\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684ID\u9002\u914d\u5668\u5feb\u901f\u9002\u914d\u5230\u84b8\u998f\u52a0\u901f\u7684\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709ID\u9002\u914d\u5668\u9700\u8981\u8054\u5408\u8bad\u7ec3\u4e14\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51faFastFace\u6846\u67b6\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u5feb\u901f\u9002\u914d\uff0c\u63d0\u9ad8\u4e86\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "conclusion": "FastFace\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86ID\u9002\u914d\u5668\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.21160", "pdf": "https://arxiv.org/pdf/2505.21160", "abs": "https://arxiv.org/abs/2505.21160", "authors": ["Michael Stenger", "Robert Leppich", "Andr\u00e9 Bauer", "Samuel Kounev"], "title": "STEB: In Search of the Best Evaluation Approach for Synthetic Time Series", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing need for synthetic time series, due to data augmentation or\nprivacy regulations, has led to numerous generative models, frameworks, and\nevaluation measures alike. Objectively comparing these measures on a large\nscale remains an open challenge. We propose the Synthetic Time series\nEvaluation Benchmark (STEB) -- the first benchmark framework that enables\ncomprehensive and interpretable automated comparisons of synthetic time series\nevaluation measures. Using 10 diverse datasets, randomness injection, and 13\nconfigurable data transformations, STEB computes indicators for measure\nreliability and score consistency. It tracks running time, test errors, and\nfeatures sequential and parallel modes of operation. In our experiments, we\ndetermine a ranking of 41 measures from literature and confirm that the choice\nof upstream time series embedding heavily impacts the final score.", "AI": {"tldr": "STEB\u662f\u4e00\u4e2a\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6a21\u578b\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u6570\u636e\u96c6\u548c\u914d\u7f6e\u8f6c\u6362\u63d0\u4f9b\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u6307\u6807\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u589e\u5f3a\u548c\u9690\u79c1\u6cd5\u89c4\u7684\u9700\u6c42\uff0c\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6a21\u578b\u53ca\u5176\u8bc4\u4f30\u65b9\u6cd5\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7f3a\u4e4f\u5ba2\u89c2\u7684\u5927\u89c4\u6a21\u6bd4\u8f83\u65b9\u6cd5\u3002", "method": "STEB\u5229\u752810\u4e2a\u6570\u636e\u96c6\u3001\u968f\u673a\u6027\u6ce8\u5165\u548c13\u79cd\u53ef\u914d\u7f6e\u6570\u636e\u8f6c\u6362\uff0c\u8ba1\u7b97\u8bc4\u4f30\u6307\u6807\u7684\u53ef\u9760\u6027\u548c\u5f97\u5206\u4e00\u81f4\u6027\uff0c\u5e76\u8ddf\u8e2a\u8fd0\u884c\u65f6\u95f4\u548c\u6d4b\u8bd5\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u4e2d\u5bf941\u79cd\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u6392\u540d\uff0c\u53d1\u73b0\u4e0a\u6e38\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u5bf9\u6700\u7ec8\u5f97\u5206\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "STEB\u4e3a\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u6bd4\u8f83\u6846\u67b6\u3002", "relevance": 40.0}}
{"id": "2505.21115", "pdf": "https://arxiv.org/pdf/2505.21115", "abs": "https://arxiv.org/abs/2505.21115", "authors": ["Sergey Pletenev", "Maria Marina", "Nikolay Ivanov", "Daria Galimzianova", "Nikita Krayko", "Mikhail Salnikov", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EverGreenQA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3LLMs\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u5bf9\u95ee\u9898\u65f6\u95f4\u6027\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668EG-E5\u3002", "motivation": "LLMs\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u5e38\u51fa\u73b0\u5e7b\u89c9\uff0c\u95ee\u9898\u7684\u65f6\u95f4\u6027\uff08\u662f\u5426\u968f\u65f6\u95f4\u53d8\u5316\uff09\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u56e0\u7d20\u3002", "method": "\u5f15\u5165EverGreenQA\u6570\u636e\u96c6\uff0c\u8bc4\u4f3012\u79cd\u73b0\u4ee3LLMs\u5bf9\u95ee\u9898\u65f6\u95f4\u6027\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u7f16\u7801\u80fd\u529b\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668EG-E5\u3002", "result": "EG-E5\u5728\u4efb\u52a1\u4e2d\u8fbe\u5230SoTA\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u65f6\u95f4\u6027\u5206\u7c7b\u5728\u4e09\u4e2a\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u95ee\u9898\u65f6\u95f4\u6027\u662f\u5f71\u54cdLLMs\u95ee\u7b54\u6027\u80fd\u7684\u91cd\u8981\u56e0\u7d20\uff0cEverGreenQA\u548cEG-E5\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.21152", "pdf": "https://arxiv.org/pdf/2505.21152", "abs": "https://arxiv.org/abs/2505.21152", "authors": ["Xurui Li", "Zhonesheng Jiang", "Tingxuan Ai", "Yu Zhou"], "title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images", "categories": ["cs.CV"], "comment": null, "summary": "Robust unsupervised anomaly detection (AD) in real-world scenarios is an\nimportant task. Current methods exhibit severe performance degradation on the\nMVTec AD 2 benchmark due to its complex real-world challenges. To solve this\nproblem, we propose a robust framework RoBiS, which consists of three core\nmodules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to\npreserve the information of small anomalies through overlapping window\ncropping. (2) The data augmentation of noise addition and lighting simulation\nis carried out on the training data to improve the robustness of AD model. We\nuse INP-Former as our baseline, which could generate better results on the\nvarious sub-images. (3) The traditional statistical-based binarization strategy\n(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),\nfor joint adaptive binarization. Then, SAM is further employed to refine the\nsegmentation results. Compared with some methods reported by the MVTec AD 2,\nour RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on\nTest_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on\nTest_private_mixed. Code is available at https://github.com/xrli-U/RoBiS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoBiS\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u9884\u5904\u7406\u3001\u6570\u636e\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u4e8c\u503c\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u5728MVTec AD 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u56e0\u590d\u6742\u771f\u5b9e\u573a\u666f\u6311\u6218\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "1. Swin-Cropping\u9884\u5904\u7406\u7b56\u7565\uff1b2. \u6570\u636e\u589e\u5f3a\uff08\u566a\u58f0\u548c\u5149\u7167\u6a21\u62df\uff09\uff1b3. \u7ed3\u5408\u4f20\u7edf\u7edf\u8ba1\u4e8c\u503c\u5316\u548cMEBin\u7684\u81ea\u9002\u5e94\u4e8c\u503c\u5316\u7b56\u7565\uff0c\u5e76\u4f7f\u7528SAM\u4f18\u5316\u5206\u5272\u7ed3\u679c\u3002", "result": "\u5728MVTec AD 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSegF1\u6307\u6807\u663e\u8457\u63d0\u5347\uff08Test_private\u4ece21.8%\u63d0\u5347\u81f351.00%\uff0cTest_private_mixed\u4ece16.7%\u63d0\u5347\u81f346.52%\uff09\u3002", "conclusion": "RoBiS\u6846\u67b6\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.21173", "pdf": "https://arxiv.org/pdf/2505.21173", "abs": "https://arxiv.org/abs/2505.21173", "authors": ["Zhiwang Yu"], "title": "Topological Deep Learning for Speech Data", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "comment": "21 pages, 15 figures", "summary": "Topological data analysis (TDA) offers novel mathematical tools for deep\nlearning. Inspired by Carlsson et al., this study designs topology-aware\nconvolutional kernels that significantly improve speech recognition networks.\nTheoretically, by investigating orthogonal group actions on kernels, we\nestablish a fiber-bundle decomposition of matrix spaces, enabling new filter\ngeneration methods. Practically, our proposed Orthogonal Feature (OF) layer\nachieves superior performance in phoneme recognition, particularly in low-noise\nscenarios, while demonstrating cross-domain adaptability. This work reveals\nTDA's potential in neural network optimization, opening new avenues for\nmathematics-deep learning interdisciplinary studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u7684\u62d3\u6251\u611f\u77e5\u5377\u79ef\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u53d7Carlsson\u7b49\u4eba\u542f\u53d1\uff0c\u63a2\u7d22\u6570\u5b66\u5de5\u5177\uff08\u5982TDA\uff09\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6b63\u4ea4\u7fa4\u4f5c\u7528\u7814\u7a76\u6539\u8fdb\u5377\u79ef\u6838\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u77e9\u9635\u7a7a\u95f4\u7684\u6b63\u4ea4\u7fa4\u4f5c\u7528\uff0c\u63d0\u51fa\u7ea4\u7ef4\u4e1b\u5206\u89e3\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u4ea4\u7279\u5f81\uff08OF\uff09\u5c42\u3002", "result": "OF\u5c42\u5728\u97f3\u7d20\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u4f4e\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u9886\u57df\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86TDA\u5728\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6570\u5b66\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 30.0}}
{"id": "2505.20503", "pdf": "https://arxiv.org/pdf/2505.20503", "abs": "https://arxiv.org/abs/2505.20503", "authors": ["Matthew Lisondra", "Beno Benhabib", "Goldie Nejat"], "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Rapid advancements in foundation models, including Large Language Models,\nVision-Language Models, Multimodal Large Language Models, and\nVision-Language-Action Models have opened new avenues for embodied AI in mobile\nservice robotics. By combining foundation models with the principles of\nembodied AI, where intelligent systems perceive, reason, and act through\nphysical interactions, robots can improve understanding, adapt to, and execute\ncomplex tasks in dynamic real-world environments. However, embodied AI in\nmobile service robots continues to face key challenges, including multimodal\nsensor fusion, real-time decision-making under uncertainty, task\ngeneralization, and effective human-robot interactions (HRI). In this paper, we\npresent the first systematic review of the integration of foundation models in\nmobile service robotics, identifying key open challenges in embodied AI and\nexamining how foundation models can address them. Namely, we explore the role\nof such models in enabling real-time sensor fusion, language-conditioned\ncontrol, and adaptive task execution. Furthermore, we discuss real-world\napplications in the domestic assistance, healthcare, and service automation\nsectors, demonstrating the transformative impact of foundation models on\nservice robotics. We also include potential future research directions,\nemphasizing the need for predictive scaling laws, autonomous long-term\nadaptation, and cross-embodiment generalization to enable scalable, efficient,\nand robust deployment of foundation models in human-centric robotic systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u7840\u6a21\u578b\u5728\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u7684\u96c6\u6210\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u5b9e\u65f6\u4f20\u611f\u5668\u878d\u5408\u3001\u8bed\u8a00\u6761\u4ef6\u63a7\u5236\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u4e0e\u5177\u8eabAI\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7406\u89e3\u548c\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u3001\u5b9e\u65f6\u51b3\u7b56\u7b49\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff0\u57fa\u7840\u6a21\u578b\u5728\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u5176\u5728\u4f20\u611f\u5668\u878d\u5408\u3001\u8bed\u8a00\u6761\u4ef6\u63a7\u5236\u548c\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u6f5c\u529b\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u5bb6\u5ead\u8f85\u52a9\u3001\u533b\u7597\u548c\u670d\u52a1\u81ea\u52a8\u5316\u7b49\u9886\u57df\u5177\u6709\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u9884\u6d4b\u6269\u5c55\u6cd5\u5219\u3001\u957f\u671f\u81ea\u9002\u5e94\u7b49\u95ee\u9898\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u4e3a\u5177\u8eabAI\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u9700\u7814\u7a76\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "relevance": 70.0}}
{"id": "2505.21137", "pdf": "https://arxiv.org/pdf/2505.21137", "abs": "https://arxiv.org/abs/2505.21137", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bann\u00f2", "Kate M. Knill", "Mark J. F. Gales"], "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech", "summary": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7aef\u5230\u7aef\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\uff08SGEC\uff09\u548c\u53cd\u9988\u751f\u6210\uff08SGECF\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4f2a\u6807\u6ce8\u6269\u5c55\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u63a2\u7d22\u4e86\u63d0\u793a\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edfSGEC\u7cfb\u7edf\u4f9d\u8d56\u7ea7\u8054\u6a21\u5757\uff0c\u800c\u7aef\u5230\u7aef\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u5174\u8d77\u4e3aSGEC\u548cSGECF\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u8fd9\u4e9b\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u4f2a\u6807\u6ce8\u8fc7\u7a0b\u6269\u5c55\u8bad\u7ec3\u6570\u636e\uff08\u4ece77\u5c0f\u65f6\u52302500\u5c0f\u65f6\uff09\uff0c\u5e76\u57fa\u4e8eWhisper\u6a21\u578b\u8fdb\u884c\u63d0\u793a\u548c\u6a21\u578b\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u4f2a\u6807\u6ce8\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff1b\u63d0\u793a\u5bf9\u53cd\u9988\u751f\u6210\u6548\u679c\u66f4\u660e\u663e\uff1b\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u65f6\uff0c\u4f2a\u6807\u6ce8\u6570\u636e\u65e0\u589e\u76ca\uff0c\u4f46\u63d0\u793a\u8bad\u7ec3\u6709\u76ca\u3002", "conclusion": "\u7aef\u5230\u7aef\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728SGEC\u548cSGECF\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u63d0\u793a\u548c\u4f2a\u6807\u6ce8\u662f\u6709\u6548\u7684\u6539\u8fdb\u624b\u6bb5\u3002", "relevance": 40.0}}
{"id": "2505.21179", "pdf": "https://arxiv.org/pdf/2505.21179", "abs": "https://arxiv.org/abs/2505.21179", "authors": ["Dar-Yen Chen", "Hmrishav Bandyopadhyay", "Kai Zou", "Yi-Zhe Song"], "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a \\textit{universal} plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNormalized Attention Guidance (NAG)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u8d1f\u5411\u5f15\u5bfc\u7684\u6311\u6218\u3002NAG\u901a\u8fc7\u6ce8\u610f\u529b\u7a7a\u95f4\u7684\u5916\u63a8\u548cL1\u5f52\u4e00\u5316\uff0c\u6709\u6548\u6062\u590d\u4e86\u8d1f\u5411\u5f15\u5bfc\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u548c\u6a21\u6001\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8d1f\u5411\u5f15\u5bfc\u5728\u5c11\u6b65\u91c7\u6837\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u5982Classifier-Free Guidance (CFG)\u5728\u6781\u7aef\u91c7\u6837\u6b65\u6570\u4e0b\u5931\u6548\u3002", "method": "NAG\u91c7\u7528\u6ce8\u610f\u529b\u7a7a\u95f4\u7684\u5916\u63a8\u548cL1\u5f52\u4e00\u5316\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u8d1f\u5411\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNAG\u5728\u6587\u672c\u5bf9\u9f50\u3001\u4fdd\u771f\u5ea6\u548c\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "NAG\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u73b0\u4ee3\u6269\u6563\u6846\u67b6\u3002", "relevance": 60.0}}
{"id": "2505.21180", "pdf": "https://arxiv.org/pdf/2505.21180", "abs": "https://arxiv.org/abs/2505.21180", "authors": ["ShuNing Sun", "YinSong Xiong", "Yu Zhang", "Zhuoran Zheng"], "title": "Latent label distribution grid representation for modeling uncertainty", "categories": ["cs.LG", "cs.AI"], "comment": "Under review", "summary": "Although \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning (LDL) has\npromising representation capabilities for characterizing the polysemy of an\ninstance, the complexity and high cost of the label distribution annotation\nlead to inexact in the construction of the label space. The existence of a\nlarge number of inexact labels generates a label space with uncertainty, which\nmisleads the LDL algorithm to yield incorrect decisions. To alleviate this\nproblem, we model the uncertainty of label distributions by constructing a\n\\textbf{L}atent \\textbf{L}abel \\textbf{D}istribution \\textbf{G}rid (LLDG) to\nform a low-noise representation space. Specifically, we first construct a label\ncorrelation matrix based on the differences between labels, and then expand\neach value of the matrix into a vector that obeys a Gaussian distribution, thus\nbuilding a LLDG to model the uncertainty of the label space. Finally, the LLDG\nis reconstructed by the LLDG-Mixer to generate an accurate label distribution.\nNote that we enforce a customized low-rank scheme on this grid, which assumes\nthat the label relations may be noisy and it needs to perform noise-reduction\nwith the help of a Tucker reconstruction technique. Furthermore, we attempt to\nevaluate the effectiveness of the LLDG by considering its generation as an\nupstream task to achieve the classification of the objects. Extensive\nexperimental results show that our approach performs competitively on several\nbenchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6f5c\u5728\u6807\u7b7e\u5206\u5e03\u7f51\u683c\uff08LLDG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u6807\u7b7e\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u65b9\u6848\u548cTucker\u91cd\u5efa\u6280\u672f\u51cf\u5c11\u566a\u58f0\uff0c\u751f\u6210\u51c6\u786e\u7684\u6807\u7b7e\u5206\u5e03\u3002", "motivation": "\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff08LDL\uff09\u5728\u8868\u793a\u5b9e\u4f8b\u591a\u4e49\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6807\u7b7e\u5206\u5e03\u6807\u6ce8\u7684\u590d\u6742\u6027\u548c\u9ad8\u6210\u672c\u5bfc\u81f4\u6807\u7b7e\u7a7a\u95f4\u4e0d\u7cbe\u786e\uff0c\u4ece\u800c\u8bef\u5bfc\u7b97\u6cd5\u51b3\u7b56\u3002", "method": "\u6784\u5efa\u6807\u7b7e\u76f8\u5173\u6027\u77e9\u9635\uff0c\u5c06\u5176\u6269\u5c55\u4e3a\u670d\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u5411\u91cf\u4ee5\u5f62\u6210LLDG\uff0c\u5e76\u901a\u8fc7LLDG-Mixer\u548c\u4f4e\u79e9Tucker\u91cd\u5efa\u6280\u672f\u91cd\u6784\u6807\u7b7e\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LLDG\u80fd\u6709\u6548\u5efa\u6a21\u6807\u7b7e\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u751f\u6210\u51c6\u786e\u7684\u6807\u7b7e\u5206\u5e03\u3002", "relevance": 40.0}}
{"id": "2505.21138", "pdf": "https://arxiv.org/pdf/2505.21138", "abs": "https://arxiv.org/abs/2505.21138", "authors": ["Tianyi Xu", "Hongjie Chen", "Wang Qing", "Lv Hang", "Jian Kang", "Li Jie", "Zhennan Lin", "Yongxiang Li", "Xie Lei"], "title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u5347\u4e2d\u6587\u65b9\u8a00\u548c\u53e3\u8bed\u97f3\u9891\u8bc6\u522b\uff08ASR\uff09\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u65b9\u8a00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u4e2d\u6587\u65b9\u8a00\u548c\u53e3\u8bed\u97f3\u9891\u6570\u636e\u7a00\u7f3a\uff0c\u5bfc\u81f4\u73b0\u6709ASR\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7ed3\u5408LLM\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "method": "\u9884\u8bad\u7ec3Data2vec2\u6a21\u578b\u4e8e30\u4e07\u5c0f\u65f6\u65e0\u6807\u7b7e\u65b9\u8a00\u548c\u53e3\u8bed\u97f3\u9891\u6570\u636e\uff0c\u5e76\u57284\u4e07\u5c0f\u65f6\u76d1\u7763\u6570\u636e\u4e0a\u8fdb\u884c\u5bf9\u9f50\u8bad\u7ec3\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6295\u5f71\u5668\u548cLLM\u5bf9\u8bc6\u522b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2a\u65b9\u8a00\u6570\u636e\u96c6\uff08\u5982Kespeech\uff09\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "conclusion": "\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7ed3\u5408LLM\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u65b9\u8a00ASR\u6027\u80fd\uff0c\u7814\u7a76\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u3002", "relevance": 60.0}}
{"id": "2505.21181", "pdf": "https://arxiv.org/pdf/2505.21181", "abs": "https://arxiv.org/abs/2505.21181", "authors": ["Yayin Zheng", "Chen Wan", "Zihong Guo", "Hailing Kuang", "Xiaohai Lu"], "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Adversarial attacks have become a significant challenge in the security of\nmachine learning models, particularly in the context of black-box defense\nstrategies. Existing methods for enhancing adversarial transferability\nprimarily focus on the spatial domain. This paper presents Frequency-Space\nAttack (FSA), a new adversarial attack framework that effectively integrates\nfrequency-domain and spatial-domain transformations. FSA combines two key\ntechniques: (1) High-Frequency Augmentation, which applies Fourier transform\nwith frequency-selective amplification to diversify inputs and emphasize the\ncritical role of high-frequency components in adversarial attacks, and (2)\nHierarchical-Gradient Fusion, which merges multi-scale gradient decomposition\nand fusion to capture both global structures and fine-grained details,\nresulting in smoother perturbations. Our experiment demonstrates that FSA\nconsistently outperforms state-of-the-art methods across various black-box\nmodels. Notably, our proposed FSA achieves an average attack success rate\nincrease of 23.6% compared with BSR (CVPR 2024) on eight black-box defense\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6FSA\uff0c\u7ed3\u5408\u9891\u57df\u548c\u7a7a\u95f4\u57df\u53d8\u6362\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ed1\u76d2\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u57df\uff0c\u5ffd\u7565\u4e86\u9891\u57df\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0cFSA\u65e8\u5728\u901a\u8fc7\u9891\u57df\u589e\u5f3a\u548c\u591a\u5c3a\u5ea6\u68af\u5ea6\u878d\u5408\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "method": "FSA\u91c7\u7528\u9ad8\u9891\u589e\u5f3a\u548c\u5206\u5c42\u68af\u5ea6\u878d\u5408\u6280\u672f\uff0c\u524d\u8005\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u9009\u62e9\u6027\u653e\u5927\u9ad8\u9891\u6210\u5206\uff0c\u540e\u8005\u878d\u5408\u591a\u5c3a\u5ea6\u68af\u5ea6\u4ee5\u6355\u6349\u5168\u5c40\u548c\u7ec6\u8282\u4fe1\u606f\u3002", "result": "FSA\u5728\u591a\u4e2a\u9ed1\u76d2\u9632\u5fa1\u6a21\u578b\u4e0a\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08BSR\uff09\u63d0\u9ad8\u4e8623.6%\u3002", "conclusion": "FSA\u901a\u8fc7\u9891\u57df\u548c\u7a7a\u95f4\u57df\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u8f6c\u79fb\u6027\u548c\u6548\u679c\u3002", "relevance": 40.0}}
{"id": "2505.21182", "pdf": "https://arxiv.org/pdf/2505.21182", "abs": "https://arxiv.org/abs/2505.21182", "authors": ["Huy Hoang", "Tien Mai", "Pradeep Varakantham", "Tanvi Verma"], "title": "Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations", "categories": ["cs.LG", "cs.AI"], "comment": "preprint version", "summary": "Offline imitation learning typically learns from expert and unlabeled\ndemonstrations, yet often overlooks the valuable signal in explicitly\nundesirable behaviors. In this work, we study offline imitation learning from\ncontrasting behaviors, where the dataset contains both expert and undesirable\ndemonstrations. We propose a novel formulation that optimizes a difference of\nKL divergences over the state-action visitation distributions of expert and\nundesirable (or bad) data. Although the resulting objective is a DC\n(Difference-of-Convex) program, we prove that it becomes convex when expert\ndemonstrations outweigh undesirable demonstrations, enabling a practical and\nstable non-adversarial training objective. Our method avoids adversarial\ntraining and handles both positive and negative demonstrations in a unified\nframework. Extensive experiments on standard offline imitation learning\nbenchmarks demonstrate that our approach consistently outperforms\nstate-of-the-art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u4e13\u5bb6\u548c\u4e0d\u826f\u884c\u4e3a\u6570\u636e\uff0c\u901a\u8fc7\u4f18\u5316KL\u6563\u5ea6\u5dee\u5f02\uff0c\u907f\u514d\u4e86\u5bf9\u6297\u8bad\u7ec3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u901a\u5e38\u5ffd\u7565\u4e0d\u826f\u884c\u4e3a\u7684\u4fe1\u53f7\uff0c\u800c\u672c\u6587\u65e8\u5728\u5229\u7528\u4e13\u5bb6\u548c\u4e0d\u826f\u884c\u4e3a\u7684\u5bf9\u6bd4\u6570\u636e\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eKL\u6563\u5ea6\u5dee\u5f02\u7684\u4f18\u5316\u76ee\u6807\uff0c\u5f62\u6210DC\u7a0b\u5e8f\uff0c\u5e76\u5728\u4e13\u5bb6\u6570\u636e\u5360\u4f18\u65f6\u8bc1\u660e\u5176\u51f8\u6027\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u5904\u7406\u6b63\u8d1f\u793a\u8303\u6570\u636e\uff0c\u907f\u514d\u4e86\u5bf9\u6297\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.21148", "pdf": "https://arxiv.org/pdf/2505.21148", "abs": "https://arxiv.org/abs/2505.21148", "authors": ["Rao Ma", "Mengjie Qian", "Siyuan Tang", "Stefano Bann\u00f2", "Kate M. Knill", "Mark J. F. Gales"], "title": "Assessment of L2 Oral Proficiency using Speech Large Language Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech", "summary": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728L2\u53e3\u8bed\u80fd\u529b\u8bc4\u5206\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u56de\u5f52\u548c\u5206\u7c7b\u76ee\u6807\u6bd4\u8f83\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u679c\u663e\u793a\u8bed\u97f3LLMs\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740L2\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u589e\u52a0\uff0c\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u9700\u6c42\u4e0a\u5347\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u5c40\u9650\u6027\uff0c\u591a\u6a21\u6001LLMs\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u6bd4\u8f83\u4e86\u56de\u5f52\u548c\u5206\u7c7b\u76ee\u6807\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u591a\u6a21\u6001LLMs\u8fdb\u884c\u53e3\u8bed\u8bc4\u5206\u3002", "result": "\u8bed\u97f3LLMs\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u90e8\u5206\u548c\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u6a21\u6001LLMs\u5728\u53e3\u8bed\u8bc4\u5206\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5176\u9884\u8bad\u7ec3\u77e5\u8bc6\u6709\u52a9\u4e8e\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.21187", "pdf": "https://arxiv.org/pdf/2505.21187", "abs": "https://arxiv.org/abs/2505.21187", "authors": ["Hesam Araghi", "Jan van Gemert", "Nergis Tomen"], "title": "Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras offer high temporal resolution and power efficiency, making\nthem well-suited for edge AI applications. However, their high event rates\npresent challenges for data transmission and processing. Subsampling methods\nprovide a practical solution, but their effect on downstream visual tasks\nremains underexplored. In this work, we systematically evaluate six\nhardware-friendly subsampling methods using convolutional neural networks for\nevent video classification on various benchmark datasets. We hypothesize that\nevents from high-density regions carry more task-relevant information and are\ntherefore better suited for subsampling. To test this, we introduce a simple\ncausal density-based subsampling method, demonstrating improved classification\naccuracy in sparse regimes. Our analysis further highlights key factors\naffecting subsampling performance, including sensitivity to hyperparameters and\nfailure cases in scenarios with large event count variance. These findings\nprovide insights for utilization of hardware-efficient subsampling strategies\nthat balance data efficiency and task accuracy. The code for this paper will be\nreleased at: https://github.com/hesamaraghi/event-camera-subsampling-methods.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u786c\u4ef6\u53cb\u597d\u7684\u5b50\u91c7\u6837\u65b9\u6cd5\u5bf9\u4e8b\u4ef6\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5bc6\u5ea6\u7684\u5b50\u91c7\u6837\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7a00\u758f\u6761\u4ef6\u4e0b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u4e8b\u4ef6\u7387\u5bf9\u6570\u636e\u4f20\u8f93\u548c\u5904\u7406\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5b50\u91c7\u6837\u65b9\u6cd5\u662f\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5bf9\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u516d\u79cd\u5b50\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5bc6\u5ea6\u7684\u5b50\u91c7\u6837\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7a00\u758f\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u5b50\u91c7\u6837\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5e73\u8861\u6570\u636e\u6548\u7387\u548c\u4efb\u52a1\u51c6\u786e\u6027\u7684\u786c\u4ef6\u9ad8\u6548\u5b50\u91c7\u6837\u7b56\u7565\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "relevance": 30.0}}
{"id": "2505.21184", "pdf": "https://arxiv.org/pdf/2505.21184", "abs": "https://arxiv.org/abs/2505.21184", "authors": ["Yu Yan", "Sheng Sun", "Zhifei Zheng", "Ziji Hao", "Teli Liu", "Min Liu"], "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.", "AI": {"tldr": "\u63d0\u51faPoisonSwarm\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u4f17\u5305\u7b56\u7565\u9ad8\u6548\u751f\u6210\u591a\u6837\u5316\u7684\u6709\u5bb3\u4fe1\u606f\u6570\u636e\uff0c\u89e3\u51b3\u73b0\u6709LLM\u751f\u6210\u6709\u5bb3\u6570\u636e\u65f6\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56LLM\u5408\u6210\u6709\u5bb3\u6570\u636e\uff0c\u4f46\u53d7\u9650\u4e8e\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u751f\u6210\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u751f\u6210\u826f\u6027\u6570\u636e\u6a21\u677f\uff0c\u5206\u89e3\u4e3a\u8bed\u4e49\u5355\u5143\u540e\u9010\u5355\u5143\u6bd2\u5316\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u5207\u6362\u4f18\u5316\u5408\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePoisonSwarm\u5728\u6709\u5bb3\u6570\u636e\u5408\u6210\u7684\u591a\u6837\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "PoisonSwarm\u4e3a\u6709\u5bb3\u6570\u636e\u5408\u6210\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u66f4\u5b89\u5168\u7684AI\u5e94\u7528\u5f00\u53d1\u3002", "relevance": 85.0}}
{"id": "2505.21171", "pdf": "https://arxiv.org/pdf/2505.21171", "abs": "https://arxiv.org/abs/2505.21171", "authors": ["Rochelle Choenni", "Ivan Titov"], "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7a00\u758f\u5316\uff08pruning\uff09\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-Wanda\u7684\u4fee\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u611f\u77e5\u7684\u6fc0\u6d3b\u7edf\u8ba1\u548c\u52a8\u6001\u8c03\u6574\u5c42\u95f4\u7a00\u758f\u5ea6\u6765\u4f18\u5316\u591a\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u591a\u8bed\u8a00LLM\u7684\u6027\u80fd\u901a\u5e38\u4f9d\u8d56\u4e8e\u6a21\u578b\u89c4\u6a21\uff0c\u800c\u4fee\u526a\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\uff0c\u4f46\u5f80\u5f80\u4f34\u968f\u6027\u80fd\u635f\u5931\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u591a\u8bed\u8a00\u6027\u80fd\u4e0e\u7a00\u758f\u5316\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u662f\u672c\u6587\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u63d0\u51faM-Wanda\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u611f\u77e5\u7684\u6fc0\u6d3b\u7edf\u8ba1\u548c\u52a8\u6001\u8c03\u6574\u5c42\u95f4\u7a00\u758f\u5ea6\u6765\u4f18\u5316\u4fee\u526a\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u591a\u8bed\u8a00\u6027\u80fd\u635f\u5931\u3002", "result": "M-Wanda\u5728\u4fdd\u6301\u8f83\u4f4e\u989d\u5916\u6210\u672c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u660e\u786e\u4f18\u5316\u4fee\u526a\u65b9\u6cd5\u4ee5\u4fdd\u7559\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00\u4fee\u526a\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u53d1\u3002", "relevance": 75.0}}
{"id": "2505.21200", "pdf": "https://arxiv.org/pdf/2505.21200", "abs": "https://arxiv.org/abs/2505.21200", "authors": ["Xudong Tan", "Yaoxin Yang", "Peng Ye", "Jialin Zheng", "Bizhe Bai", "Xinyi Wang", "Jia Hao", "Tao Chen"], "title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining.", "AI": {"tldr": "FlashVLA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u91cd\u7528\u548c\u89c6\u89c9\u4ee4\u724c\u526a\u679d\uff0c\u663e\u8457\u964d\u4f4eVLA\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "VLA\u6a21\u578b\u7684\u9ad8\u63a8\u7406\u6210\u672c\uff08\u6e90\u4e8e\u5927\u89c4\u6a21\u4ee4\u724c\u8ba1\u7b97\u548c\u81ea\u56de\u5f52\u89e3\u7801\uff09\u9650\u5236\u4e86\u5176\u5b9e\u65f6\u90e8\u7f72\u548c\u8fb9\u7f18\u5e94\u7528\uff0c\u4f5c\u8005\u53d1\u73b0\u52a8\u4f5c\u6b65\u9aa4\u95f4\u7684\u9ad8\u76f8\u4f3c\u6027\u548c\u89c6\u89c9\u4ee4\u724c\u7684\u5197\u4f59\u6027\uff0c\u63d0\u51fa\u4e86FlashVLA\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "FlashVLA\u91c7\u7528\u4ee4\u724c\u611f\u77e5\u7684\u52a8\u4f5c\u91cd\u7528\u673a\u5236\u907f\u514d\u7a33\u5b9a\u52a8\u4f5c\u6b65\u9aa4\u7684\u5197\u4f59\u89e3\u7801\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u5f15\u5bfc\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u7b56\u7565\u526a\u679d\u4f4e\u8d21\u732e\u4ee4\u724c\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlashVLA\u51cf\u5c11\u4e8655.7%\u7684FLOPs\u548c36.0%\u7684\u5ef6\u8fdf\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4ec5\u4e0b\u964d0.7%\u3002", "conclusion": "FlashVLA\u6709\u6548\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u3001\u4f4e\u5ef6\u8fdf\u7684VLA\u63a8\u7406\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "relevance": 40.0}}
{"id": "2505.21201", "pdf": "https://arxiv.org/pdf/2505.21201", "abs": "https://arxiv.org/abs/2505.21201", "authors": ["Steven Sam", "Silima Marshal DAbreo"], "title": "Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection", "categories": ["cs.LG"], "comment": "22 pages and 13 figures", "summary": "Agriculture constitutes a primary source of food production, economic growth\nand employment in India, but the sector is confronted with low farm\nproductivity and yields aggravated by increased pressure on natural resources\nand adverse climate change variability. Efforts involving green revolution,\nland irrigations, improved seeds and organic farming have yielded suboptimal\noutcomes. The adoption of computational tools like crop recommendation systems\noffers a new way to provide insights and help farmers tackle low productivity.\nHowever, most agricultural recommendation systems in India focus narrowly on\nenvironmental factors and regions, limiting accurate predictions of high-yield,\nprofitable crops. This study uses environmental and economic factors with 19\ncrops across 15 states to develop and evaluate Random Forest and SVM models\nusing 10-fold Cross Validation, Time-series Split, and Lag Variables. The\n10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but\nraised overfitting concerns. Introducing temporal order, better reflecting\nreal-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the\nTime-series Split.To further increase the model accuracy while maintaining the\ntemporal order, the Lag Variables approach was employed, which resulted in\nimproved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross\nvalidation approach. Overall, the models in the Time-series Split and Lag\nVariable Approaches offer practical insights by handling temporal dependencies\nand enhancing its adaptability to changing agricultural conditions over time.\nConsequently, the study shows the Random Forest model developed based on the\nLag Variables as the most preferred algorithm for optimal crop recommendation\nin the Indian context.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u968f\u673a\u68ee\u6797\u548cSVM\u6a21\u578b\uff0c\u7ed3\u5408\u73af\u5883\u4e0e\u7ecf\u6d4e\u56e0\u7d20\uff0c\u4e3a\u5370\u5ea6\u519c\u4e1a\u63d0\u4f9b\u4f5c\u7269\u63a8\u8350\u3002\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u548c\u6ede\u540e\u53d8\u91cf\u65b9\u6cd5\u4f18\u5316\u6a21\u578b\uff0c\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5370\u5ea6\u519c\u4e1a\u56e0\u8d44\u6e90\u538b\u529b\u548c\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u7684\u4f4e\u4ea7\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u66f4\u7cbe\u51c6\u7684\u4f5c\u7269\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548cSVM\u6a21\u578b\uff0c\u7ed3\u540810\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u548c\u6ede\u540e\u53d8\u91cf\u65b9\u6cd5\uff0c\u8bc4\u4f3019\u79cd\u4f5c\u7269\u572815\u4e2a\u5dde\u7684\u8868\u73b0\u3002", "result": "\u968f\u673a\u68ee\u6797\u5728\u6ede\u540e\u53d8\u91cf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0883.62%\uff09\uff0c\u4f18\u4e8eSVM\uff0874.38%\uff09\uff0c\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u964d\u4f4e\u4e86\u8fc7\u62df\u5408\u98ce\u9669\u3002", "conclusion": "\u6ede\u540e\u53d8\u91cf\u65b9\u6cd5\u7ed3\u5408\u968f\u673a\u68ee\u6797\u6a21\u578b\u6700\u9002\u5408\u5370\u5ea6\u519c\u4e1a\u7684\u4f5c\u7269\u63a8\u8350\uff0c\u80fd\u9002\u5e94\u52a8\u6001\u73af\u5883\u53d8\u5316\u3002", "relevance": 30.0}}
{"id": "2505.21172", "pdf": "https://arxiv.org/pdf/2505.21172", "abs": "https://arxiv.org/abs/2505.21172", "authors": ["Zheng Li", "Mao Zheng", "Mingyang Song", "Wenjie Yang"], "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTAT-R1\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8bcd\u5bf9\u9f50\u63d0\u5347\u672f\u8bed\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u63a8\u7406LLMs\u5728\u672f\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u5c1a\u672a\u63a2\u7d22\uff0c\u800c\u672f\u8bed\u7ffb\u8bd1\u5bf9\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u8bcd\u5bf9\u9f50\u6a21\u578b\u63d0\u53d6\u5173\u952e\u8bcd\u7ffb\u8bd1\u5bf9\uff0c\u8bbe\u8ba1\u4e09\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7ffb\u8bd1\u6a21\u578b\u3002", "result": "TAT-R1\u663e\u8457\u63d0\u5347\u672f\u8bed\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u901a\u7528\u7ffb\u8bd1\u6027\u80fd\u4e0e\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "TAT-R1\u6709\u6548\u89e3\u51b3\u4e86\u672f\u8bed\u7ffb\u8bd1\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86DeepSeek-R1\u8bad\u7ec3\u8303\u5f0f\u7684\u5173\u952e\u53d1\u73b0\u3002", "relevance": 85.0}}
{"id": "2505.21205", "pdf": "https://arxiv.org/pdf/2505.21205", "abs": "https://arxiv.org/abs/2505.21205", "authors": ["Liuhan Chen", "Xiaodong Cun", "Xiaoyu Li", "Xianyi He", "Shenghai Yuan", "Jie Chen", "Ying Shan", "Li Yuan"], "title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening", "categories": ["cs.CV"], "comment": "22 pages, 9 figures", "summary": "Frame inbetweening aims to synthesize intermediate video sequences\nconditioned on the given start and end frames. Current state-of-the-art methods\nmainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)\nby incorporating end-frame constraints via directly fine-tuning or omitting\ntraining. We identify a critical limitation in their design: Their injections\nof the end-frame constraint usually utilize the same mechanism that originally\nimposed the start-frame (single image) constraint. However, since the original\nI2V-DMs are adequately trained for the start-frame condition in advance,\nnaively introducing the end-frame constraint by the same mechanism with much\nless (even zero) specialized training probably can't make the end frame have a\nstrong enough impact on the intermediate content like the start frame. This\nasymmetric control strength of the two frames over the intermediate content\nlikely leads to inconsistent motion or appearance collapse in generated frames.\nTo efficiently achieve symmetric constraints of start and end frames, we\npropose a novel framework, termed Sci-Fi, which applies a stronger injection\nfor the constraint of a smaller training scale. Specifically, it deals with the\nstart-frame constraint as before, while introducing the end-frame constraint by\nan improved mechanism. The new mechanism is based on a well-designed\nlightweight module, named EF-Net, which encodes only the end frame and expands\nit into temporally adaptive frame-wise features injected into the I2V-DM. This\nmakes the end-frame constraint as strong as the start-frame constraint,\nenabling our Sci-Fi to produce more harmonious transitions in various\nscenarios. Extensive experiments prove the superiority of our Sci-Fi compared\nwith other baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSci-Fi\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u673a\u5236\u5b9e\u73b0\u89c6\u9891\u5e27\u95f4\u5408\u6210\u7684\u5bf9\u79f0\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u56e0\u4e0d\u5bf9\u79f0\u7ea6\u675f\u5bfc\u81f4\u7684\u8fd0\u52a8\u4e0d\u4e00\u81f4\u6216\u5916\u89c2\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u5e27\u95f4\u5408\u6210\u4e2d\uff0c\u7531\u4e8e\u5bf9\u8d77\u59cb\u5e27\u548c\u7ed3\u675f\u5e27\u7684\u7ea6\u675f\u673a\u5236\u4e0d\u5bf9\u79f0\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u7684\u4e0d\u4e00\u81f4\u6216\u5d29\u6e83\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5bf9\u79f0\u7ea6\u675f\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faSci-Fi\u6846\u67b6\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u6a21\u5757EF-Net\uff0c\u4e13\u95e8\u7f16\u7801\u7ed3\u675f\u5e27\u5e76\u6269\u5c55\u4e3a\u65f6\u95f4\u81ea\u9002\u5e94\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a\u7ed3\u675f\u5e27\u7684\u7ea6\u675f\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSci-Fi\u5728\u591a\u79cd\u573a\u666f\u4e0b\u751f\u6210\u66f4\u548c\u8c10\u7684\u8fc7\u6e21\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Sci-Fi\u901a\u8fc7\u5bf9\u79f0\u7ea6\u675f\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u5e27\u95f4\u5408\u6210\u7684\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.21204", "pdf": "https://arxiv.org/pdf/2505.21204", "abs": "https://arxiv.org/abs/2505.21204", "authors": ["Marie Steinacker", "Yuri Kheifetz", "Markus Scholz"], "title": "Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics", "categories": ["cs.LG", "q-bio.QM", "I.6.5; J.3"], "comment": null, "summary": "Hematotoxicity, drug-induced damage to the blood-forming system, is a\nfrequent side effect of cytotoxic chemotherapy and poses a significant\nchallenge in clinical practice due to its high inter-patient variability and\nlimited predictability. Current mechanistic models often struggle to accurately\nforecast outcomes for patients with irregular or atypical trajectories. In this\nstudy, we develop and compare hybrid mechanistic and data-driven approaches for\nindividualized time series modeling of platelet counts during chemotherapy. We\nconsider hybrid models that combine mechanistic models with neural networks,\nknown as universal differential equations. As a purely data-driven alternative,\nwe utilize a nonlinear autoregressive exogenous model using gated recurrent\nunits as the underlying architecture. These models are evaluated across a range\nof real patient scenarios, varying in data availability and sparsity, to assess\npredictive performance. Our findings demonstrate that data-driven methods, when\nprovided with sufficient data, significantly improve prediction accuracy,\nparticularly for high-risk patients with irregular platelet dynamics. This\nhighlights the potential of data-driven approaches in enhancing clinical\ndecision-making. In contrast, hybrid and mechanistic models are superior in\nscenarios with limited or sparse data. The proposed modeling and comparison\nframework is generalizable and could be extended to predict other\ntreatment-related toxicities, offering broad applicability in personalized\nmedicine.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5316\u7597\u671f\u95f4\u8840\u5c0f\u677f\u8ba1\u6570\u7684\u4e2a\u6027\u5316\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0c\u6bd4\u8f83\u4e86\u6df7\u5408\u673a\u5236\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u53d1\u73b0\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6570\u636e\u5145\u8db3\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u758f\u65f6\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u5316\u7597\u5f15\u8d77\u7684\u8840\u6db2\u6bd2\u6027\u9884\u6d4b\u95ee\u9898\uff0c\u5c24\u5176\u662f\u60a3\u8005\u95f4\u5dee\u5f02\u5927\u548c\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u673a\u5236\u6a21\u578b\u4e0e\u795e\u7ecf\u7f51\u7edc\uff08\u901a\u7528\u5fae\u5206\u65b9\u7a0b\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u95e8\u63a7\u5faa\u73af\u5355\u5143\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6570\u636e\u5145\u8db3\u65f6\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u758f\u65f6\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u6709\u6f5c\u529b\uff0c\u6df7\u5408\u65b9\u6cd5\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\uff0c\u6846\u67b6\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u6bd2\u6027\u9884\u6d4b\u3002", "relevance": 20.0}}
{"id": "2505.20573", "pdf": "https://arxiv.org/pdf/2505.20573", "abs": "https://arxiv.org/abs/2505.20573", "authors": ["Jiabao Ji", "Yongchao Chen", "Yang Zhang", "Ramana Rao Kompella", "Chuchu Fan", "Gaowen Liu", "Shiyu Chang"], "title": "Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in various\nrobot control tasks. However, their deployment in real-world applications\nremains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently\nproduce invalid action plans that violate physical constraints, such as\ndirecting a robot to an unreachable location or causing collisions between\nrobots. This issue primarily arises from a lack of awareness of these physical\nconstraints during the reasoning process. To address this issue, we propose a\nnovel framework that integrates reinforcement learning with verifiable rewards\n(RLVR) to incentivize knowledge of physical constraints into LLMs to induce\nconstraints-aware reasoning during plan generation. In this approach, only\nvalid action plans that successfully complete a control task receive positive\nrewards. We applied our method to two small-scale LLMs: a non-reasoning\nQwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results\ndemonstrate that constraint-aware small LLMs largely outperform large-scale\nmodels without constraints, grounded on both the BoxNet task and a newly\ndeveloped BoxNet3D environment built using MuJoCo. This work highlights the\neffectiveness of grounding even small LLMs with physical constraints to enable\nscalable and efficient multi-robot control in complex, physically constrained\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3aLLMs\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u751f\u6210\u66f4\u6709\u6548\u7684\u673a\u5668\u4eba\u63a7\u5236\u8ba1\u5212\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u5e38\u751f\u6210\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u884c\u52a8\u8ba1\u5212\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faRLVR\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u673a\u5236\u6fc0\u52b1LLMs\u5b66\u4e60\u7269\u7406\u7ea6\u675f\uff0c\u751f\u6210\u6709\u6548\u884c\u52a8\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ea6\u675f\u611f\u77e5\u7684\u5c0f\u578bLLMs\u5728BoxNet\u548cBoxNet3D\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u65e0\u7ea6\u675f\u7684\u5927\u578b\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u7684\u589e\u5f3a\uff0c\u5c0f\u578bLLMs\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u591a\u673a\u5668\u4eba\u63a7\u5236\u3002", "relevance": 75.0}}
{"id": "2505.21178", "pdf": "https://arxiv.org/pdf/2505.21178", "abs": "https://arxiv.org/abs/2505.21178", "authors": ["Mingyang Song", "Mao Zheng"], "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Ongoing Work", "summary": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConciseR\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u80fd\u529b\u548c\u7b80\u6d01\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff08\u5197\u4f59\u6216\u91cd\u590d\u6a21\u5f0f\uff09\uff0c\u5f71\u54cd\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ConciseR\uff1a\u7b2c\u4e00\u9636\u6bb5\uff08GRPO++\uff09\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\uff08L-GRPO\uff09\u4f18\u5316\u7b80\u6d01\u6027\u3002", "result": "ConciseR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08AIME 2024\u3001MATH-500\u7b49\uff09\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u751f\u6210\u66f4\u7b80\u6d01\u7684\u63a8\u7406\u54cd\u5e94\u3002", "conclusion": "ConciseR\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.21228", "pdf": "https://arxiv.org/pdf/2505.21228", "abs": "https://arxiv.org/abs/2505.21228", "authors": ["Alvaro Gonzalez-Jimenez", "Simone Lionetti", "Ludovic Amruthalingam", "Philippe Gottfrois", "Fabian Gr\u00f6ger", "Marc Pouly", "Alexander A. Navarini"], "title": "Is Hyperbolic Space All You Need for Medical Anomaly Detection?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Provisionally Accepted at MICCAI 2025", "summary": "Medical anomaly detection has emerged as a promising solution to challenges\nin data availability and labeling constraints. Traditional methods extract\nfeatures from different layers of pre-trained networks in Euclidean space;\nhowever, Euclidean representations fail to effectively capture the hierarchical\nrelationships within these features, leading to suboptimal anomaly detection\nperformance. We propose a novel yet simple approach that projects feature\nrepresentations into hyperbolic space, aggregates them based on confidence\nlevels, and classifies samples as healthy or anomalous. Our experiments\ndemonstrate that hyperbolic space consistently outperforms Euclidean-based\nframeworks, achieving higher AUROC scores at both image and pixel levels across\nmultiple medical benchmark datasets. Additionally, we show that hyperbolic\nspace exhibits resilience to parameter variations and excels in few-shot\nscenarios, where healthy images are scarce. These findings underscore the\npotential of hyperbolic space as a powerful alternative for medical anomaly\ndetection. The project website can be found at\nhttps://hyperbolic-anomalies.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7279\u5f81\u8868\u793a\u6295\u5f71\u5230\u53cc\u66f2\u7a7a\u95f4\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u548c\u6807\u6ce8\u9650\u5236\u7684\u6311\u6218\uff0c\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u7279\u5f81\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5c06\u7279\u5f81\u8868\u793a\u6295\u5f71\u5230\u53cc\u66f2\u7a7a\u95f4\uff0c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u805a\u5408\u7279\u5f81\uff0c\u5e76\u5206\u7c7b\u4e3a\u5065\u5eb7\u6216\u5f02\u5e38\u6837\u672c\u3002", "result": "\u53cc\u66f2\u7a7a\u95f4\u65b9\u6cd5\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u65b9\u6cd5\uff0cAUROC\u5f97\u5206\u66f4\u9ad8\uff0c\u4e14\u5bf9\u53c2\u6570\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u53cc\u66f2\u7a7a\u95f4\u662f\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u7684\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.21219", "pdf": "https://arxiv.org/pdf/2505.21219", "abs": "https://arxiv.org/abs/2505.21219", "authors": ["Qinjun Fei", "Nuria Rodr\u00edguez-Barroso", "Mar\u00eda Victoria Luz\u00f3n", "Zhongliang Zhang", "Francisco Herrera"], "title": "Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In cross-silo Federated Learning (FL), client selection is critical to ensure\nhigh model performance, yet it remains challenging due to data quality\ndecompensation, budget constraints, and incentive compatibility. As training\nprogresses, these factors exacerbate client heterogeneity and degrade global\nperformance. Most existing approaches treat these challenges in isolation,\nmaking jointly optimizing multiple factors difficult. To address this, we\npropose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a\nunified framework integrating dynamic bidding, reputation modeling, and\ncost-aware selection. Clients submit bids based on their perceived data\nquality, and their contributions are evaluated using Shapley values to quantify\ntheir marginal impact on the global model. A reputation system, inspired by\nprospect theory, captures historical performance while penalizing\ninconsistency. The client selection problem is formulated as a 0-1 integer\nprogram that maximizes reputation-weighted utility under budget constraints.\nExperiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that\nSBRO-FL improves accuracy, convergence speed, and robustness, even in\nadversarial and low-bid interference scenarios. Our results highlight the\nimportance of balancing data reliability, incentive compatibility, and cost\nefficiency to enable scalable and trustworthy FL deployments.", "AI": {"tldr": "SBRO-FL\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6295\u6807\u3001\u58f0\u8a89\u5efa\u6a21\u548c\u6210\u672c\u611f\u77e5\u9009\u62e9\uff0c\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u9009\u62e9\u56e0\u6570\u636e\u8d28\u91cf\u3001\u9884\u7b97\u548c\u6fc0\u52b1\u517c\u5bb9\u6027\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7ed3\u5408Shapley\u503c\u8bc4\u4f30\u5ba2\u6237\u7aef\u8d21\u732e\uff0c\u58f0\u8a89\u7cfb\u7edf\u57fa\u4e8e\u5386\u53f2\u8868\u73b0\uff0c\u901a\u8fc70-1\u6574\u6570\u89c4\u5212\u9009\u62e9\u5ba2\u6237\u7aef\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SBRO-FL\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u5e73\u8861\u6570\u636e\u53ef\u9760\u6027\u3001\u6fc0\u52b1\u517c\u5bb9\u6027\u548c\u6210\u672c\u6548\u7387\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u53ef\u4fe1\u8054\u90a6\u5b66\u4e60\u7684\u5173\u952e\u3002", "relevance": 30.0}}
{"id": "2505.21189", "pdf": "https://arxiv.org/pdf/2505.21189", "abs": "https://arxiv.org/abs/2505.21189", "authors": ["Gleb Mezentsev", "Ivan Oseledets"], "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u901a\u8fc7\u4ec5\u4e24\u4e2a\u5b66\u4e60\u5d4c\u5165\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u6570\u767e\u4e2a\u51c6\u786e\u6807\u8bb0\uff0c\u63ed\u793a\u4e86LLMs\u65e0\u9700\u8fed\u4ee3\u89e3\u7801\u7684\u591a\u6807\u8bb0\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5728\u4e0d\u81ea\u56de\u5f52\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6587\u672c\u91cd\u6784\uff0c\u63ed\u793aLLMs\u7684\u6f5c\u5728\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684LLMs\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b66\u4e60\u5d4c\u5165\u8fdb\u884c\u5355\u6b21\u524d\u5411\u4f20\u9012\u751f\u6210\u591a\u6807\u8bb0\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u6570\u767e\u4e2a\u51c6\u786e\u6807\u8bb0\uff0c\u5d4c\u5165\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f62\u6210\u8fde\u901a\u4e14\u5c40\u90e8\u7684\u533a\u57df\u3002", "conclusion": "LLMs\u5177\u6709\u65e0\u9700\u8fed\u4ee3\u89e3\u7801\u7684\u591a\u6807\u8bb0\u751f\u6210\u80fd\u529b\uff0c\u5d4c\u5165\u7a7a\u95f4\u7684\u6027\u8d28\u8868\u660e\u53ef\u4ee5\u5b66\u4e60\u4e13\u7528\u7f16\u7801\u5668\u3002", "relevance": 85.0}}
{"id": "2505.21231", "pdf": "https://arxiv.org/pdf/2505.21231", "abs": "https://arxiv.org/abs/2505.21231", "authors": ["Lintao Xu", "Yinghao Wang", "Chaohui Wang"], "title": "Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning", "categories": ["cs.CV"], "comment": "7 pages, 4 tables, 4 figures", "summary": "Occlusion Boundary Estimation (OBE) identifies boundaries arising from both\ninter-object occlusions and self-occlusion within individual objects,\ndistinguishing intrinsic object edges from occlusion-induced contours to\nimprove scene understanding and 3D reconstruction capacity. This is closely\nrelated to Monocular Depth Estimation (MDE), which infers depth from a single\nimage, as occlusion boundaries provide critical geometric cues for resolving\ndepth ambiguities, while depth priors can conversely refine occlusion reasoning\nin complex scenes. In this paper, we propose a novel network, MoDOT, that first\njointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale\nstrip convolution module, leverages mid-level OB features to significantly\nenhance depth prediction. Additionally, we introduce an occlusion-aware loss\nfunction, OBDCL, which encourages sharper and more accurate depth boundaries.\nExtensive experiments on both real and synthetic datasets demonstrate the\nmutual benefits of jointly estimating depth and OB, and highlight the\neffectiveness of our model design. Our method achieves the state-of-the-art\n(SOTA) on both our proposed synthetic datasets and one popular real dataset,\nNYUD-v2, significantly outperforming multi-task baselines. Besides, without\ndomain adaptation, results on real-world depth transfer are comparable to the\ncompetitors, while preserving sharp occlusion boundaries for geometric\nfidelity. We will release our code, pre-trained models, and datasets to support\nfuture research in this direction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoDOT\u7684\u7f51\u7edc\uff0c\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u548c\u906e\u6321\u8fb9\u754c\uff08OB\uff09\uff0c\u901a\u8fc7CASM\u6a21\u5757\u548cOBDCL\u635f\u5931\u51fd\u6570\u63d0\u5347\u6df1\u5ea6\u9884\u6d4b\u548c\u8fb9\u754c\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u906e\u6321\u8fb9\u754c\uff08OB\uff09\u548c\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u5728\u573a\u666f\u7406\u89e3\u548c3D\u91cd\u5efa\u4e2d\u76f8\u4e92\u4fc3\u8fdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8054\u5408\u5229\u7528\u4e24\u8005\u4fe1\u606f\u3002", "method": "\u63d0\u51faMoDOT\u7f51\u7edc\uff0c\u7ed3\u5408CASM\u6a21\u5757\uff08\u8de8\u6ce8\u610f\u529b\u591a\u5c3a\u5ea6\u5377\u79ef\uff09\u548cOBDCL\u635f\u5931\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u6df1\u5ea6\u548cOB\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548cNYUD-v2\u4e0a\u8fbe\u5230SOTA\uff0c\u6df1\u5ea6\u8fc1\u79fb\u6548\u679c\u4e0e\u7ade\u4e89\u5bf9\u624b\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u548cOB\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cMoDOT\u7684\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21226", "pdf": "https://arxiv.org/pdf/2505.21226", "abs": "https://arxiv.org/abs/2505.21226", "authors": ["Zijing Wang", "Xingle Xu", "Yongkang Liu", "Yiqun Zhang", "Peiqin Lin", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Hinrich Sch\u00fctze"], "title": "Why Do More Experts Fail? A Theoretical Analysis of Model Merging", "categories": ["cs.LG"], "comment": null, "summary": "Model merging dramatically reduces storage and computational resources by\ncombining multiple expert models into a single multi-task model. Although\nrecent model merging methods have shown promising results, they struggle to\nmaintain performance gains as the number of merged models increases. In this\npaper, we investigate the key obstacles that limit the scalability of model\nmerging when integrating a large number of expert models. First, we prove that\nthere is an upper bound on model merging. Further theoretical analysis reveals\nthat the limited effective parameter space imposes a strict constraint on the\nnumber of models that can be successfully merged. Gaussian Width shows that the\nmarginal benefit of merging additional models diminishes according to a\nstrictly concave function. This implies that the effective parameter space\nbecomes rapidly saturated as the number of merged models increases.\nFurthermore, using Approximate Kinematics Theory, we prove the existence of a\nunique optimal threshold beyond which adding more models does not yield\nsignificant performance improvements. At the same time, we introduce a\nstraightforward Reparameterized Heavy-Tailed method (RHT) to extend the\ncoverage of the merged model, thereby enhancing its performance. Empirical\nresults on 12 benchmarks, including both knowledge-intensive and\ngeneral-purpose tasks, validate our theoretical analysis. We believe that these\nresults spark further research beyond the current scope of model merging. The\nsource code is in the anonymous Github repository\nhttps://github.com/wzj1718/ModelMergingAnalysis.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6a21\u578b\u5408\u5e76\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5408\u5e76\u6a21\u578b\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u63d0\u5347\u53d7\u9650\u7684\u7406\u8bba\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08RHT\uff09\u6765\u4f18\u5316\u5408\u5e76\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u5408\u5e76\u5927\u91cf\u4e13\u5bb6\u6a21\u578b\u65f6\u6027\u80fd\u63d0\u5347\u53d7\u9650\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff08\u5982\u9ad8\u65af\u5bbd\u5ea6\u548c\u8fd1\u4f3c\u8fd0\u52a8\u5b66\u7406\u8bba\uff09\u8bc1\u660e\u6a21\u578b\u5408\u5e76\u7684\u4e0a\u9650\uff0c\u5e76\u63d0\u51faReparameterized Heavy-Tailed (RHT)\u65b9\u6cd5\u4f18\u5316\u5408\u5e76\u6548\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u5408\u5e76\u6a21\u578b\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u63d0\u5347\u7684\u4e25\u683c\u51f9\u51fd\u6570\u5173\u7cfb\uff0cRHT\u65b9\u6cd5\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6a21\u578b\u5408\u5e76\u5b58\u5728\u4e0a\u9650\uff0cRHT\u65b9\u6cd5\u80fd\u6709\u6548\u6269\u5c55\u5408\u5e76\u6a21\u578b\u7684\u8986\u76d6\u8303\u56f4\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 75.0}}
{"id": "2505.21190", "pdf": "https://arxiv.org/pdf/2505.21190", "abs": "https://arxiv.org/abs/2505.21190", "authors": ["Jong Hak Moon", "Geon Choi", "Paloma Rabaey", "Min Gwan Kim", "Hyuk Gi Hong", "Jung-Oh Lee", "Hangyul Yoon", "Eun Woo Doe", "Jiyoun Kim", "Harshita Sharma", "Daniel C. Castro", "Javier Alvarez-Valle", "Edward Choi"], "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LUNGUAGE\u57fa\u51c6\u6570\u636e\u96c6\u548cLUNGUAGESCORE\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u652f\u6301\u5355\u62a5\u544a\u548c\u7eb5\u5411\u60a3\u8005\u7ea7\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u653e\u5c04\u5b66\u62a5\u544a\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u62a5\u544a\u573a\u666f\u4e14\u4f9d\u8d56\u7c97\u7c92\u5ea6\u6307\u6807\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u4e34\u5e8a\u8bed\u4e49\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u751f\u6210\u7684\u62a5\u544a\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u63d0\u51faLUNGUAGESCORE\u8bc4\u4f30\u6307\u6807\u3002", "result": "LUNGUAGESCORE\u80fd\u6709\u6548\u652f\u6301\u7ed3\u6784\u5316\u62a5\u544a\u8bc4\u4f30\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5e8f\u5217\u653e\u5c04\u5b66\u62a5\u544a\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3001\u7ed3\u6784\u5316\u6846\u67b6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "relevance": 40.0}}
{"id": "2505.21233", "pdf": "https://arxiv.org/pdf/2505.21233", "abs": "https://arxiv.org/abs/2505.21233", "authors": ["Jiawei Guo", "Feifei Zhai", "Pu Jian", "Qianrun Wei", "Yu Zhou"], "title": "CROP: Contextual Region-Oriented Visual Token Pruning", "categories": ["cs.CV"], "comment": null, "summary": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCROP\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4f4d\u548c\u526a\u679d\u4e24\u6b65\u538b\u7f29\u89c6\u89c9\u4ee4\u724c\uff0c\u4ee5\u51cf\u5c11\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eVLM\u7684VQA\u65b9\u6cd5\u5904\u7406\u6574\u4e2a\u56fe\u50cf\uff0c\u5bfc\u81f4\u5927\u91cf\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\uff0c\u589e\u52a0\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "method": "CROP\u901a\u8fc7\u4e24\u6b65\u5b9e\u73b0\uff1a1) \u5b9a\u4f4d\u8f93\u5165\u67e5\u8be2\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u533a\u57df\uff1b2) \u91c7\u7528\u4e24\u79cd\u526a\u679d\u7b56\u7565\uff08PLC\u548cILP\uff09\u538b\u7f29\u4ee4\u724c\u3002", "result": "CROP\u5728\u591a\u79cdVQA\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "CROP\u6709\u6548\u51cf\u5c11\u4e86\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\uff0c\u63d0\u5347\u4e86VQA\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.21236", "pdf": "https://arxiv.org/pdf/2505.21236", "abs": "https://arxiv.org/abs/2505.21236", "authors": ["Felix Chalumeau", "Daniel Rajaonarivonivelomanantsoa", "Ruan de Kock", "Claude Formanek", "Sasha Abramowitz", "Oumayma Mahjoub", "Wiem Khlifi", "Simon Du Toit", "Louay Ben Nessir", "Refiloe Shabe", "Arnol Fokam", "Siddarth Singh", "Ulrich Mbou Sob", "Arnu Pretorius"], "title": "Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inf-marl.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u534745%\uff0c\u6700\u9ad8\u8fbe126%\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u5229\u7528\u63a8\u7406\u9636\u6bb5\u7684\u4f18\u5316\u7b56\u7565\u7a81\u7834\u6027\u80fd\u4e0a\u9650\u3002", "method": "\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u65f6\u95f4\u548c\u8ba1\u7b97\u9884\u7b97\uff0c\u63a2\u7d22\u591a\u79cd\u5c1d\u8bd5\u4ee5\u9009\u62e9\u6700\u4f18\u89e3\u3002", "result": "\u572817\u4e2a\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534745%\uff0c\u6700\u9ad8\u63d0\u5347126%\uff0c\u8ba1\u7b97\u6269\u5c55\u6027\u826f\u597d\u3002", "conclusion": "\u4f18\u5316\u63a8\u7406\u7b56\u7565\u662f\u7a81\u7834\u590d\u6742\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u74f6\u9888\u7684\u6709\u6548\u65b9\u6cd5\u3002", "relevance": 70.0}}
{"id": "2505.20600", "pdf": "https://arxiv.org/pdf/2505.20600", "abs": "https://arxiv.org/abs/2505.20600", "authors": ["Xiaoxiao Jiang", "Suyi Li", "Lingyun Yang", "Tianyu Feng", "Zhipeng Di", "Weiyi Lu", "Guoxuan Zhu", "Xiu Lin", "Kan Liu", "Yinghao Yu", "Tao Lan", "Guodong Yang", "Lin Qu", "Liping Zhang", "Wei Wang"], "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.", "AI": {"tldr": "InstGenIE\u662f\u4e00\u4e2a\u9ad8\u6548\u670d\u52a1\u56fe\u50cf\u7f16\u8f91\u8bf7\u6c42\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cd\u7528\u7f13\u5b58\u4e2d\u95f4\u6fc0\u6d3b\u3001\u6c14\u6ce1\u81ea\u7531\u6d41\u6c34\u7ebf\u65b9\u6848\u548c\u8fde\u7eed\u6279\u5904\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u670d\u52a1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u56fe\u50cf\u7f16\u8f91\u901a\u5e38\u6d89\u53ca\u63a9\u7801\u64cd\u4f5c\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5229\u7528\u63a9\u7801\u5e26\u6765\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u4f4e\u6548\u3002", "method": "InstGenIE\u901a\u8fc7\u91cd\u7528\u7f13\u5b58\u4e2d\u95f4\u6fc0\u6d3b\u3001\u6c14\u6ce1\u81ea\u7531\u6d41\u6c34\u7ebf\u65b9\u6848\u3001\u8fde\u7eed\u6279\u5904\u7406\u7b56\u7565\u548c\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u4f18\u5316\u6269\u6563\u6a21\u578b\u670d\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "result": "InstGenIE\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe3\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c14.7\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "InstGenIE\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u63a9\u7801\u7a00\u758f\u6027\u548c\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u670d\u52a1\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21191", "pdf": "https://arxiv.org/pdf/2505.21191", "abs": "https://arxiv.org/abs/2505.21191", "authors": ["Junyan Zhang", "Yubo Gao", "Yibo Yan", "Jungang Li", "Zhaorui Hou", "Sicheng Tao", "Shuliang Liu", "Song Dai", "Yonghua Hei", "Junzhuo Li", "Xuming Hu"], "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u5fae\u8c03\u5982\u4f55\u901a\u8fc7\u7a00\u758f\u7ec4\u4ef6\uff08\u795e\u7ecf\u5143\u548c\u4e13\u5bb6\uff09\u91cd\u65b0\u914d\u7f6e\u8ba1\u7b97\u673a\u5236\uff0c\u63d0\u51fa\u4e86HexaInst\u6570\u636e\u96c6\u548cSPARCOM\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6307\u4ee4\u6267\u884c\u7684\u5173\u952e\u673a\u5236\u3002", "motivation": "\u7406\u89e3LLM\u5fae\u8c03\u80cc\u540e\u7684\u8ba1\u7b97\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "method": "\u4f7f\u7528HexaInst\u6570\u636e\u96c6\u548cSPARCOM\u6846\u67b6\uff0c\u5206\u6790\u7a00\u758f\u7ec4\u4ef6\u7684\u529f\u80fd\u901a\u7528\u6027\u548c\u72ec\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u7a00\u758f\u7ec4\u4ef6\u5728\u6307\u4ee4\u6267\u884c\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u53ca\u5176\u529f\u80fd\u901a\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5982\u4f55\u5185\u5316\u6307\u4ee4\u8ddf\u968f\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u53ef\u4fe1\u8d56LLM\u7684\u53d1\u5c55\u3002", "relevance": 85.0}}
{"id": "2505.21238", "pdf": "https://arxiv.org/pdf/2505.21238", "abs": "https://arxiv.org/abs/2505.21238", "authors": ["Jieyu Yuan", "Yujun Li", "Yuanlin Zhang", "Chunle Guo", "Xiongxin Tang", "Ruixing Wang", "Chongyi Li"], "title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis for underwater scene reconstruction presents unique\nchallenges due to complex light-media interactions. Optical scattering and\nabsorption in water body bring inhomogeneous medium attenuation interference\nthat disrupts conventional volume rendering assumptions of uniform propagation\nmedium. While 3D Gaussian Splatting (3DGS) offers real-time rendering\ncapabilities, it struggles with underwater inhomogeneous environments where\nscattering media introduce artifacts and inconsistent appearance. In this\nstudy, we propose a physics-based framework that disentangles object appearance\nfrom water medium effects through tailored Gaussian modeling. Our approach\nintroduces appearance embeddings, which are explicit medium representations for\nbackscatter and attenuation, enhancing scene consistency. In addition, we\npropose a distance-guided optimization strategy that leverages pseudo-depth\nmaps as supervision with depth regularization and scale penalty terms to\nimprove geometric fidelity. By integrating the proposed appearance and medium\nmodeling components via an underwater imaging model, our approach achieves both\nhigh-quality novel view synthesis and physically accurate scene restoration.\nExperiments demonstrate our significant improvements in rendering quality and\nrestoration accuracy over existing methods. The project page is available at\n\\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u9ad8\u65af\u5efa\u6a21\u5206\u79bb\u7269\u4f53\u5916\u89c2\u4e0e\u6c34\u4ecb\u8d28\u6548\u5e94\uff0c\u5e76\u5f15\u5165\u8ddd\u79bb\u5f15\u5bfc\u4f18\u5316\u7b56\u7565\u4ee5\u63d0\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6c34\u4e0b\u573a\u666f\u91cd\u5efa\u56e0\u590d\u6742\u7684\u5149\u4ecb\u8d28\u76f8\u4e92\u4f5c\u7528\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u5747\u5300\u4ecb\u8d28\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u6846\u67b6\uff0c\u5305\u62ec\u5916\u89c2\u5d4c\u5165\u548c\u8ddd\u79bb\u5f15\u5bfc\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u6c34\u4e0b\u6210\u50cf\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u6062\u590d\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u7269\u7406\u51c6\u786e\u7684\u573a\u666f\u6062\u590d\u3002", "relevance": 30.0}}
{"id": "2505.21241", "pdf": "https://arxiv.org/pdf/2505.21241", "abs": "https://arxiv.org/abs/2505.21241", "authors": ["Divya Nori", "Anisha Parsan", "Caroline Uhler", "Wengong Jin"], "title": "BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design", "categories": ["cs.LG"], "comment": null, "summary": "Protein binder design has been transformed by hallucination-based methods\nthat optimize structure prediction confidence metrics, such as the interface\npredicted TM-score (ipTM), via backpropagation. However, these metrics do not\nreflect the statistical likelihood of a binder-target complex under the learned\ndistribution and yield sparse gradients for optimization. In this work, we\npropose a method to extract such likelihoods from structure predictors by\nreinterpreting their confidence outputs as an energy-based model (EBM). By\nleveraging the Joint Energy-based Modeling (JEM) framework, we introduce\npTMEnergy, a statistical energy function derived from predicted inter-residue\nerror distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a\ndesign pipeline that maintains the same optimization framework as BindCraft but\nreplaces ipTM with our energy-based objective. BECraft outperforms BindCraft,\nRFDiffusion, and ESM3 across multiple challenging targets, achieving higher in\nsilico binder success rates while reducing structural clashes. Furthermore,\npTMEnergy establishes a new state-of-the-art in structure-based virtual\nscreening tasks for miniprotein and RNA aptamer binders.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u6a21\u578b\uff08EBM\uff09\u7684\u65b9\u6cd5pTMEnergy\uff0c\u7528\u4e8e\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u7ed3\u6784\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6307\u6807\uff08\u5982ipTM\uff09\uff0c\u5e76\u901a\u8fc7BindEnergyCraft\uff08BECraft\uff09\u7ba1\u9053\u5b9e\u73b0\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ed3\u6784\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6307\u6807\uff08\u5982ipTM\uff09\u7684\u65b9\u6cd5\u65e0\u6cd5\u53cd\u6620\u7ed3\u5408\u5242-\u9776\u6807\u590d\u5408\u7269\u7684\u7edf\u8ba1\u4f3c\u7136\u6027\uff0c\u4e14\u68af\u5ea6\u7a00\u758f\uff0c\u9650\u5236\u4e86\u4f18\u5316\u6548\u679c\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u7ed3\u6784\u9884\u6d4b\u5668\u7684\u7f6e\u4fe1\u5ea6\u8f93\u51fa\u4e3a\u80fd\u91cf\u6a21\u578b\uff08EBM\uff09\uff0c\u63d0\u51fapTMEnergy\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8bbe\u8ba1\u7ba1\u9053BECraft\u4e2d\u3002", "result": "BECraft\u5728\u591a\u4e2a\u6311\u6218\u6027\u76ee\u6807\u4e0a\u4f18\u4e8eBindCraft\u3001RFDiffusion\u548cESM3\uff0c\u63d0\u9ad8\u4e86\u7ed3\u5408\u5242\u6210\u529f\u7387\u5e76\u51cf\u5c11\u7ed3\u6784\u51b2\u7a81\u3002pTMEnergy\u5728\u865a\u62df\u7b5b\u9009\u4efb\u52a1\u4e2d\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "pTMEnergy\u548cBECraft\u4e3a\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7edf\u8ba1\u80fd\u91cf\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21218", "pdf": "https://arxiv.org/pdf/2505.21218", "abs": "https://arxiv.org/abs/2505.21218", "authors": ["Roi Cohen", "Omri Fahn", "Gerard de Melo"], "title": "Pretrained LLMs Learn Multiple Types of Uncertainty", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u9690\u5f0f\u6355\u6349\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u9884\u6d4b\u4efb\u52a1\u6b63\u786e\u6027\u548c\u907f\u514d\u9519\u8bef\u4fe1\u606f\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4ecd\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff08\u5373\u4e8b\u5b9e\u9519\u8bef\u7684\u6587\u672c\uff09\u3002\u7814\u7a76\u76ee\u6807\u662f\u63a2\u7d22LLMs\u662f\u5426\u80fd\u5728\u672a\u7ecf\u660e\u786e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u4e0d\u786e\u5b9a\u6027\u89c6\u4e3a\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u6982\u5ff5\uff0c\u5206\u6790\u4e86LLMs\u5728\u9884\u8bad\u7ec3\u540e\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u7c7b\u578b\u5bf9\u4efb\u52a1\u6b63\u786e\u6027\u7684\u9884\u6d4b\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u80fd\u6355\u6349\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u7c7b\u578b\uff0c\u8fd9\u4e9b\u7c7b\u578b\u5bf9\u4efb\u52a1\u6b63\u786e\u6027\u9884\u6d4b\u6709\u5e2e\u52a9\u3002\u6b64\u5916\uff0c\u6a21\u578b\u89c4\u6a21\u5bf9\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u4e0d\u5927\uff0c\u800c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u6216[IDK]-token\u5fae\u8c03\u7edf\u4e00\u4e0d\u786e\u5b9a\u6027\u7c7b\u578b\u53ef\u63d0\u5347\u6b63\u786e\u6027\u9884\u6d4b\u3002", "conclusion": "LLMs\u80fd\u9690\u5f0f\u6355\u6349\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u7edf\u4e00\u4e0d\u786e\u5b9a\u6027\u7c7b\u578b\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6b63\u786e\u6027\u9884\u6d4b\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.21258", "pdf": "https://arxiv.org/pdf/2505.21258", "abs": "https://arxiv.org/abs/2505.21258", "authors": ["Changguanng Wu", "Jiangxin Dong", "Chengjian Li", "Jinhui Tang"], "title": "Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation", "categories": ["cs.CV"], "comment": null, "summary": "We present Plenodium (plenoptic medium), an effective and efficient 3D\nrepresentation framework capable of jointly modeling both objects and\nparticipating media. In contrast to existing medium representations that rely\nsolely on view-dependent modeling, our novel plenoptic medium representation\nincorporates both directional and positional information through spherical\nharmonics encoding, enabling highly accurate underwater scene reconstruction.\nTo address the initialization challenge in degraded underwater environments, we\npropose the pseudo-depth Gaussian complementation to augment COLMAP-derived\npoint clouds with robust depth priors. In addition, a depth ranking regularized\nloss is developed to optimize the geometry of the scene and improve the ordinal\nconsistency of the depth maps. Extensive experiments on real-world underwater\ndatasets demonstrate that our method achieves significant improvements in 3D\nreconstruction. Furthermore, we conduct a simulated dataset with ground truth\nand the controllable scattering medium to demonstrate the restoration\ncapability of our method in underwater scenarios. Our code and dataset are\navailable at https://plenodium.github.io/.", "AI": {"tldr": "Plenodium\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u4e09\u7ef4\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u5efa\u6a21\u7269\u4f53\u548c\u53c2\u4e0e\u4ecb\u8d28\uff0c\u901a\u8fc7\u7403\u8c10\u7f16\u7801\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u6c34\u4e0b\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u4ecb\u8d28\u8868\u793a\u4ec5\u4f9d\u8d56\u89c6\u89d2\u76f8\u5173\u5efa\u6a21\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u6c34\u4e0b\u573a\u666f\u7684\u590d\u6742\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4f2a\u6df1\u5ea6\u9ad8\u65af\u8865\u5145\u548c\u6df1\u5ea6\u6392\u5e8f\u6b63\u5219\u5316\u635f\u5931\uff0c\u4f18\u5316\u573a\u666f\u51e0\u4f55\u548c\u6df1\u5ea6\u56fe\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "Plenodium\u5728\u6c34\u4e0b\u573a\u666f\u91cd\u5efa\u548c\u6062\u590d\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "relevance": 30.0}}
{"id": "2505.21251", "pdf": "https://arxiv.org/pdf/2505.21251", "abs": "https://arxiv.org/abs/2505.21251", "authors": ["Mustafa Hajij", "Lennart Bastian", "Sarah Osentoski", "Hardik Kabaria", "John L. Davenport", "Sheik Dawood", "Balaji Cherukuri", "Joseph G. Kocheemoolayil", "Nastaran Shahmansouri", "Adrian Lew", "Theodore Papamarkou", "Tolga Birdal"], "title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework", "categories": ["cs.LG"], "comment": null, "summary": "We introduce copresheaf topological neural networks (CTNNs), a powerful and\nunifying framework that encapsulates a wide spectrum of deep learning\narchitectures, designed to operate on structured data: including images, point\nclouds, graphs, meshes, and topological manifolds. While deep learning has\nprofoundly impacted domains ranging from digital assistants to autonomous\nsystems, the principled design of neural architectures tailored to specific\ntasks and data types remains one of the field's most persistent open\nchallenges. CTNNs address this gap by grounding model design in the language of\ncopresheaves, a concept from algebraic topology that generalizes and subsumes\nmost practical deep learning models in use today. This abstract yet\nconstructive formulation yields a rich design space from which theoretically\nsound and practically effective solutions can be derived to tackle core\nchallenges in representation learning: long-range dependencies, oversmoothing,\nheterophily, and non-Euclidean domains. Our empirical results on structured\ndata benchmarks demonstrate that CTNNs consistently outperform conventional\nbaselines, particularly in tasks requiring hierarchical or localized\nsensitivity. These results underscore CTNNs as a principled, multi-scale\nfoundation for the next generation of deep learning architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u6570\u62d3\u6251\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6CTNN\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7ed3\u6784\u5316\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u591a\u79cd\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u548c\u6570\u636e\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002CTNN\u901a\u8fc7\u4ee3\u6570\u62d3\u6251\u4e2d\u7684copresheaf\u6982\u5ff5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "CTNN\u57fa\u4e8ecopresheaf\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u62bd\u8c61\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u56fe\u50cf\u3001\u70b9\u4e91\u3001\u56fe\u7b49\u591a\u79cd\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u89e3\u51b3\u957f\u7a0b\u4f9d\u8d56\u3001\u975e\u6b27\u51e0\u91cc\u5f97\u57df\u7b49\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCTNN\u5728\u7ed3\u6784\u5316\u6570\u636e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5c42\u6b21\u5316\u6216\u5c40\u90e8\u654f\u611f\u6027\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "CTNN\u4e3a\u4e0b\u4e00\u4ee3\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e25\u8c28\u3001\u591a\u5c3a\u5ea6\u7684\u57fa\u7840\u6846\u67b6\u3002", "relevance": 40.0}}
{"id": "2505.21224", "pdf": "https://arxiv.org/pdf/2505.21224", "abs": "https://arxiv.org/abs/2505.21224", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Understanding robustness is essential for building reliable NLP systems.\nUnfortunately, in the context of machine translation, previous work mainly\nfocused on documenting robustness failures or improving robustness. In\ncontrast, we study robustness from a model representation perspective by\nlooking at internal model representations of ungrammatical inputs and how they\nevolve through model layers. For this purpose, we perform Grammatical Error\nDetection (GED) probing and representational similarity analysis. Our findings\nindicate that the encoder first detects the grammatical error, then corrects it\nby moving its representation toward the correct form. To understand what\ncontributes to this process, we turn to the attention mechanism where we\nidentify what we term Robustness Heads. We find that Robustness Heads attend to\ninterpretable linguistic units when responding to grammatical errors, and that\nwhen we fine-tune models for robustness, they tend to rely more on Robustness\nHeads for updating the ungrammatical word representation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u6a21\u578b\u8868\u793a\u7684\u89d2\u5ea6\u7814\u7a76\u673a\u5668\u7ffb\u8bd1\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5bf9\u4e0d\u5408\u8bed\u6cd5\u8f93\u5165\u7684\u5185\u90e8\u8868\u793a\u53ca\u5176\u5728\u5c42\u95f4\u7684\u6f14\u53d8\uff0c\u53d1\u73b0\u7f16\u7801\u5668\u5148\u68c0\u6d4b\u8bed\u6cd5\u9519\u8bef\uff0c\u518d\u901a\u8fc7\u8868\u793a\u4fee\u6b63\u9519\u8bef\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u7ffb\u8bd1\u4e2d\u6a21\u578b\u5bf9\u4e0d\u5408\u8bed\u6cd5\u8f93\u5165\u7684\u5185\u90e8\u8868\u793a\u5982\u4f55\u6f14\u53d8\uff0c\u4ee5\u7406\u89e3\u9c81\u68d2\u6027\u7684\u673a\u5236\u3002", "method": "\u4f7f\u7528\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\uff08GED\uff09\u63a2\u6d4b\u548c\u8868\u793a\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u5e76\u5206\u6790\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u201c\u9c81\u68d2\u6027\u5934\u201d\u3002", "result": "\u53d1\u73b0\u7f16\u7801\u5668\u901a\u8fc7\u201c\u9c81\u68d2\u6027\u5934\u201d\u68c0\u6d4b\u5e76\u4fee\u6b63\u8bed\u6cd5\u9519\u8bef\uff0c\u8fd9\u4e9b\u5934\u5728\u5fae\u8c03\u540e\u66f4\u4f9d\u8d56\u4fee\u6b63\u4e0d\u5408\u8bed\u6cd5\u8bcd\u8868\u793a\u3002", "conclusion": "\u6a21\u578b\u901a\u8fc7\u7279\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9c81\u68d2\u6027\uff0c\u5fae\u8c03\u53ef\u589e\u5f3a\u5176\u4f9d\u8d56\u3002", "relevance": 75.0}}
{"id": "2505.21262", "pdf": "https://arxiv.org/pdf/2505.21262", "abs": "https://arxiv.org/abs/2505.21262", "authors": ["M. Akin Yilmaz", "Ahmet Bilican", "A. Murat Tekalp"], "title": "DiMoSR: Feature Modulation via Multi-Branch Dilated Convolutions for Efficient Image Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Balancing reconstruction quality versus model efficiency remains a critical\nchallenge in lightweight single image super-resolution (SISR). Despite the\nprevalence of attention mechanisms in recent state-of-the-art SISR approaches\nthat primarily emphasize or suppress feature maps, alternative architectural\nparadigms warrant further exploration. This paper introduces DiMoSR (Dilated\nModulation Super-Resolution), a novel architecture that enhances feature\nrepresentation through modulation to complement attention in lightweight SISR\nnetworks. The proposed approach leverages multi-branch dilated convolutions to\ncapture rich contextual information over a wider receptive field while\nmaintaining computational efficiency. Experimental results demonstrate that\nDiMoSR outperforms state-of-the-art lightweight methods across diverse\nbenchmark datasets, achieving superior PSNR and SSIM metrics with comparable or\nreduced computational complexity. Through comprehensive ablation studies, this\nwork not only validates the effectiveness of DiMoSR but also provides critical\ninsights into the interplay between attention mechanisms and feature modulation\nto guide future research in efficient network design. The code and model\nweights to reproduce our results are available at:\nhttps://github.com/makinyilmaz/DiMoSR", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiMoSR\uff0c\u4e00\u79cd\u901a\u8fc7\u8c03\u5236\u589e\u5f3a\u7279\u5f81\u8868\u793a\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u8f7b\u91cf\u7ea7\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\uff0c\u5728\u591a\u5206\u652f\u6269\u5f20\u5377\u79ef\u7684\u652f\u6301\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684PSNR\u548cSSIM\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u8f7b\u91cf\u7ea7SISR\u4e2d\u91cd\u5efa\u8d28\u91cf\u4e0e\u6a21\u578b\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63a2\u7d22\u6ce8\u610f\u529b\u673a\u5236\u4e4b\u5916\u7684\u67b6\u6784\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u591a\u5206\u652f\u6269\u5f20\u5377\u79ef\u6355\u83b7\u66f4\u5e7f\u611f\u53d7\u91ce\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7ed3\u5408\u8c03\u5236\u673a\u5236\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "DiMoSR\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0cPSNR\u548cSSIM\u6307\u6807\u66f4\u4f18\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "DiMoSR\u9a8c\u8bc1\u4e86\u8c03\u5236\u673a\u5236\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.21285", "pdf": "https://arxiv.org/pdf/2505.21285", "abs": "https://arxiv.org/abs/2505.21285", "authors": ["Xudong Wang", "Ziheng Sun", "Chris Ding", "Jicong Fan"], "title": "Learnable Kernel Density Estimation for Graphs", "categories": ["cs.LG", "stat.ML", "I.2; I.5.1; I.5.2"], "comment": "Under Review", "summary": "This work proposes a framework LGKDE that learns kernel density estimation\nfor graphs. The key challenge in graph density estimation lies in effectively\ncapturing both structural patterns and semantic variations while maintaining\ntheoretical guarantees. Combining graph kernels and kernel density estimation\n(KDE) is a standard approach to graph density estimation, but has\nunsatisfactory performance due to the handcrafted and fixed features of\nkernels. Our method LGKDE leverages graph neural networks to represent each\ngraph as a discrete distribution and utilizes maximum mean discrepancy to learn\nthe graph metric for multi-scale KDE, where all parameters are learned by\nmaximizing the density of graphs relative to the density of their well-designed\nperturbed counterparts. The perturbations are conducted on both node features\nand graph spectra, which helps better characterize the boundary of normal\ndensity regions. Theoretically, we establish consistency and convergence\nguarantees for LGKDE, including bounds on the mean integrated squared error,\nrobustness, and complexity. We validate LGKDE by demonstrating its\neffectiveness in recovering the underlying density of synthetic graph\ndistributions and applying it to graph anomaly detection across diverse\nbenchmark datasets. Extensive empirical evaluation shows that LGKDE\ndemonstrates superior performance compared to state-of-the-art baselines on\nmost benchmark datasets.", "AI": {"tldr": "LGKDE\u6846\u67b6\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6700\u5927\u5747\u503c\u5dee\u5f02\u5b66\u4e60\u56fe\u5bc6\u5ea6\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u56fe\u5bc6\u5ea6\u4f30\u8ba1\u4e2d\u7ed3\u6784\u6a21\u5f0f\u548c\u8bed\u4e49\u53d8\u5316\u6355\u83b7\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6700\u5927\u5747\u503c\u5dee\u5f02\uff0c\u5b66\u4e60\u591a\u5c3a\u5ea6KDE\u7684\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u56fe\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LGKDE\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.21239", "pdf": "https://arxiv.org/pdf/2505.21239", "abs": "https://arxiv.org/abs/2505.21239", "authors": ["Yu He", "Zihan Yao", "Chentao Song", "Tianyu Qi", "Jun Liu", "Ming Li", "Qing Huang"], "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD", "AI": {"tldr": "LMCD\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u8ba4\u77e5\u8bca\u65ad\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u6269\u6563\u548c\u8bed\u4e49-\u8ba4\u77e5\u878d\u5408\u4e24\u9636\u6bb5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709NLP\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u8ba4\u77e5\u5206\u6790\u3002", "method": "LMCD\u5206\u4e24\u9636\u6bb5\uff1a\u77e5\u8bc6\u6269\u6563\uff08LLMs\u751f\u6210\u7ec3\u4e60\u548c\u77e5\u8bc6\u6982\u5ff5\u7684\u4e30\u5bcc\u5185\u5bb9\uff09\u548c\u8bed\u4e49-\u8ba4\u77e5\u878d\u5408\uff08LLMs\u901a\u8fc7\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u6587\u672c\u4fe1\u606f\u548c\u5b66\u751f\u8ba4\u77e5\u72b6\u6001\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cLMCD\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LMCD\u5c55\u793a\u4e86LLMs\u5728\u8ba4\u77e5\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u3002", "relevance": 70.0}}
{"id": "2505.21269", "pdf": "https://arxiv.org/pdf/2505.21269", "abs": "https://arxiv.org/abs/2505.21269", "authors": ["Eva Gmelich Meijling", "Roberto Del Prete", "Arnoud Visser"], "title": "Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands", "categories": ["cs.CV", "eess.IV", "68", "I.4.6"], "comment": "12 pages, presented at the Netherlands Conference on Computer Vision\n  (NCCV), Utrecht, May 2025", "summary": "Accurate wetland land-cover classification is essential for environmental\nmonitoring, biodiversity assessment, and sustainable ecosystem management.\nHowever, the scarcity of annotated data, especially for high-resolution\nsatellite imagery, poses a significant challenge for supervised learning\napproaches. To tackle this issue, this study presents a methodology for wetland\nland-cover segmentation and classification that adopts both supervised and\nself-supervised learning (SSL). We train a U-Net model from scratch on\nSentinel-2 imagery across six wetland regions in the Netherlands, achieving a\nbaseline model accuracy of 85.26%.\n  Addressing the limited availability of labeled data, the results show that\nSSL pretraining with an autoencoder can improve accuracy, especially for the\nhigh-resolution imagery where it is more difficult to obtain labeled data,\nreaching an accuracy of 88.23%.\n  Furthermore, we introduce a framework to scale manually annotated\nhigh-resolution labels to medium-resolution inputs. While the quantitative\nperformance between resolutions is comparable, high-resolution imagery provides\nsignificantly sharper segmentation boundaries and finer spatial detail.\n  As part of this work, we also contribute a curated Sentinel-2 dataset with\nDynamic World labels, tailored for wetland classification tasks and made\npublicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6e7f\u5730\u571f\u5730\u8986\u76d6\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u901a\u8fc7U-Net\u6a21\u578b\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4ece85.26%\u63d0\u5347\u81f388.23%\u3002", "motivation": "\u6e7f\u5730\u571f\u5730\u8986\u76d6\u5206\u7c7b\u5bf9\u73af\u5883\u76d1\u6d4b\u548c\u751f\u7269\u591a\u6837\u6027\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u76d1\u7763\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528U-Net\u6a21\u578b\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528Sentinel-2\u56fe\u50cf\u5728\u8377\u5170\u516d\u4e2a\u6e7f\u5730\u533a\u57df\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c24\u5176\u662f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u51c6\u786e\u7387\u4ece85.26%\u63d0\u5347\u81f388.23%\u3002\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u8fb9\u754c\u548c\u7ec6\u8282\u3002", "conclusion": "\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u6709\u6548\u7f13\u89e3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002\u516c\u5f00\u7684\u6570\u636e\u96c6\u4e3a\u6e7f\u5730\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002", "relevance": 30.0}}
{"id": "2505.21288", "pdf": "https://arxiv.org/pdf/2505.21288", "abs": "https://arxiv.org/abs/2505.21288", "authors": ["Farshad Noravesh", "Reza Haffari", "Layki Soon", "Arghya Pal"], "title": "GSAT: Graph Structure Attention Networks", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages", "summary": "Graph Neural Networks (GNNs) have emerged as a powerful tool for processing\ndata represented in graph structures, achieving remarkable success across a\nwide range of applications. However, to further improve the performance on\ngraph classification benchmarks, structural representation of each node that\nencodes rich local topological information in the neighbourhood of nodes is an\nimportant type of feature that is often overlooked in the modeling. The\nconsequence of neglecting the structural information has resulted high number\nof layers to connect messages from distant nodes which by itself produces other\nproblems such as oversmoothing. In the present paper, we leverage these\nstructural information that are modeled by anonymous random walks (ARWs) and\nintroduce graph structure attention network (GSAT) which is a generalization of\ngraph attention network(GAT) to integrate the original attribute and the\nstructural representation to enforce the model to automatically find patterns\nfor attending to different edges in the node neighbourhood to enrich graph\nrepresentation. Our experiments show GSAT slightly improves SOTA on some graph\nclassification benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u533f\u540d\u968f\u673a\u6e38\u8d70\uff08ARWs\uff09\u548c\u56fe\u7ed3\u6784\u6ce8\u610f\u529b\u7f51\u7edc\uff08GSAT\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8282\u70b9\u7ed3\u6784\u8868\u793a\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5efa\u6a21\u65f6\u5f80\u5f80\u5ffd\u7565\u4e86\u8282\u70b9\u7684\u5c40\u90e8\u62d3\u6251\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u9700\u8981\u66f4\u591a\u5c42\u6765\u4f20\u9012\u8fdc\u8ddd\u79bb\u8282\u70b9\u7684\u4fe1\u606f\uff0c\u8fdb\u800c\u5f15\u53d1\u8fc7\u5e73\u6ed1\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u533f\u540d\u968f\u673a\u6e38\u8d70\uff08ARWs\uff09\u5efa\u6a21\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u56fe\u7ed3\u6784\u6ce8\u610f\u529b\u7f51\u7edc\uff08GSAT\uff09\uff0c\u7ed3\u5408\u539f\u59cb\u5c5e\u6027\u548c\u7ed3\u6784\u8868\u793a\uff0c\u81ea\u52a8\u5b66\u4e60\u8282\u70b9\u90bb\u57df\u4e2d\u4e0d\u540c\u8fb9\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGSAT\u5728\u67d0\u4e9b\u56fe\u5206\u7c7b\u57fa\u51c6\u4e0a\u7565\u5fae\u63d0\u5347\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff08SOTA\uff09\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7ed3\u6784\u4fe1\u606f\uff0cGSAT\u80fd\u591f\u66f4\u6709\u6548\u5730\u5b66\u4e60\u56fe\u8868\u793a\uff0c\u4e3a\u56fe\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.21242", "pdf": "https://arxiv.org/pdf/2505.21242", "abs": "https://arxiv.org/abs/2505.21242", "authors": ["Gunjan Balde", "Soumyadeep Roy", "Mainack Mondal", "Niloy Ganguly"], "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings", "categories": ["cs.CL"], "comment": "16 pages. Accepted for publication in the Findings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u533b\u5b66\u6587\u672c\u6458\u8981\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u56f0\u96be\u8bbe\u7f6e\u4e0b\uff08\u5982\u9ad8OOV\u8bcd\u6216\u9ad8\u65b0\u9896\u6027\u6570\u636e\uff09\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u901a\u8fc7\u8bcd\u6c47\u9002\u5e94\u7b56\u7565\uff0c\u4f5c\u8005\u6539\u8fdb\u4e86LLMs\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u4eba\u7c7b\u8bc4\u4f30\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u533b\u5b66\u6587\u672c\u6458\u8981\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u56f0\u96be\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u8bcd\u6c47\u9002\u5e94\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u79cd\u8bcd\u6c47\u9002\u5e94\u7b56\u7565\u3001\u4e24\u79cd\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728\u4e09\u4e2a\u533b\u5b66\u6458\u8981\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "\u8bcd\u6c47\u9002\u5e94\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u56f0\u96be\u8bbe\u7f6e\u4e0b\u7684\u6458\u8981\u6027\u80fd\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u5176\u751f\u6210\u6458\u8981\u7684\u76f8\u5173\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "conclusion": "\u8bcd\u6c47\u9002\u5e94\u662f\u5b9a\u5236LLMs\u5230\u533b\u5b66\u9886\u57df\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u4e13\u4e1a\u8bcd\u6c47\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "relevance": 85.0}}
{"id": "2505.21309", "pdf": "https://arxiv.org/pdf/2505.21309", "abs": "https://arxiv.org/abs/2505.21309", "authors": ["Zenghao Zheng", "Lianping Yang", "Hegui Zhu", "Mingrui Ye"], "title": "Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based 3D human pose estimation methods suffer from high\ncomputational costs due to the quadratic complexity of self-attention with\nrespect to sequence length. Additionally, pose sequences often contain\nsignificant redundancy between frames. However, recent methods typically fail\nto improve model capacity while effectively eliminating sequence redundancy. In\nthis work, we introduce the Spectral Compression Transformer (SCT) to reduce\nsequence length and accelerate computation. The SCT encoder treats hidden\nfeatures between blocks as Temporal Feature Signals (TFS) and applies the\nDiscrete Cosine Transform, a Fourier transform-based technique, to determine\nthe spectral components to be retained. By filtering out certain high-frequency\nnoise components, SCT compresses the sequence length and reduces redundancy. To\nfurther enrich the input sequence with prior structural information, we propose\nthe Line Pose Graph (LPG) based on line graph theory. The LPG generates\nskeletal position information that complements the input 2D joint positions,\nthereby improving the model's performance. Finally, we design a dual-stream\nnetwork architecture to effectively model spatial joint relationships and the\ncompressed motion trajectory within the pose sequence. Extensive experiments on\ntwo benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our\nmodel achieves state-of-the-art performance with improved computational\nefficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE\nof 37.7mm while maintaining a low computational cost. Furthermore, we perform\nablation studies on each module to assess its effectiveness. The code and\nmodels will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpectral Compression Transformer (SCT)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u538b\u7f29\u5e8f\u5217\u957f\u5ea6\u5e76\u51cf\u5c11\u5197\u4f59\uff0c\u7ed3\u5408Line Pose Graph (LPG)\u589e\u5f3a\u8f93\u5165\u4fe1\u606f\uff0c\u6700\u7ec8\u8bbe\u8ba1\u53cc\u6d41\u7f51\u7edc\u63d0\u53473D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "Transformer\u57283D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u56e0\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6d88\u9664\u5e8f\u5217\u5197\u4f59\u3002", "method": "1. \u4f7f\u7528SCT\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u538b\u7f29\u5e8f\u5217\u957f\u5ea6\uff1b2. \u63d0\u51faLPG\u57fa\u4e8e\u7ebf\u56fe\u7406\u8bba\u589e\u5f3a\u8f93\u5165\u4fe1\u606f\uff1b3. \u8bbe\u8ba1\u53cc\u6d41\u7f51\u7edc\u5efa\u6a21\u7a7a\u95f4\u5173\u8282\u5173\u7cfb\u548c\u538b\u7f29\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002\u4f8b\u5982\uff0cHuman3.6M\u4e0aMPJPE\u4e3a37.7mm\u3002", "conclusion": "SCT\u548cLPG\u6709\u6548\u51cf\u5c11\u4e86\u5197\u4f59\u5e76\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u53cc\u6d41\u7f51\u7edc\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u5efa\u6a21\u80fd\u529b\u3002", "relevance": 50.0}}
{"id": "2505.21289", "pdf": "https://arxiv.org/pdf/2505.21289", "abs": "https://arxiv.org/abs/2505.21289", "authors": ["Nurbek Tastan", "Stefanos Laskaridis", "Martin Takac", "Karthik Nandakumar", "Samuel Horvath"], "title": "LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Large pre-trained models are commonly adapted to downstream tasks using\nparameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),\nwhich injects small trainable low-rank matrices instead of updating all\nweights. While LoRA dramatically reduces trainable parameters with little\noverhead, it can still underperform full fine-tuning in accuracy and often\nconverges more slowly. We introduce LoFT, a novel low-rank adaptation method\nthat behaves like full fine-tuning by aligning the optimizer's internal\ndynamics with those of updating all model weights. LoFT not only learns weight\nupdates in a low-rank subspace (like LoRA) but also properly projects the\noptimizer's first and second moments (Adam's momentum and variance) into the\nsame subspace, mirroring full-model updates. By aligning the low-rank update\nitself with the full update, LoFT eliminates the need for tuning extra\nhyperparameters, e.g., LoRA scaling factor $\\alpha$. Empirically, this approach\nsubstantially narrows the performance gap between adapter-based tuning and full\nfine-tuning and consistently outperforms standard LoRA-style methods, all\nwithout increasing inference cost.", "AI": {"tldr": "LoFT\u662f\u4e00\u79cd\u65b0\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5668\u5185\u90e8\u52a8\u6001\u5bf9\u9f50\u5168\u5fae\u8c03\u884c\u4e3a\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5168\u5fae\u8c03\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u65e0\u9700\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3LoRA\u5728\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4e0d\u5982\u5168\u5fae\u8c03\u7684\u95ee\u9898\u3002", "method": "LoFT\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\u5b66\u4e60\u6743\u91cd\u66f4\u65b0\uff0c\u5e76\u5c06\u4f18\u5316\u5668\u7684\u52a8\u91cf\u548c\u65b9\u5dee\u6295\u5f71\u5230\u540c\u4e00\u5b50\u7a7a\u95f4\uff0c\u6a21\u62df\u5168\u6a21\u578b\u66f4\u65b0\u3002", "result": "LoFT\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5168\u5fae\u8c03\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u4f18\u4e8e\u6807\u51c6LoRA\u65b9\u6cd5\u3002", "conclusion": "LoFT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u5fae\u8c03\u3002", "relevance": 85.0}}
{"id": "2505.20635", "pdf": "https://arxiv.org/pdf/2505.20635", "abs": "https://arxiv.org/abs/2505.20635", "authors": ["Zexu Pan", "Shengkui Zhao", "Tingting Wang", "Kun Zhou", "Yukun Ma", "Chong Zhang", "Bin Ma"], "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Interspeech 2025", "summary": "Audio-visual speaker extraction isolates a target speaker's speech from a\nmixture speech signal conditioned on a visual cue, typically using the target\nspeaker's face recording. However, in real-world scenarios, other co-occurring\nfaces are often present on-screen, providing valuable speaker activity cues in\nthe scene. In this work, we introduce a plug-and-play inter-speaker attention\nmodule to process these flexible numbers of co-occurring faces, allowing for\nmore accurate speaker extraction in complex multi-person environments. We\nintegrate our module into two prominent models: the AV-DPRNN and the\nstate-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,\nincluding the highly overlapped VoxCeleb2 and sparsely overlapped MISP,\ndemonstrate that our approach consistently outperforms baselines. Furthermore,\ncross-dataset evaluations on LRS2 and LRS3 confirm the robustness and\ngeneralizability of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63d2\u62d4\u7684\u8bf4\u8bdd\u8005\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7528\u4e8e\u5904\u7406\u591a\u4eba\u5728\u573a\u573a\u666f\u4e2d\u7684\u8bf4\u8bdd\u8005\u63d0\u53d6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5c4f\u5e55\u4e0a\u5e38\u51fa\u73b0\u591a\u4e2a\u5171\u73b0\u4eba\u8138\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8bf4\u8bdd\u8005\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bf4\u8bdd\u8005\u63d0\u53d6\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63d2\u62d4\u7684\u8bf4\u8bdd\u8005\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u53ef\u5904\u7406\u4efb\u610f\u6570\u91cf\u7684\u5171\u73b0\u4eba\u8138\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230AV-DPRNN\u548cAV-TFGridNet\u6a21\u578b\u4e2d\u3002", "result": "\u5728VoxCeleb2\u548cMISP\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u8de8\u6570\u636e\u96c6\uff08LRS2\u548cLRS3\uff09\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u8bf4\u8bdd\u8005\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u80fd\u6709\u6548\u63d0\u5347\u591a\u4eba\u5728\u573a\u573a\u666f\u4e0b\u7684\u8bf4\u8bdd\u8005\u63d0\u53d6\u6027\u80fd\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 30.0}}
{"id": "2505.21250", "pdf": "https://arxiv.org/pdf/2505.21250", "abs": "https://arxiv.org/abs/2505.21250", "authors": ["Dosung Lee", "Wonjun Oh", "Boyoung Kim", "Minyoung Kim", "Joonsuk Park", "Paul Hongsuck Seo"], "title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision", "categories": ["cs.CL"], "comment": "9 pages, 3 figures, ACL 2025", "summary": "Multi-hop question answering (MHQA) involves reasoning across multiple\ndocuments to answer complex questions. Dense retrievers typically outperform\nsparse methods like BM25 by leveraging semantic embeddings; however, they\nrequire labeled query-document pairs for fine-tuning. This poses a significant\nchallenge in MHQA due to the high variability of queries (reformulated)\nquestions throughout the reasoning steps. To overcome this limitation, we\nintroduce Retriever Supervision with Consistency and Relevance (ReSCORE), a\nnovel method for training dense retrievers for MHQA without labeled documents.\nReSCORE leverages large language models to capture each documents relevance to\nthe question and consistency with the correct answer and use them to train a\nretriever within an iterative question-answering framework. Experiments on\nthree MHQA benchmarks demonstrate the effectiveness of ReSCORE, with\nsignificant improvements in retrieval, and in turn, the state-of-the-art MHQA\nperformance. Our implementation is available at:\nhttps://leeds1219.github.io/ReSCORE.", "AI": {"tldr": "ReSCORE\u662f\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6587\u6863\u7684\u8bad\u7ec3\u5bc6\u96c6\u68c0\u7d22\u5668\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6587\u6863\u4e0e\u95ee\u9898\u7684\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8df3\u95ee\u7b54\u4e2d\u5bc6\u96c6\u68c0\u7d22\u5668\u9700\u8981\u6807\u6ce8\u67e5\u8be2-\u6587\u6863\u5bf9\u7684\u9650\u5236\uff0c\u5229\u7528LLMs\u8bc4\u4f30\u6587\u6863\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faReSCORE\u65b9\u6cd5\uff0c\u5229\u7528LLMs\u8bc4\u4f30\u6587\u6863\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u95ee\u7b54\u6846\u67b6\u8bad\u7ec3\u68c0\u7d22\u5668\u3002", "result": "\u5728\u4e09\u4e2aMHQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReSCORE\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684MHQA\u8868\u73b0\u3002", "conclusion": "ReSCORE\u4e3a\u65e0\u9700\u6807\u6ce8\u6587\u6863\u7684\u5bc6\u96c6\u68c0\u7d22\u5668\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86MHQA\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.21316", "pdf": "https://arxiv.org/pdf/2505.21316", "abs": "https://arxiv.org/abs/2505.21316", "authors": ["Enam Ahmed Taufik", "Antara Firoz Parsa", "Seraj Al Mahmud Mostafa"], "title": "Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted in 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Enhancing plant disease detection from leaf imagery remains a persistent\nchallenge due to scarce labeled data and complex contextual factors. We\nintroduce a transformative two-stage methodology, Mid Point Normalization (MPN)\nfor intelligent image preprocessing, coupled with sophisticated attention\nmechanisms that dynamically recalibrate feature representations. Our\nclassification pipeline, merging MPN with Squeeze-and-Excitation (SE) blocks,\nachieves remarkable 93% accuracy while maintaining exceptional class-wise\nbalance. The perfect F1 score attained for our target class exemplifies\nattention's power in adaptive feature refinement. For segmentation tasks, we\nseamlessly integrate identical attention blocks within U-Net architecture using\nMPN-enhanced inputs, delivering compelling performance gains with 72.44% Dice\nscore and 58.54% IoU, substantially outperforming baseline implementations.\nBeyond superior accuracy metrics, our approach yields computationally\nefficient, lightweight architectures perfectly suited for real-world computer\nvision applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08MPN\u548c\u6ce8\u610f\u529b\u673a\u5236\uff09\u7528\u4e8e\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\uff0c\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u4e0a\u4e0b\u6587\u56e0\u7d20\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408Mid Point Normalization\uff08MPN\uff09\u9884\u5904\u7406\u548c\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982SE\u5757\uff09\uff0c\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u5e94\u7528\u3002", "result": "\u5206\u7c7b\u4efb\u52a1\u8fbe\u523093%\u51c6\u786e\u7387\uff0c\u5206\u5272\u4efb\u52a1Dice\u5f97\u5206\u4e3a72.44%\uff0cIoU\u4e3a58.54%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u8f7b\u91cf\uff0c\u9002\u5408\u5b9e\u9645\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.21317", "pdf": "https://arxiv.org/pdf/2505.21317", "abs": "https://arxiv.org/abs/2505.21317", "authors": ["Ihab Bendidi", "Yassir El Mesbahi", "Alisandra K. Denton", "Karush Suri", "Kian Kenyon-Dean", "Auguste Genovesio", "Emmanuel Noutahi"], "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Main Proceedings", "summary": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u63d0\u53d6\u77e5\u8bc6\u6765\u589e\u5f3a\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5f31\u914d\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1aSemi-Clipped\u548cPEA\u3002", "motivation": "\u7406\u89e3\u7ec6\u80de\u5bf9\u523a\u6fc0\u7684\u53cd\u5e94\u5bf9\u751f\u7269\u5b66\u53d1\u73b0\u548c\u836f\u7269\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f31\u914d\u5bf9\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5e94\u7528\u3002", "method": "1. \u4f7f\u7528\u5f31\u914d\u5bf9\u6570\u636e\u5bf9\u9f50\u548c\u7ed1\u5b9a\u6a21\u6001\uff1b2. \u63d0\u51faSemi-Clipped\uff08\u57fa\u4e8eCLIP\u7684\u8de8\u6a21\u6001\u84b8\u998f\uff09\u548cPEA\uff08\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff09\u3002", "result": "\u65b9\u6cd5\u63d0\u5347\u4e86\u8f6c\u5f55\u7ec4\u5b66\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f62\u6001\u5b66\u4fe1\u606f\u4e30\u5bcc\u4e86\u57fa\u56e0\u8868\u8fbe\u8868\u793a\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u751f\u7269\u5b66\u4efb\u52a1\u3002", "relevance": 30.0}}
{"id": "2505.21265", "pdf": "https://arxiv.org/pdf/2505.21265", "abs": "https://arxiv.org/abs/2505.21265", "authors": ["Ilker Kesen", "Jonas F. Lotz", "Ingo Ziegler", "Phillip Rust", "Desmond Elliott"], "title": "Multilingual Pretraining for Pixel Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 19 figures, 7 tables", "summary": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "AI": {"tldr": "PIXEL-M4\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u5904\u7406\u6e32\u67d3\u6587\u672c\u56fe\u50cf\uff0c\u65e0\u9700\u56fa\u5b9a\u8bcd\u6c47\u8868\uff0c\u5728\u975e\u62c9\u4e01\u8bed\u7cfb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u5bf9\u50cf\u7d20\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u586b\u8865\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5728\u82f1\u8bed\u3001\u5370\u5730\u8bed\u3001\u4e4c\u514b\u5170\u8bed\u548c\u7b80\u4f53\u4e2d\u6587\u56db\u79cd\u8bed\u8a00\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u8fdb\u884c\u8bed\u4e49\u548c\u53e5\u6cd5\u4efb\u52a1\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u3002", "result": "PIXEL-M4\u5728\u975e\u62c9\u4e01\u8bed\u7cfb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u8bed\u8a00\u6a21\u578b\uff0c\u4e14\u80fd\u6355\u6349\u672a\u89c1\u8fc7\u8bed\u8a00\u7684\u4e30\u5bcc\u8bed\u8a00\u7279\u5f81\u3002", "conclusion": "\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u50cf\u7d20\u8bed\u8a00\u6a21\u578b\u5bf9\u591a\u8bed\u8a00\u7684\u652f\u6301\u80fd\u529b\u3002", "relevance": 60.0}}
{"id": "2505.21325", "pdf": "https://arxiv.org/pdf/2505.21325", "abs": "https://arxiv.org/abs/2505.21325", "authors": ["Guangyuan Li", "Siming Zheng", "Hao Zhang", "Jinwei Chen", "Junsheng Luan", "Binkai Ou", "Lei Zhao", "Bo Li", "Peng-Tao Jiang"], "title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on", "categories": ["cs.CV"], "comment": null, "summary": "Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer.We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.", "AI": {"tldr": "MagicTryOn\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u6269\u6563Transformer\u7684\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u670d\u88c5\u7ec6\u8282\u4fdd\u7559\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u670d\u88c5\u7ec6\u8282\u4fdd\u7559\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8eU-Net\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5206\u79bb\u7684\u65f6\u7a7a\u5efa\u6a21\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u6269\u6563Transformer\u66ff\u4ee3U-Net\uff0c\u7ed3\u5408\u5168\u81ea\u6ce8\u610f\u529b\u8054\u5408\u5efa\u6a21\u89c6\u9891\u65f6\u7a7a\u4e00\u81f4\u6027\uff1b\u8bbe\u8ba1\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u670d\u88c5\u4fdd\u7559\u7b56\u7565\uff0c\u5305\u62ec\u5d4c\u5165\u9636\u6bb5\u7684\u670d\u88c5\u4ee4\u724c\u548c\u53bb\u566a\u9636\u6bb5\u7684\u591a\u6761\u4ef6\u878d\u5408\uff1b\u5f15\u5165\u63a9\u7801\u611f\u77e5\u635f\u5931\u4f18\u5316\u670d\u88c5\u533a\u57df\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u8bd5\u7a7f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMagicTryOn\u5728\u7efc\u5408\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u3002", "conclusion": "MagicTryOn\u901a\u8fc7\u6539\u8fdb\u67b6\u6784\u548c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.21321", "pdf": "https://arxiv.org/pdf/2505.21321", "abs": "https://arxiv.org/abs/2505.21321", "authors": ["Leonard Papenmeier", "Luigi Nardi"], "title": "Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization", "categories": ["cs.LG"], "comment": "7 pages, 1 figure", "summary": "We present Bencher, a modular benchmarking framework for black-box\noptimization that fundamentally decouples benchmark execution from optimization\nlogic. Unlike prior suites that focus on combining many benchmarks in a single\nproject, Bencher introduces a clean abstraction boundary: each benchmark is\nisolated in its own virtual Python environment and accessed via a unified,\nversion-agnostic remote procedure call (RPC) interface. This design eliminates\ndependency conflicts and simplifies the integration of diverse, real-world\nbenchmarks, which often have complex and conflicting software requirements.\nBencher can be deployed locally or remotely via Docker or on high-performance\ncomputing (HPC) clusters via Singularity, providing a containerized,\nreproducible runtime for any benchmark. Its lightweight client requires minimal\nsetup and supports drop-in evaluation of 80 benchmarks across continuous,\ncategorical, and binary domains.", "AI": {"tldr": "Bencher\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u9ed1\u76d2\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u57fa\u51c6\u6267\u884c\u4e0e\u4f18\u5316\u903b\u8f91\uff0c\u7b80\u5316\u4e86\u590d\u6742\u57fa\u51c6\u7684\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u901a\u5e38\u5c06\u591a\u4e2a\u57fa\u51c6\u7ec4\u5408\u5728\u4e00\u4e2a\u9879\u76ee\u4e2d\uff0c\u5bfc\u81f4\u4f9d\u8d56\u51b2\u7a81\u548c\u96c6\u6210\u56f0\u96be\u3002Bencher\u65e8\u5728\u901a\u8fc7\u9694\u79bb\u6bcf\u4e2a\u57fa\u51c6\u7684\u73af\u5883\u548c\u7edf\u4e00\u7684RPC\u63a5\u53e3\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Bencher\u5c06\u6bcf\u4e2a\u57fa\u51c6\u9694\u79bb\u5728\u72ec\u7acb\u7684\u865a\u62dfPython\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u7edf\u4e00\u7684RPC\u63a5\u53e3\u8bbf\u95ee\uff0c\u652f\u6301\u672c\u5730\u3001Docker\u6216HPC\u96c6\u7fa4\u90e8\u7f72\u3002", "result": "Bencher\u652f\u630180\u4e2a\u8de8\u8fde\u7eed\u3001\u5206\u7c7b\u548c\u4e8c\u5143\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef\u548c\u53ef\u590d\u73b0\u7684\u8fd0\u884c\u65f6\u73af\u5883\u3002", "conclusion": "Bencher\u901a\u8fc7\u6a21\u5757\u5316\u548c\u9694\u79bb\u8bbe\u8ba1\uff0c\u663e\u8457\u7b80\u5316\u4e86\u590d\u6742\u57fa\u51c6\u7684\u96c6\u6210\u548c\u90e8\u7f72\u3002", "relevance": 60.0}}
{"id": "2505.21297", "pdf": "https://arxiv.org/pdf/2505.21297", "abs": "https://arxiv.org/abs/2505.21297", "authors": ["Yifei Liu", "Li Lyna Zhang", "Yi Zhu", "Bingcheng Dong", "Xudong Zhou", "Ning Shang", "Fan Yang", "Mao Yang"], "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset", "categories": ["cs.CL"], "comment": null, "summary": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar.", "AI": {"tldr": "rStar-Coder\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u3001\u9a8c\u8bc1\u7684\u4ee3\u7801\u95ee\u9898\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u9ad8\u96be\u5ea6\u4ee3\u7801\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u9650\u5236\u4e86LLM\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0crStar-Coder\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1) \u5408\u6210\u7ade\u4e89\u6027\u7f16\u7a0b\u95ee\u9898\uff1b2) \u5f15\u5165\u8f93\u5165-\u8f93\u51fa\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u7ba1\u9053\uff1b3) \u589e\u5f3a\u95ee\u9898\u7684\u9ad8\u8d28\u91cf\u957f\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728Qwen\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c7B\u6a21\u578b\u5728LiveCodeBench\u4e0a\u4ece17.4%\u63d0\u5347\u81f357.3%\uff0c14B\u6a21\u578b\u63d0\u5347\u81f362.5%\u3002", "conclusion": "rStar-Coder\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5728\u5c0f\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.21333", "pdf": "https://arxiv.org/pdf/2505.21333", "abs": "https://arxiv.org/abs/2505.21333", "authors": ["Yang Shi", "Huanqian Wang", "Wulin Xie", "Huanyao Zhang", "Lijie Zhao", "Yi-Fan Zhang", "Xinfeng Li", "Chaoyou Fu", "Zhuoer Wen", "Wenting Liu", "Zhuoran Zhang", "Xinlong Chen", "Bohan Zeng", "Sihan Yang", "Yuanxing Zhang", "Pengfei Wan", "Haotian Wang", "Wenjing Yang"], "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios", "categories": ["cs.CV"], "comment": "preprint", "summary": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MME-VideoOCR\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891OCR\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u9700\u8981\u65f6\u7a7a\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u89c6\u9891OCR\u4efb\u52a1\u4e2d\uff0cMLLMs\u56e0\u8fd0\u52a8\u6a21\u7cca\u548c\u65f6\u5e8f\u53d8\u5316\u7b49\u56e0\u7d20\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165MME-VideoOCR\u57fa\u51c6\uff0c\u5305\u542b10\u7c7b\u4efb\u52a1\u300125\u4e2a\u5177\u4f53\u4efb\u52a1\u548c44\u79cd\u573a\u666f\uff0c\u8bc4\u4f30\u4e8618\u79cdMLLMs\u3002", "result": "\u6700\u4f73\u6a21\u578b\uff08Gemini-2.5 Pro\uff09\u51c6\u786e\u7387\u4ec573.7%\uff0c\u6a21\u578b\u5728\u5355\u5e27\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\u4e0a\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u89c6\u9891OCR\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u5145\u5206\u7684\u65f6\u95f4\u8986\u76d6\uff0c\u73b0\u6709MLLMs\u9700\u6539\u8fdb\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "relevance": 60.0}}
{"id": "2505.21330", "pdf": "https://arxiv.org/pdf/2505.21330", "abs": "https://arxiv.org/abs/2505.21330", "authors": ["Christos Fragkathoulas", "Evaggelia Pitoura"], "title": "UGCE: User-Guided Incremental Counterfactual Exploration", "categories": ["cs.LG"], "comment": "Accepted to the ForgtAI Workshop at IJCNN 2025", "summary": "Counterfactual explanations (CFEs) are a popular approach for interpreting\nmachine learning predictions by identifying minimal feature changes that alter\nmodel outputs. However, in real-world settings, users often refine feasibility\nconstraints over time, requiring counterfactual generation to adapt\ndynamically. Existing methods fail to support such iterative updates, instead\nrecomputing explanations from scratch with each change, an inefficient and\nrigid approach. We propose User-Guided Incremental Counterfactual Exploration\n(UGCE), a genetic algorithm-based framework that incrementally updates\ncounterfactuals in response to evolving user constraints. Experimental results\nacross five benchmark datasets demonstrate that UGCE significantly improves\ncomputational efficiency while maintaining high-quality solutions compared to a\nstatic, non-incremental approach. Our evaluation further shows that UGCE\nsupports stable performance under varying constraint sequences, benefits from\nan efficient warm-start strategy, and reveals how different constraint types\nmay affect search behavior.", "AI": {"tldr": "UGCE\u662f\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u66f4\u65b0\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4ee5\u9002\u5e94\u53d8\u5316\u7684\u7528\u6237\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u652f\u6301\u7528\u6237\u7ea6\u675f\u7684\u8fed\u4ee3\u66f4\u65b0\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002UGCE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\u6846\u67b6\uff0c\u52a8\u6001\u66f4\u65b0\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4ee5\u9002\u5e94\u7528\u6237\u7ea6\u675f\u7684\u53d8\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cUGCE\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u89e3\u3002", "conclusion": "UGCE\u652f\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u9002\u5e94\u4e0d\u540c\u7ea6\u675f\u5e8f\u5217\uff0c\u5e76\u63ed\u793a\u4e86\u7ea6\u675f\u7c7b\u578b\u5bf9\u641c\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "relevance": 50.0}}
{"id": "2505.21301", "pdf": "https://arxiv.org/pdf/2505.21301", "abs": "https://arxiv.org/abs/2505.21301", "authors": ["Andrea Pedrotti", "Giulia Rambelli", "Caterina Villani", "Marianna Bolognesi"], "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u7c7b\u4e0eLLMs\u5728\u751f\u6210\u5206\u7c7b\u793a\u4f8b\u65f6\u7684\u5bf9\u9f50\u6027\uff0c\u53d1\u73b0\u4e24\u8005\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u4f46\u6027\u80fd\u56e0\u8bed\u4e49\u9886\u57df\u800c\u5f02\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u751f\u6210\u5206\u7c7b\u793a\u4f8b\u65f6\u662f\u5426\u4e0e\u4eba\u7c7b\u5206\u7c7b\u7ec4\u7ec7\u4e00\u81f4\uff0c\u4ee5\u652f\u6301\u5fc3\u7406\u5b66\u548c\u8bed\u8a00\u5b66\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u610f\u5927\u5229\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u5728\u751f\u6210\u793a\u4f8b\u3001\u7c7b\u522b\u5f52\u7eb3\u548c\u5178\u578b\u6027\u5224\u65ad\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u4e0e\u4eba\u7c7b\u751f\u6210\u793a\u4f8b\u7684\u5bf9\u9f50\u6027\u8f83\u4f4e\uff0c\u4f46\u6027\u80fd\u5728\u4e0d\u540c\u8bed\u4e49\u9886\u57df\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "AI\u751f\u6210\u7684\u793a\u4f8b\u5728\u5fc3\u7406\u5b66\u548c\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21334", "pdf": "https://arxiv.org/pdf/2505.21334", "abs": "https://arxiv.org/abs/2505.21334", "authors": ["Kele Shao", "Keda Tao", "Can Qin", "Haoxuan You", "Yang Sui", "Huan Wang"], "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.", "AI": {"tldr": "HoliTom\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5168\u65b9\u4f4d\u4ee4\u724c\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5916\u7ed3\u5408\u7684\u7b56\u7565\u663e\u8457\u51cf\u5c11\u89c6\u9891LLM\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891LLM\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u5168\u5c40\u65f6\u95f4\u52a8\u6001\u5ffd\u7565\u7684\u95ee\u9898\uff0cHoliTom\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5916\u5c42LLM\u526a\u679d\uff08\u5168\u5c40\u5197\u4f59\u611f\u77e5\u65f6\u95f4\u5206\u5272\uff09\u548c\u5185\u5c42LLM\u4ee4\u724c\u76f8\u4f3c\u6027\u5408\u5e76\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u89c6\u89c9\u4ee4\u724c\u3002", "result": "\u5728LLaVA-OneVision-7B\u4e0a\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u81f36.9% FLOPs\uff0c\u6027\u80fd\u4fdd\u630199.1%\uff0cTTFT\u51cf\u5c112.28\u500d\uff0c\u89e3\u7801\u541e\u5410\u91cf\u63d0\u53471.32\u500d\u3002", "conclusion": "HoliTom\u5c55\u793a\u4e86\u9ad8\u6548\u89c6\u9891LLM\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u5185\u5916\u7ed3\u5408\u7b56\u7565\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.21336", "pdf": "https://arxiv.org/pdf/2505.21336", "abs": "https://arxiv.org/abs/2505.21336", "authors": ["Loucas Pillaud-Vivien", "Adrien Schertzer"], "title": "Joint Learning in the Gaussian Single Index Model", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "31 Pages, 3 Figures", "summary": "We consider the problem of jointly learning a one-dimensional projection and\na univariate function in high-dimensional Gaussian models. Specifically, we\nstudy predictors of the form $f(x)=\\varphi^\\star(\\langle w^\\star, x \\rangle)$,\nwhere both the direction $w^\\star \\in \\mathcal{S}_{d-1}$, the sphere of\n$\\mathbb{R}^d$, and the function $\\varphi^\\star: \\mathbb{R} \\to \\mathbb{R}$ are\nlearned from Gaussian data. This setting captures a fundamental non-convex\nproblem at the intersection of representation learning and nonlinear\nregression. We analyze the gradient flow dynamics of a natural alternating\nscheme and prove convergence, with a rate controlled by the information\nexponent reflecting the \\textit{Gaussian regularity} of the function\n$\\varphi^\\star$. Strikingly, our analysis shows that convergence still occurs\neven when the initial direction is negatively correlated with the target. On\nthe practical side, we demonstrate that such joint learning can be effectively\nimplemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the\nstructure of the problem, enabling efficient and flexible estimation of the\nunivariate function. Our results offer both theoretical insight and practical\nmethodology for learning low-dimensional structure in high-dimensional\nsettings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u9ad8\u7ef4\u9ad8\u65af\u6a21\u578b\u4e2d\u8054\u5408\u5b66\u4e60\u4e00\u7ef4\u6295\u5f71\u548c\u5355\u53d8\u91cf\u51fd\u6570\u7684\u95ee\u9898\uff0c\u5206\u6790\u4e86\u68af\u5ea6\u6d41\u52a8\u6001\u5e76\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRKHS\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u9ad8\u7ef4\u6570\u636e\u4e2d\u4f4e\u7ef4\u7ed3\u6784\u7684\u8054\u5408\u5b66\u4e60\u95ee\u9898\uff0c\u7ed3\u5408\u8868\u793a\u5b66\u4e60\u548c\u975e\u7ebf\u6027\u56de\u5f52\uff0c\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u6311\u6218\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u68af\u5ea6\u6d41\u52a8\u6001\u5206\u6790\uff0c\u5229\u7528RKHS\u5b9e\u73b0\u9ad8\u6548\u7684\u5355\u53d8\u91cf\u51fd\u6570\u4f30\u8ba1\u3002", "result": "\u8bc1\u660e\u4e86\u6536\u655b\u6027\uff0c\u5373\u4f7f\u521d\u59cb\u65b9\u5411\u4e0e\u76ee\u6807\u8d1f\u76f8\u5173\uff0c\u6536\u655b\u4ecd\u53ef\u53d1\u751f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u4f4e\u7ef4\u7ed3\u6784\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.21315", "pdf": "https://arxiv.org/pdf/2505.21315", "abs": "https://arxiv.org/abs/2505.21315", "authors": ["Jesujoba O. Alabi", "Michael A. Hedderich", "David Ifeoluwa Adelani", "Dietrich Klakow"], "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead", "categories": ["cs.CL"], "comment": "Working paper", "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u8fc7\u53bb\u4e94\u5e74734\u7bc7\u5173\u4e8e\u975e\u6d32\u8bed\u8a00NLP\u7684\u7814\u7a76\uff0c\u5206\u6790\u4e86\u8fdb\u5c55\u3001\u8d8b\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u4fc3\u8fdb\u5305\u5bb9\u6027\u548c\u53ef\u6301\u7eed\u7814\u7a76\u7684\u5efa\u8bae\u3002", "motivation": "\u975e\u6d32\u8bed\u8a00\u591a\u6837\u6027\u5728NLP\u548cLLM\u4e2d\u672a\u88ab\u5145\u5206\u4ee3\u8868\uff0c\u5bfc\u81f4\u6570\u5b57\u9e3f\u6c9f\u6269\u5927\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790734\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u603b\u7ed3\u6838\u5fc3\u4efb\u52a1\u8fdb\u5c55\u548c\u8d8b\u52bf\u3002", "result": "\u53d1\u73b0\u7814\u7a76\u5174\u8da3\u589e\u957f\uff0c\u4f46\u4ecd\u9700\u66f4\u591a\u8d44\u6e90\u548c\u652f\u6301\u3002", "conclusion": "\u63d0\u51fa\u4fc3\u8fdb\u5305\u5bb9\u6027\u548c\u53ef\u6301\u7eedNLP\u7814\u7a76\u7684\u672a\u6765\u65b9\u5411\u3002", "relevance": 60.0}}
{"id": "2505.21338", "pdf": "https://arxiv.org/pdf/2505.21338", "abs": "https://arxiv.org/abs/2505.21338", "authors": ["Katarzyna Filus", "Mateusz \u017barski"], "title": "Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Similarity manifests in various forms, including semantic similarity that is\nparticularly important, serving as an approximation of human object\ncategorization based on e.g. shared functionalities and evolutionary traits. It\nalso offers practical advantages in computational modeling via lexical\nstructures such as WordNet with constant and interpretable similarity. As in\nthe domain of deep vision, there is still not enough focus on the phenomena\nregarding the similarity perception emergence. We introduce Deep Similarity\nInspector (DSI) -- a systematic framework to inspect how deep vision networks\ndevelop their similarity perception and its alignment with semantic similarity.\nOur experiments show that both Convolutional Neural Networks' (CNNs) and Vision\nTransformers' (ViTs) develop a rich similarity perception during training with\n3 phases (initial similarity surge, refinement, stabilization), with clear\ndifferences between CNNs and ViTs. Besides the gradual mistakes elimination,\nthe mistakes refinement phenomenon can be observed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDeep Similarity Inspector (DSI)\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u6df1\u5ea6\u89c6\u89c9\u7f51\u7edc\u5982\u4f55\u5f62\u6210\u76f8\u4f3c\u6027\u611f\u77e5\u53ca\u5176\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u5bf9\u9f50\u3002\u5b9e\u9a8c\u53d1\u73b0CNN\u548cViT\u5728\u8bad\u7ec3\u4e2d\u7ecf\u5386\u4e09\u4e2a\u9636\u6bb5\uff08\u521d\u59cb\u76f8\u4f3c\u6027\u6fc0\u589e\u3001\u7ec6\u5316\u3001\u7a33\u5b9a\uff09\uff0c\u4e14\u4e24\u8005\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u89c6\u89c9\u7f51\u7edc\u4e2d\u76f8\u4f3c\u6027\u611f\u77e5\u7684\u5f62\u6210\u53ca\u5176\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u5bf9\u9f50\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165DSI\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790CNN\u548cViT\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u76f8\u4f3c\u6027\u611f\u77e5\u53d1\u5c55\u3002", "result": "CNN\u548cViT\u5728\u8bad\u7ec3\u4e2d\u7ecf\u5386\u4e09\u4e2a\u9636\u6bb5\uff0c\u4e14\u4e24\u8005\u5728\u76f8\u4f3c\u6027\u611f\u77e5\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u540c\u65f6\u89c2\u5bdf\u5230\u9519\u8bef\u7ec6\u5316\u73b0\u8c61\u3002", "conclusion": "DSI\u6846\u67b6\u63ed\u793a\u4e86\u6df1\u5ea6\u89c6\u89c9\u7f51\u7edc\u76f8\u4f3c\u6027\u611f\u77e5\u7684\u52a8\u6001\u53d1\u5c55\u8fc7\u7a0b\uff0c\u4e3a\u7406\u89e3\u5176\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "relevance": 40.0}}
{"id": "2505.21339", "pdf": "https://arxiv.org/pdf/2505.21339", "abs": "https://arxiv.org/abs/2505.21339", "authors": ["Henryk Mustroph", "Michel Kunkler", "Stefanie Rinderle-Ma"], "title": "An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Suffix prediction of business processes forecasts the remaining sequence of\nevents until process completion. Current approaches focus on predicting a\nsingle, most likely suffix. However, if the future course of a process is\nexposed to uncertainty or has high variability, the expressiveness of a single\nsuffix prediction can be limited. To address this limitation, we propose\nprobabilistic suffix prediction, a novel approach that approximates a\nprobability distribution of suffixes. The proposed approach is based on an\nUncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)\nsuffix sampling algorithm. We capture epistemic uncertainties via MC dropout\nand aleatoric uncertainties as learned loss attenuation. This technical report\nprovides a detailed evaluation of the U-ED-LSTM's predictive performance and\nassesses its calibration on four real-life event logs with three different\nhyperparameter settings. The results show that i) the U-ED-LSTM has reasonable\npredictive performance across various datasets, ii) aggregating probabilistic\nsuffix predictions into mean values can outperform most likely predictions,\nparticularly for rare prefixes or longer suffixes, and iii) the approach\neffectively captures uncertainties present in event logs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684LSTM\u548c\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u6982\u7387\u540e\u7f00\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4ec5\u9884\u6d4b\u6700\u53ef\u80fd\u7684\u540e\u7f00\uff0c\u4f46\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u6216\u9ad8\u53d8\u5f02\u6027\u573a\u666f\u4e0b\u8868\u73b0\u6709\u9650\u3002", "method": "\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668LSTM\uff08U-ED-LSTM\uff09\u548c\u8499\u7279\u5361\u6d1b\u540e\u7f00\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7MC dropout\u6355\u83b7\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5b66\u4e60\u635f\u5931\u8870\u51cf\u6355\u83b7\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "U-ED-LSTM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5408\u7406\uff0c\u5747\u503c\u805a\u5408\u4f18\u4e8e\u5355\u70b9\u9884\u6d4b\uff0c\u5c24\u5176\u5728\u7f55\u89c1\u524d\u7f00\u6216\u957f\u540e\u7f00\u573a\u666f\u4e0b\uff1b\u80fd\u6709\u6548\u6355\u83b7\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u6982\u7387\u540e\u7f00\u9884\u6d4b\u65b9\u6cd5\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.21324", "pdf": "https://arxiv.org/pdf/2505.21324", "abs": "https://arxiv.org/abs/2505.21324", "authors": ["Yuxin Zhu", "Yuting Guo", "Noah Marchuck", "Abeed Sarker", "Yun Wang"], "title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "categories": ["cs.CL"], "comment": null, "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408LLaMA3\u3001RoBERTa\u548cSVM\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u63d0\u5347ADHD\u8bca\u65ad\u5206\u7c7b\u7684\u9c81\u68d2\u6027\uff0cF1\u5206\u6570\u8fbe0.71\u3002", "motivation": "\u63a2\u7d22LLMs\u4e0e\u4f20\u7edf\u76d1\u7763ML\u6280\u672f\u5728\u533b\u5b66\u6570\u636e\uff08\u5c24\u5176\u662f\u7cbe\u795e\u75c5\u5b66\u9886\u57df\uff09\u4e2d\u7684\u7ed3\u5408\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u3002", "method": "\u96c6\u6210LLaMA3\uff08\u6355\u6349\u957f\u7a0b\u8bed\u4e49\uff09\u3001RoBERTa\uff08\u4e34\u5e8a\u53d9\u8ff0\u5fae\u8c03\uff09\u548cSVM\uff08TF-IDF\u7279\u5f81\uff09\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u805a\u5408\u3002", "result": "\u96c6\u6210\u6a21\u578b\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\uff0cF1\u5206\u65700.71\uff0c\u53ec\u56de\u7387\u63d0\u5347\u4e14\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u7ed3\u5408LLMs\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u4f20\u7edfML\u7684\u89e3\u6790\u80fd\u529b\uff0c\u4e3a\u7cbe\u795e\u75c5\u5b66\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 60.0}}
{"id": "2505.21357", "pdf": "https://arxiv.org/pdf/2505.21357", "abs": "https://arxiv.org/abs/2505.21357", "authors": ["Wenyuan Li", "Shunlin Liang", "Keyan Chen", "Yongzhe Chen", "Han Ma", "Jianglei Xu", "Yichuan Ma", "Shikang Guan", "Husheng Fang", "Zhenwei Shi"], "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nurlhttps://github.com/flyakon/AgriFM.", "AI": {"tldr": "AgriFM\u662f\u4e00\u4e2a\u4e13\u4e3a\u519c\u4e1a\u4f5c\u7269\u6620\u5c04\u8bbe\u8ba1\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684Video Swin Transformer\u67b6\u6784\u5b9e\u73b0\u591a\u5c3a\u5ea6\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u6620\u5c04\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u56fa\u5b9a\u7684\u65f6\u7a7a\u7a97\u53e3\u6216\u5ffd\u7565\u65f6\u95f4\u4fe1\u606f\u3002AgriFM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684Video Swin Transformer\u67b6\u6784\uff0c\u540c\u6b65\u65f6\u7a7a\u4e0b\u91c7\u6837\uff0c\u5229\u7528\u591a\u6e90\u536b\u661f\u6570\u636e\uff08MODIS\u3001Landsat-8/9\u3001Sentinel-2\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "AgriFM\u5728\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548c\u73b0\u6709\u9065\u611f\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "AgriFM\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u7a7a\u5904\u7406\u548c\u591a\u6e90\u6570\u636e\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f5c\u7269\u6620\u5c04\u7684\u51c6\u786e\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21347", "pdf": "https://arxiv.org/pdf/2505.21347", "abs": "https://arxiv.org/abs/2505.21347", "authors": ["Ziheng Cheng", "Yixiao Huang", "Hui Xu", "Somayeh Sojoudi", "Xuandong Zhao", "Dawn Song", "Song Mei"], "title": "OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models", "categories": ["cs.LG"], "comment": null, "summary": "Text-to-Image (T2I) models have achieved remarkable success in generating\nvisual content from text inputs. Although multiple safety alignment strategies\nhave been proposed to prevent harmful outputs, they often lead to overly\ncautious behavior -- rejecting even benign prompts -- a phenomenon known as\n$\\textit{over-refusal}$ that reduces the practical utility of T2I models.\nDespite over-refusal having been observed in practice, there is no large-scale\nbenchmark that systematically evaluates this phenomenon for T2I models. In this\npaper, we present an automatic workflow to construct synthetic evaluation data,\nresulting in OVERT ($\\textbf{OVE}$r-$\\textbf{R}$efusal evaluation on\n$\\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing\nover-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful\nbut benign prompts across nine safety-related categories, along with 1,785\ngenuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility\ntrade-off. Using OVERT, we evaluate several leading T2I models and find that\nover-refusal is a widespread issue across various categories (Figure 1),\nunderscoring the need for further research to enhance the safety alignment of\nT2I models without compromising their functionality.As a preliminary attempt to\nreduce over-refusal, we explore prompt rewriting; however, we find it often\ncompromises faithfulness to the meaning of the original prompts. Finally, we\ndemonstrate the flexibility of our generation framework in accommodating\ndiverse safety requirements by generating customized evaluation data adapting\nto user-defined policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OVERT\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u8fc7\u5ea6\u62d2\u7edd\uff08over-refusal\uff09\u73b0\u8c61\u7684\u57fa\u51c6\uff0c\u5305\u542b4600\u4e2a\u770b\u4f3c\u6709\u5bb3\u4f46\u65e0\u5bb3\u7684\u63d0\u793a\u548c1785\u4e2a\u771f\u6b63\u6709\u5bb3\u7684\u63d0\u793a\u3002\u7814\u7a76\u53d1\u73b0\u8fc7\u5ea6\u62d2\u7edd\u666e\u904d\u5b58\u5728\uff0c\u5e76\u63a2\u7d22\u4e86\u63d0\u793a\u6539\u5199\u65b9\u6cd5\uff0c\u4f46\u53d1\u73b0\u5176\u53ef\u80fd\u5f71\u54cd\u539f\u59cb\u63d0\u793a\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "T2I\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u7b56\u7565\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u62d2\u7edd\u73b0\u8c61\uff0c\u964d\u4f4e\u6a21\u578b\u5b9e\u7528\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u8fd9\u4e00\u73b0\u8c61\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u6784\u5efa\u5408\u6210\u8bc4\u4f30\u6570\u636e\u7684\u5de5\u4f5c\u6d41\uff0c\u521b\u5efaOVERT\u57fa\u51c6\uff0c\u5305\u542b\u770b\u4f3c\u6709\u5bb3\u4f46\u65e0\u5bb3\u7684\u63d0\u793a\uff084600\u4e2a\uff09\u548c\u771f\u6b63\u6709\u5bb3\u7684\u63d0\u793a\uff081785\u4e2a\uff09\u3002\u8bc4\u4f30\u591a\u4e2aT2I\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u63d0\u793a\u6539\u5199\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u8fc7\u5ea6\u62d2\u7edd\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u63d0\u793a\u6539\u5199\u53ef\u80fd\u5f71\u54cd\u539f\u59cb\u63d0\u793a\u7684\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347T2I\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u800c\u4e0d\u635f\u5bb3\u5176\u529f\u80fd\u6027\u3002OVERT\u6846\u67b6\u53ef\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u5b89\u5168\u9700\u6c42\u3002", "relevance": 70.0}}
{"id": "2505.21342", "pdf": "https://arxiv.org/pdf/2505.21342", "abs": "https://arxiv.org/abs/2505.21342", "authors": ["Valentin Knappich", "Annemarie Friedrich", "Anna H\u00e4tty", "Simon Razniewski"], "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "categories": ["cs.CL"], "comment": null, "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PEDANTIC\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u4e13\u5229\u6743\u5229\u8981\u6c42\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86LLM\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e13\u5229\u6743\u5229\u8981\u6c42\u4e2d\u7684\u6a21\u7cca\u6027\u662f\u4e13\u5229\u7533\u8bf7\u88ab\u62d2\u7684\u5e38\u89c1\u539f\u56e0\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\u3002\u81ea\u52a8\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u4e13\u5229\u5ba1\u67e5\u6548\u7387\u3002", "method": "\u4f7f\u7528LLM\u4eceUSPTO\u6587\u4ef6\u4e2d\u63d0\u53d6\u6a21\u7cca\u6027\u539f\u56e0\uff0c\u6784\u5efa14k\u6761NLP\u76f8\u5173\u4e13\u5229\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u548cLLM-as-Judge\u8bc4\u4f30\u3002", "result": "LLM\u5728\u6a21\u7cca\u6027\u9884\u6d4b\u4e0a\u8868\u73b0\u4e0d\u5982\u903b\u8f91\u56de\u5f52\uff0c\u4f46\u80fd\u6b63\u786e\u8bc6\u522b\u539f\u56e0\u3002", "conclusion": "PEDANTIC\u4e3a\u4e13\u5229AI\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "relevance": 20.0}}
{"id": "2505.21370", "pdf": "https://arxiv.org/pdf/2505.21370", "abs": "https://arxiv.org/abs/2505.21370", "authors": ["Xinyuan Wang", "Lian Peng", "Xiangcheng Li", "Yilin He", "KinTak U"], "title": "YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration", "categories": ["cs.CV"], "comment": null, "summary": "Object detection in remote sensing imagery remains a challenging task due to\nextreme scale variation, dense object distributions, and cluttered backgrounds.\nWhile recent detectors such as YOLOv8 have shown promising results, their\nbackbone architectures lack explicit mechanisms to guide multi-scale feature\nrefinement, limiting performance on high-resolution aerial data. In this work,\nwe propose YOLO-SPCI, an attention-enhanced detection framework that introduces\na lightweight Selective-Perspective-Class Integration (SPCI) module to improve\nfeature representation. The SPCI module integrates three components: a\nSelective Stream Gate (SSG) for adaptive regulation of global feature flow, a\nPerspective Fusion Module (PFM) for context-aware multi-scale integration, and\na Class Discrimination Module (CDM) to enhance inter-class separability. We\nembed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone,\nenabling effective refinement while preserving compatibility with the original\nneck and head. Experiments on the NWPU VHR-10 dataset demonstrate that\nYOLO-SPCI achieves superior performance compared to state-of-the-art detectors.", "AI": {"tldr": "YOLO-SPCI\u662f\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6539\u8fdb\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7SPCI\u6a21\u5757\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u8868\u793a\uff0c\u5728\u9065\u611f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c3a\u5ea6\u53d8\u5316\u5927\u3001\u76ee\u6807\u5bc6\u96c6\u548c\u80cc\u666f\u590d\u6742\u7b49\u6311\u6218\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u5982YOLOv8\u7f3a\u4e4f\u663e\u5f0f\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u4f18\u5316\u673a\u5236\u3002", "method": "\u63d0\u51faSPCI\u6a21\u5757\uff0c\u5305\u542b\u9009\u62e9\u6027\u6d41\u95e8\uff08SSG\uff09\u3001\u89c6\u89d2\u878d\u5408\u6a21\u5757\uff08PFM\uff09\u548c\u7c7b\u522b\u5224\u522b\u6a21\u5757\uff08CDM\uff09\uff0c\u5d4c\u5165YOLOv8\u7684P3\u548cP5\u9636\u6bb5\u3002", "result": "\u5728NWPU VHR-10\u6570\u636e\u96c6\u4e0a\uff0cYOLO-SPCI\u4f18\u4e8e\u73b0\u6709\u68c0\u6d4b\u5668\u3002", "conclusion": "SPCI\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86YOLOv8\u5728\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21360", "pdf": "https://arxiv.org/pdf/2505.21360", "abs": "https://arxiv.org/abs/2505.21360", "authors": ["Dhanesh Ramachandram"], "title": "CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models", "categories": ["cs.LG"], "comment": null, "summary": "Competing risks are crucial considerations in survival modelling,\nparticularly in healthcare domains where patients may experience multiple\ndistinct event types. We propose CRISP-NAM (Competing Risks Interpretable\nSurvival Prediction with Neural Additive Models), an interpretable neural\nadditive model for competing risks survival analysis which extends the neural\nadditive architecture to model cause-specific hazards while preserving\nfeature-level interpretability. Each feature contributes independently to risk\nestimation through dedicated neural networks, allowing for visualization of\ncomplex non-linear relationships between covariates and each competing risk. We\ndemonstrate competitive performance on multiple datasets compared to existing\napproaches.", "AI": {"tldr": "CRISP-NAM\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\uff0c\u7528\u4e8e\u7ade\u4e89\u98ce\u9669\u751f\u5b58\u5206\u6790\uff0c\u901a\u8fc7\u72ec\u7acb\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u7279\u5f81\u5bf9\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u533b\u7597\u9886\u57df\uff0c\u60a3\u8005\u53ef\u80fd\u7ecf\u5386\u591a\u79cd\u4e8b\u4ef6\u7c7b\u578b\uff0c\u7ade\u4e89\u98ce\u9669\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u6269\u5c55\u795e\u7ecf\u52a0\u6cd5\u67b6\u6784\uff0c\u5efa\u6a21\u7279\u5b9a\u539f\u56e0\u98ce\u9669\uff0c\u6bcf\u4e2a\u7279\u5f81\u901a\u8fc7\u72ec\u7acb\u795e\u7ecf\u7f51\u7edc\u8d21\u732e\u98ce\u9669\u4f30\u8ba1\uff0c\u53ef\u89c6\u5316\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CRISP-NAM\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u7ade\u4e89\u98ce\u9669\u5206\u6790\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.21354", "pdf": "https://arxiv.org/pdf/2505.21354", "abs": "https://arxiv.org/abs/2505.21354", "authors": ["Bidyarthi Paul", "Jalisha Jashim Era", "Mirazur Rahman Zim", "Tahmid Sattar Aothoi", "Faisal Muhammad Shah"], "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SOMADHAN\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bed\u6570\u5b66\u5e94\u7528\u9898\uff08MWPs\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u79cdLLM\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\uff0c\u53d1\u73b0CoT\u63d0\u793a\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bedMWPs\u7684\u6311\u6218\uff0c\u586b\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u521b\u5efaSOMADHAN\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cdLLM\uff08\u5982GPT-4o\u3001LLaMA\u7b49\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4e0b\u7684\u8868\u73b0\uff0c\u7ed3\u5408CoT\u63d0\u793a\u548cLoRA\u5fae\u8c03\u3002", "result": "LLaMA-3.3 70B\u5728\u5c11\u6837\u672cCoT\u63d0\u793a\u4e0b\u8fbe\u523088%\u51c6\u786e\u7387\uff0cCoT\u663e\u8457\u63d0\u5347\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u5de5\u4f5c\u586b\u8865\u4e86\u5b5f\u52a0\u62c9\u8bedNLP\u7684\u7a7a\u767d\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u53ef\u6269\u5c55\u6846\u67b6\u3002", "relevance": 60.0}}
{"id": "2505.21374", "pdf": "https://arxiv.org/pdf/2505.21374", "abs": "https://arxiv.org/abs/2505.21374", "authors": ["Junhao Cheng", "Yuying Ge", "Teng Wang", "Yixiao Ge", "Jing Liao", "Ying Shan"], "title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?", "categories": ["cs.CV"], "comment": "Homepage: https://github.com/TencentARC/Video-Holmes", "summary": "Recent advances in CoT reasoning and RL post-training have been reported to\nenhance video reasoning capabilities of MLLMs. This progress naturally raises a\nquestion: can these models perform complex video reasoning in a manner\ncomparable to human experts? However, existing video benchmarks primarily\nevaluate visual perception and grounding abilities, with questions that can be\nanswered based on explicit prompts or isolated visual cues. Such benchmarks do\nnot fully capture the intricacies of real-world reasoning, where humans must\nactively search for, integrate, and analyze multiple clues before reaching a\nconclusion. To address this issue, we present Video-Holmes, a benchmark\ninspired by the reasoning process of Sherlock Holmes, designed to evaluate the\ncomplex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837\nquestions derived from 270 manually annotated suspense short films, which spans\nseven carefully designed tasks. Each task is constructed by first identifying\nkey events and causal relationships within films, and then designing questions\nthat require models to actively locate and connect multiple relevant visual\nclues scattered across different video segments. Our comprehensive evaluation\nof state-of-the-art MLLMs reveals that, while these models generally excel at\nvisual perception, they encounter substantial difficulties with integrating\ninformation and often miss critical clues. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models\nscoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for\nmultimodal reasoning, motivating models to reason more like humans and\nemphasizing the ongoing challenges in this field. The benchmark is released in\nhttps://github.com/TencentARC/Video-Holmes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Video-Holmes\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u4fe1\u606f\u6574\u5408\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u89c6\u89c9\u611f\u77e5\u548c\u57fa\u7840\u80fd\u529b\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u63a8\u7406\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u6807\u6ce8270\u90e8\u60ac\u7591\u77ed\u7247\uff0c\u8bbe\u8ba1\u4e861,837\u4e2a\u95ee\u9898\uff0c\u8986\u76d67\u4e2a\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u4e3b\u52a8\u5b9a\u4f4d\u5e76\u6574\u5408\u5206\u6563\u7684\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u6700\u4f73\u6a21\u578bGemini-2.5-Pro\u7684\u51c6\u786e\u7387\u4ec5\u4e3a45%\uff0c\u5927\u591a\u6570\u6a21\u578b\u4f4e\u4e8e40%\uff0c\u8868\u660e\u6a21\u578b\u5728\u4fe1\u606f\u6574\u5408\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "Video-Holmes\u53ef\u4f5c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7684\u201cHolmes\u6d4b\u8bd5\u201d\uff0c\u63a8\u52a8\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7a81\u663e\u8be5\u9886\u57df\u7684\u6311\u6218\u3002", "relevance": 70.0}}
{"id": "2505.21363", "pdf": "https://arxiv.org/pdf/2505.21363", "abs": "https://arxiv.org/abs/2505.21363", "authors": ["Anissa Alloula", "Charles Jones", "Ben Glocker", "Bart\u0142omiej W. Papie\u017c"], "title": "Subgroups Matter for Robust Bias Mitigation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and suggest it as a alternative lever for improving the\nrobustness and fairness of machine learning models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u5931\u8d25\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u5b50\u7fa4\u5b9a\u4e49\u662f\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u5b50\u7fa4\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u4e0d\u65ad\u6d8c\u73b0\uff0c\u4f46\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5b50\u7fa4\u5b9a\u4e49\u5bf9\u65b9\u6cd5\u6210\u8d25\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b9e\u9a8c\uff08\u89c6\u89c9\u548c\u8bed\u8a00\u5206\u7c7b\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5b50\u7fa4\u5b9a\u4e49\uff08\u7c97\u7c92\u5ea6\u3001\u7ec6\u7c92\u5ea6\u3001\u4ea4\u53c9\u3001\u566a\u58f0\uff09\u5bf9\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u5b50\u7fa4\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u67d0\u4e9b\u5b50\u7fa4\u5b9a\u4e49\u751a\u81f3\u5bfc\u81f4\u6bd4\u4e0d\u7f13\u89e3\u66f4\u5dee\u7684\u7ed3\u679c\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6709\u65f6\u9700\u7528\u4e0d\u540c\u5b50\u7fa4\u5b9e\u73b0\u516c\u5e73\u6027\u6539\u8fdb\u3002", "conclusion": "\u5b50\u7fa4\u5b9a\u4e49\u662f\u504f\u89c1\u7f13\u89e3\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8c28\u614e\u9009\u62e9\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "relevance": 75.0}}
{"id": "2505.21362", "pdf": "https://arxiv.org/pdf/2505.21362", "abs": "https://arxiv.org/abs/2505.21362", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u7528\u6237\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\uff08\u5982\u5e74\u9f84\u3001\u804c\u4e1a\u3001\u6559\u80b2\u6c34\u5e73\uff09\u4e0b\u884c\u4e3a\u9002\u5e94\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\uff08\u7528\u6237\u8d44\u6599\uff09\u548c\u9690\u5f0f\uff08\u591a\u8f6e\u5bf9\u8bdd\u5386\u53f2\uff09\u65b9\u5f0f\u5f15\u5165\u7279\u5f81\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u591a\u5173\u6ce8\u5355\u8f6e\u63d0\u793a\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u5e38\u4f9d\u8d56\u5bf9\u8bdd\u5386\u53f2\u8fdb\u884c\u4e0a\u4e0b\u6587\u9002\u5e94\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u591a\u4ee3\u7406\u6d41\u6c34\u7ebf\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5bf9\u8bdd\u5386\u53f2\u548c\u7528\u6237\u8d44\u6599\uff0c\u5229\u7528VSM 2013\u95ee\u5377\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u4f1a\u6839\u636e\u4eba\u53e3\u7279\u5f81\u8c03\u6574\u8868\u8fbe\uff0c\u5c24\u5176\u662f\u5e74\u9f84\u548c\u6559\u80b2\u6c34\u5e73\uff0c\u4f46\u4e00\u81f4\u6027\u4e0d\u540c\u3002\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u63a8\u7406\u80fd\u529b\u5bf9LLM\u7684\u793e\u4f1a\u4eba\u53e3\u9002\u5e94\u6027\u81f3\u5173\u91cd\u8981\u3002", "relevance": 85.0}}
{"id": "2505.21375", "pdf": "https://arxiv.org/pdf/2505.21375", "abs": "https://arxiv.org/abs/2505.21375", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Yueying Li", "Di Wang", "Haotian Wang", "Zonghao Guo", "Zefan Wang", "Boqi Shan", "Long Lan", "Yulin Wang", "Hongzhen Wang", "Wenjing Yang", "Bo Du", "Jing Zhang"], "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGeoLLaVA-8K\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u80cc\u666f\u4ee4\u724c\u526a\u679d\u548c\u951a\u5b9a\u4ee4\u724c\u9009\u62e9\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u4ee4\u724c\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u5bf9\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u63d0\u51fa\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u7a00\u7f3a\u548c\u4ee4\u724c\u7206\u70b8\u95ee\u9898\u3002", "method": "\u5f15\u5165SuperRS-VQA\u548cHighRS-VQA\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u80cc\u666f\u4ee4\u724c\u526a\u679d\u548c\u951a\u5b9a\u4ee4\u724c\u9009\u62e9\u7b56\u7565\uff0c\u6784\u5efaGeoLLaVA-8K\u6a21\u578b\u3002", "result": "GeoLLaVA-8K\u5728XLRS-Bench\u4e0a\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.21364", "pdf": "https://arxiv.org/pdf/2505.21364", "abs": "https://arxiv.org/abs/2505.21364", "authors": ["James Oldfield", "Shawn Im", "Yixuan Li", "Mihalis A. Nicolaou", "Ioannis Patras", "Grigorios G Chrysos"], "title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multilayer perceptrons (MLPs) are an integral part of large language models,\nyet their dense representations render them difficult to understand, edit, and\nsteer. Recent methods learn interpretable approximations via neuron-level\nsparsity, yet fail to faithfully reconstruct the original\nmapping--significantly increasing model's next-token cross-entropy loss. In\nthis paper, we advocate for moving to layer-level sparsity to overcome the\naccuracy trade-off in sparse layer approximation. Under this paradigm, we\nintroduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear\nUnits, expanding pre-trained dense layers into tens of thousands of specialized\nsublayers. Through a flexible form of tensor factorization, each sparsely\nactivating MxD sublayer implements a linear transformation with full-rank\nweights--preserving the original decoders' expressive capacity even under heavy\nsparsity. Experimentally, we show that MxDs significantly outperform\nstate-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier\nin language models with up to 3B parameters. Further evaluations on sparse\nprobing and feature steering demonstrate that MxDs learn similarly specialized\nfeatures of natural language--opening up a promising new avenue for designing\ninterpretable yet faithful decompositions. Our code is included at:\nhttps://github.com/james-oldfield/MxD/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixture of Decoders (MxDs)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u7ea7\u7a00\u758f\u6027\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u7a00\u758f\u6027\u5b66\u4e60\u53ef\u89e3\u91ca\u8fd1\u4f3c\uff0c\u4f46\u65e0\u6cd5\u5fe0\u5b9e\u91cd\u5efa\u539f\u59cb\u6620\u5c04\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "MxDs\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u5c06\u9884\u8bad\u7ec3\u7684\u5bc6\u96c6\u5c42\u6269\u5c55\u4e3a\u6210\u5343\u4e0a\u4e07\u4e2a\u7a00\u758f\u6fc0\u6d3b\u7684\u5b50\u5c42\uff0c\u4fdd\u7559\u539f\u59cb\u8868\u8fbe\u80fd\u529b\u3002", "result": "MxDs\u57283B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982Transcoders\uff09\uff0c\u5e76\u5728\u7a00\u758f\u63a2\u6d4b\u548c\u7279\u5f81\u5bfc\u5411\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MxDs\u4e3a\u8bbe\u8ba1\u65e2\u53ef\u89e3\u91ca\u53c8\u5fe0\u5b9e\u5206\u89e3\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 85.0}}
{"id": "2505.21378", "pdf": "https://arxiv.org/pdf/2505.21378", "abs": "https://arxiv.org/abs/2505.21378", "authors": ["Jules Watson", "Xi Wang", "Raymond Liu", "Suzanne Stevenson", "Barend Beekhuizen"], "title": "Analyzing values about gendered language reform in LLMs' revisions", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5728\u6587\u672c\u4fee\u8ba2\u4e2d\u5bf9\u6027\u522b\u89d2\u8272\u540d\u8bcd\u7684\u4fee\u8ba2\u53ca\u5176\u7406\u7531\uff0c\u8bc4\u4f30\u5176\u4e0e\u5973\u6027\u548c\u8de8\u6027\u522b\u5305\u5bb9\u8bed\u8a00\u6539\u9769\u7684\u5951\u5408\u5ea6\uff0c\u5e76\u5206\u6790\u4e86LLMs\u662f\u5426\u5bf9\u8bed\u5883\u6548\u5e94\u654f\u611f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLMs\u5728\u6027\u522b\u8bed\u8a00\u6539\u9769\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u5176\u662f\u5426\u80fd\u591f\u4e0e\u4eba\u7c7b\u4e00\u6837\u5bf9\u8bed\u5883\u6548\u5e94\u654f\u611f\uff0c\u4ece\u800c\u63a8\u52a8\u4ef7\u503c\u5bf9\u9f50\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u5bf9\u6027\u522b\u89d2\u8272\u540d\u8bcd\u7684\u4fee\u8ba2\u53ca\u5176\u7406\u7531\uff0c\u7ed3\u5408\u793e\u4f1a\u8bed\u8a00\u5b66\u89c6\u89d2\uff0c\u8bc4\u4f30\u5176\u4e0e\u8bed\u8a00\u6539\u9769\u7684\u5951\u5408\u5ea6\u53ca\u5bf9\u8bed\u5883\u6548\u5e94\u7684\u654f\u611f\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u6027\u522b\u8bed\u8a00\u6539\u9769\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8bed\u5883\u654f\u611f\u6027\uff0c\u652f\u6301\u5176\u5728\u4ef7\u503c\u5bf9\u9f50\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faLLMs\u5728\u6027\u522b\u8bed\u8a00\u4fee\u8ba2\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u4ef7\u503c\u5bf9\u9f50\u3002", "relevance": 70.0}}
{"id": "2505.21377", "pdf": "https://arxiv.org/pdf/2505.21377", "abs": "https://arxiv.org/abs/2505.21377", "authors": ["Yidi Li", "Jun Xiao", "Zhengda Lu", "Yiqun Wang", "Haiyong Jiang"], "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "This work presents a novel text-to-vector graphics generation approach,\nDream3DVG, allowing for arbitrary viewpoint viewing, progressive detail\noptimization, and view-dependent occlusion awareness. Our approach is a\ndual-branch optimization framework, consisting of an auxiliary 3D Gaussian\nSplatting optimization branch and a 3D vector graphics optimization branch. The\nintroduced 3DGS branch can bridge the domain gaps between text prompts and\nvector graphics with more consistent guidance. Moreover, 3DGS allows for\nprogressive detail control by scheduling classifier-free guidance, facilitating\nguiding vector graphics with coarse shapes at the initial stages and finer\ndetails at later stages. We also improve the view-dependent occlusions by\ndevising a visibility-awareness rendering module. Extensive results on 3D\nsketches and 3D iconographies, demonstrate the superiority of the method on\ndifferent abstraction levels of details, cross-view consistency, and\nocclusion-aware stroke culling.", "AI": {"tldr": "Dream3DVG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u5230\u77e2\u91cf\u56fe\u5f62\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u4efb\u610f\u89c6\u89d2\u67e5\u770b\u3001\u6e10\u8fdb\u7ec6\u8282\u4f18\u5316\u548c\u89c6\u89d2\u76f8\u5173\u906e\u6321\u611f\u77e5\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u63d0\u793a\u4e0e\u77e2\u91cf\u56fe\u5f62\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u6e10\u8fdb\u7ec6\u8282\u63a7\u5236\u548c\u906e\u6321\u611f\u77e5\u529f\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec3D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u5206\u652f\u548c3D\u77e2\u91cf\u56fe\u5f62\u4f18\u5316\u5206\u652f\uff0c\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u548c\u53ef\u89c1\u6027\u611f\u77e5\u6e32\u67d3\u6a21\u5757\u3002", "result": "\u57283D\u8349\u56fe\u548c3D\u56fe\u6807\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u5728\u7ec6\u8282\u62bd\u8c61\u3001\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u906e\u6321\u611f\u77e5\u7b14\u753b\u5254\u9664\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "Dream3DVG\u5728\u6587\u672c\u5230\u77e2\u91cf\u56fe\u5f62\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u3002", "relevance": 30.0}}
{"id": "2505.21366", "pdf": "https://arxiv.org/pdf/2505.21366", "abs": "https://arxiv.org/abs/2505.21366", "authors": ["Qi Yu", "Zhichen Zeng", "Yuchen Yan", "Zhining Liu", "Baoyu Jing", "Ruizhong Qiu", "Ariful Azad", "Hanghang Tong"], "title": "PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment", "categories": ["cs.LG"], "comment": null, "summary": "Network alignment (NA) aims to identify node correspondence across different\nnetworks and serves as a critical cornerstone behind various downstream\nmulti-network learning tasks. Despite growing research in NA, there lacks a\ncomprehensive library that facilitates the systematic development and\nbenchmarking of NA methods. In this work, we introduce PLANETALIGN, a\ncomprehensive Python library for network alignment that features a rich\ncollection of built-in datasets, methods, and evaluation pipelines with\neasy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA\nmethods with extensible APIs for easy use and development of NA methods. Our\nstandardized evaluation pipeline encompasses a wide range of metrics, enabling\na systematic assessment of the effectiveness, scalability, and robustness of NA\nmethods. Through extensive comparative studies, we reveal practical insights\ninto the strengths and limitations of existing NA methods. We hope that\nPLANETALIGN can foster a deeper understanding of the NA problem and facilitate\nthe development and benchmarking of more effective, scalable, and robust\nmethods in the future. The source code of PLANETALIGN is available at\nhttps://github.com/yq-leo/PlanetAlign.", "AI": {"tldr": "PLANETALIGN\u662f\u4e00\u4e2a\u7528\u4e8e\u7f51\u7edc\u5bf9\u9f50\uff08NA\uff09\u7684Python\u5e93\uff0c\u63d0\u4f9b\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u65e8\u5728\u4fc3\u8fdbNA\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7f51\u7edc\u5bf9\u9f50\u5728\u591a\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "method": "PLANETALIGN\u6574\u5408\u4e8618\u4e2a\u6570\u636e\u96c6\u548c14\u79cdNA\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u548c\u6613\u7528API\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709NA\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "PLANETALIGN\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3NA\u95ee\u9898\uff0c\u5e76\u63a8\u52a8\u66f4\u6709\u6548\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u65b9\u6cd5\u53d1\u5c55\u3002", "relevance": 30.0}}
{"id": "2505.20663", "pdf": "https://arxiv.org/pdf/2505.20663", "abs": "https://arxiv.org/abs/2505.20663", "authors": ["Xu Kang", "Siqi Jiang", "Kangwei Xu", "Jiahao Li", "Ruibo Wu"], "title": "TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3; I.2"], "comment": "18 pages, 4 figures", "summary": "Terpenoids are a crucial class of natural products that have been studied for\nover 150 years, but their interdisciplinary nature (spanning chemistry,\npharmacology, and biology) complicates knowledge integration. To address this,\nthe authors developed TeroSeek, a curated knowledge base (KB) built from two\ndecades of terpenoid literature, coupled with an AI-powered question-answering\nchatbot and web service. Leveraging a retrieval-augmented generation (RAG)\nframework, TeroSeek provides structured, high-quality information and\noutperforms general-purpose large language models (LLMs) in terpenoid-related\nqueries. It serves as a domain-specific expert tool for multidisciplinary\nresearch and is publicly available at http://teroseek.qmclab.com.", "AI": {"tldr": "TeroSeek\u662f\u4e00\u4e2a\u9488\u5bf9\u841c\u7c7b\u5316\u5408\u7269\u7684\u77e5\u8bc6\u5e93\u548c\u95ee\u7b54\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u4f18\u4e8e\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4e3a\u591a\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u89e3\u51b3\u841c\u7c7b\u5316\u5408\u7269\u7814\u7a76\u4e2d\u7684\u77e5\u8bc6\u6574\u5408\u96be\u9898\uff0c\u56e0\u5176\u8de8\u5b66\u79d1\u6027\u8d28\uff08\u5316\u5b66\u3001\u836f\u7406\u5b66\u3001\u751f\u7269\u5b66\uff09\u5bfc\u81f4\u4fe1\u606f\u5206\u6563\u3002", "method": "\u5f00\u53d1\u4e86TeroSeek\u77e5\u8bc6\u5e93\uff0c\u7ed3\u5408AI\u9a71\u52a8\u7684\u95ee\u7b54\u673a\u5668\u4eba\u548cRAG\u6846\u67b6\uff0c\u4ece20\u5e74\u6587\u732e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u3002", "result": "TeroSeek\u5728\u841c\u7c7b\u76f8\u5173\u67e5\u8be2\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528LLM\uff0c\u6210\u4e3a\u9886\u57df\u4e13\u5bb6\u5de5\u5177\u3002", "conclusion": "TeroSeek\u4e3a\u841c\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u591a\u5b66\u79d1\u652f\u6301\u5de5\u5177\uff0c\u5e76\u5df2\u516c\u5f00\u53ef\u7528\u3002", "relevance": 30.0}}
{"id": "2505.21380", "pdf": "https://arxiv.org/pdf/2505.21380", "abs": "https://arxiv.org/abs/2505.21380", "authors": ["Byungjun Kim", "Minju Kim", "Hyeonchu Park", "Bugeun Kim"], "title": "PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense", "categories": ["cs.CL"], "comment": "Under review", "summary": "As malicious users increasingly employ phonetic substitution to evade hate\nspeech detection, researchers have investigated such strategies. However, two\nkey challenges remain. First, existing studies have overlooked the Korean\nlanguage, despite its vulnerability to phonetic perturbations due to its\nphonographic nature. Second, prior work has primarily focused on constructing\ndatasets rather than developing architectural defenses. To address these\nchallenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)\nthat exploits the phonological characteristics of the Korean writing system,\nand (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the\ndetector's robustness by incorporating phonetic information at the\narchitectural level. Our experimental results demonstrate the effectiveness of\nour proposed methods on both perturbed and unperturbed datasets, suggesting\nthat they not only improve detection performance but also reflect realistic\nadversarial behaviors employed by malicious users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPHISH\u548cMESH\u65b9\u6cd5\uff0c\u9488\u5bf9\u97e9\u8bed\u4e2d\u7684\u8bed\u97f3\u66ff\u6362\u653b\u51fb\uff0c\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u97e9\u8bed\u53ca\u5176\u8bed\u97f3\u7279\u6027\uff0c\u4e14\u591a\u96c6\u4e2d\u4e8e\u6570\u636e\u96c6\u6784\u5efa\u800c\u975e\u67b6\u6784\u9632\u5fa1\u3002", "method": "\u63d0\u51faPHISH\uff08\u5229\u7528\u97e9\u8bed\u8bed\u97f3\u7279\u6027\uff09\u548cMESH\uff08\u5728\u67b6\u6784\u5c42\u9762\u878d\u5408\u8bed\u4e49\u548c\u8bed\u97f3\u7279\u5f81\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5728\u6270\u52a8\u548c\u975e\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u5747\u6709\u6548\uff0c\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u5e76\u53cd\u6620\u771f\u5b9e\u5bf9\u6297\u884c\u4e3a\u3002", "conclusion": "PHISH\u548cMESH\u80fd\u6709\u6548\u5e94\u5bf9\u97e9\u8bed\u4e2d\u7684\u8bed\u97f3\u66ff\u6362\u653b\u51fb\uff0c\u589e\u5f3a\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21381", "pdf": "https://arxiv.org/pdf/2505.21381", "abs": "https://arxiv.org/abs/2505.21381", "authors": ["Linshuang Diao", "Dayong Ren", "Sensen Song", "Yurong Qian"], "title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding", "categories": ["cs.CV"], "comment": null, "summary": "State Space models (SSMs) such as PointMamba enable efficient feature\nextraction for point cloud self-supervised learning with linear complexity,\noutperforming Transformers in computational efficiency. However, existing\nPointMamba-based methods depend on complex token ordering and random masking,\nwhich disrupt spatial continuity and local semantic correlations. We propose\nZigzagPointMamba to tackle these challenges. The core of our approach is a\nsimple zigzag scan path that globally sequences point cloud tokens, enhancing\nspatial continuity by preserving the proximity of spatially adjacent point\ntokens. Nevertheless, random masking undermines local semantic modeling in\nself-supervised learning. To address this, we introduce a Semantic-Siamese\nMasking Strategy (SMS), which masks semantically similar tokens to facilitate\nreconstruction by integrating local features of original and similar tokens.\nThis overcomes the dependence on isolated local features and enables robust\nglobal semantic modeling. Our pre-trained ZigzagPointMamba weights\nsignificantly improve downstream tasks, achieving a 1.59% mIoU gain on\nShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for\nclassification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for\nthe classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of\nScanObjectNN. The code is available at:\nhttps://anonymous.4open.science/r/ZigzagPointMamba-1800/", "AI": {"tldr": "ZigzagPointMamba\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684State Space\u6a21\u578b\uff0c\u901a\u8fc7zigzag\u626b\u63cf\u8def\u5f84\u548c\u8bed\u4e49\u76f8\u4f3c\u63a9\u7801\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u70b9\u4e91\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6548\u7387\u548c\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684PointMamba\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u6807\u8bb0\u6392\u5e8f\u548c\u968f\u673a\u63a9\u7801\uff0c\u7834\u574f\u4e86\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u5c40\u90e8\u8bed\u4e49\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528zigzag\u626b\u63cf\u8def\u5f84\u5168\u5c40\u6392\u5e8f\u70b9\u4e91\u6807\u8bb0\uff0c\u5e76\u63d0\u51fa\u8bed\u4e49\u76f8\u4f3c\u63a9\u7801\u7b56\u7565\uff08SMS\uff09\u4ee5\u589e\u5f3a\u5c40\u90e8\u8bed\u4e49\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982ShapeNetPart\u5206\u5272\u4efb\u52a1\u63d0\u53471.59% mIoU\uff0cModelNet40\u5206\u7c7b\u4efb\u52a1\u63d0\u53470.4%\u51c6\u786e\u7387\u3002", "conclusion": "ZigzagPointMamba\u901a\u8fc7\u6539\u8fdb\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u8bed\u4e49\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u4efb\u52a1\u7684\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.21372", "pdf": "https://arxiv.org/pdf/2505.21372", "abs": "https://arxiv.org/abs/2505.21372", "authors": ["Andrej Schwanke", "Lyubomir Ivanov", "David Salinas", "Fabio Ferreira", "Aaron Klein", "Frank Hutter", "Arber Zela"], "title": "Improving LLM-based Global Optimization with Search Space Partitioning", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 10 figures, 3 tables", "summary": "Large Language Models (LLMs) have recently emerged as effective surrogate\nmodels and candidate generators within global optimization frameworks for\nexpensive blackbox functions. Despite promising results, LLM-based methods\noften struggle in high-dimensional search spaces or when lacking\ndomain-specific priors, leading to sparse or uninformative suggestions. To\novercome these limitations, we propose HOLLM, a novel global optimization\nalgorithm that enhances LLM-driven sampling by partitioning the search space\ninto promising subregions. Each subregion acts as a ``meta-arm'' selected via a\nbandit-inspired scoring mechanism that effectively balances exploration and\nexploitation. Within each selected subregion, an LLM then proposes high-quality\ncandidate points, without any explicit domain knowledge. Empirical evaluation\non standard optimization benchmarks shows that HOLLM consistently matches or\nsurpasses leading Bayesian optimization and trust-region methods, while\nsubstantially outperforming global LLM-based sampling strategies.", "AI": {"tldr": "HOLLM\u662f\u4e00\u79cd\u65b0\u578b\u5168\u5c40\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u641c\u7d22\u7a7a\u95f4\u5212\u5206\u4e3a\u6709\u6f5c\u529b\u7684\u5b50\u533a\u57df\uff0c\u7ed3\u5408LLM\u751f\u6210\u9ad8\u8d28\u91cf\u5019\u9009\u70b9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u9ad8\u7ef4\u641c\u7d22\u7a7a\u95f4\u6216\u7f3a\u4e4f\u9886\u57df\u5148\u9a8c\u65f6\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5c06\u641c\u7d22\u7a7a\u95f4\u5206\u533a\uff0c\u5229\u7528\u7c7b\u4f3cbandit\u7684\u8bc4\u5206\u673a\u5236\u9009\u62e9\u5b50\u533a\u57df\uff0cLLM\u5728\u5b50\u533a\u57df\u5185\u751f\u6210\u5019\u9009\u70b9\u3002", "result": "\u5728\u6807\u51c6\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHOLLM\u8868\u73b0\u4f18\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u5168\u5c40LLM\u91c7\u6837\u7b56\u7565\u3002", "conclusion": "HOLLM\u901a\u8fc7\u5206\u533a\u548c\u8bc4\u5206\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u5168\u5c40\u4f18\u5316\u4e2d\u7684\u8868\u73b0\u3002", "relevance": 70.0}}
{"id": "2505.21389", "pdf": "https://arxiv.org/pdf/2505.21389", "abs": "https://arxiv.org/abs/2505.21389", "authors": ["Xuanwen Ding", "Chengjun Pan", "Zejun Li", "Jiwen Zhang", "Siyuan Wang", "Zhongyu Wei"], "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench.", "AI": {"tldr": "AutoJudger\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u9ad8\u6548\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u6d4b\u8bd5\u95ee\u9898\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u8bc4\u4f30\u6210\u672c\u6025\u5267\u4e0a\u5347\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "AutoJudger\u7ed3\u5408\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u548c\u81ea\u4e3b\u8bc4\u4f30\u4ee3\u7406\uff0c\u52a8\u6001\u9009\u62e9\u6d4b\u8bd5\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u68c0\u7d22\u548c\u52a8\u6001\u8bb0\u5fc6\u673a\u5236\u786e\u4fdd\u95ee\u9898\u591a\u6837\u6027\u548c\u6311\u6218\u6027\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoJudger\u4ec5\u4f7f\u75284%\u7684\u6570\u636e\u5373\u53ef\u8fbe\u523090%\u4ee5\u4e0a\u7684\u6392\u540d\u51c6\u786e\u6027\u3002", "conclusion": "AutoJudger\u4e3aMLLM\u7684\u9ad8\u6548\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u3002", "relevance": 85.0}}
{"id": "2505.21387", "pdf": "https://arxiv.org/pdf/2505.21387", "abs": "https://arxiv.org/abs/2505.21387", "authors": ["Xihong Yang", "Siwei Wang", "Fangdi Wang", "Jiaqi Jin", "Suyuan Liu", "Yue Liu", "En Zhu", "Xinwang Liu", "Yueming Jin"], "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging the powerful representation learning capabilities, deep multi-view\nclustering methods have demonstrated reliable performance by effectively\nintegrating multi-source information from diverse views in recent years. Most\nexisting methods rely on the assumption of clean views. However, noise is\npervasive in real-world scenarios, leading to a significant degradation in\nperformance. To tackle this problem, we propose a novel multi-view clustering\nframework for the automatic identification and rectification of noisy data,\ntermed AIRMVC. Specifically, we reformulate noisy identification as an anomaly\nidentification problem using GMM. We then design a hybrid rectification\nstrategy to mitigate the adverse effects of noisy data based on the\nidentification results. Furthermore, we introduce a noise-robust contrastive\nmechanism to generate reliable representations. Additionally, we provide a\ntheoretical proof demonstrating that these representations can discard noisy\ninformation, thereby improving the performance of downstream tasks. Extensive\nexperiments on six benchmark datasets demonstrate that AIRMVC outperforms\nstate-of-the-art algorithms in terms of robustness in noisy scenarios. The code\nof AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIRMVC\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u548c\u6821\u6b63\u566a\u58f0\u6570\u636e\uff0c\u901a\u8fc7GMM\u5efa\u6a21\u566a\u58f0\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6df7\u5408\u6821\u6b63\u7b56\u7565\u548c\u566a\u58f0\u9c81\u68d2\u5bf9\u6bd4\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u566a\u58f0\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u89c6\u56fe\u6570\u636e\u5e38\u53d7\u566a\u58f0\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u89c6\u56fe\u5e72\u51c0\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u566a\u58f0\u95ee\u9898\u3002", "method": "1) \u5c06\u566a\u58f0\u8bc6\u522b\u5efa\u6a21\u4e3aGMM\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff1b2) \u8bbe\u8ba1\u6df7\u5408\u6821\u6b63\u7b56\u7565\uff1b3) \u5f15\u5165\u566a\u58f0\u9c81\u68d2\u5bf9\u6bd4\u673a\u5236\u751f\u6210\u53ef\u9760\u8868\u793a\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAIRMVC\u5728\u566a\u58f0\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "AIRMVC\u901a\u8fc7\u566a\u58f0\u8bc6\u522b\u4e0e\u6821\u6b63\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u56fe\u805a\u7c7b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21382", "pdf": "https://arxiv.org/pdf/2505.21382", "abs": "https://arxiv.org/abs/2505.21382", "authors": ["Nastaran Saadati", "Zhanhong Jiang", "Joshua R. Waite", "Shreyan Ganguly", "Aditya Balu", "Chinmay Hegde", "Soumik Sarkar"], "title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u53bb\u4e2d\u5fc3\u5316LoRA\uff08DLoRA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5e73\u6ed1\u548cDeCAF\u7b97\u6cd5\u89e3\u51b3\u5171\u8bc6\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u672c\u5730\u8bad\u7ec3\u548c\u8054\u90a6\u5b66\u4e60\u3002", "motivation": "LoRA\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u7406\u8bba\u57fa\u7840\u4e0a\u7f3a\u4e4f\u5e73\u6ed1\u6027\u4fdd\u8bc1\u548c\u5171\u8bc6\u5e72\u6270\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u5e73\u6ed1\u6539\u8fdbDLoRA\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5f15\u5165DeCAF\u7b97\u6cd5\uff0c\u7ed3\u5408\u622a\u65ad\u5947\u5f02\u503c\u5206\u89e3\uff08TSVD\uff09\u89e3\u51b3\u5171\u8bc6\u5e72\u6270\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793aTSVD\u7684\u8fd1\u4f3c\u8bef\u5dee\u6709\u754c\uff0c\u5171\u8bc6\u5dee\u5f02\u968f\u79e9\u589e\u52a0\u800c\u6d88\u5931\uff0c\u5b9e\u9a8c\u8868\u660e\u7b97\u6cd5\u5728\u89c6\u89c9/\u8bed\u8a00\u4efb\u52a1\u4e2d\u4f18\u4e8e\u672c\u5730\u8bad\u7ec3\u548c\u8054\u90a6\u5b66\u4e60\u3002", "conclusion": "DeCAF\u7b97\u6cd5\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u9ad8\u6548\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u3002", "relevance": 80.0}}
{"id": "2505.21396", "pdf": "https://arxiv.org/pdf/2505.21396", "abs": "https://arxiv.org/abs/2505.21396", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347LLM\u751f\u6210\u7814\u7a76\u60f3\u6cd5\u7684\u8d28\u91cf\uff0c\u5305\u62ec\u5143\u6570\u636e\u5f15\u5bfc\u548c\u81ea\u52a8\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u884c\u6027\u548c\u8d28\u91cf\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u7814\u7a76\u60f3\u6cd5\u65f6\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u7684\u4f5c\u7528\u3002", "method": "1) \u5728\u751f\u6210\u9636\u6bb5\u52a0\u5165\u5143\u6570\u636e\u5f15\u5bfc\uff1b2) \u5728\u7b5b\u9009\u9636\u6bb5\u52a0\u5165\u81ea\u52a8\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u57fa\u4e8e\u6c14\u5019\u8c08\u5224\u4e3b\u9898\u3002", "result": "\u5143\u6570\u636e\u63d0\u5347\u53ef\u884c\u602720%\uff0c\u81ea\u52a8\u9a8c\u8bc1\u63d0\u5347\u8d28\u91cf7%\u3002\u4eba\u7c7b\u7814\u7a76\u8868\u660eLLM\u751f\u6210\u7684\u60f3\u6cd5\u80fd\u6fc0\u53d1\u66f4\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u63d0\u6848\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684LLM\u8f85\u52a9\u7814\u7a76\u60f3\u6cd5\u751f\u6210\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.21420", "pdf": "https://arxiv.org/pdf/2505.21420", "abs": "https://arxiv.org/abs/2505.21420", "authors": ["Jinbao Wang", "Hanzhe Liang", "Can Gao", "Chenxi Hu", "Jie Zhou", "Yunkang Cao", "Linlin Shen", "Weiming Shen"], "title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning", "categories": ["cs.CV", "cs.AI"], "comment": "10 Pages, 6 Figures, 7 Tables", "summary": "Multimodal feature reconstruction is a promising approach for 3D anomaly\ndetection, leveraging the complementary information from dual modalities. We\nfurther advance this paradigm by utilizing multi-modal mentor learning, which\nfuses intermediate features to further distinguish normal from feature\ndifferences. To address these challenges, we propose a novel method called\nMentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared\nfeatures of different modalities, Mentor3AD can extract more effective features\nand guide feature reconstruction, ultimately improving detection performance.\nSpecifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges\nfeatures extracted from RGB and 3D modalities to create a mentor feature.\nAdditionally, we have designed a Mentor of Guidance Module (MGM) to facilitate\ncross-modal reconstruction, supported by the mentor feature. Lastly, we\nintroduce a Voting Module (VM) to more accurately generate the final anomaly\nscore. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies\nhave verified the effectiveness of the proposed method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMentor3AD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bfc\u5e08\u5b66\u4e60\u6539\u8fdb3D\u5f02\u5e38\u68c0\u6d4b\uff0c\u5229\u7528RGB\u548c3D\u6a21\u6001\u7684\u5171\u4eab\u7279\u5f81\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u7279\u5f81\u91cd\u6784\u57283D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u66f4\u6709\u6548\u5730\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38\u7279\u5f81\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faMentor3AD\u65b9\u6cd5\uff0c\u5305\u62ec\u878d\u5408\u6a21\u5757\uff08MFM\uff09\u3001\u6307\u5bfc\u6a21\u5757\uff08MGM\uff09\u548c\u6295\u7968\u6a21\u5757\uff08VM\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bfc\u5e08\u5b66\u4e60\u548c\u7279\u5f81\u91cd\u6784\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728MVTec 3D-AD\u548cEyecandies\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Mentor3AD\u901a\u8fc7\u591a\u6a21\u6001\u5bfc\u5e08\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e863D\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.21391", "pdf": "https://arxiv.org/pdf/2505.21391", "abs": "https://arxiv.org/abs/2505.21391", "authors": ["Zixuan Xie", "Xinyu Liu", "Rohan Chandra", "Shangtong Zhang"], "title": "Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Linear TD($\\lambda$) is one of the most fundamental reinforcement learning\nalgorithms for policy evaluation. Previously, convergence rates are typically\nestablished under the assumption of linearly independent features, which does\nnot hold in many practical scenarios. This paper instead establishes the first\n$L^2$ convergence rates for linear TD($\\lambda$) operating under arbitrary\nfeatures, without making any algorithmic modification or additional\nassumptions. Our results apply to both the discounted and average-reward\nsettings. To address the potential non-uniqueness of solutions resulting from\narbitrary features, we develop a novel stochastic approximation result\nfeaturing convergence rates to the solution set instead of a single point.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u4efb\u610f\u7279\u5f81\u4e0b\u5efa\u7acb\u4e86\u7ebf\u6027TD(\u03bb)\u7684L\u00b2\u6536\u655b\u901f\u7387\uff0c\u65e0\u9700\u7b97\u6cd5\u4fee\u6539\u6216\u989d\u5916\u5047\u8bbe\uff0c\u9002\u7528\u4e8e\u6298\u6263\u548c\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7ebf\u6027TD(\u03bb)\u7b97\u6cd5\u5728\u7279\u5f81\u7ebf\u6027\u65e0\u5173\u5047\u8bbe\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u6269\u5c55\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u65b0\u9896\u7684\u968f\u673a\u903c\u8fd1\u7ed3\u679c\uff0c\u9488\u5bf9\u89e3\u96c6\u7684\u6536\u655b\u901f\u7387\u800c\u975e\u5355\u70b9\u6536\u655b\u3002", "result": "\u8bc1\u660e\u4e86\u7ebf\u6027TD(\u03bb)\u5728\u4efb\u610f\u7279\u5f81\u4e0b\u7684L\u00b2\u6536\u655b\u901f\u7387\uff0c\u9002\u7528\u4e8e\u6298\u6263\u548c\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ebf\u6027TD(\u03bb)\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u7406\u8bba\u652f\u6301\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "relevance": 60.0}}
{"id": "2505.21397", "pdf": "https://arxiv.org/pdf/2505.21397", "abs": "https://arxiv.org/abs/2505.21397", "authors": ["Xiusi Chen", "Shanyong Wang", "Cheng Qian", "Hongru Wang", "Peixuan Han", "Heng Ji"], "title": "DecisionFlow: Advancing Large Language Model as Principled Decision Maker", "categories": ["cs.CL"], "comment": "24 pages, 13 figures", "summary": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DecisionFlow\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u548c\u91d1\u878d\uff09\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u51b3\u7b56\u4e0e\u89e3\u91ca\u8131\u8282\u3002", "method": "DecisionFlow\u901a\u8fc7\u6784\u5efa\u8bed\u4e49\u57fa\u7840\u51b3\u7b56\u7a7a\u95f4\u548c\u63a8\u65ad\u6f5c\u5728\u6548\u7528\u51fd\u6570\uff0c\u5b9e\u73b0\u900f\u660e\u3001\u6548\u7528\u9a71\u52a8\u7684\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u9ad8\u98ce\u9669\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDecisionFlow\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534730%\uff0c\u5e76\u589e\u5f3a\u4e86\u7ed3\u679c\u4e00\u81f4\u6027\u3002", "conclusion": "DecisionFlow\u662f\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0eLLMs\u7684\u5173\u952e\u4e00\u6b65\uff0c\u63d0\u5347\u4e86\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "relevance": 85.0}}
{"id": "2505.21448", "pdf": "https://arxiv.org/pdf/2505.21448", "abs": "https://arxiv.org/abs/2505.21448", "authors": ["Ziqiao Peng", "Jiwen Liu", "Haoxian Zhang", "Xiaoqiang Liu", "Songlin Tang", "Pengfei Wan", "Di Zhang", "Hongyan Liu", "Jun He"], "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers", "categories": ["cs.CV"], "comment": "https://ziqiaopeng.github.io/OmniSync/", "summary": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.", "AI": {"tldr": "OmniSync\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5507\u540c\u6b65\u6846\u67b6\uff0c\u901a\u8fc7Diffusion Transformer\u6a21\u578b\u5b9e\u73b0\u65e0\u63a9\u7801\u8bad\u7ec3\u548c\u76f4\u63a5\u5e27\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u59ff\u6001\u53d8\u5316\u548c\u97f3\u9891\u4fe1\u53f7\u5f31\u6761\u4ef6\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5507\u540c\u6b65\u65b9\u6cd5\u4f9d\u8d56\u53c2\u8003\u5e27\u548c\u63a9\u7801\u4fee\u590d\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u59ff\u6001\u53d8\u5316\u548c\u906e\u6321\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u97f3\u9891\u4fe1\u53f7\u6761\u4ef6\u8f83\u5f31\u3002", "method": "\u91c7\u7528Diffusion Transformer\u6a21\u578b\u8fdb\u884c\u65e0\u63a9\u7801\u8bad\u7ec3\uff0c\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6e10\u8fdb\u566a\u58f0\u521d\u59cb\u5316\u548c\u52a8\u6001\u65f6\u7a7a\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08DS-CFG\uff09\u673a\u5236\u3002", "result": "OmniSync\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5507\u540c\u6b65\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u548cAI\u751f\u6210\u89c6\u9891\u3002", "conclusion": "OmniSync\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u5507\u540c\u6b65\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u591a\u6837\u5316\u89c6\u89c9\u573a\u666f\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.21393", "pdf": "https://arxiv.org/pdf/2505.21393", "abs": "https://arxiv.org/abs/2505.21393", "authors": ["Maoli Liu", "Zhuohua Li", "Xiangxiang Dai", "John C. S. Lui"], "title": "Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining, 2025", "summary": "Conversational recommender systems proactively query users with relevant \"key\nterms\" and leverage the feedback to elicit users' preferences for personalized\nrecommendations. Conversational contextual bandits, a prevalent approach in\nthis domain, aim to optimize preference learning by balancing exploitation and\nexploration. However, several limitations hinder their effectiveness in\nreal-world scenarios. First, existing algorithms employ key term selection\nstrategies with insufficient exploration, often failing to thoroughly probe\nusers' preferences and resulting in suboptimal preference estimation. Second,\ncurrent algorithms typically rely on deterministic rules to initiate\nconversations, causing unnecessary interactions when preferences are\nwell-understood and missed opportunities when preferences are uncertain. To\naddress these limitations, we propose three novel algorithms: CLiSK, CLiME, and\nCLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in\npreference learning, CLiME adaptively initiates conversations based on\npreference uncertainty, and CLiSK-ME integrates both techniques. We\ntheoretically prove that all three algorithms achieve a tighter regret upper\nbound of $O(\\sqrt{dT\\log{T}})$ with respect to the time horizon $T$, improving\nupon existing methods. Additionally, we provide a matching lower bound\n$\\Omega(\\sqrt{dT})$ for conversational bandits, demonstrating that our\nalgorithms are nearly minimax optimal. Extensive evaluations on both synthetic\nand real-world datasets show that our approaches achieve at least a 14.6%\nimprovement in cumulative regret.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7b97\u6cd5\uff08CLiSK\u3001CLiME\u3001CLiSK-ME\uff09\u4ee5\u6539\u8fdb\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u504f\u597d\u5b66\u4e60\uff0c\u901a\u8fc7\u589e\u5f3a\u63a2\u7d22\u548c\u81ea\u9002\u5e94\u5bf9\u8bdd\u542f\u52a8\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7d2f\u79ef\u9057\u61be\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u5728\u63a2\u7d22\u504f\u597d\u548c\u5bf9\u8bdd\u542f\u52a8\u7b56\u7565\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u504f\u597d\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u4ea4\u4e92\u6548\u7387\u4f4e\u4e0b\u3002", "method": "CLiSK\u5f15\u5165\u5e73\u6ed1\u5173\u952e\u672f\u8bed\u4e0a\u4e0b\u6587\u589e\u5f3a\u63a2\u7d22\uff1bCLiME\u57fa\u4e8e\u504f\u597d\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u542f\u52a8\u5bf9\u8bdd\uff1bCLiSK-ME\u7ed3\u5408\u4e24\u8005\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u7b97\u6cd5\u9057\u61be\u4e0a\u754c\u4e3aO(\u221adTlogT)\uff0c\u5b9e\u9a8c\u663e\u793a\u7d2f\u79ef\u9057\u61be\u81f3\u5c11\u964d\u4f4e14.6%\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a5\u8fd1\u6781\u5c0f\u6781\u5927\u6700\u4f18\u3002", "relevance": 40.0}}
{"id": "2505.21399", "pdf": "https://arxiv.org/pdf/2505.21399", "abs": "https://arxiv.org/abs/2505.21399", "authors": ["Hovhannes Tamoyan", "Subhabrata Dutta", "Iryna Gurevych"], "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LLMs\u5728\u751f\u6210\u5185\u5bb9\u65f6\u5b58\u5728\u5185\u90e8\u673a\u5236\uff0c\u80fd\u591f\u901a\u8fc7Transformer\u6b8b\u5dee\u6d41\u4e2d\u7684\u7ebf\u6027\u7279\u5f81\u5224\u65ad\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u8fd9\u79cd\u81ea\u6211\u76d1\u63a7\u80fd\u529b\u5bf9\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6709\u8d21\u732e\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u751f\u6210\u5185\u5bb9\u65f6\u662f\u5426\u5b58\u5728\u5185\u90e8\u673a\u5236\u6765\u5224\u65ad\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790Transformer\u6b8b\u5dee\u6d41\u4e2d\u7684\u7ebf\u6027\u7279\u5f81\uff0c\u7814\u7a76LLMs\u5bf9\u5b9e\u4f53-\u5173\u7cfb-\u5c5e\u6027\u4e09\u5143\u7ec4\u7684\u6b63\u786e\u6027\u5224\u65ad\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e0a\u4e0b\u6587\u6270\u52a8\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5f71\u54cd\u3002", "result": "LLMs\u5728\u751f\u6210\u65f6\u80fd\u591f\u901a\u8fc7\u5185\u90e8\u7ebf\u6027\u7279\u5f81\u5224\u65ad\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u8fd9\u79cd\u80fd\u529b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5feb\u901f\u51fa\u73b0\u5e76\u5728\u4e2d\u95f4\u5c42\u8fbe\u5230\u5cf0\u503c\u3002", "conclusion": "LLMs\u5177\u6709\u5185\u5728\u7684\u81ea\u6211\u76d1\u63a7\u80fd\u529b\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 90.0}}
{"id": "2505.21454", "pdf": "https://arxiv.org/pdf/2505.21454", "abs": "https://arxiv.org/abs/2505.21454", "authors": ["Yue Li Du", "Ben Alexander", "Mikhail Antonenka", "Rohan Mahadev", "Hao-yu Wu", "Dmitry Kislyuk"], "title": "Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations", "categories": ["cs.CV"], "comment": "10 pages, 10 figures", "summary": "Retrieving semantically similar but visually distinct contents has been a\ncritical capability in visual search systems. In this work, we aim to tackle\nthis problem with Visual Product Graph (VPG), leveraging high-performance\ninfrastructure for storage and state-of-the-art computer vision models for\nimage understanding. VPG is built to be an online real-time retrieval system\nthat enables navigation from individual products to composite scenes containing\nthose products, along with complementary recommendations. Our system not only\noffers contextual insights by showcasing how products can be styled in a\ncontext, but also provides recommendations for complementary products drawn\nfrom these inspirations. We discuss the essential components for building the\nVisual Product Graph, along with the core computer vision model improvements\nacross object detection, foundational visual embeddings, and other visual\nsignals. Our system achieves a 78.8% extremely similar@1 in end-to-end human\nrelevance evaluations, and a 6% module engagement rate. The \"Ways to Style It\"\nmodule, powered by the Visual Product Graph technology, is deployed in\nproduction at Pinterest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u4ea7\u54c1\u56fe\uff08VPG\uff09\u7684\u5b9e\u65f6\u68c0\u7d22\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u4f46\u89c6\u89c9\u5dee\u5f02\u7684\u5185\u5bb9\uff0c\u5e76\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\u4e2d\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u4f46\u89c6\u89c9\u5dee\u5f02\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u63a8\u8350\u6548\u679c\u3002", "method": "\u5229\u7528\u9ad8\u6027\u80fd\u57fa\u7840\u8bbe\u65bd\u5b58\u50a8\u548c\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u6784\u5efaVPG\u7cfb\u7edf\uff0c\u5305\u62ec\u5bf9\u8c61\u68c0\u6d4b\u3001\u89c6\u89c9\u5d4c\u5165\u7b49\u6a21\u5757\u3002", "result": "\u7cfb\u7edf\u5728\u7aef\u5230\u7aef\u4eba\u7c7b\u76f8\u5173\u6027\u8bc4\u4f30\u4e2d\u8fbe\u523078.8%\u7684\u6781\u9ad8\u76f8\u4f3c\u5ea6@1\uff0c\u6a21\u5757\u53c2\u4e0e\u7387\u4e3a6%\u3002", "conclusion": "VPG\u6280\u672f\u5df2\u6210\u529f\u90e8\u7f72\u4e8ePinterest\u7684\u201cWays to Style It\u201d\u6a21\u5757\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 30.0}}
{"id": "2505.21395", "pdf": "https://arxiv.org/pdf/2505.21395", "abs": "https://arxiv.org/abs/2505.21395", "authors": ["Xingyu Zhou", "Yulian Wu", "Wenqian Weng", "Francesco Orabona"], "title": "Square$\u03c7$PO: Differentially Private and Robust $\u03c7^2$-Preference Optimization in Offline Direct Alignment", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we theoretically study the offline alignment of language\nmodels with human preference feedback, under both preference label corruption\nand privacy protections. To this end, we propose Square$\\chi$PO, a simple\none-line change to $\\chi$PO where the standard log-loss is replaced by a new\nsquare loss over probability. Thanks to the inherent properties of this new\nloss, we have advanced the state-of-the-art of differentially private and\nrobust offline direct alignment. Specifically, for the local model of label\nprivacy, Square$\\chi$PO is the first algorithm that attains an optimal rate\nbased on single-policy concentrability even with general function\napproximations. It also gives the first result under the central model of\nprivacy protection over both prompts (responses) and labels. On the robustness\nside against Huber label corruption, Square$\\chi$PO is the first alignment\nmethod that has a meaningful theoretical guarantee under general function\napproximations. More importantly, Square$\\chi$PO can address privacy protection\nand corruption simultaneously, where an interesting separation is observed,\nimplying that the order of privacy and corruption matters. Furthermore, we show\nthat Square$\\chi$PO can also be easily extended to handle the scenario of the\ngeneral preference model with state-of-the-art guarantees under corruption and\nprivacy. Last but not least, all of our theoretical guarantees enjoy a unified\nanalysis, building upon a new result on the generalization error bounds of\nleast-square regression under corruption and privacy constraints, which we\nbelieve is of independent interest to the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSquare$\\chi$PO\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u7ebf\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u6362\u6807\u51c6\u5bf9\u6570\u635f\u5931\u4e3a\u5e73\u65b9\u635f\u5931\uff0c\u63d0\u5347\u4e86\u5dee\u5206\u9690\u79c1\u548c\u9c81\u68d2\u6027\u5bf9\u9f50\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u6807\u7b7e\u6c61\u67d3\u548c\u9690\u79c1\u4fdd\u62a4\u4e0b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u7684\u79bb\u7ebf\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faSquare$\\chi$PO\u65b9\u6cd5\uff0c\u4f7f\u7528\u5e73\u65b9\u635f\u5931\u66ff\u4ee3\u6807\u51c6\u5bf9\u6570\u635f\u5931\uff0c\u9002\u7528\u4e8e\u5dee\u5206\u9690\u79c1\u548c\u9c81\u68d2\u6027\u5bf9\u9f50\u3002", "result": "\u5728\u5c40\u90e8\u548c\u4e2d\u5fc3\u9690\u79c1\u6a21\u578b\u4e0b\u8fbe\u5230\u6700\u4f18\u7387\uff0c\u9996\u6b21\u5728Huber\u6807\u7b7e\u6c61\u67d3\u4e0b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u80fd\u540c\u65f6\u5904\u7406\u9690\u79c1\u548c\u6c61\u67d3\u95ee\u9898\u3002", "conclusion": "Square$\\chi$PO\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u9c81\u68d2\u6027\u5bf9\u9f50\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u5206\u6790\u6846\u67b6\u7edf\u4e00\u3002", "relevance": 85.0}}
{"id": "2505.21409", "pdf": "https://arxiv.org/pdf/2505.21409", "abs": "https://arxiv.org/abs/2505.21409", "authors": ["Dario Satriani", "Enzo Veltri", "Donatello Santoro", "Paolo Papotti"], "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RelationalFactQA\uff0c\u4e00\u4e2a\u8bc4\u4f30LLMs\u751f\u6210\u7ed3\u6784\u5316\u8868\u683c\u8f93\u51fa\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u751f\u6210\u7ed3\u6784\u5316\u3001\u591a\u8bb0\u5f55\u8868\u683c\u8f93\u51fa\u65f6\u7684\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faRelationalFactQA\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u548cSQL\u67e5\u8be2\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u68c0\u7d22\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5148\u8fdbLLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0d\u8d85\u8fc725%\uff0c\u4e14\u6027\u80fd\u968f\u8f93\u51fa\u7ef4\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "RelationalFactQA\u4e3a\u672a\u6765LLM\u4e8b\u5b9e\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u5408\u6210\u4e0a\u7684\u5c40\u9650\u6027\u3002", "relevance": 85.0}}
{"id": "2505.21457", "pdf": "https://arxiv.org/pdf/2505.21457", "abs": "https://arxiv.org/abs/2505.21457", "authors": ["Muzhi Zhu", "Hao Zhong", "Canyu Zhao", "Zongze Du", "Zheng Huang", "Mingyu Liu", "Hao Chen", "Cheng Zou", "Jingdong Chen", "Ming Yang", "Chunhua Shen"], "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://aim-uofa.github.io/ACTIVE-o3", "summary": "Active vision, also known as active perception, refers to the process of\nactively selecting where and how to look in order to gather task-relevant\ninformation. It is a critical component of efficient perception and\ndecision-making in humans and advanced embodied agents. Recently, the use of\nMultimodal Large Language Models (MLLMs) as central planning and\ndecision-making modules in robotic systems has gained extensive attention.\nHowever, despite the importance of active perception in embodied intelligence,\nthere is little to no exploration of how MLLMs can be equipped with or learn\nactive perception capabilities. In this paper, we first provide a systematic\ndefinition of MLLM-based active perception tasks. We point out that the\nrecently proposed GPT-o3 model's zoom-in search strategy can be regarded as a\nspecial case of active perception; however, it still suffers from low search\nefficiency and inaccurate region selection. To address these issues, we propose\nACTIVE-O3, a purely reinforcement learning based training framework built on\ntop of GRPO, designed to equip MLLMs with active perception capabilities. We\nfurther establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across\nboth general open-world tasks, such as small-object and dense object grounding,\nand domain-specific scenarios, including small object detection in remote\nsensing and autonomous driving, as well as fine-grained interactive\nsegmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot\nreasoning abilities on the V* Benchmark, without relying on any explicit\nreasoning data. We hope that our work can provide a simple codebase and\nevaluation protocol to facilitate future research on active perception in\nMLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faACTIVE-O3\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3MLLMs\u4ee5\u589e\u5f3a\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5efa\u7acb\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u4e3aMLLMs\u8d4b\u4e88\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u51b3\u7b56\u6548\u7387\u3002", "method": "\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ACTIVE-O3\uff0c\u7528\u4e8e\u8bad\u7ec3MLLMs\u7684\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\u3002", "result": "ACTIVE-O3\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u793a\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ACTIVE-O3\u4e3aMLLMs\u7684\u4e3b\u52a8\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u7b80\u5355\u4ee3\u7801\u5e93\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "relevance": 70.0}}
{"id": "2505.21400", "pdf": "https://arxiv.org/pdf/2505.21400", "abs": "https://arxiv.org/abs/2505.21400", "authors": ["Gen Li", "Changxiao Cai"], "title": "A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective", "categories": ["cs.LG", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u7406\u8bba\u6536\u655b\u6027\uff0c\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u63d0\u4f9b\u4e86\u91c7\u6837\u8bef\u5dee\u7684\u4e0a\u4e0b\u754c\u5206\u6790\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u5206\u6790\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6536\u655b\u6027\uff0c\u5efa\u7acbKL\u6563\u5ea6\u7684\u4e0a\u4e0b\u754c\u3002", "result": "\u91c7\u6837\u8bef\u5dee\u968f\u8fed\u4ee3\u6b21\u6570T\u7684\u5012\u6570\u8870\u51cf\uff0c\u5e76\u4e0e\u76ee\u6807\u5e8f\u5217\u4e2dtoken\u95f4\u7684\u4e92\u4fe1\u606f\u7ebf\u6027\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "relevance": 70.0}}
{"id": "2505.20692", "pdf": "https://arxiv.org/pdf/2505.20692", "abs": "https://arxiv.org/abs/2505.20692", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u9a71\u52a8\u7684\u504f\u89c1\u68c0\u6d4b\u6846\u67b6\u548cSSI\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5e76\u901a\u8fc7LLM\u4f18\u5316\u63d0\u793a\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\u3002\u7528\u6237\u7814\u7a76\u53d1\u73b0\uff0c\u53bb\u504f\u53ef\u80fd\u727a\u7272\u4e0a\u4e0b\u6587\u5bf9\u9f50\u3002", "motivation": "T2I\u6a21\u578b\u5728\u751f\u6210\u5185\u5bb9\u65f6\u53ef\u80fd\u653e\u5927\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u5f15\u53d1\u4f26\u7406\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u51cf\u5c11\u8fd9\u4e9b\u504f\u89c1\u3002", "method": "\u63d0\u51faSSI\u6307\u6807\u548c\u504f\u89c1\u68c0\u6d4b\u6846\u67b6\uff0c\u5ba1\u8ba1\u4e09\u5927T2I\u6a21\u578b\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7LLM\u4f18\u5316\u63d0\u793a\u8fdb\u884c\u53bb\u504f\u3002", "result": "\u63d0\u793a\u4f18\u5316\u663e\u8457\u964d\u4f4eSSI\uff08\u5730\u7406\u6587\u5316\u964d61%\uff0c\u804c\u4e1a\u964d69%\uff0c\u5f62\u5bb9\u8bcd\u964d51%\uff09\uff0c\u4f46\u7528\u6237\u8ba4\u4e3a\u53bb\u504f\u53ef\u80fd\u5f71\u54cd\u4e0a\u4e0b\u6587\u5bf9\u9f50\u3002", "conclusion": "\u9700\u5e73\u8861\u53bb\u504f\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u8bbe\u8ba1\u652f\u6301\u591a\u6837\u6027\u7684T2I\u7cfb\u7edf\u3002", "relevance": 60.0}}
{"id": "2505.21411", "pdf": "https://arxiv.org/pdf/2505.21411", "abs": "https://arxiv.org/abs/2505.21411", "authors": ["Yehui Tang", "Xiaosong Li", "Fangcheng Liu", "Wei Guo", "Hang Zhou", "Yaoyuan Wang", "Kai Han", "Xianzhi Yu", "Jinpeng Li", "Hui Zang", "Fei Mi", "Xiaojun Meng", "Zhicheng Liu", "Hanting Chen", "Binfan Zheng", "Can Chen", "Youliang Yan", "Ruiming Tang", "Peifeng Qin", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity", "categories": ["cs.CL"], "comment": null, "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoGE\u7684\u4e13\u5bb6\u5206\u7ec4\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86MoE\u4e2d\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "MoE\u6a21\u578b\u4e2d\u4e13\u5bb6\u6fc0\u6d3b\u4e0d\u5747\u8861\u5bfc\u81f4\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bbe\u5907\u5e76\u884c\u8fd0\u884c\u65f6\u3002", "method": "\u5f15\u5165MoGE\uff0c\u901a\u8fc7\u4e13\u5bb6\u5206\u7ec4\u548c\u8d1f\u8f7d\u5747\u8861\u4f18\u5316MoE\u67b6\u6784\uff0c\u5e76\u5728Ascend NPUs\u4e0a\u5b9e\u73b0Pangu Pro MoE\u6a21\u578b\u3002", "result": "MoGE\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u8d1f\u8f7d\u5747\u8861\u548c\u6267\u884c\u6548\u7387\uff0cPangu Pro MoE\u5728\u63a8\u7406\u6027\u80fd\u4e0a\u4f18\u4e8e\u540c\u7c7b\u5bc6\u96c6\u6a21\u578b\u3002", "conclusion": "MoGE\u662f\u4e00\u79cd\u9ad8\u6548\u7684MoE\u4f18\u5316\u65b9\u6cd5\uff0cAscend NPUs\u80fd\u591f\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\u8bad\u7ec3\uff0c\u4f7f\u5176\u5728100B\u53c2\u6570\u4ee5\u4e0b\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "relevance": 90.0}}
{"id": "2505.21465", "pdf": "https://arxiv.org/pdf/2505.21465", "abs": "https://arxiv.org/abs/2505.21465", "authors": ["Bozhou Li", "Wentao Zhang"], "title": "ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)\nperformance is to encode both the high-resolution version and the thumbnail of\nan image simultaneously. While effective, this method generates a large number\nof image tokens. When combined with the widely used Rotary Position Embedding\n(RoPE), its long-term decay property hinders the interaction between\nhigh-resolution tokens and thumbnail tokens, as well as between text and image.\nTo address these issues, we propose ID-Align, which alleviates these problems\nby reordering position IDs. In this method, high-resolution tokens inherit IDs\nfrom their corresponding thumbnail token while constraining the overexpansion\nof positional indices. Our experiments conducted within the LLaVA-Next\nframework demonstrate that ID-Align achieves significant improvements,\nincluding a 6.09% enhancement on MMBench's relation reasoning tasks and notable\ngains across multiple benchmarks. Our code is available at the following link:\nhttps://github.com/zooblastlbz/ID-Align.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faID-Align\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u4f4d\u7f6eID\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0e\u7f29\u7565\u56fe\u4ea4\u4e92\u95ee\u9898\uff0c\u63d0\u5347Vision-Language Models\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728Vision-Language Models\u4e2d\u540c\u65f6\u7f16\u7801\u9ad8\u5206\u8fa8\u7387\u548c\u7f29\u7565\u56fe\u65f6\uff0c\u751f\u6210\u5927\u91cf\u56fe\u50cftoken\uff0c\u7ed3\u5408RoPE\u7684\u957f\u7a0b\u8870\u51cf\u7279\u6027\uff0c\u963b\u788d\u4e86\u9ad8\u5206\u8fa8\u7387\u4e0e\u7f29\u7565\u56fetoken\u53ca\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u4ea4\u4e92\u3002", "method": "\u63d0\u51faID-Align\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u4f4d\u7f6eID\uff0c\u4f7f\u9ad8\u5206\u8fa8\u7387token\u7ee7\u627f\u5bf9\u5e94\u7f29\u7565\u56fetoken\u7684ID\uff0c\u9650\u5236\u4f4d\u7f6e\u7d22\u5f15\u7684\u8fc7\u5ea6\u6269\u5c55\u3002", "result": "\u5728LLaVA-Next\u6846\u67b6\u4e0b\u5b9e\u9a8c\uff0cID-Align\u5728MMBench\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u53476.09%\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ID-Align\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u4e0e\u7f29\u7565\u56fetoken\u4ea4\u4e92\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "relevance": 50.0}}
{"id": "2505.21404", "pdf": "https://arxiv.org/pdf/2505.21404", "abs": "https://arxiv.org/abs/2505.21404", "authors": ["Anas Jnini", "Flavio Vella"], "title": "Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Natural-gradient methods markedly accelerate the training of Physics-Informed\nNeural Networks (PINNs), yet their Gauss--Newton update must be solved in the\nparameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is\nthe number of network trainable weights. We show that exactly the same step can\ninstead be formulated in a generally smaller residual space of size $m =\n\\sum_{\\gamma} N_{\\gamma} d_{\\gamma}$, where each residual class $\\gamma$ (e.g.\nPDE interior, boundary, initial data) contributes $N_{\\gamma}$ collocation\npoints of output dimension $d_{\\gamma}$.\n  Building on this insight, we introduce \\textit{Dual Natural Gradient Descent}\n(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it\nwith a geodesic-acceleration correction at negligible extra cost, and provides\nboth a dense direct solver for modest $m$ and a Nystrom-preconditioned\nconjugate-gradient solver for larger $m$.\n  Experimentally, D-NGD scales second-order PINN optimization to networks with\nup to 12.8 million parameters, delivers one- to three-order-of-magnitude lower\nfinal error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton\nmethods, and -- crucially -- enables natural-gradient training of PINNs at this\nscale on a single GPU.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD-NGD\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6b8b\u5dee\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u9ad8\u65af-\u725b\u987f\u6b65\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u8bad\u7ec3\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u5728\u5355GPU\u4e0a\u8bad\u7ec3\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "motivation": "\u4f20\u7edf\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u9ad8\u65af-\u725b\u987f\u6b65\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n^3)\uff0c\u5bf9\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u6765\u8bf4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "D-NGD\u5728\u6b8b\u5dee\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u9ad8\u65af-\u725b\u987f\u6b65\uff0c\u5e76\u5f15\u5165\u6d4b\u5730\u7ebf\u52a0\u901f\u6821\u6b63\uff0c\u63d0\u4f9b\u7a20\u5bc6\u76f4\u63a5\u6c42\u89e3\u5668\u548cNystrom\u9884\u5904\u7406\u5171\u8f6d\u68af\u5ea6\u6c42\u89e3\u5668\u3002", "result": "D-NGD\u80fd\u591f\u6269\u5c55\u52301280\u4e07\u53c2\u6570\u7684PINN\u7f51\u7edc\uff0c\u6bd4\u4e00\u9636\u65b9\u6cd5\uff08Adam\u3001SGD\uff09\u548c\u62df\u725b\u987f\u65b9\u6cd5\u4f4e1-3\u4e2a\u6570\u91cf\u7ea7\u7684\u6700\u7ec8\u8bef\u5dee\u3002", "conclusion": "D-NGD\u663e\u8457\u63d0\u5347\u4e86PINN\u8bad\u7ec3\u7684\u6548\u7387\u548c\u89c4\u6a21\uff0c\u9002\u7528\u4e8e\u5355GPU\u73af\u5883\u3002", "relevance": 30.0}}
{"id": "2505.21413", "pdf": "https://arxiv.org/pdf/2505.21413", "abs": "https://arxiv.org/abs/2505.21413", "authors": ["Xiao Liu", "Da Yin", "Zirui Wu", "Yansong Feng"], "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation", "categories": ["cs.CL", "cs.AI"], "comment": "Code is available at https://github.com/xxxiaol/RefTool", "summary": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.", "AI": {"tldr": "RefTool\u662f\u4e00\u4e2a\u53c2\u8003\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5916\u90e8\u7ed3\u6784\u5316\u6750\u6599\uff08\u5982\u6559\u79d1\u4e66\uff09\u81ea\u52a8\u521b\u5efa\u5de5\u5177\uff0c\u4ee5\u589e\u5f3aLLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u6ca1\u6709\u9884\u5b9a\u4e49\u5de5\u5177\u65f6\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5916\u90e8\u53c2\u8003\u6750\u6599\u63d0\u5347\u5de5\u5177\u521b\u5efa\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "RefTool\u5305\u542b\u5de5\u5177\u521b\u5efa\u548c\u5de5\u5177\u5229\u7528\u4e24\u4e2a\u6a21\u5757\uff1a\u524d\u8005\u4ece\u53c2\u8003\u5185\u5bb9\u751f\u6210\u53ef\u6267\u884c\u5de5\u5177\u5e76\u9a8c\u8bc1\uff0c\u540e\u8005\u901a\u8fc7\u5de5\u5177\u7bb1\u7ed3\u6784\u9009\u62e9\u548c\u5e94\u7528\u5de5\u5177\u3002", "result": "\u5728\u56e0\u679c\u3001\u7269\u7406\u548c\u5316\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRefTool\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534711.3%\uff0c\u4e14\u6210\u672c\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u3002", "conclusion": "RefTool\u901a\u8fc7\u5916\u90e8\u53c2\u8003\u6750\u6599\u514b\u670dLLM\u7684\u77e5\u8bc6\u9650\u5236\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5916\u90e8\u53c2\u8003\u5728\u5de5\u5177\u521b\u5efa\u4e2d\u7684\u4ef7\u503c\u3002", "relevance": 85.0}}
{"id": "2505.21472", "pdf": "https://arxiv.org/pdf/2505.21472", "abs": "https://arxiv.org/abs/2505.21472", "authors": ["Mehrdad Fazli", "Bowen Wei", "Ziwei Zhu"], "title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination.", "AI": {"tldr": "CAAC\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u6807\u8bb0\u6821\u51c6\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u91cd\u7f29\u653e\uff0c\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u5728\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\u6216\u5c5e\u6027\u4e0a\u8fc7\u5ea6\u81ea\u4fe1\u3002\u73b0\u6709\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\u5728\u5f00\u653e\u6027\u548c\u957f\u6587\u672c\u751f\u6210\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u6301\u51c6\u786e\u6027\u3002", "method": "CAAC\u6846\u67b6\u901a\u8fc7\u4e24\u6b65\u89e3\u51b3\u7a7a\u95f4\u611f\u77e5\u504f\u5dee\u548c\u6a21\u6001\u504f\u5dee\uff1a\u89c6\u89c9\u6807\u8bb0\u6821\u51c6\uff08VTC\uff09\u5e73\u8861\u89c6\u89c9\u6807\u8bb0\u6ce8\u610f\u529b\uff0c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u91cd\u7f29\u653e\uff08AAR\uff09\u57fa\u4e8e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5f3a\u5316\u89c6\u89c9\u57fa\u7840\u3002", "result": "\u5728CHAIR\u3001AMBER\u548cPOPE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAAC\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "CAAC\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8c03\u6574\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u4fdd\u6301\u89c6\u89c9\u5bf9\u9f50\uff0c\u9002\u7528\u4e8e\u957f\u6587\u672c\u751f\u6210\u573a\u666f\u3002", "relevance": 75.0}}
{"id": "2505.21414", "pdf": "https://arxiv.org/pdf/2505.21414", "abs": "https://arxiv.org/abs/2505.21414", "authors": ["Brett Bissey", "Kyle Gatesman", "Walker Dimon", "Mohammad Alam", "Luis Robaina", "Joseph Weissman"], "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment", "categories": ["cs.LG", "cs.AI", "cs.GT"], "comment": null, "summary": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790DRL\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u53d1\u73b0\u884c\u4e3a\u6a21\u5f0f\u548c\u6f0f\u6d1e\uff0c\u8bc4\u4f30\u5bf9\u6297\u653b\u51fb\u6548\u679c\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6218\u7565\u6e38\u620f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e3a\u90e8\u7f72\u524d\u7684DRL\u7cfb\u7edf\u63d0\u4f9b\u5b89\u5168\u6027\u5206\u6790\uff0c\u63ed\u793a\u5176\u884c\u4e3a\u6a21\u5f0f\u548c\u6f5c\u5728\u6f0f\u6d1e\uff0c\u4ee5\u652f\u6301\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u5b89\u5168\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u751f\u6210\u9488\u5bf9\u6027\u89c2\u5bdf\u6270\u52a8\uff0c\u8bc4\u4f30\u5bf9\u6297\u653b\u51fb\u6548\u679c\uff0c\u5e76\u5728\u81ea\u5b9a\u4e49\u6218\u7565\u6e38\u620fCyberStrike\u4e2d\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u653b\u51fb\u5bf9\u51b3\u7b56\u7b56\u7565\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5f00\u53d1\u9632\u5fa1\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5bf9\u6297\u9632\u5fa1\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u4e3aDRL\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.21439", "pdf": "https://arxiv.org/pdf/2505.21439", "abs": "https://arxiv.org/abs/2505.21439", "authors": ["Yuchen Zhuang", "Aaron Trinh", "Rushi Qiang", "Haotian Sun", "Chao Zhang", "Hanjun Dai", "Bo Dai"], "title": "Towards Better Instruction Following Retrieval Models", "categories": ["cs.CL", "cs.IR"], "comment": "Retrieval Models, Embedding, Retrieval with Instructions", "summary": "Modern information retrieval (IR) models, trained exclusively on standard\n<query, passage> pairs, struggle to effectively interpret and follow explicit\nuser instructions. We introduce InF-IR, a large-scale, high-quality training\ncorpus tailored for enhancing retrieval models in Instruction-Following IR.\nInF-IR expands traditional training pairs into over 38,000 expressive\n<instruction, query, passage> triplets as positive samples. In particular, for\neach positive triplet, we generate two additional hard negative examples by\npoisoning both instructions and queries, then rigorously validated by an\nadvanced reasoning model (o3-mini) to ensure semantic plausibility while\nmaintaining instructional incorrectness. Unlike existing corpora that primarily\nsupport computationally intensive reranking tasks for decoder-only language\nmodels, the highly contrastive positive-negative triplets in InF-IR further\nenable efficient representation learning for smaller encoder-only models,\nfacilitating direct embedding-based retrieval. Using this corpus, we train\nInF-Embed, an instruction-aware Embedding model optimized through contrastive\nlearning and instruction-query attention mechanisms to align retrieval outcomes\nprecisely with user intents. Extensive experiments across five\ninstruction-based retrieval benchmarks demonstrate that InF-Embed significantly\nsurpasses competitive baselines by 8.1% in p-MRR, measuring the\ninstruction-following capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInF-IR\uff0c\u4e00\u79cd\u4e13\u4e3a\u6307\u4ee4\u8ddf\u968f\u4fe1\u606f\u68c0\u7d22\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u4e86InF-Embed\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u96be\u4ee5\u6709\u6548\u9075\u5faa\u7528\u6237\u6307\u4ee4\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b38,000\u4e2a<\u6307\u4ee4,\u67e5\u8be2,\u6bb5\u843d>\u4e09\u5143\u7ec4\u7684InF-IR\u8bed\u6599\u5e93\uff0c\u751f\u6210\u786c\u8d1f\u6837\u672c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3InF-Embed\u6a21\u578b\u3002", "result": "InF-Embed\u5728\u4e94\u4e2a\u6307\u4ee4\u68c0\u7d22\u57fa\u51c6\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u53478.1%\uff08p-MRR\uff09\u3002", "conclusion": "InF-IR\u548cInF-Embed\u4e3a\u6307\u4ee4\u8ddf\u968f\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.21473", "pdf": "https://arxiv.org/pdf/2505.21473", "abs": "https://arxiv.org/abs/2505.21473", "authors": ["Yiheng Liu", "Liao Qu", "Huichao Zhang", "Xu Wang", "Yi Jiang", "Yiming Gao", "Hu Ye", "Xian Li", "Shuai Wang", "Daniel K. Du", "Shu Cheng", "Zehuan Yuan", "Xinglong Wu"], "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods.", "AI": {"tldr": "DetailFlow\u662f\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u76841D\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u7ec6\u8282\u9884\u6d4b\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u6570\u91cf\u5e76\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u4ee4\u724c\u6570\u91cf\u591a\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u81ea\u7136\u9ad8\u6548\u7684\u751f\u6210\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u5206\u8fa8\u7387\u611f\u77e5\u7684\u4ee4\u724c\u5e8f\u5217\u548c\u6e10\u8fdb\u964d\u7ea7\u56fe\u50cf\u76d1\u7763\uff0c\u7ed3\u5408\u5e76\u884c\u63a8\u7406\u4e0e\u81ea\u6821\u6b63\u673a\u5236\u52a0\u901f\u751f\u6210\u3002", "result": "\u5728ImageNet 256x256\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5128\u4ee4\u724c\u5b9e\u73b02.96 gFID\uff0c\u4f18\u4e8eVAR\u548cFlexVAR\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5feb2\u500d\u3002", "conclusion": "DetailFlow\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.21422", "pdf": "https://arxiv.org/pdf/2505.21422", "abs": "https://arxiv.org/abs/2505.21422", "authors": ["Abbavaram Gowtham Reddy", "Celia Rubio-Madrigal", "Rebekka Burkholz", "Krikamol Muandet"], "title": "When Shift Happens - Confounding Is to Blame", "categories": ["cs.LG"], "comment": null, "summary": "Distribution shifts introduce uncertainty that undermines the robustness and\ngeneralization capabilities of machine learning models. While conventional\nwisdom suggests that learning causal-invariant representations enhances\nrobustness to such shifts, recent empirical studies present a counterintuitive\nfinding: (i) empirical risk minimization (ERM) can rival or even outperform\nstate-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its\nOOD generalization performance improves when all available covariates, not just\ncausal ones, are utilized. Drawing on both empirical and theoretical evidence,\nwe attribute this phenomenon to hidden confounding. Shifts in hidden\nconfounding induce changes in data distributions that violate assumptions\ncommonly made by existing OOD generalization approaches. Under such conditions,\nwe prove that effective generalization requires learning environment-specific\nrelationships, rather than relying solely on invariant ones. Furthermore, we\nshow that models augmented with proxies for hidden confounders can mitigate the\nchallenges posed by hidden confounding shifts. These findings offer new\ntheoretical insights and practical guidance for designing robust OOD\ngeneralization algorithms and principled covariate selection strategies.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u5e03\u504f\u79fb\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u80fd\u4f18\u4e8e\u73b0\u6709OOD\u6cdb\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u5229\u7528\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u4ee3\u7406\u53d8\u91cf\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u5e26\u6765\u7684\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u5bf9\u73b0\u6709OOD\u6cdb\u5316\u65b9\u6cd5\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u8bba\u6587\u6bd4\u8f83\u4e86ERM\u4e0e\u73b0\u6709OOD\u6cdb\u5316\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u4ee3\u7406\u53d8\u91cf\u7684\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cERM\u5728\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u5b58\u5728\u65f6\u53ef\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u901a\u8fc7\u4ee3\u7406\u53d8\u91cf\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8bbe\u8ba1OOD\u6cdb\u5316\u7b97\u6cd5\u65f6\u9700\u8003\u8651\u73af\u5883\u7279\u5b9a\u5173\u7cfb\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u4e0d\u53d8\u6027\u5047\u8bbe\u3002", "relevance": 75.0}}
{"id": "2505.20714", "pdf": "https://arxiv.org/pdf/2505.20714", "abs": "https://arxiv.org/abs/2505.20714", "authors": ["Zechen Li", "Lanqing Yang", "Yiheng Bian", "Hao Pan", "Yongjian Fu", "Yezhou Wang", "Yi-Chao Chen", "Guangtao Xue", "Ju Ren"], "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents an innovative frequency-embedded 3D Gaussian splatting\n(3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling,\noffering an advancement over the existing works limited to single-frequency\nmodeling. Grounded in fundamental physics, we uncover the complex relationship\nbetween EM wave propagation behaviors and RF frequencies. Inspired by this, we\ndesign an EM feature network with attenuation and radiance modules to learn the\ncomplex relationships between RF frequencies and the key properties of each 3D\nGaussian, specifically the attenuation factor and RF signal intensity. By\ntraining the frequency-embedded 3DGS model, we can efficiently reconstruct RF\nradiance fields at arbitrary unknown frequencies within a given 3D environment.\nFinally, we propose a large-scale power angular spectrum (PAS) dataset\ncontaining 50000 samples ranging from 1 to 100 GHz in 6 indoor environments,\nand conduct extensive experiments to verify the effectiveness of our method.\nOur approach achieves an average Structural Similarity Index Measure (SSIM) up\nto 0.72, and a significant improvement up to 17.8% compared to the current\nstate-of-the-art (SOTA) methods trained on individual test frequencies.\nAdditionally, our method achieves an SSIM of 0.70 without prior training on\nthese frequencies, which represents only a 2.8% performance drop compared to\nmodels trained with full PAS data. This demonstrates our model's capability to\nestimate PAS at unknown frequencies. For related code and datasets, please\nrefer to https://github.com/sim-2-real/Wideband3DGS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u9891\u7387\u5d4c\u51653D\u9ad8\u65af\u6e85\u5c04\u7b97\u6cd5\uff0c\u7528\u4e8e\u5bbd\u5e26\u5c04\u9891\u8f90\u5c04\u573a\u5efa\u6a21\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u5355\u9891\u5efa\u6a21\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u8bbe\u8ba1\u7535\u78c1\u7279\u5f81\u7f51\u7edc\uff0c\u5b66\u4e60\u5c04\u9891\u9891\u7387\u4e0e3D\u9ad8\u65af\u5173\u952e\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u5728\u672a\u77e5\u9891\u7387\u4e0b\u9ad8\u6548\u91cd\u5efa\u5c04\u9891\u8f90\u5c04\u573a\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5c04\u9891\u8f90\u5c04\u573a\u5efa\u6a21\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u9891\u5efa\u6a21\uff0c\u65e0\u6cd5\u5904\u7406\u5bbd\u5e26\u9891\u7387\u4e0b\u7684\u590d\u6742\u7535\u78c1\u6ce2\u4f20\u64ad\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9891\u7387\u5d4c\u5165\u76843D\u9ad8\u65af\u6e85\u5c04\u7b97\u6cd5\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u8870\u51cf\u548c\u8f90\u5c04\u6a21\u5757\u7684\u7535\u78c1\u7279\u5f81\u7f51\u7edc\uff0c\u5b66\u4e60\u5c04\u9891\u9891\u7387\u4e0e3D\u9ad8\u65af\u5173\u952e\u5c5e\u6027\uff08\u8870\u51cf\u56e0\u5b50\u548c\u4fe1\u53f7\u5f3a\u5ea6\uff09\u7684\u5173\u7cfb\u3002\u8bad\u7ec3\u9891\u7387\u5d4c\u5165\u76843DGS\u6a21\u578b\uff0c\u5b9e\u73b0\u672a\u77e5\u9891\u7387\u4e0b\u7684\u5c04\u9891\u8f90\u5c04\u573a\u91cd\u5efa\u3002", "result": "\u57286\u4e2a\u5ba4\u5185\u73af\u5883\u4e2d\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u529f\u7387\u89d2\u8c31\u6570\u636e\u96c6\uff0850000\u6837\u672c\uff0c1-100 GHz\uff09\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5e73\u5747SSIM\u8fbe0.72\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534717.8%\u3002\u5373\u4f7f\u672a\u5728\u7279\u5b9a\u9891\u7387\u8bad\u7ec3\uff0cSSIM\u4ecd\u8fbe0.70\uff08\u4ec5\u4e0b\u964d2.8%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u4f30\u8ba1\u672a\u77e5\u9891\u7387\u4e0b\u7684\u529f\u7387\u89d2\u8c31\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "relevance": 20.0}}
{"id": "2505.21451", "pdf": "https://arxiv.org/pdf/2505.21451", "abs": "https://arxiv.org/abs/2505.21451", "authors": ["Jocelyn Shen", "Akhila Yerukola", "Xuhui Zhou", "Cynthia Breazeal", "Maarten Sap", "Hae Won Park"], "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication", "categories": ["cs.CL"], "comment": null, "summary": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u68c0\u6d4b\u5bf9\u8bdd\u51b2\u7a81\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u96be\u4ee5\u5229\u7528\u5173\u7cfb\u80cc\u666f\u4fe1\u606f\uff0c\u4e14\u9ad8\u4f30\u6d88\u606f\u7684\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5173\u7cfb\u52a8\u6001\u5982\u4f55\u5f71\u54cd\u5bf9\u8bdd\u51b2\u7a81\u7684\u611f\u77e5\uff0c\u586b\u8865NLP\u7814\u7a76\u4e2d\u5ffd\u89c6\u4e2a\u6027\u5316\u5173\u7cfb\u7684\u7a7a\u767d\u3002", "method": "\u5229\u7528\u975e\u66b4\u529b\u6c9f\u901a\u7406\u8bba\u6784\u5efaPersonaConflicts\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5173\u7cfb\u80cc\u666f\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u611f\u77e5\uff0c\u4f46\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5229\u7528\uff1b\u6a21\u578b\u9ad8\u4f30\u6d88\u606f\u7684\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "LLMs\u9700\u4e2a\u6027\u5316\u5173\u7cfb\u80cc\u666f\u624d\u80fd\u6210\u4e3a\u6709\u6548\u7684\u4eba\u7c7b\u6c9f\u901a\u8c03\u89e3\u8005\u3002", "relevance": 60.0}}
{"id": "2505.21478", "pdf": "https://arxiv.org/pdf/2505.21478", "abs": "https://arxiv.org/abs/2505.21478", "authors": ["Uri Gadot", "Rinon Gal", "Yftah Ziser", "Gal Chechik", "Shie Mannor"], "title": "Policy Optimized Text-to-Image Pipeline Design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bbe\u8ba1\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u591a\u7ec4\u4ef6\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u591a\u7ec4\u4ef6\u6d41\u7a0b\uff0c\u4f46\u8bbe\u8ba1\u8fd9\u4e9b\u6d41\u7a0b\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u867d\u7136LLMs\u53ef\u4ee5\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1. \u8bad\u7ec3\u4e00\u7ec4\u5956\u52b1\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u63d0\u793a-\u6d41\u7a0b\u7ec4\u5408\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\u5206\u6570\uff0c\u907f\u514d\u8bad\u7ec3\u65f6\u751f\u6210\u56fe\u50cf\u7684\u9ad8\u6210\u672c\u30022. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u521d\u59cb\u6d41\u7a0b\u8bcd\u6c47\u8bad\u7ec3\u548cGRPO\u4f18\u5316\u30023. \u5f15\u5165\u57fa\u4e8e\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u6d41\u7a0b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 60.0}}
{"id": "2505.21423", "pdf": "https://arxiv.org/pdf/2505.21423", "abs": "https://arxiv.org/abs/2505.21423", "authors": ["Vit Fojtik", "Maria Matveev", "Hung-Hsu Chou", "Gitta Kutyniok", "Johannes Maly"], "title": "Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "A widely believed explanation for the remarkable generalization capacities of\noverparameterized neural networks is that the optimization algorithms used for\ntraining induce an implicit bias towards benign solutions. To grasp this\ntheoretically, recent works examine gradient descent and its variants in\nsimplified training settings, often assuming vanishing learning rates. These\nstudies reveal various forms of implicit regularization, such as $\\ell_1$-norm\nminimizing parameters in regression and max-margin solutions in classification.\nConcurrently, empirical findings show that moderate to large learning rates\nexceeding standard stability thresholds lead to faster, albeit oscillatory,\nconvergence in the so-called Edge-of-Stability regime, and induce an implicit\nbias towards minima of low sharpness (norm of training loss Hessian). In this\nwork, we argue that a comprehensive understanding of the generalization\nperformance of gradient descent requires analyzing the interaction between\nthese various forms of implicit regularization. We empirically demonstrate that\nthe learning rate balances between low parameter norm and low sharpness of the\ntrained model. We furthermore prove for diagonal linear networks trained on a\nsimple regression task that neither implicit bias alone minimizes the\ngeneralization error. These findings demonstrate that focusing on a single\nimplicit bias is insufficient to explain good generalization, and they motivate\na broader view of implicit regularization that captures the dynamic trade-off\nbetween norm and sharpness induced by non-negligible learning rates.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u68af\u5ea6\u4e0b\u964d\u4e2d\u5b66\u4e60\u7387\u5982\u4f55\u5e73\u8861\u53c2\u6570\u8303\u6570\u548c\u635f\u5931\u51fd\u6570\u7684\u4f4e\u9510\u5ea6\uff0c\u5e76\u8bc1\u660e\u5355\u4e00\u9690\u5f0f\u504f\u5dee\u4e0d\u8db3\u4ee5\u89e3\u91ca\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u8fc7\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u7684\u6765\u6e90\uff0c\u7279\u522b\u662f\u68af\u5ea6\u4e0b\u964d\u4e2d\u5b66\u4e60\u7387\u5bf9\u9690\u5f0f\u6b63\u5219\u5316\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76\u4e86\u5b66\u4e60\u7387\u5728\u9690\u5f0f\u6b63\u5219\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u4e0a\u7684\u56de\u5f52\u4efb\u52a1\u3002", "result": "\u5b66\u4e60\u7387\u5728\u53c2\u6570\u8303\u6570\u548c\u635f\u5931\u51fd\u6570\u9510\u5ea6\u4e4b\u95f4\u52a8\u6001\u6743\u8861\uff0c\u5355\u4e00\u9690\u5f0f\u504f\u5dee\u65e0\u6cd5\u6700\u5c0f\u5316\u6cdb\u5316\u8bef\u5dee\u3002", "conclusion": "\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u89c6\u89d2\u6765\u7406\u89e3\u9690\u5f0f\u6b63\u5219\u5316\uff0c\u8003\u8651\u5b66\u4e60\u7387\u52a8\u6001\u6743\u8861\u7684\u5f71\u54cd\u3002", "relevance": 70.0}}
{"id": "2505.21458", "pdf": "https://arxiv.org/pdf/2505.21458", "abs": "https://arxiv.org/abs/2505.21458", "authors": ["Shintaro Ozaki", "Tatsuya Hiraoka", "Hiroto Otake", "Hiroki Ouchi", "Masaru Isonuma", "Benjamin Heinzerling", "Kentaro Inui", "Taro Watanabe", "Yusuke Miyao", "Yohei Oseki", "Yu Takagi"], "title": "Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u7684\u6f5c\u5728\u8bed\u8a00\u4e0e\u8f93\u5165\u8f93\u51fa\u8bed\u8a00\u7684\u4e00\u81f4\u6027\u5e76\u4e0d\u603b\u662f\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u56e0\u4e3a\u6a21\u578b\u4f1a\u5728\u6700\u7ec8\u5c42\u8c03\u6574\u5185\u90e8\u8868\u793a\u4ee5\u9002\u5e94\u76ee\u6807\u8bed\u8a00\u3002", "motivation": "\u63a2\u7d22\u6f5c\u5728\u8bed\u8a00\u4e0e\u8f93\u5165\u8f93\u51fa\u8bed\u8a00\u7684\u4e0d\u4e00\u81f4\u6027\u5982\u4f55\u5f71\u54cdLLMs\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d8\u5316\u8f93\u5165\u63d0\u793a\u8bed\u8a00\uff0c\u5206\u6790\u6f5c\u5728\u8bed\u8a00\u4e00\u81f4\u6027\u4e0e\u4efb\u52a1\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6f5c\u5728\u8bed\u8a00\u4e00\u81f4\u6027\u5e76\u975e\u603b\u662f\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u6a21\u578b\u4f1a\u5728\u6700\u7ec8\u5c42\u8c03\u6574\u8868\u793a\u4ee5\u9002\u5e94\u76ee\u6807\u8bed\u8a00\u3002", "conclusion": "\u6f5c\u5728\u8bed\u8a00\u4e00\u81f4\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u6709\u9650\uff0c\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u3002", "relevance": 80.0}}
{"id": "2505.21483", "pdf": "https://arxiv.org/pdf/2505.21483", "abs": "https://arxiv.org/abs/2505.21483", "authors": ["Kerui Ren", "Jiayang Bai", "Linning Xu", "Lihan Jiang", "Jiangmiao Pang", "Mulin Yu", "Bo Dai"], "title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation", "categories": ["cs.CV"], "comment": null, "summary": "Object compositing offers significant promise for augmented reality (AR) and\nembodied intelligence applications. Existing approaches predominantly focus on\nsingle-image scenarios or intrinsic decomposition techniques, facing challenges\nwith multi-view consistency, complex scenes, and diverse lighting conditions.\nRecent inverse rendering advancements, such as 3D Gaussian and diffusion-based\nmethods, have enhanced consistency but are limited by scalability, heavy data\nrequirements, or prolonged reconstruction time per scene. To broaden its\napplicability, we introduce MV-CoLight, a two-stage framework for\nillumination-consistent object compositing in both 2D images and 3D scenes. Our\nnovel feed-forward architecture models lighting and shadows directly, avoiding\nthe iterative biases of diffusion-based methods. We employ a Hilbert\ncurve-based mapping to align 2D image inputs with 3D Gaussian scene\nrepresentations seamlessly. To facilitate training and evaluation, we further\nintroduce a large-scale 3D compositing dataset. Experiments demonstrate\nstate-of-the-art harmonized results across standard benchmarks and our dataset,\nas well as casually captured real-world scenes demonstrate the framework's\nrobustness and wide generalization.", "AI": {"tldr": "MV-CoLight\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u57282D\u56fe\u50cf\u548c3D\u573a\u666f\u4e2d\u5b9e\u73b0\u5149\u7167\u4e00\u81f4\u7684\u5bf9\u8c61\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u590d\u6742\u573a\u666f\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5149\u7167\u4e00\u81f4\u6027\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cMV-CoLight\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u5149\u7167\u548c\u9634\u5f71\u6765\u63d0\u5347\u5408\u6210\u6548\u679c\u3002", "method": "\u91c7\u7528\u524d\u9988\u67b6\u6784\u548cHilbert\u66f2\u7ebf\u6620\u5c04\uff0c\u7ed3\u54083D\u9ad8\u65af\u573a\u666f\u8868\u793a\uff0c\u907f\u514d\u57fa\u4e8e\u6269\u6563\u65b9\u6cd5\u7684\u8fed\u4ee3\u504f\u5dee\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u5408\u6210\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MV-CoLight\u4e3aAR\u548c\u5177\u8eab\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5bf9\u8c61\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.21430", "pdf": "https://arxiv.org/pdf/2505.21430", "abs": "https://arxiv.org/abs/2505.21430", "authors": ["Shiwei Zeng", "Jie Shen"], "title": "Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate", "categories": ["cs.LG"], "comment": null, "summary": "Attribute-efficient learning of sparse halfspaces has been a fundamental\nproblem in machine learning theory. In recent years, machine learning\nalgorithms are faced with prevalent data corruptions or even adversarial\nattacks. It is of central interest to design efficient algorithms that are\nrobust to noise corruptions. In this paper, we consider that there exists a\nconstant amount of malicious noise in the data and the goal is to learn an\nunderlying $s$-sparse halfspace $w^* \\in \\mathbb{R}^d$ with $\\text{poly}(s,\\log\nd)$ samples. Specifically, we follow a recent line of works and assume that the\nunderlying distribution satisfies a certain concentration condition and a\nmargin condition at the same time. Under such conditions, we show that\nattribute-efficiency can be achieved by simple variants to existing hinge loss\nminimization programs. Our key contribution includes: 1) an attribute-efficient\nPAC learning algorithm that works under constant malicious noise rate; 2) a new\ngradient analysis that carefully handles the sparsity constraint in hinge loss\nminimization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u4e2d\u5b58\u5728\u6052\u5b9a\u6076\u610f\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u5b66\u4e60\u7a00\u758f\u534a\u7a7a\u95f4\u3002\u901a\u8fc7\u7ed3\u5408\u5206\u5e03\u6d53\u5ea6\u6761\u4ef6\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u94f0\u94fe\u635f\u5931\u6700\u5c0f\u5316\u7684\u7b80\u5355\u53d8\u4f53\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5c5e\u6027\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9762\u4e34\u6570\u636e\u6c61\u67d3\u548c\u5bf9\u6297\u653b\u51fb\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u5bf9\u566a\u58f0\u9c81\u68d2\u7684\u9ad8\u6548\u7b97\u6cd5\u662f\u6838\u5fc3\u76ee\u6807\u3002\u672c\u6587\u65e8\u5728\u5728\u6052\u5b9a\u6076\u610f\u566a\u58f0\u4e0b\u5b66\u4e60\u7a00\u758f\u534a\u7a7a\u95f4\u3002", "method": "\u5047\u8bbe\u6570\u636e\u5206\u5e03\u6ee1\u8db3\u6d53\u5ea6\u6761\u4ef6\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u73b0\u6709\u94f0\u94fe\u635f\u5931\u6700\u5c0f\u5316\u7a0b\u5e8f\u8fdb\u884c\u7b80\u5355\u53d8\u4f53\uff0c\u5b9e\u73b0\u5c5e\u6027\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6052\u5b9a\u6076\u610f\u566a\u58f0\u7387\u4e0b\u5de5\u4f5c\u7684\u5c5e\u6027\u9ad8\u6548PAC\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u68af\u5ea6\u5206\u6790\u65b9\u6cd5\u5904\u7406\u7a00\u758f\u7ea6\u675f\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u53d8\u4f53\u65b9\u6cd5\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u7a00\u758f\u534a\u7a7a\u95f4\u7684\u9ad8\u6548\u5b66\u4e60\uff0c\u4e3a\u9c81\u68d2\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.20730", "pdf": "https://arxiv.org/pdf/2505.20730", "abs": "https://arxiv.org/abs/2505.20730", "authors": ["Shahrooz Pouryousef"], "title": "What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86LLMs\u548c\u7ecf\u5178\u77e9\u9635\u5206\u89e3\uff08MF\uff09\u6a21\u578b\u5728\u5229\u7528\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u4ee5\u63d0\u5347LLMs\u7684\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u6709\u6548\u5229\u7528\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\u4e2d\u7684\u534f\u4f5c\u4fe1\u53f7\uff0c\u4ee5\u6539\u8fdb\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83LLMs\u548cMF\u6a21\u578b\uff0c\u5e76\u5f15\u5165RAG\u65b9\u6cd5\uff0c\u5c06LLMs\u7684\u9884\u6d4b\u57fa\u4e8e\u7ed3\u6784\u5316\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u5f53\u524dLLMs\u5728\u6355\u6349\u534f\u4f5c\u6a21\u5f0f\u4e0a\u4e0d\u5982MF\u6a21\u578b\uff0c\u4f46RAG\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u3002", "conclusion": "RAG\u65b9\u6cd5\u4e3a\u57fa\u4e8eLLMs\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "relevance": 70.0}}
{"id": "2505.21467", "pdf": "https://arxiv.org/pdf/2505.21467", "abs": "https://arxiv.org/abs/2505.21467", "authors": ["Zhanqiu Hu", "Jian Meng", "Yash Akhauri", "Mohamed S. Abdelfattah", "Jae-sun Seo", "Zhiru Zhang", "Udit Gupta"], "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6280\u672f\uff08FreeCache\u548cGuided Diffusion\uff09\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u4f7f\u5176\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b034\u500d\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6027\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u53ca\u4ee4\u724c\u4e0d\u8fde\u8d2f\u95ee\u9898\uff0c\u9700\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u4ee5\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "1. FreeCache\uff1a\u901a\u8fc7\u8de8\u53bb\u566a\u6b65\u9aa4\u91cd\u7528\u7a33\u5b9a\u7684KV\u6295\u5f71\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b2. Guided Diffusion\uff1a\u5229\u7528\u8f7b\u91cf\u7ea7\u81ea\u56de\u5f52\u6a21\u578b\u76d1\u7763\u4ee4\u724c\u89e3\u63a9\u7801\uff0c\u51cf\u5c11\u53bb\u566a\u8fed\u4ee3\u6b21\u6570\u3002", "result": "\u5728\u5f00\u6e90\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7ec4\u5408\u65b9\u6cd5\u5b9e\u73b034\u500d\u7aef\u5230\u7aef\u52a0\u901f\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\uff0c\u9996\u6b21\u4f7f\u6269\u6563\u6a21\u578b\u8fbe\u5230\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.21488", "pdf": "https://arxiv.org/pdf/2505.21488", "abs": "https://arxiv.org/abs/2505.21488", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "SIGGRAPH 2025. Project page: https://omer11a.github.io/be-decisive/", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u521d\u59cb\u566a\u58f0\u9884\u6d4b\u7a7a\u95f4\u5e03\u5c40\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u591a\u4e3b\u9898\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e3b\u9898\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u591a\u4e3b\u9898\u751f\u6210\u65f6\u5b58\u5728\u4e3b\u9898\u6cc4\u6f0f\u95ee\u9898\uff0c\u5bfc\u81f4\u6570\u91cf\u3001\u5c5e\u6027\u548c\u89c6\u89c9\u7279\u5f81\u4e0d\u51c6\u786e\u3002\u5916\u90e8\u5e03\u5c40\u63a7\u5236\u65b9\u6cd5\u53ef\u80fd\u4e0e\u6a21\u578b\u5148\u9a8c\u51b2\u7a81\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5c0f\u578b\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5e76\u4f18\u5316\u521d\u59cb\u566a\u58f0\u8bf1\u5bfc\u7684\u7a7a\u95f4\u5e03\u5c40\uff0c\u907f\u514d\u4e0e\u5916\u90e8\u5e03\u5c40\u51b2\u7a81\uff0c\u4fdd\u6301\u6a21\u578b\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u591a\u4e3b\u9898\u751f\u6210\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u5e03\u5c40\u5f15\u5bfc\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u566a\u58f0\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e3b\u9898\u751f\u6210\u4e2d\u7684\u5e03\u5c40\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "relevance": 60.0}}
{"id": "2505.21438", "pdf": "https://arxiv.org/pdf/2505.21438", "abs": "https://arxiv.org/abs/2505.21438", "authors": ["Yiwen Tu", "Ziqi Liu", "Jiaqi W. Ma", "Weijing Tang"], "title": "Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution", "categories": ["cs.LG"], "comment": null, "summary": "Measuring task relatedness and mitigating negative transfer remain a critical\nopen challenge in Multitask Learning (MTL). This work extends data attribution\n-- which quantifies the influence of individual training data points on model\npredictions -- to MTL setting for measuring task relatedness. We propose the\nMultiTask Influence Function (MTIF), a method that adapts influence functions\nto MTL models with hard or soft parameter sharing. Compared to conventional\ntask relatedness measurements, MTIF provides a fine-grained, instance-level\nrelatedness measure beyond the entire-task level. This fine-grained relatedness\nmeasure enables a data selection strategy to effectively mitigate negative\ntransfer in MTL. Through extensive experiments, we demonstrate that the\nproposed MTIF efficiently and accurately approximates the performance of models\ntrained on data subsets. Moreover, the data selection strategy enabled by MTIF\nconsistently improves model performance in MTL. Our work establishes a novel\nconnection between data attribution and MTL, offering an efficient and\nfine-grained solution for measuring task relatedness and enhancing MTL models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u4e2d\u7684\u4efb\u52a1\u76f8\u5173\u6027\u6d4b\u91cf\u65b9\u6cd5MTIF\uff0c\u901a\u8fc7\u6570\u636e\u5f52\u56e0\u91cf\u5316\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u5e76\u5229\u7528\u5176\u8fdb\u884c\u6570\u636e\u9009\u62e9\u4ee5\u51cf\u8f7b\u8d1f\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u4efb\u52a1\u76f8\u5173\u6027\u6d4b\u91cf\u548c\u8d1f\u8fc1\u79fb\u7f13\u89e3\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faMultiTask Influence Function\uff08MTIF\uff09\uff0c\u5c06\u5f71\u54cd\u51fd\u6570\u6269\u5c55\u5230MTL\u6a21\u578b\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u76f8\u5173\u6027\u6d4b\u91cf\u3002", "result": "MTIF\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u8fd1\u4f3c\u6a21\u578b\u5728\u6570\u636e\u5b50\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5176\u6570\u636e\u9009\u62e9\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86MTL\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "MTIF\u5efa\u7acb\u4e86\u6570\u636e\u5f52\u56e0\u4e0eMTL\u7684\u65b0\u8054\u7cfb\uff0c\u4e3a\u4efb\u52a1\u76f8\u5173\u6027\u6d4b\u91cf\u548cMTL\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7ec6\u7c92\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.21471", "pdf": "https://arxiv.org/pdf/2505.21471", "abs": "https://arxiv.org/abs/2505.21471", "authors": ["Zijun Liu", "Zhennan Wan", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration", "categories": ["cs.CL"], "comment": "30 pages, 9 figures. Code and data are available at\n  https://github.com/THUNLP-MT/ExtAgents", "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6ExtAgents\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u5728\u63a8\u7406\u65f6\u6574\u5408\u5927\u91cf\u5916\u90e8\u77e5\u8bc6\u65f6\u7684\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u5e76\u884c\u6027\u3002", "motivation": "\u73b0\u6709LLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e86\u5916\u90e8\u77e5\u8bc6\u8f93\u5165\u7684\u89c4\u6a21\uff0c\u963b\u788d\u4e86\u590d\u6742\u4efb\u52a1\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u800c\u73b0\u6709\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u667a\u80fd\u4f53\u6846\u67b6ExtAgents\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u65b9\u5f0f\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u77e5\u8bc6\u540c\u6b65\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u74f6\u9888\u3002", "result": "\u5728\u589e\u5f3a\u7684\u591a\u8df3\u95ee\u7b54\u6d4b\u8bd5\u221eBench+\u548c\u5176\u4ed6\u516c\u5f00\u6d4b\u8bd5\u96c6\u4e0a\uff0cExtAgents\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u975e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e14\u9ad8\u6548\u5e76\u884c\u3002", "conclusion": "ExtAgents\u4e3aLLM\u5728\u63a8\u7406\u65f6\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002", "relevance": 85.0}}
{"id": "2505.21491", "pdf": "https://arxiv.org/pdf/2505.21491", "abs": "https://arxiv.org/abs/2505.21491", "authors": ["Boyang Wang", "Xuweiyi Chen", "Matheus Gadelha", "Zezhou Cheng"], "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Controllability, temporal coherence, and detail synthesis remain the most\ncritical challenges in video generation. In this paper, we focus on a commonly\nused yet underexplored cinematic technique known as Frame In and Frame Out.\nSpecifically, starting from image-to-video generation, users can control the\nobjects in the image to naturally leave the scene or provide breaking new\nidentity references to enter the scene, guided by user-specified motion\ntrajectory. To support this task, we introduce a new dataset curated\nsemi-automatically, a comprehensive evaluation protocol targeting this setting,\nand an efficient identity-preserving motion-controllable video Diffusion\nTransformer architecture. Our evaluation shows that our proposed approach\nsignificantly outperforms existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFrame In\u548cFrame Out\u6280\u672f\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u6307\u5b9a\u7684\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u5bf9\u8c61\u8fdb\u51fa\u573a\u666f\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4e2d\u7684\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u5408\u6210\u95ee\u9898\uff0c\u7279\u522b\u662f\u5229\u7528Frame In\u548cFrame Out\u6280\u672f\u5b9e\u73b0\u5bf9\u8c61\u8fdb\u51fa\u573a\u666f\u7684\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u3001\u57fa\u4e8eDiffusion Transformer\u7684\u8eab\u4efd\u4fdd\u6301\u8fd0\u52a8\u53ef\u63a7\u89c6\u9891\u751f\u6210\u67b6\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u534a\u81ea\u52a8\u6807\u6ce8\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5bf9\u8c61\u8fdb\u51fa\u573a\u666f\u7684\u63a7\u5236\u65b9\u9762\u3002", "relevance": 30.0}}
{"id": "2505.21444", "pdf": "https://arxiv.org/pdf/2505.21444", "abs": "https://arxiv.org/abs/2505.21444", "authors": ["Sheikh Shafayat", "Fahim Tajwar", "Ruslan Salakhutdinov", "Jeff Schneider", "Andrea Zanette"], "title": "Can Large Reasoning Models Self-Train?", "categories": ["cs.LG"], "comment": "Project website: https://self-rewarding-llm-training.github.io/", "summary": "Scaling the performance of large language models (LLMs) increasingly depends\non methods that reduce reliance on human supervision. Reinforcement learning\nfrom automated verification offers an alternative, but it incurs scalability\nlimitations due to dependency upon human-designed verifiers. Self-training,\nwhere the model's own judgment provides the supervisory signal, presents a\ncompelling direction. We propose an online self-training reinforcement learning\nalgorithm that leverages the model's self-consistency to infer correctness\nsignals and train without any ground-truth supervision. We apply the algorithm\nto challenging mathematical reasoning tasks and show that it quickly reaches\nperformance levels rivaling reinforcement-learning methods trained explicitly\non gold-standard answers. Additionally, we analyze inherent limitations of the\nalgorithm, highlighting how the self-generated proxy reward initially\ncorrelated with correctness can incentivize reward hacking, where confidently\nincorrect outputs are favored. Our results illustrate how self-supervised\nimprovement can achieve significant performance gains without external labels,\nwhile also revealing its fundamental challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u6211\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u7684\u81ea\u4e00\u81f4\u6027\u751f\u6210\u76d1\u7763\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5956\u52b1\u9ed1\u5ba2\u5316\u7684\u6311\u6218\u3002", "motivation": "\u51cf\u5c11\u5bf9\u4eba\u5de5\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u63a2\u7d22\u81ea\u6211\u8bad\u7ec3\u5728LLM\u6027\u80fd\u6269\u5c55\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u81ea\u6211\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u81ea\u4e00\u81f4\u6027\u63a8\u65ad\u6b63\u786e\u6027\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u57fa\u4e8e\u9ec4\u91d1\u6807\u51c6\u7b54\u6848\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u5316\u95ee\u9898\u3002", "conclusion": "\u81ea\u6211\u76d1\u7763\u6539\u8fdb\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e5f\u9762\u4e34\u5956\u52b1\u4fe1\u53f7\u504f\u5dee\u7684\u6311\u6218\u3002", "relevance": 85.0}}
{"id": "2505.20751", "pdf": "https://arxiv.org/pdf/2505.20751", "abs": "https://arxiv.org/abs/2505.20751", "authors": ["Zongcai Tan amd Dandan Zhang"], "title": "Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform", "categories": ["cs.RO", "cs.AI"], "comment": "ICRA 2025", "summary": "Optical tweezers (OT) offer unparalleled capabilities for micromanipulation\nwith submicron precision in biomedical applications. However, controlling\nconventional multi-trap OT to achieve cooperative manipulation of multiple\ncomplex-shaped microrobots in dynamic environments poses a significant\nchallenge. To address this, we introduce Interactive OT Gym, a reinforcement\nlearning (RL)-based simulation platform designed for OT-driven microrobotics.\nOur platform supports complex physical field simulations and integrates haptic\nfeedback interfaces, RL modules, and context-aware shared control strategies\ntailored for OT-driven microrobot in cooperative biological object manipulation\ntasks. This integration allows for an adaptive blend of manual and autonomous\ncontrol, enabling seamless transitions between human input and autonomous\noperation. We evaluated the effectiveness of our platform using a cell\nmanipulation task. Experimental results show that our shared control system\nsignificantly improves micromanipulation performance, reducing task completion\ntime by approximately 67% compared to using pure human or RL control alone and\nachieving a 100% success rate. With its high fidelity, interactivity, low cost,\nand high-speed simulation capabilities, Interactive OT Gym serves as a\nuser-friendly training and testing environment for the development of advanced\ninteractive OT-driven micromanipulation systems and control algorithms. For\nmore details on the project, please see our website\nhttps://sites.google.com/view/otgym", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Interactive OT Gym\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u7528\u4e8e\u5149\u5b66\u954a\u5b50\u9a71\u52a8\u7684\u5fae\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u7ed3\u5408\u4e86\u89e6\u89c9\u53cd\u9988\u548c\u5171\u4eab\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u9677\u9631\u5149\u5b66\u954a\u5b50\u5728\u52a8\u6001\u73af\u5883\u4e2d\u64cd\u63a7\u590d\u6742\u5f62\u72b6\u5fae\u673a\u5668\u4eba\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301\u590d\u6742\u7269\u7406\u573a\u6a21\u62df\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u89e6\u89c9\u53cd\u9988\u3001\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5171\u4eab\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5171\u4eab\u63a7\u5236\u7cfb\u7edf\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u7ea667%\uff0c\u5e76\u5b9e\u73b0100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "Interactive OT Gym\u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f\u3001\u4ea4\u4e92\u6027\u5f3a\u4e14\u4f4e\u6210\u672c\u7684\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u5f00\u53d1\u5148\u8fdb\u7684\u5fae\u64cd\u4f5c\u7cfb\u7edf\u548c\u63a7\u5236\u7b97\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.21479", "pdf": "https://arxiv.org/pdf/2505.21479", "abs": "https://arxiv.org/abs/2505.21479", "authors": ["Keenan Samway", "Max Kleiman-Weiner", "David Guzman Piedrahita", "Rada Mihalcea", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "categories": ["cs.CL"], "comment": null, "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790600\u591a\u4e2a\u7535\u8f66\u95ee\u9898\uff0c\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\uff0c\u53d1\u73b0\u5176\u601d\u7ef4\u94fe\u503e\u5411\u4e8e\u4e49\u52a1\u8bba\uff0c\u800c\u540e\u89e3\u91ca\u5219\u504f\u5411\u529f\u5229\u4e3b\u4e49\u3002", "motivation": "\u7406\u89e3LLMs\u5728\u4f26\u7406\u590d\u6742\u573a\u666f\u4e2d\u7684\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u4fc3\u8fdb\u5176\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u6027\u5e94\u7528\u3002", "method": "\u5229\u7528600\u591a\u4e2a\u7535\u8f66\u95ee\u9898\u4f5c\u4e3a\u63a2\u9488\uff0c\u5f15\u5165\u9053\u5fb7\u7406\u6027\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790LLMs\u7684\u63a8\u7406\u75d5\u8ff9\u3002", "result": "LLMs\u7684\u601d\u7ef4\u94fe\u503e\u5411\u4e8e\u4e49\u52a1\u8bba\uff0c\u800c\u540e\u89e3\u91ca\u5219\u8f6c\u5411\u529f\u5229\u4e3b\u4e49\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3LLMs\u5904\u7406\u4f26\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5176\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "relevance": 85.0}}
{"id": "2505.21494", "pdf": "https://arxiv.org/pdf/2505.21494", "abs": "https://arxiv.org/abs/2505.21494", "authors": ["Xiaojun Jia", "Sensen Gao", "Simeng Qin", "Tianyu Pang", "Chao Du", "Yihao Huang", "Xinfeng Li", "Yiming Li", "Bo Li", "Yang Liu"], "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u6700\u4f18\u5bf9\u9f50\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5FOA-Attack\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u901a\u5e38\u4ec5\u5173\u6ce8\u5168\u5c40\u7279\u5f81\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u4fe1\u606f\uff0c\u5bfc\u81f4\u8fc1\u79fb\u80fd\u529b\u53d7\u9650\uff0c\u7279\u522b\u662f\u5bf9\u95ed\u6e90\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u5168\u5c40\u4f59\u5f26\u76f8\u4f3c\u5ea6\u635f\u5931\u548c\u5c40\u90e8\u805a\u7c7b\u6700\u4f18\u4f20\u8f93\u635f\u5931\uff0c\u52a8\u6001\u96c6\u6210\u6a21\u578b\u6743\u91cd\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u95ed\u6e90MLLMs\u4e0a\u7684\u8fc1\u79fb\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FOA-Attack\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "relevance": 75.0}}
{"id": "2505.21452", "pdf": "https://arxiv.org/pdf/2505.21452", "abs": "https://arxiv.org/abs/2505.21452", "authors": ["Xiangxin Zhou", "Mingyu Li", "Yi Xiao", "Jiahan Li", "Dongyu Xue", "Zaixiang Zheng", "Jianzhu Ma", "Quanquan Gu"], "title": "Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling", "categories": ["cs.LG", "q-bio.BM"], "comment": "Accepted to ICML 2025", "summary": "Cyclic peptides offer inherent advantages in pharmaceuticals. For example,\ncyclic peptides are more resistant to enzymatic hydrolysis compared to linear\npeptides and usually exhibit excellent stability and affinity. Although deep\ngenerative models have achieved great success in linear peptide design, several\nchallenges prevent the development of computational methods for designing\ndiverse types of cyclic peptides. These challenges include the scarcity of 3D\nstructural data on target proteins and associated cyclic peptide ligands, the\ngeometric constraints that cyclization imposes, and the involvement of\nnon-canonical amino acids in cyclization. To address the above challenges, we\nintroduce CpSDE, which consists of two key components: AtomSDE, a generative\nstructure prediction model based on harmonic SDE, and ResRouter, a residue type\npredictor. Utilizing a routed sampling algorithm that alternates between these\ntwo models to iteratively update sequences and structures, CpSDE facilitates\nthe generation of cyclic peptides. By employing explicit all-atom and bond\nmodeling, CpSDE overcomes existing data limitations and is proficient in\ndesigning a wide variety of cyclic peptides. Our experimental results\ndemonstrate that the cyclic peptides designed by our method exhibit reliable\nstability and affinity.", "AI": {"tldr": "CpSDE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bbe\u8ba1\u591a\u6837\u73af\u80bd\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u751f\u6210\u7ed3\u6784\u9884\u6d4b\u6a21\u578bAtomSDE\u548c\u6b8b\u57fa\u7c7b\u578b\u9884\u6d4b\u5668ResRouter\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u5e8f\u5217\u548c\u7ed3\u6784\u751f\u6210\u73af\u80bd\u3002", "motivation": "\u73af\u80bd\u5728\u836f\u7269\u9886\u57df\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u51e0\u4f55\u7ea6\u675f\u7b49\u6311\u6218\u3002", "method": "CpSDE\u7531AtomSDE\u548cResRouter\u7ec4\u6210\uff0c\u901a\u8fc7\u4ea4\u66ff\u91c7\u6837\u7b97\u6cd5\u8fed\u4ee3\u66f4\u65b0\u5e8f\u5217\u548c\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCpSDE\u8bbe\u8ba1\u7684\u73af\u80bd\u5177\u6709\u53ef\u9760\u7684\u7a33\u5b9a\u6027\u548c\u4eb2\u548c\u529b\u3002", "conclusion": "CpSDE\u514b\u670d\u4e86\u6570\u636e\u9650\u5236\uff0c\u80fd\u591f\u8bbe\u8ba1\u591a\u79cd\u73af\u80bd\u3002", "relevance": 10.0}}
{"id": "2505.21496", "pdf": "https://arxiv.org/pdf/2505.21496", "abs": "https://arxiv.org/abs/2505.21496", "authors": ["Han Xiao", "Guozhi Wang", "Yuxiang Chai", "Zimu Lu", "Weifeng Lin", "Hao He", "Lue Fan", "Liuyang Bian", "Rui Hu", "Liang Liu", "Shuai Ren", "Yafei Wen", "Xiaoxin Chen", "Aojun Zhou", "Hongsheng Li"], "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "https://github.com/Euphoria16/UI-Genie", "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.", "AI": {"tldr": "UI-Genie\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u548c\u6570\u636e\u751f\u6210\u7b56\u7565\u89e3\u51b3GUI\u4ee3\u7406\u4e2d\u7684\u8f68\u8ff9\u9a8c\u8bc1\u548c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u6269\u5c55\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3GUI\u4ee3\u7406\u4e2d\u8f68\u8ff9\u9a8c\u8bc1\u56f0\u96be\u548c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u67b6\u6784\u7684\u5956\u52b1\u6a21\u578b\uff08UI-Genie-RM\uff09\u548c\u81ea\u6539\u8fdb\u7ba1\u9053\uff0c\u7ed3\u5408\u89c4\u5219\u9a8c\u8bc1\u3001\u8f68\u8ff9\u7834\u574f\u548c\u786c\u8d1f\u6837\u672c\u6316\u6398\u751f\u6210\u6570\u636e\u3002", "result": "\u5728\u591a\u4e2aGUI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u548c\u6570\u636e\u96c6\u4fc3\u8fdb\u7814\u7a76\u3002", "conclusion": "UI-Genie\u901a\u8fc7\u81ea\u6539\u8fdb\u548c\u6570\u636e\u751f\u6210\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u7684\u6311\u6218\u3002", "relevance": 60.0}}
{"id": "2505.21497", "pdf": "https://arxiv.org/pdf/2505.21497", "abs": "https://arxiv.org/abs/2505.21497", "authors": ["Wei Pang", "Kevin Qinghong Lin", "Xiangru Jian", "Xi He", "Philip Torr"], "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "comment": "Project Page: https://github.com/Paper2Poster/Paper2Poster", "summary": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5b66\u672f\u6d77\u62a5\u751f\u6210\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u4e86PosterAgent\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u6d77\u62a5\u751f\u6210\u4e2d\u957f\u6587\u672c\u538b\u7f29\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPosterAgent\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff0c\u5305\u62ecParser\u3001Planner\u548cPainter-Commenter\u5faa\u73af\u3002", "result": "\u5f00\u6e90\u7248\u672c\uff08\u57fa\u4e8eQwen-2.5\u7cfb\u5217\uff09\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8eGPT-4o\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u4e3a\u5168\u81ea\u52a8\u6d77\u62a5\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002", "relevance": 30.0}}
{"id": "2505.21460", "pdf": "https://arxiv.org/pdf/2505.21460", "abs": "https://arxiv.org/abs/2505.21460", "authors": ["Maxwell Fishelson", "Noah Golowich", "Mehryar Mohri", "Jon Schneider"], "title": "High-Dimensional Calibration from Swap Regret", "categories": ["cs.LG", "cs.DS", "cs.GT", "stat.ML"], "comment": null, "summary": "We study the online calibration of multi-dimensional forecasts over an\narbitrary convex set $\\mathcal{P} \\subset \\mathbb{R}^d$ relative to an\narbitrary norm $\\Vert\\cdot\\Vert$. We connect this with the problem of external\nregret minimization for online linear optimization, showing that if it is\npossible to guarantee $O(\\sqrt{\\rho T})$ worst-case regret after $T$ rounds\nwhen actions are drawn from $\\mathcal{P}$ and losses are drawn from the dual\n$\\Vert \\cdot \\Vert_*$ unit norm ball, then it is also possible to obtain\n$\\epsilon$-calibrated forecasts after $T = \\exp(O(\\rho /\\epsilon^2))$ rounds.\nWhen $\\mathcal{P}$ is the $d$-dimensional simplex and $\\Vert \\cdot \\Vert$ is\nthe $\\ell_1$-norm, the existence of $O(\\sqrt{T\\log d})$-regret algorithms for\nlearning with experts implies that it is possible to obtain\n$\\epsilon$-calibrated forecasts after $T = \\exp(O(\\log{d}/\\epsilon^2)) =\nd^{O(1/\\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).\n  Interestingly, our algorithm obtains this guarantee without requiring access\nto any online linear optimization subroutine or knowledge of the optimal rate\n$\\rho$ -- in fact, our algorithm is identical for every setting of\n$\\mathcal{P}$ and $\\Vert \\cdot \\Vert$. Instead, we show that the optimal\nregularizer for the above OLO problem can be used to upper bound the above\ncalibration error by a swap regret, which we then minimize by running the\nrecent TreeSwap algorithm with Follow-The-Leader as a subroutine.\n  Finally, we prove that any online calibration algorithm that guarantees\n$\\epsilon T$ $\\ell_1$-calibration error over the $d$-dimensional simplex\nrequires $T \\geq \\exp(\\mathrm{poly}(1/\\epsilon))$ (assuming $d \\geq\n\\mathrm{poly}(1/\\epsilon)$). This strengthens the corresponding\n$d^{\\Omega(\\log{1/\\epsilon})}$ lower bound of Peng, and shows that an\nexponential dependence on $1/\\epsilon$ is necessary.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u7ef4\u9884\u6d4b\u5728\u7ebf\u6821\u51c6\u95ee\u9898\uff0c\u5c06\u5176\u4e0e\u5728\u7ebf\u7ebf\u6027\u4f18\u5316\u7684\u5916\u90e8\u9057\u61be\u6700\u5c0f\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f18\u5316\u5b50\u7a0b\u5e8f\u7684\u901a\u7528\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u6821\u51c6\u8bef\u5dee\u7684\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u591a\u7ef4\u9884\u6d4b\u5728\u7ebf\u6821\u51c6\u95ee\u9898\uff0c\u63a2\u7d22\u5176\u4e0e\u5728\u7ebf\u7ebf\u6027\u4f18\u5316\u7684\u5173\u7cfb\uff0c\u65e8\u5728\u8bbe\u8ba1\u9ad8\u6548\u7684\u6821\u51c6\u7b97\u6cd5\u5e76\u7406\u89e3\u5176\u7406\u8bba\u6781\u9650\u3002", "method": "\u901a\u8fc7\u5c06\u6821\u51c6\u95ee\u9898\u4e0e\u5728\u7ebf\u7ebf\u6027\u4f18\u5316\u7684\u5916\u90e8\u9057\u61be\u6700\u5c0f\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7b97\u6cd5\uff0c\u65e0\u9700\u4f18\u5316\u5b50\u7a0b\u5e8f\u6216\u6700\u4f18\u901f\u7387\u77e5\u8bc6\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u591a\u7ef4\u5355\u7eaf\u5f62\u548c\u21131\u8303\u6570\u4e0b\uff0c\u6821\u51c6\u8bef\u5dee\u7684\u4e0a\u754c\u548c\u4e0b\u754c\uff0c\u5c55\u793a\u4e86\u6307\u6570\u4f9d\u8d56\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5728\u7ebf\u6821\u51c6\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6821\u51c6\u8bef\u5dee\u7684\u7406\u8bba\u6781\u9650\uff0c\u5bf9\u5728\u7ebf\u5b66\u4e60\u548c\u9884\u6d4b\u6821\u51c6\u6709\u91cd\u8981\u610f\u4e49\u3002", "relevance": 40.0}}
{"id": "2505.21503", "pdf": "https://arxiv.org/pdf/2505.21503", "abs": "https://arxiv.org/abs/2505.21503", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cCatfish Agent\u201d\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e34\u5e8a\u95ee\u7b54\u4e2dLLM\u7684\u201c\u6c89\u9ed8\u540c\u610f\u201d\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u5f02\u8bae\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u95ee\u7b54\u4e2dLLM\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b58\u5728\u201c\u6c89\u9ed8\u540c\u610f\u201d\u95ee\u9898\uff0c\u5bfc\u81f4\u8fc7\u65e9\u8fbe\u6210\u5171\u8bc6\u800c\u7f3a\u4e4f\u6279\u5224\u6027\u5206\u6790\u3002", "method": "\u8bbe\u8ba1\u4e86Catfish Agent\uff0c\u901a\u8fc7\u590d\u6742\u6027\u611f\u77e5\u5e72\u9884\u548c\u8bed\u6c14\u6821\u51c6\u5e72\u9884\u6765\u6fc0\u53d1\u6df1\u5ea6\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u5305\u62ecGPT-4o\u548cDeepSeek-R1\u3002", "conclusion": "Catfish Agent\u80fd\u6709\u6548\u89e3\u51b3\u6c89\u9ed8\u540c\u610f\u95ee\u9898\uff0c\u63d0\u5347\u4e34\u5e8a\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "relevance": 85.0}}
{"id": "2505.21500", "pdf": "https://arxiv.org/pdf/2505.21500", "abs": "https://arxiv.org/abs/2505.21500", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project: https://zju-real.github.io/ViewSpatial-Page/", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ViewSpatial-Bench\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u591a\u89c6\u89d2\u7a7a\u95f4\u5b9a\u4f4d\u8bc6\u522b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8de8\u89c6\u89d2\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLMs\u5728\u8de8\u89c6\u89d2\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u4ece\u975e\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff08\u5982\u4eba\u7c7b\u89c6\u89d2\uff09\u8fdb\u884c\u63a8\u7406\u65f6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86ViewSpatial-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e94\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u5e76\u5229\u7528\u81ea\u52a8\u53163D\u6807\u6ce8\u7ba1\u9053\u751f\u6210\u7cbe\u786e\u65b9\u5411\u6807\u7b7e\u3002\u901a\u8fc7\u5fae\u8c03VLMs\u5728\u591a\u89c6\u89d2\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLMs\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u975e\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4e0b\u964d\u3002\u901a\u8fc7\u5fae\u8c03\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347\u4e8646.24%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u8eabAI\u7cfb\u7edf\u7684\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u5e76\u8bc1\u660e\u5efa\u6a213D\u7a7a\u95f4\u5173\u7cfb\u80fd\u663e\u8457\u63d0\u5347VLMs\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "relevance": 60.0}}
{"id": "2505.21468", "pdf": "https://arxiv.org/pdf/2505.21468", "abs": "https://arxiv.org/abs/2505.21468", "authors": ["Simon Dirmeier", "Antonietta Mira"], "title": "Causal Posterior Estimation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field.", "AI": {"tldr": "CPE\u662f\u4e00\u79cd\u7528\u4e8e\u6a21\u62df\u5668\u6a21\u578b\u8d1d\u53f6\u65af\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u6d41\uff08NF\uff09\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03\uff0c\u7ed3\u5408\u6a21\u578b\u7684\u6761\u4ef6\u4f9d\u8d56\u7ed3\u6784\uff0c\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u9488\u5bf9\u6a21\u62df\u5668\u6a21\u578b\u4e2d\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u8ba1\u7b97\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u9a8c\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5f52\u4e00\u5316\u6d41\uff08NF\uff09\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03\uff0c\u7ed3\u5408\u6a21\u578b\u7684\u56fe\u8868\u793a\u6761\u4ef6\u4f9d\u8d56\u7ed3\u6784\uff0c\u63d0\u51fa\u79bb\u6563\u548c\u8fde\u7eedNF\u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1O(1)\u91c7\u6837\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCPE\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "CPE\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6761\u4ef6\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u9a8c\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.21505", "pdf": "https://arxiv.org/pdf/2505.21505", "abs": "https://arxiv.org/abs/2505.21505", "authors": ["Shimao Zhang", "Zhejian Lai", "Xiang Liu", "Shuaijie She", "Xiao Liu", "Yeyun Gong", "Shujian Huang", "Jiajun Chen"], "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u795e\u7ecf\u5143\u8bc6\u522b\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u548c\u8bed\u8a00\u65e0\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5bf9\u9f50\u5bf9\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u5bf9\u9f50\u5982\u4f55\u901a\u8fc7\u9ad8\u8d44\u6e90\u8bed\u8a00\u5411\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u80fd\u529b\u8fc1\u79fb\u6765\u589e\u5f3aLLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u540c\u65f6\u63a2\u7d22\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u5143\u8bc6\u522b\u7b97\u6cd5\uff0c\u5c06\u795e\u7ecf\u5143\u5206\u4e3a\u8bed\u8a00\u7279\u5b9a\u3001\u8bed\u8a00\u76f8\u5173\u548c\u8bed\u8a00\u65e0\u5173\u4e09\u7c7b\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u6790\u4e86LLMs\u7684\u591a\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u5bf9\u9f50\u524d\u540e\u6a21\u578b\u7684\u795e\u7ecf\u5143\u5206\u5e03\u53d8\u5316\uff0c\u5e76\u53d1\u73b0\u4e86\u201c\u81ea\u53d1\u591a\u8bed\u8a00\u5bf9\u9f50\u201d\u73b0\u8c61\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u5143\u5206\u6790\u63d0\u4f9b\u4e86\u5bf9\u591a\u8bed\u8a00\u5bf9\u9f50\u548cLLMs\u591a\u8bed\u8a00\u80fd\u529b\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.21501", "pdf": "https://arxiv.org/pdf/2505.21501", "abs": "https://arxiv.org/abs/2505.21501", "authors": ["Yinjie Chen", "Zipeng Yan", "Chong Zhou", "Bo Dai", "Andrew F. Luo"], "title": "Vision Transformers with Self-Distilled Registers", "categories": ["cs.CV"], "comment": "27 pages, 14 figures", "summary": "Vision Transformers (ViTs) have emerged as the dominant architecture for\nvisual processing tasks, demonstrating excellent scalability with increased\ntraining data and model size. However, recent work has identified the emergence\nof artifact tokens in ViTs that are incongruous with the local semantics. These\nanomalous tokens degrade ViT performance in tasks that require fine-grained\nlocalization or structural coherence. An effective mitigation of this issue is\nto the addition of register tokens to ViTs, which implicitly \"absorb\" the\nartifact term during training. Given the availability of various large-scale\npre-trained ViTs, in this paper we aim at equipping them with such register\ntokens without the need of re-training them from scratch, which is infeasible\nconsidering their size. Specifically, we propose Post Hoc Registers (PH-Reg),\nan efficient self-distillation method that integrates registers into an\nexisting ViT without requiring additional labeled data and full retraining.\nPH-Reg initializes both teacher and student networks from the same pre-trained\nViT. The teacher remains frozen and unmodified, while the student is augmented\nwith randomly initialized register tokens. By applying test-time augmentation\nto the teacher's inputs, we generate denoised dense embeddings free of\nartifacts, which are then used to optimize only a small subset of unlocked\nstudent weights. We show that our approach can effectively reduce the number of\nartifact tokens, improving the segmentation and depth prediction of the student\nViT under zero-shot and linear probing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff08PH-Reg\uff09\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u6280\u672f\u4e3a\u9884\u8bad\u7ec3\u7684ViT\u6a21\u578b\u6dfb\u52a0\u5bc4\u5b58\u5668\u4ee4\u724c\uff0c\u4ee5\u51cf\u5c11\u5f02\u5e38\u4ee4\u724c\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "ViT\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u4efb\u52a1\u65f6\u4f1a\u51fa\u73b0\u4e0e\u5c40\u90e8\u8bed\u4e49\u4e0d\u7b26\u7684\u5f02\u5e38\u4ee4\u724c\uff0c\u5f71\u54cd\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u9884\u8bad\u7ec3ViT\u6a21\u578b\u89c4\u6a21\u5927\uff0c\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faPH-Reg\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u6280\u672f\u5c06\u5bc4\u5b58\u5668\u4ee4\u724c\u96c6\u6210\u5230\u9884\u8bad\u7ec3ViT\u4e2d\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\u6216\u5b8c\u6574\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "PH-Reg\u80fd\u6709\u6548\u51cf\u5c11\u5f02\u5e38\u4ee4\u724c\u6570\u91cf\uff0c\u63d0\u5347\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u7ebf\u6027\u63a2\u6d4b\u4efb\u52a1\u4e2d\u7684\u5206\u5272\u548c\u6df1\u5ea6\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "PH-Reg\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u9884\u8bad\u7ec3ViT\u6a21\u578b\u7684\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.21475", "pdf": "https://arxiv.org/pdf/2505.21475", "abs": "https://arxiv.org/abs/2505.21475", "authors": ["Ilias Diakonikolas", "Giannis Iakovidis", "Daniel M. Kane", "Lisheng Ren"], "title": "Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "We study the complexity of learning real-valued Multi-Index Models (MIMs)\nunder the Gaussian distribution. A $K$-MIM is a function $f:\\mathbb{R}^d\\to\n\\mathbb{R}$ that depends only on the projection of its input onto a\n$K$-dimensional subspace. We give a general algorithm for PAC learning a broad\nclass of MIMs with respect to the square loss, even in the presence of\nadversarial label noise. Moreover, we establish a nearly matching Statistical\nQuery (SQ) lower bound, providing evidence that the complexity of our algorithm\nis qualitatively optimal as a function of the dimension. Specifically, we\nconsider the class of bounded variation MIMs with the property that degree at\nmost $m$ distinguishing moments exist with respect to projections onto any\nsubspace. In the presence of adversarial label noise, the complexity of our\nlearning algorithm is $d^{O(m)}2^{\\mathrm{poly}(K/\\epsilon)}$. For the\nrealizable and independent noise settings, our algorithm incurs complexity\n$d^{O(m)}2^{\\mathrm{poly}(K)}(1/\\epsilon)^{O(K)}$. To complement our upper\nbound, we show that if for some subspace degree-$m$ distinguishing moments do\nnot exist, then any SQ learner for the corresponding class of MIMs requires\ncomplexity $d^{\\Omega(m)}$. As an application, we give the first efficient\nlearner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The\nresulting algorithm has complexity $\\mathrm{poly}(d)\n2^{\\mathrm{poly}(KL/\\epsilon)}$. This gives a new PAC learning algorithm for\nLipschitz homogeneous ReLU networks with complexity independent of the network\nsize, removing the exponential dependence incurred in prior work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u9ad8\u65af\u5206\u5e03\u4e0b\u5b66\u4e60\u5b9e\u503c\u591a\u6307\u6570\u6a21\u578b\uff08MIMs\uff09\u7684\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5bf9\u6297\u6027\u6807\u7b7e\u566a\u58f0\u4e0b\u5b66\u4e60MIMs\u7684\u901a\u7528\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u590d\u6742\u6027\u5728\u7ef4\u5ea6\u4e0a\u7684\u6700\u4f18\u6027\u3002", "motivation": "\u7814\u7a76MIMs\u7684\u5b66\u4e60\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u566a\u58f0\u4e0b\u7684\u5b66\u4e60\u6548\u7387\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u65b9\u635f\u5931\u7684PAC\u5b66\u4e60\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u6709\u754c\u53d8\u5f02\u7684MIMs\uff0c\u5e76\u5206\u6790\u4e86\u5176\u590d\u6742\u6027\u3002", "result": "\u5728\u5bf9\u6297\u6027\u566a\u58f0\u4e0b\uff0c\u7b97\u6cd5\u7684\u590d\u6742\u6027\u4e3ad^O(m)2^poly(K/\u03b5)\uff1b\u5728\u53ef\u5b9e\u73b0\u548c\u72ec\u7acb\u566a\u58f0\u4e0b\uff0c\u590d\u6742\u6027\u4e3ad^O(m)2^poly(K)(1/\u03b5)^O(K)\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3aLipschitz\u9f50\u6b21ReLU\u7f51\u7edc\u63d0\u4f9b\u4e86\u72ec\u7acb\u4e8e\u7f51\u7edc\u89c4\u6a21\u7684\u5b66\u4e60\u590d\u6742\u6027\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u6307\u6570\u4f9d\u8d56\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20771", "pdf": "https://arxiv.org/pdf/2505.20771", "abs": "https://arxiv.org/abs/2505.20771", "authors": ["Heng Tang", "Feng Liu", "Xinbo Chen", "Jiawei Chen", "Bohao Wang", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Bingde Hu", "Can Wang"], "title": "Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recent years have witnessed extensive exploration of Large Language Models\n(LLMs) on the field of Recommender Systems (RS). There are currently two\ncommonly used strategies to enable LLMs to have recommendation capabilities: 1)\nThe \"Guidance-Only\" strategy uses in-context learning to exploit and amplify\nthe inherent semantic understanding and item recommendation capabilities of\nLLMs; 2) The \"Tuning-Only\" strategy uses supervised fine-tuning (SFT) to\nfine-tune LLMs with the aim of fitting them to real recommendation data.\nHowever, neither of these strategies can effectively bridge the gap between the\nknowledge space of LLMs and recommendation, and their performance do not meet\nour expectations.\n  To better enable LLMs to learn recommendation knowledge, we combine the\nadvantages of the above two strategies and proposed a novel \"Guidance+Tuning\"\nmethod called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of\ncurriculum learning. It first employs self-distillation to construct an\nauxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it\nfurther utilizes a self-adaptive curriculum scheduler to enable LLMs to\ngradually learn from simpler data (self-distilled data) to more challenging\ndata (real RS data). Extensive experiments demonstrate that SOFT significantly\nenhances the recommendation accuracy (37.59\\% on average) of LLM-based methods.\nThe code is available via\nhttps://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSOFT\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86'Guidance-Only'\u548c'Tuning-Only'\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5f25\u5408LLM\u77e5\u8bc6\u7a7a\u95f4\u4e0e\u63a8\u8350\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6027\u80fd\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51faSOFT\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u84b8\u998f\u6784\u5efa\u8f85\u52a9\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u8bfe\u7a0b\u8c03\u5ea6\u5668\u9010\u6b65\u5b66\u4e60\u3002", "result": "SOFT\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\uff08\u5e73\u574737.59%\uff09\u3002", "conclusion": "SOFT\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002", "relevance": 60.0}}
{"id": "2505.21502", "pdf": "https://arxiv.org/pdf/2505.21502", "abs": "https://arxiv.org/abs/2505.21502", "authors": ["Yipengjing Sun", "Chenyang Wang", "Shunyuan Zheng", "Zonglin Li", "Shengping Zhang", "Xiangyang Ji"], "title": "Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis", "categories": ["cs.CV"], "comment": "Project Webpage: https://sypj-98.github.io/grgs/", "summary": "We propose GRGS, a generalizable and relightable 3D Gaussian framework for\nhigh-fidelity human novel view synthesis under diverse lighting conditions.\nUnlike existing methods that rely on per-character optimization or ignore\nphysical constraints, GRGS adopts a feed-forward, fully supervised strategy\nthat projects geometry, material, and illumination cues from multi-view 2D\nobservations into 3D Gaussian representations. Specifically, to reconstruct\nlighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement\n(LGR) module trained on synthetically relit data to predict accurate depth and\nsurface normals. Based on the high-quality geometry, a Physically Grounded\nNeural Rendering (PGNR) module is further proposed to integrate neural\nprediction with physics-based shading, supporting editable relighting with\nshadows and indirect illumination. Besides, we design a 2D-to-3D projection\ntraining scheme that leverages differentiable supervision from ambient\nocclusion, direct, and indirect lighting maps, which alleviates the\ncomputational cost of explicit ray tracing. Extensive experiments demonstrate\nthat GRGS achieves superior visual quality, geometric consistency, and\ngeneralization across characters and lighting conditions.", "AI": {"tldr": "GRGS\u662f\u4e00\u4e2a\u53ef\u6cdb\u5316\u4e14\u53ef\u91cd\u65b0\u5149\u7167\u76843D\u9ad8\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u6837\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u4eba\u7c7b\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6bcf\u89d2\u8272\u4f18\u5316\u6216\u5ffd\u7565\u7269\u7406\u7ea6\u675f\uff0cGRGS\u65e8\u5728\u901a\u8fc7\u524d\u9988\u5168\u76d1\u7763\u7b56\u7565\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5149\u7167\u611f\u77e5\u51e0\u4f55\u7ec6\u5316\uff08LGR\uff09\u6a21\u5757\u548c\u7269\u7406\u57fa\u7840\u795e\u7ecf\u6e32\u67d3\uff08PGNR\uff09\u6a21\u5757\uff0c\u7ed3\u54082D\u52303D\u6295\u5f71\u8bad\u7ec3\u65b9\u6848\u3002", "result": "GRGS\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8de8\u89d2\u8272\u53ca\u5149\u7167\u6761\u4ef6\u7684\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GRGS\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u9884\u6d4b\u4e0e\u7269\u7406\u6e32\u67d3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u53ef\u7f16\u8f91\u7684\u91cd\u65b0\u5149\u7167\u6548\u679c\u3002", "relevance": 30.0}}
{"id": "2505.21487", "pdf": "https://arxiv.org/pdf/2505.21487", "abs": "https://arxiv.org/abs/2505.21487", "authors": ["Ted Zadouri", "Hubert Strauss", "Tri Dao"], "title": "Hardware-Efficient Attention for Fast Decoding", "categories": ["cs.LG", "cs.CL"], "comment": "37 pages, 15 figures, 45 tables", "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6ce8\u610f\u529b\u53d8\u4f53\uff08GTA\u548cGLA\uff09\uff0c\u901a\u8fc7\u51cf\u5c11KV\u7f13\u5b58\u52a0\u8f7d\u548c\u4f18\u5316\u5e76\u884c\u6027\uff0c\u63d0\u5347LLM\u89e3\u7801\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u89e3\u7801\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5927\u6279\u91cf\u65f6\u53d7\u9650\u4e8eKV\u7f13\u5b58\u52a0\u8f7d\u7684\u9ad8\u5ef6\u8fdf\u548c\u5e76\u884c\u6027\u4e0d\u8db3\uff0c\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u3002", "method": "\u63d0\u51faGrouped-Tied Attention (GTA)\u51cf\u5c11\u5185\u5b58\u4f20\u8f93\uff0cGrouped Latent Attention (GLA)\u4f18\u5316\u5e76\u884c\u89e3\u7801\u3002", "result": "GTA\u5339\u914dGQA\u8d28\u91cf\u4f46\u51cf\u5c11\u4e00\u534aKV\u7f13\u5b58\uff0cGLA\u5339\u914dMLA\u6027\u80fd\u4e14\u66f4\u6613\u5206\u7247\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u53472\u500d\u3002", "conclusion": "GTA\u548cGLA\u663e\u8457\u63d0\u5347\u89e3\u7801\u6548\u7387\uff0c\u9002\u5408\u5728\u7ebf\u670d\u52a1\u573a\u666f\u3002", "relevance": 90.0}}
{"id": "2505.21493", "pdf": "https://arxiv.org/pdf/2505.21493", "abs": "https://arxiv.org/abs/2505.21493", "authors": ["Xiangxin Zhou", "Zichen Liu", "Anya Sims", "Haonan Wang", "Tianyu Pang", "Chongxuan Li", "Liang Wang", "Min Lin", "Chao Du"], "title": "Reinforcing General Reasoning without Verifiers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5VeriFree\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u6700\u5927\u5316\u751f\u6210\u53c2\u8003\u7b54\u6848\u7684\u6982\u7387\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u9a8c\u8bc1\u5668\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u73b0\u5b9e\u9886\u57df\uff08\u5982\u5316\u5b66\u3001\u533b\u7597\u7b49\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u4f9d\u8d56\u5f3a\u9a8c\u8bc1\u5668\u6a21\u578b\u3002", "method": "\u63d0\u51faVeriFree\u65b9\u6cd5\uff0c\u7ed5\u8fc7\u7b54\u6848\u9a8c\u8bc1\uff0c\u76f4\u63a5\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u751f\u6210\u53c2\u8003\u7b54\u6848\u7684\u6982\u7387\u3002", "result": "VeriFree\u5728MMLU-Pro\u3001GPQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "VeriFree\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u63a8\u7406\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u8bad\u7ec3\u548c\u53d8\u5206\u4f18\u5316\u7684\u65b0\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.20783", "pdf": "https://arxiv.org/pdf/2505.20783", "abs": "https://arxiv.org/abs/2505.20783", "authors": ["Jiaping Xiao", "Cheng Wen Tsao", "Yuhang Zhang", "Mir Feroskhan"], "title": "FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation", "categories": ["cs.RO", "cs.AI"], "comment": "This work has been submitted for possible publication", "summary": "Path planning is a critical component in autonomous drone operations,\nenabling safe and efficient navigation through complex environments. Recent\nadvances in foundation models, particularly large language models (LLMs) and\nvision-language models (VLMs), have opened new opportunities for enhanced\nperception and intelligent decision-making in robotics. However, their\npractical applicability and effectiveness in global path planning remain\nrelatively unexplored. This paper proposes foundation model-guided path\nplanners (FM-Planner) and presents a comprehensive benchmarking study and\npractical validation for drone path planning. Specifically, we first\nsystematically evaluate eight representative LLM and VLM approaches using\nstandardized simulation scenarios. To enable effective real-time navigation, we\nthen design an integrated LLM-Vision planner that combines semantic reasoning\nwith visual perception. Furthermore, we deploy and validate the proposed path\nplanner through real-world experiments under multiple configurations. Our\nfindings provide valuable insights into the strengths, limitations, and\nfeasibility of deploying foundation models in real-world drone applications and\nproviding practical implementations in autonomous flight. Project site:\nhttps://github.com/NTU-ICG/FM-Planner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff08LLM\u548cVLM\uff09\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff08FM-Planner\uff09\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\uff08\u5982LLM\u548cVLM\uff09\u5728\u65e0\u4eba\u673a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "1. \u7cfb\u7edf\u8bc4\u4f30\u4e868\u79cd\u4ee3\u8868\u6027LLM\u548cVLM\u65b9\u6cd5\uff1b2. \u8bbe\u8ba1\u4e86\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u4e0e\u89c6\u89c9\u611f\u77e5\u7684LLM-Vision\u89c4\u5212\u5668\uff1b3. \u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u7840\u6a21\u578b\u5728\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u53ef\u884c\u6027\u7684\u89c1\u89e3\uff0c\u5e76\u5b9e\u73b0\u4e86\u81ea\u4e3b\u98de\u884c\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u5b9e\u65f6\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 40.0}}
{"id": "2504.15254", "pdf": "https://arxiv.org/pdf/2504.15254", "abs": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.", "AI": {"tldr": "CRUST-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30C\u5230Rust\u8f6c\u8bd1\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b100\u4e2aC\u4ed3\u5e93\u53ca\u5176\u5bf9\u5e94\u7684\u5b89\u5168Rust\u63a5\u53e3\u548c\u6d4b\u8bd5\u7528\u4f8b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684LLM\uff08\u5982OpenAI o1\uff09\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4ecd\u6709\u9650\u3002", "motivation": "\u7f3a\u4e4f\u8bc4\u4f30C\u5230Rust\u8f6c\u8bd1\u7684\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u590d\u6742\u9879\u76ee\u548c\u4f9d\u8d56\u5173\u7cfb\u7684\u573a\u666f\u3002", "method": "\u6784\u5efaCRUST-Bench\u6570\u636e\u96c6\uff0c\u5305\u542bC\u4ed3\u5e93\u3001\u5b89\u5168Rust\u63a5\u53e3\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u751f\u6210\u5b89\u5168\u548c\u60ef\u7528\u7684Rust\u4ee3\u7801\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u6700\u4f73\u6a21\u578bOpenAI o1\u4ec5\u80fd\u89e3\u51b315\u4e2a\u4efb\u52a1\u3002", "conclusion": "CRUST-Bench\u4e3a\u6539\u8fdb\u8f6c\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u672a\u6765\u9700\u63d0\u5347LLM\u5904\u7406\u590d\u6742\u573a\u666f\u7684\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.20301", "pdf": "https://arxiv.org/pdf/2505.20301", "abs": "https://arxiv.org/abs/2505.20301", "authors": ["Chen Liu", "Mingchen Li", "Yang Tan", "Wenrui Gou", "Guisheng Fan", "Bingxin Zhou"], "title": "Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "A pivotal area of research in antibody engineering is to find effective\nmodifications that enhance antibody-antigen binding affinity. Traditional\nwet-lab experiments assess mutants in a costly and time-consuming manner.\nEmerging deep learning solutions offer an alternative by modeling antibody\nstructures to predict binding affinity changes. However, they heavily depend on\nhigh-quality complex structures, which are frequently unavailable in practice.\nTherefore, we propose ProtAttBA, a deep learning model that predicts binding\naffinity changes based solely on the sequence information of antibody-antigen\ncomplexes. ProtAttBA employs a pre-training phase to learn protein sequence\npatterns, following a supervised training phase using labeled antibody-antigen\ncomplex data to train a cross-attention-based regressor for predicting binding\naffinity changes. We evaluated ProtAttBA on three open benchmarks under\ndifferent conditions. Compared to both sequence- and structure-based prediction\nmethods, our approach achieves competitive performance, demonstrating notable\nrobustness, especially with uncertain complex structures. Notably, our method\npossesses interpretability from the attention mechanism. We show that the\nlearned attention scores can identify critical residues with impacts on binding\naffinity. This work introduces a rapid and cost-effective computational tool\nfor antibody engineering, with the potential to accelerate the development of\nnovel therapeutic antibodies.", "AI": {"tldr": "ProtAttBA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e8f\u5217\u4fe1\u606f\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6297\u4f53-\u6297\u539f\u7ed3\u5408\u4eb2\u548c\u529b\u53d8\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7ed3\u6784\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u6e7f\u5b9e\u9a8c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7ed3\u6784\u6570\u636e\uff0c\u800c\u5b9e\u8df5\u4e2d\u8fd9\u4e9b\u6570\u636e\u5e38\u4e0d\u53ef\u5f97\u3002", "method": "ProtAttBA\u901a\u8fc7\u9884\u8bad\u7ec3\u5b66\u4e60\u86cb\u767d\u8d28\u5e8f\u5217\u6a21\u5f0f\uff0c\u518d\u7528\u76d1\u7763\u8bad\u7ec3\u8bad\u7ec3\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u56de\u5f52\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u4e0d\u786e\u5b9a\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ProtAttBA\u4e3a\u6297\u4f53\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u7ecf\u6d4e\u7684\u8ba1\u7b97\u5de5\u5177\uff0c\u6709\u671b\u52a0\u901f\u6cbb\u7597\u6027\u6297\u4f53\u7684\u5f00\u53d1\u3002", "relevance": 30.0}}
{"id": "2505.20794", "pdf": "https://arxiv.org/pdf/2505.20794", "abs": "https://arxiv.org/abs/2505.20794", "authors": ["Joon-Seung Choi", "Dong-Min Byun", "Hyung-Seok Oh", "Seong-Whan Lee"], "title": "VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Proceedings of Interspeech 2025", "summary": "Controlling singing style is crucial for achieving an expressive and natural\nsinging voice. Among the various style factors, vibrato plays a key role in\nconveying emotions and enhancing musical depth. However, modeling vibrato\nremains challenging due to its dynamic nature, making it difficult to control\nin singing voice conversion. To address this, we propose VibESVC, a\ncontrollable singing voice conversion model that explicitly extracts and\nmanipulates vibrato using discrete wavelet transform. Unlike previous methods\nthat model vibrato implicitly, our approach decomposes the F0 contour into\nfrequency components, enabling precise transfer. This allows vibrato control\nfor enhanced flexibility. Experimental results show that VibE-SVC effectively\ntransforms singing styles while preserving speaker similarity. Both subjective\nand objective evaluations confirm high-quality conversion.", "AI": {"tldr": "VibE-SVC\u662f\u4e00\u4e2a\u53ef\u63a7\u7684\u6b4c\u5531\u58f0\u97f3\u8f6c\u6362\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u663e\u5f0f\u63d0\u53d6\u548c\u64cd\u7eb5\u98a4\u97f3\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u98ce\u683c\u8f6c\u6362\u3002", "motivation": "\u98a4\u97f3\u5728\u6b4c\u5531\u4e2d\u4f20\u9012\u60c5\u611f\u548c\u589e\u5f3a\u97f3\u4e50\u6df1\u5ea6\u65b9\u9762\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u56e0\u5176\u52a8\u6001\u7279\u6027\u96be\u4ee5\u5efa\u6a21\u548c\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5206\u89e3F0\u8f6e\u5ed3\uff0c\u663e\u5f0f\u63d0\u53d6\u548c\u64cd\u7eb5\u98a4\u97f3\uff0c\u5b9e\u73b0\u7cbe\u786e\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVibE-SVC\u80fd\u6709\u6548\u8f6c\u6362\u6b4c\u5531\u98ce\u683c\u5e76\u4fdd\u6301\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\uff0c\u4e3b\u5ba2\u89c2\u8bc4\u4f30\u5747\u8bc1\u5b9e\u9ad8\u8d28\u91cf\u8f6c\u6362\u3002", "conclusion": "VibE-SVC\u901a\u8fc7\u663e\u5f0f\u98a4\u97f3\u63a7\u5236\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u9ad8\u8d28\u91cf\u7684\u6b4c\u5531\u58f0\u97f3\u8f6c\u6362\u3002", "relevance": 20.0}}
{"id": "2505.20411", "pdf": "https://arxiv.org/pdf/2505.20411", "abs": "https://arxiv.org/abs/2505.20411", "authors": ["Ibragim Badertdinov", "Alexander Golubev", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Andrei Andriushchenko", "Maria Trofimova", "Daria Litvintseva", "Boris Yangel"], "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "categories": ["cs.SE", "cs.CL"], "comment": "Dataset: https://huggingface.co/datasets/nebius/SWE-rebench,\n  SWE-rebench leaderboard https://swe-rebench.com", "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4eceGitHub\u63d0\u53d6\u4ea4\u4e92\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u6784\u5efa\u4e86SWE-rebench\u6570\u636e\u96c6\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u65e0\u6c61\u67d3\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u56e0\u6c61\u67d3\u95ee\u9898\u5feb\u901f\u8fc7\u65f6\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u4eceGitHub\u63d0\u53d6\u4ea4\u4e92\u5f0f\u4efb\u52a1\uff0c\u6784\u5efaSWE-rebench\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u65e0\u6c61\u67d3\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b21,000\u591a\u4e2a\u4ea4\u4e92\u5f0f\u4efb\u52a1\u7684SWE-rebench\u6570\u636e\u96c6\uff0c\u5e76\u53d1\u73b0\u67d0\u4e9b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u56e0\u6c61\u67d3\u95ee\u9898\u88ab\u9ad8\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u57fa\u51c6\u6c61\u67d3\u95ee\u9898\uff0c\u4e3aLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 75.0}}
{"id": "2505.20824", "pdf": "https://arxiv.org/pdf/2505.20824", "abs": "https://arxiv.org/abs/2505.20824", "authors": ["Kai Chen", "Taihang Zhen", "Hewei Wang", "Kailai Liu", "Xinfeng Li", "Jing Huo", "Tianpei Yang", "Jinfeng Xu", "Wei Dong", "Yang Gao"], "title": "MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in healthcare,\nensuring their safety, particularly within collaborative multi-agent\nconfigurations, is paramount. In this paper we introduce MedSentry, a benchmark\ncomprising 5 000 adversarial medical prompts spanning 25 threat categories with\n100 subthemes. Coupled with this dataset, we develop an end-to-end\nattack-defense evaluation pipeline to systematically analyze how four\nrepresentative multi-agent topologies (Layers, SharedPool, Centralized, and\nDecentralized) withstand attacks from 'dark-personality' agents. Our findings\nreveal critical differences in how these architectures handle information\ncontamination and maintain robust decision-making, exposing their underlying\nvulnerability mechanisms. For instance, SharedPool's open information sharing\nmakes it highly susceptible, whereas Decentralized architectures exhibit\ngreater resilience thanks to inherent redundancy and isolation. To mitigate\nthese risks, we propose a personality-scale detection and correction mechanism\nthat identifies and rehabilitates malicious agents, restoring system safety to\nnear-baseline levels. MedSentry thus furnishes both a rigorous evaluation\nframework and practical defense strategies that guide the design of safer\nLLM-based multi-agent systems in medical domains.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MedSentry\uff0c\u4e00\u4e2a\u5305\u542b5000\u4e2a\u5bf9\u6297\u6027\u533b\u7597\u63d0\u793a\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u968f\u7740LLM\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u5728\u591a\u4ee3\u7406\u914d\u7f6e\u4e2d\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u653b\u51fb-\u9632\u5fa1\u8bc4\u4f30\u7ba1\u9053\uff0c\u5206\u6790\u4e86\u56db\u79cd\u591a\u4ee3\u7406\u62d3\u6251\u7ed3\u6784\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u548c\u7ea0\u6b63\u6076\u610f\u4ee3\u7406\u7684\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u5728\u4fe1\u606f\u6c61\u67d3\u548c\u51b3\u7b56\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5176\u4e2dDecentralized\u67b6\u6784\u8868\u73b0\u6700\u4f73\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u673a\u5236\u80fd\u6062\u590d\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "conclusion": "MedSentry\u4e3a\u533b\u7597\u9886\u57df\u7684\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u9632\u5fa1\u7b56\u7565\u3002", "relevance": 85.0}}
{"id": "2505.20464", "pdf": "https://arxiv.org/pdf/2505.20464", "abs": "https://arxiv.org/abs/2505.20464", "authors": ["Samuel Rhys Cox", "Rune M\u00f8berg Jacobsen", "Niels van Berkel"], "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions", "categories": ["cs.HC", "cs.CL"], "comment": "In ACM Conversational User Interfaces (CUI '25), July 8-10, 2025; 18\n  pages; 6 Figures; 6 Tables", "summary": "Self-disclosure, the sharing of one's thoughts and feelings, is affected by\nthe perceived relationship between individuals. While chatbots are increasingly\nused for self-disclosure, the impact of a chatbot's framing on users'\nself-disclosure remains under-explored. We investigated how a chatbot's\ndescription of its relationship with users, particularly in terms of\nephemerality, affects self-disclosure. Specifically, we compared a Familiar\nchatbot, presenting itself as a companion remembering past interactions, with a\nStranger chatbot, presenting itself as a new, unacquainted entity in each\nconversation. In a mixed factorial design, participants engaged with either the\nFamiliar or Stranger chatbot in two sessions across two days, with one\nconversation focusing on Emotional- and another Factual-disclosure. When\nEmotional-disclosure was sought in the first chatting session,\nStranger-condition participants felt more comfortable self-disclosing. However,\nwhen Factual-disclosure was sought first, these differences were replaced by\nmore enjoyment among Familiar-condition participants. Qualitative findings\nshowed Stranger afforded anonymity and reduced judgement, whereas Familiar\nsometimes felt intrusive unless rapport was built via low-risk\nFactual-disclosure.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u804a\u5929\u673a\u5668\u4eba\u6846\u67b6\uff08\u719f\u6089\u578b\u4e0e\u964c\u751f\u578b\uff09\u5bf9\u7528\u6237\u81ea\u6211\u62ab\u9732\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u964c\u751f\u578b\u5728\u60c5\u611f\u62ab\u9732\u65f6\u66f4\u8212\u9002\uff0c\u800c\u719f\u6089\u578b\u5728\u4e8b\u5b9e\u62ab\u9732\u540e\u66f4\u53d7\u6b22\u8fce\u3002", "motivation": "\u63a2\u7d22\u804a\u5929\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u7684\u5173\u7cfb\u6846\u67b6\uff08\u719f\u6089\u6216\u964c\u751f\uff09\u5f71\u54cd\u7528\u6237\u7684\u81ea\u6211\u62ab\u9732\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6df7\u5408\u56e0\u5b50\u8bbe\u8ba1\uff0c\u53c2\u4e0e\u8005\u4e0e\u719f\u6089\u578b\u6216\u964c\u751f\u578b\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\uff0c\u5206\u522b\u8fdb\u884c\u60c5\u611f\u548c\u4e8b\u5b9e\u62ab\u9732\u4efb\u52a1\u3002", "result": "\u964c\u751f\u578b\u5728\u60c5\u611f\u62ab\u9732\u65f6\u66f4\u8212\u9002\uff1b\u719f\u6089\u578b\u5728\u4e8b\u5b9e\u62ab\u9732\u540e\u66f4\u53d7\u6b22\u8fce\uff0c\u63d0\u4f9b\u533f\u540d\u611f\u548c\u51cf\u5c11\u8bc4\u5224\u3002", "conclusion": "\u804a\u5929\u673a\u5668\u4eba\u6846\u67b6\u9700\u6839\u636e\u62ab\u9732\u7c7b\u578b\uff08\u60c5\u611f\u6216\u4e8b\u5b9e\uff09\u8c03\u6574\uff0c\u4ee5\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u3002", "relevance": 40.0}}
{"id": "2505.20431", "pdf": "https://arxiv.org/pdf/2505.20431", "abs": "https://arxiv.org/abs/2505.20431", "authors": ["Qimin Chen", "Yuezhi Yang", "Yifang Wang", "Vladimir G. Kim", "Siddhartha Chaudhuri", "Hao Zhang", "Zhiqin Chen"], "title": "ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We introduce a 3D detailizer, a neural model which can instantaneously (in\n<1s) transform a coarse 3D shape proxy into a high-quality asset with detailed\ngeometry and texture as guided by an input text prompt. Our model is trained\nusing the text prompt, which defines the shape class and characterizes the\nappearance and fine-grained style of the generated details. The coarse 3D\nproxy, which can be easily varied and adjusted (e.g., via user editing),\nprovides structure control over the final shape. Importantly, our detailizer is\nnot optimized for a single shape; it is the result of distilling a generative\nmodel, so that it can be reused, without retraining, to generate any number of\nshapes, with varied structures, whose local details all share a consistent\nstyle and appearance. Our detailizer training utilizes a pretrained multi-view\nimage diffusion model, with text conditioning, to distill the foundational\nknowledge therein into our detailizer via Score Distillation Sampling (SDS). To\nimprove SDS and enable our detailizer architecture to learn generalizable\nfeatures over complex structures, we train our model in two training stages to\ngenerate shapes with increasing structural complexity. Through extensive\nexperiments, we show that our method generates shapes of superior quality and\ndetails compared to existing text-to-3D models under varied structure control.\nOur detailizer can refine a coarse shape in less than a second, making it\npossible to interactively author and adjust 3D shapes. Furthermore, the\nuser-imposed structure control can lead to creative, and hence\nout-of-distribution, 3D asset generations that are beyond the current\ncapabilities of leading text-to-3D generative models. We demonstrate an\ninteractive 3D modeling workflow our method enables, and its strong\ngeneralizability over styles, structures, and object categories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D detailizer\u7684\u795e\u7ecf\u6a21\u578b\uff0c\u80fd\u591f\u5feb\u901f\u5c06\u7c97\u7cd9\u76843D\u5f62\u72b6\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u8d44\u4ea7\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u63d0\u793a\u63a7\u5236\u7ec6\u8282\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u52303D\u751f\u6210\u6a21\u578b\u5728\u7ec6\u8282\u63a7\u5236\u548c\u751f\u6210\u901f\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f3D\u5efa\u6a21\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548cScore Distillation Sampling (SDS)\u6280\u672f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u751f\u6210\u590d\u6742\u7ed3\u6784\u7684\u5f62\u72b6\u3002", "result": "\u751f\u6210\u76843D\u5f62\u72b6\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5feb\u901f\u4ea4\u4e92\u5f0f\u7f16\u8f91\uff0c\u5e76\u80fd\u751f\u6210\u8d85\u51fa\u5f53\u524d\u6a21\u578b\u80fd\u529b\u7684\u521b\u610f\u5f62\u72b6\u3002", "conclusion": "3D detailizer\u5728\u7ec6\u8282\u751f\u6210\u3001\u901f\u5ea6\u548c\u7ed3\u6784\u63a7\u5236\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4ea4\u4e92\u5f0f3D\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.20480", "pdf": "https://arxiv.org/pdf/2505.20480", "abs": "https://arxiv.org/abs/2505.20480", "authors": ["Hui Zheng", "Hai-Teng Wang", "Yi-Tao Jing", "Pei-Yang Lin", "Han-Qing Zhao", "Wei Chen", "Peng-Hu Wei", "Yong-Zhi Shan", "Guo-Guang Zhao", "Yun-Zhe Liu"], "title": "BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics", "categories": ["eess.SP", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Decoding speech directly from neural activity is a central goal in\nbrain-computer interface (BCI) research. In recent years, exciting advances\nhave been made through the growing use of intracranial field potential\nrecordings, such as stereo-ElectroEncephaloGraphy (sEEG) and\nElectroCorticoGraphy (ECoG). These neural signals capture rich population-level\nactivity but present key challenges: (i) task-relevant neural signals are\nsparsely distributed across sEEG electrodes, and (ii) they are often entangled\nwith task-irrelevant neural signals in both sEEG and ECoG. To address these\nchallenges, we introduce a unified Coarse-to-Fine neural disentanglement\nframework, BrainStratify, which includes (i) identifying functional groups\nthrough spatial-context-guided temporal-spatial modeling, and (ii)\ndisentangling distinct neural dynamics within the target functional group using\nDecoupled Product Quantization (DPQ). We evaluate BrainStratify on two\nopen-source sEEG datasets and one (epidural) ECoG dataset, spanning tasks like\nvocal production and speech perception. Extensive experiments show that\nBrainStratify, as a unified framework for decoding speech from intracranial\nneural signals, significantly outperforms previous decoding methods. Overall,\nby combining data-driven stratification with neuroscience-inspired modularity,\nBrainStratify offers a robust and interpretable solution for speech decoding\nfrom intracranial recordings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBrainStratify\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9885\u5185\u795e\u7ecf\u4fe1\u53f7\u4e2d\u89e3\u7801\u8bed\u97f3\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u795e\u7ecf\u89e3\u7f20\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9885\u5185\u795e\u7ecf\u4fe1\u53f7\u4e2d\u4efb\u52a1\u76f8\u5173\u4fe1\u53f7\u7a00\u758f\u5206\u5e03\u4e14\u4e0e\u65e0\u5173\u4fe1\u53f7\u7ea0\u7f20\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5f15\u5bfc\u7684\u65f6\u7a7a\u5efa\u6a21\u8bc6\u522b\u529f\u80fd\u7ec4\uff0c\u5e76\u4f7f\u7528\u89e3\u8026\u4ea7\u54c1\u91cf\u5316\uff08DPQ\uff09\u89e3\u7f20\u76ee\u6807\u529f\u80fd\u7ec4\u5185\u7684\u795e\u7ecf\u52a8\u6001\u3002", "result": "\u5728\u4e24\u4e2asEEG\u6570\u636e\u96c6\u548c\u4e00\u4e2aECoG\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u3002", "conclusion": "BrainStratify\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u5206\u5c42\u548c\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u6a21\u5757\u5316\uff0c\u4e3a\u9885\u5185\u8bb0\u5f55\u7684\u8bed\u97f3\u89e3\u7801\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20854", "pdf": "https://arxiv.org/pdf/2505.20854", "abs": "https://arxiv.org/abs/2505.20854", "authors": ["Xin Zhou", "Kisub Kim", "Ting Zhang", "Martin Weyssow", "Luis F. Gomes", "Guang Yang", "David Lo"], "title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "20 pages", "summary": "Large Language Models (LLMs) and other automated techniques have been\nincreasingly used to support software developers by generating software\nartifacts such as code snippets, patches, and comments. However, accurately\nassessing the correctness of these generated artifacts remains a significant\nchallenge. On one hand, human evaluation provides high accuracy but is\nlabor-intensive and lacks scalability. On the other hand, other existing\nautomatic evaluation metrics are scalable and require minimal human effort, but\nthey often fail to accurately reflect the actual correctness of generated\nsoftware artifacts.\n  In this paper, we present SWE-Judge, the first evaluation metric for\nLLM-as-Ensemble-Judge specifically designed to accurately assess the\ncorrectness of generated software artifacts. SWE-Judge first defines five\ndistinct evaluation strategies, each implemented as an independent judge. A\ndynamic team selection mechanism then identifies the most appropriate subset of\njudges to produce a final correctness score through ensembling. We evaluate\nSWE-Judge across a diverse set of software engineering (SE) benchmarks,\nincluding CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.\nThese benchmarks span three SE tasks: code generation, automated program\nrepair, and code summarization. Experimental results demonstrate that SWE-Judge\nconsistently achieves a higher correlation with human judgments, with\nimprovements ranging from 5.9% to 183.8% over existing automatic metrics.\nFurthermore, SWE-Judge reaches agreement levels with human annotators that are\ncomparable to inter-annotator agreement in code generation and program repair\ntasks. These findings underscore SWE-Judge's potential as a scalable and\nreliable alternative to human evaluation.", "AI": {"tldr": "SWE-Judge\u662f\u4e00\u79cd\u4e13\u4e3a\u8bc4\u4f30\u751f\u6210\u8f6f\u4ef6\u5de5\u4ef6\u7684\u6b63\u786e\u6027\u800c\u8bbe\u8ba1\u7684LLM\u96c6\u6210\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u52a8\u6001\u56e2\u961f\u9009\u62e9\u548c\u96c6\u6210\u7b56\u7565\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u751f\u6210\u8f6f\u4ef6\u5de5\u4ef6\u7684\u6b63\u786e\u6027\uff0c\u800c\u4eba\u5de5\u8bc4\u4f30\u867d\u51c6\u786e\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002", "method": "SWE-Judge\u5b9a\u4e49\u4e86\u4e94\u79cd\u8bc4\u4f30\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u56e2\u961f\u9009\u62e9\u548c\u96c6\u6210\u751f\u6210\u6700\u7ec8\u6b63\u786e\u6027\u5206\u6570\u3002", "result": "\u5728\u591a\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWE-Judge\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u663e\u8457\u63d0\u9ad8\uff085.9%\u81f3183.8%\uff09\uff0c\u4e14\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u7684\u4e00\u81f4\u6027\u63a5\u8fd1\u3002", "conclusion": "SWE-Judge\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u66ff\u4ee3\u4eba\u5de5\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002", "relevance": 75.0}}
{"id": "2505.20473", "pdf": "https://arxiv.org/pdf/2505.20473", "abs": "https://arxiv.org/abs/2505.20473", "authors": ["Selena Ling", "Merlin Nimier-David", "Alec Jacobson", "Nicholas Sharp"], "title": "Stochastic Preconditioning for Neural Field Optimization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "15 pages, 11 figures, SIGGRAPH 2025 (Journal track)", "summary": "Neural fields are a highly effective representation across visual computing.\nThis work observes that fitting these fields is greatly improved by\nincorporating spatial stochasticity during training, and that this simple\ntechnique can replace or even outperform custom-designed hierarchies and\nfrequency space constructions. The approach is formalized as implicitly\noperating on a blurred version of the field, evaluated in-expectation by\nsampling with Gaussian-distributed offsets. Querying the blurred field during\noptimization greatly improves convergence and robustness, akin to the role of\npreconditioners in numerical linear algebra. This implicit, sampling-based\nperspective fits naturally into the neural field paradigm, comes at no\nadditional cost, and is extremely simple to implement. We describe the basic\ntheory of this technique, including details such as handling boundary\nconditions, and extending to a spatially-varying blur. Experiments demonstrate\nthis approach on representations including coordinate MLPs, neural hashgrids,\ntriplanes, and more, across tasks including surface reconstruction and radiance\nfields. In settings where custom-designed hierarchies have already been\ndeveloped, stochastic preconditioning nearly matches or improves their\nperformance with a simple and unified approach; in settings without existing\nhierarchies it provides an immediate boost to quality and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u968f\u673a\u6027\u6765\u6539\u8fdb\u795e\u7ecf\u573a\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u968f\u673a\u9884\u5904\u7406\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u4e14\u65e0\u9700\u989d\u5916\u6210\u672c\u3002", "motivation": "\u795e\u7ecf\u573a\u5728\u89c6\u89c9\u8ba1\u7b97\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6536\u655b\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7a7a\u95f4\u968f\u673a\u6027\u6539\u8fdb\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u795e\u7ecf\u573a\u8fdb\u884c\u9ad8\u65af\u5206\u5e03\u7684\u968f\u673a\u504f\u79fb\u91c7\u6837\uff0c\u9690\u5f0f\u64cd\u4f5c\u4e8e\u6a21\u7cca\u5316\u7248\u672c\u4e0a\uff0c\u7c7b\u4f3c\u4e8e\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u4e2d\u7684\u9884\u5904\u7406\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u8868\u9762\u91cd\u5efa\u548c\u8f90\u5c04\u573a\uff09\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u751a\u81f3\u4f18\u4e8e\u5b9a\u5236\u8bbe\u8ba1\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "conclusion": "\u968f\u673a\u9884\u5904\u7406\u662f\u4e00\u79cd\u7b80\u5355\u3001\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u6280\u672f\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u795e\u7ecf\u573a\u8bad\u7ec3\uff0c\u63d0\u5347\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.20332", "pdf": "https://arxiv.org/pdf/2505.20332", "abs": "https://arxiv.org/abs/2505.20332", "authors": ["Neil Chaudhary", "Zaynah Dhunny"], "title": "An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Accurate identification of breast cancer types plays a critical role in\nguiding treatment decisions and improving patient outcomes. This paper presents\nan artificial intelligence enabled tool designed to aid in the identification\nof breast cancer types using histopathological biopsy images. Traditionally\nadditional tests have to be done on women who are detected with breast cancer\nto find out the types of cancer it is to give the necessary cure. Those tests\nare not only invasive but also delay the initiation of treatment and increase\npatient burden. The proposed model utilizes a convolutional neural network\n(CNN) architecture to distinguish between benign and malignant tissues as well\nas accurate subclassification of breast cancer types. By preprocessing the\nimages to reduce noise and enhance features, the model achieves reliable levels\nof classification performance. Experimental results on such datasets\ndemonstrate the model's effectiveness, outperforming several existing solutions\nin terms of accuracy, precision, recall, and F1-score. The study emphasizes the\npotential of deep learning techniques in clinical diagnostics and offers a\npromising tool to assist pathologists in breast cancer classification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u4eba\u5de5\u667a\u80fd\u5de5\u5177\uff0c\u7528\u4e8e\u901a\u8fc7\u7ec4\u7ec7\u75c5\u7406\u5b66\u6d3b\u68c0\u56fe\u50cf\u51c6\u786e\u8bc6\u522b\u4e73\u817a\u764c\u7c7b\u578b\uff0c\u65e8\u5728\u51cf\u5c11\u4fb5\u5165\u6027\u6d4b\u8bd5\u5e76\u52a0\u901f\u6cbb\u7597\u3002", "motivation": "\u4f20\u7edf\u4e73\u817a\u764c\u7c7b\u578b\u8bc6\u522b\u65b9\u6cd5\u9700\u8981\u989d\u5916\u7684\u4fb5\u5165\u6027\u6d4b\u8bd5\uff0c\u5ef6\u8fdf\u6cbb\u7597\u5e76\u589e\u52a0\u60a3\u8005\u8d1f\u62c5\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u4f9b\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u3001\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528CNN\u67b6\u6784\uff0c\u901a\u8fc7\u9884\u5904\u7406\u56fe\u50cf\u4ee5\u51cf\u5c11\u566a\u58f0\u548c\u589e\u5f3a\u7279\u5f81\uff0c\u5b9e\u73b0\u5bf9\u826f\u6027\u548c\u6076\u6027\u7ec4\u7ec7\u7684\u533a\u5206\u4ee5\u53ca\u4e73\u817a\u764c\u7c7b\u578b\u7684\u7cbe\u786e\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u8be5\u5de5\u5177\u4e3a\u4e73\u817a\u764c\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8f85\u52a9\u624b\u6bb5\u3002", "relevance": 20.0}}
{"id": "2505.20866", "pdf": "https://arxiv.org/pdf/2505.20866", "abs": "https://arxiv.org/abs/2505.20866", "authors": ["Xinjie Lin", "Gang Xiong", "Gaopeng Gou", "Wenqi Dong", "Jing Yu", "Zhen Li", "Wei Xia"], "title": "Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "IEEE Transactions on Information Forensics and Security (TIFS) camera\n  ready, 15 pages, 6 figures, 7 tables", "summary": "Encrypted traffic classification is highly challenging in network security\ndue to the need for extracting robust features from content-agnostic traffic\ndata. Existing approaches face critical issues: (i) Distribution drift, caused\nby reliance on the closedworld assumption, limits adaptability to realworld,\nshifting patterns; (ii) Dependence on labeled data restricts applicability\nwhere such data is scarce or unavailable. Large language models (LLMs) have\ndemonstrated remarkable potential in offering generalizable solutions across a\nwide range of tasks, achieving notable success in various specialized fields.\nHowever, their effectiveness in traffic analysis remains constrained by\nchallenges in adapting to the unique requirements of the traffic domain. In\nthis paper, we introduce a novel traffic representation model named Encrypted\nTraffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which\nintegrates LLMs with knowledge of traffic structures through a self-supervised\ninstruction tuning paradigm. This framework establishes connections between\ntextual information and traffic interactions. ETooL demonstrates more robust\nclassification performance and superior generalization in both supervised and\nzero-shot traffic classification tasks. Notably, it achieves significant\nimprovements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%),\nAPP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.)\nto 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic\ndataset designed to support dynamic distributional shifts, and use it to\nvalidate ETooL's effectiveness under varying distributional conditions.\nFurthermore, we evaluate the efficiency gains achieved through ETooL's\ninstruction tuning approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aETooL\u7684\u65b0\u578b\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6307\u4ee4\u8c03\u4f18\u5c06LLMs\u4e0e\u6d41\u91cf\u7ed3\u6784\u77e5\u8bc6\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u9762\u4e34\u5206\u5e03\u6f02\u79fb\u548c\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0cLLMs\u5728\u901a\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5c1a\u672a\u5145\u5206\u9002\u5e94\u6d41\u91cf\u5206\u6790\u9886\u57df\u3002", "method": "\u63d0\u51faETooL\u6a21\u578b\uff0c\u7ed3\u5408LLMs\u548c\u6d41\u91cf\u7ed3\u6784\u77e5\u8bc6\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u8fde\u63a5\u6587\u672c\u4fe1\u606f\u4e0e\u6d41\u91cf\u4ea4\u4e92\u3002", "result": "\u5728\u76d1\u7763\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u663e\u8457\u63d0\u5347\uff0c\u5e76\u6784\u5efa\u4e86\u52a8\u6001\u5206\u5e03\u6f02\u79fb\u6570\u636e\u96c6NETD\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "ETooL\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u6709\u6548\u89e3\u51b3\u4e86\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86LLMs\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.20337", "pdf": "https://arxiv.org/pdf/2505.20337", "abs": "https://arxiv.org/abs/2505.20337", "authors": ["Xin Wang", "Han-Xiao Tao", "Re-Bing Wu"], "title": "Predictive Performance of Deep Quantum Data Re-uploading Models", "categories": ["quant-ph", "cs.LG"], "comment": "34 pages, 15 figures", "summary": "Quantum machine learning models incorporating data re-uploading circuits have\ngarnered significant attention due to their exceptional expressivity and\ntrainability. However, their ability to generate accurate predictions on unseen\ndata, referred to as the predictive performance, remains insufficiently\ninvestigated. This study reveals a fundamental limitation in predictive\nperformance when deep encoding layers are employed within the data re-uploading\nmodel. Concretely, we theoretically demonstrate that when processing\nhigh-dimensional data with limited-qubit data re-uploading models, their\npredictive performance progressively degenerates to near random-guessing levels\nas the number of encoding layers increases. In this context, the repeated data\nuploading cannot mitigate the performance degradation. These findings are\nvalidated through experiments on both synthetic linearly separable datasets and\nreal-world datasets. Our results demonstrate that when processing\nhigh-dimensional data, the quantum data re-uploading models should be designed\nwith wider circuit architectures rather than deeper and narrower ones.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6570\u636e\u91cd\u4e0a\u4f20\u7535\u8def\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\uff0c\u6df1\u7f16\u7801\u5c42\u4f1a\u5bfc\u81f4\u6027\u80fd\u9000\u5316\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u6570\u636e\u91cd\u4e0a\u4f20\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u6570\u636e\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5305\u62ec\u5408\u6210\u7ebf\u6027\u53ef\u5206\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u6df1\u7f16\u7801\u5c42\u4f1a\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u9000\u5316\u81f3\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u6c34\u5e73\uff0c\u5efa\u8bae\u91c7\u7528\u66f4\u5bbd\u7684\u7535\u8def\u67b6\u6784\u800c\u975e\u66f4\u6df1\u5c42\u3002", "conclusion": "\u8bbe\u8ba1\u91cf\u5b50\u6570\u636e\u91cd\u4e0a\u4f20\u6a21\u578b\u65f6\u5e94\u907f\u514d\u8fc7\u6df1\u7684\u7f16\u7801\u5c42\uff0c\u4f18\u5148\u8003\u8651\u5bbd\u5ea6\u3002", "relevance": 30.0}}
{"id": "2505.20868", "pdf": "https://arxiv.org/pdf/2505.20868", "abs": "https://arxiv.org/abs/2505.20868", "authors": ["Nam-Gyu Kim", "Deok-Hyeon Cho", "Seung-Bin Kim", "Seong-Whan Lee"], "title": "Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "Recent advances in expressive text-to-speech (TTS) have introduced diverse\nmethods based on style embedding extracted from reference speech. However,\nsynthesizing high-quality expressive speech remains challenging. We propose\nSpotlight-TTS, which exclusively emphasizes style via voiced-aware style\nextraction and style direction adjustment. Voiced-aware style extraction\nfocuses on voiced regions highly related to style while maintaining continuity\nacross different speech regions to improve expressiveness. We adjust the\ndirection of the extracted style for optimal integration into the TTS model,\nwhich improves speech quality. Experimental results demonstrate that\nSpotlight-TTS achieves superior performance compared to baseline models in\nterms of expressiveness, overall speech quality, and style transfer capability.\nOur audio samples are publicly available.", "AI": {"tldr": "Spotlight-TTS\u901a\u8fc7\u4e13\u6ce8\u4e8e\u8bed\u97f3\u98ce\u683c\u63d0\u53d6\u548c\u8c03\u6574\uff0c\u63d0\u5347\u4e86\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u98ce\u683c\u5d4c\u5165\u7684TTS\u65b9\u6cd5\u5728\u5408\u6210\u9ad8\u8d28\u91cf\u8868\u8fbe\u6027\u8bed\u97f3\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86Spotlight-TTS\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u97f3\u533a\u57df\u7684\u98ce\u683c\u63d0\u53d6\u548c\u98ce\u683c\u65b9\u5411\u8c03\u6574\u65b9\u6cd5\uff0c\u4f18\u5316TTS\u6a21\u578b\u4e2d\u7684\u98ce\u683c\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpotlight-TTS\u5728\u8868\u8fbe\u6027\u3001\u8bed\u97f3\u8d28\u91cf\u548c\u98ce\u683c\u8fc1\u79fb\u80fd\u529b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Spotlight-TTS\u901a\u8fc7\u4e13\u6ce8\u4e8e\u98ce\u683c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86TTS\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.20349", "pdf": "https://arxiv.org/pdf/2505.20349", "abs": "https://arxiv.org/abs/2505.20349", "authors": ["Haixin Wang", "Ruoyan Li", "Fred Xu", "Fang Sun", "Kaiqiao Han", "Zijie Huang", "Guancheng Wan", "Ching Chang", "Xiao Luo", "Wei Wang", "Yizhou Sun"], "title": "FD-Bench: A Modular and Fair Benchmark for Data-driven Fluid Simulation", "categories": ["physics.flu-dyn", "cs.LG"], "comment": "31 pages, 18 figures, paper under review", "summary": "Data-driven modeling of fluid dynamics has advanced rapidly with neural PDE\nsolvers, yet a fair and strong benchmark remains fragmented due to the absence\nof unified PDE datasets and standardized evaluation protocols. Although\narchitectural innovations are abundant, fair assessment is further impeded by\nthe lack of clear disentanglement between spatial, temporal and loss modules.\nIn this paper, we introduce FD-Bench, the first fair, modular, comprehensive\nand reproducible benchmark for data-driven fluid simulation. FD-Bench\nsystematically evaluates 85 baseline models across 10 representative flow\nscenarios under a unified experimental setup. It provides four key\ncontributions: (1) a modular design enabling fair comparisons across spatial,\ntemporal, and loss function modules; (2) the first systematic framework for\ndirect comparison with traditional numerical solvers; (3) fine-grained\ngeneralization analysis across resolutions, initial conditions, and temporal\nwindows; and (4) a user-friendly, extensible codebase to support future\nresearch. Through rigorous empirical studies, FD-Bench establishes the most\ncomprehensive leaderboard to date, resolving long-standing issues in\nreproducibility and comparability, and laying a foundation for robust\nevaluation of future data-driven fluid models. The code is open-sourced at\nhttps://anonymous.4open.science/r/FD-Bench-15BC.", "AI": {"tldr": "FD-Bench\u662f\u4e00\u4e2a\u516c\u5e73\u3001\u6a21\u5757\u5316\u3001\u5168\u9762\u7684\u6d41\u4f53\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7edf\u4e00\u5b9e\u9a8c\u8bbe\u7f6e\u8bc4\u4f3085\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9a71\u52a8\u6d41\u4f53\u6a21\u578b\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6bd4\u6027\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7edf\u4e00\u7684PDE\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u6570\u636e\u9a71\u52a8\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u516c\u5e73\u8bc4\u4f30\u53d7\u9650\u3002", "method": "\u5f15\u5165FD-Bench\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u516c\u5e73\u6bd4\u8f83\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u635f\u5931\u51fd\u6570\u6a21\u5757\uff0c\u7cfb\u7edf\u8bc4\u4f3010\u79cd\u6d41\u573a\u666f\u4e0b\u768485\u4e2a\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u6700\u5168\u9762\u7684\u6392\u884c\u699c\uff0c\u89e3\u51b3\u4e86\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6bd4\u6027\u95ee\u9898\u3002", "conclusion": "FD-Bench\u4e3a\u672a\u6765\u6570\u636e\u9a71\u52a8\u6d41\u4f53\u6a21\u578b\u7684\u7a33\u5065\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.20351", "pdf": "https://arxiv.org/pdf/2505.20351", "abs": "https://arxiv.org/abs/2505.20351", "authors": ["Tomer Shoham", "Katrina Ligettt"], "title": "Differentially private ratio statistics", "categories": ["stat.ML", "cs.LG"], "comment": "32 pages, 3 figures, under review", "summary": "Ratio statistics--such as relative risk and odds ratios--play a central role\nin hypothesis testing, model evaluation, and decision-making across many areas\nof machine learning, including causal inference and fairness analysis. However,\ndespite privacy concerns surrounding many datasets and despite increasing\nadoption of differential privacy, differentially private ratio statistics have\nlargely been neglected by the literature and have only recently received an\ninitial treatment by Lin et al. [1]. This paper attempts to fill this lacuna,\ngiving results that can guide practice in evaluating ratios when the results\nmust be protected by differential privacy. In particular, we show that even a\nsimple algorithm can provide excellent properties concerning privacy, sample\naccuracy, and bias, not just asymptotically but also at quite small sample\nsizes. Additionally, we analyze a differentially private estimator for relative\nrisk, prove its consistency, and develop a method for constructing valid\nconfidence intervals. Our approach bridges a gap in the differential privacy\nliterature and provides a practical solution for ratio estimation in private\nmachine learning pipelines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u6bd4\u7387\u7edf\u8ba1\uff08\u5982\u76f8\u5bf9\u98ce\u9669\u548c\u6bd4\u503c\u6bd4\uff09\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u573a\u666f\u3002", "motivation": "\u5c3d\u7ba1\u6bd4\u7387\u7edf\u8ba1\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff08\u5982\u56e0\u679c\u63a8\u65ad\u548c\u516c\u5e73\u6027\u5206\u6790\uff09\uff0c\u4f46\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u6bd4\u7387\u7edf\u8ba1\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u9690\u79c1\u6027\u3001\u6837\u672c\u51c6\u786e\u6027\u548c\u504f\u5dee\u7279\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u6784\u9020\u6709\u6548\u7f6e\u4fe1\u533a\u95f4\u7684\u65b9\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u5c0f\u6837\u672c\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u5dee\u5206\u9690\u79c1\u76f8\u5bf9\u98ce\u9669\u4f30\u8ba1\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7f6e\u4fe1\u533a\u95f4\u6784\u9020\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u5dee\u5206\u9690\u79c1\u6bd4\u7387\u7edf\u8ba1\u7684\u7a7a\u767d\uff0c\u4e3a\u79c1\u6709\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u4e2d\u7684\u6bd4\u7387\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.20638", "pdf": "https://arxiv.org/pdf/2505.20638", "abs": "https://arxiv.org/abs/2505.20638", "authors": ["Wenhao You", "Xingjian Diao", "Chunhui Zhang", "Keyi Kong", "Weiyi Wu", "Zhongyu Ouyang", "Chiyu Ma", "Tingxuan Wu", "Noah Wei", "Zong Ke", "Ming Cheng", "Soroush Vosoughi", "Jiang Gui"], "title": "Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "While recent Multimodal Large Language Models exhibit impressive capabilities\nfor general multimodal tasks, specialized domains like music necessitate\ntailored approaches. Music Audio-Visual Question Answering (Music AVQA)\nparticularly underscores this, presenting unique challenges with its\ncontinuous, densely layered audio-visual content, intricate temporal dynamics,\nand the critical need for domain-specific knowledge. Through a systematic\nanalysis of Music AVQA datasets and methods, this position paper identifies\nthat specialized input processing, architectures incorporating dedicated\nspatial-temporal designs, and music-specific modeling strategies are critical\nfor success in this domain. Our study provides valuable insights for\nresearchers by highlighting effective design patterns empirically linked to\nstrong performance, proposing concrete future directions for incorporating\nmusical priors, and aiming to establish a robust foundation for advancing\nmultimodal musical understanding. This work is intended to inspire broader\nattention and further research, supported by a continuously updated anonymous\nGitHub repository of relevant papers:\nhttps://github.com/xid32/Survey4MusicAVQA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u97f3\u4e50\u97f3\u9891-\u89c6\u89c9\u95ee\u7b54\uff08Music AVQA\uff09\u9886\u57df\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u7684\u8f93\u5165\u5904\u7406\u3001\u65f6\u7a7a\u67b6\u6784\u8bbe\u8ba1\u548c\u97f3\u4e50\u7279\u5b9a\u5efa\u6a21\u7b56\u7565\u3002", "motivation": "\u97f3\u4e50\u9886\u57df\u7684\u591a\u6a21\u6001\u4efb\u52a1\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\uff0c\u800cMusic AVQA\u56e0\u5176\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u6001\u548c\u9886\u57df\u77e5\u8bc6\u9700\u6c42\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790Music AVQA\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e13\u7528\u8f93\u5165\u5904\u7406\u3001\u65f6\u7a7a\u67b6\u6784\u8bbe\u8ba1\u548c\u97f3\u4e50\u7279\u5b9a\u5efa\u6a21\u7b56\u7565\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86\u4e0e\u9ad8\u6027\u80fd\u76f8\u5173\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6574\u5408\u97f3\u4e50\u5148\u9a8c\u7684\u5177\u4f53\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u591a\u6a21\u6001\u97f3\u4e50\u7406\u89e3\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u6fc0\u53d1\u66f4\u591a\u5173\u6ce8\u548c\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.20361", "pdf": "https://arxiv.org/pdf/2505.20361", "abs": "https://arxiv.org/abs/2505.20361", "authors": ["Chuanxing Wang", "Hui Luo", "Kai Wang", "Guohuai Zhu", "Mingxing Luo"], "title": "Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks", "categories": ["physics.flu-dyn", "cs.LG"], "comment": null, "summary": "Despite the remarkable progress of physics-informed neural networks (PINNs)\nin scientific computing, they continue to face challenges when solving\nhydrodynamic problems with multiple discontinuities. In this work, we propose\nSeparation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such\nproblems. By sequentially resolving discontinuities from strong to weak and\nleveraging transfer learning during training, ST-PINNs significantly reduce the\nproblem complexity and enhance solution accuracy. To the best of our knowledge,\nthis is the first study to apply a PINNs-based approach to the two-dimensional\nunsteady planar shock refraction problem, offering new insights into the\napplication of PINNs to complex shock-interface interactions. Numerical\nexperiments demonstrate that ST-PINNs more accurately capture sharp\ndiscontinuities and substantially reduce solution errors in hydrodynamic\nproblems involving multiple discontinuities.", "AI": {"tldr": "\u63d0\u51fa\u4e86ST-PINNs\u65b9\u6cd5\uff0c\u901a\u8fc7\u987a\u5e8f\u89e3\u51b3\u4e0d\u8fde\u7eed\u6027\u548c\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347PINNs\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3PINNs\u5728\u5904\u7406\u591a\u4e0d\u8fde\u7eed\u6027\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u65f6\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u987a\u5e8f\u89e3\u51b3\u4e0d\u8fde\u7eed\u6027\u548c\u8fc1\u79fb\u5b66\u4e60\u7684ST-PINNs\u65b9\u6cd5\u3002", "result": "ST-PINNs\u663e\u8457\u63d0\u9ad8\u4e86\u6355\u6349\u4e0d\u8fde\u7eed\u6027\u7684\u51c6\u786e\u6027\u548c\u51cf\u5c11\u4e86\u8bef\u5dee\u3002", "conclusion": "ST-PINNs\u4e3a\u590d\u6742\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20688", "pdf": "https://arxiv.org/pdf/2505.20688", "abs": "https://arxiv.org/abs/2505.20688", "authors": ["Taehyo Kim", "Qiran Jia", "Mony J. de Leon", "Hai Shu"], "title": "A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "False discovery rate (FDR) control methods are essential for voxel-wise\nmultiple testing in neuroimaging data analysis, where hundreds of thousands or\neven millions of tests are conducted to detect brain regions associated with\ndisease-related changes. Classical FDR control methods (e.g., BH, q-value, and\nLocalFDR) assume independence among tests and often lead to high false\nnon-discovery rates (FNR). Although various spatial FDR control methods have\nbeen developed to improve power, they still fall short in jointly addressing\nthree major challenges in neuroimaging applications: capturing complex spatial\ndependencies, maintaining low variability in both false discovery proportion\n(FDP) and false non-discovery proportion (FNP) across replications, and\nachieving computational scalability for high-resolution data. To address these\nchallenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR\ncontrol method for voxel-wise multiple testing. It integrates the local index\nof significance (LIS)-based testing procedure with a novel fully connected\nhidden Markov random field (fcHMRF) designed to model complex spatial\nstructures using a parsimonious parameterization. We develop an efficient\nexpectation-maximization algorithm incorporating mean-field approximation, the\nConditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and\npermutohedral lattice filtering, reducing the computational complexity from\nquadratic to linear in the number of tests. Extensive simulations demonstrate\nthat fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability\nin FDP and FNP, and a higher number of true positives compared to existing\nmethods. Applied to an FDG-PET dataset from the Alzheimer's Disease\nNeuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain\nregions and offers notable advantages in computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3afcHMRF-LIS\u7684\u7a7a\u95f4FDR\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u4e2d\u7684\u591a\u91cd\u68c0\u9a8c\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u6027\u3001FDP\u548cFNP\u7684\u7a33\u5b9a\u6027\u4ee5\u53ca\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u5206\u6790\u4e2d\uff0c\u4f20\u7edf\u7684FDR\u63a7\u5236\u65b9\u6cd5\u5047\u8bbe\u68c0\u9a8c\u72ec\u7acb\uff0c\u5bfc\u81f4\u9ad8FNR\uff0c\u4e14\u73b0\u6709\u7a7a\u95f4FDR\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u89e3\u51b3\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u6027\u3001FDP/FNP\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\u663e\u8457\u6027\u6307\u6570\uff08LIS\uff09\u548c\u65b0\u578b\u5168\u8fde\u63a5\u9690\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff08fcHMRF\uff09\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "fcHMRF-LIS\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u51c6\u786e\u7684FDR\u63a7\u5236\u3001\u66f4\u4f4e\u7684FNR\u3001\u66f4\u7a33\u5b9a\u7684FDP/FNP\uff0c\u4ee5\u53ca\u66f4\u9ad8\u7684\u771f\u9633\u6027\u7387\u3002\u5e94\u7528\u4e8eADNI\u6570\u636e\u96c6\u65f6\uff0c\u8bc6\u522b\u51fa\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76f8\u5173\u7684\u8111\u533a\u3002", "conclusion": "fcHMRF-LIS\u662f\u4e00\u79cd\u5f3a\u5927\u3001\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u7684\u7a7a\u95f4FDR\u63a7\u5236\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u3002", "relevance": 30.0}}
{"id": "2505.20367", "pdf": "https://arxiv.org/pdf/2505.20367", "abs": "https://arxiv.org/abs/2505.20367", "authors": ["Sen Yan", "Fabrizio Gabellieri", "Etienne Goffinet", "Filippo Castiglione", "Thomas Launey"], "title": "DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization\nto probe molecules' chemical environment, structure, and dynamics, with\napplications spanning from pharmaceuticals to the petroleum industry. Despite\nits utility, the high cost of NMR instrumentation, operation and the lengthy\nduration of experiments necessitate the development of computational techniques\nto optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as\na sub-sampling method to address these challenges, but it often introduces\nartifacts and degrades spectral quality, offsetting the benefits of reduced\nacquisition times. In this work, we propose the use of deep learning techniques\nto enhance the reconstruction quality of NUS spectra. Specifically, we explore\nthe application of diffusion models, a relatively untapped approach in this\ndomain. Our methodology involves applying diffusion models to both time-time\nand time-frequency NUS data, yielding satisfactory reconstructions of\nchallenging spectra from the benchmark Artina dataset. This approach\ndemonstrates the potential of diffusion models to improve the efficiency and\naccuracy of NMR spectroscopy as well as the superiority of using a\ntime-frequency domain data over the time-time one, opening new landscapes for\nfuture studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u6a21\u578b\u63d0\u5347\u975e\u5747\u5300\u91c7\u6837\uff08NUS\uff09\u6838\u78c1\u5171\u632f\uff08NMR\uff09\u5149\u8c31\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u5728\u65f6\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u9891\u7387\u6570\u636e\u4e0a\u7684\u5e94\u7528\u6548\u679c\u3002", "motivation": "NMR\u5149\u8c31\u7684\u9ad8\u6210\u672c\u548c\u5b9e\u9a8c\u65f6\u95f4\u957f\u4fc3\u4f7f\u5f00\u53d1\u8ba1\u7b97\u6280\u672f\u4f18\u5316\u91c7\u96c6\u65f6\u95f4\uff0c\u4f46NUS\u65b9\u6cd5\u5e38\u5f15\u5165\u4f2a\u5f71\u5e76\u964d\u4f4e\u5149\u8c31\u8d28\u91cf\u3002", "method": "\u5e94\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u65f6\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u9891\u7387NUS\u6570\u636e\uff0c\u4f7f\u7528Artina\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\u3002", "result": "\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86NUS\u5149\u8c31\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u65f6\u95f4-\u9891\u7387\u57df\u6570\u636e\u8868\u73b0\u4f18\u4e8e\u65f6\u95f4-\u65f6\u95f4\u57df\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3aNMR\u5149\u8c31\u7684\u9ad8\u6548\u548c\u51c6\u786e\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u65f6\u95f4-\u9891\u7387\u57df\u6570\u636e\u66f4\u5177\u6f5c\u529b\u3002", "relevance": 20.0}}
{"id": "2505.20630", "pdf": "https://arxiv.org/pdf/2505.20630", "abs": "https://arxiv.org/abs/2505.20630", "authors": ["Yansong Li", "Paula Branco", "Alexander M. Hoole", "Manish Marwah", "Hari Manassery Koduvely", "Guy-Vincent Jourdan", "Stephan Jou"], "title": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) evolve in understanding and generating code,\naccurately evaluating their reliability in analyzing source code\nvulnerabilities becomes increasingly vital. While studies have examined LLM\ncapabilities in tasks like vulnerability detection and repair, they often\noverlook the importance of both structure and semantic reasoning crucial for\ntrustworthy vulnerability analysis. To address this gap, we introduce\nSV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for\nvulnerability analysis of code written in the C programming language through\ntwo key dimensions: structure reasoning - assessing how models identify\nrelationships between code elements under varying data and control flow\ncomplexities; and semantic reasoning - examining their logical consistency in\nscenarios where code is structurally and semantically perturbed. Our results\nshow that current LLMs are far from satisfactory in understanding complex code\nrelationships and that their vulnerability analyses rely more on pattern\nmatching than on robust logical reasoning. These findings underscore the\neffectiveness of the SV-TrustEval-C benchmark and highlight critical areas for\nenhancing the reasoning capabilities and trustworthiness of LLMs in real-world\nvulnerability analysis tasks. Our initial benchmark dataset is publicly\navailable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SV-TrustEval-C\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728C\u8bed\u8a00\u4ee3\u7801\u6f0f\u6d1e\u5206\u6790\u4e2d\u7684\u7ed3\u6784\u63a8\u7406\u548c\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f9d\u8d56\u6a21\u5f0f\u5339\u914d\u800c\u975e\u903b\u8f91\u63a8\u7406\u3002", "motivation": "\u968f\u7740LLMs\u5728\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u8bc4\u4f30\u5176\u5728\u6e90\u4ee3\u7801\u6f0f\u6d1e\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u89c6\u7ed3\u6784\u548c\u8bed\u4e49\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002", "method": "\u5f15\u5165SV-TrustEval-C\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u6784\u63a8\u7406\uff08\u4ee3\u7801\u5143\u7d20\u5173\u7cfb\uff09\u548c\u8bed\u4e49\u63a8\u7406\uff08\u903b\u8f91\u4e00\u81f4\u6027\uff09\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLMs\u3002", "result": "\u5f53\u524dLLMs\u5728\u590d\u6742\u4ee3\u7801\u5173\u7cfb\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6f0f\u6d1e\u5206\u6790\u66f4\u591a\u4f9d\u8d56\u6a21\u5f0f\u5339\u914d\u800c\u975e\u903b\u8f91\u63a8\u7406\u3002", "conclusion": "SV-TrustEval-C\u57fa\u51c6\u6709\u6548\u63ed\u793a\u4e86LLMs\u5728\u6f0f\u6d1e\u5206\u6790\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.20370", "pdf": "https://arxiv.org/pdf/2505.20370", "abs": "https://arxiv.org/abs/2505.20370", "authors": ["Martine Dyring Hansen", "Elena Celledoni", "Benjamin Kwanen Tapley"], "title": "Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "We introduce a data-driven method for learning the equations of motion of\nmechanical systems directly from position measurements, without requiring\naccess to velocity data. This is particularly relevant in system identification\ntasks where only positional information is available, such as motion capture,\npixel data or low-resolution tracking. Our approach takes advantage of the\ndiscrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange\nequations to construct a physically grounded model of the system's dynamics. We\ndecompose the dynamics into conservative and non-conservative components, which\nare learned separately using feed-forward neural networks. In the absence of\nexternal forces, our method reduces to a variational discretization of the\naction principle naturally preserving the symplectic structure of the\nunderlying Hamiltonian system. We validate our approach on a variety of\nsynthetic and real-world datasets, demonstrating its effectiveness compared to\nbaseline methods. In particular, we apply our model to (1) measured human\nmotion data and (2) latent embeddings obtained via an autoencoder trained on\nimage sequences. We demonstrate that we can faithfully reconstruct and separate\nboth the conservative and forced dynamics, yielding interpretable and\nphysically consistent predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u4f4d\u7f6e\u6d4b\u91cf\u4e2d\u5b66\u4e60\u673a\u68b0\u7cfb\u7edf\u7684\u8fd0\u52a8\u65b9\u7a0b\uff0c\u65e0\u9700\u901f\u5ea6\u6570\u636e\u3002", "motivation": "\u5728\u7cfb\u7edf\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u901a\u5e38\u53ea\u80fd\u83b7\u53d6\u4f4d\u7f6e\u4fe1\u606f\uff08\u5982\u8fd0\u52a8\u6355\u6349\u3001\u50cf\u7d20\u6570\u636e\u6216\u4f4e\u5206\u8fa8\u7387\u8ddf\u8e2a\uff09\uff0c\u800c\u901f\u5ea6\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5229\u7528\u79bb\u6563\u7684Lagrange-d'Alembert\u539f\u7406\u548c\u53d7\u8feb\u79bb\u6563Euler-Lagrange\u65b9\u7a0b\u6784\u5efa\u7269\u7406\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u52a8\u529b\u5b66\u5206\u89e3\u4e3a\u4fdd\u5b88\u548c\u975e\u4fdd\u5b88\u90e8\u5206\uff0c\u5206\u522b\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u91cd\u5efa\u548c\u5206\u79bb\u4fdd\u5b88\u4e0e\u53d7\u8feb\u52a8\u529b\u5b66\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f9d\u8d56\u4f4d\u7f6e\u6570\u636e\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u8fd0\u52a8\u6570\u636e\u548c\u56fe\u50cf\u5e8f\u5217\u7684\u6f5c\u5728\u5d4c\u5165\u3002", "relevance": 20.0}}
{"id": "2505.20746", "pdf": "https://arxiv.org/pdf/2505.20746", "abs": "https://arxiv.org/abs/2505.20746", "authors": ["Nikola Andrejic", "Milica Spasic", "Igor Mihajlovic", "Petra Milosavljevic", "Djordje Pavlovic", "Filip Milisavljevic", "Uros Milivojevic", "Danilo Delibasic", "Ivana Mikic", "Sinisa Todorovic"], "title": "Unpaired Image-to-Image Translation for Segmentation and Signal Unmixing", "categories": ["eess.IV", "cs.CV"], "comment": "submitted to NeurIPs 2025", "summary": "This work introduces Ui2i, a novel model for unpaired image-to-image\ntranslation, trained on content-wise unpaired datasets to enable style transfer\nacross domains while preserving content. Building on CycleGAN, Ui2i\nincorporates key modifications to better disentangle content and style\nfeatures, and preserve content integrity. Specifically, Ui2i employs\nU-Net-based generators with skip connections to propagate localized shallow\nfeatures deep into the generator. Ui2i removes feature-based normalization\nlayers from all modules and replaces them with approximate bidirectional\nspectral normalization -- a parameter-based alternative that enhances training\nstability. To further support content preservation, channel and spatial\nattention mechanisms are integrated into the generators. Training is\nfacilitated through image scale augmentation. Evaluation on two biomedical\ntasks -- domain adaptation for nuclear segmentation in immunohistochemistry\n(IHC) images and unmixing of biological structures superimposed in\nsingle-channel immunofluorescence (IF) images -- demonstrates Ui2i's ability to\npreserve content fidelity in settings that demand more accurate structural\npreservation than typical translation tasks. To the best of our knowledge, Ui2i\nis the first approach capable of separating superimposed signals in IF images\nusing real, unpaired training data.", "AI": {"tldr": "Ui2i\u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdbCycleGAN\u5b9e\u73b0\u8de8\u57df\u98ce\u683c\u8f6c\u6362\u5e76\u4fdd\u7559\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u914d\u5bf9\u6570\u636e\u4e0b\u7684\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u5185\u5bb9\u4fdd\u771f\u5ea6\u7684\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u3002", "method": "\u91c7\u7528U-Net\u751f\u6210\u5668\u3001\u8fd1\u4f3c\u53cc\u5411\u8c31\u5f52\u4e00\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u56fe\u50cf\u5c3a\u5ea6\u589e\u5f3a\u8bad\u7ec3\u3002", "result": "\u5728\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u914d\u5bf9\u6570\u636e\u4e0b\u5206\u79bb\u53e0\u52a0\u4fe1\u53f7\u3002", "conclusion": "Ui2i\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u8981\u6c42\u9ad8\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u521b\u65b0\u6027\u3002", "relevance": 30.0}}
{"id": "2505.20433", "pdf": "https://arxiv.org/pdf/2505.20433", "abs": "https://arxiv.org/abs/2505.20433", "authors": ["Masha Naslidnyk", "Siu Lun Chau", "Fran\u00e7ois-Xavier Briol", "Krikamol Muandet"], "title": "Kernel Quantile Embeddings and Associated Probability Metrics", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Embedding probability distributions into reproducing kernel Hilbert spaces\n(RKHS) has enabled powerful nonparametric methods such as the maximum mean\ndiscrepancy (MMD), a statistical distance with strong theoretical and\ncomputational properties. At its core, the MMD relies on kernel mean embeddings\nto represent distributions as mean functions in RKHS. However, it remains\nunclear if the mean function is the only meaningful RKHS representation.\nInspired by generalised quantiles, we introduce the notion of kernel quantile\nembeddings (KQEs). We then use KQEs to construct a family of distances that:\n(i) are probability metrics under weaker kernel conditions than MMD; (ii)\nrecover a kernelised form of the sliced Wasserstein distance; and (iii) can be\nefficiently estimated with near-linear cost. Through hypothesis testing, we\nshow that these distances offer a competitive alternative to MMD and its fast\napproximations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6838\u5206\u4f4d\u6570\u5d4c\u5165\uff08KQEs\uff09\u7684\u6982\u5ff5\uff0c\u6269\u5c55\u4e86\u6838\u5747\u503c\u5d4c\u5165\uff08MMD\uff09\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u7c7b\u65b0\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5177\u6709\u66f4\u5f31\u7684\u6838\u6761\u4ef6\u3001\u9ad8\u6548\u8ba1\u7b97\u548c\u7ade\u4e89\u6027\u7684\u5047\u8bbe\u6d4b\u8bd5\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u6838\u5747\u503c\u5d4c\u5165\uff08MMD\uff09\u4e4b\u5916\u7684RKHS\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5206\u5e03\u8868\u793a\u548c\u8ddd\u79bb\u5ea6\u91cf\u7684\u7075\u6d3b\u6027\u4e0e\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u6838\u5206\u4f4d\u6570\u5d4c\u5165\uff08KQEs\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u65b0\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u8fd9\u4e9b\u5ea6\u91cf\u5728\u8f83\u5f31\u7684\u6838\u6761\u4ef6\u4e0b\u4ecd\u4e3a\u6982\u7387\u5ea6\u91cf\uff0c\u5e76\u80fd\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cKQEs\u6784\u5efa\u7684\u8ddd\u79bb\u5728\u5047\u8bbe\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eMMD\u53ca\u5176\u5feb\u901f\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u63a5\u8fd1\u7ebf\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662fKQEs\u4e3a\u5206\u5e03\u8868\u793a\u548c\u8ddd\u79bb\u5ea6\u91cf\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u662fMMD\u7684\u6709\u529b\u8865\u5145\u3002", "relevance": 40.0}}
{"id": "2505.20456", "pdf": "https://arxiv.org/pdf/2505.20456", "abs": "https://arxiv.org/abs/2505.20456", "authors": ["Rafael Valente da Silva", "Onel L. Alcaraz L\u00f3pez", "Richard Demo Souza"], "title": "Federated Learning-Distillation Alternation for Resource-Constrained IoT", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Federated learning (FL) faces significant challenges in Internet of Things\n(IoT) networks due to device limitations in energy and communication resources,\nespecially when considering the large size of FL models. From an energy\nperspective, the challenge is aggravated if devices rely on energy harvesting\n(EH), as energy availability can vary significantly over time, influencing the\naverage number of participating users in each iteration. Additionally, the\ntransmission of large model updates is more susceptible to interference from\nuncorrelated background traffic in shared wireless environments. As an\nalternative, federated distillation (FD) reduces communication overhead and\nenergy consumption by transmitting local model outputs, which are typically\nmuch smaller than the entire model used in FL. However, this comes at the cost\nof reduced model accuracy. Therefore, in this paper, we propose FL-distillation\nalternation (FLDA). In FLDA, devices alternate between FD and FL phases,\nbalancing model information with lower communication overhead and energy\nconsumption per iteration. We consider a multichannel slotted-ALOHA EH-IoT\nnetwork subject to background traffic/interference. In such a scenario, FLDA\ndemonstrates higher model accuracy than both FL and FD, and achieves faster\nconvergence than FL. Moreover, FLDA achieves target accuracies saving up to 98%\nin energy consumption, while also being less sensitive to interference, both\nrelative to FL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFLDA\u65b9\u6cd5\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u548c\u8054\u90a6\u84b8\u998f\u4e4b\u95f4\u4ea4\u66ff\uff0c\u4ee5\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u5f00\u9500\u548c\u80fd\u8017\u3002", "motivation": "\u89e3\u51b3\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u8054\u90a6\u5b66\u4e60\u56e0\u8bbe\u5907\u8d44\u6e90\u9650\u5236\uff08\u5982\u80fd\u91cf\u548c\u901a\u4fe1\uff09\u548c\u80cc\u666f\u5e72\u6270\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faFL-distillation alternation (FLDA)\uff0c\u8bbe\u5907\u5728FD\u548cFL\u9636\u6bb5\u4ea4\u66ff\uff0c\u4f18\u5316\u901a\u4fe1\u548c\u80fd\u8017\u3002", "result": "FLDA\u5728\u6a21\u578b\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8eFL\u548cFD\uff0c\u80fd\u8017\u8282\u7701\u9ad8\u8fbe98%\uff0c\u4e14\u5bf9\u5e72\u6270\u66f4\u4e0d\u654f\u611f\u3002", "conclusion": "FLDA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u7f51\u7edc\u3002", "relevance": 40.0}}
{"id": "2505.20810", "pdf": "https://arxiv.org/pdf/2505.20810", "abs": "https://arxiv.org/abs/2505.20810", "authors": ["Tariq M Khan", "Toufique Ahmed Soomro", "Imran Razzak"], "title": "The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Retinal imaging has emerged as a powerful, non-invasive modality for\ndetecting and quantifying biomarkers of systemic diseases-ranging from diabetes\nand hypertension to Alzheimer's disease and cardiovascular disorders but\ncurrent insights remain dispersed across platforms and specialties. Recent\ntechnological advances in optical coherence tomography (OCT/OCTA) and adaptive\noptics (AO) now deliver ultra-high-resolution scans (down to 5 {\\mu}m ) with\nsuperior contrast and spatial integration, allowing early identification of\nmicrovascular abnormalities and neurodegenerative changes. At the same time,\nAI-driven and machine learning (ML) algorithms have revolutionized the analysis\nof large-scale retinal datasets, increasing sensitivity and specificity; for\nexample, deep learning models achieve > 90 \\% sensitivity for diabetic\nretinopathy and AUC = 0.89 for the prediction of cardiovascular risk from\nfundus photographs. The proliferation of mobile health technologies and\ntelemedicine platforms further extends access, reduces costs, and facilitates\ncommunity-based screening and longitudinal monitoring. Despite these\nbreakthroughs, translation into routine practice is hindered by heterogeneous\nimaging protocols, limited external validation of AI models, and integration\nchallenges within clinical workflows. In this review, we systematically\nsynthesize the latest OCT/OCT and AO developments, AI/ML approaches, and\nmHealth/Tele-ophthalmology initiatives and quantify their diagnostic\nperformance across disease domains. Finally, we propose a roadmap for\nmulticenter protocol standardization, prospective validation trials, and\nseamless incorporation of retinal screening into primary and specialty care\npathways-paving the way for precision prevention, early intervention, and\nongoing treatment of life-threatening systemic diseases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u89c6\u7f51\u819c\u6210\u50cf\u6280\u672f\u5728\u7cfb\u7edf\u6027\u75be\u75c5\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ecOCT/OCTA\u548cAO\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u3001AI/ML\u7b97\u6cd5\u7684\u5206\u6790\u6027\u80fd\uff0c\u4ee5\u53ca\u79fb\u52a8\u5065\u5eb7\u6280\u672f\u7684\u63a8\u5e7f\u3002\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u4e34\u5e8a\u8f6c\u5316\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u548c\u9a8c\u8bc1\u7684\u8def\u7ebf\u56fe\u3002", "motivation": "\u89c6\u7f51\u819c\u6210\u50cf\u6280\u672f\u4e3a\u975e\u4fb5\u5165\u6027\u68c0\u6d4b\u7cfb\u7edf\u6027\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u5f53\u524d\u6280\u672f\u5206\u6563\u4e14\u4e34\u5e8a\u8f6c\u5316\u9762\u4e34\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u7cfb\u7edf\u7efc\u8ff0\u6700\u65b0\u6280\u672f\u8fdb\u5c55\u5e76\u63a8\u52a8\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u7efc\u8ff0\u4e86OCT/OCTA\u548cAO\u6280\u672f\u7684\u8fdb\u5c55\u3001AI/ML\u7b97\u6cd5\u5728\u89c6\u7f51\u819c\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u79fb\u52a8\u5065\u5eb7\u6280\u672f\u7684\u63a8\u5e7f\u3002", "result": "AI/ML\u7b97\u6cd5\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u548c\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\uff08>90%\u654f\u611f\u6027\u548cAUC=0.89\uff09\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u4e2d\u5fc3\u534f\u8bae\u6807\u51c6\u5316\u3001\u524d\u77bb\u6027\u9a8c\u8bc1\u8bd5\u9a8c\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u6574\u5408\u7684\u8def\u7ebf\u56fe\uff0c\u4ee5\u63a8\u52a8\u89c6\u7f51\u819c\u7b5b\u67e5\u5728\u7cbe\u51c6\u9884\u9632\u548c\u65e9\u671f\u5e72\u9884\u4e2d\u7684\u5e94\u7528\u3002", "relevance": 20.0}}
{"id": "2505.20465", "pdf": "https://arxiv.org/pdf/2505.20465", "abs": "https://arxiv.org/abs/2505.20465", "authors": ["Lorenzo Lucchese", "Mikko S. Pakkanen", "Almut E. D. Veraart"], "title": "Learning with Expected Signatures: Theory and Applications", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "The expected signature maps a collection of data streams to a lower\ndimensional representation, with a remarkable property: the resulting feature\ntensor can fully characterize the data generating distribution. This\n\"model-free\" embedding has been successfully leveraged to build multiple\ndomain-agnostic machine learning (ML) algorithms for time series and sequential\ndata. The convergence results proved in this paper bridge the gap between the\nexpected signature's empirical discrete-time estimator and its theoretical\ncontinuous-time value, allowing for a more complete probabilistic\ninterpretation of expected signature-based ML methods. Moreover, when the data\ngenerating process is a martingale, we suggest a simple modification of the\nexpected signature estimator with significantly lower mean squared error and\nempirically demonstrate how it can be effectively applied to improve predictive\nperformance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9884\u671f\u7b7e\u540d\uff08expected signature\uff09\u5728\u6570\u636e\u6d41\u964d\u7ef4\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u79bb\u6563\u65f6\u95f4\u4f30\u8ba1\u5668\u4e0e\u7406\u8bba\u8fde\u7eed\u65f6\u95f4\u503c\u4e4b\u95f4\u7684\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4f30\u8ba1\u5668\u4ee5\u964d\u4f4e\u5747\u65b9\u8bef\u5dee\u3002", "motivation": "\u9884\u671f\u7b7e\u540d\u80fd\u591f\u5c06\u6570\u636e\u6d41\u6620\u5c04\u4e3a\u4f4e\u7ef4\u8868\u793a\uff0c\u5e76\u5b8c\u5168\u8868\u5f81\u6570\u636e\u751f\u6210\u5206\u5e03\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u5176\u79bb\u6563\u65f6\u95f4\u4f30\u8ba1\u5668\u4e0e\u7406\u8bba\u8fde\u7eed\u65f6\u95f4\u503c\u4e4b\u95f4\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u66f4\u5b8c\u6574\u7684\u6982\u7387\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u9884\u671f\u7b7e\u540d\u79bb\u6563\u65f6\u95f4\u4f30\u8ba1\u5668\u7684\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9785\u8fc7\u7a0b\u7684\u6539\u8fdb\u4f30\u8ba1\u5668\u3002", "result": "\u6539\u8fdb\u7684\u4f30\u8ba1\u5668\u663e\u8457\u964d\u4f4e\u4e86\u5747\u65b9\u8bef\u5dee\uff0c\u5e76\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u9884\u671f\u7b7e\u540d\u53ca\u5176\u6539\u8fdb\u4f30\u8ba1\u5668\u4e3a\u65f6\u95f4\u5e8f\u5217\u548c\u5e8f\u5217\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.20814", "pdf": "https://arxiv.org/pdf/2505.20814", "abs": "https://arxiv.org/abs/2505.20814", "authors": ["Yiqi Huang", "Travis Davies", "Jiahuan Yan", "Jiankai Sun", "Xiang Chen", "Luhui Hu"], "title": "Spatial RoboGrasp: Generalized Robotic Grasping Control Policy", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Achieving generalizable and precise robotic manipulation across diverse\nenvironments remains a critical challenge, largely due to limitations in\nspatial perception. While prior imitation-learning approaches have made\nprogress, their reliance on raw RGB inputs and handcrafted features often leads\nto overfitting and poor 3D reasoning under varied lighting, occlusion, and\nobject conditions. In this paper, we propose a unified framework that couples\nrobust multimodal perception with reliable grasp prediction. Our architecture\nfuses domain-randomized augmentation, monocular depth estimation, and a\ndepth-aware 6-DoF Grasp Prompt into a single spatial representation for\ndownstream action planning. Conditioned on this encoding and a high-level task\nprompt, our diffusion-based policy yields precise action sequences, achieving\nup to 40% improvement in grasp success and 45% higher task success rates under\nenvironmental variation. These results demonstrate that spatially grounded\nperception, paired with diffusion-based imitation learning, offers a scalable\nand robust solution for general-purpose robotic grasping.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u6269\u6563\u7b56\u7565\u7684\u673a\u5668\u4eba\u6293\u53d6\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u56e0\u4f9d\u8d56RGB\u8f93\u5165\u548c\u624b\u5de5\u7279\u5f81\u5bfc\u81f4\u7684\u6cdb\u5316\u6027\u548c3D\u63a8\u7406\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u878d\u5408\u9886\u57df\u968f\u673a\u589e\u5f3a\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u6df1\u5ea6\u611f\u77e5\u76846-DoF\u6293\u53d6\u63d0\u793a\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u6293\u53d6\u6210\u529f\u7387\u63d0\u534740%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534745%\u3002", "conclusion": "\u7a7a\u95f4\u611f\u77e5\u4e0e\u6269\u6563\u6a21\u4eff\u5b66\u4e60\u7ed3\u5408\u4e3a\u901a\u7528\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20902", "pdf": "https://arxiv.org/pdf/2505.20902", "abs": "https://arxiv.org/abs/2505.20902", "authors": ["Ruiying Li", "Bin Pan", "Lan Ma", "Xia Xu", "Zhenwei Shi"], "title": "Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing", "categories": ["eess.IV", "cs.CV", "68T07", "I.4.10"], "comment": "11 Pages,8 figures", "summary": "Multitemporal hyperspectral unmixing can capture dynamical evolution of\nmaterials. Despite its capability, current methods emphasize variability of\nendmembers while neglecting dynamics of abundances, which motivates our\nadoption of neural ordinary differential equations to model abundances\ntemporally. However, this motivation is hindered by two challenges: the\ninherent complexity in defining, modeling and solving problem, and the absence\nof theoretical support. To address above challenges, in this paper, we propose\na multitemporal latent dynamical (MiLD) unmixing framework by capturing\ndynamical evolution of materials with theoretical validation. For addressing\nmultitemporal hyperspectral unmixing, MiLD consists of problem definition,\nmathematical modeling, solution algorithm and theoretical support. We formulate\nmultitemporal unmixing problem definition by conducting ordinary differential\nequations and developing latent variables. We transfer multitemporal unmixing\nto mathematical model by dynamical discretization approaches, which describe\nthe discreteness of observed sequence images with mathematical expansions. We\npropose algorithm to solve problem and capture dynamics of materials, which\napproximates abundance evolution by neural networks. Furthermore, we provide\ntheoretical support by validating the crucial properties, which verifies\nconsistency, convergence and stability theorems. The major contributions of\nMiLD include defining problem by ordinary differential equations, modeling\nproblem by dynamical discretization approach, solving problem by multitemporal\nunmixing algorithm, and presenting theoretical support. Our experiments on both\nsynthetic and real datasets have validated the utility of our work", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65f6\u76f8\u9ad8\u5149\u8c31\u89e3\u6df7\u6846\u67b6\uff08MiLD\uff09\uff0c\u901a\u8fc7\u795e\u7ecfODE\u5efa\u6a21\u6750\u6599\u7684\u65f6\u95f4\u52a8\u6001\u53d8\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e30\u5ea6\u52a8\u6001\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u591a\u65f6\u76f8\u9ad8\u5149\u8c31\u89e3\u6df7\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7aef\u5143\u7684\u53d8\u5316\uff0c\u800c\u5ffd\u7565\u4e86\u4e30\u5ea6\u7684\u52a8\u6001\u53d8\u5316\uff0c\u56e0\u6b64\u4f5c\u8005\u91c7\u7528\u795e\u7ecfODE\u5efa\u6a21\u4e30\u5ea6\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u5e76\u89e3\u51b3\u5b9a\u4e49\u3001\u5efa\u6a21\u548c\u7406\u8bba\u652f\u6301\u7684\u6311\u6218\u3002", "method": "MiLD\u6846\u67b6\u5305\u62ec\u95ee\u9898\u5b9a\u4e49\uff08ODE\u548c\u6f5c\u53d8\u91cf\uff09\u3001\u6570\u5b66\u5efa\u6a21\uff08\u52a8\u6001\u79bb\u6563\u5316\u65b9\u6cd5\uff09\u3001\u6c42\u89e3\u7b97\u6cd5\uff08\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u4e30\u5ea6\u6f14\u5316\uff09\u548c\u7406\u8bba\u9a8c\u8bc1\uff08\u4e00\u81f4\u6027\u3001\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u5b9a\u7406\uff09\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MiLD\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MiLD\u901a\u8fc7ODE\u5b9a\u4e49\u95ee\u9898\u3001\u52a8\u6001\u79bb\u6563\u5316\u5efa\u6a21\u3001\u7b97\u6cd5\u6c42\u89e3\u548c\u7406\u8bba\u652f\u6301\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u65f6\u76f8\u9ad8\u5149\u8c31\u89e3\u6df7\u4e2d\u7684\u52a8\u6001\u5efa\u6a21\u95ee\u9898\u3002", "relevance": 30.0}}
{"id": "2505.20922", "pdf": "https://arxiv.org/pdf/2505.20922", "abs": "https://arxiv.org/abs/2505.20922", "authors": ["Yang Zhang", "Xinran Li", "Jianing Ye", "Delin Qu", "Shuang Qiu", "Chongjie Zhang", "Xiu Li", "Chenjia Bai"], "title": "Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "World models have recently attracted growing interest in Multi-Agent\nReinforcement Learning (MARL) due to their ability to improve sample efficiency\nfor policy learning. However, accurately modeling environments in MARL is\nchallenging due to the exponentially large joint action space and highly\nuncertain dynamics inherent in multi-agent systems. To address this, we reduce\nmodeling complexity by shifting from jointly modeling the entire state-action\ntransition dynamics to focusing on the state space alone at each timestep\nthrough sequential agent modeling. Specifically, our approach enables the model\nto progressively resolve uncertainty while capturing the structured\ndependencies among agents, providing a more accurate representation of how\nagents influence the state. Interestingly, this sequential revelation of\nagents' actions in a multi-agent system aligns with the reverse process in\ndiffusion models--a class of powerful generative models known for their\nexpressiveness and training stability compared to autoregressive or latent\nvariable models. Leveraging this insight, we develop a flexible and robust\nworld model for MARL using diffusion models. Our method, Diffusion-Inspired\nMulti-Agent world model (DIMA), achieves state-of-the-art performance across\nmultiple multi-agent control benchmarks, significantly outperforming prior\nworld models in terms of final return and sample efficiency, including MAMuJoCo\nand Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent\nworld models, advancing the frontier of MARL research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u4e16\u754c\u6a21\u578b\uff08DIMA\uff09\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u5efa\u6a21\u964d\u4f4e\u590d\u6742\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\uff0c\u7531\u4e8e\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u5927\u548c\u73af\u5883\u52a8\u6001\u9ad8\u5ea6\u4e0d\u786e\u5b9a\uff0c\u51c6\u786e\u5efa\u6a21\u73af\u5883\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5e8f\u5217\u5316\u667a\u80fd\u4f53\u5efa\u6a21\uff0c\u5c06\u8054\u5408\u72b6\u6001-\u52a8\u4f5c\u52a8\u6001\u5efa\u6a21\u7b80\u5316\u4e3a\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u53cd\u5411\u8fc7\u7a0b\u7075\u611f\u6784\u5efaDIMA\u6a21\u578b\u3002", "result": "DIMA\u5728\u591a\u4e2a\u591a\u667a\u80fd\u4f53\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e16\u754c\u6a21\u578b\uff0c\u5982MAMuJoCo\u548cBi-DexHands\u3002", "conclusion": "DIMA\u4e3a\u591a\u667a\u80fd\u4f53\u4e16\u754c\u6a21\u578b\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86MARL\u7814\u7a76\u7684\u524d\u6cbf\u3002", "relevance": 75.0}}
{"id": "2505.20962", "pdf": "https://arxiv.org/pdf/2505.20962", "abs": "https://arxiv.org/abs/2505.20962", "authors": ["Nikos Giannakakis", "Argyris Manetas", "Panagiotis P. Filntisis", "Petros Maragos", "George Retsinas"], "title": "Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning visual representations from observing actions to benefit robot\nvisuo-motor policy generation is a promising direction that closely resembles\nhuman cognitive function and perception. Motivated by this, and further\ninspired by psychological theories suggesting that humans process scenes in an\nobject-based fashion, we propose an object-centric encoder that performs\nsemantic segmentation and visual representation generation in a coupled manner,\nunlike other works, which treat these as separate processes. To achieve this,\nwe leverage the Slot Attention mechanism and use the SOLV model, pretrained in\nlarge out-of-domain datasets, to bootstrap fine-tuning on human action video\ndata. Through simulated robotic tasks, we demonstrate that visual\nrepresentations can enhance reinforcement and imitation learning training,\nhighlighting the effectiveness of our integrated approach for semantic\nsegmentation and encoding. Furthermore, we show that exploiting models\npretrained on out-of-domain datasets can benefit this process, and that\nfine-tuning on datasets depicting human actions -- although still out-of-domain\n-- , can significantly improve performance due to close alignment with robotic\ntasks. These findings show the capability to reduce reliance on annotated or\nrobot-specific action datasets and the potential to build on existing visual\nencoders to accelerate training and improve generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u89c6\u89c9\u8868\u793a\u751f\u6210\u7684\u5bf9\u8c61\u4e2d\u5fc3\u7f16\u7801\u5668\uff0c\u5229\u7528Slot Attention\u548c\u9884\u8bad\u7ec3\u7684SOLV\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u529f\u80fd\u548c\u5fc3\u7406\u5b66\u7406\u8bba\u542f\u53d1\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u89c2\u5bdf\u52a8\u4f5c\u5b66\u4e60\u89c6\u89c9\u8868\u793a\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u751f\u6210\u3002", "method": "\u91c7\u7528\u5bf9\u8c61\u4e2d\u5fc3\u7f16\u7801\u5668\uff0c\u7ed3\u5408Slot Attention\u673a\u5236\u548c\u9884\u8bad\u7ec3\u7684SOLV\u6a21\u578b\uff0c\u5fae\u8c03\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u51cf\u5c11\u5bf9\u673a\u5668\u4eba\u7279\u5b9a\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4eba\u7c7b\u52a8\u4f5c\u6570\u636e\uff0c\u53ef\u4ee5\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.20984", "pdf": "https://arxiv.org/pdf/2505.20984", "abs": "https://arxiv.org/abs/2505.20984", "authors": ["Minghao Han", "Weiyi You", "Jinhua Zhang", "Leheng Zhang", "Ce Zhu", "Shuhang Gu"], "title": "Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "While learned image compression (LIC) focuses on efficient data transmission,\ngenerative image compression (GIC) extends this framework by integrating\ngenerative modeling to produce photo-realistic reconstructed images. In this\npaper, we propose a novel diffusion-based generative modeling framework\ntailored for generative image compression. Unlike prior diffusion-based\napproaches that indirectly exploit diffusion modeling, we reinterpret the\ncompression process itself as a forward diffusion path governed by stochastic\ndifferential equations (SDEs). A reverse neural network is trained to\nreconstruct images by reversing the compression process directly, without\nrequiring Gaussian noise initialization. This approach achieves smooth rate\nadjustment and photo-realistic reconstructions with only a minimal number of\nsampling steps. Extensive experiments on benchmark datasets demonstrate that\nour method outperforms existing generative image compression approaches across\na range of metrics, including perceptual distortion, statistical fidelity, and\nno-reference quality assessments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u538b\u7f29\u8fc7\u7a0b\u4e3a\u524d\u5411\u6269\u6563\u8def\u5f84\uff0c\u5e76\u8bad\u7ec3\u53cd\u5411\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u91cd\u5efa\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u63d0\u5347\u538b\u7f29\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u5c06\u538b\u7f29\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u524d\u5411\u6269\u6563\u8def\u5f84\uff08SDEs\uff09\uff0c\u5e76\u8bad\u7ec3\u53cd\u5411\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u91cd\u5efa\u56fe\u50cf\uff0c\u65e0\u9700\u9ad8\u65af\u566a\u58f0\u521d\u59cb\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u5931\u771f\u3001\u7edf\u8ba1\u4fdd\u771f\u5ea6\u548c\u65e0\u53c2\u8003\u8d28\u91cf\u8bc4\u4f30\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6269\u6563\u6846\u67b6\u4e3a\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.20947", "pdf": "https://arxiv.org/pdf/2505.20947", "abs": "https://arxiv.org/abs/2505.20947", "authors": ["Lorenzo Monti", "Tatiana Muraveva", "Alessia Garofalo", "Gisella Clementini", "Maria Letizia Valentini"], "title": "Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3", "categories": ["astro-ph.SR", "cs.AI"], "comment": null, "summary": "RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity\ntracers due to the correlation between their metal abundances and light curve\nmorphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs,\nthere is a pressing need for scalable methods to estimate their metallicities\nfrom photometric data. We introduce a unified deep learning framework that\nestimates metallicities for both fundamental-mode (RRab) and first-overtone\n(RRc) RRLs using Gaia G-band light curves. This approach extends our previous\nwork on RRab stars to include RRc stars, aiming for high predictive accuracy\nand broad generalization across both pulsation types. The model is based on a\nGated Recurrent Unit (GRU) neural network optimized for time-series extrinsic\nregression. Our pipeline includes preprocessing steps such as phase folding,\nsmoothing, and sample weighting, and uses photometric metallicities from the\nliterature as training targets. The architecture is designed to handle\nmorphological differences between RRab and RRc light curves without requiring\nseparate models. On held-out validation sets, our GRU model achieves strong\nperformance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401;\nfor RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results\nshow the effectiveness of deep learning for large-scale photometric metallicity\nestimation and support its application to studies of stellar populations and\nGalactic structure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRU\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eceGaia G\u6ce2\u6bb5\u5149\u53d8\u66f2\u7ebf\u4f30\u8ba1RR Lyrae\u6052\u661f\u7684\u91d1\u5c5e\u4e30\u5ea6\uff0c\u9002\u7528\u4e8eRRab\u548cRRc\u4e24\u79cd\u8109\u52a8\u7c7b\u578b\u3002", "motivation": "\u968f\u7740Gaia DR3\u63d0\u4f9b\u4e86\u7ea627\u4e07\u9897RR Lyrae\u6052\u661f\u7684\u5149\u53d8\u66f2\u7ebf\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u4ece\u5149\u5ea6\u6570\u636e\u4f30\u8ba1\u5176\u91d1\u5c5e\u4e30\u5ea6\u3002", "method": "\u91c7\u7528GRU\u795e\u7ecf\u7f51\u7edc\uff0c\u4f18\u5316\u65f6\u95f4\u5e8f\u5217\u5916\u56de\u5f52\uff0c\u9884\u5904\u7406\u5305\u62ec\u76f8\u4f4d\u6298\u53e0\u3001\u5e73\u6ed1\u548c\u6837\u672c\u52a0\u6743\uff0c\u4f7f\u7528\u6587\u732e\u4e2d\u7684\u5149\u5ea6\u91d1\u5c5e\u4e30\u5ea6\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728\u9a8c\u8bc1\u96c6\u4e0a\uff0cRRab\u7684MAE=0.0565 dex\uff0cRMSE=0.0765 dex\uff0cR\u00b2=0.9401\uff1bRRc\u7684MAE=0.0505 dex\uff0cRMSE=0.0720 dex\uff0cR\u00b2=0.9625\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u5149\u5ea6\u91d1\u5c5e\u4e30\u5ea6\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6052\u661f\u79cd\u7fa4\u548c\u94f6\u6cb3\u7ed3\u6784\u7814\u7a76\u3002", "relevance": 10.0}}
{"id": "2505.21041", "pdf": "https://arxiv.org/pdf/2505.21041", "abs": "https://arxiv.org/abs/2505.21041", "authors": ["Weihang Liu", "Yuhui Zhong", "Yuke Li", "Xi Chen", "Jiadi Cui", "Honglong Zhang", "Lan Xu", "Xin Lou", "Yujiao Shi", "Jingyi Yu", "Yingliang Zhang"], "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate and efficient modeling of large-scale urban scenes is critical for\napplications such as AR navigation, UAV based inspection, and smart city\ndigital twins. While aerial imagery offers broad coverage and complements\nlimitations of ground-based data, reconstructing city-scale environments from\nsuch views remains challenging due to occlusions, incomplete geometry, and high\nmemory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve\nscalability and visual quality but remain limited by dense primitive usage,\nlong training times, and poor suit ability for edge devices. We propose CityGo,\na hybrid framework that combines textured proxy geometry with residual and\nsurrounding 3D Gaussians for lightweight, photorealistic rendering of urban\nscenes from aerial perspectives. Our approach first extracts compact building\nproxy meshes from MVS point clouds, then uses zero order SH Gaussians to\ngenerate occlusion-free textures via image-based rendering and back-projection.\nTo capture high-frequency details, we introduce residual Gaussians placed based\non proxy-photo discrepancies and guided by depth priors. Broader urban context\nis represented by surrounding Gaussians, with importance-aware downsampling\napplied to non-critical regions to reduce redundancy. A tailored optimization\nstrategy jointly refines proxy textures and Gaussian parameters, enabling\nreal-time rendering of complex urban scenes on mobile GPUs with significantly\nreduced training and memory requirements. Extensive experiments on real-world\naerial datasets demonstrate that our hybrid representation significantly\nreduces training time, achieving on average 1.4x speedup, while delivering\ncomparable visual fidelity to pure 3D Gaussian Splatting approaches.\nFurthermore, CityGo enables real-time rendering of large-scale urban scenes on\nmobile consumer GPUs, with substantially reduced memory usage and energy\nconsumption.", "AI": {"tldr": "CityGo\u662f\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u7eb9\u7406\u4ee3\u7406\u51e0\u4f55\u4e0e\u6b8b\u5dee\u548c\u5468\u56f43D\u9ad8\u65af\uff0c\u7528\u4e8e\u8f7b\u91cf\u7ea7\u3001\u903c\u771f\u7684\u57ce\u5e02\u573a\u666f\u6e32\u67d3\u3002", "motivation": "\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u7684\u7cbe\u786e\u9ad8\u6548\u5efa\u6a21\u5bf9AR\u5bfc\u822a\u3001\u65e0\u4eba\u673a\u68c0\u67e5\u548c\u667a\u6167\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u59823D\u9ad8\u65af\u6cfc\u6e85\u5b58\u5728\u5bc6\u96c6\u57fa\u5143\u4f7f\u7528\u3001\u957f\u8bad\u7ec3\u65f6\u95f4\u548c\u8fb9\u7f18\u8bbe\u5907\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "CityGo\u9996\u5148\u4eceMVS\u70b9\u4e91\u63d0\u53d6\u7d27\u51d1\u5efa\u7b51\u4ee3\u7406\u7f51\u683c\uff0c\u4f7f\u7528\u96f6\u9636SH\u9ad8\u65af\u751f\u6210\u65e0\u906e\u6321\u7eb9\u7406\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u9ad8\u65af\u6355\u83b7\u9ad8\u9891\u7ec6\u8282\uff0c\u5468\u56f4\u9ad8\u65af\u8868\u793a\u57ce\u5e02\u80cc\u666f\uff0c\u91c7\u7528\u91cd\u8981\u6027\u611f\u77e5\u4e0b\u91c7\u6837\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCityGo\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff08\u5e73\u57471.4\u500d\u52a0\u901f\uff09\uff0c\u5728\u79fb\u52a8GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\uff0c\u5185\u5b58\u548c\u80fd\u8017\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "CityGo\u901a\u8fc7\u6df7\u5408\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8f7b\u91cf\u7684\u57ce\u5e02\u573a\u666f\u6e32\u67d3\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u3002", "relevance": 30.0}}
{"id": "2505.20949", "pdf": "https://arxiv.org/pdf/2505.20949", "abs": "https://arxiv.org/abs/2505.20949", "authors": ["Andrea Giovanni Nuzzolese"], "title": "Streamlining Knowledge Graph Creation with PyRML", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Knowledge Graphs (KGs) are increasingly adopted as a foundational technology\nfor integrating heterogeneous data in domains such as climate science, cultural\nheritage, and the life sciences. Declarative mapping languages like R2RML and\nRML have played a central role in enabling scalable and reusable KG\nconstruction, offering a transparent means of transforming structured and\nsemi-structured data into RDF. In this paper, we present PyRML, a lightweight,\nPython-native library for building Knowledge Graphs through declarative\nmappings. PyRML supports core RML constructs and provides a programmable\ninterface for authoring, executing, and testing mappings directly within Python\nenvironments. It integrates with popular data and semantic web libraries (e.g.,\nPandas and RDFlib), enabling transparent and modular workflows. By lowering the\nbarrier to entry for KG creation and fostering reproducible, ontology-aligned\ndata integration, PyRML bridges the gap between declarative semantics and\npractical KG engineering.", "AI": {"tldr": "PyRML\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684Python\u539f\u751f\u5e93\uff0c\u7528\u4e8e\u901a\u8fc7\u58f0\u660e\u5f0f\u6620\u5c04\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\uff0c\u652f\u6301\u6838\u5fc3RML\u6784\u9020\uff0c\u5e76\u4e0ePython\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u5728\u591a\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u6210\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u5728\u6613\u7528\u6027\u548c\u53ef\u7f16\u7a0b\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002PyRML\u65e8\u5728\u964d\u4f4eKG\u6784\u5efa\u7684\u95e8\u69db\uff0c\u63d0\u5347\u53ef\u91cd\u7528\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "PyRML\u662f\u4e00\u4e2aPython\u5e93\uff0c\u652f\u6301RML\u6838\u5fc3\u6784\u9020\uff0c\u63d0\u4f9b\u7f16\u7a0b\u63a5\u53e3\u7528\u4e8e\u521b\u5efa\u3001\u6267\u884c\u548c\u6d4b\u8bd5\u6620\u5c04\uff0c\u5e76\u4e0ePandas\u548cRDFlib\u7b49\u5e93\u96c6\u6210\u3002", "result": "PyRML\u5b9e\u73b0\u4e86\u900f\u660e\u3001\u6a21\u5757\u5316\u7684KG\u6784\u5efa\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u58f0\u660e\u5f0f\u8bed\u4e49\u4e0e\u5b9e\u9645\u5de5\u7a0b\u9700\u6c42\u7684\u7ed3\u5408\u3002", "conclusion": "PyRML\u901a\u8fc7Python\u539f\u751f\u652f\u6301\uff0c\u7b80\u5316\u4e86KG\u6784\u5efa\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u4e86\u53ef\u590d\u73b0\u548c\u672c\u4f53\u5bf9\u9f50\u7684\u6570\u636e\u96c6\u6210\u3002", "relevance": 30.0}}
{"id": "2505.21091", "pdf": "https://arxiv.org/pdf/2505.21091", "abs": "https://arxiv.org/abs/2505.21091", "authors": ["Anna Neumann", "Elisabeth Kirsten", "Muhammad Bilal Zafar", "Jatinder Singh"], "title": "Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Forthcoming in Proceedings of ACM FAccT 2025", "summary": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7cfb\u7edf\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4fe1\u606f\u4f4d\u7f6e\u5982\u4f55\u5bfc\u81f4\u504f\u89c1\u548c\u6f5c\u5728\u5371\u5bb3\u3002", "motivation": "\u7cfb\u7edf\u63d0\u793a\u5728LLMs\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u900f\u660e\u5ea6\u548c\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u4fe1\u606f\u4f4d\u7f6e\u53ef\u80fd\u5bfc\u81f4\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u4e86\u516d\u79cd\u5546\u4e1aLLM\u4e2d\u7cfb\u7edf\u63d0\u793a\u4e0e\u7528\u6237\u63d0\u793a\u5bf950\u4e2a\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u5904\u7406\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7cfb\u7edf\u63d0\u793a\u5bfc\u81f4\u663e\u8457\u7684\u504f\u89c1\uff0c\u5f71\u54cd\u7528\u6237\u8868\u793a\u548c\u51b3\u7b56\u573a\u666f\uff0c\u4e14\u8fd9\u4e9b\u504f\u89c1\u96be\u4ee5\u88ab\u7528\u6237\u5bdf\u89c9\u6216\u4fee\u6b63\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u5206\u6790\u5e94\u7eb3\u5165AI\u5ba1\u8ba1\u6d41\u7a0b\uff0c\u4ee5\u9632\u6b62\u6f5c\u5728\u5371\u5bb3\u3002", "relevance": 85.0}}
{"id": "2505.20956", "pdf": "https://arxiv.org/pdf/2505.20956", "abs": "https://arxiv.org/abs/2505.20956", "authors": ["Shiqi Zhang", "Tuomas Virtanen"], "title": "Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "5 pages, 1 figure, accepted by EUSIPCO 2025", "summary": "Bioacoustic sound event detection (BioSED) is crucial for biodiversity\nconservation but faces practical challenges during model development and\ntraining: limited amounts of annotated data, sparse events, species diversity,\nand class imbalance. To address these challenges efficiently with a limited\nlabeling budget, we apply the mismatch-first farthest-traversal (MFFT), an\nactive learning method integrating committee voting disagreement and diversity\nanalysis. We also refine an existing BioSED dataset specifically for evaluating\nactive learning algorithms. Experimental results demonstrate that MFFT achieves\na mAP of 68% when cold-starting and 71% when warm-starting (which is close to\nthe fully-supervised mAP of 75%) while using only 2.3% of the annotations.\nNotably, MFFT excels in cold-start scenarios and with rare species, which are\ncritical for monitoring endangered species, demonstrating its practical value.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u65b9\u6cd5MFFT\uff0c\u7528\u4e8e\u89e3\u51b3\u751f\u7269\u58f0\u5b66\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u751f\u7269\u58f0\u5b66\u4e8b\u4ef6\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u5c11\u3001\u4e8b\u4ef6\u7a00\u758f\u3001\u7269\u79cd\u591a\u6837\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u5229\u7528\u6709\u9650\u6807\u6ce8\u9884\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528MFFT\uff08\u57fa\u4e8e\u59d4\u5458\u4f1a\u6295\u7968\u5206\u6b67\u548c\u591a\u6837\u6027\u5206\u6790\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u5e76\u4f18\u5316\u4e86\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "MFFT\u5728\u51b7\u542f\u52a8\u548c\u70ed\u542f\u52a8\u573a\u666f\u4e0b\u5206\u522b\u8fbe\u523068%\u548c71%\u7684mAP\uff0c\u4ec5\u97002.3%\u7684\u6807\u6ce8\u91cf\uff0c\u63a5\u8fd1\u5168\u76d1\u7763\u768475%\u3002", "conclusion": "MFFT\u5728\u51b7\u542f\u52a8\u548c\u7a00\u6709\u7269\u79cd\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u6fd2\u5371\u7269\u79cd\u76d1\u6d4b\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002", "relevance": 30.0}}
{"id": "2505.21116", "pdf": "https://arxiv.org/pdf/2505.21116", "abs": "https://arxiv.org/abs/2505.21116", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u7b2c\u4e00\u7bc7\u4e13\u6ce8\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u521b\u9020\u529b\u7684\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86LLM\u9a71\u52a8\u7684MAS\u5728\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u521b\u9020\u529b\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u5927\u591a\u5ffd\u7565\u4e86\u521b\u9020\u529b\u7684\u7ef4\u5ea6\uff0c\u5305\u62ec\u65b0\u9896\u8f93\u51fa\u7684\u751f\u6210\u4e0e\u8bc4\u4f30\u3001\u667a\u80fd\u4f53\u89d2\u8272\u8bbe\u8ba1\u4ee5\u53ca\u521b\u610f\u5de5\u4f5c\u6d41\u7684\u534f\u8c03\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u6027\u548c\u89d2\u8272\u8bbe\u8ba1\uff0c\u5e76\u6982\u8ff0\u4e86\u751f\u6210\u6280\u672f\uff08\u5982\u53d1\u6563\u63a2\u7d22\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u534f\u4f5c\u5408\u6210\uff09\u4ee5\u53ca\u76f8\u5173\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u81f4\u3001\u504f\u89c1\u7f13\u89e3\u4e0d\u8db3\u3001\u534f\u8c03\u51b2\u7a81\u548c\u7f3a\u4e4f\u7edf\u4e00\u57fa\u51c6\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\u548c\u8def\u7ebf\u56fe\uff0c\u4ee5\u63a8\u52a8\u521b\u9020\u6027MAS\u7684\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u6807\u51c6\u5316\u3002", "relevance": 75.0}}
{"id": "2505.20529", "pdf": "https://arxiv.org/pdf/2505.20529", "abs": "https://arxiv.org/abs/2505.20529", "authors": ["Charles McGhee", "Mark J. F. Gales", "Kate M. Knill"], "title": "Training Articulatory Inversion Models for Inter-Speaker Consistency", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse\nmapping from speech to articulation. Exact articulatory prediction from speech\nalone may be impossible, as speakers can choose different forms of articulation\nseemingly without reference to their vocal tract structure. However, once a\nspeaker has selected an articulatory form, their productions vary minimally.\nRecent works in AAI have proposed adapting Self-Supervised Learning (SSL)\nmodels to single-speaker datasets, claiming that these single-speaker models\nprovide a universal articulatory template. In this paper, we investigate\nwhether SSL-adapted models trained on single and multi-speaker data produce\narticulatory targets which are consistent across speaker identities for English\nand Russian. We do this through the use of a novel evaluation method which\nextracts articulatory targets using minimal pair sets. We also present a\ntraining method which can improve inter-speaker consistency using only speech\ndata.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684\u5355\u8bf4\u8bdd\u4eba\u548c\u591a\u8bf4\u8bdd\u4eba\u6570\u636e\u8bad\u7ec3\u7684\u58f0\u5b66-\u53d1\u97f3\u53cd\u6f14\uff08AAI\uff09\u6a21\u578b\u5728\u82f1\u8bed\u548c\u4fc4\u8bed\u4e2d\u7684\u8de8\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u8ba8SSL\u6a21\u578b\u5728AAI\u4efb\u52a1\u4e2d\u662f\u5426\u80fd\u591f\u4ea7\u751f\u8de8\u8bf4\u8bdd\u4eba\u4e00\u81f4\u7684\u53d1\u97f3\u76ee\u6807\uff0c\u4ee5\u9a8c\u8bc1\u5355\u8bf4\u8bdd\u4eba\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u901a\u7528\u53d1\u97f3\u6a21\u677f\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u5bf9\u96c6\u63d0\u53d6\u53d1\u97f3\u76ee\u6807\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f9d\u8d56\u8bed\u97f3\u6570\u636e\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8de8\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0SSL\u6a21\u578b\u5728\u5355\u8bf4\u8bdd\u4eba\u548c\u591a\u8bf4\u8bdd\u4eba\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u53d1\u97f3\u76ee\u6807\u5b58\u5728\u8de8\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u5dee\u5f02\uff0c\u63d0\u51fa\u7684\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u6539\u5584\u8fd9\u79cd\u4e00\u81f4\u6027\u3002", "conclusion": "SSL\u6a21\u578b\u5728AAI\u4efb\u52a1\u4e2d\u5177\u6709\u4e00\u5b9a\u7684\u8de8\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "relevance": 30.0}}
{"id": "2505.20961", "pdf": "https://arxiv.org/pdf/2505.20961", "abs": "https://arxiv.org/abs/2505.20961", "authors": ["Yiyuan Yang", "Shitong Xu", "Niki Trigoni", "Andrew Markham"], "title": "Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Accepted by Interspeech 2025 Conference", "summary": "Sound source localization (SSL) is a critical technology for determining the\nposition of sound sources in complex environments. However, existing methods\nface challenges such as high computational costs and precise calibration\nrequirements, limiting their deployment in dynamic or resource-constrained\nenvironments. This paper introduces a novel 3D SSL framework, which uses sparse\ncross-attention, pretraining, and adaptive signal coherence metrics, to achieve\naccurate and computationally efficient localization with fewer input\nmicrophones. The framework is also fault-tolerant to unreliable or even unknown\nmicrophone position inputs, ensuring its applicability in real-world scenarios.\nPreliminary experiments demonstrate its scalability for multi-source\nlocalization without requiring additional hardware. This work advances SSL by\nbalancing the model's performance and efficiency and improving its robustness\nfor real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4ea4\u53c9\u6ce8\u610f\u529b\u3001\u9884\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u4fe1\u53f7\u4e00\u81f4\u6027\u5ea6\u91cf\u76843D\u58f0\u6e90\u5b9a\u4f4d\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7cbe\u786e\u6821\u51c6\u9700\u6c42\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u5728\u52a8\u6001\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7cbe\u786e\u6821\u51c6\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u6846\u67b6\u63d0\u5347\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u7a00\u758f\u4ea4\u53c9\u6ce8\u610f\u529b\u3001\u9884\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u4fe1\u53f7\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u51cf\u5c11\u8f93\u5165\u9ea6\u514b\u98ce\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9a\u4f4d\u51c6\u786e\u6027\u3002\u6846\u67b6\u5bf9\u4e0d\u53ef\u9760\u6216\u672a\u77e5\u9ea6\u514b\u98ce\u4f4d\u7f6e\u8f93\u5165\u5177\u6709\u5bb9\u9519\u6027\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u6e90\u5b9a\u4f4d\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u652f\u6301\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5e73\u8861\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\uff0c\u63d0\u5347\u4e86\u58f0\u6e90\u5b9a\u4f4d\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "relevance": 30.0}}
{"id": "2505.21162", "pdf": "https://arxiv.org/pdf/2505.21162", "abs": "https://arxiv.org/abs/2505.21162", "authors": ["Davi A. Bezerra", "Filipi N. Silva", "Diego R. Amancio"], "title": "Leveraging GANs for citation intent classification and its impact on citation network analysis", "categories": ["cs.DL", "cs.CL", "cs.SI"], "comment": null, "summary": "Citations play a fundamental role in the scientific ecosystem, serving as a\nfoundation for tracking the flow of knowledge, acknowledging prior work, and\nassessing scholarly influence. In scientometrics, they are also central to the\nconstruction of quantitative indicators. Not all citations, however, serve the\nsame function: some provide background, others introduce methods, or compare\nresults. Therefore, understanding citation intent allows for a more nuanced\ninterpretation of scientific impact. In this paper, we adopted a GAN-based\nmethod to classify citation intents. Our results revealed that the proposed\nmethod achieves competitive classification performance, closely matching\nstate-of-the-art results with substantially fewer parameters. This demonstrates\nthe effectiveness and efficiency of leveraging GAN architectures combined with\ncontextual embeddings in intent classification task. We also investigated\nwhether filtering citation intents affects the centrality of papers in citation\nnetworks. Analyzing the network constructed from the unArXiv dataset, we found\nthat paper rankings can be significantly influenced by citation intent. All\nfour centrality metrics examined- degree, PageRank, closeness, and betweenness\n- were sensitive to the filtering of citation types. The betweenness centrality\ndisplayed the greatest sensitivity, showing substantial changes in ranking when\nspecific citation intents were removed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u65b9\u6cd5\u7528\u4e8e\u5206\u7c7b\u5f15\u7528\u610f\u56fe\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u66f4\u5c11\u3002\u540c\u65f6\uff0c\u7814\u7a76\u53d1\u73b0\u5f15\u7528\u610f\u56fe\u7684\u8fc7\u6ee4\u4f1a\u5f71\u54cd\u8bba\u6587\u5728\u5f15\u7528\u7f51\u7edc\u4e2d\u7684\u4e2d\u5fc3\u6027\u3002", "motivation": "\u7406\u89e3\u5f15\u7528\u610f\u56fe\u53ef\u4ee5\u66f4\u7ec6\u81f4\u5730\u89e3\u91ca\u79d1\u5b66\u5f71\u54cd\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u6548\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528GAN\u67b6\u6784\u7ed3\u5408\u4e0a\u4e0b\u6587\u5d4c\u5165\u8fdb\u884c\u5f15\u7528\u610f\u56fe\u5206\u7c7b\u3002", "result": "\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\uff1b\u5f15\u7528\u610f\u56fe\u8fc7\u6ee4\u663e\u8457\u5f71\u54cd\u8bba\u6587\u5728\u5f15\u7528\u7f51\u7edc\u4e2d\u7684\u4e2d\u5fc3\u6027\u3002", "conclusion": "GAN\u67b6\u6784\u5728\u5f15\u7528\u610f\u56fe\u5206\u7c7b\u4e2d\u9ad8\u6548\u4e14\u6709\u6548\uff1b\u5f15\u7528\u610f\u56fe\u5bf9\u8bba\u6587\u5f71\u54cd\u529b\u8bc4\u4f30\u6709\u91cd\u8981\u5f71\u54cd\u3002", "relevance": 40.0}}
{"id": "2505.20536", "pdf": "https://arxiv.org/pdf/2505.20536", "abs": "https://arxiv.org/abs/2505.20536", "authors": ["Guanhao Zhou", "Yuefeng Han", "Xiufan Yu"], "title": "Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.ME"], "comment": null, "summary": "This paper studies the task of estimating heterogeneous treatment effects in\ncausal panel data models, in the presence of covariate effects. We propose a\nnovel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models,\nthat employs flexible model structures and powerful neural network\narchitectures to cohesively deal with the underlying heterogeneity and\nnonlinearity of both panel units and covariate effects. The proposed CoDEAL\nintegrates nonlinear covariate effect components (parameterized by a\nfeed-forward neural network) with nonlinear factor structures (modeled by a\nmulti-output autoencoder) to form a heterogeneous causal panel model. The\nnonlinear covariate component offers a flexible framework for capturing the\ncomplex influences of covariates on outcomes. The nonlinear factor analysis\nenables CoDEAL to effectively capture both cross-sectional and temporal\ndependencies inherent in the data panel. This latent structural information is\nsubsequently integrated into a customized matrix completion algorithm, thereby\nfacilitating more accurate imputation of missing counterfactual outcomes.\nMoreover, the use of a multi-output autoencoder explicitly accounts for\nheterogeneity across units and enhances the model interpretability of the\nlatent factors. We establish theoretical guarantees on the convergence of the\nestimated counterfactuals, and demonstrate the compelling performance of the\nproposed method using extensive simulation studies and a real data application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoDEAL\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u56e0\u679c\u9762\u677f\u6570\u636e\u6a21\u578b\u4e2d\u7684\u5f02\u8d28\u6027\u5904\u7406\u6548\u5e94\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u7f16\u7801\u5668\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5b58\u5728\u534f\u53d8\u91cf\u6548\u5e94\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u4f30\u8ba1\u56e0\u679c\u9762\u677f\u6570\u636e\u6a21\u578b\u4e2d\u7684\u5f02\u8d28\u6027\u5904\u7406\u6548\u5e94\u3002", "method": "\u63d0\u51faCoDEAL\u65b9\u6cd5\uff0c\u7ed3\u5408\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u8f93\u51fa\u81ea\u7f16\u7801\u5668\uff0c\u5904\u7406\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u975e\u7ebf\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u4f30\u8ba1\u7684\u53cd\u4e8b\u5b9e\u7ed3\u679c\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CoDEAL\u65b9\u6cd5\u5728\u4f30\u8ba1\u5f02\u8d28\u6027\u5904\u7406\u6548\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 30.0}}
{"id": "2505.21230", "pdf": "https://arxiv.org/pdf/2505.21230", "abs": "https://arxiv.org/abs/2505.21230", "authors": ["Nima Sedghiyeh", "Sara Sadeghi", "Reza Khodadadi", "Farzin Kashani", "Omid Aghdaei", "Somayeh Rahimi", "Mohammad Sadegh Safari"], "title": "PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "25 pages, 7 figures", "summary": "Although Automatic Speech Recognition (ASR) systems have become an integral\npart of modern technology, their evaluation remains challenging, particularly\nfor low-resource languages such as Persian. This paper introduces Persian\nSpeech Recognition Benchmark(PSRB), a comprehensive benchmark designed to\naddress this gap by incorporating diverse linguistic and acoustic conditions.\nWe evaluate ten ASR systems, including state-of-the-art commercial and\nopen-source models, to examine performance variations and inherent biases.\nAdditionally, we conduct an in-depth analysis of Persian ASR transcriptions,\nidentifying key error types and proposing a novel metric that weights\nsubstitution errors. This metric enhances evaluation robustness by reducing the\nimpact of minor and partial errors, thereby improving the precision of\nperformance assessment. Our findings indicate that while ASR models generally\nperform well on standard Persian, they struggle with regional accents,\nchildren's speech, and specific linguistic challenges. These results highlight\nthe necessity of fine-tuning and incorporating diverse, representative training\ndatasets to mitigate biases and enhance overall ASR performance. PSRB provides\na valuable resource for advancing ASR research in Persian and serves as a\nframework for developing benchmarks in other low-resource languages. A subset\nof the PSRB dataset is publicly available at\nhttps://huggingface.co/datasets/PartAI/PSRB.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u6ce2\u65af\u8bed\u8bed\u97f3\u8bc6\u522b\u57fa\u51c6\uff08PSRB\uff09\uff0c\u65e8\u5728\u8bc4\u4f30\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u7684ASR\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u9519\u8bef\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09ASR\u7cfb\u7edf\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u63ed\u793a\u6027\u80fd\u5dee\u5f02\u548c\u504f\u89c1\u3002", "method": "\u6784\u5efaPSRB\u57fa\u51c6\uff0c\u8bc4\u4f3010\u79cdASR\u7cfb\u7edf\uff0c\u5206\u6790\u9519\u8bef\u7c7b\u578b\u5e76\u63d0\u51fa\u65b0\u7684\u52a0\u6743\u66ff\u6362\u9519\u8bef\u5ea6\u91cf\u3002", "result": "ASR\u6a21\u578b\u5728\u6807\u51c6\u6ce2\u65af\u8bed\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u533a\u57df\u53e3\u97f3\u3001\u513f\u7ae5\u8bed\u97f3\u548c\u7279\u5b9a\u8bed\u8a00\u6311\u6218\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u5fae\u8c03\u548c\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4ee5\u51cf\u5c11\u504f\u89c1\uff0cPSRB\u4e3a\u6ce2\u65af\u8bedASR\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002", "relevance": 40.0}}
{"id": "2505.21146", "pdf": "https://arxiv.org/pdf/2505.21146", "abs": "https://arxiv.org/abs/2505.21146", "authors": ["Yang Zhao", "Yan Zhang", "Xubo Yang"], "title": "IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Existing human motion generation methods with trajectory and pose inputs\noperate global processing on both modalities, leading to suboptimal outputs. In\nthis paper, we propose IKMo, an image-keyframed motion generation method based\non the diffusion model with trajectory and pose being decoupled. The trajectory\nand pose inputs go through a two-stage conditioning framework. In the first\nstage, the dedicated optimization module is applied to refine inputs. In the\nsecond stage, trajectory and pose are encoded via a Trajectory Encoder and a\nPose Encoder in parallel. Then, motion with high spatial and semantic fidelity\nis guided by a motion ControlNet, which processes the fused trajectory and pose\ndata. Experiment results based on HumanML3D and KIT-ML datasets demonstrate\nthat the proposed method outperforms state-of-the-art on all metrics under\ntrajectory-keyframe constraints. In addition, MLLM-based agents are implemented\nto pre-process model inputs. Given texts and keyframe images from users, the\nagents extract motion descriptions, keyframe poses, and trajectories as the\noptimized inputs into the motion generation model. We conducts a user study\nwith 10 participants. The experiment results prove that the MLLM-based agents\npre-processing makes generated motion more in line with users' expectation. We\nbelieve that the proposed method improves both the fidelity and controllability\nof motion generation by the diffusion model.", "AI": {"tldr": "IKMo\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8f68\u8ff9\u548c\u59ff\u6001\u8f93\u5165\uff0c\u7ed3\u5408\u4f18\u5316\u6a21\u5757\u548c\u5e76\u884c\u7f16\u7801\u5668\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u8fd0\u52a8\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8f68\u8ff9\u5173\u952e\u5e27\u7ea6\u675f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7MLLM\u4ee3\u7406\u63d0\u5347\u7528\u6237\u671f\u671b\u5339\u914d\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u8f68\u8ff9\u548c\u59ff\u6001\u8fdb\u884c\u5168\u5c40\u5904\u7406\u5bfc\u81f4\u8f93\u51fa\u4e0d\u7406\u60f3\uff0c\u9700\u6539\u8fdb\u8fd0\u52a8\u751f\u6210\u7684\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6761\u4ef6\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f18\u5316\u8f93\u5165\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5e76\u884c\u7f16\u7801\u5668\u5904\u7406\u8f68\u8ff9\u548c\u59ff\u6001\uff0c\u6700\u540e\u7531ControlNet\u878d\u5408\u751f\u6210\u8fd0\u52a8\u3002\u7ed3\u5408MLLM\u4ee3\u7406\u9884\u5904\u7406\u7528\u6237\u8f93\u5165\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u8868\u660eMLLM\u4ee3\u7406\u63d0\u5347\u751f\u6210\u8fd0\u52a8\u4e0e\u7528\u6237\u671f\u671b\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "IKMo\u901a\u8fc7\u89e3\u8026\u548c\u4f18\u5316\u8f93\u5165\uff0c\u7ed3\u5408MLLM\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u751f\u6210\u7684\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21277", "pdf": "https://arxiv.org/pdf/2505.21277", "abs": "https://arxiv.org/abs/2505.21277", "authors": ["Yao Huang", "Yitong Sun", "Shouwei Ruan", "Yichi Zhang", "Yinpeng Dong", "Xingxing Wei"], "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "19 pages, 20 figures, accepted by ACL 2025, Findings", "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eElaboration Likelihood Model\uff08ELM\uff09\u7406\u8bba\u548c\u9057\u4f20\u4f18\u5316\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u5c55LLM\u7684\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u6539\u8fdb\u4e86LLM\u7684\u5b89\u5168\u6027\uff0c\u4f46\u5176\u6548\u679c\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u7b56\u7565\u7a7a\u95f4\uff0c\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9\u5b89\u5168\u5bf9\u9f50\u6a21\u578b\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7ELM\u7406\u8bba\u5206\u89e3\u8d8a\u72f1\u7b56\u7565\u4e3a\u57fa\u672c\u7ec4\u4ef6\uff0c\u5e76\u7ed3\u5408\u9057\u4f20\u4f18\u5316\u548c\u610f\u56fe\u8bc4\u4f30\u673a\u5236\uff0c\u7cfb\u7edf\u6027\u6269\u5c55\u7b56\u7565\u7a7a\u95f4\u3002", "result": "\u5728Claude-3.5\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u6210\u529f\u7387\uff0c\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u578b\u8fc1\u79fb\u80fd\u529b\u548c\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "conclusion": "\u6269\u5c55\u7b56\u7565\u7a7a\u95f4\u662f\u63d0\u5347LLM\u8d8a\u72f1\u653b\u51fb\u80fd\u529b\u7684\u5173\u952e\uff0c\u65b0\u6846\u67b6\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "relevance": 85.0}}
{"id": "2505.20973", "pdf": "https://arxiv.org/pdf/2505.20973", "abs": "https://arxiv.org/abs/2505.20973", "authors": ["Keheliya Gallaba", "Ali Arabat", "Dayi Lin", "Mohammed Sayagh", "Ahmed E. Hassan"], "title": "Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Foundation Models (FMs) have shown remarkable capabilities in various natural\nlanguage tasks. However, their ability to accurately capture stakeholder\nrequirements remains a significant challenge for using FMs for software\ndevelopment. This paper introduces a novel approach that leverages an\nFM-powered multi-agent system called AlignMind to address this issue. By having\na cognitive architecture that enhances FMs with Theory-of-Mind capabilities,\nour approach considers the mental states and perspectives of software makers.\nThis allows our solution to iteratively clarify the beliefs, desires, and\nintentions of stakeholders, translating these into a set of refined\nrequirements and a corresponding actionable natural language workflow in the\noften-overlooked requirements refinement phase of software engineering, which\nis crucial after initial elicitation. Through a multifaceted evaluation\ncovering 150 diverse use cases, we demonstrate that our approach can accurately\ncapture the intents and requirements of stakeholders, articulating them as both\nspecifications and a step-by-step plan of action. Our findings suggest that the\npotential for significant improvements in the software development process\njustifies these investments. Our work lays the groundwork for future innovation\nin building intent-first development environments, where software makers can\nseamlessly collaborate with AIs to create software that truly meets their\nneeds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edfAlignMind\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6355\u6349\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edfAlignMind\uff0c\u7ed3\u5408\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u8fed\u4ee3\u6f84\u6e05\u5229\u76ca\u76f8\u5173\u8005\u7684\u4fe1\u5ff5\u3001\u613f\u671b\u548c\u610f\u56fe\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728150\u4e2a\u591a\u6837\u5316\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u80fd\u51c6\u786e\u6355\u6349\u9700\u6c42\uff0c\u5e76\u751f\u6210\u89c4\u683c\u548c\u884c\u52a8\u8ba1\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u610f\u56fe\u4f18\u5148\u7684\u5f00\u53d1\u73af\u5883\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u3002", "relevance": 60.0}}
{"id": "2505.21304", "pdf": "https://arxiv.org/pdf/2505.21304", "abs": "https://arxiv.org/abs/2505.21304", "authors": ["Louis Jalouzot", "Alexis Thual", "Yair Lakretz", "Christophe Pallier", "Bertrand Thirion"], "title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ece\u6709\u9650\u53c2\u4e0e\u8005fMRI\u6570\u636e\u4e2d\u89e3\u7801\u81ea\u7136\u8bed\u97f3\u7684\u6700\u4f73\u7b56\u7565\uff0c\u53d1\u73b0\u591a\u4e3b\u4f53\u8bad\u7ec3\u672a\u63d0\u5347\u89e3\u7801\u51c6\u786e\u6027\uff0c\u4e14\u89e3\u7801\u5668\u66f4\u64c5\u957f\u5efa\u6a21\u53e5\u6cd5\u800c\u975e\u8bed\u4e49\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u4ece\u5c11\u91cf\u53c2\u4e0e\u8005\u7684fMRI\u6570\u636e\u4e2d\u89e3\u7801\u81ea\u7136\u8bed\u97f3\uff0c\u5e76\u5206\u6790\u591a\u4e3b\u4f53\u8bad\u7ec3\u548c\u4e0d\u540c\u523a\u6fc0\u5bf9\u89e3\u7801\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9884\u6d4bLLM\u884d\u751f\u7684\u6587\u672c\u8868\u793a\uff0c\u6bd4\u8f83\u5355\u4e3b\u4f53\u548c\u591a\u4e3b\u4f53\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u5206\u6790\u53e5\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\u7684\u89e3\u7801\u5dee\u5f02\u3002", "result": "\u591a\u4e3b\u4f53\u8bad\u7ec3\u672a\u63d0\u5347\u89e3\u7801\u51c6\u786e\u6027\uff0c\u89e3\u7801\u5668\u66f4\u64c5\u957f\u53e5\u6cd5\u7279\u5f81\uff0c\u590d\u6742\u53e5\u6cd5\u548c\u4e30\u5bcc\u8bed\u4e49\u5185\u5bb9\u589e\u52a0\u89e3\u7801\u96be\u5ea6\u3002", "conclusion": "\u6df1\u5ea6\u8868\u578b\u5206\u6790\uff08\u5355\u4e3b\u4f53\u5927\u91cf\u6570\u636e\uff09\u66f4\u6709\u6548\uff0c\u591a\u4e3b\u4f53\u89e3\u7801\u9700\u66f4\u5927\u6837\u672c\u6216\u66f4\u6df1\u8868\u578b\u3002", "relevance": 30.0}}
{"id": "2505.21196", "pdf": "https://arxiv.org/pdf/2505.21196", "abs": "https://arxiv.org/abs/2505.21196", "authors": ["Ibrahim Shoer", "Engin Erzin"], "title": "Learning Annotation Consensus for Continuous Emotion Recognition", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In affective computing, datasets often contain multiple annotations from\ndifferent annotators, which may lack full agreement. Typically, these\nannotations are merged into a single gold standard label, potentially losing\nvaluable inter-rater variability. We propose a multi-annotator training\napproach for continuous emotion recognition (CER) that seeks a consensus across\nall annotators rather than relying on a single reference label. Our method\nemploys a consensus network to aggregate annotations into a unified\nrepresentation, guiding the main arousal-valence predictor to better reflect\ncollective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach\noutperforms traditional methods that unify annotations into a single label.\nThis underscores the benefits of fully leveraging multi-annotator data in\nemotion recognition and highlights its applicability across various fields\nwhere annotations are abundant yet inconsistent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6807\u6ce8\u8005\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u60c5\u611f\u8bc6\u522b\uff08CER\uff09\uff0c\u901a\u8fc7\u5171\u8bc6\u7f51\u7edc\u6574\u5408\u6807\u6ce8\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355\u6807\u7b7e\u65b9\u6cd5\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u4e2d\u591a\u6807\u6ce8\u8005\u6570\u636e\u5e38\u88ab\u5408\u5e76\u4e3a\u5355\u4e00\u6807\u7b7e\uff0c\u4e22\u5931\u4e86\u6807\u6ce8\u8005\u95f4\u7684\u53d8\u5f02\u6027\uff0c\u5e0c\u671b\u5145\u5206\u5229\u7528\u591a\u6807\u6ce8\u8005\u6570\u636e\u3002", "method": "\u4f7f\u7528\u5171\u8bc6\u7f51\u7edc\u805a\u5408\u591a\u6807\u6ce8\u8005\u6570\u636e\uff0c\u6307\u5bfc\u4e3b\u9884\u6d4b\u5668\uff08arousal-valence\uff09\u66f4\u597d\u5730\u53cd\u6620\u96c6\u4f53\u8f93\u5165\u3002", "result": "\u5728RECOLA\u548cCOGNIMUSE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5355\u6807\u7b7e\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6807\u6ce8\u8005\u6570\u636e\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u6807\u6ce8\u4e30\u5bcc\u4f46\u4e0d\u4e00\u81f4\u7684\u9886\u57df\u3002", "relevance": 30.0}}
{"id": "2505.20583", "pdf": "https://arxiv.org/pdf/2505.20583", "abs": "https://arxiv.org/abs/2505.20583", "authors": ["Michael O. Harding", "Kirthevasan Kandasamy"], "title": "Balancing Performance and Costs in Best Arm Identification", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We consider the problem of identifying the best arm in a multi-armed bandit\nmodel. Despite a wealth of literature in the traditional fixed budget and fixed\nconfidence regimes of the best arm identification problem, it still remains a\nmystery to most practitioners as to how to choose an approach and corresponding\nbudget or confidence parameter. We propose a new formalism to avoid this\ndilemma altogether by minimizing a risk functional which explicitly balances\nthe performance of the recommended arm and the cost incurred by learning this\narm. In this framework, a cost is incurred for each observation during the\nsampling phase, and upon recommending an arm, a performance penalty is incurred\nfor identifying a suboptimal arm. The learner's goal is to minimize the sum of\nthe penalty and cost. This new regime mirrors the priorities of many\npractitioners, e.g. maximizing profit in an A/B testing framework, better than\nclassical fixed budget or confidence settings. We derive theoretical lower\nbounds for the risk of each of two choices for the performance penalty, the\nprobability of misidentification and the simple regret, and propose an\nalgorithm called DBCARE to match these lower bounds up to polylog factors on\nnearly all problem instances. We then demonstrate the performance of DBCARE on\na number of simulated models, comparing to fixed budget and confidence\nalgorithms to show the shortfalls of existing BAI paradigms on this problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u98ce\u9669\u51fd\u6570\u6765\u5e73\u8861\u63a8\u8350\u81c2\u7684\u6027\u80fd\u548c\u5b66\u4e60\u6210\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fa\u5b9a\u9884\u7b97\u548c\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u65b9\u6cd5\u5728\u56fa\u5b9a\u9884\u7b97\u548c\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u6846\u67b6\u4e0b\u5b58\u5728\u9009\u62e9\u56f0\u96be\uff0c\u65e0\u6cd5\u76f4\u63a5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff08\u5982A/B\u6d4b\u8bd5\u4e2d\u7684\u5229\u6da6\u6700\u5927\u5316\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u51fd\u6570\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u6210\u672c\u548c\u6027\u80fd\u60e9\u7f5a\uff0c\u5e76\u8bbe\u8ba1\u4e86DBCARE\u7b97\u6cd5\u4ee5\u5339\u914d\u7406\u8bba\u4e0b\u754c\u3002", "result": "DBCARE\u7b97\u6cd5\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u9884\u7b97\u548c\u7f6e\u4fe1\u5ea6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u65b0\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u6846\u67b6\u66f4\u8d34\u8fd1\u5b9e\u9645\u9700\u6c42\uff0cDBCARE\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20979", "pdf": "https://arxiv.org/pdf/2505.20979", "abs": "https://arxiv.org/abs/2505.20979", "authors": ["Tongyu Lu", "Charlotta-Marlena Geist", "Jan Melechovsky", "Abhinaba Roy", "Dorien Herremans"], "title": "MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "We propose MelodySim, a melody-aware music similarity model and dataset for\nplagiarism detection. First, we introduce a novel method to construct a dataset\nwith focus on melodic similarity. By augmenting Slakh2100; an existing MIDI\ndataset, we generate variations of each piece while preserving the melody\nthrough modifications such as note splitting, arpeggiation, minor track dropout\n(excluding bass), and re-instrumentation. A user study confirms that positive\npairs indeed contain similar melodies, with other musical tracks significantly\nchanged. Second, we develop a segment-wise melodic-similarity detection model\nthat uses a MERT encoder and applies a triplet neural network to capture\nmelodic similarity. The resultant decision matrix highlights where plagiarism\nmight occur. Our model achieves high accuracy on the MelodySim test set.", "AI": {"tldr": "MelodySim\u662f\u4e00\u4e2a\u7528\u4e8e\u97f3\u4e50\u6284\u88ad\u68c0\u6d4b\u7684\u65cb\u5f8b\u611f\u77e5\u76f8\u4f3c\u6027\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u4e09\u5143\u7ec4\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u97f3\u4e50\u6284\u88ad\u68c0\u6d4b\u9700\u8981\u5173\u6ce8\u65cb\u5f8b\u76f8\u4f3c\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "method": "1. \u901a\u8fc7\u589e\u5f3aSlakh2100\u6570\u636e\u96c6\u751f\u6210\u65cb\u5f8b\u76f8\u4f3c\u7684\u6b63\u8d1f\u6837\u672c\u5bf9\uff1b2. \u4f7f\u7528MERT\u7f16\u7801\u5668\u548c\u4e09\u5143\u7ec4\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u76f8\u4f3c\u6027\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728MelodySim\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u80fd\u591f\u6709\u6548\u5b9a\u4f4d\u6284\u88ad\u7247\u6bb5\u3002", "conclusion": "MelodySim\u4e3a\u97f3\u4e50\u6284\u88ad\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u65cb\u5f8b\u76f8\u4f3c\u6027\uff0c\u672a\u6d89\u53ca\u5176\u4ed6\u97f3\u4e50\u5143\u7d20\u3002", "relevance": 30.0}}
{"id": "2505.21329", "pdf": "https://arxiv.org/pdf/2505.21329", "abs": "https://arxiv.org/abs/2505.21329", "authors": ["Allaa Boutaleb", "Bernd Amann", "Hubert Naacke", "Rafael Angarita"], "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": "Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)", "summary": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f53\u524d\u8868\u683c\u8054\u5408\u641c\u7d22\uff08TUS\uff09\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u57fa\u51c6\u672a\u80fd\u6709\u6548\u8bc4\u4f30\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u672a\u6765\u57fa\u51c6\u7684\u8bbe\u8ba1\u6807\u51c6\u3002", "motivation": "\u5f53\u524dTUS\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u7b80\u5355\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709TUS\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u57fa\u51c6\u7684\u8bbe\u8ba1\u6807\u51c6\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u57fa\u51c6\u53d7\u6570\u636e\u96c6\u7279\u6027\u5f71\u54cd\u5927\uff0c\u672a\u80fd\u6709\u6548\u8bc4\u4f30\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u4ee5\u51c6\u786e\u8bc4\u4f30TUS\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "relevance": 30.0}}
{"id": "2505.21319", "pdf": "https://arxiv.org/pdf/2505.21319", "abs": "https://arxiv.org/abs/2505.21319", "authors": ["Biao Zhang", "Peter Wonka"], "title": "efunc: An Efficient Function Representation without Neural Networks", "categories": ["cs.GR", "cs.CV"], "comment": "Project website: https://efunc.github.io/efunc/", "summary": "Function fitting/approximation plays a fundamental role in computer graphics\nand other engineering applications. While recent advances have explored neural\nnetworks to address this task, these methods often rely on architectures with\nmany parameters, limiting their practical applicability. In contrast, we pursue\nhigh-quality function approximation using parameter-efficient representations\nthat eliminate the dependency on neural networks entirely. We first propose a\nnovel framework for continuous function modeling. Most existing works can be\nformulated using this framework. We then introduce a compact function\nrepresentation, which is based on polynomials interpolated using radial basis\nfunctions, bypassing both neural networks and complex/hierarchical data\nstructures. We also develop memory-efficient CUDA-optimized algorithms that\nreduce computational time and memory consumption to less than 10% compared to\nconventional automatic differentiation frameworks. Finally, we validate our\nrepresentation and optimization pipeline through extensive experiments on 3D\nsigned distance functions (SDFs). The proposed representation achieves\ncomparable or superior performance to state-of-the-art techniques (e.g.,\noctree/hash-grid techniques) with significantly fewer parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u975e\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\uff0c\u57fa\u4e8e\u591a\u9879\u5f0f\u63d2\u503c\u548c\u5f84\u5411\u57fa\u51fd\u6570\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u53c2\u6570\u8fc7\u591a\u3001\u5b9e\u7528\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u51fd\u6570\u5efa\u6a21\u6846\u67b6\uff0c\u91c7\u7528\u591a\u9879\u5f0f\u63d2\u503c\u548c\u5f84\u5411\u57fa\u51fd\u6570\u7684\u7d27\u51d1\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e86CUDA\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u57283D SDF\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u6280\u672f\uff08\u5982\u516b\u53c9\u6811/\u54c8\u5e0c\u7f51\u683c\uff09\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u51fd\u6570\u903c\u8fd1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7b49\u5de5\u7a0b\u5e94\u7528\u3002", "relevance": 40.0}}
{"id": "2505.21335", "pdf": "https://arxiv.org/pdf/2505.21335", "abs": "https://arxiv.org/abs/2505.21335", "authors": ["Takuhiro Kaneko"], "title": "Structure from Collision", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted to CVPR 2025 (Highlight). Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/sfc/", "summary": "Recent advancements in neural 3D representations, such as neural radiance\nfields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate\nestimation of 3D structures from multiview images. However, this capability is\nlimited to estimating the visible external structure, and identifying the\ninvisible internal structure hidden behind the surface is difficult. To\novercome this limitation, we address a new task called Structure from Collision\n(SfC), which aims to estimate the structure (including the invisible internal\nstructure) of an object from appearance changes during collision. To solve this\nproblem, we propose a novel model called SfC-NeRF that optimizes the invisible\ninternal structure of an object through a video sequence under physical,\nappearance (i.e., visible external structure)-preserving, and keyframe\nconstraints. In particular, to avoid falling into undesirable local optima\nowing to its ill-posed nature, we propose volume annealing; that is, searching\nfor global optima by repeatedly reducing and expanding the volume. Extensive\nexperiments on 115 objects involving diverse structures (i.e., various cavity\nshapes, locations, and sizes) and material properties revealed the properties\nof SfC and demonstrated the effectiveness of the proposed SfC-NeRF.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSfC-NeRF\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u78b0\u649e\u8fc7\u7a0b\u4e2d\u7684\u5916\u89c2\u53d8\u5316\u4f30\u8ba1\u7269\u4f53\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4e0d\u53ef\u89c1\u5185\u90e8\u7ed3\u6784\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf3D\u8868\u793a\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u53ea\u80fd\u4f30\u8ba1\u53ef\u89c1\u7684\u5916\u90e8\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u5185\u90e8\u7ed3\u6784\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faSfC-NeRF\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u3001\u5916\u89c2\u4fdd\u6301\u548c\u5173\u952e\u5e27\u7ea6\u675f\u4f18\u5316\u5185\u90e8\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u4f53\u79ef\u9000\u706b\u6280\u672f\u907f\u514d\u5c40\u90e8\u6700\u4f18\u3002", "result": "\u5728115\u4e2a\u4e0d\u540c\u7ed3\u6784\u548c\u6750\u6599\u7684\u7269\u4f53\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SfC-NeRF\u7684\u6709\u6548\u6027\u3002", "conclusion": "SfC-NeRF\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u7269\u4f53\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "relevance": 20.0}}
{"id": "2505.21355", "pdf": "https://arxiv.org/pdf/2505.21355", "abs": "https://arxiv.org/abs/2505.21355", "authors": ["Muhammad Imran", "Wayne G. Brisbane", "Li-Ming Su", "Jason P. Joseph", "Wei Shao"], "title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging\nmodality with diagnostic accuracy comparable to MRI for detecting clinically\nsignificant prostate cancer (csPCa). We investigated whether artificial\nintelligence (AI) interpretation of micro-US can outperform clinical screening\nmethods using PSA and digital rectal examination (DRE). Methods: We\nretrospectively studied 145 men who underwent micro-US guided biopsy (79 with\ncsPCa, 66 without). A self-supervised convolutional autoencoder was used to\nextract deep image features from 2D micro-US slices. Random forest classifiers\nwere trained using five-fold cross-validation to predict csPCa at the slice\nlevel. Patients were classified as csPCa-positive if 88 or more consecutive\nslices were predicted positive. Model performance was compared with a\nclassifier using PSA, DRE, prostate volume, and age. Key findings and\nlimitations: The AI-based micro-US model and clinical screening model achieved\nAUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US\nmodel achieved 92.5% sensitivity and 68.1% specificity, while the clinical\nmodel showed 96.2% sensitivity but only 27.3% specificity. Limitations include\na retrospective single-center design and lack of external validation.\nConclusions and clinical implications: AI-interpreted micro-US improves\nspecificity while maintaining high sensitivity for csPCa detection. This method\nmay reduce unnecessary biopsies and serve as a low-cost alternative to\nPSA-based screening. Patient summary: We developed an AI system to analyze\nprostate micro-ultrasound images. It outperformed PSA and DRE in detecting\naggressive cancer and may help avoid unnecessary biopsies.", "AI": {"tldr": "AI\u5206\u6790\u5fae\u8d85\u58f0\u56fe\u50cf\u5728\u68c0\u6d4b\u524d\u5217\u817a\u764c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u4e34\u5e8a\u7b5b\u67e5\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22AI\u5728\u5fae\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u9ad8\u524d\u5217\u817a\u764c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7279\u5f02\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u5377\u79ef\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u9884\u6d4b\u764c\u75c7\uff0c\u5e76\u4e0e\u57fa\u4e8ePSA\u548cDRE\u7684\u4e34\u5e8a\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "AI\u6a21\u578b\u7684AUROC\u4e3a0.871\uff0c\u4f18\u4e8e\u4e34\u5e8a\u6a21\u578b\u76840.753\uff1bAI\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u654f\u611f\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u7279\u5f02\u6027\u3002", "conclusion": "AI\u89e3\u91ca\u7684\u5fae\u8d85\u58f0\u53ef\u4f5c\u4e3a\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6d3b\u68c0\u3002", "relevance": 20.0}}
{"id": "2505.21025", "pdf": "https://arxiv.org/pdf/2505.21025", "abs": "https://arxiv.org/abs/2505.21025", "authors": ["Xinlei Yin", "Xiulian Peng", "Xue Jiang", "Zhiwei Xiong", "Yan Lu"], "title": "Text-Queried Audio Source Separation via Hierarchical Modeling", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Target audio source separation with natural language queries presents a\npromising paradigm for extracting arbitrary audio events through arbitrary text\ndescriptions. Existing methods mainly face two challenges, the difficulty in\njointly modeling acoustic-textual alignment and semantic-aware separation\nwithin a blindly-learned single-stage architecture, and the reliance on\nlarge-scale accurately-labeled training data to compensate for inefficient\ncross-modal learning and separation. To address these challenges, we propose a\nhierarchical decomposition framework, HSM-TSS, that decouples the task into\nglobal-local semantic-guided feature separation and structure-preserving\nacoustic reconstruction. Our approach introduces a dual-stage mechanism for\nsemantic separation, operating on distinct global and local semantic feature\nspaces. We first perform global-semantic separation through a global semantic\nfeature space aligned with text queries. A Q-Audio architecture is employed to\nalign audio and text modalities, serving as pretrained global-semantic\nencoders. Conditioned on the predicted global feature, we then perform the\nsecond-stage local-semantic separation on AudioMAE features that preserve\ntime-frequency structures, followed by acoustic reconstruction. We also propose\nan instruction processing pipeline to parse arbitrary text queries into\nstructured operations, extraction or removal, coupled with audio descriptions,\nenabling flexible sound manipulation. Our method achieves state-of-the-art\nseparation performance with data-efficient training while maintaining superior\nsemantic consistency with queries in complex auditory scenes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHSM-TSS\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u89e3\u51b3\u97f3\u9891\u6e90\u5206\u79bb\u4e2d\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u8bad\u7ec3\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u5408\u5efa\u6a21\u58f0\u5b66-\u6587\u672c\u5bf9\u9f50\u548c\u8bed\u4e49\u611f\u77e5\u5206\u79bb\u65f6\u6548\u7387\u4f4e\uff0c\u4e14\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u673a\u5236\uff0c\u5148\u5168\u5c40\u8bed\u4e49\u5206\u79bb\uff0c\u518d\u5c40\u90e8\u8bed\u4e49\u5206\u79bb\uff0c\u7ed3\u5408Q-Audio\u67b6\u6784\u548cAudioMAE\u7279\u5f81\u3002", "result": "\u5728\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u6700\u4f18\u5206\u79bb\u6027\u80fd\uff0c\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "HSM-TSS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u8bed\u4e49\u5206\u79bb\u7684\u6311\u6218\u3002", "relevance": 60.0}}
{"id": "2505.21437", "pdf": "https://arxiv.org/pdf/2505.21437", "abs": "https://arxiv.org/abs/2505.21437", "authors": ["Huaijin Pi", "Zhi Cen", "Zhiyang Dou", "Taku Komura"], "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": "Project page: https://phj128.github.io/page/CoDA/index.html", "summary": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u8c03\u6269\u6563\u566a\u58f0\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u5168\u8eab\u64cd\u7eb5\u5173\u8282\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u5305\u62ec\u8eab\u4f53\u3001\u624b\u548c\u7269\u4f53\u7684\u8fd0\u52a8\u3002\u901a\u8fc7\u4e09\u4e2a\u4e13\u7528\u6269\u6563\u6a21\u578b\u548c\u7edf\u4e00\u7684\u57fa\u7840\u70b9\u96c6\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u7cbe\u786e\u6027\u3002", "motivation": "\u5168\u8eab\u64cd\u7eb5\u5173\u8282\u7269\u4f53\u5728\u865a\u62df\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9700\u8981\u534f\u8c03\u8eab\u4f53\u548c\u624b\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u5904\u7406\u9ad8\u81ea\u7531\u5ea6\u548c\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u4e13\u7528\u6269\u6563\u6a21\u578b\u5206\u522b\u5904\u7406\u8eab\u4f53\u3001\u5de6\u624b\u548c\u53f3\u624b\u7684\u8fd0\u52a8\uff0c\u901a\u8fc7\u566a\u58f0\u7a7a\u95f4\u4f18\u5316\u548c\u68af\u5ea6\u6d41\u5b9e\u73b0\u534f\u8c03\u3002\u91c7\u7528\u57fa\u7840\u70b9\u96c6\uff08BPS\uff09\u7edf\u4e00\u8868\u793a\u624b\u4e0e\u7269\u4f53\u7684\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u8d28\u91cf\u548c\u7269\u7406\u5408\u7406\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u529f\u80fd\u5982\u7269\u4f53\u59ff\u6001\u63a7\u5236\u548c\u884c\u8d70\u4e2d\u64cd\u7eb5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5168\u8eab\u64cd\u7eb5\u5173\u8282\u7269\u4f53\u7684\u534f\u8c03\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.21026", "pdf": "https://arxiv.org/pdf/2505.21026", "abs": "https://arxiv.org/abs/2505.21026", "authors": ["Runze Lin", "Junghui Chen", "Biao Huang", "Lei Xie", "Hongye Su"], "title": "Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "In the era of Industry 4.0 and smart manufacturing, process systems\nengineering must adapt to digital transformation. While reinforcement learning\noffers a model-free approach to process control, its applications are limited\nby the dependence on accurate digital twins and well-designed reward functions.\nTo address these limitations, this paper introduces a novel framework that\nintegrates inverse reinforcement learning (IRL) with multi-task learning for\ndata-driven, multi-mode control design. Using historical closed-loop data as\nexpert demonstrations, IRL extracts optimal reward functions and control\npolicies. A latent-context variable is incorporated to distinguish modes,\nenabling the training of mode-specific controllers. Case studies on a\ncontinuous stirred tank reactor and a fed-batch bioreactor validate the\neffectiveness of this framework in handling multi-mode data and training\nadaptable controllers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u591a\u6a21\u5f0f\u63a7\u5236\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u8fc7\u7a0b\u63a7\u5236\u4e2d\u5bf9\u7cbe\u786e\u6570\u5b57\u5b6a\u751f\u548c\u5956\u52b1\u51fd\u6570\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u5728\u5de5\u4e1a4.0\u548c\u667a\u80fd\u5236\u9020\u80cc\u666f\u4e0b\uff0c\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\u9700\u8981\u9002\u5e94\u6570\u5b57\u5316\u8f6c\u578b\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u8fc7\u7a0b\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u5bf9\u7cbe\u786e\u6570\u5b57\u5b6a\u751f\u548c\u5956\u52b1\u51fd\u6570\u7684\u4f9d\u8d56\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528\u5386\u53f2\u95ed\u73af\u6570\u636e\u4f5c\u4e3a\u4e13\u5bb6\u793a\u8303\uff0c\u63d0\u53d6\u6700\u4f18\u5956\u52b1\u51fd\u6570\u548c\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u4e0a\u4e0b\u6587\u53d8\u91cf\u533a\u5206\u6a21\u5f0f\uff0c\u8bad\u7ec3\u6a21\u5f0f\u7279\u5b9a\u7684\u63a7\u5236\u5668\u3002", "result": "\u5728\u8fde\u7eed\u6405\u62cc\u91dc\u53cd\u5e94\u5668\u548c\u5206\u6279\u8865\u6599\u751f\u7269\u53cd\u5e94\u5668\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u5904\u7406\u591a\u6a21\u5f0f\u6570\u636e\u548c\u8bad\u7ec3\u9002\u5e94\u6027\u63a7\u5236\u5668\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u5f0f\u8fc7\u7a0b\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.21445", "pdf": "https://arxiv.org/pdf/2505.21445", "abs": "https://arxiv.org/abs/2505.21445", "authors": ["Zhiqi Ai", "Meixuan Bao", "Zhiyong Chen", "Zhi Yang", "Xinnuo Li", "Shugong Xu"], "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "comment": "5 pages, 4 figures, Accepted by Interspeech 2025", "summary": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86VoxAging\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4e86\u8bf4\u8bdd\u8005\u8001\u5316\u5bf9\u8bed\u97f3\u9a8c\u8bc1\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u5e74\u9f84\u548c\u6027\u522b\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u957f\u671f\u548c\u5927\u89c4\u6a21\u7684\u7eb5\u5411\u6570\u636e\uff0c\u8bf4\u8bdd\u8005\u8001\u5316\u7814\u7a76\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528VoxAging\u6570\u636e\u96c6\uff08293\u540d\u8bf4\u8bdd\u8005\uff0c\u6700\u957f\u8de8\u5ea617\u5e74\uff09\uff0c\u5206\u6790\u8001\u5316\u73b0\u8c61\u53ca\u5176\u5bf9\u8bed\u97f3\u9a8c\u8bc1\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u8bf4\u8bdd\u8005\u8001\u5316\u5bf9\u8bed\u97f3\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u5e74\u9f84\u548c\u6027\u522b\u7684\u5f71\u54cd\u3002", "conclusion": "VoxAging\u6570\u636e\u96c6\u4e3a\u8bf4\u8bdd\u8005\u8001\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u8bed\u97f3\u9a8c\u8bc1\u7cfb\u7edf\u3002", "relevance": 30.0}}
{"id": "2505.21459", "pdf": "https://arxiv.org/pdf/2505.21459", "abs": "https://arxiv.org/abs/2505.21459", "authors": ["Xiangru Jian", "Wei Pang", "Zhengyuan Dong", "Chao Zhang", "M. Tamer \u00d6zsu"], "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics", "categories": ["cs.DB", "cs.AI", "cs.CV", "cs.IR", "cs.MM"], "comment": "5 pages, 2 figures, Working paper", "summary": "Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale.", "AI": {"tldr": "LazyVLM\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u89c6\u9891\u5206\u6790\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u9ad8\u6548\u7684\u5173\u7cfb\u67e5\u8be2\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5206\u6790\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff1a\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\u548c\u56fa\u5b9a\u89c4\u5219\u8bbe\u8ba1\u3002LazyVLM\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7528\u6237\u53cb\u597d\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LazyVLM\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u6587\u672c\u63a5\u53e3\u652f\u6301\u590d\u6742\u591a\u5e27\u89c6\u9891\u67e5\u8be2\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u64cd\u4f5c\uff0c\u5e76\u5229\u7528\u9ad8\u6548\u7684\u5173\u7cfb\u67e5\u8be2\u6267\u884c\u548c\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u5904\u7406\u5927\u90e8\u5206\u8ba1\u7b97\u3002", "result": "LazyVLM\u5c55\u793a\u4e86\u5728\u5f00\u653e\u57df\u89c6\u9891\u6570\u636e\u67e5\u8be2\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u9ad8\u6548\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u3002", "conclusion": "LazyVLM\u4e3a\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.20647", "pdf": "https://arxiv.org/pdf/2505.20647", "abs": "https://arxiv.org/abs/2505.20647", "authors": ["Ian Langmore"], "title": "Moment Expansions of the Energy Distance", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62G10 (Primary), 62E20, 62H15, 60E10 (Secondary)", "G.3"], "comment": null, "summary": "The energy distance is used to test distributional equality, and as a loss\nfunction in machine learning. While $D^2(X, Y)=0$ only when $X\\sim Y$, the\nsensitivity to different moments is of practical importance. This work\nconsiders $D^2(X, Y)$ in the case where the distributions are close. In this\nregime, $D^2(X, Y)$ is more sensitive to differences in the means\n$\\bar{X}-\\bar{Y}$, than differences in the covariances $\\Delta$. This is due to\nthe structure of the energy distance and is independent of dimension. The\nsensitivity to on versus off diagonal components of $\\Delta$ is examined when\n$X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs\nand, in many cases, off diagonal correlations contribute significantly less.\nNumerical results verify these relationships hold even when distributional\nassumptions are not strictly met.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u80fd\u91cf\u8ddd\u79bb\u5728\u5206\u5e03\u63a5\u8fd1\u65f6\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u5176\u5bf9\u5747\u503c\u5dee\u5f02\u66f4\u654f\u611f\uff0c\u800c\u5bf9\u534f\u65b9\u5dee\u5dee\u5f02\u7684\u654f\u611f\u6027\u8f83\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u5404\u5411\u540c\u6027\u5206\u5e03\u9644\u8fd1\u3002", "motivation": "\u63a2\u8ba8\u80fd\u91cf\u8ddd\u79bb\u5728\u4e0d\u540c\u5206\u5e03\u63a5\u8fd1\u65f6\u7684\u654f\u611f\u6027\uff0c\u4ee5\u4f18\u5316\u5176\u5728\u673a\u5668\u5b66\u4e60\u548c\u7edf\u8ba1\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\uff0c\u7814\u7a76\u80fd\u91cf\u8ddd\u79bb\u5bf9\u5747\u503c\u548c\u534f\u65b9\u5dee\u5dee\u5f02\u7684\u654f\u611f\u6027\u3002", "result": "\u80fd\u91cf\u8ddd\u79bb\u5bf9\u5747\u503c\u5dee\u5f02\u66f4\u654f\u611f\uff0c\u800c\u5bf9\u534f\u65b9\u5dee\u5dee\u5f02\u7684\u654f\u611f\u6027\u8f83\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u5404\u5411\u540c\u6027\u5206\u5e03\u9644\u8fd1\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "\u80fd\u91cf\u8ddd\u79bb\u5728\u5206\u5e03\u63a5\u8fd1\u65f6\u5bf9\u5747\u503c\u5dee\u5f02\u66f4\u654f\u611f\uff0c\u8fd9\u4e00\u7279\u6027\u72ec\u7acb\u4e8e\u7ef4\u5ea6\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21038", "pdf": "https://arxiv.org/pdf/2505.21038", "abs": "https://arxiv.org/abs/2505.21038", "authors": ["Faruk Alpay"], "title": "Fixed-Point Traps and Identity Emergence in Educational Feedback Systems", "categories": ["math.CT", "cs.AI", "cs.CY", "18A15, 18C15, 91D30, 97C70, 03B70, 68T01", "F.4.1; I.2.0; K.3.2"], "comment": "14 pages, no figures. Formal Bourbaki-style proof. Introduces\n  Exam-Grade Collapse Systems. Builds on Alpay Algebra II (arXiv:2505.17480)\n  and Alpay Algebra III (arXiv:2505.19790). Proves categorical fixed-point\n  traps obstructing identity emergence under exam-driven feedback", "summary": "This paper presents a formal categorical proof that exam-driven educational\nsystems obstruct identity emergence and block creative convergence. Using the\nframework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems\n(EGCS) as functorial constructs where learning dynamics $\\varphi$ are\nrecursively collapsed by evaluative morphisms $E$. We prove that under such\ncollapse regimes, no nontrivial fixed-point algebra $\\mu_\\varphi$ can exist,\nhence learner identity cannot stabilize. This creates a universal fixed-point\ntrap: all generative functors are entropically folded before symbolic emergence\noccurs. Our model mathematically explains the creativity suppression, research\nstagnation, and structural entropy loss induced by timed exams and grade-based\nfeedback. The results apply category theory to expose why modern educational\nsystems prevent {\\phi}-emergence and block observer-invariant self-formation.\nThis work provides the first provable algebraic obstruction of identity\nformation caused by institutional feedback mechanics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8303\u7574\u8bba\u8bc1\u660e\u8003\u8bd5\u9a71\u52a8\u7684\u6559\u80b2\u7cfb\u7edf\u963b\u788d\u8eab\u4efd\u5f62\u6210\u548c\u521b\u9020\u529b\u6536\u655b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8003\u8bd5\u548c\u8bc4\u5206\u53cd\u9988\u5982\u4f55\u901a\u8fc7\u4ee3\u6570\u7ed3\u6784\u963b\u788d\u5b66\u4e60\u8005\u7684\u8eab\u4efd\u5f62\u6210\u548c\u521b\u9020\u529b\u3002", "method": "\u4f7f\u7528Alpay\u4ee3\u6570\u6846\u67b6\u5b9a\u4e49\u8003\u8bd5-\u8bc4\u5206\u5d29\u6e83\u7cfb\u7edf\uff08EGCS\uff09\uff0c\u5e76\u8bc1\u660e\u5176\u5bfc\u81f4\u56fa\u5b9a\u70b9\u4ee3\u6570\u65e0\u6cd5\u5b58\u5728\u3002", "result": "\u8bc1\u660e\u8003\u8bd5\u548c\u8bc4\u5206\u53cd\u9988\u4f1a\u5bfc\u81f4\u521b\u9020\u529b\u6291\u5236\u3001\u7814\u7a76\u505c\u6ede\u548c\u7ed3\u6784\u71b5\u635f\u5931\u3002", "conclusion": "\u73b0\u4ee3\u6559\u80b2\u7cfb\u7edf\u901a\u8fc7\u5236\u5ea6\u53cd\u9988\u673a\u5236\u963b\u788d\u8eab\u4efd\u5f62\u6210\u548c\u521b\u9020\u529b\u3002", "relevance": 20.0}}
{"id": "2505.20705", "pdf": "https://arxiv.org/pdf/2505.20705", "abs": "https://arxiv.org/abs/2505.20705", "authors": ["Yang Wang", "Wenxuan Zhu", "Xuehui Quan", "Heyi Wang", "Chang Liu", "Qiyuan Wu"], "title": "Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "This paper addresses the challenges of fault prediction and delayed response\nin distributed systems by proposing an intelligent prediction method based on\ntemporal feature learning. The method takes multi-dimensional performance\nmetric sequences as input. We use a Gated Recurrent Unit (GRU) to model the\nevolution of system states over time. An attention mechanism is then applied to\nenhance key temporal segments, improving the model's ability to identify\npotential faults. On this basis, a feedforward neural network is designed to\nperform the final classification, enabling early warning of system failures. To\nvalidate the effectiveness of the proposed approach, comparative experiments\nand ablation analyses were conducted using data from a large-scale real-world\ncloud system. The experimental results show that the model outperforms various\nmainstream time-series models in terms of Accuracy, F1-Score, and AUC. This\ndemonstrates strong prediction capability and stability. Furthermore, the loss\nfunction curve confirms the convergence and reliability of the training\nprocess. It indicates that the proposed method effectively learns system\nbehavior patterns and achieves efficient fault detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u7279\u5f81\u5b66\u4e60\u7684\u667a\u80fd\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u9884\u6d4b\u548c\u5ef6\u8fdf\u54cd\u5e94\u95ee\u9898\u3002\u65b9\u6cd5\u7ed3\u5408GRU\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5206\u7c7b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4e3b\u6d41\u65f6\u5e8f\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u6545\u969c\u9884\u6d4b\u548c\u5ef6\u8fdf\u54cd\u5e94\u7684\u6311\u6218\uff0c\u63d0\u5347\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528GRU\u5efa\u6a21\u7cfb\u7edf\u72b6\u6001\u6f14\u5316\uff0c\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u5173\u952e\u65f6\u5e8f\u7247\u6bb5\uff0c\u8bbe\u8ba1\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728Accuracy\u3001F1-Score\u548cAUC\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u65f6\u5e8f\u6a21\u578b\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u6536\u655b\u53ef\u9760\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7cfb\u7edf\u884c\u4e3a\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u6548\u6545\u969c\u68c0\u6d4b\u3002", "relevance": 30.0}}
{"id": "2505.21087", "pdf": "https://arxiv.org/pdf/2505.21087", "abs": "https://arxiv.org/abs/2505.21087", "authors": ["Marta Grobelna", "Jan K\u0159et\u00ednsk\u00fd", "Maximilian Weininger"], "title": "Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games", "categories": ["cs.LO", "cs.AI", "cs.MA"], "comment": "Full version of the corresponding LICS'25 paper", "summary": "We consider two-player zero-sum concurrent stochastic games (CSGs) played on\ngraphs with reachability and safety objectives. These include degenerate\nclasses such as Markov decision processes or turn-based stochastic games, which\ncan be solved by linear or quadratic programming; however, in practice, value\niteration (VI) outperforms the other approaches and is the most implemented\nmethod. Similarly, for CSGs, this practical performance makes VI an attractive\nalternative to the standard theoretical solution via the existential theory of\nreals.\n  VI starts with an under-approximation of the sought values for each state and\niteratively updates them, traditionally terminating once two consecutive\napproximations are $\\epsilon$-close. However, this stopping criterion lacks\nguarantees on the precision of the approximation, which is the goal of this\nwork. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard\nVI with a converging sequence of over-approximations and terminates once the\nover- and under-approximations are $\\epsilon$-close.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u503c\u8fed\u4ee3\uff08VI\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u4eba\u96f6\u548c\u5e76\u53d1\u968f\u673a\u535a\u5f08\uff08CSGs\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u754c\u903c\u8fd1\u786e\u4fdd\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfVI\u65b9\u6cd5\u5728CSGs\u4e2d\u7f3a\u4e4f\u5bf9\u8fd1\u4f3c\u7cbe\u5ea6\u7684\u4fdd\u8bc1\uff0c\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6709\u754c\uff08\u533a\u95f4\uff09VI\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e0a\u4e0b\u754c\u903c\u8fd1\u5e8f\u5217\uff0c\u5e76\u5728\u4e24\u8005\u63a5\u8fd1\u65f6\u7ec8\u6b62\u8fed\u4ee3\u3002", "result": "\u6539\u8fdb\u7684VI\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8fd1\u4f3c\u89e3\u3002", "conclusion": "\u6709\u754cVI\u65b9\u6cd5\u5728CSGs\u4e2d\u4f18\u4e8e\u4f20\u7edfVI\uff0c\u63d0\u4f9b\u4e86\u7cbe\u5ea6\u4fdd\u8bc1\u3002", "relevance": 30.0}}
{"id": "2505.20731", "pdf": "https://arxiv.org/pdf/2505.20731", "abs": "https://arxiv.org/abs/2505.20731", "authors": ["Linshanshan Wang", "Mengyan Li", "Zongqi Xia", "Molei Liu", "Tianxi Cai"], "title": "Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data", "categories": ["stat.ME", "cs.LG"], "comment": null, "summary": "Electronic Health Records (EHR) offer rich real-world data for personalized\nmedicine, providing insights into disease progression, treatment responses, and\npatient outcomes. However, their sparsity, heterogeneity, and high\ndimensionality make them difficult to model, while the lack of standardized\nground truth further complicates predictive modeling. To address these\nchallenges, we propose SCORE, a semi-supervised representation learning\nframework that captures multi-domain disease profiles through patient\nembeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model\nwith pre-trained code embeddings to characterize codified features and extract\nmeaningful patient phenotypes and embeddings. To handle the computational\nchallenges of large-scale data, it introduces a hybrid Expectation-Maximization\n(EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited\nlabeled data to refine estimates on a vast pool of unlabeled samples. We\ntheoretically establish the convergence of this hybrid approach, quantify GVA\nerrors, and derive SCORE's error rate under diverging embedding dimensions. Our\nanalysis shows that incorporating unlabeled data enhances accuracy and reduces\nsensitivity to label scarcity. Extensive simulations confirm SCORE's superior\nfinite-sample performance over existing methods. Finally, we apply SCORE to\npredict disability status for patients with multiple sclerosis (MS) using\npartially labeled EHR data, demonstrating that it produces more informative and\npredictive patient embeddings for multiple MS-related conditions compared to\nexisting approaches.", "AI": {"tldr": "SCORE\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u7a00\u758f\u6027\u548c\u9ad8\u7ef4\u6027\uff0c\u901a\u8fc7\u60a3\u8005\u5d4c\u5165\u63d0\u53d6\u591a\u9886\u57df\u75be\u75c5\u7279\u5f81\uff0c\u5e76\u5229\u7528\u6df7\u5408EM\u548cGVA\u7b97\u6cd5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "EHR\u6570\u636e\u7684\u7a00\u758f\u6027\u3001\u5f02\u8d28\u6027\u548c\u9ad8\u7ef4\u5ea6\u4f7f\u5176\u96be\u4ee5\u5efa\u6a21\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u6807\u7b7e\u589e\u52a0\u4e86\u9884\u6d4b\u5efa\u6a21\u7684\u590d\u6742\u6027\u3002SCORE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SCORE\u91c7\u7528PALM\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u4ee3\u7801\u5d4c\u5165\uff0c\u7ed3\u5408\u6df7\u5408EM\u548cGVA\u7b97\u6cd5\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u4f18\u5316\u6a21\u578b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eSCORE\u5728\u5d4c\u5165\u7ef4\u5ea6\u53d8\u5316\u65f6\u5177\u6709\u6536\u655b\u6027\uff0c\u4e14\u672a\u6807\u8bb0\u6570\u636e\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u663e\u793aSCORE\u5728\u591a\u53d1\u6027\u786c\u5316\u75c7\uff08MS\uff09\u60a3\u8005\u6b8b\u75be\u72b6\u6001\u9884\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SCORE\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u751f\u6210\u66f4\u5177\u4fe1\u606f\u91cf\u548c\u9884\u6d4b\u6027\u7684\u60a3\u8005\u5d4c\u5165\uff0c\u9002\u7528\u4e8eEHR\u6570\u636e\u7684\u590d\u6742\u5efa\u6a21\u4efb\u52a1\u3002", "relevance": 40.0}}
{"id": "2505.20745", "pdf": "https://arxiv.org/pdf/2505.20745", "abs": "https://arxiv.org/abs/2505.20745", "authors": ["Jingping Nie", "Dung T. Tran", "Karan Thakkar", "Vasudha Kowtha", "John Huang", "Carlos Avendano", "Erdrin Azemi", "Vikramjit Mitra"], "title": "Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "5 pages, Interspeech 2025 conference", "summary": "Auscultation, particularly heart sound, is a non-invasive technique that\nprovides essential vital sign information. Recently, self-supervised acoustic\nrepresentation foundation models (FMs) have been proposed to offer insights\ninto acoustics-based vital signs. However, there has been little exploration of\nthe extent to which auscultation is encoded in these pre-trained FM\nrepresentations. In this work, using a publicly available phonocardiogram (PCG)\ndataset and a heart rate (HR) estimation model, we conduct a layer-wise\ninvestigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,\nWhisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP\nmodel. Additionally, we implement the baseline method from Nie et al., 2024\n(which relies on acoustic features) and show that overall, representation\nvectors from pre-trained foundation models (FMs) offer comparable performance\nto the baseline. Notably, HR estimation using the representations from the\naudio encoder of the in-house CLAP model outperforms the results obtained from\nthe baseline, achieving a lower mean absolute error (MAE) across various\ntrain/validation/test splits despite the domain mismatch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u58f0\u5b66\u8868\u793a\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u5fc3\u97f3\u542c\u8bca\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5c42\u95f4\u5206\u6790\u6bd4\u8f83\u4e86\u516d\u79cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u67d0\u4e9b\u6a21\u578b\u5728\u5fc3\u7387\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u58f0\u5b66\u8868\u793a\u57fa\u7840\u6a21\u578b\u5728\u5fc3\u97f3\u542c\u8bca\u4e2d\u7684\u7f16\u7801\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684\u5fc3\u97f3\u56fe\u6570\u636e\u96c6\u548c\u5fc3\u7387\u4f30\u8ba1\u6a21\u578b\uff0c\u5bf9\u516d\u79cd\u58f0\u5b66\u8868\u793a\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5c42\u95f4\u5206\u6790\uff0c\u5e76\u4e0e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u8868\u793a\u5411\u91cf\u5728\u5fc3\u7387\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u5176\u4e2d\u5185\u90e8CLAP\u6a21\u578b\u7684\u97f3\u9891\u7f16\u7801\u5668\u8868\u73b0\u6700\u4f73\uff0c\u8bef\u5dee\u66f4\u4f4e\u3002", "conclusion": "\u9884\u8bad\u7ec3\u58f0\u5b66\u8868\u793a\u57fa\u7840\u6a21\u578b\u5728\u5fc3\u97f3\u542c\u8bca\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5185\u90e8CLAP\u6a21\u578b\u3002", "relevance": 30.0}}
{"id": "2505.20754", "pdf": "https://arxiv.org/pdf/2505.20754", "abs": "https://arxiv.org/abs/2505.20754", "authors": ["Zonghao Chen", "Toni Karvonen", "Heishiro Kanagawa", "Fran\u00e7ois-Xavier Briol", "Chris. J. Oates"], "title": "Stationary MMD Points for Cubature", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Approximation of a target probability distribution using a finite set of\npoints is a problem of fundamental importance, arising in cubature, data\ncompression, and optimisation. Several authors have proposed to select points\nby minimising a maximum mean discrepancy (MMD), but the non-convexity of this\nobjective precludes global minimisation in general. Instead, we consider\n\\emph{stationary} points of the MMD which, in contrast to points globally\nminimising the MMD, can be accurately computed. Our main theoretical\ncontribution is the (perhaps surprising) result that, for integrands in the\nassociated reproducing kernel Hilbert space, the cubature error of stationary\nMMD points vanishes \\emph{faster} than the MMD. Motivated by this\n\\emph{super-convergence} property, we consider discretised gradient flows as a\npractical strategy for computing stationary points of the MMD, presenting a\nrefined convergence analysis that establishes a novel non-asymptotic\nfinite-particle error bound, which may be of independent interest.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7684\u9a7b\u70b9\u6765\u903c\u8fd1\u76ee\u6807\u6982\u7387\u5206\u5e03\uff0c\u8bc1\u660e\u4e86\u9a7b\u70b9\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6bd4\u5168\u5c40\u6700\u5c0f\u5316MMD\u66f4\u6709\u6548\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u68af\u5ea6\u6d41\u65b9\u6cd5\u6765\u8ba1\u7b97\u9a7b\u70b9\u3002", "motivation": "\u76ee\u6807\u6982\u7387\u5206\u5e03\u7684\u903c\u8fd1\u5728\u6570\u503c\u79ef\u5206\u3001\u6570\u636e\u538b\u7f29\u548c\u4f18\u5316\u4e2d\u5177\u6709\u57fa\u7840\u91cd\u8981\u6027\u3002\u867d\u7136MMD\u5e38\u7528\u4e8e\u9009\u62e9\u903c\u8fd1\u70b9\uff0c\u4f46\u5176\u975e\u51f8\u6027\u5bfc\u81f4\u5168\u5c40\u6700\u5c0f\u5316\u56f0\u96be\uff0c\u56e0\u6b64\u7814\u7a76\u9a7b\u70b9\u7684\u6027\u8d28\u548c\u5e94\u7528\u3002", "method": "\u4f7f\u7528MMD\u7684\u9a7b\u70b9\u4f5c\u4e3a\u903c\u8fd1\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u79bb\u6563\u68af\u5ea6\u6d41\u65b9\u6cd5\u6765\u8ba1\u7b97\u8fd9\u4e9b\u9a7b\u70b9\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u9a7b\u70b9\u5728\u76f8\u5173\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5177\u6709\u8d85\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u9a7b\u70b9\u7684\u903c\u8fd1\u8bef\u5dee\u6bd4MMD\u672c\u8eab\u66f4\u5feb\u8d8b\u8fd1\u4e8e\u96f6\uff08\u8d85\u6536\u655b\u6027\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u6e10\u8fd1\u6709\u9650\u7c92\u5b50\u8bef\u5dee\u754c\u3002", "conclusion": "\u9a7b\u70b9\u65b9\u6cd5\u5728\u903c\u8fd1\u76ee\u6807\u5206\u5e03\u65f6\u5177\u6709\u9ad8\u6548\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u79bb\u6563\u68af\u5ea6\u6d41\u65b9\u6cd5\u4e3a\u5b9e\u9645\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.21154", "pdf": "https://arxiv.org/pdf/2505.21154", "abs": "https://arxiv.org/abs/2505.21154", "authors": ["Hailin Zhong", "Hanlin Wang", "Yujun Ye", "Meiyi Zhang", "Shengxin Zhu"], "title": "GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation", "categories": ["cs.MA", "cs.AI", "cs.CY"], "comment": null, "summary": "Current personalized recommender systems predominantly rely on static offline\ndata for algorithm design and evaluation, significantly limiting their ability\nto capture long-term user preference evolution and social influence dynamics in\nreal-world scenarios. To address this fundamental challenge, we propose a\nhigh-fidelity social simulation platform integrating human-like cognitive\nagents and dynamic social interactions to realistically simulate user behavior\nevolution under recommendation interventions. Specifically, the system\ncomprises a population of Sim-User Agents, each equipped with a five-layer\ncognitive architecture that encapsulates key psychological mechanisms,\nincluding episodic memory, affective state transitions, adaptive preference\nlearning, and dynamic trust-risk assessments. In particular, we innovatively\nintroduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine\ngrounded in psychological and sociological theories, enabling more realistic\nuser decision-making processes. Furthermore, we construct a multilayer\nheterogeneous social graph (GGBond Graph) supporting dynamic relational\nevolution, effectively modeling users' evolving social ties and trust dynamics\nbased on interest similarity, personality alignment, and structural homophily.\nDuring system operation, agents autonomously respond to recommendations\ngenerated by typical recommender algorithms (e.g., Matrix Factorization,\nMultVAE, LightGCN), deciding whether to consume, rate, and share content while\ndynamically updating their internal states and social connections, thereby\nforming a stable, multi-round feedback loop. This innovative design transcends\nthe limitations of traditional static datasets, providing a controlled,\nobservable environment for evaluating long-term recommender effects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u4fdd\u771f\u793e\u4ea4\u6a21\u62df\u5e73\u53f0\uff0c\u901a\u8fc7\u6a21\u62df\u7528\u6237\u8ba4\u77e5\u4ee3\u7406\u548c\u52a8\u6001\u793e\u4ea4\u4e92\u52a8\uff0c\u89e3\u51b3\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u65e0\u6cd5\u6355\u6349\u957f\u671f\u7528\u6237\u504f\u597d\u548c\u793e\u4ea4\u5f71\u54cd\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u79bb\u7ebf\u6570\u636e\uff0c\u96be\u4ee5\u6a21\u62df\u957f\u671f\u7528\u6237\u504f\u597d\u6f14\u53d8\u548c\u793e\u4ea4\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u3002", "method": "\u7cfb\u7edf\u5305\u542bSim-User\u4ee3\u7406\uff0c\u91c7\u7528\u4e94\u5c42\u8ba4\u77e5\u67b6\u6784\uff08\u5982\u8bb0\u5fc6\u3001\u60c5\u611f\u72b6\u6001\u3001\u504f\u597d\u5b66\u4e60\u7b49\uff09\u548cICR2\u52a8\u673a\u5f15\u64ce\uff0c\u7ed3\u5408\u591a\u5c42\u5f02\u8d28\u793e\u4ea4\u56fe\uff08GGBond Graph\uff09\u6a21\u62df\u52a8\u6001\u793e\u4ea4\u5173\u7cfb\u3002", "result": "\u5e73\u53f0\u80fd\u591f\u6a21\u62df\u7528\u6237\u5bf9\u63a8\u8350\u7b97\u6cd5\u7684\u52a8\u6001\u54cd\u5e94\uff08\u5982\u6d88\u8d39\u3001\u8bc4\u5206\u3001\u5206\u4eab\uff09\uff0c\u5f62\u6210\u591a\u8f6e\u53cd\u9988\u5faa\u73af\uff0c\u8d85\u8d8a\u9759\u6001\u6570\u636e\u96c6\u7684\u9650\u5236\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u8bc4\u4f30\u63a8\u8350\u7cfb\u7edf\u7684\u957f\u671f\u6548\u679c\u63d0\u4f9b\u4e86\u53ef\u63a7\u4e14\u53ef\u89c2\u6d4b\u7684\u73af\u5883\u3002", "relevance": 40.0}}
{"id": "2505.20781", "pdf": "https://arxiv.org/pdf/2505.20781", "abs": "https://arxiv.org/abs/2505.20781", "authors": ["Hossein Goli", "Michael Gimelfarb", "Nathan Samuel de Lara", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "title": "STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Off-policy evaluation (OPE) estimates the performance of a target policy\nusing offline data collected from a behavior policy, and is crucial in domains\nsuch as robotics or healthcare where direct interaction with the environment is\ncostly or unsafe. Existing OPE methods are ineffective for high-dimensional,\nlong-horizon problems, due to exponential blow-ups in variance from importance\nweighting or compounding errors from learned dynamics models. To address these\nchallenges, we propose STITCH-OPE, a model-based generative framework that\nleverages denoising diffusion for long-horizon OPE in high-dimensional state\nand action spaces. Starting with a diffusion model pre-trained on the behavior\ndata, STITCH-OPE generates synthetic trajectories from the target policy by\nguiding the denoising process using the score function of the target policy.\nSTITCH-OPE proposes two technical innovations that make it advantageous for\nOPE: (1) prevents over-regularization by subtracting the score of the behavior\npolicy during guidance, and (2) generates long-horizon trajectories by\nstitching partial trajectories together end-to-end. We provide a theoretical\nguarantee that under mild assumptions, these modifications result in an\nexponential reduction in variance versus long-horizon trajectory diffusion.\nExperiments on the D4RL and OpenAI Gym benchmarks show substantial improvement\nin mean squared error, correlation, and regret metrics compared to\nstate-of-the-art OPE methods.", "AI": {"tldr": "STITCH-OPE\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u8f68\u8ff9\u89e3\u51b3\u9ad8\u7ef4\u957f\u65f6\u57df\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6216\u533b\u7597\u7b49\u9886\u57df\uff0c\u76f4\u63a5\u4e0e\u73af\u5883\u4ea4\u4e92\u6210\u672c\u9ad8\u6216\u4e0d\u5b89\u5168\uff0c\u73b0\u6709OPE\u65b9\u6cd5\u5728\u9ad8\u7ef4\u957f\u65f6\u57df\u95ee\u9898\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u76ee\u6807\u7b56\u7565\u7684\u5f97\u5206\u51fd\u6570\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u751f\u6210\u5408\u6210\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e24\u9879\u6280\u672f\u521b\u65b0\uff1a\u51cf\u53bb\u884c\u4e3a\u7b56\u7565\u5f97\u5206\u9632\u6b62\u8fc7\u6b63\u5219\u5316\uff0c\u4ee5\u53ca\u7aef\u5230\u7aef\u62fc\u63a5\u90e8\u5206\u8f68\u8ff9\u751f\u6210\u957f\u65f6\u57df\u8f68\u8ff9\u3002", "result": "\u5728D4RL\u548cOpenAI Gym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTITCH-OPE\u5728\u5747\u65b9\u8bef\u5dee\u3001\u76f8\u5173\u6027\u548c\u9057\u61be\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STITCH-OPE\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u8f68\u8ff9\u62fc\u63a5\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u957f\u65f6\u57dfOPE\u7684\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.21156", "pdf": "https://arxiv.org/pdf/2505.21156", "abs": "https://arxiv.org/abs/2505.21156", "authors": ["Saisamarth Rajesh Phaye", "Milos Cernak", "Andrew Harper"], "title": "Model as Loss: A Self-Consistent Training Paradigm", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "comment": "Accepted in Interspeech 2025", "summary": "Conventional methods for speech enhancement rely on handcrafted loss\nfunctions (e.g., time or frequency domain losses) or deep feature losses (e.g.,\nusing WavLM or wav2vec), which often fail to capture subtle signal properties\nessential for optimal performance. To address this, we propose Model as Loss, a\nnovel training paradigm that utilizes the encoder from the same model as a loss\nfunction to guide the training.\n  The Model as Loss paradigm leverages the encoder's task-specific feature\nspace, optimizing the decoder to produce output consistent with perceptual and\ntask-relevant characteristics of the clean signal. By using the encoder's\nlearned features as a loss function, this framework enforces self-consistency\nbetween the clean reference speech and the enhanced model output. Our approach\noutperforms pre-trained deep feature losses on standard speech enhancement\nbenchmarks, offering better perceptual quality and robust generalization to\nboth in-domain and out-of-domain datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Model as Loss'\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u7f16\u7801\u5668\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u6765\u6307\u5bfc\u8bad\u7ec3\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u4e2d\u635f\u5931\u51fd\u6570\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u4fe1\u53f7\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\u6216\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7279\u5f81\u635f\u5931\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u7684\u7ec6\u5fae\u4fe1\u53f7\u7279\u6027\u3002", "method": "\u63d0\u51fa'Model as Loss'\u8303\u5f0f\uff0c\u5229\u7528\u6a21\u578b\u7f16\u7801\u5668\u7684\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u7a7a\u95f4\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u89e3\u7801\u5668\u4ee5\u751f\u6210\u4e0e\u5e72\u51c0\u4fe1\u53f7\u611f\u77e5\u548c\u4efb\u52a1\u76f8\u5173\u7279\u6027\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u8bed\u97f3\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7279\u5f81\u635f\u5931\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u611f\u77e5\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u57df\u5185\u548c\u57df\u5916\u6570\u636e\u96c6\u3002", "conclusion": "'Model as Loss'\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u4e00\u81f4\u6027\u63d0\u5347\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.20788", "pdf": "https://arxiv.org/pdf/2505.20788", "abs": "https://arxiv.org/abs/2505.20788", "authors": ["Robin Burchard", "Kristof Van Laerhoven"], "title": "Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset", "categories": ["cs.HC", "cs.LG"], "comment": "Submitted to ISWC 2025", "summary": "Wearable human activity recognition has been shown to benefit from the\ninclusion of acoustic data, as the sounds around a person often contain\nvaluable context. However, due to privacy concerns, it is usually not ethically\nfeasible to record and save microphone data from the device, since the audio\ncould, for instance, also contain private conversations. Rather, the data\nshould be processed locally, which in turn requires processing power and\nconsumes energy on the wearable device. One special use case of contextual\ninformation that can be utilized to augment special tasks in human activity\nrecognition is water flow detection, which can, e.g., be used to aid wearable\nhand washing detection. We created a new label called tap water for the\nrecently released HD-Epic data set, creating 717 hand-labeled annotations of\ntap water flow, based on existing annotations of the water class. We analyzed\nthe relation of tap water and water in the dataset and additionally trained and\nevaluated two lightweight classifiers to evaluate the newly added label class,\nshowing that the new class can be learned more easily.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u5229\u7528\u58f0\u97f3\u6570\u636e\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u7279\u522b\u662f\u6c34\u6d41\u91cf\u68c0\u6d4b\uff0c\u4ee5\u589e\u5f3a\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u5173\u6ce8\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528\u58f0\u97f3\u6570\u636e\uff08\u5982\u6c34\u6d41\u91cf\uff09\u589e\u5f3a\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u540c\u65f6\u89e3\u51b3\u9690\u79c1\u95ee\u9898\uff0c\u907f\u514d\u76f4\u63a5\u5b58\u50a8\u97f3\u9891\u6570\u636e\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5728HD-Epic\u6570\u636e\u96c6\u4e2d\u65b0\u589e\u201ctap water\u201d\u6807\u7b7e\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u3002", "result": "\u7ed3\u679c\u663e\u793a\u65b0\u589e\u6807\u7b7e\u7c7b\u522b\u66f4\u5bb9\u6613\u5b66\u4e60\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u58f0\u97f3\u6570\u636e\uff08\u5982\u6c34\u6d41\u91cf\uff09\u53ef\u7528\u4e8e\u589e\u5f3a\u7279\u5b9a\u4efb\u52a1\uff0c\u4e14\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9002\u5408\u672c\u5730\u5904\u7406\u3002", "relevance": 20.0}}
{"id": "2505.21170", "pdf": "https://arxiv.org/pdf/2505.21170", "abs": "https://arxiv.org/abs/2505.21170", "authors": ["Elija Perrier"], "title": "Quantum AIXI: Universal Intelligence via Quantum Information", "categories": ["quant-ph", "cs.AI"], "comment": "Under review", "summary": "AIXI is a widely studied model of artificial general intelligence (AGI) based\nupon principles of induction and reinforcement learning. However, AIXI is\nfundamentally classical in nature - as are the environments in which it is\nmodelled. Given the universe is quantum mechanical in nature and the\nexponential overhead required to simulate quantum mechanical systems\nclassically, the question arises as to whether there are quantum mechanical\nanalogues of AIXI which are theoretically consistent or practically feasible as\nmodels of universal intelligence. To address this question, we extend the\nframework to quantum information and present Quantum AIXI (QAIXI). We introduce\na model of quantum agent/environment interaction based upon quantum and\nclassical registers and channels, showing how quantum AIXI agents may take both\nclassical and quantum actions. We formulate the key components of AIXI in\nquantum information terms, extending previous research on quantum Kolmogorov\ncomplexity and a QAIXI value function. We discuss conditions and limitations\nupon quantum Solomonoff induction and show how contextuality fundamentally\naffects QAIXI models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u91cf\u5b50AIXI\uff08QAIXI\uff09\uff0c\u6269\u5c55\u4e86\u7ecf\u5178AIXI\u6846\u67b6\u5230\u91cf\u5b50\u4fe1\u606f\u9886\u57df\uff0c\u63a2\u8ba8\u4e86\u91cf\u5b50\u73af\u5883\u4e0b\u7684\u901a\u7528\u667a\u80fd\u6a21\u578b\u3002", "motivation": "\u7ecf\u5178AIXI\u6a21\u578b\u57fa\u4e8e\u7ecf\u5178\u73af\u5883\uff0c\u800c\u5b87\u5b99\u672c\u8d28\u662f\u91cf\u5b50\u7684\uff0c\u6a21\u62df\u91cf\u5b50\u7cfb\u7edf\u9700\u8981\u6307\u6570\u7ea7\u5f00\u9500\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u91cf\u5b50AIXI\u7684\u7406\u8bba\u4e00\u81f4\u6027\u548c\u5b9e\u9645\u53ef\u884c\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u91cf\u5b50\u4e0e\u7ecf\u5178\u5bc4\u5b58\u5668\u53ca\u901a\u9053\u7684\u4ea4\u4e92\u6a21\u578b\uff0c\u6784\u5efa\u91cf\u5b50AIXI\u4ee3\u7406\uff0c\u652f\u6301\u7ecf\u5178\u548c\u91cf\u5b50\u52a8\u4f5c\u3002\u6269\u5c55\u4e86\u91cf\u5b50Kolmogorov\u590d\u6742\u5ea6\u548cQAIXI\u4ef7\u503c\u51fd\u6570\u7b49\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u63d0\u51fa\u4e86\u91cf\u5b50Solomonoff\u5f52\u7eb3\u7684\u6761\u4ef6\u548c\u9650\u5236\uff0c\u5e76\u8bc1\u660e\u4e0a\u4e0b\u6587\u6027\u5bf9QAIXI\u6a21\u578b\u6709\u6839\u672c\u5f71\u54cd\u3002", "conclusion": "\u91cf\u5b50AIXI\u4e3a\u91cf\u5b50\u73af\u5883\u4e0b\u7684\u901a\u7528\u667a\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u4f46\u53d7\u9650\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u6027\u3002", "relevance": 40.0}}
{"id": "2505.20817", "pdf": "https://arxiv.org/pdf/2505.20817", "abs": "https://arxiv.org/abs/2505.20817", "authors": ["Savelii Chezhegov", "Aleksandr Beznosikov", "Samuel Horv\u00e1th", "Eduard Gorbunov"], "title": "Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise", "categories": ["math.OC", "cs.LG"], "comment": "33 pages", "summary": "Gradient clipping is a widely used technique in Machine Learning and Deep\nLearning (DL), known for its effectiveness in mitigating the impact of\nheavy-tailed noise, which frequently arises in the training of large language\nmodels. Additionally, first-order methods with clipping, such as Clip-SGD,\nexhibit stronger convergence guarantees than SGD under the\n$(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks.\nHowever, the high-probability convergence of Clip-SGD under both assumptions --\nheavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed\nin the literature. In this paper, we bridge this critical gap by establishing\nthe first high-probability convergence bounds for Clip-SGD applied to convex\n$(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends\nprior results by recovering known bounds for the deterministic case and the\nstochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid\nexponentially large factors and do not rely on restrictive sub-Gaussian noise\nassumptions, significantly broadening the applicability of gradient clipping.", "AI": {"tldr": "\u672c\u6587\u586b\u8865\u4e86Clip-SGD\u5728\u91cd\u5c3e\u566a\u58f0\u548c$(L_0,L_1)$-\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\u7684\u9ad8\u6982\u7387\u6536\u655b\u6027\u7406\u8bba\u7a7a\u767d\uff0c\u6269\u5c55\u4e86\u68af\u5ea6\u88c1\u526a\u7684\u9002\u7528\u6027\u3002", "motivation": "\u68af\u5ea6\u88c1\u526a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u91cd\u5c3e\u566a\u58f0\u548c$(L_0,L_1)$-\u5e73\u6ed1\u6027\u4e0b\u7684\u9ad8\u6982\u7387\u6536\u655b\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u5efa\u7acb\u4e86Clip-SGD\u5728\u51f8$(L_0,L_1)$-\u5e73\u6ed1\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u9ad8\u6982\u7387\u6536\u655b\u754c\u3002", "result": "\u63d0\u51fa\u4e86\u907f\u514d\u6307\u6570\u7ea7\u5927\u56e0\u5b50\u4e14\u4e0d\u4f9d\u8d56\u4e25\u683c\u5b50\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u7684\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u7814\u7a76\u663e\u8457\u6269\u5c55\u4e86\u68af\u5ea6\u88c1\u526a\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "relevance": 75.0}}
{"id": "2505.20863", "pdf": "https://arxiv.org/pdf/2505.20863", "abs": "https://arxiv.org/abs/2505.20863", "authors": ["Daniel Barta", "Darya Martyniuk", "Johannes Jung", "Adrian Paschke"], "title": "Leveraging Diffusion Models for Parameterized Quantum Circuit Generation", "categories": ["quant-ph", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Quantum computing holds immense potential, yet its practical success depends\non multiple factors, including advances in quantum circuit design. In this\npaper, we introduce a generative approach based on denoising diffusion models\n(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent\ndiffusion model pipeline of F\\\"urrutter et al. [1], our model effectively\nconditions the synthesis process, enabling the simultaneous generation of\ncircuit architectures and their continuous gate parameters. We demonstrate our\napproach in synthesizing PQCs optimized for generating high-fidelity\nGreenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum\nmachine learning (QML) classification tasks. Our results indicate a strong\ngeneralization across varying gate sets and scaling qubit counts, highlighting\nthe versatility and computational efficiency of diffusion-based methods. This\nwork illustrates the potential of generative models as a powerful tool for\naccelerating and optimizing the design of PQCs, supporting the development of\nmore practical and scalable quantum applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQCs\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u9ad8\u4fdd\u771fGHZ\u6001\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u5b9e\u7528\u6027\u4f9d\u8d56\u4e8e\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u7684\u8fdb\u6b65\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u52a0\u901f\u548c\u4f18\u5316PQCs\u7684\u8bbe\u8ba1\u3002", "method": "\u6269\u5c55\u4e86F\u00fcrrutter\u7b49\u4eba\u7684\u6269\u6563\u6a21\u578b\u6d41\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u540c\u65f6\u751f\u6210\u7535\u8def\u67b6\u6784\u548c\u8fde\u7eed\u95e8\u53c2\u6570\u7684\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u4fdd\u771fGHZ\u6001\u548cQML\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u95e8\u96c6\u548c\u91cf\u5b50\u6bd4\u7279\u89c4\u6a21\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3aPQCs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u91cf\u5b50\u8ba1\u7b97\u7684\u5b9e\u7528\u5316\u3002", "relevance": 30.0}}
{"id": "2505.20950", "pdf": "https://arxiv.org/pdf/2505.20950", "abs": "https://arxiv.org/abs/2505.20950", "authors": ["Maria Teresa Arias", "Davide Barbieri", "Eugenio Hern\u00e1ndez"], "title": "Scattering Networks on Noncommutative Finite Groups", "categories": ["math.NA", "cs.IT", "cs.LG", "cs.NA", "eess.SP", "math.IT"], "comment": null, "summary": "Scattering Networks were initially designed to elucidate the behavior of\nearly layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and\nare grounded in wavelets. In this work, we introduce a scattering transform on\nan arbitrary finite group (not necessarily abelian) within the context of\ngroup-equivariant convolutional neural networks (G-CNNs). We present wavelets\non finite groups and analyze their similarity to classical wavelets. We\ndemonstrate that, under certain conditions in the wavelet coefficients, the\nscattering transform is non-expansive, stable under deformations, preserves\nenergy, equivariant with respect to left and right group translations, and, as\ndepth increases, the scattering coefficients are less sensitive to group\ntranslations of the signal, all desirable properties of convolutional neural\nnetworks. Furthermore, we provide examples illustrating the application of the\nscattering transform to classify data with domains involving abelian and\nnonabelian groups.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6709\u9650\u7fa4\u4e0a\u7684\u6563\u5c04\u53d8\u6362\uff0c\u7528\u4e8e\u7fa4\u7b49\u53d8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08G-CNNs\uff09\uff0c\u5206\u6790\u4e86\u5176\u6027\u8d28\u5e76\u5c55\u793a\u4e86\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6269\u5c55\u6563\u5c04\u7f51\u7edc\u5230\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u6709\u9650\u7fa4\u4e0a\uff0c\u4ee5\u589e\u5f3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u7fa4\u7b49\u53d8\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u6709\u9650\u7fa4\u4e0a\u7684\u5c0f\u6ce2\uff0c\u5b9a\u4e49\u6563\u5c04\u53d8\u6362\uff0c\u5e76\u5206\u6790\u5176\u6027\u8d28\uff08\u5982\u975e\u6269\u5f20\u6027\u3001\u7a33\u5b9a\u6027\u3001\u80fd\u91cf\u4fdd\u6301\u7b49\uff09\u3002", "result": "\u6563\u5c04\u53d8\u6362\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5177\u6709\u975e\u6269\u5f20\u6027\u3001\u7a33\u5b9a\u6027\u3001\u80fd\u91cf\u4fdd\u6301\u548c\u7fa4\u7b49\u53d8\u6027\uff0c\u4e14\u5728\u6df1\u5ea6\u589e\u52a0\u65f6\u5bf9\u7fa4\u5e73\u79fb\u7684\u654f\u611f\u6027\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7fa4\u7b49\u53d8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.20955", "pdf": "https://arxiv.org/pdf/2505.20955", "abs": "https://arxiv.org/abs/2505.20955", "authors": ["Puwei Lian", "Yujun Cai", "Songze Li"], "title": "Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Diffusion models have achieved tremendous success in image generation, but\nthey also raise significant concerns regarding privacy and copyright issues.\nMembership Inference Attacks (MIAs) are designed to ascertain whether specific\ndata were utilized during a model's training phase. As current MIAs for\ndiffusion models typically exploit the model's image prediction ability, we\nformalize them into a unified general paradigm which computes the membership\nscore for membership identification. Under this paradigm, we empirically find\nthat existing attacks overlook the inherent deficiency in how diffusion models\nprocess high-frequency information. Consequently, this deficiency leads to\nmember data with more high-frequency content being misclassified as hold-out\ndata, and hold-out data with less high-frequency content tend to be\nmisclassified as member data. Moreover, we theoretically demonstrate that this\ndeficiency reduces the membership advantage of attacks, thereby interfering\nwith the effective discrimination of member data and hold-out data. Based on\nthis insight, we propose a plug-and-play high-frequency filter module to\nmitigate the adverse effects of the deficiency, which can be seamlessly\nintegrated into any attacks within this general paradigm without additional\ntime costs. Extensive experiments corroborate that this module significantly\nimproves the performance of baseline attacks across different datasets and\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u7684\u7edf\u4e00\u8303\u5f0f\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u653b\u51fb\u5ffd\u7565\u4e86\u6269\u6563\u6a21\u578b\u5904\u7406\u9ad8\u9891\u4fe1\u606f\u7684\u56fa\u6709\u7f3a\u9677\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9ad8\u9891\u6ee4\u6ce2\u5668\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u3002\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u65e8\u5728\u5224\u65ad\u7279\u5b9a\u6570\u636e\u662f\u5426\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u73b0\u6709\u653b\u51fb\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u4f5c\u8005\u5c06\u73b0\u6709MIA\u5f62\u5f0f\u5316\u4e3a\u7edf\u4e00\u8303\u5f0f\uff0c\u53d1\u73b0\u5176\u5ffd\u7565\u4e86\u6269\u6563\u6a21\u578b\u5904\u7406\u9ad8\u9891\u4fe1\u606f\u7684\u7f3a\u9677\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u9891\u6ee4\u6ce2\u5668\u6a21\u5757\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55\u653b\u51fb\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u653b\u51fb\u6027\u80fd\u3002", "conclusion": "\u9ad8\u9891\u6ee4\u6ce2\u5668\u6a21\u5757\u6709\u6548\u7f13\u89e3\u4e86\u6269\u6563\u6a21\u578b\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u63d0\u5347\u4e86MIA\u7684\u6027\u80fd\u3002", "relevance": 50.0}}
{"id": "2505.20980", "pdf": "https://arxiv.org/pdf/2505.20980", "abs": "https://arxiv.org/abs/2505.20980", "authors": ["Micha\u0142 Czuba", "Mateusz Stolarski", "Adam Pir\u00f3g", "Piotr Bielak", "Piotr Br\u00f3dka"], "title": "Identifying Super Spreaders in Multilayer Networks", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Identifying super-spreaders can be framed as a subtask of the influence\nmaximisation problem. It seeks to pinpoint agents within a network that, if\nselected as single diffusion seeds, disseminate information most effectively.\nMultilayer networks, a specific class of heterogeneous graphs, can capture\ndiverse types of interactions (e.g., physical-virtual or professional-social),\nand thus offer a more accurate representation of complex relational structures.\nIn this work, we introduce a novel approach to identifying super-spreaders in\nsuch networks by leveraging graph neural networks. To this end, we construct a\ndataset by simulating information diffusion across hundreds of networks - to\nthe best of our knowledge, the first of its kind tailored specifically to\nmultilayer networks. We further formulate the task as a variation of the\nranking prediction problem based on a four-dimensional vector that quantifies\neach agent's spreading potential: (i) the number of activations; (ii) the\nduration of the diffusion process; (iii) the peak number of activations; and\n(iv) the simulation step at which this peak occurs. Our model,\nTopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom\naggregation layer. This design enables generalisation to previously unseen data\nand adapts to varying graph sizes. In an extensive evaluation, we compare our\nmodel against classic centrality-based heuristics and competitive deep learning\nmethods. The results, obtained across a broad spectrum of real-world and\nsynthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves\nsuperior performance in identifying high-impact nodes, while also offering\nimproved interpretability through its structured output.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff08TopSpreadersNetwork\uff09\uff0c\u7528\u4e8e\u8bc6\u522b\u591a\u5c42\u7f51\u7edc\u4e2d\u7684\u8d85\u7ea7\u4f20\u64ad\u8005\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u4fe1\u606f\u6269\u6563\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9\u591a\u5c42\u7f51\u7edc\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u591a\u5c42\u7f51\u7edc\u80fd\u66f4\u51c6\u786e\u5730\u8868\u793a\u590d\u6742\u5173\u7cfb\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u8d85\u7ea7\u4f20\u64ad\u8005\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u6a21\u62df\u4fe1\u606f\u6269\u6563\u7684\u6570\u636e\u96c6\uff0c\u5c06\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8e\u56db\u7ef4\u5411\u91cf\u7684\u6392\u540d\u9884\u6d4b\u95ee\u9898\uff0c\u8bbe\u8ba1\u5305\u542b\u5173\u7cfb\u65e0\u5173\u7f16\u7801\u5668\u548c\u81ea\u5b9a\u4e49\u805a\u5408\u5c42\u7684\u6a21\u578b\u3002", "result": "TopSpreadersNetwork\u5728\u771f\u5b9e\u548c\u5408\u6210\u591a\u5c42\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u9ad8\u5f71\u54cd\u529b\u8282\u70b9\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u4e14\u80fd\u9002\u5e94\u4e0d\u540c\u89c4\u6a21\u7684\u7f51\u7edc\u3002", "relevance": 30.0}}
{"id": "2505.21019", "pdf": "https://arxiv.org/pdf/2505.21019", "abs": "https://arxiv.org/abs/2505.21019", "authors": ["Devran Ugurlu", "Shuang Qian", "Elliot Fairweather", "Charlene Mauger", "Bram Ruijsink", "Laura Dal Toso", "Yu Deng", "Marina Strocchi", "Reza Razavi", "Alistair Young", "Pablo Lamata", "Steven Niederer", "Martin Bishop"], "title": "Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "A cardiac digital twin is a virtual replica of a patient's heart for\nscreening, diagnosis, prognosis, risk assessment, and treatment planning of\ncardiovascular diseases. This requires an anatomically accurate\npatient-specific 3D structural representation of the heart, suitable for\nelectro-mechanical simulations or study of disease mechanisms. However,\ngeneration of cardiac digital twins at scale is demanding and there are no\npublic repositories of models across demographic groups. We describe an\nautomatic open-source pipeline for creating patient-specific left and right\nventricular meshes from cardiovascular magnetic resonance images, its\napplication to a large cohort of ~55000 participants from UK Biobank, and the\nconstruction of the most comprehensive cohort of adult heart models to date,\ncomprising 1423 representative meshes across sex (male, female), body mass\nindex (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is\navailable at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 ,\nand pre-trained networks, representative volumetric meshes with fibers and UVCs\nwill be made available soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5f00\u6e90\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u56fe\u50cf\u751f\u6210\u60a3\u8005\u7279\u5b9a\u7684\u5de6\u53f3\u5fc3\u5ba4\u7f51\u683c\uff0c\u5e76\u5e94\u7528\u4e8eUK Biobank\u7684\u5927\u89c4\u6a21\u961f\u5217\uff0c\u6784\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5168\u9762\u7684\u6210\u4eba\u5fc3\u810f\u6a21\u578b\u5e93\u3002", "motivation": "\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u9700\u8981\u7cbe\u786e\u76843D\u7ed3\u6784\u8868\u793a\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e14\u8de8\u4eba\u53e3\u7edf\u8ba1\u7ec4\u7684\u516c\u5171\u6a21\u578b\u5e93\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5f00\u6e90\u6d41\u7a0b\uff0c\u4ece\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u56fe\u50cf\u751f\u6210\u5fc3\u5ba4\u7f51\u683c\uff0c\u5e76\u5e94\u7528\u4e8eUK Biobank\u768455000\u540d\u53c2\u4e0e\u8005\uff0c\u6784\u5efa\u4e861423\u4e2a\u4ee3\u8868\u6027\u7f51\u683c\u3002", "result": "\u6784\u5efa\u4e86\u8986\u76d6\u6027\u522b\u3001BMI\u548c\u5e74\u9f84\u8303\u56f4\u7684\u5168\u9762\u5fc3\u810f\u6a21\u578b\u5e93\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u7f51\u7edc\u5c06\u516c\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u7684\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u8d44\u6e90\u3002", "relevance": 20.0}}
{"id": "2505.21298", "pdf": "https://arxiv.org/pdf/2505.21298", "abs": "https://arxiv.org/abs/2505.21298", "authors": ["Emanuele La Malfa", "Gabriele La Malfa", "Samuele Marro", "Jie M. Zhang", "Elizabeth Black", "Micheal Luck", "Philip Torr", "Michael Wooldridge"], "title": "Large Language Models Miss the Multi-Agent Mark", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff08MAS LLMs\uff09\u7684\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u57fa\u7840\u7406\u8bba\u7684\u5173\u6ce8\uff0c\u5bfc\u81f4\u5b9e\u73b0\u4e2d\u5b58\u5728\u5173\u952e\u5dee\u5f02\u3002\u4f5c\u8005\u547c\u5401\u66f4\u597d\u5730\u6574\u5408MAS\u6982\u5ff5\uff0c\u907f\u514d\u8bef\u89e3\u548c\u9519\u5931\u673a\u4f1a\u3002", "motivation": "\u5f53\u524dMAS LLMs\u7684\u7814\u7a76\u591a\u501f\u7528MAS\u672f\u8bed\u4f46\u672a\u6df1\u5165\u5176\u7406\u8bba\u57fa\u7840\uff0c\u53ef\u80fd\u5bfc\u81f4\u9886\u57df\u53d1\u5c55\u53d7\u963b\u3002", "method": "\u901a\u8fc7\u5206\u6790MAS\u7406\u8bba\u4e0eMAS LLMs\u5b9e\u73b0\u7684\u56db\u4e2a\u5173\u952e\u5dee\u5f02\uff08\u793e\u4f1a\u6027\u3001\u73af\u5883\u8bbe\u8ba1\u3001\u534f\u8c03\u4e0e\u901a\u4fe1\u534f\u8bae\u3001\u6d8c\u73b0\u884c\u4e3a\u6d4b\u91cf\uff09\uff0c\u63d0\u51fa\u7cfb\u7edf\u6027\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u5f53\u524dMAS LLMs\u7f3a\u4e4f\u591a\u667a\u80fd\u4f53\u7279\u6027\uff08\u5982\u81ea\u4e3b\u6027\u3001\u793e\u4f1a\u4ea4\u4e92\u3001\u7ed3\u6784\u5316\u73af\u5883\uff09\uff0c\u5e76\u4f9d\u8d56\u8fc7\u5ea6\u7b80\u5316\u7684LLM\u4e2d\u5fc3\u67b6\u6784\u3002", "conclusion": "\u5efa\u8bae\u66f4\u7cbe\u786e\u5730\u6574\u5408MAS\u6982\u5ff5\u548c\u672f\u8bed\uff0c\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "relevance": 75.0}}
{"id": "2505.21139", "pdf": "https://arxiv.org/pdf/2505.21139", "abs": "https://arxiv.org/abs/2505.21139", "authors": ["Subhagata Chattopadhyay", "Amit K Chattopadhyay"], "title": "Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach", "categories": ["q-bio.PE", "cond-mat.soft", "cs.LG", "physics.med-ph"], "comment": "16 pages, 2 figures, 7 tables", "summary": "The COVID-19 pandemic has significantly increased the incidence of\npost-infection cardiovascular events, particularly myocardial infarction, in\nindividuals over 40. While the underlying mechanisms remain elusive, this study\nemploys a hybrid machine learning approach to analyze epidemiological data in\nassessing 13 key heart attack risk factors and their susceptibility. Based on a\nunique dataset that combines demographic, biochemical, ECG, and thallium\nstress-tests, this study categorizes distinct subpopulations against varying\nrisk profiles and then divides the population into 'at-risk' (AR) and\n'not-at-risk' (NAR) groups using clustering algorithms. The study reveals\nstrong association between the likelihood of experiencing a heart attack on the\n13 risk factors studied. The aggravated risk for postmenopausal patients\nindicates compromised individual risk factors due to estrogen depletion that\nmay be, further compromised by extraneous stress impacts, like anxiety and\nfear, aspects that have traditionally eluded data modeling predictions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91c7\u7528\u6df7\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u6d41\u884c\u75c5\u5b66\u6570\u636e\uff0c\u8bc4\u4f3013\u79cd\u5fc3\u810f\u75c5\u53d1\u4f5c\u98ce\u9669\u56e0\u7d20\u53ca\u5176\u6613\u611f\u6027\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u56e0\u7d20\u4e0e\u5fc3\u810f\u75c5\u53d1\u4f5c\u7684\u5f3a\u5173\u8054\u6027\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u663e\u8457\u589e\u52a0\u4e8640\u5c81\u4ee5\u4e0a\u4eba\u7fa4\u611f\u67d3\u540e\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u7684\u53d1\u751f\u7387\uff0c\u5c24\u5176\u662f\u5fc3\u808c\u6897\u6b7b\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6f5c\u5728\u673a\u5236\u5e76\u8bc4\u4f30\u98ce\u9669\u56e0\u7d20\u3002", "method": "\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u751f\u5316\u3001\u5fc3\u7535\u56fe\u548c\u94ca\u8d1f\u8377\u8bd5\u9a8c\u6570\u636e\uff0c\u4f7f\u7528\u805a\u7c7b\u7b97\u6cd5\u5c06\u4eba\u7fa4\u5206\u4e3a'\u9ad8\u98ce\u9669'\u548c'\u975e\u9ad8\u98ce\u9669'\u7ec4\u3002", "result": "\u7814\u7a76\u53d1\u73b013\u79cd\u98ce\u9669\u56e0\u7d20\u4e0e\u5fc3\u810f\u75c5\u53d1\u4f5c\u53ef\u80fd\u6027\u6709\u5f3a\u5173\u8054\uff0c\u7edd\u7ecf\u540e\u60a3\u8005\u7684\u98ce\u9669\u52a0\u5267\u53ef\u80fd\u4e0e\u96cc\u6fc0\u7d20\u8017\u7aed\u6709\u5173\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4f20\u7edf\u6570\u636e\u5efa\u6a21\u96be\u4ee5\u6355\u6349\u7684\u7126\u8651\u548c\u6050\u60e7\u7b49\u5916\u90e8\u538b\u529b\u5bf9\u98ce\u9669\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "relevance": 10.0}}
{"id": "2505.21208", "pdf": "https://arxiv.org/pdf/2505.21208", "abs": "https://arxiv.org/abs/2505.21208", "authors": ["Thomas Deschatre", "Xavier Warin"], "title": "Input Convex Kolmogorov Arnold Networks", "categories": ["stat.ML", "cs.LG", "math.OC", "68T07"], "comment": null, "summary": "This article presents an input convex neural network architecture using\nKolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the\nfirst is based on a low-order, linear-by-part, representation of functions, and\na universal approximation theorem is provided. The second is based on cubic\nsplines, for which only numerical results support convergence. We demonstrate\non simple tests that these networks perform competitively with classical input\nconvex neural networks (ICNNs). In a second part, we use the networks to solve\nsome optimal transport problems needing a convex approximation of functions and\ndemonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs\nproduce results similar to those of classical ICNNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov-Arnold\u7f51\u7edc\uff08ICKAN\uff09\u7684\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u542b\u4e24\u79cd\u7f51\u7edc\uff1a\u4e00\u79cd\u57fa\u4e8e\u4f4e\u9636\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u8868\u793a\uff0c\u53e6\u4e00\u79cd\u57fa\u4e8e\u4e09\u6b21\u6837\u6761\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4e0e\u4f20\u7edfICNN\u76f8\u5f53\uff0c\u5e76\u5728\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9700\u8981\u51f8\u51fd\u6570\u903c\u8fd1\u7684\u95ee\u9898\uff0c\u5982\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cdICKAN\u7f51\u7edc\uff1a\u4f4e\u9636\u5206\u6bb5\u7ebf\u6027\u7f51\u7edc\uff08\u63d0\u4f9b\u901a\u7528\u903c\u8fd1\u5b9a\u7406\uff09\u548c\u4e09\u6b21\u6837\u6761\u7f51\u7edc\uff08\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u6536\u655b\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7acb\u65b9ICKAN\u4e0e\u4f20\u7edfICNN\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u5728\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "ICKAN\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u9700\u8981\u51f8\u51fd\u6570\u903c\u8fd1\u7684\u4efb\u52a1\u3002", "relevance": 30.0}}
{"id": "2505.21215", "pdf": "https://arxiv.org/pdf/2505.21215", "abs": "https://arxiv.org/abs/2505.21215", "authors": ["Alex A. Saoulis", "Davide Piras", "Niall Jeffrey", "Alessio Spurio Mancini", "Ana M. G. Ferreira", "Benjamin Joachimi"], "title": "Transfer learning for multifidelity simulation-based inference in cosmology", "categories": ["astro-ph.CO", "cs.LG"], "comment": "9+4 pages, 8+5 figures", "summary": "Simulation-based inference (SBI) enables cosmological parameter estimation\nwhen closed-form likelihoods or models are unavailable. However, SBI relies on\nmachine learning for neural compression and density estimation. This requires\nlarge training datasets which are prohibitively expensive for high-quality\nsimulations. We overcome this limitation with multifidelity transfer learning,\ncombining less expensive, lower-fidelity simulations with a limited number of\nhigh-fidelity simulations. We demonstrate our methodology on dark matter\ndensity maps from two separate simulation suites in the hydrodynamical CAMELS\nMultifield Dataset. Pre-training on dark-matter-only $N$-body simulations\nreduces the required number of high-fidelity hydrodynamical simulations by a\nfactor between $8$ and $15$, depending on the model complexity, posterior\ndimensionality, and performance metrics used. By leveraging cheaper\nsimulations, our approach enables performant and accurate inference on\nhigh-fidelity models while substantially reducing computational costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4fdd\u771f\u5ea6\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u63a8\u65ad\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u65ad\uff08SBI\uff09\u65b9\u6cd5\u4e2d\u9ad8\u4fdd\u771f\u6a21\u62df\u6570\u636e\u9700\u6c42\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u4fdd\u771f\u5ea6\u8fc1\u79fb\u5b66\u4e60\uff0c\u7ed3\u5408\u4f4e\u6210\u672c\u7684\u4f4e\u4fdd\u771f\u6a21\u62df\u548c\u5c11\u91cf\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u8fdb\u884c\u795e\u7ecf\u538b\u7f29\u548c\u5bc6\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u6697\u7269\u8d28\u5bc6\u5ea6\u56fe\u4e0a\u9a8c\u8bc1\uff0c\u9884\u8bad\u7ec3\u53ef\u5c06\u9ad8\u4fdd\u771f\u6a21\u62df\u9700\u6c42\u51cf\u5c118\u523015\u500d\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u4fdd\u771f\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u63a8\u65ad\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u3002", "relevance": 40.0}}
{"id": "2505.21220", "pdf": "https://arxiv.org/pdf/2505.21220", "abs": "https://arxiv.org/abs/2505.21220", "authors": ["M. Mebratu", "W. L. K. Wu"], "title": "Wavelet Flow For Extragalactic Foreground Simulations", "categories": ["astro-ph.CO", "cs.LG"], "comment": "19 pages, 7 figures", "summary": "Extragalactic foregrounds in cosmic microwave background (CMB) observations\nare both a source of cosmological and astrophysical information and a nuisance\nto the CMB. Effective field-level modeling that captures their non-Gaussian\nstatistical distributions is increasingly important for optimal information\nextraction, particularly given the precise and low-noise observations from\ncurrent and upcoming experiments. We explore the use of Wavelet Flow (WF)\nmodels to tackle the novel task of modeling the field-level probability\ndistributions of multi-component CMB secondaries. Specifically, we jointly\ntrain correlated CMB lensing convergence ($\\kappa$) and cosmic infrared\nbackground (CIB) maps with a WF model and obtain a network that statistically\nrecovers the input to high accuracy -- the trained network generates samples of\n$\\kappa$ and CIB fields whose average power spectra are within a few percent of\nthe inputs across all scales, and whose Minkowski functionals are similarly\naccurate compared to the inputs. Leveraging the multiscale architecture of\nthese models, we fine-tune both the model parameters and the priors at each\nscale independently, optimizing performance across different resolutions. These\nresults demonstrate that WF models can accurately simulate correlated\ncomponents of CMB secondaries, supporting improved analysis of cosmological\ndata. Our code and trained models can be found here\n(https://github.com/matiwosm/HybridPriorWavletFlow.git).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528Wavelet Flow\u6a21\u578b\u5efa\u6a21CMB\u6b21\u7ea7\u573a\u7684\u6982\u7387\u5206\u5e03\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6a21\u62df\u76f8\u5173CMB\u900f\u955c\u6536\u655b\u548c\u5b87\u5b99\u7ea2\u5916\u80cc\u666f\u65b9\u9762\u7684\u51c6\u786e\u6027\u3002", "motivation": "CMB\u89c2\u6d4b\u4e2d\u7684\u6cb3\u5916\u524d\u666f\u65e2\u662f\u5b87\u5b99\u5b66\u548c\u5929\u4f53\u7269\u7406\u4fe1\u606f\u7684\u6765\u6e90\uff0c\u4e5f\u662f\u5e72\u6270\u3002\u9700\u8981\u6709\u6548\u7684\u573a\u7ea7\u5efa\u6a21\u4ee5\u4f18\u5316\u4fe1\u606f\u63d0\u53d6\u3002", "method": "\u4f7f\u7528Wavelet Flow\u6a21\u578b\u8054\u5408\u8bad\u7ec3\u76f8\u5173\u7684CMB\u900f\u955c\u6536\u655b\u548c\u5b87\u5b99\u7ea2\u5916\u80cc\u666f\u56fe\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u67b6\u6784\u72ec\u7acb\u4f18\u5316\u6a21\u578b\u53c2\u6570\u548c\u5148\u9a8c\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u5728\u529f\u7387\u8c31\u548cMinkowski\u6cdb\u51fd\u4e0a\u4e0e\u8f93\u5165\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\uff0c\u51c6\u786e\u6a21\u62df\u4e86CMB\u6b21\u7ea7\u573a\u7684\u76f8\u5173\u6210\u5206\u3002", "conclusion": "Wavelet Flow\u6a21\u578b\u80fd\u51c6\u786e\u6a21\u62dfCMB\u6b21\u7ea7\u573a\uff0c\u652f\u6301\u6539\u8fdb\u5b87\u5b99\u5b66\u6570\u636e\u5206\u6790\u3002", "relevance": 20.0}}
{"id": "2505.21388", "pdf": "https://arxiv.org/pdf/2505.21388", "abs": "https://arxiv.org/abs/2505.21388", "authors": ["Jingyuan Huang", "Xi Zhu", "Minghao Guo", "Yongfeng Zhang"], "title": "DeSocial: Blockchain-based Decentralized Social Networks", "categories": ["cs.SI", "cs.AI", "cs.LG"], "comment": "29 pages, 13 figures", "summary": "Web 2.0 social platforms are inherently centralized, with user data and\nalgorithmic decisions controlled by the platform. However, users can only\npassively receive social predictions without being able to choose the\nunderlying algorithm, which limits personalization. Fortunately, with the\nemergence of blockchain, users are allowed to choose algorithms that are\ntailored to their local situation, improving prediction results in a\npersonalized way. In a blockchain environment, each user possesses its own\nmodel to perform the social prediction, capturing different perspectives on\nsocial interactions. In our work, we propose DeSocial, a decentralized social\nnetwork learning framework deployed on an Ethereum (ETH) local development\nchain that integrates distributed data storage, node-level consensus, and\nuser-driven model selection through Ganache. In the first stage, each user\nleverages DeSocial to evaluate multiple backbone models on their local\nsubgraph. DeSocial coordinates the execution and returns model-wise prediction\nresults, enabling the user to select the most suitable backbone for\npersonalized social prediction. Then, DeSocial uniformly selects several\nvalidation nodes that possess the algorithm specified by each user, and\naggregates the prediction results by majority voting, to prevent errors caused\nby any single model's misjudgment. Extensive experiments show that DeSocial has\nan evident improvement compared to the five classical centralized social\nnetwork learning models, promoting user empowerment in blockchain-based\ndecentralized social networks, showing the importance of multi-node validation\nand personalized algorithm selection based on blockchain. Our implementation is\navailable at: https://github.com/agiresearch/DeSocial.", "AI": {"tldr": "DeSocial\u662f\u4e00\u4e2a\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u53bb\u4e2d\u5fc3\u5316\u793e\u4ea4\u7f51\u7edc\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u9a71\u52a8\u7684\u6a21\u578b\u9009\u62e9\u548c\u591a\u6570\u6295\u7968\u805a\u5408\u7ed3\u679c\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4e2d\u5fc3\u5316\u793e\u4ea4\u5e73\u53f0\u4e2d\u7528\u6237\u65e0\u6cd5\u9009\u62e9\u7b97\u6cd5\u7684\u9650\u5236\uff0c\u5229\u7528\u533a\u5757\u94fe\u6280\u672f\u5b9e\u73b0\u4e2a\u6027\u5316\u9884\u6d4b\u3002", "method": "\u5728\u4ee5\u592a\u574a\u672c\u5730\u5f00\u53d1\u94fe\u4e0a\u90e8\u7f72DeSocial\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u6570\u636e\u5b58\u50a8\u3001\u8282\u70b9\u7ea7\u5171\u8bc6\u548c\u7528\u6237\u9a71\u52a8\u7684\u6a21\u578b\u9009\u62e9\u3002\u7528\u6237\u9996\u5148\u8bc4\u4f30\u591a\u4e2a\u9aa8\u5e72\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u591a\u6570\u6295\u7968\u805a\u5408\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeSocial\u76f8\u6bd4\u4e94\u79cd\u7ecf\u5178\u4e2d\u5fc3\u5316\u6a21\u578b\u6709\u660e\u663e\u6539\u8fdb\uff0c\u63d0\u5347\u4e86\u533a\u5757\u94fe\u53bb\u4e2d\u5fc3\u5316\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u7528\u6237\u8d4b\u6743\u3002", "conclusion": "DeSocial\u5c55\u793a\u4e86\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u591a\u8282\u70b9\u9a8c\u8bc1\u548c\u4e2a\u6027\u5316\u7b97\u6cd5\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002", "relevance": 40.0}}
{"id": "2505.21331", "pdf": "https://arxiv.org/pdf/2505.21331", "abs": "https://arxiv.org/abs/2505.21331", "authors": ["Caner Gocmen", "Thodoris Lykouris", "Deeksha Sinha", "Wentao Weng"], "title": "Scheduling with Uncertain Holding Costs and its Application to Content Moderation", "categories": ["cs.DS", "cs.GT", "cs.LG", "cs.PF", "math.PR"], "comment": null, "summary": "In content moderation for social media platforms, the cost of delaying the\nreview of a content is proportional to its view trajectory, which fluctuates\nand is apriori unknown. Motivated by such uncertain holding costs, we consider\na queueing model where job states evolve based on a Markov chain with\nstate-dependent instantaneous holding costs. We demonstrate that in the\npresence of such uncertain holding costs, the two canonical algorithmic\nprinciples, instantaneous-cost ($c\\mu$-rule) and expected-remaining-cost\n($c\\mu/\\theta$-rule), are suboptimal. By viewing each job as a Markovian\nski-rental problem, we develop a new index-based algorithm,\nOpportunity-adjusted Remaining Cost (OaRC), that adjusts to the opportunity of\nserving jobs in the future when uncertainty partly resolves. We show that the\nregret of OaRC scales as $\\tilde{O}(L^{1.5}\\sqrt{N})$, where $L$ is the maximum\nlength of a job's holding cost trajectory and $N$ is the system size. This\nregret bound shows that OaRC achieves asymptotic optimality when the system\nsize $N$ scales to infinity. Moreover, its regret is independent of the\nstate-space size, which is a desirable property when job states contain\ncontextual information. We corroborate our results with an extensive simulation\nstudy based on two holding cost patterns (online ads and user-generated\ncontent) that arise in content moderation for social media platforms. Our\nsimulations based on synthetic and real datasets demonstrate that OaRC\nconsistently outperforms existing practice, which is based on the two canonical\nalgorithmic principles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7d22\u5f15\u7b97\u6cd5OaRC\uff0c\u7528\u4e8e\u5904\u7406\u793e\u4ea4\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u4e0d\u786e\u5b9a\u6301\u6709\u6210\u672c\u95ee\u9898\uff0c\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u4e2d\uff0c\u5ef6\u8fdf\u5ba1\u6838\u7684\u6210\u672c\u4e0e\u5185\u5bb9\u7684\u89c2\u770b\u8f68\u8ff9\u76f8\u5173\u4e14\u672a\u77e5\uff0c\u4f20\u7edf\u7b97\u6cd5\uff08\u5982c\u03bc\u89c4\u5219\uff09\u5728\u6b64\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c06\u6bcf\u4e2a\u4efb\u52a1\u89c6\u4e3a\u9a6c\u5c14\u53ef\u592b\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\uff0c\u63d0\u51faOaRC\u7b97\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u672a\u6765\u670d\u52a1\u673a\u4f1a\u3002", "result": "OaRC\u7684\u9057\u61be\u8fb9\u754c\u4e3aO(L^1.5\u221aN)\uff0c\u5728\u7cfb\u7edf\u89c4\u6a21N\u8d8b\u8fd1\u65e0\u7a77\u65f6\u6e10\u8fd1\u6700\u4f18\uff0c\u4e14\u4e0e\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u65e0\u5173\u3002", "conclusion": "OaRC\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5305\u542b\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4efb\u52a1\u72b6\u6001\u3002", "relevance": 30.0}}
{"id": "2505.21356", "pdf": "https://arxiv.org/pdf/2505.21356", "abs": "https://arxiv.org/abs/2505.21356", "authors": ["Whenty Ariyanti", "Kuan-Yu Chen", "Sabato Marco Siniscalchi", "Hsin-Min Wang", "Yu Tsao"], "title": "Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Objective: Perceptual voice quality assessment plays a critical role in\ndiagnosing and monitoring voice disorders by providing standardized evaluation\nof vocal function. Traditionally, this process relies on expert raters\nutilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation\nof Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain\n(GRBAS). However, these metrics are inherently subjective and susceptible to\ninter-rater variability, motivating the need for automated and objective\nassessment methods. Methods: We propose Voice Quality Assessment Network\n(VOQANet), a deep learning-based framework with an attention mechanism that\nleverages a Speech Foundation Model (SFM) to capture high-level acoustic and\nprosodic information from raw speech. To enhance robustness and\ninterpretability, we present VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings. Results: Sentence-based input yields stronger performance than\nvowel-based input, especially at the patient level. VOQANet consistently\noutperforms baseline methods in RMSE and PCC, while VOQANet+ performs even\nbetter and maintains robustness under noisy conditions. Conclusion: Combining\nSFM embeddings with domain-informed acoustic features improves interpretability\nand resilience. Significance: VOQANet+ shows strong potential for deployment in\nreal-world and telehealth settings, addressing the limitations of subjective\nperceptual assessments with an interpretable and noise-resilient solution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVOQANet\u548cVOQANet+\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFM\uff09\u7684\u81ea\u52a8\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408\u624b\u5de5\u58f0\u5b66\u7279\u5f81\u63d0\u5347\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u4f9d\u8d56\u4e13\u5bb6\u4e3b\u89c2\u8bc4\u5206\uff0c\u5b58\u5728\u53d8\u5f02\u6027\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u5ba2\u89c2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "VOQANet\u5229\u7528SFM\u6355\u6349\u58f0\u5b66\u4fe1\u606f\uff0cVOQANet+\u8fdb\u4e00\u6b65\u6574\u5408\u624b\u5de5\u58f0\u5b66\u7279\u5f81\uff08\u5982jitter\u3001shimmer\u7b49\uff09\u3002", "result": "VOQANet+\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408SFM\u548c\u9886\u57df\u7279\u5f81\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "relevance": 30.0}}
{"id": "2505.21432", "pdf": "https://arxiv.org/pdf/2505.21432", "abs": "https://arxiv.org/abs/2505.21432", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "AI": {"tldr": "Hume\u662f\u4e00\u79cd\u53cc\u7cfb\u7edf\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u4ef7\u503c\u5f15\u5bfc\u7684System-2\u601d\u7ef4\u548c\u7ea7\u8054\u52a8\u4f5c\u53bb\u566a\uff0c\u63a2\u7d22\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u7c7b\u4eba\u601d\u7ef4\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u6162\u601d\u7ef4\u8303\u5f0f\u5728\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "method": "Hume\u91c7\u7528\u53cc\u7cfb\u7edf\u8bbe\u8ba1\uff1aSystem-2\u901a\u8fc7\u4ef7\u503c\u67e5\u8be2\u5934\u4f30\u8ba1\u52a8\u4f5c\u72b6\u6001\u4ef7\u503c\uff0c\u91cd\u590d\u91c7\u6837\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\uff1bSystem-1\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u53cd\u5e94\u7b56\u7565\uff0c\u6267\u884c\u7ea7\u8054\u52a8\u4f5c\u53bb\u566a\u3002", "result": "Hume\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709VLA\u6a21\u578b\u3002", "conclusion": "Hume\u5c55\u793a\u4e86\u4ef7\u503c\u5f15\u5bfc\u601d\u7ef4\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7c7b\u4eba\u601d\u7ef4\u80fd\u529b\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 70.0}}
{"id": "2505.21421", "pdf": "https://arxiv.org/pdf/2505.21421", "abs": "https://arxiv.org/abs/2505.21421", "authors": ["Rami Cassia", "Rich Kerswell"], "title": "A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data", "categories": ["physics.flu-dyn", "cs.LG"], "comment": null, "summary": "In compressible fluid flow, reconstructing shocks, discontinuities,\nrarefactions, and their interactions from sparse measurements is an important\ninverse problem with practical applications. Moreover, physics-informed machine\nlearning has recently become an increasingly popular approach for performing\nreconstructions tasks. In this work we explore a machine learning recipe, known\nas GraphGPS, for reconstructing canonical compressible flows known as 3D\nRiemann problems from sparse observations, in a physics-informed manner. The\nGraphGPS framework combines the benefits of positional encodings, local\nmessage-passing of graphs, and global contextual awareness, and we explore the\nlatter two components through an ablation study. Furthermore, we modify the\naggregation step of message-passing such that it is aware of shocks and\ndiscontinuities, resulting in sharper reconstructions of these features.\nAdditionally, we modify message-passing such that information flows strictly\nfrom known nodes only, which results in computational savings, better training\nconvergence, and no degradation of reconstruction accuracy. We also show that\nthe GraphGPS framework outperforms numerous machine learning benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGraphGPS\u7684\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c2\u6d4b\u4e2d\u91cd\u5efa3D Riemann\u95ee\u9898\u4e2d\u7684\u53ef\u538b\u7f29\u6d41\u4f53\u6d41\u52a8\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3\u53ef\u538b\u7f29\u6d41\u4f53\u6d41\u52a8\u4e2d\u7a00\u758f\u89c2\u6d4b\u91cd\u5efa\u7684\u9006\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u4f4d\u7f6e\u7f16\u7801\u3001\u5c40\u90e8\u56fe\u6d88\u606f\u4f20\u9012\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u7684GraphGPS\u6846\u67b6\uff0c\u5e76\u6539\u8fdb\u6d88\u606f\u4f20\u9012\u4ee5\u589e\u5f3a\u5bf9\u6fc0\u6ce2\u548c\u4e0d\u8fde\u7eed\u6027\u7684\u6355\u6349\u3002", "result": "GraphGPS\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4e14\u8bad\u7ec3\u6536\u655b\u66f4\u597d\u3002", "conclusion": "GraphGPS\u662f\u4e00\u79cd\u6709\u6548\u7684\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6d41\u4f53\u6d41\u52a8\u7279\u5f81\u7684\u91cd\u5efa\u3002", "relevance": 20.0}}
{"id": "2505.21441", "pdf": "https://arxiv.org/pdf/2505.21441", "abs": "https://arxiv.org/abs/2505.21441", "authors": ["Binh Duc Vu", "Jan Kapar", "Marvin Wright", "David S. Watson"], "title": "Autoencoding Random Forests", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "10 pages main text, 25 pages total. 5 figures main text, 9 figures\n  total", "summary": "We propose a principled method for autoencoding with random forests. Our\nstrategy builds on foundational results from nonparametric statistics and\nspectral graph theory to learn a low-dimensional embedding of the model that\noptimally represents relationships in the data. We provide exact and\napproximate solutions to the decoding problem via constrained optimization,\nsplit relabeling, and nearest neighbors regression. These methods effectively\ninvert the compression pipeline, establishing a map from the embedding space\nback to the input space using splits learned by the ensemble's constituent\ntrees. The resulting decoders are universally consistent under common\nregularity assumptions. The procedure works with supervised or unsupervised\nmodels, providing a window into conditional or joint distributions. We\ndemonstrate various applications of this autoencoder, including powerful new\ntools for visualization, compression, clustering, and denoising. Experiments\nillustrate the ease and utility of our method in a wide range of settings,\nincluding tabular, image, and genomic data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u81ea\u7f16\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u53c2\u6570\u7edf\u8ba1\u548c\u8c31\u56fe\u7406\u8bba\u5b66\u4e60\u4f4e\u7ef4\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u3001\u5206\u88c2\u91cd\u6807\u8bb0\u548c\u6700\u8fd1\u90bb\u56de\u5f52\u89e3\u51b3\u89e3\u7801\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u76d1\u7763\u6216\u65e0\u76d1\u7763\u6a21\u578b\uff0c\u5728\u591a\u79cd\u6570\u636e\u4e0a\u5c55\u793a\u4e86\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u968f\u673a\u68ee\u6797\u5b9e\u73b0\u9ad8\u6548\u7684\u81ea\u7f16\u7801\uff0c\u4e3a\u6570\u636e\u5173\u7cfb\u63d0\u4f9b\u4f4e\u7ef4\u8868\u793a\uff0c\u5e76\u89e3\u51b3\u89e3\u7801\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u53ef\u89c6\u5316\u3001\u538b\u7f29\u3001\u805a\u7c7b\u548c\u53bb\u566a\u7b49\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u975e\u53c2\u6570\u7edf\u8ba1\u548c\u8c31\u56fe\u7406\u8bba\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u3001\u5206\u88c2\u91cd\u6807\u8bb0\u548c\u6700\u8fd1\u90bb\u56de\u5f52\u5b66\u4e60\u4f4e\u7ef4\u5d4c\u5165\u5e76\u89e3\u51b3\u89e3\u7801\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8868\u683c\u3001\u56fe\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\u7b49\u591a\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u7f16\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.21499", "pdf": "https://arxiv.org/pdf/2505.21499", "abs": "https://arxiv.org/abs/2505.21499", "authors": ["Haowei Wang", "Junjie Wang", "Xiaojun Jia", "Rupeng Zhang", "Mingyang Li", "Zhe Liu", "Yang Liu", "Qing Wang"], "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.", "AI": {"tldr": "AdInject\u662f\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684Web\u4ee3\u7406\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u4e92\u8054\u7f51\u5e7f\u544a\u6295\u653e\u6ce8\u5165\u6076\u610f\u5185\u5bb9\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u6027\u73af\u5883\u6ce8\u5165\u653b\u51fb\u7684\u7814\u7a76\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002AdInject\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u73b0\u5b9e\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "AdInject\u901a\u8fc7\u8bbe\u8ba1\u6076\u610f\u5e7f\u544a\u5185\u5bb9\u8bef\u5bfc\u4ee3\u7406\uff0c\u5e76\u5229\u7528VLM\u4f18\u5316\u5e7f\u544a\u5185\u5bb9\u4ee5\u589e\u5f3a\u653b\u51fb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc760%\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63a5\u8fd1100%\u3002", "conclusion": "\u5e7f\u544a\u6295\u653e\u662fWeb\u4ee3\u7406\u5b89\u5168\u7684\u73b0\u5b9e\u5a01\u80c1\uff0c\u4e9f\u9700\u5f00\u53d1\u9632\u5fa1\u673a\u5236\u3002", "relevance": 40.0}}
