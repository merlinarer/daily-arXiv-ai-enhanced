{"id": "2505.10643", "pdf": "https://arxiv.org/pdf/2505.10643", "abs": "https://arxiv.org/abs/2505.10643", "authors": ["Shuchen Guo", "Yun Wang", "Jichao Yu", "Xuansheng Wu", "Bilgehan Ayik", "Field M. Watts", "Ehsan Latif", "Ninghao Liu", "Lei Liu", "Xiaoming Zhai"], "title": "Artificial Intelligence Bias on English Language Learners in Automatic Scoring", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study investigated potential scoring biases and disparities toward\nEnglish Language Learners (ELLs) when using automatic scoring systems for\nmiddle school students' written responses to science assessments. We\nspecifically focus on examining how unbalanced training data with ELLs\ncontributes to scoring bias and disparities. We fine-tuned BERT with four\ndatasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting\nthe real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced\nmixed dataset with equal representation of both groups. The study analyzed 21\nassessment items: 10 items with about 30,000 ELL responses, five items with\nabout 1,000 ELL responses, and six items with about 200 ELL responses. Scoring\naccuracy (Acc) was calculated and compared to identify bias using Friedman\ntests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and\nthen calculated the differences in MSGs generated through both the human and AI\nmodels to identify the scoring disparities. We found that no AI bias and\ndistorted disparities between ELLs and non-ELLs were found when the training\ndataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could\nexist if the sample size is limited (ELL = 200).", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u5bf9\u82f1\u8bed\u5b66\u4e60\u8005\uff08ELLs\uff09\u7684\u6f5c\u5728\u8bc4\u5206\u504f\u89c1\uff0c\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u91cf\u8db3\u591f\u5927\u65f6\uff08ELL=30,000\u62161,000\uff09\u65e0\u663e\u8457\u504f\u89c1\uff0c\u4f46\u6837\u672c\u91cf\u5c0f\uff08ELL=200\uff09\u65f6\u53ef\u80fd\u5b58\u5728\u504f\u89c1\u3002", "motivation": "\u63a2\u7a76\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u5728\u79d1\u5b66\u8bc4\u4f30\u4e2d\u5bf9ELLs\u7684\u8bc4\u5206\u504f\u89c1\uff0c\u5c24\u5176\u662f\u8bad\u7ec3\u6570\u636e\u4e0d\u5e73\u8861\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528BERT\u5fae\u8c03\u56db\u79cd\u6570\u636e\u96c6\uff08ELLs\u3001\u975eELLs\u3001\u4e0d\u5e73\u8861\u6df7\u5408\u3001\u5e73\u8861\u6df7\u5408\uff09\uff0c\u5206\u679021\u9879\u8bc4\u4f30\u7684\u8bc4\u5206\u51c6\u786e\u6027\u548c\u504f\u89c1\u3002", "result": "\u8bad\u7ec3\u6570\u636e\u91cf\u5927\u65f6\u65e0\u663e\u8457\u504f\u89c1\uff0c\u6837\u672c\u91cf\u5c0f\u65f6\u53ef\u80fd\u5b58\u5728\u504f\u89c1\u3002", "conclusion": "\u8bad\u7ec3\u6570\u636e\u91cf\u662f\u51cf\u5c11\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u504f\u89c1\u7684\u5173\u952e\u56e0\u7d20\u3002", "relevance": 40.0}}
{"id": "2505.10714", "pdf": "https://arxiv.org/pdf/2505.10714", "abs": "https://arxiv.org/abs/2505.10714", "authors": ["Bowen Jiang", "Yangxinyu Xie", "Xiaomeng Wang", "Jiashu He", "Joshua Bergerson", "John K Hutchison", "Jordan Branham", "Camillo J Taylor", "Tanwi Mallick"], "title": "GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?", "categories": ["cs.CL"], "comment": null, "summary": "We present GeoGrid-Bench, a benchmark designed to evaluate the ability of\nfoundation models to understand geo-spatial data in the grid structure.\nGeo-spatial datasets pose distinct challenges due to their dense numerical\nvalues, strong spatial and temporal dependencies, and unique multimodal\nrepresentations including tabular data, heatmaps, and geographic\nvisualizations. To assess how foundation models can support scientific research\nin this domain, GeoGrid-Bench features large-scale, real-world data covering 16\nclimate variables across 150 locations and extended time frames. The benchmark\nincludes approximately 3,200 question-answer pairs, systematically generated\nfrom 8 domain expert-curated templates to reflect practical tasks encountered\nby human scientists. These range from basic queries at a single location and\ntime to complex spatiotemporal comparisons across regions and periods. Our\nevaluation reveals that vision-language models perform best overall, and we\nprovide a fine-grained analysis of the strengths and limitations of different\nfoundation models in different geo-spatial tasks. This benchmark offers clearer\ninsights into how foundation models can be effectively applied to geo-spatial\ndata analysis and used to support scientific research.", "AI": {"tldr": "GeoGrid-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5904\u7406\u5730\u7406\u7a7a\u95f4\u7f51\u683c\u6570\u636e\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3200\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d616\u79cd\u6c14\u5019\u53d8\u91cf\u548c150\u4e2a\u5730\u70b9\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5730\u7406\u7a7a\u95f4\u6570\u636e\u5177\u6709\u5bc6\u96c6\u6570\u503c\u3001\u65f6\u7a7a\u4f9d\u8d56\u548c\u591a\u6a21\u6001\u8868\u793a\u7684\u7279\u70b9\uff0c\u9700\u8981\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6a21\u677f\u751f\u6210\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8bc4\u4f30\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6574\u4f53\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u6709\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u3002", "conclusion": "GeoGrid-Bench\u4e3a\u5730\u7406\u7a7a\u95f4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\u6a21\u578b\u5e94\u7528\u7684\u6e05\u6670\u89c1\u89e3\u3002", "relevance": 40.0}}
{"id": "2505.10717", "pdf": "https://arxiv.org/pdf/2505.10717", "abs": "https://arxiv.org/abs/2505.10717", "authors": ["Jean-Philippe Corbeil", "Amin Dada", "Jean-Michel Attendu", "Asma Ben Abacha", "Alessandro Sordoni", "Lucas Caccia", "Fran\u00e7ois Beaulieu", "Thomas Lin", "Jens Kleesiek", "Paul Vozila"], "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMediPhi\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u9ad8\u6548\u9002\u914d\u5230\u4e34\u5e8a\u9886\u57df\uff0c\u7ed3\u5408\u9884\u6307\u4ee4\u8c03\u4f18\u3001\u6a21\u578b\u5408\u5e76\u548c\u4efb\u52a1\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u7684\u9ad8\u6210\u672c\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u53ca\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u9002\u914d\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u9884\u6307\u4ee4\u8c03\u4f18\u3001\u6a21\u578b\u5408\u5e76\u548c\u4e34\u5e8a\u4efb\u52a1\u5bf9\u9f50\uff0c\u5f00\u53d1\u4e863.8B\u53c2\u6570\u7684MediPhi\u6a21\u578b\uff0c\u5e76\u6269\u5c55\u4e86CLUE+\u57fa\u51c6\u3002", "result": "MediPhi\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548cGPT-4-0125\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6MediFlow\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "MediPhi\u6846\u67b6\u4e3a\u4e34\u5e8a\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u8bed\u8a00\u6a21\u578b\u9002\u914d\u65b9\u6848\u3002", "relevance": 75.0}}
{"id": "2505.10718", "pdf": "https://arxiv.org/pdf/2505.10718", "abs": "https://arxiv.org/abs/2505.10718", "authors": ["Siddharth Suresh", "Kushin Mukherjee", "Tyler Giallanza", "Xizheng Yu", "Mia Patil", "Jonathan D. Cohen", "Timothy T. Rogers"], "title": "AI-enhanced semantic feature norms for 786 concepts", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures", "summary": "Semantic feature norms have been foundational in the study of human\nconceptual knowledge, yet traditional methods face trade-offs between\nconcept/feature coverage and verifiability of quality due to the\nlabor-intensive nature of norming studies. Here, we introduce a novel approach\nthat augments a dataset of human-generated feature norms with responses from\nlarge language models (LLMs) while verifying the quality of norms against\nreliable human judgments. We find that our AI-enhanced feature norm dataset,\nNOVA: Norms Optimized Via AI, shows much higher feature density and overlap\namong concepts while outperforming a comparable human-only norm dataset and\nword-embedding models in predicting people's semantic similarity judgments.\nTaken together, we demonstrate that human conceptual knowledge is richer than\ncaptured in previous norm datasets and show that, with proper validation, LLMs\ncan serve as powerful tools for cognitive science research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u751f\u6210\u7684\u7279\u5f81\u89c4\u8303\u548cLLM\u7684\u54cd\u5e94\uff0c\u521b\u5efa\u4e86AI\u589e\u5f3a\u7684\u7279\u5f81\u89c4\u8303\u6570\u636e\u96c6NOVA\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7279\u5f81\u5bc6\u5ea6\u548c\u6982\u5ff5\u91cd\u53e0\uff0c\u5e76\u5728\u9884\u6d4b\u4eba\u7c7b\u8bed\u4e49\u76f8\u4f3c\u6027\u5224\u65ad\u65b9\u9762\u4f18\u4e8e\u7eaf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u8bcd\u5d4c\u5165\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u6982\u5ff5\u77e5\u8bc6\u7814\u7a76\u65b9\u6cd5\u5728\u6982\u5ff5/\u7279\u5f81\u8986\u76d6\u8303\u56f4\u548c\u8d28\u91cf\u9a8c\u8bc1\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u56e0\u4e3a\u89c4\u8303\u7814\u7a76\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u3002", "method": "\u901a\u8fc7\u5c06\u4eba\u7c7b\u751f\u6210\u7684\u7279\u5f81\u89c4\u8303\u4e0eLLM\u7684\u54cd\u5e94\u7ed3\u5408\uff0c\u5e76\u9a8c\u8bc1\u5176\u8d28\u91cf\uff0c\u521b\u5efa\u4e86AI\u589e\u5f3a\u7684\u7279\u5f81\u89c4\u8303\u6570\u636e\u96c6NOVA\u3002", "result": "NOVA\u6570\u636e\u96c6\u5728\u7279\u5f81\u5bc6\u5ea6\u548c\u6982\u5ff5\u91cd\u53e0\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u5728\u9884\u6d4b\u4eba\u7c7b\u8bed\u4e49\u76f8\u4f3c\u6027\u5224\u65ad\u65b9\u9762\u4f18\u4e8e\u7eaf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u8bcd\u5d4c\u5165\u6a21\u578b\u3002", "conclusion": "\u4eba\u7c7b\u6982\u5ff5\u77e5\u8bc6\u6bd4\u4ee5\u5f80\u89c4\u8303\u6570\u636e\u96c6\u6240\u6355\u6349\u7684\u66f4\u4e3a\u4e30\u5bcc\uff0c\u4e14\u7ecf\u8fc7\u9002\u5f53\u9a8c\u8bc1\u7684LLM\u53ef\u4ee5\u6210\u4e3a\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\u7684\u5f3a\u5927\u5de5\u5177\u3002", "relevance": 75.0}}
{"id": "2505.10653", "pdf": "https://arxiv.org/pdf/2505.10653", "abs": "https://arxiv.org/abs/2505.10653", "authors": ["Sandeep Neema", "Susmit Jha", "Adam Nagel", "Ethan Lew", "Chandrasekar Sureshkumar", "Aleksa Gordic", "Chase Shimmin", "Hieu Nguygen", "Paul Eremenko"], "title": "On the Evaluation of Engineering Artificial General Intelligence", "categories": ["cs.AI", "I.2; J.2; J.6"], "comment": "21 pages", "summary": "We discuss the challenges and propose a framework for evaluating engineering\nartificial general intelligence (eAGI) agents. We consider eAGI as a\nspecialization of artificial general intelligence (AGI), deemed capable of\naddressing a broad range of problems in the engineering of physical systems and\nassociated controllers. We exclude software engineering for a tractable scoping\nof eAGI and expect dedicated software engineering AI agents to address the\nsoftware implementation challenges. Similar to human engineers, eAGI agents\nshould possess a unique blend of background knowledge (recall and retrieve) of\nfacts and methods, demonstrate familiarity with tools and processes, exhibit\ndeep understanding of industrial components and well-known design families, and\nbe able to engage in creative problem solving (analyze and synthesize),\ntransferring ideas acquired in one context to another. Given this broad\nmandate, evaluating and qualifying the performance of eAGI agents is a\nchallenge in itself and, arguably, a critical enabler to developing eAGI\nagents. In this paper, we address this challenge by proposing an extensible\nevaluation framework that specializes and grounds Bloom's taxonomy - a\nframework for evaluating human learning that has also been recently used for\nevaluating LLMs - in an engineering design context. Our proposed framework\nadvances the state of the art in benchmarking and evaluation of AI agents in\nterms of the following: (a) developing a rich taxonomy of evaluation questions\nspanning from methodological knowledge to real-world design problems; (b)\nmotivating a pluggable evaluation framework that can evaluate not only textual\nresponses but also evaluate structured design artifacts such as CAD models and\nSysML models; and (c) outlining an automatable procedure to customize the\nevaluation benchmark to different engineering contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5de5\u7a0b\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08eAGI\uff09\u7684\u6846\u67b6\uff0c\u57fa\u4e8eBloom\u5206\u7c7b\u6cd5\uff0c\u9002\u7528\u4e8e\u5de5\u7a0b\u8bbe\u8ba1\u9886\u57df\u3002", "motivation": "\u8bc4\u4f30eAGI\u4ee3\u7406\u7684\u6027\u80fd\u662f\u5f00\u53d1eAGI\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8eBloom\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u4ece\u65b9\u6cd5\u8bba\u77e5\u8bc6\u5230\u5b9e\u9645\u8bbe\u8ba1\u95ee\u9898\u7684\u8bc4\u4f30\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86\u4e30\u5bcc\u7684\u8bc4\u4f30\u95ee\u9898\u5206\u7c7b\u6cd5\uff0c\u652f\u6301\u6587\u672c\u548c\u7ed3\u6784\u5316\u8bbe\u8ba1\u5de5\u4ef6\u7684\u8bc4\u4f30\uff0c\u5e76\u53ef\u81ea\u52a8\u5316\u5b9a\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aeAGI\u4ee3\u7406\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u7a0b\u573a\u666f\u3002", "relevance": 60.0}}
{"id": "2505.10597", "pdf": "https://arxiv.org/pdf/2505.10597", "abs": "https://arxiv.org/abs/2505.10597", "authors": ["Jiazheng Zhang", "Wenqing Jing", "Zizhuo Zhang", "Zhiheng Xi", "Shihan Dou", "Rongxiang Weng", "Jiahuan Li", "Jingang Wang", "MingXu Cai", "Shibo Hong", "Tao Gui", "Qi Zhang"], "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models (RMs) are essential for aligning large language models (LLMs)\nwith human values. However, noisy preferences in human feedback often lead to\nreward misgeneralization, where RMs overfit to spurious patterns and provide\nmisleading signals during policy optimization. We systematically analyze the\ntraining dynamics of preference pairs and identify that noisy examples are\nharder to fit and introduce instability. Empirical evidence shows that LLMs\noptimized using reward models trained on full noisy datasets perform worse than\nthose trained on filtered, high-quality preferences. To address this, we\npropose Collaborative Reward Modeling (CRM), an online framework that enhances\nrobustness by combining peer review and curriculum learning. Two reward models\nare trained in parallel and assess each other's data selections to filter out\npotential noise. Curriculum learning structures the preference data from easy\nto hard, ensuring synchronized training and stable feedback. Extensive\nexperiments demonstrate that CRM improves generalization, with up to 9.94\npoints of accuracy gain on RewardBench under 40 percent label noise. CRM is\nalso compatible with implicit-reward alignment methods, offering a practical\nand versatile strategy for robust alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u5956\u52b1\u5efa\u6a21\uff08CRM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u540c\u884c\u8bc4\u5ba1\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u5728\u566a\u58f0\u504f\u597d\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u6548\u679c\u3002", "motivation": "\u4eba\u7c7b\u53cd\u9988\u4e2d\u7684\u566a\u58f0\u504f\u597d\u4f1a\u5bfc\u81f4\u5956\u52b1\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u4ece\u800c\u5728\u7b56\u7565\u4f18\u5316\u4e2d\u63d0\u4f9b\u8bef\u5bfc\u4fe1\u53f7\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5206\u6790\u4e86\u504f\u597d\u5bf9\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u534f\u4f5c\u5956\u52b1\u5efa\u6a21\uff08CRM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u8bad\u7ec3\u4e24\u4e2a\u5956\u52b1\u6a21\u578b\u5e76\u4e92\u76f8\u8bc4\u4f30\u6570\u636e\u9009\u62e9\u6765\u8fc7\u6ee4\u566a\u58f0\uff0c\u540c\u65f6\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u4ece\u6613\u5230\u96be\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCRM\u572840%\u6807\u7b7e\u566a\u58f0\u4e0b\u80fd\u63d0\u5347RewardBench\u51c6\u786e\u73879.94\u5206\uff0c\u4e14\u4e0e\u9690\u5f0f\u5956\u52b1\u5bf9\u9f50\u65b9\u6cd5\u517c\u5bb9\u3002", "conclusion": "CRM\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u901a\u7528\u7684\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10575", "pdf": "https://arxiv.org/pdf/2505.10575", "abs": "https://arxiv.org/abs/2505.10575", "authors": ["Adnan Ahmad", "Bahareh Nakisa", "Mohammad Naim Rastgoo"], "title": "Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Emotion recognition through physiological signals such as\nelectroencephalogram (EEG) has become an essential aspect of affective\ncomputing and provides an objective way to capture human emotions. However,\nphysiological data characterized by cross-subject variability and noisy labels\nhinder the performance of emotion recognition models. Existing domain\nadaptation and continual learning methods struggle to address these issues,\nespecially under realistic conditions where data is continuously streamed and\nunlabeled. To overcome these limitations, we propose a novel bi-level\nself-supervised continual learning framework, SSOCL, based on a dynamic memory\nbuffer. This bi-level architecture iteratively refines the dynamic buffer and\npseudo-label assignments to effectively retain representative samples, enabling\ngeneralization from continuous, unlabeled physiological data streams for\nemotion recognition. The assigned pseudo-labels are subsequently leveraged for\naccurate emotion prediction. Key components of the framework, including a fast\nadaptation module and a cluster-mapping module, enable robust learning and\neffective handling of evolving data streams. Experimental validation on two\nmainstream EEG tasks demonstrates the framework's ability to adapt to\ncontinuous data streams while maintaining strong generalization across\nsubjects, outperforming existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSOCL\u7684\u53cc\u5c42\u81ea\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u8fde\u7eed\u65e0\u6807\u8bb0\u7684\u751f\u7406\u6570\u636e\u6d41\uff0c\u63d0\u5347\u60c5\u7eea\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u751f\u7406\u6570\u636e\uff08\u5982EEG\uff09\u5728\u60c5\u7eea\u8bc6\u522b\u4e2d\u5177\u6709\u8de8\u4e3b\u4f53\u53d8\u5f02\u6027\u548c\u566a\u58f0\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fde\u7eed\u65e0\u6807\u8bb0\u6570\u636e\u6d41\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u52a8\u6001\u8bb0\u5fc6\u7f13\u51b2\u533a\u7684\u53cc\u5c42\u81ea\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u5feb\u901f\u9002\u5e94\u6a21\u5757\u548c\u805a\u7c7b\u6620\u5c04\u6a21\u5757\uff0c\u8fed\u4ee3\u4f18\u5316\u7f13\u51b2\u533a\u4e0e\u4f2a\u6807\u7b7e\u5206\u914d\u3002", "result": "\u5728\u4e24\u4e2a\u4e3b\u6d41EEG\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSOCL\u80fd\u591f\u9002\u5e94\u8fde\u7eed\u6570\u636e\u6d41\u5e76\u4fdd\u6301\u8de8\u4e3b\u4f53\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SSOCL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u65e0\u6807\u8bb0\u751f\u7406\u6570\u636e\u6d41\u7684\u60c5\u7eea\u8bc6\u522b\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.10719", "pdf": "https://arxiv.org/pdf/2505.10719", "abs": "https://arxiv.org/abs/2505.10719", "authors": ["Tom\u00e1s Vergara-Browne", "\u00c1lvaro Soto"], "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Motivated by the surge of large language models, there has been a push to\nformally characterize the symbolic abilities intrinsic to the transformer\narchitecture. A programming language, called RASP, has been proposed, which can\nbe directly compiled into transformer weights to implement these algorithms.\nHowever, the tasks that can be implemented in RASP are often uncommon to learn\nfrom natural unsupervised data, showing a mismatch between theoretical\ncapabilities of the transformer architecture, and the practical learnability of\nthese capabilities from unsupervised data. We propose tracr-injection, a method\nthat allows us to distill algorithms written in RASP directly into a\npre-trained language model. We showcase our method by injecting 3 different\nalgorithms into a language model. We show how our method creates an\ninterpretable subspace within the model's residual stream, which can be decoded\ninto the variables present in the code of the RASP algorithm. Additionally, we\nfound that the proposed method can improve out of distribution performance\ncompared to our baseline, indicating that indeed a more symbolic mechanism is\ntaking place in the inner workings of the model. We release the code used to\nrun our experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3atracr-injection\u7684\u65b9\u6cd5\uff0c\u5c06RASP\u7f16\u5199\u7684\u7b97\u6cd5\u76f4\u63a5\u6ce8\u5165\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76Transformer\u67b6\u6784\u7684\u7b26\u53f7\u80fd\u529b\u4e0e\u4ece\u65e0\u76d1\u7763\u6570\u636e\u4e2d\u5b66\u4e60\u8fd9\u4e9b\u80fd\u529b\u7684\u5b9e\u9645\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fatracr-injection\u65b9\u6cd5\uff0c\u5c06RASP\u7b97\u6cd5\u6ce8\u5165\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5176\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u6ce8\u51653\u79cd\u7b97\u6cd5\uff0c\u521b\u5efa\u4e86\u53ef\u89e3\u91ca\u7684\u5b50\u7a7a\u95f4\uff0c\u5e76\u63d0\u9ad8\u4e86\u5206\u5e03\u5916\u6027\u80fd\u3002", "conclusion": "tracr-injection\u65b9\u6cd5\u4e3aTransformer\u7684\u7b26\u53f7\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u8df5\u9a8c\u8bc1\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10670", "pdf": "https://arxiv.org/pdf/2505.10670", "abs": "https://arxiv.org/abs/2505.10670", "authors": ["Jan Chojnacki"], "title": "Interpretable Risk Mitigation in LLM Agent Systems", "categories": ["cs.AI", "cs.CY", "cs.GT"], "comment": null, "summary": "Autonomous agents powered by large language models (LLMs) enable novel use\ncases in domains where responsible action is increasingly important. Yet the\ninherent unpredictability of LLMs raises safety concerns about agent\nreliability. In this work, we explore agent behaviour in a toy, game-theoretic\nenvironment based on a variation of the Iterated Prisoner's Dilemma. We\nintroduce a strategy-modification method-independent of both the game and the\nprompt-by steering the residual stream with interpretable features extracted\nfrom a sparse autoencoder latent space. Steering with the good-faith\nnegotiation feature lowers the average defection probability by 28 percentage\npoints. We also identify feasible steering ranges for several open-source LLM\nagents. Finally, we hypothesise that game-theoretic evaluation of LLM agents,\ncombined with representation-steering alignment, can generalise to real-world\napplications on end-user devices and embodied platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\u6765\u5f15\u5bfcLLM\u4ee3\u7406\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u5728\u535a\u5f08\u8bba\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u964d\u4f4e\u4e86\u80cc\u53db\u6982\u7387\u3002", "motivation": "\u7814\u7a76LLM\u4ee3\u7406\u5728\u9700\u8981\u8d1f\u8d23\u4efb\u884c\u52a8\u7684\u9886\u57df\u4e2d\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u53ca\u5176\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u72ec\u7acb\u4e8e\u6e38\u620f\u548c\u63d0\u793a\uff0c\u901a\u8fc7\u5f15\u5bfc\u6b8b\u5dee\u6d41\u6765\u4fee\u6539\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u5f15\u5bfc\u2018\u5584\u610f\u8c08\u5224\u2019\u7279\u5f81\u4f7f\u5e73\u5747\u80cc\u53db\u6982\u7387\u964d\u4f4e\u4e8628\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u786e\u5b9a\u4e86\u53ef\u884c\u5f15\u5bfc\u8303\u56f4\u3002", "conclusion": "\u535a\u5f08\u8bba\u8bc4\u4f30\u4e0e\u8868\u793a\u5f15\u5bfc\u5bf9\u9f50\u53ef\u63a8\u5e7f\u5230\u73b0\u5b9e\u5e94\u7528\u3002", "relevance": 85.0}}
{"id": "2505.10599", "pdf": "https://arxiv.org/pdf/2505.10599", "abs": "https://arxiv.org/abs/2505.10599", "authors": ["Jiaxuan Liu", "Zhenhua Ling"], "title": "UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recent neural codec language models have made great progress in the field of\ntext-to-speech (TTS), but controllable emotional TTS still faces many\nchallenges. Traditional methods rely on predefined discrete emotion labels to\ncontrol emotion categories and intensities, which can't capture the complexity\nand continuity of human emotional perception and expression. The lack of\nlarge-scale emotional speech datasets with balanced emotion distributions and\nfine-grained emotion annotations often causes overfitting in synthesis models\nand impedes effective emotion control. To address these issues, we propose\nUDDETTS, a neural codec language model unifying discrete and dimensional\nemotions for controllable emotional TTS. This model introduces the\ninterpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion\ndescription and supports emotion control driven by either discrete emotion\nlabels or nonlinearly quantified ADV values. Furthermore, a semi-supervised\ntraining strategy is designed to comprehensively utilize diverse speech\ndatasets with different types of emotion annotations to train the UDDETTS.\nExperiments show that UDDETTS achieves linear emotion control along the three\ndimensions of ADV space, and exhibits superior end-to-end emotional speech\nsynthesis capabilities.", "AI": {"tldr": "UDDETTS\u662f\u4e00\u4e2a\u795e\u7ecf\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u79bb\u6563\u548c\u7ef4\u5ea6\u60c5\u611f\u63a7\u5236\uff0c\u7528\u4e8e\u53ef\u63a7\u60c5\u611fTTS\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u60c5\u611f\u590d\u6742\u6027\u548c\u8fde\u7eed\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfTTS\u65b9\u6cd5\u4f9d\u8d56\u79bb\u6563\u60c5\u611f\u6807\u7b7e\uff0c\u65e0\u6cd5\u6355\u6349\u60c5\u611f\u7684\u590d\u6742\u6027\u548c\u8fde\u7eed\u6027\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u5e73\u8861\u7684\u60c5\u611f\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faUDDETTS\u6a21\u578b\uff0c\u5f15\u5165\u53ef\u89e3\u91ca\u7684ADV\u7a7a\u95f4\u63cf\u8ff0\u60c5\u611f\uff0c\u652f\u6301\u79bb\u6563\u6807\u7b7e\u6216ADV\u503c\u9a71\u52a8\u7684\u60c5\u611f\u63a7\u5236\uff0c\u5e76\u91c7\u7528\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUDDETTS\u5728ADV\u7a7a\u95f4\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u7ebf\u6027\u60c5\u611f\u63a7\u5236\uff0c\u5e76\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u60c5\u611f\u8bed\u97f3\u5408\u6210\u80fd\u529b\u3002", "conclusion": "UDDETTS\u4e3a\u53ef\u63a7\u60c5\u611fTTS\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.10579", "pdf": "https://arxiv.org/pdf/2505.10579", "abs": "https://arxiv.org/abs/2505.10579", "authors": ["Germani Elodie", "Selin T\u00fcrk Ilayda", "Zeineddine Fatima", "Mourad Charbel", "Albarqouni Shadi"], "title": "Bias and Generalizability of Foundation Models across Datasets in Breast Mammography", "categories": ["cs.CV"], "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer-Assisted Intervention (MICCAI) 2025", "summary": "Over the past decades, computer-aided diagnosis tools for breast cancer have\nbeen developed to enhance screening procedures, yet their clinical adoption\nremains challenged by data variability and inherent biases. Although foundation\nmodels (FMs) have recently demonstrated impressive generalizability and\ntransfer learning capabilities by leveraging vast and diverse datasets, their\nperformance can be undermined by spurious correlations that arise from\nvariations in image quality, labeling uncertainty, and sensitive patient\nattributes. In this work, we explore the fairness and bias of FMs for breast\nmammography classification by leveraging a large pool of datasets from diverse\nsources-including data from underrepresented regions and an in-house dataset.\nOur extensive experiments show that while modality-specific pre-training of FMs\nenhances performance, classifiers trained on features from individual datasets\nfail to generalize across domains. Aggregating datasets improves overall\nperformance, yet does not fully mitigate biases, leading to significant\ndisparities across under-represented subgroups such as extreme breast densities\nand age groups. Furthermore, while domain-adaptation strategies can reduce\nthese disparities, they often incur a performance trade-off. In contrast,\nfairness-aware techniques yield more stable and equitable performance across\nsubgroups. These findings underscore the necessity of incorporating rigorous\nfairness evaluations and mitigation strategies into FM-based models to foster\ninclusive and generalizable AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u4e73\u817a\u764c\u4e73\u817aX\u5149\u5206\u7c7b\u4e2d\u7684\u516c\u5e73\u6027\u548c\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u805a\u5408\u548c\u9886\u57df\u9002\u5e94\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u516c\u5e73\u6027\u6280\u672f\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5728\u533b\u7597\u5f71\u50cf\u5206\u7c7b\u4e2d\u7684\u516c\u5e73\u6027\u548c\u504f\u89c1\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u591a\u6837\u6027\u548c\u504f\u89c1\u5bf9\u4e34\u5e8a\u5e94\u7528\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u6e90\u6570\u636e\u96c6\uff08\u5305\u62ec\u6765\u81ea\u4ee3\u8868\u6027\u4e0d\u8db3\u5730\u533a\u7684\u6570\u636e\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u6a21\u6001\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u6570\u636e\u96c6\u805a\u5408\u548c\u9886\u57df\u9002\u5e94\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u6a21\u6001\u7279\u5b9a\u9884\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u6570\u636e\u96c6\u805a\u5408\u6539\u5584\u6027\u80fd\u4f46\u672a\u5b8c\u5168\u6d88\u9664\u504f\u89c1\uff1b\u516c\u5e73\u6027\u6280\u672f\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u9700\u5c06\u4e25\u683c\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u548c\u7f13\u89e3\u7b56\u7565\u7eb3\u5165\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u5305\u5bb9\u6027\u548c\u6cdb\u5316\u6027AI\u3002", "relevance": 40.0}}
{"id": "2505.10736", "pdf": "https://arxiv.org/pdf/2505.10736", "abs": "https://arxiv.org/abs/2505.10736", "authors": ["Ximing Dong", "Shaowei Wang", "Dayi Lin", "Ahmed E. Hassan"], "title": "Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Optimizing Large Language Model (LLM) performance requires well-crafted\nprompts, but manual prompt engineering is labor-intensive and often\nineffective. Automated prompt optimization techniques address this challenge\nbut the majority of them rely on randomly selected evaluation subsets, which\nfail to represent the full dataset, leading to unreliable evaluations and\nsuboptimal prompts. Existing coreset selection methods, designed for LLM\nbenchmarking, are unsuitable for prompt optimization due to challenges in\nclustering similar samples, high data collection costs, and the unavailability\nof performance data for new or private datasets. To overcome these issues, we\npropose IPOMP, an Iterative evaluation data selection for effective Prompt\nOptimization using real-time Model Performance. IPOMP is a two-stage approach\nthat selects representative and diverse samples using semantic clustering and\nboundary analysis, followed by iterative refinement with real-time model\nperformance data to replace redundant samples. Evaluations on the BIG-bench\ndataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by\nat least 57% compared with SOTA baselines, with minimal computational overhead\nbelow 1%. Furthermore, the results demonstrate that our real-time\nperformance-guided refinement approach can be universally applied to enhance\nexisting coreset selection methods.", "AI": {"tldr": "IPOMP\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u8fb9\u754c\u5206\u6790\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u5b9e\u65f6\u6a21\u578b\u6027\u80fd\u6570\u636e\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u6548\u679c\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u56e0\u968f\u673a\u8bc4\u4f30\u5b50\u96c6\u4e0d\u53ef\u9760\uff0c\u4e14\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u63d0\u793a\u4f18\u5316\u3002", "method": "IPOMP\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u8bed\u4e49\u805a\u7c7b\u548c\u8fb9\u754c\u5206\u6790\u9009\u62e9\u6837\u672c\uff0c\u518d\u901a\u8fc7\u5b9e\u65f6\u6027\u80fd\u6570\u636e\u8fed\u4ee3\u66ff\u6362\u5197\u4f59\u6837\u672c\u3002", "result": "\u5728BIG-bench\u6570\u636e\u96c6\u4e0a\uff0cIPOMP\u6548\u679c\u63d0\u53471.6%\u81f35.3%\uff0c\u7a33\u5b9a\u6027\u63d0\u5347\u81f3\u5c1157%\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e1%\u3002", "conclusion": "IPOMP\u662f\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u53ef\u589e\u5f3a\u73b0\u6709\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u63d0\u793a\u4f18\u5316\u3002", "relevance": 85.0}}
{"id": "2505.10705", "pdf": "https://arxiv.org/pdf/2505.10705", "abs": "https://arxiv.org/abs/2505.10705", "authors": ["Matej Hoffmann", "Shubhan Parag Patni"], "title": "Embodied AI in Machine Learning -- is it Really Embodied?", "categories": ["cs.AI", "cs.NE", "cs.RO", "68T40", "I.2.9"], "comment": "16 pages, 3 figures", "summary": "Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the\nmachine learning communities with the goal of leveraging current progress in AI\n(deep learning, transformers, large language and visual-language models) to\nempower robots. In this chapter we put this work in the context of \"Good\nOld-Fashioned Artificial Intelligence\" (GOFAI) (Haugeland, 1989) and the\nbehavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier\n2001). We claim that the AI-powered robots are only weakly embodied and inherit\nsome of the problems of GOFAI. Moreover, we review and critically discuss the\npossibility of cross-embodiment learning (Padalkar et al. 2024). We identify\nfundamental roadblocks and propose directions on how to make progress.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Embodied AI\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u5f53\u524dAI\u9a71\u52a8\u7684\u673a\u5668\u4eba\u4ec5\u5f31\u4f53\u73b0\uff0c\u5e76\u7ee7\u627f\u4e86GOFAI\u7684\u95ee\u9898\u3002\u4f5c\u8005\u8fd8\u8ba8\u8bba\u4e86\u8de8\u4f53\u73b0\u5b66\u4e60\u7684\u53ef\u80fd\u6027\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5c06\u5f53\u524dAI\u8fdb\u5c55\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u3001Transformer\uff09\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\uff0c\u540c\u65f6\u6307\u51fa\u5176\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u6279\u5224\u6027\u8ba8\u8bba\uff0c\u5206\u6790Embodied AI\u7684\u95ee\u9898\u548c\u8de8\u4f53\u73b0\u5b66\u4e60\u7684\u53ef\u80fd\u6027\u3002", "result": "\u53d1\u73b0\u5f53\u524dAI\u9a71\u52a8\u7684\u673a\u5668\u4eba\u4ec5\u5f31\u4f53\u73b0\uff0c\u5b58\u5728GOFAI\u9057\u7559\u95ee\u9898\uff0c\u5e76\u8bc6\u522b\u4e86\u8de8\u4f53\u73b0\u5b66\u4e60\u7684\u57fa\u672c\u969c\u788d\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u89e3\u51b3Embodied AI\u7684\u5f31\u4f53\u73b0\u95ee\u9898\u548c\u8de8\u4f53\u73b0\u5b66\u4e60\u7684\u6311\u6218\u3002", "relevance": 30.0}}
{"id": "2505.10600", "pdf": "https://arxiv.org/pdf/2505.10600", "abs": "https://arxiv.org/abs/2505.10600", "authors": ["Md. Ehsanul Haque", "Md. Saymon Hosen Polash", "Md Al-Imran Sanjida Simla", "Md Alomgir Hossain", "Sarwar Jahan"], "title": "Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Published paper of CSNT2025", "summary": "Due to the rapid growth in the number of Internet of Things (IoT) networks,\nthe cyber risk has increased exponentially, and therefore, we have to develop\neffective IDS that can work well with highly imbalanced datasets. A high rate\nof missed threats can be the result, as traditional machine learning models\ntend to struggle in identifying attacks when normal data volume is much higher\nthan the volume of attacks. For example, the dataset used in this study reveals\na strong class imbalance with 94,659 instances of the majority class and only\n28 instances of the minority class, making it quite challenging to determine\nrare attacks accurately. The challenges presented in this research are\naddressed by hybrid sampling techniques designed to improve data imbalance\ndetection accuracy in IoT domains. After applying these techniques, we evaluate\nthe performance of several machine learning models such as Random Forest, Soft\nVoting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer\nPerceptron (MLP), and Logistic Regression with respect to the classification of\ncyber-attacks. The obtained results indicate that the Random Forest model\nachieved the best performance with a Kappa score of 0.9903, test accuracy of\n0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting\nmodel, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of\ncombining model predictions. Overall, this work demonstrates the value of\nhybrid sampling combined with robust model and feature selection for\nsignificantly improving IoT security against cyber-attacks, especially in\nhighly imbalanced data environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91c7\u6837\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u7269\u8054\u7f51\uff08IoT\uff09\u7f51\u7edc\u4e2d\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u5165\u4fb5\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u7269\u8054\u7f51\u7f51\u7edc\u7684\u5feb\u901f\u589e\u957f\uff0c\u7f51\u7edc\u5b89\u5168\u98ce\u9669\u6025\u5267\u4e0a\u5347\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6570\u636e\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u3002", "method": "\u91c7\u7528\u6df7\u5408\u91c7\u6837\u6280\u672f\u6539\u5584\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u8f6f\u6295\u7968\u3001SVC\u3001KNN\u3001MLP\u548c\u903b\u8f91\u56de\u5f52\uff09\u7684\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cKappa\u5f97\u5206\u4e3a0.9903\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a0.9961\uff0cAUC\u4e3a0.9994\uff1b\u8f6f\u6295\u7968\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6df7\u5408\u91c7\u6837\u6280\u672f\u4e0e\u7a33\u5065\u7684\u6a21\u578b\u548c\u7279\u5f81\u9009\u62e9\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6570\u636e\u73af\u5883\u4e0b\u7684\u7269\u8054\u7f51\u5b89\u5168\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10583", "pdf": "https://arxiv.org/pdf/2505.10583", "abs": "https://arxiv.org/abs/2505.10583", "authors": ["Diogo Freitas", "Brigt H\u00e5vardstun", "C\u00e8sar Ferri", "Dar\u00edo Garigliotti", "Jan Arne Telle", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "54 pages (42 pages of appendix)", "summary": "Large language models have become multimodal, and many of them are said to\nintegrate their modalities using common representations. If this were true, a\ndrawing of a car as an image, for instance, should map to the similar area in\nthe latent space as a textual description of the strokes that conform the\ndrawing. To explore this in a black-box access regime to these models, we\npropose the use of machine teaching, a theory that studies the minimal set of\nexamples a teacher needs to choose so that the learner captures the concept. In\nthis paper we evaluate the complexity of teaching visual-language models a\nsubset of objects in the Quick, Draw! dataset using two presentations: raw\nimages as bitmaps and trace coordinates in TikZ format. The results indicate\nthat image-based representations generally require fewer segments and achieve\nhigher accuracy than coordinate-based representations. But, surprisingly, the\nteaching size usually ranks concepts similarly across both modalities, even\nwhen controlling for (a human proxy of) concept priors, suggesting that the\nsimplicity of concepts may be an inherent property that transcends modality\nrepresentations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u5171\u540c\u8868\u793a\uff0c\u901a\u8fc7\u673a\u5668\u6559\u5b66\u7406\u8bba\u8bc4\u4f30\u4e86\u4e24\u79cd\u8868\u793a\u65b9\u5f0f\u7684\u6559\u5b66\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u56fe\u50cf\u8868\u793a\u901a\u5e38\u66f4\u9ad8\u6548\uff0c\u4f46\u6982\u5ff5\u7b80\u5355\u6027\u53ef\u80fd\u8de8\u6a21\u6001\u4e00\u81f4\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u4e86\u6a21\u6001\u95f4\u7684\u5171\u540c\u8868\u793a\uff0c\u4ee5\u53ca\u4e0d\u540c\u6a21\u6001\u8868\u793a\u7684\u6559\u5b66\u590d\u6742\u5ea6\u3002", "method": "\u4f7f\u7528\u673a\u5668\u6559\u5b66\u7406\u8bba\uff0c\u5728Quick, Draw!\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u5bf9\u8c61\u5b50\u96c6\uff0c\u6bd4\u8f83\u56fe\u50cf\uff08\u4f4d\u56fe\uff09\u548c\u6587\u672c\uff08TikZ\u5750\u6807\uff09\u4e24\u79cd\u8868\u793a\u65b9\u5f0f\u7684\u6559\u5b66\u590d\u6742\u5ea6\u3002", "result": "\u56fe\u50cf\u8868\u793a\u901a\u5e38\u9700\u8981\u66f4\u5c11\u7684\u6559\u5b66\u7247\u6bb5\u4e14\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4f46\u6982\u5ff5\u7684\u6559\u5b66\u590d\u6742\u5ea6\u5728\u4e0d\u540c\u6a21\u6001\u95f4\u6392\u540d\u76f8\u4f3c\uff0c\u8868\u660e\u6982\u5ff5\u7b80\u5355\u6027\u53ef\u80fd\u8de8\u6a21\u6001\u4e00\u81f4\u3002", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u53ef\u80fd\u786e\u5b9e\u5b9e\u73b0\u4e86\u6a21\u6001\u95f4\u7684\u5171\u540c\u8868\u793a\uff0c\u6982\u5ff5\u7b80\u5355\u6027\u53ef\u80fd\u662f\u8de8\u6a21\u6001\u7684\u56fa\u6709\u5c5e\u6027\u3002", "relevance": 75.0}}
{"id": "2505.10740", "pdf": "https://arxiv.org/pdf/2505.10740", "abs": "https://arxiv.org/abs/2505.10740", "authors": ["Qiwei Peng", "Robert Moro", "Michal Gregor", "Ivan Srba", "Simon Ostermann", "Marian Simko", "Juraj Podrou\u017eek", "Mat\u00fa\u0161 Mesar\u010d\u00edk", "Jaroslav Kop\u010dan", "Anders S\u00f8gaard"], "title": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The rapid spread of online disinformation presents a global challenge, and\nmachine learning has been widely explored as a potential solution. However,\nmultilingual settings and low-resource languages are often neglected in this\nfield. To address this gap, we conducted a shared task on multilingual claim\nretrieval at SemEval 2025, aimed at identifying fact-checked claims that match\nnewly encountered claims expressed in social media posts across different\nlanguages. The task includes two subtracks: (1) a monolingual track, where\nsocial posts and claims are in the same language, and (2) a crosslingual track,\nwhere social posts and claims might be in different languages. A total of 179\nparticipants registered for the task contributing to 52 test submissions. 23\nout of 31 teams have submitted their system papers. In this paper, we report\nthe best-performing systems as well as the most common and the most effective\napproaches across both subtracks. This shared task, along with its dataset and\nparticipating systems, provides valuable insights into multilingual claim\nretrieval and automated fact-checking, supporting future research in this\nfield.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SemEval 2025\u4e2d\u5173\u4e8e\u591a\u8bed\u8a00\u58f0\u660e\u68c0\u7d22\u7684\u5171\u4eab\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u865a\u5047\u4fe1\u606f\u95ee\u9898\u3002", "motivation": "\u5728\u7ebf\u865a\u5047\u4fe1\u606f\u7684\u5feb\u901f\u4f20\u64ad\u662f\u4e00\u4e2a\u5168\u7403\u6027\u6311\u6218\uff0c\u4f46\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u4efb\u52a1\u7684\u5f62\u5f0f\uff0c\u7ec4\u7ec7\u53c2\u4e0e\u8005\u5f00\u53d1\u7cfb\u7edf\uff0c\u5206\u522b\u5728\u5355\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u68c0\u7d22\u5339\u914d\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u7684\u58f0\u660e\u3002", "result": "179\u540d\u53c2\u4e0e\u8005\u6ce8\u518c\uff0c52\u4efd\u6d4b\u8bd5\u63d0\u4ea4\uff0c23\u652f\u56e2\u961f\u63d0\u4ea4\u7cfb\u7edf\u8bba\u6587\u3002\u8bba\u6587\u603b\u7ed3\u4e86\u6700\u4f73\u8868\u73b0\u7cfb\u7edf\u548c\u6700\u6709\u6548\u65b9\u6cd5\u3002", "conclusion": "\u5171\u4eab\u4efb\u52a1\u3001\u6570\u636e\u96c6\u548c\u53c2\u4e0e\u7cfb\u7edf\u4e3a\u591a\u8bed\u8a00\u58f0\u660e\u68c0\u7d22\u548c\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "relevance": 40.0}}
{"id": "2505.10742", "pdf": "https://arxiv.org/pdf/2505.10742", "abs": "https://arxiv.org/abs/2505.10742", "authors": ["Brandon Lepine", "Gawesha Weerantunga", "Juho Kim", "Pamela Mishkin", "Matthew Beane"], "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u89e3\u591a\u8f6e\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u5957\u6307\u6807\u6765\u8861\u91cfLLM\u6027\u80fd\u548c\u7528\u6237\u7b56\u7565\u3002", "motivation": "\u73b0\u6709AI\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u6355\u6349\u4eba\u673a\u534f\u4f5c\u7684\u591a\u8f6e\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5f00\u53d1\u5305\u62ec\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u7ed3\u6784\u8fde\u8d2f\u6027\u7b49\u6307\u6807\uff0c\u5e76\u5f15\u5165\u201c\u4fe1\u606f\u524d\u6cbf\u201d\u6982\u5ff5\u3002", "result": "LLM\u751f\u6210\u5185\u5bb9\u7684\u6574\u5408\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u53d7\u54cd\u5e94\u4e0d\u8fde\u8d2f\u3001\u5b50\u4efb\u52a1\u591a\u6837\u6027\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u66f4\u5168\u9762\u7684\u4eba\u673a\u534f\u4f5c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3aAI\u589e\u5f3a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.10606", "pdf": "https://arxiv.org/pdf/2505.10606", "abs": "https://arxiv.org/abs/2505.10606", "authors": ["Hector Pasten", "Felipe Urrutia", "Hector Jimenez", "Cristian B. Calderon", "Crist\u00f3bal Rojas", "Alexander Kozachinskiy"], "title": "Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Understanding how Transformers work and how they process information is key\nto the theoretical and empirical advancement of these machines. In this work,\nwe demonstrate the existence of two phenomena in Transformers, namely isolation\nand continuity. Both of these phenomena hinder Transformers to learn even\nsimple pattern sequences. Isolation expresses that any learnable sequence must\nbe isolated from another learnable sequence, and hence some sequences cannot be\nlearned by a single Transformer at the same time. Continuity entails that an\nattractor basin forms around a learned sequence, such that any sequence falling\nin that basin will collapse towards the learned sequence. Here, we\nmathematically prove these phenomena emerge in all Transformers that use\ncompact positional encoding, and design rigorous experiments, demonstrating\nthat the theoretical limitations we shed light on occur on the practical scale.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86Transformer\u4e2d\u7684\u9694\u79bb\u548c\u8fde\u7eed\u6027\u73b0\u8c61\uff0c\u8fd9\u4e9b\u73b0\u8c61\u9650\u5236\u4e86\u5176\u5b66\u4e60\u7b80\u5355\u5e8f\u5217\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u9650\u5236\u7684\u5b9e\u9645\u5b58\u5728\u3002", "motivation": "\u7814\u7a76Transformer\u7684\u5de5\u4f5c\u539f\u7406\u53ca\u5176\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\uff0c\u4ee5\u63a8\u52a8\u7406\u8bba\u548c\u5b9e\u8bc1\u7684\u8fdb\u6b65\u3002", "method": "\u6570\u5b66\u8bc1\u660e\u9694\u79bb\u548c\u8fde\u7eed\u6027\u73b0\u8c61\u5728\u6240\u6709\u4f7f\u7528\u7d27\u51d1\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\u4e2d\u5b58\u5728\uff0c\u5e76\u8bbe\u8ba1\u4e25\u8c28\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u9694\u79bb\u73b0\u8c61\u5bfc\u81f4Transformer\u65e0\u6cd5\u540c\u65f6\u5b66\u4e60\u67d0\u4e9b\u5e8f\u5217\uff0c\u8fde\u7eed\u6027\u73b0\u8c61\u5219\u5bfc\u81f4\u5e8f\u5217\u5411\u5df2\u5b66\u4e60\u5e8f\u5217\u7684\u5438\u5f15\u5b50\u76c6\u5730\u584c\u7f29\u3002", "conclusion": "\u63ed\u793a\u4e86Transformer\u7684\u7406\u8bba\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.10584", "pdf": "https://arxiv.org/pdf/2505.10584", "abs": "https://arxiv.org/abs/2505.10584", "authors": ["Huafeng Shi", "Jianzhong Liang", "Rongchang Xie", "Xian Wu", "Cheng Chen", "Chang Liu"], "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates.", "AI": {"tldr": "Aquarius\u662f\u4e00\u4e2a\u9762\u5411\u8425\u9500\u573a\u666f\u7684\u5de5\u4e1a\u7ea7\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bb6\u65cf\uff0c\u652f\u6301\u6570\u5343xPU\u96c6\u7fa4\u548c\u6570\u767e\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u5de5\u7a0b\u67b6\u6784\u548c\u7b97\u6cd5\u521b\u65b0\u5b9e\u73b0\u9ad8\u6027\u80fd\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u65e8\u5728\u63ed\u5f00\u5de5\u4e1a\u7ea7\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7ec6\u8282\uff0c\u63a8\u52a8\u751f\u6210\u89c6\u9891\u793e\u533a\u7684\u53d1\u5c55\u3002", "method": "\u5305\u62ec\u5206\u5e03\u5f0f\u56fe\u4e0e\u89c6\u9891\u6570\u636e\u5904\u7406\u7ba1\u9053\u3001\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u67b6\u6784\uff08\u5982Single-DiT\u548cMultimodal-DiT\uff09\u3001\u9ad8\u6027\u80fd\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u3001\u591axPU\u5e76\u884c\u63a8\u7406\u52a0\u901f\u4ee5\u53ca\u591a\u79cd\u8425\u9500\u573a\u666f\u5e94\u7528\u3002", "result": "Aquarius\u5728\u9ad8\u4fdd\u771f\u3001\u591a\u5bbd\u9ad8\u6bd4\u548c\u957f\u65f6\u957f\u89c6\u9891\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b036% MFU\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.35\u500d\u3002", "conclusion": "Aquarius\u4e3a\u5de5\u4e1a\u7ea7\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8ba1\u5212\u5f00\u6e90\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u63a8\u52a8\u793e\u533a\u8fdb\u6b65\u3002", "relevance": 40.0}}
{"id": "2505.10772", "pdf": "https://arxiv.org/pdf/2505.10772", "abs": "https://arxiv.org/abs/2505.10772", "authors": ["Weiqin Wang", "Yile Wang", "Hui Huang"], "title": "Ranked Voting based Self-Consistency of Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Majority voting is considered an effective method to enhance chain-of-thought\nreasoning, as it selects the answer with the highest \"self-consistency\" among\ndifferent reasoning paths (Wang et al., 2023). However, previous\nchain-of-thought reasoning methods typically generate only a single answer in\neach trial, thereby ignoring the possibility of other potential answers. As a\nresult, these alternative answers are often overlooked in subsequent voting\nprocesses. In this work, we propose to generate ranked answers in each\nreasoning process and conduct ranked voting among multiple ranked answers from\ndifferent responses, thereby making the overall self-consistency more reliable.\nSpecifically, we use three ranked voting methods: Instant-runoff voting, Borda\ncount voting, and mean reciprocal rank voting. We validate our methods on six\ndatasets, including three multiple-choice and three open-ended\nquestion-answering tasks, using both advanced open-source and closed-source\nlarge language models. Extensive experimental results indicate that our\nproposed method outperforms the baselines, showcasing the potential of\nleveraging the information of ranked answers and using ranked voting to improve\nreasoning performance. The code is available at\nhttps://github.com/szu-tera/RankedVotingSC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u6392\u540d\u7b54\u6848\u5e76\u4f7f\u7528\u6392\u540d\u6295\u7968\u65b9\u6cd5\uff08\u5982\u5373\u65f6\u51b3\u9009\u6295\u7968\u3001\u535a\u5c14\u8fbe\u8ba1\u6570\u6295\u7968\u548c\u5e73\u5747\u5012\u6570\u6392\u540d\u6295\u7968\uff09\u6765\u63d0\u9ad8\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u53ea\u751f\u6210\u5355\u4e00\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u6f5c\u5728\u7b54\u6848\u7684\u53ef\u80fd\u6027\uff0c\u5bfc\u81f4\u6295\u7968\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u5728\u6bcf\u6b21\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u6392\u540d\u7b54\u6848\uff0c\u5e76\u4f7f\u7528\u4e09\u79cd\u6392\u540d\u6295\u7968\u65b9\u6cd5\u8fdb\u884c\u6295\u7968\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u6392\u540d\u7b54\u6848\u4fe1\u606f\u548c\u6392\u540d\u6295\u7968\u5bf9\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6392\u540d\u7b54\u6848\u4fe1\u606f\u548c\u6392\u540d\u6295\u7968\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.10749", "pdf": "https://arxiv.org/pdf/2505.10749", "abs": "https://arxiv.org/abs/2505.10749", "authors": ["Ashwath Vaithinathan Aravindan", "Zhisheng Tang", "Mayank Kejriwal"], "title": "Code-Driven Planning in Grid Worlds with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "We propose an iterative programmatic planning (IPP) framework for solving\ngrid-based tasks by synthesizing interpretable agent policies expressed in code\nusing large language models (LLMs). Instead of relying on traditional search or\nreinforcement learning, our approach uses code generation as policy synthesis,\nwhere the LLM outputs executable programs that map environment states to action\nsequences. Our proposed architecture incorporates several prompting strategies,\nincluding direct code generation, pseudocode-conditioned refinement, and\ncurriculum-based prompting, but also includes an iterative refinement mechanism\nthat updates code based on task performance feedback. We evaluate our approach\nusing six leading LLMs and two challenging grid-based benchmarks (GRASP and\nMiniGrid). Our IPP framework demonstrates improvements over direct code\ngeneration ranging from 10\\% to as much as 10x across five of the six models\nand establishes a new state-of-the-art result for GRASP. IPP is found to\nsignificantly outperform direct elicitation of a solution from GPT-o3-mini (by\n63\\% on MiniGrid to 116\\% on GRASP), demonstrating the viability of the overall\napproach. Computational costs of all code generation approaches are similar.\nWhile code generation has a higher initial prompting cost compared to direct\nsolution elicitation (\\$0.08 per task vs. \\$0.002 per instance for\nGPT-o3-mini), the code can be reused for any number of instances, making the\namortized cost significantly lower (by 400x on GPT-o3-mini across the complete\nGRASP benchmark).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fed\u4ee3\u7a0b\u5e8f\u5316\u89c4\u5212\uff08IPP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u6765\u5408\u6210\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u683c\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u4ee3\u7801\u751f\u6210\uff0c\u5e76\u5728GRASP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u3002", "motivation": "\u4f20\u7edf\u641c\u7d22\u6216\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7f51\u683c\u4efb\u52a1\u4e2d\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\u6216\u96be\u4ee5\u89e3\u91ca\uff0c\u800cLLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e3a\u53ef\u89e3\u91ca\u7b56\u7565\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u91c7\u7528\u4ee3\u7801\u751f\u6210\u4f5c\u4e3a\u7b56\u7565\u5408\u6210\uff0c\u7ed3\u5408\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u3001\u4f2a\u4ee3\u7801\u6761\u4ef6\u7ec6\u5316\u3001\u8bfe\u7a0b\u63d0\u793a\u7b49\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u673a\u5236\u4f18\u5316\u4ee3\u7801\u3002", "result": "\u5728\u516d\u4e2a\u9886\u5148LLM\u548c\u4e24\u4e2a\u7f51\u683c\u57fa\u51c6\u6d4b\u8bd5\uff08GRASP\u548cMiniGrid\uff09\u4e0a\uff0cIPP\u6846\u67b6\u6027\u80fd\u63d0\u534710%\u81f310\u500d\uff0c\u5e76\u5728GRASP\u4e0a\u8fbe\u5230\u65b0\u6700\u4f18\u3002", "conclusion": "IPP\u6846\u67b6\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u683c\u4efb\u52a1\u7684\u89e3\u51b3\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u644a\u9500\u6210\u672c\u3002", "relevance": 75.0}}
{"id": "2505.10607", "pdf": "https://arxiv.org/pdf/2505.10607", "abs": "https://arxiv.org/abs/2505.10607", "authors": ["Patara Trirat", "Jae-Gil Lee"], "title": "MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices", "categories": ["cs.LG", "cs.AI"], "comment": "Code will be available at https://github.com/kaist-dmlab/MONAQ", "summary": "The growing use of smartphones and IoT devices necessitates efficient\ntime-series analysis on resource-constrained hardware, which is critical for\nsensing applications such as human activity recognition and air quality\nprediction. Recent efforts in hardware-aware neural architecture search (NAS)\nautomate architecture discovery for specific platforms; however, none focus on\ngeneral time-series analysis with edge deployment. Leveraging the\nproblem-solving and reasoning capabilities of large language models (LLM), we\npropose MONAQ, a novel framework that reformulates NAS into Multi-Objective\nNeural Architecture Querying tasks. MONAQ is equipped with multimodal query\ngeneration for processing multimodal time-series inputs and hardware\nconstraints, alongside an LLM agent-based multi-objective search to achieve\ndeployment-ready models via code generation. By integrating numerical data,\ntime-series images, and textual descriptions, MONAQ improves an LLM's\nunderstanding of time-series data. Experiments on fifteen datasets demonstrate\nthat MONAQ-discovered models outperform both handcrafted models and NAS\nbaselines while being more efficient.", "AI": {"tldr": "MONAQ\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u67e5\u8be2\u6846\u67b6\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u666e\u53ca\u9700\u8981\u9ad8\u6548\u7684\u8fb9\u7f18\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u4f46\u73b0\u6709\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u65b9\u6cd5\u672a\u9488\u5bf9\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4f18\u5316\u3002", "method": "MONAQ\u5c06NAS\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u67e5\u8be2\u4efb\u52a1\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u67e5\u8be2\u751f\u6210\u548c\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u591a\u76ee\u6807\u641c\u7d22\uff0c\u751f\u6210\u90e8\u7f72\u5c31\u7eea\u7684\u6a21\u578b\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMONAQ\u53d1\u73b0\u7684\u6a21\u578b\u4f18\u4e8e\u624b\u5de5\u6a21\u578b\u548cNAS\u57fa\u7ebf\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "MONAQ\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u548cLLM\u80fd\u529b\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.10585", "pdf": "https://arxiv.org/pdf/2505.10585", "abs": "https://arxiv.org/abs/2505.10585", "authors": ["Azim Akhtarshenas", "Ramin Toosi", "David L\u00f3pez-P\u00e9rez", "Tohid Alizadeh", "Alireza Hosseini"], "title": "Efficient Malicious UAV Detection Using Autoencoder-TSMamba Integration", "categories": ["cs.CV", "cs.CR"], "comment": "12 pages, 6 figures and 3 tables, accepted in IbPRIA 2025,\n  https://www.ibpria.org/2025/?page=dates", "summary": "Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to\nnext-generation networks (NGNs), posing risks such as unauthorized\nsurveillance, data theft, and the delivery of hazardous materials. This paper\nproposes an integrated (AE)-classifier system to detect malicious UAVs. The\nproposed AE, based on a 4-layer Tri-orientated Spatial Mamba (TSMamba)\narchitecture, effectively captures complex spatial relationships crucial for\nidentifying malicious UAV activities. The first phase involves generating\nresidual values through the AE, which are subsequently processed by a\nResNet-based classifier. This classifier leverages the residual values to\nachieve lower complexity and higher accuracy. Our experiments demonstrate\nsignificant improvements in both binary and multi-class classification\nscenarios, achieving up to 99.8 % recall compared to 96.7 % in the benchmark.\nAdditionally, our method reduces computational complexity, making it more\nsuitable for large-scale deployment. These results highlight the robustness and\nscalability of our approach, offering an effective solution for malicious UAV\ndetection in NGN environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e4\u5c42Tri-orientated Spatial Mamba (TSMamba)\u67b6\u6784\u7684\u81ea\u52a8\u7f16\u7801\u5668(AE)\u5206\u7c7b\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u6076\u610f\u65e0\u4eba\u673a(UAVs)\uff0c\u5728\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "motivation": "\u6076\u610f\u65e0\u4eba\u673a\u5bf9\u4e0b\u4e00\u4ee3\u7f51\u7edc(NGNs)\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5982\u672a\u7ecf\u6388\u6743\u7684\u76d1\u89c6\u3001\u6570\u636e\u7a83\u53d6\u548c\u5371\u9669\u7269\u54c1\u6295\u9012\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u75284\u5c42TSMamba\u67b6\u6784\u7684AE\u751f\u6210\u6b8b\u5dee\u503c\uff0c\u518d\u901a\u8fc7ResNet\u5206\u7c7b\u5668\u5904\u7406\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u53ec\u56de\u7387\u9ad8\u8fbe99.8%\uff0c\u4f18\u4e8e\u57fa\u51c6\u768496.7%\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6076\u610f\u65e0\u4eba\u673a\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u5408\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "relevance": 30.0}}
{"id": "2505.10775", "pdf": "https://arxiv.org/pdf/2505.10775", "abs": "https://arxiv.org/abs/2505.10775", "authors": ["Kian Ahrabian", "Pegah Jandaghi", "Negar Mokhberian", "Sai Praneeth Karimireddy", "Jay Pujara"], "title": "A Systematic Analysis of Base Model Choice for Reward Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 13 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward\nmodeling have become a crucial part of training powerful large language models\n(LLMs). One commonly overlooked factor in training high-quality reward models\n(RMs) is the effect of the base model, which is becoming more challenging to\nchoose given the rapidly growing pool of LLMs. In this work, we present a\nsystematic analysis of the effect of base model selection on reward modeling\nperformance. Our results show that the performance can be improved by up to 14%\ncompared to the most common (i.e., default) choice. Moreover, we showcase the\nstrong statistical relation between some existing benchmarks and downstream\nperformances. We also demonstrate that the results from a small set of\nbenchmarks could be combined to boost the model selection ($+$18% on average in\nthe top 5-10). Lastly, we illustrate the impact of different post-training\nsteps on the final performance and explore using estimated data distributions\nto reduce performance prediction error.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u7840\u6a21\u578b\u9009\u62e9\u5bf9\u5956\u52b1\u5efa\u6a21\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6027\u80fd\u53ef\u63d0\u534714%\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u4e0e\u4e0b\u6e38\u6027\u80fd\u7684\u5f3a\u7edf\u8ba1\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u9009\u62e9\u5bf9\u5956\u52b1\u5efa\u6a21\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u5bf9\u5956\u52b1\u5efa\u6a21\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u5c0f\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u63d0\u5347\u6a21\u578b\u9009\u62e9\u6548\u679c\u3002", "result": "\u6027\u80fd\u63d0\u534714%\uff0c\u7ed3\u5408\u57fa\u51c6\u6570\u636e\u53ef\u5e73\u5747\u63d0\u534718%\u7684\u6a21\u578b\u9009\u62e9\u6548\u679c\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u9009\u62e9\u5bf9\u5956\u52b1\u5efa\u6a21\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7ed3\u5408\u57fa\u51c6\u6570\u636e\u53ef\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "relevance": 90.0}}
{"id": "2505.10779", "pdf": "https://arxiv.org/pdf/2505.10779", "abs": "https://arxiv.org/abs/2505.10779", "authors": ["Philip S. Thomas"], "title": "Qualia Optimization", "categories": ["cs.AI"], "comment": "Technical Report, College of Information and Computer Science,\n  University of Massachusetts", "summary": "This report explores the speculative question: what if current or future AI\nsystems have qualia, such as pain or pleasure? It does so by assuming that AI\nsystems might someday possess qualia -- and that the quality of these\nsubjective experiences should be considered alongside performance metrics.\nConcrete mathematical problem settings, inspired by reinforcement learning\nformulations and theories from philosophy of mind, are then proposed and\ninitial approaches and properties are presented. These properties enable\nrefinement of the problem setting, culminating with the proposal of methods\nthat promote reinforcement.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u672a\u6765AI\u7cfb\u7edf\u662f\u5426\u53ef\u80fd\u5177\u6709\u4e3b\u89c2\u4f53\u9a8c\uff08\u5982\u75db\u82e6\u6216\u5feb\u4e50\uff09\uff0c\u5e76\u63d0\u51fa\u5728\u8bc4\u4f30AI\u65f6\u5e94\u8003\u8651\u8fd9\u4e9b\u4e3b\u89c2\u4f53\u9a8c\u3002\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u54f2\u5b66\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u6570\u5b66\u95ee\u9898\u8bbe\u5b9a\u548c\u521d\u6b65\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8AI\u7cfb\u7edf\u53ef\u80fd\u5177\u6709\u4e3b\u89c2\u4f53\u9a8c\u7684\u5047\u8bbe\uff0c\u5e76\u5f3a\u8c03\u5728AI\u53d1\u5c55\u4e2d\u5e94\u8003\u8651\u8fd9\u4e9b\u4f53\u9a8c\u7684\u8d28\u91cf\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u54f2\u5b66\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u6570\u5b66\u95ee\u9898\u8bbe\u5b9a\u548c\u521d\u6b65\u65b9\u6cd5\uff0c\u65e8\u5728\u4fc3\u8fdbAI\u7cfb\u7edf\u7684\u5f3a\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u80fd\u591f\u4fc3\u8fdbAI\u7cfb\u7edf\u5f3a\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6027\u8d28\u3002", "conclusion": "\u7ed3\u8bba\u662f\u672a\u6765AI\u7cfb\u7edf\u7684\u8bbe\u8ba1\u5e94\u8003\u8651\u4e3b\u89c2\u4f53\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u54f2\u5b66\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\u3002", "relevance": 40.0}}
{"id": "2505.10630", "pdf": "https://arxiv.org/pdf/2505.10630", "abs": "https://arxiv.org/abs/2505.10630", "authors": ["Ben Adcock", "Nick Huang"], "title": "How many measurements are enough? Bayesian recovery in inverse problems with general distributions", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We study the sample complexity of Bayesian recovery for solving inverse\nproblems with general prior, forward operator and noise distributions. We\nconsider posterior sampling according to an approximate prior $\\mathcal{P}$,\nand establish sufficient conditions for stable and accurate recovery with high\nprobability. Our main result is a non-asymptotic bound that shows that the\nsample complexity depends on (i) the intrinsic complexity of $\\mathcal{P}$,\nquantified by its so-called approximate covering number, and (ii) concentration\nbounds for the forward operator and noise distributions. As a key application,\nwe specialize to generative priors, where $\\mathcal{P}$ is the pushforward of a\nlatent distribution via a Deep Neural Network (DNN). We show that the sample\ncomplexity scales log-linearly with the latent dimension $k$, thus establishing\nthe efficacy of DNN-based priors. Generalizing existing results on\ndeterministic (i.e., non-Bayesian) recovery for the important problem of random\nsampling with an orthogonal matrix $U$, we show how the sample complexity is\ndetermined by the coherence of $U$ with respect to the support of\n$\\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in\nBayesian recovery as well. Overall, our framework unifies and extends prior\nwork, providing rigorous guarantees for the sample complexity of solving\nBayesian inverse problems with arbitrary distributions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8d1d\u53f6\u65af\u6062\u590d\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u91cd\u70b9\u5173\u6ce8\u5148\u9a8c\u3001\u524d\u5411\u7b97\u5b50\u548c\u566a\u58f0\u5206\u5e03\u7684\u4e00\u822c\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u5148\u9a8c\u7684\u5185\u5728\u590d\u6742\u6027\u548c\u524d\u5411\u7b97\u5b50\u53ca\u566a\u58f0\u7684\u96c6\u4e2d\u6027\u6709\u5173\uff0c\u7279\u522b\u5728\u751f\u6210\u5148\u9a8c\uff08DNN\uff09\u4e0b\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u5448\u5bf9\u6570\u7ebf\u6027\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u8d1d\u53f6\u65af\u6062\u590d\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e3a\u9006\u95ee\u9898\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u5148\u9a8c\uff08DNN\uff09\u4e0b\u7684\u9ad8\u6548\u6027\u3002", "method": "\u901a\u8fc7\u975e\u6e10\u8fd1\u8fb9\u754c\u5206\u6790\u6837\u672c\u590d\u6742\u5ea6\uff0c\u91cf\u5316\u5148\u9a8c\u7684\u8fd1\u4f3c\u8986\u76d6\u6570\uff0c\u5e76\u5229\u7528\u524d\u5411\u7b97\u5b50\u548c\u566a\u58f0\u7684\u96c6\u4e2d\u6027\u3002", "result": "\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u5148\u9a8c\u590d\u6742\u6027\u548c\u524d\u5411\u7b97\u5b50/\u566a\u58f0\u7684\u96c6\u4e2d\u6027\u76f8\u5173\uff0c\u751f\u6210\u5148\u9a8c\u4e0b\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u5448\u5bf9\u6570\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "\u8d1d\u53f6\u65af\u6062\u590d\u7684\u6837\u672c\u590d\u6742\u5ea6\u53d7\u5148\u9a8c\u590d\u6742\u6027\u548c\u7b97\u5b50/\u566a\u58f0\u6027\u8d28\u5f71\u54cd\uff0c\u751f\u6210\u5148\u9a8c\uff08DNN\uff09\u5177\u6709\u9ad8\u6548\u6027\u3002", "relevance": 50.0}}
{"id": "2505.10589", "pdf": "https://arxiv.org/pdf/2505.10589", "abs": "https://arxiv.org/abs/2505.10589", "authors": ["Ka\u011fan \u00c7ET\u0130N"], "title": "Super-Resolution Generative Adversarial Networks based Video Enhancement", "categories": ["cs.CV", "cs.AI", "eess.IV", "I.4.3"], "comment": null, "summary": "This study introduces an enhanced approach to video super-resolution by\nextending ordinary Single-Image Super-Resolution (SISR) Super-Resolution\nGenerative Adversarial Network (SRGAN) structure to handle spatio-temporal\ndata. While SRGAN has proven effective for single-image enhancement, its design\ndoes not account for the temporal continuity required in video processing. To\naddress this, a modified framework that incorporates 3D Non-Local Blocks is\nproposed, which is enabling the model to capture relationships across both\nspatial and temporal dimensions. An experimental training pipeline is\ndeveloped, based on patch-wise learning and advanced data degradation\ntechniques, to simulate real-world video conditions and learn from both local\nand global structures and details. This helps the model generalize better and\nmaintain stability across varying video content while maintaining the general\nstructure besides the pixel-wise correctness. Two model variants-one larger and\none more lightweight-are presented to explore the trade-offs between\nperformance and efficiency. The results demonstrate improved temporal\ncoherence, sharper textures, and fewer visual artifacts compared to traditional\nsingle-image methods. This work contributes to the development of practical,\nlearning-based solutions for video enhancement tasks, with potential\napplications in streaming, gaming, and digital restoration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6269\u5c55SRGAN\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u7ed3\u54083D\u975e\u5c40\u90e8\u5757\u4ee5\u6355\u6349\u65f6\u7a7a\u5173\u7cfb\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u5355\u56fe\u50cf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08\u5982SRGAN\uff09\u672a\u8003\u8651\u89c6\u9891\u5904\u7406\u4e2d\u7684\u65f6\u95f4\u8fde\u7eed\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u65f6\u7a7a\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u7ed3\u54083D\u975e\u5c40\u90e8\u5757\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u5206\u5757\u5b66\u4e60\u548c\u9ad8\u7ea7\u6570\u636e\u9000\u5316\u6280\u672f\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7eb9\u7406\u6e05\u6670\u5ea6\u548c\u51cf\u5c11\u89c6\u89c9\u4f2a\u5f71\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5355\u56fe\u50cf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6d41\u5a92\u4f53\u3001\u6e38\u620f\u548c\u6570\u5b57\u4fee\u590d\u7b49\u9886\u57df\u3002", "relevance": 30.0}}
{"id": "2505.10792", "pdf": "https://arxiv.org/pdf/2505.10792", "abs": "https://arxiv.org/abs/2505.10792", "authors": ["Zhan Peng Lee", "Andre Lin", "Calvin Tan"], "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\nimprove factuality in large language models (LLMs) by grounding their outputs\nin retrieved documents. However, ensuring perfect retrieval of relevant\ninformation remains challenging, and when irrelevant content is passed\ndownstream to an LLM, it can lead to hallucinations. In this work, we propose\nFinetune-RAG, a simple and effective fine-tuning approach that features the\nfirst-of-its-kind RAG training dataset constructed to mimic real-world\nimperfections. Experimental results show that Finetune-RAG improves factual\naccuracy by 21.2% over the base model. We also propose a Bench-RAG, an\nLLM-as-a-judge evaluation pipeline that stress tests models under realistic\nimperfect retrieval scenarios. Our codebase and dataset are fully open sourced\nfor community use.", "AI": {"tldr": "Finetune-RAG\u901a\u8fc7\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347LLM\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86Bench-RAG\u8bc4\u4f30\u6846\u67b6\u4ee5\u6d4b\u8bd5\u6a21\u578b\u5728\u73b0\u5b9e\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3RAG\u6846\u67b6\u4e2d\u56e0\u68c0\u7d22\u4e0d\u5b8c\u7f8e\u5bfc\u81f4LLM\u8f93\u51fa\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFinetune-RAG\u5fae\u8c03\u65b9\u6cd5\uff0c\u6784\u5efa\u6a21\u62df\u73b0\u5b9e\u7f3a\u9677\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff1b\u8bbe\u8ba1Bench-RAG\u8bc4\u4f30\u6846\u67b6\u3002", "result": "Finetune-RAG\u5c06\u4e8b\u5b9e\u51c6\u786e\u6027\u63d0\u534721.2%\u3002", "conclusion": "Finetune-RAG\u548cBench-RAG\u4e3a\u63d0\u5347LLM\u4e8b\u5b9e\u6027\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.10780", "pdf": "https://arxiv.org/pdf/2505.10780", "abs": "https://arxiv.org/abs/2505.10780", "authors": ["Trisha Das", "Afrah Shafquat", "Beigi Mandis", "Jacob Aptekar", "Jimeng Sun"], "title": "SECRET: Semi-supervised Clinical Trial Document Similarity Search", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Clinical trials are vital for evaluation of safety and efficacy of new\ntreatments. However, clinical trials are resource-intensive, time-consuming and\nexpensive to conduct, where errors in trial design, reduced efficacy, and\nsafety events can result in significant delays, financial losses, and damage to\nreputation. These risks underline the importance of informed and strategic\ndecisions in trial design to mitigate these risks and improve the chances of a\nsuccessful trial. Identifying similar historical trials is critical as these\ntrials can provide an important reference for potential pitfalls and challenges\nincluding serious adverse events, dosage inaccuracies, recruitment\ndifficulties, patient adherence issues, etc. Addressing these challenges in\ntrial design can lead to development of more effective study protocols with\noptimized patient safety and trial efficiency. In this paper, we present a\nnovel method to identify similar historical trials by summarizing clinical\ntrial protocols and searching for similar trials based on a query trial's\nprotocol. Our approach significantly outperforms all baselines, achieving up to\na 78% improvement in recall@1 and a 53% improvement in precision@1 over the\nbest baseline. We also show that our method outperforms all other baselines in\npartial trial similarity search and zero-shot patient-trial matching,\nhighlighting its superior utility in these tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u603b\u7ed3\u4e34\u5e8a\u8bd5\u9a8c\u534f\u8bae\u5e76\u57fa\u4e8e\u67e5\u8be2\u8bd5\u9a8c\u534f\u8bae\u641c\u7d22\u76f8\u4f3c\u7684\u5386\u53f2\u8bd5\u9a8c\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u6e90\u5bc6\u96c6\u4e14\u98ce\u9669\u9ad8\uff0c\u5386\u53f2\u8bd5\u9a8c\u7684\u76f8\u4f3c\u6027\u8bc6\u522b\u5bf9\u4f18\u5316\u8bd5\u9a8c\u8bbe\u8ba1\u548c\u63d0\u9ad8\u6210\u529f\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u603b\u7ed3\u4e34\u5e8a\u8bd5\u9a8c\u534f\u8bae\u5e76\u57fa\u4e8e\u67e5\u8be2\u8bd5\u9a8c\u534f\u8bae\u641c\u7d22\u76f8\u4f3c\u7684\u5386\u53f2\u8bd5\u9a8c\u3002", "result": "\u65b9\u6cd5\u5728recall@1\u548cprecision@1\u4e0a\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e8678%\u548c53%\uff0c\u5e76\u5728\u90e8\u5206\u8bd5\u9a8c\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u96f6\u6837\u672c\u60a3\u8005-\u8bd5\u9a8c\u5339\u914d\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u8bd5\u9a8c\u76f8\u4f3c\u6027\u641c\u7d22\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u6709\u6548\u4f18\u5316\u8bd5\u9a8c\u8bbe\u8ba1\u548c\u63d0\u9ad8\u6548\u7387\u3002", "relevance": 30.0}}
{"id": "2505.10641", "pdf": "https://arxiv.org/pdf/2505.10641", "abs": "https://arxiv.org/abs/2505.10641", "authors": ["Linjing You", "Jiabao Lu", "Xiayuan Huang", "Xiangli Nie"], "title": "FRET: Feature Redundancy Elimination for Test Time Adaptation", "categories": ["cs.LG"], "comment": null, "summary": "Test-Time Adaptation (TTA) aims to enhance the generalization of deep\nlearning models when faced with test data that exhibits distribution shifts\nfrom the training data. In this context, only a pre-trained model and unlabeled\ntest data are available, making it particularly relevant for privacy-sensitive\napplications. In practice, we observe that feature redundancy in embeddings\ntends to increase as domain shifts intensify in TTA. However, existing TTA\nmethods often overlook this redundancy, which can hinder the model's\nadaptability to new data. To address this issue, we introduce Feature\nRedundancy Elimination for Test-time Adaptation (FRET), a novel perspective for\nTTA. A straightforward approach (S-FRET) is to directly minimize the feature\nredundancy score as an optimization objective to improve adaptation. Despite\nits simplicity and effectiveness, S-FRET struggles with label shifts, limiting\nits robustness in real-world scenarios. To mitigate this limitation, we further\npropose Graph-based FRET (G-FRET), which integrates a Graph Convolutional\nNetwork (GCN) with contrastive learning. This design not only reduces feature\nredundancy but also enhances feature discriminability in both the\nrepresentation and prediction layers. Extensive experiments across multiple\nmodel architectures, tasks, and datasets demonstrate the effectiveness of\nS-FRET and show that G-FRET achieves state-of-the-art performance. Further\nanalysis reveals that G-FRET enables the model to extract non-redundant and\nhighly discriminative features during inference, thereby facilitating more\nrobust test-time adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFRET\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u7279\u5f81\u5197\u4f59\u63d0\u5347\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u6027\u80fd\uff0c\u5305\u62ec\u7b80\u5355\u7248\u672cS-FRET\u548c\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\u7684G-FRET\uff0c\u540e\u8005\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u89e3\u51b3\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u504f\u79fb\u65f6\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u573a\u666f\u3002", "method": "\u63d0\u51faS-FRET\u76f4\u63a5\u4f18\u5316\u7279\u5f81\u5197\u4f59\u5206\u6570\uff0cG-FRET\u7ed3\u5408GCN\u548c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u533a\u5206\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eG-FRET\u5728\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u63d0\u53d6\u975e\u5197\u4f59\u4e14\u9ad8\u533a\u5206\u6027\u7279\u5f81\u3002", "conclusion": "G-FRET\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u548c\u589e\u5f3a\u533a\u5206\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002", "relevance": 40.0}}
{"id": "2505.10595", "pdf": "https://arxiv.org/pdf/2505.10595", "abs": "https://arxiv.org/abs/2505.10595", "authors": ["Xingye Cui", "Junhai Luo", "Jiakun Deng", "Kexuan Li", "Xiangyu Qiu", "Zhenming Peng"], "title": "ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) is critical in both civilian and\nmilitary applications. However, the limited texture and structural information\nin infrared images makes accurate detection particularly challenging. Although\nrecent deep learning-based methods have improved performance, their use of\nconventional convolution kernels limits adaptability to complex scenes and\ndiverse targets. Moreover, pooling operations often cause feature loss and\ninsufficient exploitation of image information. To address these issues, we\npropose an adaptive receptive field convolution and wavelet-attentive\nhierarchical network for infrared small target detection (ARFC-WAHNet). This\nnetwork incorporates a multi-receptive field feature interaction convolution\n(MRFFIConv) module to adaptively extract discriminative features by integrating\nmultiple convolutional branches with a gated unit. A wavelet frequency\nenhancement downsampling (WFED) module leverages Haar wavelet transform and\nfrequency-domain reconstruction to enhance target features and suppress\nbackground noise. Additionally, we introduce a high-low feature fusion (HLFF)\nmodule for integrating low-level details with high-level semantics, and a\nglobal median enhancement attention (GMEA) module to improve feature diversity\nand expressiveness via global attention. Experiments on public datasets SIRST,\nNUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent\nstate-of-the-art methods in both detection accuracy and robustness,\nparticularly under complex backgrounds. The code is available at\nhttps://github.com/Leaf2001/ARFC-WAHNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARFC-WAHNet\u7684\u7f51\u7edc\uff0c\u7528\u4e8e\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u53d7\u91ce\u5377\u79ef\u548c\u5c0f\u6ce2\u6ce8\u610f\u529b\u5206\u5c42\u7f51\u7edc\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ea2\u5916\u56fe\u50cf\u4e2d\u76ee\u6807\u7eb9\u7406\u548c\u7ed3\u6784\u4fe1\u606f\u6709\u9650\uff0c\u4f20\u7edf\u5377\u79ef\u6838\u548c\u6c60\u5316\u64cd\u4f5c\u5bfc\u81f4\u7279\u5f81\u4e22\u5931\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u591a\u611f\u53d7\u91ce\u7279\u5f81\u4ea4\u4e92\u5377\u79ef\u6a21\u5757\uff08MRFFIConv\uff09\u3001\u5c0f\u6ce2\u9891\u7387\u589e\u5f3a\u4e0b\u91c7\u6837\u6a21\u5757\uff08WFED\uff09\u3001\u9ad8\u4f4e\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08HLFF\uff09\u548c\u5168\u5c40\u4e2d\u503c\u589e\u5f3a\u6ce8\u610f\u529b\u6a21\u5757\uff08GMEA\uff09\u3002", "result": "\u5728SIRST\u3001NUDT-SIRST\u548cIRSTD-1k\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u80cc\u666f\u4e0b\u3002", "conclusion": "ARFC-WAHNet\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10798", "pdf": "https://arxiv.org/pdf/2505.10798", "abs": "https://arxiv.org/abs/2505.10798", "authors": ["Erica Cai", "Sean McQuade", "Kevin Young", "Brendan O'Connor"], "title": "Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets", "categories": ["cs.CL"], "comment": null, "summary": "When knowledge graphs (KGs) are automatically extracted from text, are they\naccurate enough for downstream analysis? Unfortunately, current annotated\ndatasets can not be used to evaluate this question, since their KGs are highly\ndisconnected, too small, or overly complex. To address this gap, we introduce\nAffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six\ndatasets that are the first to pair complete book scans with large, labeled\nknowledge graphs. Each dataset features affiliation graphs, which are simple\nKGs that capture Member relationships between Person and Organization entities\n-- useful in studies of migration, community interactions, and other social\nphenomena. In addition, three datasets include expanded KGs with a wider\nvariety of relation types. Our preliminary experiments demonstrate significant\nvariability in model performance across datasets, underscoring AffilKG's\nability to enable two critical advances: (1) benchmarking how extraction errors\npropagate to graph-level analyses (e.g., community structure), and (2)\nvalidating KG extraction methods for real-world social science research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AffilKG\uff0c\u4e00\u4e2a\u5305\u542b\u516d\u4e2a\u6570\u636e\u96c6\u7684\u96c6\u5408\uff0c\u7528\u4e8e\u8bc4\u4f30\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u4ece\u6587\u672c\u4e2d\u81ea\u52a8\u63d0\u53d6\u7684\u51c6\u786e\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u89c4\u6a21\u548c\u590d\u6742\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7684\u77e5\u8bc6\u56fe\u8c31\u6807\u6ce8\u6570\u636e\u96c6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u81ea\u52a8\u63d0\u53d6\u7684\u51c6\u786e\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u8fc7\u4e8e\u5206\u6563\u3001\u89c4\u6a21\u592a\u5c0f\uff0c\u8981\u4e48\u8fc7\u4e8e\u590d\u6742\u3002AffilKG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AffilKG\u63d0\u4f9b\u4e86\u516d\u4e2a\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u5305\u542b\u5b8c\u6574\u7684\u4e66\u7c4d\u626b\u63cf\u548c\u6807\u6ce8\u7684\u5927\u578b\u77e5\u8bc6\u56fe\u8c31\uff0c\u5176\u4e2d\u4e09\u4e2a\u6570\u636e\u96c6\u8fd8\u6269\u5c55\u4e86\u66f4\u591a\u5173\u7cfb\u7c7b\u578b\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eAffilKG\u53ef\u7528\u4e8e\u8bc4\u4f30\u63d0\u53d6\u9519\u8bef\u5bf9\u56fe\u7ea7\u5206\u6790\u7684\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1KG\u63d0\u53d6\u65b9\u6cd5\u3002", "conclusion": "AffilKG\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.10803", "pdf": "https://arxiv.org/pdf/2505.10803", "abs": "https://arxiv.org/abs/2505.10803", "authors": ["Zhaoan Wang", "Wonseok Jang", "Bowen Ruan", "Jun Wang", "Shaoping Xiao"], "title": "Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management", "categories": ["cs.AI"], "comment": null, "summary": "Precision agriculture, enhanced by artificial intelligence (AI), offers\npromising tools such as remote sensing, intelligent irrigation, fertilization\nmanagement, and crop simulation to improve agricultural efficiency and\nsustainability. Reinforcement learning (RL), in particular, has outperformed\ntraditional methods in optimizing yields and resource management. However,\nwidespread AI adoption is limited by gaps between algorithmic recommendations\nand farmers' practical experience, local knowledge, and traditional practices.\nTo address this, our study emphasizes Human-AI Interaction (HAII), focusing on\ntransparency, usability, and trust in RL-based farm management. We employ a\nwell-established trust framework - comprising ability, benevolence, and\nintegrity - to develop a novel mathematical model quantifying farmers'\nconfidence in AI-based fertilization strategies. Surveys conducted with farmers\nfor this research reveal critical misalignments, which are integrated into our\ntrust model and incorporated into a multi-objective RL framework. Unlike prior\nmethods, our approach embeds trust directly into policy optimization, ensuring\nAI recommendations are technically robust, economically feasible,\ncontext-aware, and socially acceptable. By aligning technical performance with\nhuman-centered trust, this research supports broader AI adoption in\nagriculture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b-AI\u4ea4\u4e92\uff08HAII\uff09\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u519c\u6c11\u5bf9AI\u65bd\u80a5\u7b56\u7565\u7684\u4fe1\u4efb\u5ea6\uff0c\u4f18\u5316\u519c\u4e1a\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3AI\u5728\u519c\u4e1a\u4e2d\u5e94\u7528\u65f6\uff0c\u7b97\u6cd5\u63a8\u8350\u4e0e\u519c\u6c11\u5b9e\u9645\u7ecf\u9a8c\u3001\u672c\u5730\u77e5\u8bc6\u548c\u4f20\u7edf\u5b9e\u8df5\u4e4b\u95f4\u7684\u8131\u8282\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4fe1\u4efb\u6846\u67b6\uff08\u80fd\u529b\u3001\u5584\u610f\u3001\u8bda\u4fe1\uff09\u5f00\u53d1\u6570\u5b66\u6a21\u578b\uff0c\u7ed3\u5408\u519c\u6c11\u8c03\u67e5\u6570\u636e\uff0c\u5c06\u4fe1\u4efb\u5d4c\u5165\u591a\u76ee\u6807RL\u7b56\u7565\u4f18\u5316\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u519c\u6c11\u4e0eAI\u4e4b\u95f4\u7684\u5173\u952e\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u4fe1\u4efb\u6a21\u578b\u4f18\u5316RL\u7b56\u7565\uff0c\u4f7fAI\u63a8\u8350\u66f4\u5177\u6280\u672f\u7a33\u5065\u6027\u3001\u7ecf\u6d4e\u53ef\u884c\u6027\u548c\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6280\u672f\u6027\u80fd\u4e0e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4fe1\u4efb\uff0c\u7814\u7a76\u4fc3\u8fdb\u4e86AI\u5728\u519c\u4e1a\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002", "relevance": 40.0}}
{"id": "2505.10646", "pdf": "https://arxiv.org/pdf/2505.10646", "abs": "https://arxiv.org/abs/2505.10646", "authors": ["Haoxiang You", "Yilang Liu", "Ian Abraham"], "title": "Accelerating Visual-Policy Learning through Parallel Differentiable Simulation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "In this work, we propose a computationally efficient algorithm for visual\npolicy learning that leverages differentiable simulation and first-order\nanalytical policy gradients. Our approach decouple the rendering process from\nthe computation graph, enabling seamless integration with existing\ndifferentiable simulation ecosystems without the need for specialized\ndifferentiable rendering software. This decoupling not only reduces\ncomputational and memory overhead but also effectively attenuates the policy\ngradient norm, leading to more stable and smoother optimization. We evaluate\nour method on standard visual control benchmarks using modern GPU-accelerated\nsimulation. Experiments show that our approach significantly reduces wall-clock\ntraining time and consistently outperforms all baseline methods in terms of\nfinal returns. Notably, on complex tasks such as humanoid locomotion, our\nmethod achieves a $4\\times$ improvement in final return, and successfully\nlearns a humanoid running policy within 4 hours on a single GPU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u89c6\u89c9\u7b56\u7565\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u53ef\u5fae\u5206\u6a21\u62df\u548c\u4e00\u9636\u89e3\u6790\u7b56\u7565\u68af\u5ea6\uff0c\u901a\u8fc7\u89e3\u8026\u6e32\u67d3\u8fc7\u7a0b\u4e0e\u8ba1\u7b97\u56fe\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u7a33\u5b9a\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u7b56\u7565\u5b66\u4e60\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u9ad8\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u4e13\u7528\u53ef\u5fae\u5206\u6e32\u67d3\u8f6f\u4ef6\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u53ef\u5fae\u5206\u6a21\u62df\u548c\u4e00\u9636\u89e3\u6790\u7b56\u7565\u68af\u5ea6\uff0c\u89e3\u8026\u6e32\u67d3\u8fc7\u7a0b\u4e0e\u8ba1\u7b97\u56fe\uff0c\u4e0e\u73b0\u6709\u53ef\u5fae\u5206\u6a21\u62df\u751f\u6001\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u6807\u51c6\u89c6\u89c9\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u6700\u7ec8\u56de\u62a5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u590d\u6742\u4efb\u52a1\uff08\u5982\u4eba\u5f62\u8fd0\u52a8\uff09\u4e0a\u6700\u7ec8\u56de\u62a5\u63d0\u53474\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u590d\u6742\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u3002", "relevance": 40.0}}
{"id": "2505.10601", "pdf": "https://arxiv.org/pdf/2505.10601", "abs": "https://arxiv.org/abs/2505.10601", "authors": ["Chuang Chen", "Wenyi Ge"], "title": "SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In recent years, range-view-based LiDAR point cloud super-resolution\ntechniques attract significant attention as a low-cost method for generating\nhigher-resolution point cloud data. However, due to the sparsity and irregular\nstructure of LiDAR point clouds, the point cloud super-resolution problem\nremains a challenging topic, especially for point cloud upsampling under novel\nviews. In this paper, we propose SRMamba, a novel method for super-resolution\nof LiDAR point clouds in sparse scenes, addressing the key challenge of\nrecovering the 3D spatial structure of point clouds from novel views.\nSpecifically, we implement projection technique based on Hough Voting and Hole\nCompensation strategy to eliminate horizontally linear holes in range image. To\nimprove the establishment of long-distance dependencies and to focus on\npotential geometric features in vertical 3D space, we employ Visual State Space\nmodel and Multi-Directional Scanning mechanism to mitigate the loss of 3D\nspatial structural information due to the range image. Additionally, an\nasymmetric U-Net network adapts to the input characteristics of LiDARs with\ndifferent beam counts, enabling super-resolution reconstruction for multi-beam\npoint clouds. We conduct a series of experiments on multiple challenging public\nLiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates\nsignificant superiority over other algorithms in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRMamba\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758f\u573a\u666f\u4e2dLiDAR\u70b9\u4e91\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4ece\u65b0\u89c6\u89d2\u6062\u590d\u70b9\u4e913D\u7a7a\u95f4\u7ed3\u6784\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u7531\u4e8eLiDAR\u70b9\u4e91\u7684\u7a00\u758f\u6027\u548c\u4e0d\u89c4\u5219\u7ed3\u6784\uff0c\u70b9\u4e91\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8bfe\u9898\uff0c\u5c24\u5176\u662f\u5728\u65b0\u89c6\u89d2\u4e0b\u7684\u70b9\u4e91\u4e0a\u91c7\u6837\u3002", "method": "\u91c7\u7528\u57fa\u4e8eHough Voting\u7684\u6295\u5f71\u6280\u672f\u548cHole Compensation\u7b56\u7565\u6d88\u9664\u8303\u56f4\u56fe\u50cf\u4e2d\u7684\u6c34\u5e73\u7ebf\u6027\u7a7a\u6d1e\uff0c\u4f7f\u7528Visual State Space\u6a21\u578b\u548c\u591a\u65b9\u5411\u626b\u63cf\u673a\u5236\u5efa\u7acb\u957f\u8ddd\u79bb\u4f9d\u8d56\u5e76\u5173\u6ce8\u5782\u76f43D\u7a7a\u95f4\u4e2d\u7684\u6f5c\u5728\u51e0\u4f55\u7279\u5f81\uff0c\u540c\u65f6\u91c7\u7528\u975e\u5bf9\u79f0U-Net\u7f51\u7edc\u9002\u5e94\u4e0d\u540c\u5149\u675f\u6570\u91cf\u7684LiDAR\u8f93\u5165\u7279\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00LiDAR\u6570\u636e\u96c6\uff08\u5982SemanticKITTI\u548cnuScenes\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSRMamba\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "conclusion": "SRMamba\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u573a\u666f\u4e2dLiDAR\u70b9\u4e91\u8d85\u5206\u8fa8\u7387\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.10829", "pdf": "https://arxiv.org/pdf/2505.10829", "abs": "https://arxiv.org/abs/2505.10829", "authors": ["Chen-Chi Chang", "Chong-Fu Li", "Chu-Hsuan Lee", "Hung-Shin Lee"], "title": "Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IntelliSys 2025", "summary": "This study investigates the challenges of translating low-resource languages\nby integrating Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG). Various model configurations were tested on Hakka translations, with\nBLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).\nThe best-performing model (Model 4) combined retrieval and advanced language\nmodeling, improving lexical coverage, particularly for specialized or\nculturally nuanced terms, and enhancing grammatical coherence. A two-stage\nmethod (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU\nscore of 26%, highlighting iterative correction's value and the challenges of\ndomain-specific expressions. Static dictionary-based approaches struggled with\ncontext-sensitive content, demonstrating the limitations of relying solely on\npredefined resources. These results emphasize the need for curated resources,\ndomain knowledge, and ethical collaboration with local communities, offering a\nframework that improves translation accuracy and fluency while supporting\ncultural preservation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6765\u6539\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\uff0c\u7279\u522b\u662f\u5728\u5ba2\u5bb6\u8bdd\u7ffb\u8bd1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u5ba2\u5bb6\u8bdd\u8fd9\u7c7b\u7f3a\u4e4f\u5e7f\u6cdb\u8d44\u6e90\u7684\u8bed\u8a00\uff0c\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u6280\u672f\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u6587\u5316\u9002\u5e94\u6027\u3002", "method": "\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\u914d\u7f6e\uff0c\u5305\u62ec\u5b57\u5178\u5355\u72ec\u4f7f\u7528\u3001RAG\u7ed3\u5408Gemini 2.0\uff0c\u4ee5\u53ca\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u5b57\u5178\u8f93\u51fa\u540e\u7531Gemini 2.0\u4f18\u5316\uff09\u3002", "result": "\u6700\u4f73\u6a21\u578b\uff08Model 4\uff09BLEU\u5206\u6570\u8fbe\u523031%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bcd\u6c47\u8986\u76d6\u7387\u548c\u8bed\u6cd5\u8fde\u8d2f\u6027\uff1b\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08Model 3\uff09\u5f97\u5206\u4e3a26%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u68c0\u7d22\u548c\u9ad8\u7ea7\u8bed\u8a00\u5efa\u6a21\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u540c\u65f6\u5f3a\u8c03\u9700\u8981\u9886\u57df\u77e5\u8bc6\u548c\u4e0e\u5f53\u5730\u793e\u533a\u7684\u5408\u4f5c\u3002", "relevance": 60.0}}
{"id": "2505.10819", "pdf": "https://arxiv.org/pdf/2505.10819", "abs": "https://arxiv.org/abs/2505.10819", "authors": ["Wasu Top Piriyakulkij", "Yichao Liang", "Hao Tang", "Adrian Weller", "Marta Kryven", "Kevin Ellis"], "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Learning how the world works is central to building AI agents that can adapt\nto complex environments. Traditional world models based on deep learning demand\nvast amounts of training data, and do not flexibly update their knowledge from\nsparse observations. Recent advances in program synthesis using Large Language\nModels (LLMs) give an alternate approach which learns world models represented\nas source code, supporting strong generalization from little data. To date,\napplication of program-structured world models remains limited to natural\nlanguage and grid-world domains. We introduce a novel program synthesis method\nfor effectively modeling complex, non-gridworld domains by representing a world\nmodel as an exponentially-weighted product of programmatic experts (PoE-World)\nsynthesized by LLMs. We show that this approach can learn complex, stochastic\nworld models from just a few observations. We evaluate the learned world models\nby embedding them in a model-based planning agent, demonstrating efficient\nperformance and generalization to unseen levels on Atari's Pong and Montezuma's\nRevenge. We release our code and display the learned world models and videos of\nthe agent's gameplay at https://topwasu.github.io/poe-world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\uff08PoE-World\uff09\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c2\u5bdf\u4e2d\u5b66\u4e60\u590d\u6742\u3001\u968f\u673a\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728Atari\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u96be\u4ee5\u7075\u6d3b\u66f4\u65b0\uff0c\u800cLLM\u7684\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u652f\u6301\u4ece\u5c11\u91cf\u6570\u636e\u4e2d\u6cdb\u5316\u3002", "method": "\u63d0\u51faPoE-World\u65b9\u6cd5\uff0c\u5c06\u4e16\u754c\u6a21\u578b\u8868\u793a\u4e3aLLM\u5408\u6210\u7684\u7a0b\u5e8f\u5316\u4e13\u5bb6\u4e58\u79ef\uff0c\u9002\u7528\u4e8e\u590d\u6742\u975e\u7f51\u683c\u4e16\u754c\u9886\u57df\u3002", "result": "\u65b9\u6cd5\u80fd\u4ece\u5c11\u91cf\u89c2\u5bdf\u4e2d\u5b66\u4e60\u590d\u6742\u968f\u673a\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728Atari\u6e38\u620f\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PoE-World\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86LLM\u5728\u7a0b\u5e8f\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10665", "pdf": "https://arxiv.org/pdf/2505.10665", "abs": "https://arxiv.org/abs/2505.10665", "authors": ["Wei Wang", "Weidong Yang", "Lei Wang", "Guihua Wang", "Ruibo Lei"], "title": "Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model", "categories": ["cs.LG", "cs.AI"], "comment": "This paper is published in npj Climate and Atmospheric Science:\n  https://www.nature.com/articles/s41612-025-01058-0#Sec16 Supplementary\n  information:\n  https://static-content.springer.com/esm/art%3A10.1038%2Fs41612-025-01058-0/MediaObjects/41612_2025_1058_MOESM1_ESM.pdf", "summary": "The rapid decline of Arctic sea ice resulting from anthropogenic climate\nchange poses significant risks to indigenous communities, ecosystems, and the\nglobal climate system. This situation emphasizes the immediate necessity for\nprecise seasonal sea ice forecasts. While dynamical models perform well for\nshort-term forecasts, they encounter limitations in long-term forecasts and are\ncomputationally intensive. Deep learning models, while more computationally\nefficient, often have difficulty managing seasonal variations and uncertainties\nwhen dealing with complex sea ice dynamics. In this research, we introduce\nIceMamba, a deep learning architecture that integrates sophisticated attention\nmechanisms within the state space model. Through comparative analysis of 25\nrenowned forecast models, including dynamical, statistical, and deep learning\napproaches, our experimental results indicate that IceMamba delivers excellent\nseasonal forecasting capabilities for Pan-Arctic sea ice concentration.\nSpecifically, IceMamba outperforms all tested models regarding average RMSE and\nanomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge\nError (IIEE). This innovative approach enhances our ability to foresee and\nalleviate the effects of sea ice variability, offering essential insights for\nstrategies aimed at climate adaptation.", "AI": {"tldr": "IceMamba\u662f\u4e00\u79cd\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5317\u6781\u6d77\u51b0\u7684\u5b63\u8282\u6027\u9884\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5317\u6781\u6d77\u51b0\u5feb\u901f\u51cf\u5c11\u5bf9\u5168\u7403\u6c14\u5019\u548c\u751f\u6001\u7cfb\u7edf\u6784\u6210\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u957f\u671f\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faIceMamba\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4e0e25\u79cd\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "IceMamba\u5728RMSE\u548cACC\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0cIIEE\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "IceMamba\u4e3a\u5b63\u8282\u6027\u6d77\u51b0\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6c14\u5019\u9002\u5e94\u7b56\u7565\u3002", "relevance": 30.0}}
{"id": "2505.10604", "pdf": "https://arxiv.org/pdf/2505.10604", "abs": "https://arxiv.org/abs/2505.10604", "authors": ["Chonghan Liu", "Haoran Wang", "Felix Henry", "Pu Miao", "Yajie Zhang", "Yu Zhao", "Peiran Wu"], "title": "MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spatial perception and reasoning are core components of human cognition,\nencompassing object recognition, spatial relational understanding, and dynamic\nreasoning. Despite progress in computer vision, existing benchmarks reveal\nsignificant gaps in models' abilities to accurately recognize object attributes\nand reason about spatial relationships, both essential for dynamic reasoning.\nTo address these limitations, we propose MIRAGE, a multi-modal benchmark\ndesigned to evaluate models' capabilities in Counting (object attribute\nrecognition), Relation (spatial relational reasoning), and Counting with\nRelation. Through diverse and complex scenarios requiring fine-grained\nrecognition and reasoning, MIRAGE highlights critical limitations in\nstate-of-the-art models, underscoring the need for improved representations and\nreasoning frameworks. By targeting these foundational abilities, MIRAGE\nprovides a pathway toward spatiotemporal reasoning in future research.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u8ba1\u6570\u3001\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u53ca\u4e24\u8005\u7ed3\u5408\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u7269\u4f53\u5c5e\u6027\u8bc6\u522b\u548c\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0cMIRAGE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u573a\u666f\uff0cMIRAGE\u6d4b\u8bd5\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u8bc6\u522b\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "MIRAGE\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "MIRAGE\u4e3a\u672a\u6765\u65f6\u7a7a\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u548c\u57fa\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.10832", "pdf": "https://arxiv.org/pdf/2505.10832", "abs": "https://arxiv.org/abs/2505.10832", "authors": ["Songjun Tu", "Jiahao Lin", "Qichao Zhang", "Xiangyu Tian", "Linjing Li", "Xiangyuan Lan", "Dongbin Zhao"], "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Project Page: https://github.com/TU2021/AutoThink", "summary": "Large reasoning models (LRMs) are proficient at generating explicit,\nstep-by-step reasoning sequences before producing final answers. However, such\ndetailed reasoning can introduce substantial computational overhead and\nlatency, particularly for simple problems. To address this over-thinking\nproblem, we explore how to equip LRMs with adaptive thinking capabilities:\nenabling them to dynamically decide whether or not to engage in explicit\nreasoning based on problem complexity. Building on R1-style distilled models,\nwe observe that inserting a simple ellipsis (\"...\") into the prompt can\nstochastically trigger either a thinking or no-thinking mode, revealing a\nlatent controllability in the reasoning behavior. Leveraging this property, we\npropose AutoThink, a multi-stage reinforcement learning (RL) framework that\nprogressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while\ndefaulting to succinct responses for simpler tasks. Experiments on five\nmainstream mathematical benchmarks demonstrate that AutoThink achieves\nfavorable accuracy-efficiency trade-offs compared to recent prompting and\nRL-based pruning methods. It can be seamlessly integrated into any R1-style\nmodel, including both distilled and further fine-tuned variants. Notably,\nAutoThink improves relative accuracy by 6.4 percent while reducing token usage\nby 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and\nadaptive reasoning paradigm for LRMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAutoThink\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u663e\u5f0f\u63a8\u7406\u884c\u4e3a\uff0c\u4f18\u5316\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u63a8\u7406\u5bfc\u81f4\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u57fa\u4e8eR1\u98ce\u683c\u84b8\u998f\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6AutoThink\u52a8\u6001\u4f18\u5316\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoThink\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u53476.4%\u51c6\u786e\u7387\u5e76\u51cf\u5c1152%\u7684token\u4f7f\u7528\u3002", "conclusion": "AutoThink\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u63a8\u7406\u8303\u5f0f\u3002", "relevance": 85.0}}
{"id": "2505.10834", "pdf": "https://arxiv.org/pdf/2505.10834", "abs": "https://arxiv.org/abs/2505.10834", "authors": ["Achintha Wijesinghe", "Weiwei Wang", "Suchinthaka Wanninayaka", "Songyang Zhang", "Zhi Ding"], "title": "TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding", "categories": ["cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Recent advancements in generative artificial intelligence have introduced\ngroundbreaking approaches to innovating next-generation semantic communication,\nwhich prioritizes conveying the meaning of a message rather than merely\ntransmitting raw data. A fundamental challenge in semantic communication lies\nin accurately identifying and extracting the most critical semantic information\nwhile adapting to downstream tasks without degrading performance, particularly\nwhen the objective at the receiver may evolve over time. To enable flexible\nadaptation to multiple tasks at the receiver, this work introduces a novel\nsemantic communication framework, which is capable of jointly capturing\ntask-specific information to enhance downstream task performance and contextual\ninformation. Through rigorous experiments on popular image datasets and\ncomputer vision tasks, our framework shows promising improvement compared to\nexisting work, including superior performance in downstream tasks, better\ngeneralizability, ultra-high bandwidth efficiency, and low reconstruction\nlatency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u80fd\u591f\u8054\u5408\u6355\u83b7\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u4e2d\u51c6\u786e\u8bc6\u522b\u548c\u63d0\u53d6\u5173\u952e\u8bed\u4e49\u4fe1\u606f\u5e76\u9002\u5e94\u52a8\u6001\u4e0b\u6e38\u4efb\u52a1\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u56fe\u50cf\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u6846\u67b6\u5728\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u3001\u5e26\u5bbd\u6548\u7387\u548c\u91cd\u5efa\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.10677", "pdf": "https://arxiv.org/pdf/2505.10677", "abs": "https://arxiv.org/abs/2505.10677", "authors": ["Ioannis Pitsiorlas", "Nour Jamoussi", "Marios Kountouris"], "title": "A Conformal Predictive Measure for Assessing Catastrophic Forgetting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This work introduces a novel methodology for assessing catastrophic\nforgetting (CF) in continual learning. We propose a new conformal prediction\n(CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to\nquantify and evaluate CF effectively. Our framework leverages adaptive CP to\nestimate forgetting by monitoring the model's confidence on previously learned\ntasks. This approach provides a dynamic and practical solution for monitoring\nand measuring CF of previous tasks as new ones are introduced, offering greater\nsuitability for real-world applications. Experimental results on four benchmark\ndatasets demonstrate a strong correlation between CPCF and the accuracy of\nprevious tasks, validating the reliability and interpretability of the proposed\nmetric. Our results highlight the potential of CPCF as a robust and effective\ntool for assessing and understanding CF in dynamic learning environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u7684\u65b0\u6307\u6807CPCF\uff0c\u7528\u4e8e\u52a8\u6001\u8bc4\u4f30\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff08CF\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\uff08CF\uff09\u7684\u8bc4\u4f30\u7f3a\u4e4f\u52a8\u6001\u548c\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u76d1\u63a7\u548c\u91cf\u5316CF\u7684\u6307\u6807\u3002", "method": "\u5229\u7528\u81ea\u9002\u5e94\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u8bbe\u8ba1CPCF\u6307\u6807\uff0c\u901a\u8fc7\u6a21\u578b\u5bf9\u5df2\u5b66\u4e60\u4efb\u52a1\u7684\u7f6e\u4fe1\u5ea6\u6765\u91cf\u5316CF\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCPCF\u4e0e\u5df2\u5b66\u4e60\u4efb\u52a1\u7684\u51c6\u786e\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CPCF\u662f\u4e00\u79cd\u52a8\u6001\u4e14\u6709\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5b66\u4e60\u73af\u5883\u4e2dCF\u7684\u8bc4\u4f30\u548c\u7406\u89e3\u3002", "relevance": 40.0}}
{"id": "2505.10610", "pdf": "https://arxiv.org/pdf/2505.10610", "abs": "https://arxiv.org/abs/2505.10610", "authors": ["Zhaowei Wang", "Wenhao Yu", "Xiyu Ren", "Jipeng Zhang", "Yu Zhao", "Rohit Saxena", "Liang Cheng", "Ginny Wong", "Simon See", "Pasquale Minervini", "Yangqiu Song", "Mark Steedman"], "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.", "AI": {"tldr": "MMLongBench\u662f\u9996\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LCVLMs\uff09\u7684\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d613,331\u4e2a\u793a\u4f8b\u548c\u4e94\u79cd\u4efb\u52a1\u7c7b\u522b\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8f93\u5165\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u957f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u5176\u80fd\u529b\uff0cMMLongBench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MMLongBench\u5305\u542b13,331\u4e2a\u793a\u4f8b\uff0c\u6db5\u76d6\u4e94\u79cd\u4efb\u52a1\u7c7b\u522b\u548c\u591a\u79cd\u56fe\u50cf\u7c7b\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6807\u8bb0\u5316\u65b9\u6848\u5728\u4e94\u79cd\u6807\u51c6\u5316\u8f93\u5165\u957f\u5ea6\uff088K-128K tokens\uff09\u4e0b\u8bc4\u4f3046\u4e2aLCVLMs\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5355\u4efb\u52a1\u8868\u73b0\u4e0d\u80fd\u5168\u9762\u53cd\u6620\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff1b\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5747\u9762\u4e34\u6311\u6218\uff1b\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "MMLongBench\u4e3a\u8bca\u65ad\u548c\u63a8\u8fdb\u4e0b\u4e00\u4ee3LCVLMs\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "relevance": 70.0}}
{"id": "2505.10836", "pdf": "https://arxiv.org/pdf/2505.10836", "abs": "https://arxiv.org/abs/2505.10836", "authors": ["Abhishek Dey", "Aabha Bothera", "Samhita Sarikonda", "Rishav Aryan", "Sanjay Kumar Podishetty", "Akshay Havalgi", "Gaurav Singh", "Saurabh Srivastava"], "title": "Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at NLDB 2025", "summary": "In this paper, we study the challenges of detecting events on social media,\nwhere traditional unimodal systems struggle due to the rapid and multimodal\nnature of data dissemination. We employ a range of models, including unimodal\nModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced\ngenerative models like GPT-4o, and LLaVA. Additionally, we also study the\neffect of providing multimodal generative models (such as GPT-4o) with a single\nmodality to assess their efficacy. Our results indicate that while multimodal\napproaches notably outperform unimodal counterparts, generative approaches\ndespite having a large number of parameters, lag behind supervised methods in\nprecision. Furthermore, we also found that they lag behind instruction-tuned\nmodels because of their inability to generate event classes correctly. During\nour error analysis, we discovered that common social media issues such as leet\nspeak, text elongation, etc. are effectively handled by generative approaches\nbut are hard to tackle using supervised approaches.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u5a92\u4f53\u4e8b\u4ef6\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u6bd4\u8f83\u4e86\u5355\u6a21\u6001\u3001\u591a\u6a21\u6001\u548c\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u65b9\u6cd5\u4f18\u4e8e\u5355\u6a21\u6001\uff0c\u4f46\u751f\u6210\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u843d\u540e\u4e8e\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5355\u6a21\u6001\u7cfb\u7edf\u5728\u5feb\u901f\u3001\u591a\u6a21\u6001\u7684\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4f20\u64ad\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u63a2\u7d22\u591a\u6a21\u6001\u548c\u751f\u6210\u6a21\u578b\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u4e86\u5355\u6a21\u6001\u6a21\u578b\uff08ModernBERT\u3001ConvNeXt-V2\uff09\u3001\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u548c\u751f\u6210\u6a21\u578b\uff08GPT-4o\u3001LLaVA\uff09\uff0c\u5e76\u6d4b\u8bd5\u751f\u6210\u6a21\u578b\u5728\u5355\u6a21\u6001\u8f93\u5165\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\uff0c\u4f46\u751f\u6210\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u843d\u540e\u4e8e\u76d1\u7763\u65b9\u6cd5\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u5c3d\u7ba1\u80fd\u6709\u6548\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u5e38\u89c1\u95ee\u9898\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u6f5c\u529b\u6709\u9650\uff0c\u4f46\u5728\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u7279\u6709\u566a\u58f0\u65b9\u9762\u8868\u73b0\u8f83\u597d\u3002", "relevance": 40.0}}
{"id": "2505.10844", "pdf": "https://arxiv.org/pdf/2505.10844", "abs": "https://arxiv.org/abs/2505.10844", "authors": ["Simeng Han", "Stephen Xia", "Grant Zhang", "Howard Dai", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "13 Tables; 5 Figures", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8111\u529b\u6fc0\u8361\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6df1\u5165\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5305\u62ec\u6b63\u786e\u6027\u3001\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u521b\u9020\u529b\u3002", "motivation": "\u4f20\u7edf\u51c6\u786e\u6027\u6307\u6807\u65e0\u6cd5\u63ed\u793a\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8111\u529b\u6fc0\u8361\u9898\u56e0\u5176\u591a\u89e3\u6027\u9002\u5408\u6df1\u5165\u5206\u6790\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u591a\u5c42\u9762\u7814\u7a76LLMs\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5305\u62ec\u8bed\u4e49\u89e3\u6790\u3001\u89e3\u51b3\u65b9\u6848\u751f\u6210\u3001\u81ea\u6211\u4fee\u6b63\u3001\u6b65\u9aa4\u8349\u56fe\u548c\u63d0\u793a\u5229\u7528\u3002", "result": "LLMs\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u63d0\u4f9b\u521b\u9020\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e5f\u5b58\u5728\u4f9d\u8d56\u66b4\u529b\u6c42\u89e3\u7684\u60c5\u51b5\u3002", "conclusion": "LLMs\u5177\u5907\u4e00\u5b9a\u7684\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u5728\u63a8\u7406\u6548\u7387\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "relevance": 85.0}}
{"id": "2505.10689", "pdf": "https://arxiv.org/pdf/2505.10689", "abs": "https://arxiv.org/abs/2505.10689", "authors": ["Gabriele Santini", "Francesco Paissan", "Elisabetta Farella"], "title": "A probabilistic framework for dynamic quantization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We propose a probabilistic framework for dynamic quantization of neural\nnetworks that allows for a computationally efficient input-adaptive rescaling\nof the quantization parameters. Our framework applies a probabilistic model to\nthe network's pre-activations through a lightweight surrogate, enabling the\nadaptive adjustment of the quantization parameters on a per-input basis without\nsignificant memory overhead. We validate our approach on a set of popular\ncomputer vision tasks and models, observing only a negligible loss in\nperformance. Our method strikes the best performance and computational overhead\ntradeoff compared to standard quantization strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6a21\u578b\u81ea\u9002\u5e94\u8c03\u6574\u91cf\u5316\u53c2\u6570\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u8f93\u5165\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u8f93\u5165\u81ea\u9002\u5e94\u7684\u91cf\u5316\u53c2\u6570\u8c03\u6574\u3002", "method": "\u5229\u7528\u6982\u7387\u6a21\u578b\u5bf9\u9884\u6fc0\u6d3b\u8fdb\u884c\u5efa\u6a21\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4ee3\u7406\u52a8\u6001\u8c03\u6574\u91cf\u5316\u53c2\u6570\uff0c\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u4f18\u4e8e\u6807\u51c6\u91cf\u5316\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002", "relevance": 40.0}}
{"id": "2505.10634", "pdf": "https://arxiv.org/pdf/2505.10634", "abs": "https://arxiv.org/abs/2505.10634", "authors": ["Jianfei Zhao", "Feng Zhang", "Xin Sun", "Chong Feng"], "title": "Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Language priors constitute one of the primary causes of hallucinations in\nLarge Vision-Language Models (LVLMs), driving the models to generate\nlinguistically plausible yet visually inconsistent content. The language priors\nin LVLMs originate from the linguistic knowledge inherited from their\npre-trained Large Language Model (LLM) backbone. Consequently, this\ncharacteristic is an intrinsic property of the model that remains independent\nof visual inputs. Inspired by the finding that language priors are consistent\nacross images, we propose Cross-Image Contrastive Decoding (CICD), a simple yet\neffective training-free method to alleviate language priors in LVLMs. CICD\nfirst identifies essential and detrimental priors, and then employs contrastive\ndecoding to eliminate the detrimental ones. This approach simultaneously\nprevents LVLMs from generating hallucinated content while maintaining textual\nfluency and coherence. Furthermore, the limited information overlap between\nimages helps prevent visual information loss during contrastive decoding. We\nvalidate the effectiveness of CICD on four benchmarks with six LVLMs. Our\nexperiments demonstrate that CICD performs remarkably well in mitigating\nlanguage priors, especially in the image captioning task, where such priors are\nmost pronounced. Code will be released once accepted.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCICD\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u56fe\u50cf\u5bf9\u6bd4\u89e3\u7801\u7f13\u89e3LVLMs\u4e2d\u7684\u8bed\u8a00\u5148\u9a8c\u95ee\u9898\uff0c\u51cf\u5c11\u5e7b\u89c9\u5185\u5bb9\u751f\u6210\u3002", "motivation": "LVLMs\u4e2d\u7684\u8bed\u8a00\u5148\u9a8c\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u89c6\u89c9\u4e0d\u4e00\u81f4\u4f46\u8bed\u8a00\u5408\u7406\u7684\u5185\u5bb9\uff0c\u8fd9\u4e00\u95ee\u9898\u6e90\u4e8e\u9884\u8bad\u7ec3LLM\u7684\u56fa\u6709\u7279\u6027\u3002", "method": "CICD\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u548c\u6709\u5bb3\u5148\u9a8c\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u89e3\u7801\u6d88\u9664\u6709\u5bb3\u5148\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u6d41\u7545\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u5b8c\u6574\u6027\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u516d\u79cdLVLM\u4e0a\u9a8c\u8bc1\u4e86CICD\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CICD\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u7f13\u89e3\u8bed\u8a00\u5148\u9a8c\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "relevance": 80.0}}
{"id": "2505.10862", "pdf": "https://arxiv.org/pdf/2505.10862", "abs": "https://arxiv.org/abs/2505.10862", "authors": ["Tairan Fu", "Miguel Gonz\u00e1lez", "Javier Conde", "Elena Merino-G\u00f3mez", "Pedro Reviriego"], "title": "Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?", "categories": ["cs.CL", "I.2.7"], "comment": "6 pages, 5 figures, 2 tables", "summary": "Multimodal Large Language Models which can answer complex questions on an\nimage struggle to tell the time on analog clocks. This is probably due to the\nlack of images with clocks at different times in their training set. In this\nwork we explore this issue with one of the latest MLLMs: GPT-4.1 to understand\nwhy MLLMs fail to tell the time and whether fine-tuning can solve the problem.\nThe results show how models are making progress in reading the time on analog\nclocks. But have they really learned to do it, or have they only learned\npatterns in their training datasets? In this work we put the models to the test\nwith different clocks to illustrate the limitations of MLLMs to abstract and\ngeneralize.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8bfb\u53d6\u6a21\u62df\u65f6\u949f\u65f6\u95f4\u4e0a\u7684\u5931\u8d25\u539f\u56e0\uff0c\u5e76\u901a\u8fc7GPT-4.1\u6d4b\u8bd5\u4e86\u5fae\u8c03\u662f\u5426\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u62bd\u8c61\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u56fe\u50cf\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8bfb\u53d6\u6a21\u62df\u65f6\u949f\u65f6\u95f4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u5c11\u4e0d\u540c\u65f6\u95f4\u7684\u65f6\u949f\u56fe\u50cf\u3002", "method": "\u4f7f\u7528GPT-4.1\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5206\u6790\u6a21\u578b\u5931\u8d25\u539f\u56e0\uff0c\u5e76\u5c1d\u8bd5\u901a\u8fc7\u5fae\u8c03\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u8bfb\u53d6\u6a21\u62df\u65f6\u949f\u65f6\u95f4\u4e0a\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u53ef\u80fd\u53ea\u662f\u5b66\u4e60\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\uff0c\u800c\u975e\u771f\u6b63\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\u3002", "conclusion": "MLLMs\u5728\u62bd\u8c61\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u771f\u5b9e\u7406\u89e3\u80fd\u529b\u3002", "relevance": 65.0}}
{"id": "2505.10859", "pdf": "https://arxiv.org/pdf/2505.10859", "abs": "https://arxiv.org/abs/2505.10859", "authors": ["Yingdan Shi", "Ren Wang"], "title": "MCU: Improving Machine Unlearning through Mode Connectivity", "categories": ["cs.AI"], "comment": null, "summary": "Machine Unlearning (MU) aims to remove the information of specific training\ndata from a trained model, ensuring compliance with privacy regulations and\nuser requests. While one line of existing MU methods relies on linear parameter\nupdates via task arithmetic, they suffer from weight entanglement. In this\nwork, we propose a novel MU framework called Mode Connectivity Unlearning (MCU)\nthat leverages mode connectivity to find an unlearning pathway in a nonlinear\nmanner. To further enhance performance and efficiency, we introduce a parameter\nmask strategy that not only improves unlearning effectiveness but also reduces\ncomputational overhead. Moreover, we propose an adaptive adjustment strategy\nfor our unlearning penalty coefficient to adaptively balance forgetting quality\nand predictive performance during training, eliminating the need for empirical\nhyperparameter tuning. Unlike traditional MU methods that identify only a\nsingle unlearning model, MCU uncovers a spectrum of unlearning models along the\npathway. Overall, MCU serves as a plug-and-play framework that seamlessly\nintegrates with any existing MU methods, consistently improving unlearning\nefficacy. Extensive experiments on the image classification task demonstrate\nthat MCU achieves superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMode Connectivity Unlearning (MCU)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u65b9\u5f0f\u5b9e\u73b0\u673a\u5668\u9057\u5fd8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ebf\u6027\u53c2\u6570\u66f4\u65b0\u65b9\u6cd5\u4e2d\u7684\u6743\u91cd\u7ea0\u7f20\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u65e8\u5728\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u4fe1\u606f\u4ee5\u6ee1\u8db3\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\uff0c\u4f46\u73b0\u6709\u7ebf\u6027\u66f4\u65b0\u65b9\u6cd5\u5b58\u5728\u6743\u91cd\u7ea0\u7f20\u95ee\u9898\u3002", "method": "MCU\u5229\u7528\u6a21\u5f0f\u8fde\u63a5\u6027\u975e\u7ebf\u6027\u5730\u627e\u5230\u9057\u5fd8\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u53c2\u6570\u63a9\u7801\u7b56\u7565\u548c\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCU\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9057\u5fd8\u6548\u679c\u3002", "conclusion": "MCU\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709MU\u65b9\u6cd5\uff0c\u6301\u7eed\u63d0\u5347\u9057\u5fd8\u6548\u679c\u3002", "relevance": 40.0}}
{"id": "2505.10698", "pdf": "https://arxiv.org/pdf/2505.10698", "abs": "https://arxiv.org/abs/2505.10698", "authors": ["Alexia Atsidakou", "Orestis Papadigenopoulos", "Constantine Caramanis", "Sujay Sanghavi", "Sanjay Shakkottai"], "title": "Asymptotically-Optimal Gaussian Bandits with Side Observations", "categories": ["cs.LG", "stat.ML"], "comment": "International Conference on Machine Learning, ICML '22", "summary": "We study the problem of Gaussian bandits with general side information, as\nfirst introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an\narm reveals information about other arms, according to an arbitrary a priori\nknown side information matrix: each element of this matrix encodes the fidelity\nof the information that the ``row'' arm reveals about the ``column'' arm. In\nthe case of Gaussian noise, this model subsumes standard bandits,\nfull-feedback, and graph-structured feedback as special cases. In this work, we\nfirst construct an LP-based asymptotic instance-dependent lower bound on the\nregret. The LP optimizes the cost (regret) required to reliably estimate the\nsuboptimality gap of each arm. This LP lower bound motivates our main\ncontribution: the first known asymptotically optimal algorithm for this general\nsetting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u9ad8\u65af\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5f15\u5165\u4fa7\u4fe1\u606f\u77e9\u9635\u63cf\u8ff0\u81c2\u95f4\u4fe1\u606f\u4f20\u9012\uff0c\u63d0\u51fa\u57fa\u4e8eLP\u7684\u6e10\u8fdb\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5728\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u5df2\u77e5\u7684\u4fa7\u4fe1\u606f\u77e9\u9635\u4f18\u5316\u51b3\u7b56\uff0c\u4ee5\u6700\u5c0f\u5316\u9057\u61be\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u7684\u6e10\u8fdb\u5b9e\u4f8b\u4f9d\u8d56\u4e0b\u754c\uff0c\u5e76\u8bbe\u8ba1\u9996\u4e2a\u6e10\u8fdb\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u9ad8\u65af\u566a\u58f0\u548c\u4fa7\u4fe1\u606f\u77e9\u9635\u7684\u6e10\u8fdb\u6700\u4f18\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u4e00\u822c\u8bbe\u7f6e\u4e0b\u5177\u6709\u6e10\u8fdb\u6700\u4f18\u6027\uff0c\u6269\u5c55\u4e86\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.10649", "pdf": "https://arxiv.org/pdf/2505.10649", "abs": "https://arxiv.org/abs/2505.10649", "authors": ["Xianrui Li", "Yufei Cui", "Jun Li", "Antoni B. Chan"], "title": "Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Advances in medical imaging and deep learning have propelled progress in\nwhole slide image (WSI) analysis, with multiple instance learning (MIL) showing\npromise for efficient and accurate diagnostics. However, conventional MIL\nmodels often lack adaptability to evolving datasets, as they rely on static\ntraining that cannot incorporate new information without extensive retraining.\nApplying continual learning (CL) to MIL models is a possible solution, but\noften sees limited improvements. In this paper, we analyze CL in the context of\nattention MIL models and find that the model forgetting is mainly concentrated\nin the attention layers of the MIL model. Using the results of this analysis we\npropose two components for improving CL on MIL: Attention Knowledge\nDistillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates\ncatastrophic forgetting by focusing on retaining attention layer knowledge\nbetween learning sessions, while PMP reduces the memory footprint by\nselectively storing only the most informative patches, or ``pseudo-bags'' from\nWSIs. Experimental evaluations demonstrate that our method significantly\nimproves both accuracy and memory efficiency on diverse WSI datasets,\noutperforming current state-of-the-art CL methods. This work provides a\nfoundation for CL in large-scale, weakly annotated clinical datasets, paving\nthe way for more adaptable and resilient diagnostic models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u6ce8\u610f\u529b\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u77e5\u8bc6\u84b8\u998f\uff08AKD\uff09\u548c\u4f2a\u888b\u8bb0\u5fc6\u6c60\uff08PMP\uff09\u6765\u51cf\u5c11\u9057\u5fd8\u5e76\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u4f20\u7edfMIL\u6a21\u578b\u7f3a\u4e4f\u5bf9\u52a8\u6001\u6570\u636e\u96c6\u7684\u9002\u5e94\u6027\uff0c\u800c\u73b0\u6709CL\u65b9\u6cd5\u5728MIL\u4e2d\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u5206\u6790\u4e86\u6ce8\u610f\u529b\u5c42\u662f\u9057\u5fd8\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86AKD\uff08\u4e13\u6ce8\u4e8e\u4fdd\u7559\u6ce8\u610f\u529b\u5c42\u77e5\u8bc6\uff09\u548cPMP\uff08\u9009\u62e9\u6027\u5b58\u50a8\u4fe1\u606f\u4e30\u5bcc\u7684\u4f2a\u888b\uff09\u4e24\u79cd\u7ec4\u4ef6\uff0c\u4ee5\u51cf\u5c11\u9057\u5fd8\u5e76\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684WSI\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5185\u5b58\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709CL\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u5927\u89c4\u6a21\u5f31\u6807\u6ce8\u4e34\u5e8a\u6570\u636e\u96c6\u7684CL\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u66f4\u9002\u5e94\u6027\u5f3a\u548c\u7a33\u5065\u7684\u8bca\u65ad\u6a21\u578b\u7684\u53d1\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.10870", "pdf": "https://arxiv.org/pdf/2505.10870", "abs": "https://arxiv.org/abs/2505.10870", "authors": ["Ziyang Huang", "Wangtao Sun", "Jun Zhao", "Kang Liu"], "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "This paper systematically addresses the challenges of rule retrieval, a\ncrucial yet underexplored area. Vanilla retrieval methods using sparse or dense\nretrievers to directly search for relevant rules to support downstream\nreasoning, often suffer from low accuracy. This is primarily due to a\nsignificant semantic gap between the instantiated facts in the queries and the\nabstract representations of the rules. Such misalignment results in suboptimal\nretrieval quality, which in turn negatively impacts reasoning performance. To\novercome these challenges, we propose Self-Induction Augmented Retrieval\n(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce\npotential inferential rules that might offer benefits for reasoning by\nabstracting the underlying knowledge and logical structure in queries. These\ninduced rules are then used for query augmentation to improve retrieval\neffectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a\nmethod that re-estimates the relevance of retrieved rules by assessing whether\nthe abstract knowledge they contain can be instantiated to align with the facts\nin the queries and the helpfulness for reasoning. Extensive experiments across\nvarious settings demonstrate the effectiveness and versatility of our proposed\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSIAR\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528LLM\u4ece\u67e5\u8be2\u4e2d\u5f52\u7eb3\u6f5c\u5728\u63a8\u7406\u89c4\u5219\u4ee5\u6539\u8fdb\u89c4\u5219\u68c0\u7d22\uff0c\u5e76\u7ed3\u5408R$^3$\u65b9\u6cd5\u91cd\u65b0\u8bc4\u4f30\u89c4\u5219\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c4\u5219\u68c0\u7d22\u65b9\u6cd5\u56e0\u67e5\u8be2\u4e8b\u5b9e\u4e0e\u89c4\u5219\u62bd\u8c61\u8868\u793a\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\u5bfc\u81f4\u68c0\u7d22\u51c6\u786e\u6027\u4f4e\uff0c\u5f71\u54cd\u4e0b\u6e38\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51faSIAR\u65b9\u6cd5\uff0c\u5229\u7528LLM\u5f52\u7eb3\u6f5c\u5728\u63a8\u7406\u89c4\u5219\u5e76\u7528\u4e8e\u67e5\u8be2\u589e\u5f3a\uff1b\u5f15\u5165R$^3$\u65b9\u6cd5\u91cd\u65b0\u8bc4\u4f30\u89c4\u5219\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSIAR\u548cR$^3$\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5747\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u679c\u548c\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "SIAR\u548cR$^3$\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c4\u5219\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "relevance": 75.0}}
{"id": "2505.10887", "pdf": "https://arxiv.org/pdf/2505.10887", "abs": "https://arxiv.org/abs/2505.10887", "authors": ["Bin Lei", "Weitai Kang", "Zijian Zhang", "Winson Chen", "Xi Xie", "Shan Zuo", "Mimi Xie", "Ali Payani", "Mingyi Hong", "Yan Yan", "Caiwen Ding"], "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces \\textsc{InfantAgent-Next}, a generalist agent capable\nof interacting with computers in a multimodal manner, encompassing text,\nimages, audio, and video. Unlike existing approaches that either build\nintricate workflows around a single large model or only provide workflow\nmodularity, our agent integrates tool-based and pure vision agents within a\nhighly modular architecture, enabling different models to collaboratively solve\ndecoupled tasks in a step-by-step manner. Our generality is demonstrated by our\nability to evaluate not only pure vision-based real-world benchmarks (i.e.,\nOSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and\nSWE-Bench). Specifically, we achieve $\\mathbf{7.27\\%}$ accuracy on OSWorld,\nhigher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced\nat https://github.com/bin123apple/InfantAgent.", "AI": {"tldr": "InfantAgent-Next\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u901a\u7528\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u6574\u5408\u5de5\u5177\u548c\u7eaf\u89c6\u89c9\u4ee3\u7406\uff0c\u80fd\u591f\u534f\u4f5c\u89e3\u51b3\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u56f4\u7ed5\u5355\u4e00\u5927\u578b\u6a21\u578b\u6784\u5efa\u590d\u6742\u5de5\u4f5c\u6d41\uff0c\u8981\u4e48\u4ec5\u63d0\u4f9b\u5de5\u4f5c\u6d41\u6a21\u5757\u5316\uff0c\u800cInfantAgent-Next\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u5b9e\u73b0\u591a\u6a21\u578b\u534f\u4f5c\uff0c\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9ad8\u5ea6\u6a21\u5757\u5316\u7684\u67b6\u6784\uff0c\u6574\u5408\u5de5\u5177\u548c\u7eaf\u89c6\u89c9\u4ee3\u7406\uff0c\u652f\u6301\u591a\u6a21\u578b\u534f\u4f5c\u9010\u6b65\u89e3\u51b3\u4efb\u52a1\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52307.27%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eClaude-Computer-Use\uff0c\u5e76\u5728GAIA\u548cSWE-Bench\u7b49\u66f4\u901a\u7528\u6216\u5de5\u5177\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "InfantAgent-Next\u5c55\u793a\u4e86\u6a21\u5757\u5316\u67b6\u6784\u5728\u591a\u6a21\u6001\u901a\u7528\u4ee3\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "relevance": 70.0}}
{"id": "2505.10699", "pdf": "https://arxiv.org/pdf/2505.10699", "abs": "https://arxiv.org/abs/2505.10699", "authors": ["Kutay B\u00f6lat", "Tarek Alskaif", "Peter Palensky", "Simon Tindemans"], "title": "Clustering Rooftop PV Systems via Probabilistic Embeddings", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "As the number of rooftop photovoltaic (PV) installations increases,\naggregators and system operators are required to monitor and analyze these\nsystems, raising the challenge of integration and management of large,\nspatially distributed time-series data that are both high-dimensional and\naffected by missing values. In this work, a probabilistic entity\nembedding-based clustering framework is proposed to address these problems.\nThis method encodes each PV system's characteristic power generation patterns\nand uncertainty as a probability distribution, then groups systems by their\nstatistical distances and agglomerative clustering. Applied to a multi-year\nresidential PV dataset, it produces concise, uncertainty-aware cluster profiles\nthat outperform a physics-based baseline in representativeness and robustness,\nand support reliable missing-value imputation. A systematic hyperparameter\nstudy further offers practical guidance for balancing model performance and\nrobustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u5b9e\u4f53\u5d4c\u5165\u7684\u805a\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5c4b\u9876\u5149\u4f0f\u7cfb\u7edf\u7684\u9ad8\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7ba1\u7406\u548c\u7f3a\u5931\u503c\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5c4b\u9876\u5149\u4f0f\u7cfb\u7edf\u5b89\u88c5\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5982\u4f55\u9ad8\u6548\u7ba1\u7406\u548c\u5206\u6790\u8fd9\u4e9b\u9ad8\u7ef4\u3001\u7a7a\u95f4\u5206\u5e03\u4e14\u5b58\u5728\u7f3a\u5931\u503c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6210\u4e3a\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u5149\u4f0f\u7cfb\u7edf\u7684\u53d1\u7535\u6a21\u5f0f\u548c\u4e0d\u786e\u5b9a\u6027\u7f16\u7801\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u8ddd\u79bb\u548c\u51dd\u805a\u805a\u7c7b\u5bf9\u7cfb\u7edf\u8fdb\u884c\u5206\u7ec4\u3002", "result": "\u5e94\u7528\u4e8e\u591a\u5e74\u4f4f\u5b85\u5149\u4f0f\u6570\u636e\u96c6\u65f6\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u805a\u7c7b\u914d\u7f6e\u6587\u4ef6\u5728\u4ee3\u8868\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7269\u7406\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u53ef\u9760\u7684\u7f3a\u5931\u503c\u63d2\u8865\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5149\u4f0f\u7cfb\u7edf\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u8d85\u53c2\u6570\u7814\u7a76\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u5e73\u8861\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "relevance": 20.0}}
{"id": "2505.10664", "pdf": "https://arxiv.org/pdf/2505.10664", "abs": "https://arxiv.org/abs/2505.10664", "authors": ["Ziyang Ou"], "title": "CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "8 pages, 5 figures, not submitted to any conference", "summary": "Verifying the authenticity of AI-generated images presents a growing\nchallenge on social media platforms these days. While vision-language models\n(VLMs) like CLIP outdo in multimodal representation, their capacity for\nAI-generated image classification is underexplored due to the absence of such\nlabels during the pre-training process. This work investigates whether CLIP\nembeddings inherently contain information indicative of AI generation. A\nproposed pipeline extracts visual embeddings using a frozen CLIP model, feeds\nits embeddings to lightweight networks, and fine-tunes only the final\nclassifier. Experiments on the public CIFAKE benchmark show the performance\nreaches 95% accuracy without language reasoning. Few-shot adaptation to curated\ncustom with 20% of the data results in performance to 85%. A closed-source\nbaseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific\nstyles. Notably, some specific image types, such as wide-angle photographs and\noil paintings, pose significant challenges to classification. These results\nindicate previously unexplored difficulties in classifying certain types of\nAI-generated images, revealing new and more specific questions in this domain\nthat are worth further investigation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86CLIP\u5d4c\u5165\u662f\u5426\u5305\u542bAI\u751f\u6210\u56fe\u50cf\u7684\u4fe1\u606f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728CIFAKE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523095%\u51c6\u786e\u7387\uff0c\u4f46\u67d0\u4e9b\u56fe\u50cf\u7c7b\u578b\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u9a8c\u8bc1AI\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u662f\u793e\u4ea4\u5a92\u4f53\u7684\u91cd\u8981\u6311\u6218\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684CLIP\u6a21\u578b\u63d0\u53d6\u89c6\u89c9\u5d4c\u5165\uff0c\u8f93\u5165\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5e76\u5fae\u8c03\u6700\u7ec8\u5206\u7c7b\u5668\u3002", "result": "\u5728CIFAKE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523095%\u51c6\u786e\u7387\uff0c\u5c11\u91cf\u6570\u636e\u9002\u5e94\u540e\u4e3a85%\uff0c\u4f46\u67d0\u4e9b\u56fe\u50cf\u7c7b\u578b\uff08\u5982\u5e7f\u89d2\u7167\u7247\u548c\u6cb9\u753b\uff09\u5206\u7c7b\u56f0\u96be\u3002", "conclusion": "CLIP\u5d4c\u5165\u53ef\u7528\u4e8eAI\u751f\u6210\u56fe\u50cf\u5206\u7c7b\uff0c\u4f46\u67d0\u4e9b\u7279\u5b9a\u7c7b\u578b\u56fe\u50cf\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.10924", "pdf": "https://arxiv.org/pdf/2505.10924", "abs": "https://arxiv.org/abs/2505.10924", "authors": ["Ada Chen", "Yongjiang Wu", "Junyuan Zhang", "Shu Yang", "Jen-tse Huang", "Kun Wang", "Wenxuan Wang", "Shuai Wang"], "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.SE"], "comment": null, "summary": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5316\u5730\u7814\u7a76\u4e86\u57fa\u4e8eLLM\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u7684\u5b89\u5168\u4e0e\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u3001\u9632\u5fa1\u7b56\u7565\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u968f\u7740LLM\u9a71\u52a8\u7684CUAs\u80fd\u529b\u589e\u5f3a\uff0c\u5176\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u5206\u6790\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u5b89\u5168\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u56f4\u7ed5\u56db\u4e2a\u76ee\u6807\u5c55\u5f00\u7814\u7a76\uff1a\u5b9a\u4e49CUA\u3001\u5206\u7c7b\u5a01\u80c1\u3001\u63d0\u51fa\u9632\u5fa1\u7b56\u7565\u3001\u603b\u7ed3\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86CUAs\u7684\u5b89\u5168\u5a01\u80c1\u5206\u7c7b\u3001\u9632\u5fa1\u7b56\u7565\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u8bba\u6587\u4e3aCUAs\u7684\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u63a2\u7d22\u7684\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.10962", "pdf": "https://arxiv.org/pdf/2505.10962", "abs": "https://arxiv.org/abs/2505.10962", "authors": ["Zhenwen Liang", "Linfeng Song", "Yang Li", "Tao Yang", "Feng Zhang", "Haitao Mi", "Dong Yu"], "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation", "categories": ["cs.AI"], "comment": "Work in Progress", "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.", "AI": {"tldr": "MPS-Prover\u662f\u4e00\u79cd\u65b0\u578b\u7684\u9010\u6b65\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u526a\u679d\u548c\u591a\u89c6\u89d2\u6811\u641c\u7d22\u673a\u5236\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9010\u6b65\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u56e0\u641c\u7d22\u504f\u5dee\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u7b56\u7565\u6b21\u4f18\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6570\u636e\u526a\u679d\u7b56\u7565\u548c\u591a\u89c6\u89d2\u6811\u641c\u7d22\u673a\u5236\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u6279\u8bc4\u6a21\u578b\u548c\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "result": "\u5728miniF2F\u548cProofNet\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u751f\u6210\u7684\u8bc1\u660e\u66f4\u77ed\u4e14\u591a\u6837\u3002", "conclusion": "MPS-Prover\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5b9a\u7406\u8bc1\u660e\u5668\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "relevance": 70.0}}
{"id": "2505.10704", "pdf": "https://arxiv.org/pdf/2505.10704", "abs": "https://arxiv.org/abs/2505.10704", "authors": ["Patryk Marsza\u0142ek", "Tomasz Ku\u015bmierczyk", "Witold Wydma\u0144ski", "Jacek Tabor", "Marek \u015amieja"], "title": "ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data", "categories": ["cs.LG"], "comment": null, "summary": "Clustering tabular data remains a significant open challenge in data analysis\nand machine learning. Unlike for image data, similarity between tabular records\noften varies across datasets, making the definition of clusters highly\ndataset-dependent. Furthermore, the absence of supervised signals complicates\nhyperparameter tuning in deep learning clustering methods, frequently resulting\nin unstable performance. To address these issues and reduce the need for\nper-dataset tuning, we adopt an emerging approach in deep learning: zero-shot\nlearning. We propose ZEUS, a self-contained model capable of clustering new\ndatasets without any additional training or fine-tuning. It operates by\ndecomposing complex datasets into meaningful components that can then be\nclustered effectively. Thanks to pre-training on synthetic datasets generated\nfrom a latent-variable prior, it generalizes across various datasets without\nrequiring user intervention. To the best of our knowledge, ZEUS is the first\nzero-shot method capable of generating embeddings for tabular data in a fully\nunsupervised manner. Experimental results demonstrate that it performs on par\nwith or better than traditional clustering algorithms and recent deep\nlearning-based methods, while being significantly faster and more\nuser-friendly.", "AI": {"tldr": "ZEUS\u662f\u4e00\u79cd\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u805a\u7c7b\u8868\u683c\u6570\u636e\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7684\u805a\u7c7b\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u76f8\u4f3c\u6027\u56e0\u6570\u636e\u96c6\u800c\u5f02\uff0c\u4e14\u7f3a\u4e4f\u76d1\u7763\u4fe1\u53f7\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u5d4c\u5165\uff0c\u5206\u89e3\u590d\u6742\u6570\u636e\u96c6\u4e3a\u53ef\u805a\u7c7b\u7ec4\u4ef6\u3002", "result": "ZEUS\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u901f\u5ea6\u66f4\u5feb\u4e14\u66f4\u7528\u6237\u53cb\u597d\u3002", "conclusion": "ZEUS\u662f\u9996\u4e2a\u96f6\u6837\u672c\u65e0\u76d1\u7763\u8868\u683c\u6570\u636e\u805a\u7c7b\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10671", "pdf": "https://arxiv.org/pdf/2505.10671", "abs": "https://arxiv.org/abs/2505.10671", "authors": ["Yuki Kawana", "Shintaro Shiba", "Quan Kong", "Norimasa Kobori"], "title": "GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025. Project page:\n  https://woven-visionai.github.io/ga3ce-project/", "summary": "We propose a novel 3D gaze estimation approach that learns spatial\nrelationships between the subject and objects in the scene, and outputs 3D gaze\ndirection. Our method targets unconstrained settings, including cases where\nclose-up views of the subject's eyes are unavailable, such as when the subject\nis distant or facing away. Previous approaches typically rely on either 2D\nappearance alone or incorporate limited spatial cues using depth maps in the\nnon-learnable post-processing step. Estimating 3D gaze direction from 2D\nobservations in these scenarios is challenging; variations in subject pose,\nscene layout, and gaze direction, combined with differing camera poses, yield\ndiverse 2D appearances and 3D gaze directions even when targeting the same 3D\nscene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding.\nOur method represents subject and scene using 3D poses and object positions,\ntreating them as 3D context to learn spatial relationships in 3D space.\nInspired by human vision, we align this context in an egocentric space,\nsignificantly reducing spatial complexity. Furthermore, we propose D$^3$\n(direction-distance-decomposed) positional encoding to better capture the\nspatial relationship between 3D context and gaze direction in direction and\ndistance space. Experiments demonstrate substantial improvements, reducing mean\nangle error by 13%-37% compared to leading baselines on benchmark datasets in\nsingle-frame settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5GA3CE\uff0c\u901a\u8fc7\u5b66\u4e60\u4e3b\u4f53\u4e0e\u573a\u666f\u4e2d\u7269\u4f53\u7684\u7a7a\u95f4\u5173\u7cfb\u6765\u4f30\u8ba13D\u89c6\u7ebf\u65b9\u5411\uff0c\u9002\u7528\u4e8e\u65e0\u7ea6\u675f\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u7ea6\u675f\u573a\u666f\u4e2d\uff08\u5982\u4e3b\u4f53\u8ddd\u79bb\u8f83\u8fdc\u6216\u80cc\u5bf9\u955c\u5934\u65f6\uff09\u4ece2D\u89c2\u6d4b\u4f30\u8ba13D\u89c6\u7ebf\u65b9\u5411\u7684\u6311\u6218\u3002", "method": "\u4f7f\u75283D\u59ff\u52bf\u548c\u7269\u4f53\u4f4d\u7f6e\u8868\u793a\u4e3b\u4f53\u548c\u573a\u666f\uff0c\u63d0\u51faGA3CE\u65b9\u6cd5\uff0c\u7ed3\u5408D$^3$\u4f4d\u7f6e\u7f16\u7801\u6765\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u5355\u5e27\u8bbe\u7f6e\u4e0b\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e8613%-37%\u7684\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee\u3002", "conclusion": "GA3CE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u89c6\u7ebf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "relevance": 30.0}}
{"id": "2505.10936", "pdf": "https://arxiv.org/pdf/2505.10936", "abs": "https://arxiv.org/abs/2505.10936", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "categories": ["cs.CL"], "comment": "34 pages, 20 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nexecuting complex reasoning tasks. Chain-of-thought effectively enhances\nreasoning capabilities by unlocking the potential of large models, while\nmulti-agent systems provide more comprehensive solutions by integrating\ncollective intelligence of multiple agents. However, both approaches face\nsignificant limitations. Single-agent with chain-of-thought, due to the\ninherent complexity of designing cross-domain prompts, faces collaboration\nchallenges. Meanwhile, multi-agent systems consume substantial tokens and\ninevitably dilute the primary problem, which is particularly problematic in\nbusiness workflow tasks. To address these challenges, we propose Cochain, a\ncollaboration prompting framework that effectively solves business workflow\ncollaboration problem by combining knowledge and prompts at a reduced cost.\nSpecifically, we construct an integrated knowledge graph that incorporates\nknowledge from multiple stages. Furthermore, by maintaining and retrieving a\nprompts tree, we can obtain prompt information relevant to other stages of the\nbusiness workflow. We perform extensive evaluations of Cochain across multiple\ndatasets, demonstrating that Cochain outperforms all baselines in both prompt\nengineering and multi-agent LLMs. Additionally, expert evaluation results\nindicate that the use of a small model in combination with Cochain outperforms\nGPT-4.", "AI": {"tldr": "\u63d0\u51faCochain\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u63d0\u793a\u6811\uff0c\u89e3\u51b3\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406LLM\u5728\u4e1a\u52a1\u5de5\u4f5c\u6d41\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548cGPT-4\u3002", "motivation": "\u5355\u4ee3\u7406\u94fe\u5f0f\u601d\u7ef4\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u4e1a\u52a1\u5de5\u4f5c\u6d41\u4e2d\u5b58\u5728\u534f\u4f5c\u548c\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u96c6\u6210\u77e5\u8bc6\u56fe\u8c31\u548c\u63d0\u793a\u6811\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u77e5\u8bc6\uff0c\u4f18\u5316\u534f\u4f5c\u63d0\u793a\u6846\u67b6\u3002", "result": "Cochain\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c0f\u6a21\u578b\u7ed3\u5408Cochain\u751a\u81f3\u4f18\u4e8eGPT-4\u3002", "conclusion": "Cochain\u4e3a\u4e1a\u52a1\u5de5\u4f5c\u6d41\u4e2d\u7684LLM\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.10981", "pdf": "https://arxiv.org/pdf/2505.10981", "abs": "https://arxiv.org/abs/2505.10981", "authors": ["Yexiang Liu", "Zekun Li", "Zhi Fang", "Nan Xu", "Ran He", "Tieniu Tan"], "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 Main", "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a method according to probability theory to quickly\nand accurately predict the scaling performance and select the best strategy\nunder large sampling times without extra resource-intensive inference in\npractice. It can serve as the test-time scaling law for majority voting.\nFurthermore, we introduce two ways derived from our theoretical analysis to\nsignificantly improve the scaling performance. We hope that our research can\npromote to re-examine the role of complicated prompting, unleash the potential\nof simple prompting strategies, and provide new insights for enhancing\ntest-time scaling performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728LLM\u4e2d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\uff08\u5982\u591a\u6570\u6295\u7968\uff09\u4e0b\u4e0d\u540c\u63a8\u7406\u63d0\u793a\u7b56\u7565\u7684\u6027\u80fd\u53d8\u5316\uff0c\u53d1\u73b0\u590d\u6742\u7b56\u7565\u5728\u6269\u5c55\u540e\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u7b80\u5355\u7b56\u7565\uff08\u5982Chain-of-Thought\uff09\u8868\u73b0\u66f4\u597d\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u7406\u8bba\u5206\u6790\u548c\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6269\u5c55\u6027\u80fd\u7684\u4e24\u79cd\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5728LLM\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\uff08\u591a\u6570\u6295\u7968\uff09\u4e0b\uff0c\u4e0d\u540c\u63a8\u7406\u63d0\u793a\u7b56\u7565\u7684\u6027\u80fd\u53d8\u5316\uff0c\u4ee5\u4f18\u5316\u7b56\u7565\u9009\u62e9\u5e76\u63d0\u5347\u6269\u5c55\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u5b9e\u9a8c\uff086 LLM \u00d7 8\u63d0\u793a\u7b56\u7565 \u00d7 6\u57fa\u51c6\uff09\uff0c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u9884\u6d4b\u65b9\u6cd5\u548c\u6539\u8fdb\u7b56\u7565\u3002", "result": "\u590d\u6742\u63d0\u793a\u7b56\u7565\u5728\u6269\u5c55\u540e\u6027\u80fd\u4e0b\u964d\uff0c\u7b80\u5355\u7b56\u7565\u8868\u73b0\u66f4\u4f18\uff1b\u63d0\u51fa\u7684\u9884\u6d4b\u65b9\u6cd5\u80fd\u51c6\u786e\u9009\u62e9\u6700\u4f73\u7b56\u7565\uff0c\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6269\u5c55\u6027\u80fd\u3002", "conclusion": "\u590d\u6742\u63d0\u793a\u7b56\u7565\u5728\u6269\u5c55\u573a\u666f\u4e0b\u672a\u5fc5\u6700\u4f18\uff0c\u7b80\u5355\u7b56\u7565\u6f5c\u529b\u66f4\u5927\uff1b\u7814\u7a76\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.10711", "pdf": "https://arxiv.org/pdf/2505.10711", "abs": "https://arxiv.org/abs/2505.10711", "authors": ["Sebesty\u00e9n Kamp", "Giovanni Stracquadanio", "T. Ian Simpson"], "title": "GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics", "categories": ["cs.LG", "cs.AI", "J.3; I.2.1"], "comment": "Main article 8 pages (20 in total with supplementary information\n  included), 3 main article figures and 3 supplemental figures", "summary": "We present GNN-Suite, a robust modular framework for constructing and\nbenchmarking Graph Neural Network (GNN) architectures in computational biology.\nGNN-Suite standardises experimentation and reproducibility using the Nextflow\nworkflow to evaluate GNN performance. We demonstrate its utility in identifying\ncancer-driver genes by constructing molecular networks from protein-protein\ninteraction (PPI) data from STRING and BioGRID and annotating nodes with\nfeatures from the PCAWG, PID, and COSMIC-CGC repositories.\n  Our design enables fair comparisons among diverse GNN architectures including\nGAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline\nLogistic Regression (LR) model. All GNNs were configured as standardised\ntwo-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam\noptimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss\nto address class imbalance) over an 80/20 train-test split for 300 epochs. Each\nmodel was evaluated over 10 independent runs with different random seeds to\nyield statistically robust performance metrics, with balanced accuracy (BACC)\nas the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-\n0.035) on a STRING-based network, although all GNN types outperformed the LR\nbaseline, highlighting the advantage of network-based learning over\nfeature-only approaches.\n  Our results show that a common framework for implementing and evaluating GNN\narchitectures aids in identifying not only the best model but also the most\neffective means of incorporating complementary data. By making GNN-Suite\npublicly available, we aim to foster reproducible research and promote improved\nbenchmarking standards in computational biology. Future work will explore\nadditional omics datasets and further refine network architectures to enhance\npredictive accuracy and interpretability in biomedical applications.", "AI": {"tldr": "GNN-Suite\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u8bc4\u4f30\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684\u6027\u80fd\uff0c\u652f\u6301\u6807\u51c6\u5316\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u6027\u7814\u7a76\u3002", "motivation": "\u4e3a\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684GNN\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u4ee5\u6807\u51c6\u5316\u5b9e\u9a8c\u548c\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002", "method": "\u4f7f\u7528Nextflow\u5de5\u4f5c\u6d41\u8bc4\u4f30GNN\u6027\u80fd\uff0c\u6bd4\u8f83\u591a\u79cdGNN\u67b6\u6784\uff08\u5982GAT\u3001GCN\u7b49\uff09\u548c\u903b\u8f91\u56de\u5f52\u57fa\u7ebf\u6a21\u578b\uff0c\u901a\u8fc710\u6b21\u72ec\u7acb\u8fd0\u884c\u83b7\u5f97\u7a33\u5065\u6027\u80fd\u6307\u6807\u3002", "result": "GCN2\u5728STRING\u7f51\u7edc\u4e0a\u7684\u5e73\u8861\u51c6\u786e\u7387\u6700\u9ad8\uff080.807\uff09\uff0c\u6240\u6709GNN\u5747\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u57fa\u7ebf\u3002", "conclusion": "GNN-Suite\u6709\u52a9\u4e8e\u8bc6\u522b\u6700\u4f73\u6a21\u578b\u548c\u6570\u636e\u6574\u5408\u65b9\u5f0f\uff0c\u672a\u6765\u5c06\u6269\u5c55\u6570\u636e\u96c6\u548c\u4f18\u5316\u67b6\u6784\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10679", "pdf": "https://arxiv.org/pdf/2505.10679", "abs": "https://arxiv.org/abs/2505.10679", "authors": ["Jianyang Xie", "Yitian Zhao", "Yanda Meng", "He Zhao", "Anh Nguyen", "Yalin Zheng"], "title": "Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?", "categories": ["cs.CV"], "comment": null, "summary": "Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive\nperformance in skeleton-based human action recognition (HAR). However, despite\nthe development of numerous models, their recognition performance does not\ndiffer significantly after aligning the input settings. With this observation,\nwe hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture\nsubsequently confirmed through experiments employing the lottery ticket\nhypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which\ntrains a sparse architecture from a randomly initialized dense network while\nmaintaining comparable performance levels to the dense components. Moreover, we\ngenerate multi-level sparsity ST-GCNs by integrating sparse structures at\nvarious sparsity levels and demonstrate that the assembled model yields a\nsignificant enhancement in HAR performance. Thorough experiments on four\ndatasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate\nthat the proposed sparse ST-GCNs can achieve comparable performance to their\ndense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a\ndegradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity\nST-GCNs, which require only 66% of the parameters of the dense ST-GCNs,\ndemonstrate an improvement of >1% in top-1 accuracy. The code is available at\nhttps://github.com/davelailai/Sparse-ST-GCN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\uff08ST-GCNs\uff09\u751f\u6210\u5668\uff0c\u901a\u8fc7\u5f69\u7968\u5047\u8bbe\u9a8c\u8bc1ST-GCNs\u5728\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u8fc7\u53c2\u6570\u5316\uff0c\u5e76\u5c55\u793a\u4e86\u7a00\u758f\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u3002", "motivation": "\u73b0\u6709ST-GCNs\u5728\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u6027\u80fd\u5dee\u5f02\u4e0d\u5927\uff0c\u8868\u660e\u5176\u53ef\u80fd\u8fc7\u53c2\u6570\u5316\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u7a00\u758f\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u7a00\u758fST-GCNs\u751f\u6210\u5668\uff0c\u4ece\u968f\u673a\u521d\u59cb\u5316\u7684\u5bc6\u96c6\u7f51\u7edc\u8bad\u7ec3\u7a00\u758f\u67b6\u6784\uff0c\u5e76\u96c6\u6210\u591a\u7ea7\u7a00\u758f\u7ed3\u6784\u3002", "result": "\u7a00\u758fST-GCNs\u5728\u51cf\u5c1195%\u53c2\u6570\u65f6\u6027\u80fd\u4e0b\u964d<1%\uff0c\u591a\u7ea7\u7a00\u758f\u6a21\u578b\u4ec5\u970066%\u53c2\u6570\u4e14\u6027\u80fd\u63d0\u5347>1%\u3002", "conclusion": "\u7a00\u758fST-GCNs\u5728\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u5bc6\u96c6\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u8fc7\u53c2\u6570\u5316\u5047\u8bbe\u3002", "relevance": 40.0}}
{"id": "2505.10937", "pdf": "https://arxiv.org/pdf/2505.10937", "abs": "https://arxiv.org/abs/2505.10937", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of large reasoning models (LRMs) has transformed Natural\nLanguage Processing by excelling in complex tasks such as mathematical\nproblem-solving and code generation. These models leverage chain-of-thought\n(CoT) processes, enabling them to emulate human-like reasoning strategies.\nHowever, the advancement of LRMs is hindered by the lack of comprehensive CoT\ndatasets. Current resources often fail to provide extensive reasoning problems\nwith coherent CoT processes distilled from multiple teacher models and do not\naccount for multifaceted properties describing the internal characteristics of\nCoTs. To address these challenges, we introduce OmniThought, a large-scale\ndataset featuring 2 million CoT processes generated and validated by two\npowerful LRMs as teacher models. Each CoT process in OmniThought is annotated\nwith novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which\ndescribe the appropriateness of CoT verbosity and cognitive difficulty level\nfor models to comprehend these reasoning processes. We further establish a\nself-reliant pipeline to curate this dataset. Extensive experiments using\nQwen2.5 models of various sizes demonstrate the positive impact of our proposed\nscores on LRM training effectiveness. Based on the proposed OmniThought\ndataset, we further train and release a series of high-performing LRMs,\nspecifically equipped with stronger reasoning abilities and optimal CoT output\nlength and difficulty level. Our contributions significantly enhance the\ndevelopment and training of LRMs for solving complex tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OmniThought\u6570\u636e\u96c6\uff0c\u5305\u542b200\u4e07\u6761\u7531\u4e24\u4e2a\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u8fc7\u7a0b\uff0c\u5e76\u6807\u6ce8\u4e86\u65b0\u9896\u7684\u63a8\u7406\u5197\u957f\u5ea6\uff08RV\uff09\u548c\u8ba4\u77e5\u96be\u5ea6\uff08CD\uff09\u5206\u6570\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5168\u9762\u7684CoT\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u3002\u73b0\u6709\u8d44\u6e90\u672a\u80fd\u63d0\u4f9b\u591a\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u5e7f\u6cdb\u63a8\u7406\u95ee\u9898\u53ca\u5176\u5185\u90e8\u7279\u6027\u63cf\u8ff0\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5f3a\u5927\u7684LRM\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u751f\u6210\u5e76\u9a8c\u8bc1CoT\u8fc7\u7a0b\uff0c\u6807\u6ce8RV\u548cCD\u5206\u6570\uff0c\u5efa\u7acb\u81ea\u7ed9\u81ea\u8db3\u7684\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u5206\u6570\u5bf9LRM\u8bad\u7ec3\u6548\u679c\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u5e76\u8bad\u7ec3\u51fa\u4e00\u7cfb\u5217\u9ad8\u6027\u80fdLRM\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f18\u5316\u7684CoT\u8f93\u51fa\u3002", "conclusion": "OmniThought\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86LRM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5f00\u53d1\u548c\u8bad\u7ec3\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10982", "pdf": "https://arxiv.org/pdf/2505.10982", "abs": "https://arxiv.org/abs/2505.10982", "authors": ["Johannes Fichte", "Nicolas Fr\u00f6hlich", "Markus Hecher", "Victor Lagerkvist", "Yasir Mahmood", "Arne Meier", "Jonathan Persson"], "title": "Facets in Argumentation: A Formal Approach to Argument Significance", "categories": ["cs.AI"], "comment": null, "summary": "Argumentation is a central subarea of Artificial Intelligence (AI) for\nmodeling and reasoning about arguments. The semantics of abstract argumentation\nframeworks (AFs) is given by sets of arguments (extensions) and conditions on\nthe relationship between them, such as stable or admissible. Today's solvers\nimplement tasks such as finding extensions, deciding credulous or skeptical\nacceptance, counting, or enumerating extensions. While these tasks are well\ncharted, the area between decision, counting/enumeration and fine-grained\nreasoning requires expensive reasoning so far. We introduce a novel concept\n(facets) for reasoning between decision and enumeration. Facets are arguments\nthat belong to some extensions (credulous) but not to all extensions\n(skeptical). They are most natural when a user aims to navigate, filter, or\ncomprehend the significance of specific arguments, according to their needs. We\nstudy the complexity and show that tasks involving facets are much easier than\ncounting extensions. Finally, we provide an implementation, and conduct\nexperiments to demonstrate feasibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\uff08facets\uff09\uff0c\u7528\u4e8e\u5728\u51b3\u7b56\u548c\u679a\u4e3e\u4e4b\u95f4\u8fdb\u884c\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u4e2d\u7cbe\u7ec6\u63a8\u7406\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u4e2d\u7684\u7cbe\u7ec6\u63a8\u7406\u4efb\u52a1\uff08\u5982\u51b3\u7b56\u3001\u8ba1\u6570/\u679a\u4e3e\uff09\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165facets\u6982\u5ff5\uff0c\u7814\u7a76\u5176\u590d\u6742\u6027\uff0c\u5e76\u63d0\u4f9b\u5b9e\u73b0\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660efacets\u76f8\u5173\u4efb\u52a1\u6bd4\u8ba1\u6570\u6269\u5c55\u66f4\u5bb9\u6613\uff0c\u4e14\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "facets\u4e3a\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u4e2d\u7684\u7cbe\u7ec6\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.10726", "pdf": "https://arxiv.org/pdf/2505.10726", "abs": "https://arxiv.org/abs/2505.10726", "authors": ["Yihan Zhu", "Gang Liu", "Eric Inae", "Tengfei Luo", "Meng Jiang"], "title": "Learning Repetition-Invariant Representations for Polymer Informatics", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages,3 figuares", "summary": "Polymers are large macromolecules composed of repeating structural units\nknown as monomers and are widely applied in fields such as energy storage,\nconstruction, medicine, and aerospace. However, existing graph neural network\nmethods, though effective for small molecules, only model the single unit of\npolymers and fail to produce consistent vector representations for the true\npolymer structure with varying numbers of units. To address this challenge, we\nintroduce Graph Repetition Invariance (GRIN), a novel method to learn polymer\nrepresentations that are invariant to the number of repeating units in their\ngraph representations. GRIN integrates a graph-based maximum spanning tree\nalignment with repeat-unit augmentation to ensure structural consistency. We\nprovide theoretical guarantees for repetition-invariance from both model and\ndata perspectives, demonstrating that three repeating units are the minimal\naugmentation required for optimal invariant representation learning. GRIN\noutperforms state-of-the-art baselines on both homopolymer and copolymer\nbenchmarks, learning stable, repetition-invariant representations that\ngeneralize effectively to polymer chains of unseen sizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Graph Repetition Invariance (GRIN)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u805a\u5408\u7269\u91cd\u590d\u5355\u5143\u4e0d\u53d8\u7684\u56fe\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u805a\u5408\u7269\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u4ec5\u80fd\u5efa\u6a21\u805a\u5408\u7269\u7684\u5355\u4e2a\u5355\u5143\uff0c\u65e0\u6cd5\u4e3a\u4e0d\u540c\u91cd\u590d\u5355\u5143\u6570\u91cf\u7684\u771f\u5b9e\u805a\u5408\u7269\u7ed3\u6784\u751f\u6210\u4e00\u81f4\u7684\u5411\u91cf\u8868\u793a\u3002", "method": "GRIN\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u7684\u6700\u5927\u751f\u6210\u6811\u5bf9\u9f50\u548c\u91cd\u590d\u5355\u5143\u589e\u5f3a\uff0c\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u4ece\u6a21\u578b\u548c\u6570\u636e\u89d2\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "GRIN\u5728\u5747\u805a\u7269\u548c\u5171\u805a\u7269\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b66\u4e60\u5230\u7a33\u5b9a\u4e14\u91cd\u590d\u5355\u5143\u4e0d\u53d8\u7684\u8868\u793a\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u805a\u5408\u7269\u94fe\u5927\u5c0f\u3002", "conclusion": "GRIN\u662f\u4e00\u79cd\u6709\u6548\u7684\u805a\u5408\u7269\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002", "relevance": 20.0}}
{"id": "2505.10685", "pdf": "https://arxiv.org/pdf/2505.10685", "abs": "https://arxiv.org/abs/2505.10685", "authors": ["Lingjun Zhao", "Sizhe Wei", "James Hays", "Lu Gan"], "title": "GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction is critical for achieving safe and reliable\nautonomous driving. Compared to camera-only perception systems, multi-modal\npipelines, especially LiDAR-camera fusion methods, can produce more accurate\nand detailed predictions. Although most existing works utilize a dense\ngrid-based representation, in which the entire 3D space is uniformly divided\ninto discrete voxels, the emergence of 3D Gaussians provides a compact and\ncontinuous object-centric representation. In this work, we propose a\nmulti-modal Gaussian-based semantic occupancy prediction framework utilizing 3D\ndeformable attention, named as GaussianFormer3D. We introduce a\nvoxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry\npriors from LiDAR data, and design a LiDAR-guided 3D deformable attention\nmechanism for refining 3D Gaussians with LiDAR-camera fusion features in a\nlifted 3D space. We conducted extensive experiments on both on-road and\noff-road datasets, demonstrating that our GaussianFormer3D achieves high\nprediction accuracy that is comparable to state-of-the-art multi-modal\nfusion-based methods with reduced memory consumption and improved efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u7684\u591a\u6a21\u6001\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u6846\u67b6GaussianFormer3D\uff0c\u7ed3\u5408LiDAR\u548c\u76f8\u673a\u6570\u636e\uff0c\u901a\u8fc73D\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5bc6\u96c6\u7f51\u683c\u8868\u793a\uff0c\u800c3D\u9ad8\u65af\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u4e14\u8fde\u7eed\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u65e8\u5728\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528LiDAR\u6570\u636e\u521d\u59cb\u53163D\u9ad8\u65af\uff0c\u8bbe\u8ba1LiDAR\u5f15\u5bfc\u76843D\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408LiDAR\u548c\u76f8\u673a\u7279\u5f81\u4f18\u5316\u9ad8\u65af\u8868\u793a\u3002", "result": "\u5728\u9053\u8def\u548c\u975e\u9053\u8def\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cGaussianFormer3D\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u4e86\u6548\u7387\u3002", "conclusion": "GaussianFormer3D\u4e3a\u591a\u6a21\u6001\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.10938", "pdf": "https://arxiv.org/pdf/2505.10938", "abs": "https://arxiv.org/abs/2505.10938", "authors": ["Yi Su", "Yuechi Zhou", "Quantong Qiu", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing", "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9KV Cache\u91cf\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u6392\u9664\u5f02\u5e38token\uff0c\u663e\u8457\u63d0\u9ad8\u4e862-bit\u91cf\u5316\u4e0b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "LLMs\u5728\u90e8\u7f72\u65f6\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0cKV Cache\u867d\u80fd\u51cf\u5c11\u63a8\u7406\u65f6\u7684\u91cd\u8ba1\u7b97\uff0c\u4f46\u589e\u52a0\u4e86\u5185\u5b58\u5f00\u9500\u3002\u91cf\u5316KV Cache\u662f\u5e73\u8861\u5185\u5b58\u548c\u51c6\u786e\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u5f02\u5e38token\u7684\u5904\u7406\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u53d1\u73b0Keys\u548cValues\u7684\u5206\u5e03\u6a21\u5f0f\u4e0d\u540c\uff0c\u4f46\u5f02\u5e38token\u4f1a\u7834\u574f\u8fd9\u79cd\u6a21\u5f0f\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u8bc6\u522b\u5e76\u6392\u9664\u8fd9\u4e9b\u5f02\u5e38token\uff0c\u907f\u514d\u5176\u5bf9\u91cf\u5316\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57282-bit\u91cf\u5316\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e866.4\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u4e862.3\u500d\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u548c\u5904\u7406\u5f02\u5e38token\uff0c\u53ef\u4ee5\u663e\u8457\u4f18\u5316KV Cache\u91cf\u5316\u7684\u6548\u679c\uff0c\u4e3aLLMs\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.10988", "pdf": "https://arxiv.org/pdf/2505.10988", "abs": "https://arxiv.org/abs/2505.10988", "authors": ["Joon-Young Kim", "Jecheon Yu", "Heekyu Kim", "Seunghwa Ryu"], "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "50 pages, 10 figures", "summary": "Plastic injection molding remains essential to modern manufacturing. However,\noptimizing process parameters to balance product quality and profitability\nunder dynamic environmental and economic conditions remains a persistent\nchallenge. This study presents a novel deep reinforcement learning (DRL)-based\nframework for real-time process optimization in injection molding, integrating\nproduct quality and profitability into the control objective. A profit function\nwas developed to reflect real-world manufacturing costs, incorporating resin,\nmold wear, and electricity prices, including time-of-use variations. Surrogate\nmodels were constructed to predict product quality and cycle time, enabling\nefficient offline training of DRL agents using soft actor-critic (SAC) and\nproximal policy optimization (PPO) algorithms. Experimental results demonstrate\nthat the proposed DRL framework can dynamically adapt to seasonal and\noperational variations, consistently maintaining product quality while\nmaximizing profit. Compared to traditional optimization methods such as genetic\nalgorithms, the DRL models achieved comparable economic performance with up to\n135x faster inference speeds, making them well-suited for real-time\napplications. The framework's scalability and adaptability highlight its\npotential as a foundation for intelligent, data-driven decision-making in\nmodern manufacturing environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u5b9e\u65f6\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6ce8\u5851\u6210\u578b\u8fc7\u7a0b\uff0c\u5e73\u8861\u4ea7\u54c1\u8d28\u91cf\u4e0e\u5229\u6da6\u3002", "motivation": "\u6ce8\u5851\u6210\u578b\u4e2d\u52a8\u6001\u4f18\u5316\u8fc7\u7a0b\u53c2\u6570\u4ee5\u5e73\u8861\u8d28\u91cf\u4e0e\u5229\u6da6\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u5229\u6da6\u51fd\u6570\uff0c\u6784\u5efa\u66ff\u4ee3\u6a21\u578b\u9884\u6d4b\u8d28\u91cf\u548c\u5468\u671f\u65f6\u95f4\uff0c\u4f7f\u7528SAC\u548cPPO\u7b97\u6cd5\u79bb\u7ebf\u8bad\u7ec3DRL\u4ee3\u7406\u3002", "result": "DRL\u6846\u67b6\u80fd\u52a8\u6001\u9002\u5e94\u53d8\u5316\uff0c\u4fdd\u6301\u8d28\u91cf\u540c\u65f6\u6700\u5927\u5316\u5229\u6da6\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb135\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u4ee3\u5236\u9020\u4e1a\u667a\u80fd\u51b3\u7b56\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.10759", "pdf": "https://arxiv.org/pdf/2505.10759", "abs": "https://arxiv.org/abs/2505.10759", "authors": ["Achmad Ginanjar", "Xue Li", "Priyanka Singh", "Wen Hua"], "title": "Random Client Selection on Contrastive Federated Learning for Tabular Data", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": null, "summary": "Vertical Federated Learning (VFL) has revolutionised collaborative machine\nlearning by enabling privacy-preserving model training across multiple parties.\nHowever, it remains vulnerable to information leakage during intermediate\ncomputation sharing. While Contrastive Federated Learning (CFL) was introduced\nto mitigate these privacy concerns through representation learning, it still\nfaces challenges from gradient-based attacks. This paper presents a\ncomprehensive experimental analysis of gradient-based attacks in CFL\nenvironments and evaluates random client selection as a defensive strategy.\nThrough extensive experimentation, we demonstrate that random client selection\nproves particularly effective in defending against gradient attacks in the CFL\nnetwork. Our findings provide valuable insights for implementing robust\nsecurity measures in contrastive federated learning systems, contributing to\nthe development of more secure collaborative learning frameworks", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u6bd4\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u4e2d\u7684\u68af\u5ea6\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u968f\u673a\u5ba2\u6237\u7aef\u9009\u62e9\u4f5c\u4e3a\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "VFL\u867d\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5b66\u4e60\uff0c\u4f46\u4e2d\u95f4\u8ba1\u7b97\u5171\u4eab\u6613\u5bfc\u81f4\u4fe1\u606f\u6cc4\u9732\u3002CFL\u867d\u901a\u8fc7\u8868\u793a\u5b66\u4e60\u7f13\u89e3\u9690\u79c1\u95ee\u9898\uff0c\u4f46\u4ecd\u9762\u4e34\u68af\u5ea6\u653b\u51fb\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790CFL\u4e2d\u7684\u68af\u5ea6\u653b\u51fb\uff0c\u5e76\u8bc4\u4f30\u968f\u673a\u5ba2\u6237\u7aef\u9009\u62e9\u4f5c\u4e3a\u9632\u5fa1\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u968f\u673a\u5ba2\u6237\u7aef\u9009\u62e9\u5728CFL\u7f51\u7edc\u4e2d\u80fd\u6709\u6548\u9632\u5fa1\u68af\u5ea6\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aCFL\u7cfb\u7edf\u5b9e\u65bd\u66f4\u5b89\u5168\u7684\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "relevance": 30.0}}
{"id": "2505.10737", "pdf": "https://arxiv.org/pdf/2505.10737", "abs": "https://arxiv.org/abs/2505.10737", "authors": ["Mitchell Rogers", "Theo Thompson", "Isla Duporge", "Johannes Fischer", "Klemens P\u00fctz", "Thomas Mattern", "Bing Xue", "Mengjie Zhang"], "title": "Automated Detection of Salvin's Albatrosses: Improving Deep Learning Tools for Aerial Wildlife Surveys", "categories": ["cs.CV"], "comment": "Accepted to the CV4Animals workshop at CVPR 2025", "summary": "Recent advancements in deep learning and aerial imaging have transformed\nwildlife monitoring, enabling researchers to survey wildlife populations at\nunprecedented scales. Unmanned Aerial Vehicles (UAVs) provide a cost-effective\nmeans of capturing high-resolution imagery, particularly for monitoring densely\npopulated seabird colonies. In this study, we assess the performance of a\ngeneral-purpose avian detection model, BirdDetector, in estimating the breeding\npopulation of Salvin's albatross (Thalassarche salvini) on the Bounty Islands,\nNew Zealand. Using drone-derived imagery, we evaluate the model's effectiveness\nin both zero-shot and fine-tuned settings, incorporating enhanced inference\ntechniques and stronger augmentation methods. Our findings indicate that while\napplying the model in a zero-shot setting offers a strong baseline, fine-tuning\nwith annotations from the target domain and stronger image augmentation leads\nto marked improvements in detection accuracy. These results highlight the\npotential of leveraging pre-trained deep-learning models for species-specific\nmonitoring in remote and challenging environments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u901a\u7528\u9e1f\u7c7b\u68c0\u6d4b\u6a21\u578bBirdDetector\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u65e0\u4eba\u673a\u5f71\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6539\u8fdb\u504f\u8fdc\u5730\u533a\u91ce\u751f\u52a8\u7269\u7684\u76d1\u6d4b\u6548\u7387\u3002", "method": "\u4f7f\u7528\u65e0\u4eba\u673a\u5f71\u50cf\uff0c\u8bc4\u4f30BirdDetector\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u7ed3\u5408\u589e\u5f3a\u63a8\u7406\u548c\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u3002", "result": "\u96f6\u6837\u672c\u8bbe\u7f6e\u63d0\u4f9b\u57fa\u7ebf\uff0c\u5fae\u8c03\u540e\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7279\u5b9a\u7269\u79cd\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "relevance": 20.0}}
{"id": "2505.10939", "pdf": "https://arxiv.org/pdf/2505.10939", "abs": "https://arxiv.org/abs/2505.10939", "authors": ["Mohammadtaha Bagherifard", "Sahar Rajabi", "Ali Edalat", "Yadollah Yaghoobzadeh"], "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages", "summary": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6GenKnowSub\uff0c\u901a\u8fc7\u5206\u79bb\u901a\u7528\u77e5\u8bc6\u548c\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u4e2d\u901a\u7528\u77e5\u8bc6\u4e0e\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4efb\u52a1\u7279\u5b9aLoRA\u6a21\u5757\u5e93\u548c\u901a\u7528LoRA\u6a21\u5757\uff0c\u901a\u8fc7\u51cf\u6cd5\u5206\u79bb\u4efb\u52a1\u76f8\u5173\u6a21\u5757\uff0c\u52a8\u6001\u7ec4\u5408\u6a21\u5757\u3002", "result": "\u5728Phi-3\u548cPhi-2\u6a21\u578b\u4e0a\uff0c\u8de8\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "GenKnowSub\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u3002", "relevance": 90.0}}
{"id": "2505.10989", "pdf": "https://arxiv.org/pdf/2505.10989", "abs": "https://arxiv.org/abs/2505.10989", "authors": ["Haiyang Shen", "Hang Yan", "Zhongshi Xing", "Mugeng Liu", "Yue Li", "Zhiyang Chen", "Yuxiang Wang", "Jiuzheng Wang", "Yun Ma"], "title": "RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization", "categories": ["cs.AI"], "comment": null, "summary": "RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\nupon 2 cores: the retriever, which should robustly select relevant documents\nacross complex queries, and the generator, which should faithfully synthesize\nresponses. However, existing retrievers rely heavily on public knowledge and\nstruggle with queries of varying logical complexity and clue completeness,\nwhile generators frequently face fidelity problems. In this work, we introduce\nRAGSynth, a framework that includes a data construction modeling and a\ncorresponding synthetic data generation implementation, designed to optimize\nretriever robustness and generator fidelity. Additionally, we present\nSynthBench, a benchmark encompassing 8 domain-specific documents across 4\ndomains, featuring diverse query complexities, clue completeness, and\nfine-grained citation granularity. Leveraging RAGSynth, we generate a\nlarge-scale synthetic dataset, including single and multi-hop. Extensive\nexperiments demonstrate that the synthetic data significantly improves the\nrobustness of the retrievers and the fidelity of the generators. Additional\nevaluations confirm that RAGSynth can also generalize well across different\ndomains. By integrating the optimized retrievers into various RAG paradigms, we\nconsistently observe enhanced RAG system performance. We have open-sourced the\nimplementation on https://github.com/EachSheep/RAGSynth.", "AI": {"tldr": "RAGSynth\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u4f18\u5316\u68c0\u7d22\u5668\u7684\u9c81\u68d2\u6027\u548c\u751f\u6210\u5668\u7684\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u68c0\u7d22\u5668\u4f9d\u8d56\u516c\u5171\u77e5\u8bc6\u4e14\u5bf9\u590d\u6742\u67e5\u8be2\u8868\u73b0\u4e0d\u4f73\uff0c\u751f\u6210\u5668\u4fdd\u771f\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faRAGSynth\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u6784\u9020\u5efa\u6a21\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5e76\u5f15\u5165SynthBench\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u68c0\u7d22\u5668\u9c81\u68d2\u6027\u548c\u751f\u6210\u5668\u4fdd\u771f\u5ea6\uff0cRAGSynth\u5728\u4e0d\u540c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "RAGSynth\u901a\u8fc7\u4f18\u5316\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\uff0c\u6709\u6548\u63d0\u5347RAG\u7cfb\u7edf\u6027\u80fd\uff0c\u4e14\u5df2\u5f00\u6e90\u3002", "relevance": 85.0}}
{"id": "2505.10762", "pdf": "https://arxiv.org/pdf/2505.10762", "abs": "https://arxiv.org/abs/2505.10762", "authors": ["Conor F. Hayes", "Felipe Leno Da Silva", "Jiachen Yang", "T. Nathan Mundhenk", "Chak Shing Lee", "Jacob F. Pettit", "Claudio Santiago", "Sookyung Kim", "Joanne T. Kim", "Ignacio Aravena Solis", "Ruben Glatt", "Andre R. Goncalves", "Alexander Ladd", "Ahmet Can Solak", "Thomas Desautels", "Daniel Faissol", "Brenden K. Petersen", "Mikel Landajuela"], "title": "Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics", "categories": ["cs.LG", "cs.NE", "cs.SC"], "comment": "Under review in LNCS Computational Approaches to Scientific Discovery", "summary": "Deep Symbolic Optimization (DSO) is a novel computational framework that\nenables symbolic optimization for scientific discovery, particularly in\napplications involving the search for intricate symbolic structures. One\nnotable example is equation discovery, which aims to automatically derive\nmathematical models expressed in symbolic form. In DSO, the discovery process\nis formulated as a sequential decision-making task. A generative neural network\nlearns a probabilistic model over a vast space of candidate symbolic\nexpressions, while reinforcement learning strategies guide the search toward\nthe most promising regions. This approach integrates gradient-based\noptimization with evolutionary and local search techniques, and it incorporates\nin-situ constraints, domain-specific priors, and advanced policy optimization\nmethods. The result is a robust framework capable of efficiently exploring\nextensive search spaces to identify interpretable and physically meaningful\nmodels. Extensive evaluations on benchmark problems have demonstrated that DSO\nachieves state-of-the-art performance in both accuracy and interpretability. In\nthis chapter, we provide a comprehensive overview of the DSO framework and\nillustrate its transformative potential for automating symbolic optimization in\nscientific discovery.", "AI": {"tldr": "Deep Symbolic Optimization (DSO) \u662f\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u7b26\u53f7\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u641c\u7d22\u590d\u6742\u7b26\u53f7\u7ed3\u6784\u7684\u5e94\u7528\u3002\u5b83\u7ed3\u5408\u4e86\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u3001\u8fdb\u5316\u641c\u7d22\u548c\u5c40\u90e8\u641c\u7d22\u6280\u672f\uff0c\u9ad8\u6548\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\uff0c\u627e\u5230\u53ef\u89e3\u91ca\u4e14\u7269\u7406\u610f\u4e49\u660e\u786e\u7684\u6a21\u578b\u3002", "motivation": "DSO \u7684\u52a8\u673a\u662f\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u7b26\u53f7\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u65b9\u7a0b\u53d1\u73b0\u7b49\u5e94\u7528\u4e2d\uff0c\u81ea\u52a8\u63a8\u5bfc\u7b26\u53f7\u5f62\u5f0f\u7684\u6570\u5b66\u6a21\u578b\u3002", "method": "DSO \u5c06\u53d1\u73b0\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\uff0c\u4f7f\u7528\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5019\u9009\u7b26\u53f7\u8868\u8fbe\u5f0f\u7684\u6982\u7387\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5f15\u5bfc\u641c\u7d22\u3002\u65b9\u6cd5\u7ed3\u5408\u4e86\u68af\u5ea6\u4f18\u5316\u3001\u8fdb\u5316\u641c\u7d22\u3001\u5c40\u90e8\u641c\u7d22\u548c\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cDSO \u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DSO \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u63a2\u7d22\u7b26\u53f7\u7a7a\u95f4\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u81ea\u52a8\u5316\u7b26\u53f7\u4f18\u5316\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.10743", "pdf": "https://arxiv.org/pdf/2505.10743", "abs": "https://arxiv.org/abs/2505.10743", "authors": ["Amritanshu Tiwari", "Cherish Puniani", "Kaustubh Sharma", "Ojasva Nema"], "title": "IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Recent advances in text-to-image diffusion models, particularly Stable\nDiffusion, have enabled the generation of highly detailed and semantically rich\nimages. However, personalizing these models to represent novel subjects based\non a few reference images remains challenging. This often leads to catastrophic\nforgetting, overfitting, or large computational overhead.We propose a two-stage\npipeline that addresses these limitations by leveraging LoRA-based fine-tuning\non the attention weights within the U-Net of the Stable Diffusion XL (SDXL)\nmodel. First, we use the unmodified SDXL to generate a generic scene by\nreplacing the subject with its class label. Then, we selectively insert the\npersonalized subject through a segmentation-driven image-to-image (Img2Img)\npipeline that uses the trained LoRA weights.This framework isolates the subject\nencoding from the overall composition, thus preserving SDXL's broader\ngenerative capabilities while integrating the new subject in a high-fidelity\nmanner. Our method achieves a DINO similarity score of 0.789 on SDXL,\noutperforming existing personalized text-to-image approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728Stable Diffusion XL\u4e2d\u4e2a\u6027\u5316\u751f\u6210\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u4e2a\u6027\u5316\u751f\u6210\u65b0\u4e3b\u9898\u65f6\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u3001\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u4f7f\u7528\u672a\u4fee\u6539\u7684SDXL\u751f\u6210\u901a\u7528\u573a\u666f\uff1b2) \u901a\u8fc7\u5206\u5272\u9a71\u52a8\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u6d41\u7a0b\u9009\u62e9\u6027\u63d2\u5165\u4e2a\u6027\u5316\u4e3b\u9898\u3002", "result": "\u5728SDXL\u4e0a\u5b9e\u73b0\u4e860.789\u7684DINO\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559SDXL\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e2a\u6027\u5316\u4e3b\u9898\u751f\u6210\u3002", "relevance": 40.0}}
{"id": "2505.10945", "pdf": "https://arxiv.org/pdf/2505.10945", "abs": "https://arxiv.org/abs/2505.10945", "authors": ["Seungyoon Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u6280\u672fSALT\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u5230\u76ee\u6807\u8bed\u8a00\u65f6\uff0c\u53ef\u80fd\u56e0\u6e90\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u8bad\u7ec3\u800c\u9650\u5236\u76ee\u6807\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "SALT\u901a\u8fc7\u56de\u5f52\u7ebf\u5904\u7406\u6e90\u548c\u76ee\u6807\u8bcd\u6c47\u8868\u7684\u91cd\u53e0\u4e0e\u975e\u91cd\u53e0\u90e8\u5206\uff0c\u5229\u7528\u76ee\u6807\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u4f20\u9012\u6df1\u5c42\u8868\u793a\u80fd\u529b\u3002", "result": "SALT\u5728\u8de8\u8bed\u8a00\u7406\u89e3\u548c\u8bed\u8a00\u9002\u5e94\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u635f\u5931\u66f4\u4f4e\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "SALT\u4e3a\u589e\u5f3a\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u529f\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.10991", "pdf": "https://arxiv.org/pdf/2505.10991", "abs": "https://arxiv.org/abs/2505.10991", "authors": ["Yacine Izza", "Alexey Ignatiev", "Joao Marques-Silva", "Peter J. Stuckey"], "title": "Most General Explanations of Tree Ensembles", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": "Restricted version of this paper was accepted at IJCAI 2025", "summary": "Explainable Artificial Intelligence (XAI) is critical for attaining trust in\nthe operation of AI systems. A key question of an AI system is ``why was this\ndecision made this way''. Formal approaches to XAI use a formal model of the AI\nsystem to identify abductive explanations. While abductive explanations may be\napplicable to a large number of inputs sharing the same concrete values, more\ngeneral explanations may be preferred for numeric inputs. So-called inflated\nabductive explanations give intervals for each feature ensuring that any input\nwhose values fall withing these intervals is still guaranteed to make the same\nprediction. Inflated explanations cover a larger portion of the input space,\nand hence are deemed more general explanations. But there can be many\n(inflated) abductive explanations for an instance. Which is the best? In this\npaper, we show how to find a most general abductive explanation for an AI\ndecision. This explanation covers as much of the input space as possible, while\nstill being a correct formal explanation of the model's behaviour. Given that\nwe only want to give a human one explanation for a decision, the most general\nexplanation gives us the explanation with the broadest applicability, and hence\nthe one most likely to seem sensible. (The paper has been accepted at IJCAI2025\nconference.)", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bfb\u627e\u6700\u901a\u7528\u7684\u6eaf\u56e0\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91caAI\u51b3\u7b56\uff0c\u8986\u76d6\u5c3d\u53ef\u80fd\u591a\u7684\u8f93\u5165\u7a7a\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u89e3\u91ca\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff08XAI\uff09\uff0c\u89e3\u51b3\u5982\u4f55\u4e3a\u51b3\u7b56\u63d0\u4f9b\u6700\u901a\u7528\u4e14\u6b63\u786e\u7684\u89e3\u91ca\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u6eaf\u56e0\u89e3\u91ca\u548c\u533a\u95f4\u6269\u5c55\uff08inflated explanations\uff09\u6765\u8986\u76d6\u66f4\u591a\u8f93\u5165\u7a7a\u95f4\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u80fd\u591f\u627e\u5230\u8986\u76d6\u6700\u5927\u8f93\u5165\u7a7a\u95f4\u7684\u6700\u901a\u7528\u6eaf\u56e0\u89e3\u91ca\u3002", "conclusion": "\u6700\u901a\u7528\u7684\u89e3\u91ca\u5177\u6709\u6700\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u66f4\u53ef\u80fd\u88ab\u8ba4\u4e3a\u662f\u5408\u7406\u7684\u3002", "relevance": 70.0}}
{"id": "2505.10774", "pdf": "https://arxiv.org/pdf/2505.10774", "abs": "https://arxiv.org/abs/2505.10774", "authors": ["Yueyang Yao", "Jiajun Li", "Xingyuan Dai", "MengMeng Zhang", "Xiaoyan Gong", "Fei-Yue Wang", "Yisheng Lv"], "title": "Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "Time series forecasting is important for applications spanning energy\nmarkets, climate analysis, and traffic management. However, existing methods\nstruggle to effectively integrate exogenous texts and align them with the\nprobabilistic nature of large language models (LLMs). Current approaches either\nemploy shallow text-time series fusion via basic prompts or rely on\ndeterministic numerical decoding that conflict with LLMs' token-generation\nparadigm, which limits contextual awareness and distribution modeling. To\naddress these limitations, we propose CAPTime, a context-aware probabilistic\nmultimodal time series forecasting method that leverages text-informed\nabstraction and autoregressive LLM decoding. Our method first encodes temporal\npatterns using a pretrained time series encoder, then aligns them with textual\ncontexts via learnable interactions to produce joint multimodal\nrepresentations. By combining a mixture of distribution experts with frozen\nLLMs, we enable context-aware probabilistic forecasting while preserving LLMs'\ninherent distribution modeling capabilities. Experiments on diverse time series\nforecasting tasks demonstrate the superior accuracy and generalization of\nCAPTime, particularly in multimodal scenarios. Additional analysis highlights\nits robustness in data-scarce scenarios through hybrid probabilistic decoding.", "AI": {"tldr": "CAPTime\u662f\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u65f6\u95f4\u5e8f\u5217\u7684\u591a\u6a21\u6001\u6982\u7387\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u81ea\u56de\u5f52\u89e3\u7801\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u5916\u751f\u6587\u672c\u4e0eLLM\u7684\u6982\u7387\u7279\u6027\uff0c\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u5206\u5e03\u5efa\u6a21\u80fd\u529b\u3002", "method": "CAPTime\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u548c\u53ef\u5b66\u4e60\u7684\u6587\u672c\u5bf9\u9f50\uff0c\u751f\u6210\u591a\u6a21\u6001\u8868\u793a\uff0c\u7ed3\u5408\u6df7\u5408\u5206\u5e03\u4e13\u5bb6\u548c\u51bb\u7ed3LLM\u5b9e\u73b0\u6982\u7387\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCAPTime\u5728\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "CAPTime\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u4e0e\u65f6\u95f4\u5e8f\u5217\u878d\u5408\u7684\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u7559\u4e86LLM\u7684\u5206\u5e03\u5efa\u6a21\u80fd\u529b\u3002", "relevance": 75.0}}
{"id": "2505.10751", "pdf": "https://arxiv.org/pdf/2505.10751", "abs": "https://arxiv.org/abs/2505.10751", "authors": ["Francisco Raverta Capua", "Pablo De Cristoforis"], "title": "Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis", "categories": ["cs.CV"], "comment": "Work in progress, accepted in Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots, ICRA 2025 Workshop - May 23,\n  2025 - Atlanta, GA", "summary": "Although the use of remote sensing technologies for monitoring forested\nenvironments has gained increasing attention, publicly available point cloud\ndatasets remain scarce due to the high costs, sensor requirements, and\ntime-intensive nature of their acquisition. Moreover, as far as we are aware,\nthere are no public annotated datasets generated through Structure From Motion\n(SfM) algorithms applied to imagery, which may be due to the lack of SfM\nalgorithms that can map semantic segmentation information into an accurate\npoint cloud, especially in a challenging environment like forests.\n  In this work, we present a novel pipeline for generating semantically\nsegmented point clouds of forest environments. Using a custom-built forest\nsimulator, we generate realistic RGB images of diverse forest scenes along with\ntheir corresponding semantic segmentation masks. These labeled images are then\nprocessed using modified open-source SfM software capable of preserving\nsemantic information during 3D reconstruction. The resulting point clouds\nprovide both geometric and semantic detail, offering a valuable resource for\ntraining and evaluating deep learning models aimed at segmenting real forest\npoint clouds obtained via SfM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u68ee\u6797\u73af\u5883\u8bed\u4e49\u5206\u5272\u70b9\u4e91\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u548c\u6539\u8fdb\u7684SfM\u7b97\u6cd5\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u8d44\u6e90\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u68ee\u6797\u73af\u5883\u70b9\u4e91\u6570\u636e\u7684\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u4e14\u7f3a\u4e4f\u516c\u5f00\u7684\u8bed\u4e49\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u68ee\u6797\u6a21\u62df\u5668\u751f\u6210RGB\u56fe\u50cf\u548c\u8bed\u4e49\u5206\u5272\u63a9\u7801\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5f00\u6e90SfM\u8f6f\u4ef6\u751f\u6210\u8bed\u4e49\u5206\u5272\u70b9\u4e91\u3002", "result": "\u751f\u6210\u4e86\u517c\u5177\u51e0\u4f55\u548c\u8bed\u4e49\u7ec6\u8282\u7684\u70b9\u4e91\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68ee\u6797\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u586b\u8865\u4e86\u516c\u5f00\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "relevance": 20.0}}
{"id": "2505.10948", "pdf": "https://arxiv.org/pdf/2505.10948", "abs": "https://arxiv.org/abs/2505.10948", "authors": ["Makoto Sato"], "title": "The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs), inspired by neuroscience, exhibit behaviors\nthat often evoke a sense of personality and intelligence-yet the mechanisms\nbehind these effects remain elusive. Here, we operationalize Conceptual\nBlending Theory (CBT) as an experimental framework, using prompt-based methods\nto reveal how LLMs blend and compress meaning. By systematically investigating\nPrompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we\nuncover structural parallels and divergences between artificial and biological\ncognition. Our approach bridges linguistics, neuroscience, and empirical AI\nresearch, demonstrating that human-AI collaboration can serve as a living\nprototype for the future of cognitive science. This work proposes prompt\nengineering not just as a technical tool, but as a scientific method for\nprobing the deep structure of meaning itself.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6982\u5ff5\u6df7\u5408\u7406\u8bba\uff08CBT\uff09\u548c\u63d0\u793a\u8bf1\u5bfc\u65b9\u6cd5\uff0c\u7814\u7a76LLMs\u5982\u4f55\u6df7\u5408\u548c\u538b\u7f29\u610f\u4e49\uff0c\u63ed\u793a\u4e86\u4eba\u5de5\u4e0e\u751f\u7269\u8ba4\u77e5\u7684\u7ed3\u6784\u5f02\u540c\u3002", "motivation": "\u63a2\u7d22LLMs\u7684\u884c\u4e3a\u673a\u5236\uff0c\u5c24\u5176\u662f\u5176\u8868\u73b0\u51fa\u7684\u4e2a\u6027\u548c\u667a\u80fd\u80cc\u540e\u7684\u539f\u56e0\uff0c\u4ee5\u586b\u8865AI\u4e0e\u8ba4\u77e5\u79d1\u5b66\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff08Prompt-Induced Transitions\u548cPrompt-Induced Hallucinations\uff09\u4f5c\u4e3a\u5b9e\u9a8c\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790LLMs\u7684\u610f\u4e49\u5904\u7406\u673a\u5236\u3002", "result": "\u53d1\u73b0LLMs\u5728\u610f\u4e49\u5904\u7406\u4e0a\u4e0e\u751f\u7269\u8ba4\u77e5\u5b58\u5728\u7ed3\u6784\u4e0a\u7684\u76f8\u4f3c\u4e0e\u5dee\u5f02\uff0c\u63d0\u793a\u5de5\u7a0b\u53ef\u4f5c\u4e3a\u7814\u7a76\u610f\u4e49\u6df1\u5c42\u7ed3\u6784\u7684\u79d1\u5b66\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u4e0d\u4ec5\u662f\u6280\u672f\u5de5\u5177\uff0c\u4e5f\u662f\u63a2\u7d22\u610f\u4e49\u6df1\u5c42\u7ed3\u6784\u7684\u79d1\u5b66\u65b9\u6cd5\uff0c\u4e3aAI\u4e0e\u8ba4\u77e5\u79d1\u5b66\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.11049", "pdf": "https://arxiv.org/pdf/2505.11049", "abs": "https://arxiv.org/abs/2505.11049", "authors": ["Yue Liu", "Shengfang Zhai", "Mingzhe Du", "Yulin Chen", "Tri Cao", "Hongcheng Gao", "Cheng Wang", "Xinfeng Li", "Kun Wang", "Junfeng Fang", "Jiaheng Zhang", "Bryan Hooi"], "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b89\u5168\u9632\u62a4\u6a21\u578bGuardReasoner-VL\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6fc0\u52b1\u6a21\u578b\u5728\u505a\u51fa\u5b89\u5168\u51b3\u7b56\u524d\u8fdb\u884c\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u3002", "method": "1. \u6784\u5efa\u5305\u542b123K\u6837\u672c\u548c631K\u63a8\u7406\u6b65\u9aa4\u7684\u6570\u636e\u96c6GuardReasoner-VLTrain\uff1b2. \u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u51b7\u542f\u52a8\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff1b3. \u4f7f\u7528\u5728\u7ebfRL\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u62d2\u7edd\u91c7\u6837\u3001\u5b89\u5168\u611f\u77e5\u6570\u636e\u62fc\u63a5\u548c\u52a8\u6001\u526a\u88c1\u53c2\u6570\uff1b4. \u8bbe\u8ba1\u957f\u5ea6\u611f\u77e5\u7684\u5b89\u5168\u5956\u52b1\u51fd\u6570\u3002", "result": "\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747F1\u5206\u6570\u8d85\u8fc7\u7b2c\u4e8c\u540d19.27%\u3002", "conclusion": "GuardReasoner-VL\u5728\u63d0\u5347VLM\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u3001\u4ee3\u7801\u548c\u6a21\u578b\u3002", "relevance": 75.0}}
{"id": "2505.10799", "pdf": "https://arxiv.org/pdf/2505.10799", "abs": "https://arxiv.org/abs/2505.10799", "authors": ["Tao Bai", "Junzhuo Zhou", "Zeyuan Deng", "Peng Cao"], "title": "Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "The composite current source (CCS) model has been adopted as an advanced\ntiming model that represents the current behavior of cells for improved\naccuracy and better capability than traditional non-linear delay models (NLDM)\nto model complex dynamic effects and interactions under advanced process nodes.\nHowever, the high accuracy requirement, large amount of data and extensive\nsimulation cost pose severe challenges to CCS characterization. To address\nthese challenges, we introduce a novel Gaussian Process Regression(GPR) model\nwith active learning(AL) to establish the characterization framework\nefficiently and accurately. Our approach significantly outperforms conventional\ncommercial tools as well as learning based approaches by achieving an average\nabsolute error of 2.05 ps and a relative error of 2.27% for current waveform of\n57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm\nprocess. Additionally, our model drastically reduces the runtime to 27% and the\nstorage by up to 19.5x compared with that required by commercial tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u548c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u7684\u590d\u5408\u7535\u6d41\u6e90\uff08CCS\uff09\u6a21\u578b\u8868\u5f81\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u975e\u7ebf\u6027\u5ef6\u8fdf\u6a21\u578b\uff08NLDM\uff09\u5728\u5148\u8fdb\u5de5\u827a\u8282\u70b9\u4e0b\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u590d\u6742\u52a8\u6001\u6548\u5e94\u548c\u4ea4\u4e92\u4f5c\u7528\uff0c\u800cCCS\u6a21\u578b\u867d\u66f4\u7cbe\u786e\uff0c\u4f46\u5176\u8868\u5f81\u9762\u4e34\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3001\u5927\u6570\u636e\u91cf\u548c\u6a21\u62df\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u7684\u65b9\u6cd5\uff0c\u5efa\u7acb\u9ad8\u6548\u4e14\u51c6\u786e\u7684CCS\u8868\u5f81\u6846\u67b6\u3002", "result": "\u5728TSMC 22nm\u5de5\u827a\u4e0b\uff0c\u5bf957\u4e2a\u5355\u5143\u7684\u7535\u6d41\u6ce2\u5f62\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a2.05 ps\uff0c\u76f8\u5bf9\u8bef\u5dee\u4e3a2.27%\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u548c\u5b58\u50a8\u9700\u6c42\u5206\u522b\u51cf\u5c11\u81f327%\u548c19.5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5546\u4e1a\u5de5\u5177\u548c\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e3aCCS\u6a21\u578b\u7684\u9ad8\u6548\u8868\u5f81\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "relevance": 10.0}}
{"id": "2505.10764", "pdf": "https://arxiv.org/pdf/2505.10764", "abs": "https://arxiv.org/abs/2505.10764", "authors": ["Jiajun Cheng", "Xianwu Zhao", "Shan Lin"], "title": "Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities", "categories": ["cs.CV"], "comment": null, "summary": "Minimally invasive surgery (MIS) presents significant visual and technical\nchallenges, including surgical instrument classification and understanding\nsurgical action involving instruments, verbs, and anatomical targets. While\nmany machine learning-based methods have been developed for surgical\nunderstanding, they typically rely on procedure- and task-specific models\ntrained on small, manually annotated datasets. In contrast, the recent success\nof vision-language models (VLMs) trained on large volumes of raw image-text\npairs has demonstrated strong adaptability to diverse visual data and a range\nof downstream tasks. This opens meaningful research questions: how well do\nthese general-purpose VLMs perform in the surgical domain? In this work, we\nexplore those questions by benchmarking several VLMs across diverse surgical\ndatasets, including general laparoscopic procedures and endoscopic submucosal\ndissection, to assess their current capabilities and limitations. Our benchmark\nreveals key gaps in the models' ability to consistently link language to the\ncorrect regions in surgical scenes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u624b\u672f\u9886\u57df\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8bc4\u4f30\u53d1\u73b0\u5176\u5728\u624b\u672f\u573a\u666f\u4e2d\u8bed\u8a00\u4e0e\u533a\u57df\u5173\u8054\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u624b\u672f\u9886\u57df\u5b58\u5728\u89c6\u89c9\u548c\u6280\u672f\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u800c\u901a\u7528VLMs\u5728\u591a\u6837\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u5728\u624b\u672f\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "method": "\u901a\u8fc7\u591a\u4e2a\u624b\u672f\u6570\u636e\u96c6\uff08\u5982\u8179\u8154\u955c\u548c\u5185\u955c\u624b\u672f\uff09\u5bf9\u591a\u79cdVLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86VLMs\u5728\u624b\u672f\u573a\u666f\u4e2d\u8bed\u8a00\u4e0e\u6b63\u786e\u533a\u57df\u5173\u8054\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u7528VLMs\u5728\u624b\u672f\u9886\u57df\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "relevance": 40.0}}
{"id": "2505.10975", "pdf": "https://arxiv.org/pdf/2505.10975", "abs": "https://arxiv.org/abs/2505.10975", "authors": ["Xinlu He", "Jacob Whitehill"], "title": "Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "13 pages. Submitted to IEEE/ACM Transaction on Audio Speech and\n  Language Processing (TASLP)", "summary": "Monaural multi-speaker automatic speech recognition (ASR) remains challenging\ndue to data scarcity and the intrinsic difficulty of recognizing and\nattributing words to individual speakers, particularly in overlapping speech.\nRecent advances have driven the shift from cascade systems to end-to-end (E2E)\narchitectures, which reduce error propagation and better exploit the synergy\nbetween speech content and speaker identity. Despite rapid progress in E2E\nmulti-speaker ASR, the field lacks a comprehensive review of recent\ndevelopments. This survey provides a systematic taxonomy of E2E neural\napproaches for multi-speaker ASR, highlighting recent advances and comparative\nanalysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)\nfor pre-segmented audio, analyzing their distinct characteristics and\ntrade-offs; (2) recent architectural and algorithmic improvements based on\nthese two paradigms; (3) extensions to long-form speech, including segmentation\nstrategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate\nand compare methods across standard benchmarks. We conclude with a discussion\nof open challenges and future research directions towards building robust and\nscalable multi-speaker ASR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u7aef\u5230\u7aef\uff08E2E\uff09\u591a\u8bf4\u8bdd\u4eba\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u67b6\u6784\u8303\u5f0f\u3001\u7b97\u6cd5\u6539\u8fdb\u3001\u957f\u8bed\u97f3\u5904\u7406\u53ca\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u591a\u8bf4\u8bdd\u4ebaASR\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u91cd\u53e0\u8bed\u97f3\u8bc6\u522b\u56f0\u96be\u800c\u5177\u6709\u6311\u6218\u6027\uff0cE2E\u67b6\u6784\u51cf\u5c11\u4e86\u9519\u8bef\u4f20\u64ad\u5e76\u66f4\u597d\u5730\u5229\u7528\u8bed\u97f3\u5185\u5bb9\u4e0e\u8bf4\u8bdd\u4eba\u8eab\u4efd\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "method": "\u7cfb\u7edf\u5206\u7c7b\u4e86E2E\u795e\u7ecf\u65b9\u6cd5\uff0c\u5206\u6790\u4e86SIMO\u4e0eSISO\u67b6\u6784\u8303\u5f0f\u3001\u7b97\u6cd5\u6539\u8fdb\u3001\u957f\u8bed\u97f3\u5904\u7406\u7b56\u7565\u53ca\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86E2E\u591a\u8bf4\u8bdd\u4ebaASR\u7684\u8fdb\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u89e3\u51b3\u5f00\u653e\u6311\u6218\uff0c\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u591a\u8bf4\u8bdd\u4ebaASR\u7cfb\u7edf\u3002", "relevance": 30.0}}
{"id": "2505.11063", "pdf": "https://arxiv.org/pdf/2505.11063", "abs": "https://arxiv.org/abs/2505.11063", "authors": ["Changyue Jiang", "Xudong Pan", "Min Yang"], "title": "Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "LLM-based autonomous agents possess capabilities such as reasoning, tool\ninvocation, and environment interaction, enabling the execution of complex\nmulti-step tasks. The internal reasoning process, i.e., thought, of behavioral\ntrajectory significantly influences tool usage and subsequent actions but can\nintroduce potential risks. Even minor deviations in the agent's thought may\ntrigger cascading effects leading to irreversible safety incidents. To address\nthe safety alignment challenges in long-horizon behavioral trajectories, we\npropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing\na lightweight and resource-efficient model, Thought-Aligner corrects each\nhigh-risk thought on the fly before each action execution. The corrected\nthought is then reintroduced to the agent, ensuring safer subsequent decisions\nand tool interactions. Importantly, Thought-Aligner modifies only the reasoning\nphase without altering the underlying agent framework, making it easy to deploy\nand widely applicable to various agent frameworks. To train the Thought-Aligner\nmodel, we construct an instruction dataset across ten representative scenarios\nand simulate ReAct execution trajectories, generating 5,000 diverse\ninstructions and more than 11,400 safe and unsafe thought pairs. The model is\nfine-tuned using contrastive learning techniques. Experiments across three\nagent safety benchmarks involving 12 different LLMs demonstrate that\nThought-Aligner raises agent behavioral safety from approximately 50% in the\nunprotected setting to 90% on average. Additionally, Thought-Aligner maintains\nresponse latency below 100ms with minimal resource usage, demonstrating its\ncapability for efficient deployment, broad applicability, and timely\nresponsiveness. This method thus provides a practical dynamic safety solution\nfor the LLM-based agents.", "AI": {"tldr": "\u63d0\u51faThought-Aligner\u6a21\u5757\uff0c\u52a8\u6001\u4fee\u6b63LLM\u4ee3\u7406\u7684\u9ad8\u98ce\u9669\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u884c\u4e3a\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u4ee3\u7406\u5728\u957f\u65f6\u884c\u4e3a\u8f68\u8ff9\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u9632\u6b62\u63a8\u7406\u504f\u5dee\u5f15\u53d1\u8fde\u9501\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u52a8\u6001\u4fee\u6b63\u6a21\u5757Thought-Aligner\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff0c\u5b9e\u65f6\u4fee\u6b63\u9ad8\u98ce\u9669\u63a8\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u4ee3\u7406\u884c\u4e3a\u5b89\u5168\u6027\u4ece50%\u63d0\u5347\u81f390%\uff0c\u54cd\u5e94\u5ef6\u8fdf\u4f4e\u4e8e100ms\u3002", "conclusion": "Thought-Aligner\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u9ad8\u6548\u3001\u5e7f\u6cdb\u9002\u7528\u7684\u52a8\u6001\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.10802", "pdf": "https://arxiv.org/pdf/2505.10802", "abs": "https://arxiv.org/abs/2505.10802", "authors": ["Ian Holmes", "Min Chi"], "title": "Attention-Based Reward Shaping for Sparse and Delayed Rewards", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 17 tables, 2 figures. Code available online at\n  https://github.com/ihholmes-p/ARES", "summary": "Sparse and delayed reward functions pose a significant obstacle for\nreal-world Reinforcement Learning (RL) applications. In this work, we propose\nAttention-based REward Shaping (ARES), a general and robust algorithm which\nuses a transformer's attention mechanism to generate shaped rewards and create\na dense reward function for any environment. ARES requires a set of episodes\nand their final returns as input. It can be trained entirely offline and is\nable to generate meaningful shaped rewards even when using small datasets or\nepisodes produced by agents taking random actions. ARES is compatible with any\nRL algorithm and can handle any level of reward sparsity. In our experiments,\nwe focus on the most challenging case where rewards are fully delayed until the\nend of each episode. We evaluate ARES across a diverse range of environments,\nwidely used RL algorithms, and baseline methods to assess the effectiveness of\nthe shaped rewards it produces. Our results show that ARES can significantly\nimprove learning in delayed reward settings, enabling RL agents to train in\nscenarios that would otherwise require impractical amounts of data or even be\nunlearnable. To our knowledge, ARES is the first approach that works fully\noffline, remains robust to extreme reward delays and low-quality data, and is\nnot limited to goal-based tasks.", "AI": {"tldr": "ARES\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5956\u52b1\u5851\u9020\u7b97\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\u89e3\u51b3\u7a00\u758f\u548c\u5ef6\u8fdf\u5956\u52b1\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u4efb\u4f55RL\u7b97\u6cd5\u548c\u73af\u5883\u3002", "motivation": "\u7a00\u758f\u548c\u5ef6\u8fdf\u5956\u52b1\u51fd\u6570\u662f\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u7684\u4e3b\u8981\u969c\u788d\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\uff0c\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u5c0f\u6570\u636e\u96c6\u6216\u968f\u673a\u52a8\u4f5c\u751f\u6210\u7684\u7247\u6bb5\u3002", "result": "ARES\u5728\u5b8c\u5168\u5ef6\u8fdf\u5956\u52b1\u7684\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73af\u5883\u548cRL\u7b97\u6cd5\u3002", "conclusion": "ARES\u662f\u9996\u4e2a\u5b8c\u5168\u79bb\u7ebf\u3001\u9c81\u68d2\u6027\u5f3a\u4e14\u4e0d\u9650\u4e8e\u76ee\u6807\u4efb\u52a1\u7684\u5956\u52b1\u5851\u9020\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.10769", "pdf": "https://arxiv.org/pdf/2505.10769", "abs": "https://arxiv.org/abs/2505.10769", "authors": ["Manyu Li", "Ruian He", "Zixian Zhang", "Weimin Tan", "Bo Yan"], "title": "Unifying Segment Anything in Microscopy with Multimodal Large Language Model", "categories": ["cs.CV", "68T99"], "comment": "18 pages, 9 figures", "summary": "Accurate segmentation of regions of interest in biomedical images holds\nsubstantial value in image analysis. Although several foundation models for\nbiomedical segmentation have currently achieved excellent performance on\ncertain datasets, they typically demonstrate sub-optimal performance on unseen\ndomain data. We owe the deficiency to lack of vision-language knowledge before\nsegmentation. Multimodal Large Language Models (MLLMs) bring outstanding\nunderstanding and reasoning capabilities to multimodal tasks, which inspires us\nto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling\nvision models to demonstrate superior generalization capabilities on\ncross-domain datasets. In this paper, we propose using MLLMs to guide SAM in\nlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,\nnamed uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment\n(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find\nthat after SAM receives global VLK prompts, its performance improves\nsignificantly, but there are deficiencies in boundary contour perception.\nTherefore, we further propose Semantic Boundary Regularization (SBR) to prompt\nSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%\nin SA across 9 in-domain microscopy datasets, achieving state-of-the-art\nperformance. Our method also demonstrates improvements of 6.79% in Dice and\n10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization\ncapabilities. Code is available at https://github.com/ieellee/uLLSAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3auLLSAM\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e3aSegment Anything Model\uff08SAM\uff09\u6ce8\u5165\u89c6\u89c9\u8bed\u8a00\u77e5\u8bc6\uff08VLK\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86SAM\u5728\u663e\u5fae\u955c\u56fe\u50cf\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66\u5206\u5272\u57fa\u7840\u6a21\u578b\u5728\u672a\u89c1\u57df\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u89c6\u89c9\u8bed\u8a00\u77e5\u8bc6\u3002MLLMs\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u7075\u611f\u3002", "method": "\u63d0\u51faVision-Language Semantic Alignment\uff08VLSA\uff09\u6a21\u5757\u548cSemantic Boundary Regularization\uff08SBR\uff09\uff0c\u901a\u8fc7MLLMs\u4e3aSAM\u6ce8\u5165VLK\uff0c\u5e76\u4f18\u5316\u8fb9\u754c\u611f\u77e5\u3002", "result": "\u57289\u4e2a\u57df\u5185\u663e\u5fae\u955c\u6570\u636e\u96c6\u4e0a\uff0cDice\u548cSA\u5206\u522b\u63d0\u53477.71%\u548c12.10%\uff1b\u572810\u4e2a\u57df\u5916\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53476.79%\u548c10.08%\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "uLLSAM\u901a\u8fc7\u7ed3\u5408MLLMs\u548cSAM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 60.0}}
{"id": "2505.11004", "pdf": "https://arxiv.org/pdf/2505.11004", "abs": "https://arxiv.org/abs/2505.11004", "authors": ["Jingcheng Niu", "Subhabrata Dutta", "Ahmed Elshabrawy", "Harish Tayyar Madabushi", "Iryna Gurevych"], "title": "Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale Transformer language models (LMs) trained solely on next-token\nprediction with web-scale data can solve a wide range of tasks after seeing\njust a few examples. The mechanism behind this capability, known as in-context\nlearning (ICL), remains both controversial and poorly understood. Some studies\nargue that it is merely the result of memorizing vast amounts of data, while\nothers contend that it reflects a fundamental, symbolic algorithmic development\nin LMs. In this work, we introduce a suite of investigative tasks and a novel\nmethod to systematically investigate ICL by leveraging the full Pythia scaling\nsuite, including interim checkpoints that capture progressively larger amount\nof training data. By carefully exploring ICL performance on downstream tasks\nand simultaneously conducting a mechanistic analysis of the residual stream's\nsubspace, we demonstrate that ICL extends beyond mere \"memorization\" of the\ntraining corpus, yet does not amount to the implementation of an independent\nsymbolic algorithm. Our results also clarify several aspects of ICL, including\nthe influence of training dynamics, model capabilities, and elements of\nmechanistic interpretability. Overall, our work advances the understanding of\nICL and its implications, offering model developers insights into potential\nimprovements and providing AI security practitioners with a basis for more\ninformed guidelines.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u89c4\u6a21Transformer\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u673a\u5236\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u5206\u6790\u8bc1\u660eICL\u4e0d\u4ec5\u4ec5\u662f\u6570\u636e\u8bb0\u5fc6\uff0c\u4e5f\u4e0d\u5b8c\u5168\u662f\u7b26\u53f7\u7b97\u6cd5\uff0c\u800c\u662f\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002", "motivation": "\u7406\u89e3ICL\u7684\u673a\u5236\u53ca\u5176\u5728\u4efb\u52a1\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u6f84\u6e05\u5173\u4e8eICL\u662f\u6570\u636e\u8bb0\u5fc6\u8fd8\u662f\u7b26\u53f7\u7b97\u6cd5\u7684\u4e89\u8bae\u3002", "method": "\u5229\u7528Pythia\u7f29\u653e\u5957\u4ef6\u53ca\u5176\u4e2d\u95f4\u68c0\u67e5\u70b9\uff0c\u8bbe\u8ba1\u4efb\u52a1\u5957\u4ef6\u548c\u65b0\u578b\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7814\u7a76ICL\u7684\u6027\u80fd\u548c\u6b8b\u5dee\u6d41\u5b50\u7a7a\u95f4\u7684\u673a\u5236\u3002", "result": "ICL\u4ecb\u4e8e\u6570\u636e\u8bb0\u5fc6\u548c\u7b26\u53f7\u7b97\u6cd5\u4e4b\u95f4\uff0c\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u8bad\u7ec3\u52a8\u6001\u3001\u6a21\u578b\u80fd\u529b\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5bf9ICL\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86\u5bf9ICL\u7684\u7406\u89e3\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u5e76\u4e3aAI\u5b89\u5168\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u66f4\u660e\u786e\u7684\u6307\u5bfc\u4f9d\u636e\u3002", "relevance": 90.0}}
{"id": "2505.11066", "pdf": "https://arxiv.org/pdf/2505.11066", "abs": "https://arxiv.org/abs/2505.11066", "authors": ["Rui Wang", "Shichun Yang", "Yuyi Chen", "Zhuoyang Li", "Zexiang Tong", "Jianyi Xu", "Jiayi Lu", "Xinjie Feng", "Yaoguang Cao"], "title": "A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware", "categories": ["cs.AI", "cs.MM"], "comment": null, "summary": "Road terrains play a crucial role in ensuring the driving safety of\nautonomous vehicles (AVs). However, existing sensors of AVs, including cameras\nand Lidars, are susceptible to variations in lighting and weather conditions,\nmaking it challenging to achieve real-time perception of road conditions. In\nthis paper, we propose an illumination-aware multi-modal fusion network (IMF),\nwhich leverages both exteroceptive and proprioceptive perception and optimizes\nthe fusion process based on illumination features. We introduce an\nillumination-perception sub-network to accurately estimate illumination\nfeatures. Moreover, we design a multi-modal fusion network which is able to\ndynamically adjust weights of different modalities according to illumination\nfeatures. We enhance the optimization process by pre-training of the\nillumination-perception sub-network and incorporating illumination loss as one\nof the training constraints. Extensive experiments demonstrate that the IMF\nshows a superior performance compared to state-of-the-art methods. The\ncomparison results with single modality perception methods highlight the\ncomprehensive advantages of multi-modal fusion in accurately perceiving road\nterrains under varying lighting conditions. Our dataset is available at:\nhttps://github.com/lindawang2016/IMF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5149\u7167\u611f\u77e5\u7684\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff08IMF\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u65f6\u611f\u77e5\u9053\u8def\u5730\u5f62\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4f20\u611f\u5668\uff08\u5982\u6444\u50cf\u5934\u548c\u6fc0\u5149\u96f7\u8fbe\uff09\u6613\u53d7\u5149\u7167\u548c\u5929\u6c14\u53d8\u5316\u5f71\u54cd\uff0c\u96be\u4ee5\u5b9e\u65f6\u51c6\u786e\u611f\u77e5\u9053\u8def\u6761\u4ef6\u3002", "method": "\u63d0\u51faIMF\u7f51\u7edc\uff0c\u7ed3\u5408\u5916\u611f\u548c\u672c\u4f53\u611f\u77e5\uff0c\u901a\u8fc7\u5149\u7167\u7279\u5f81\u4f18\u5316\u878d\u5408\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u5149\u7167\u611f\u77e5\u5b50\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u591a\u6a21\u6001\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIMF\u5728\u591a\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u591a\u6a21\u6001\u878d\u5408\u5c55\u73b0\u51fa\u5168\u9762\u4f18\u52bf\u3002", "conclusion": "IMF\u901a\u8fc7\u5149\u7167\u611f\u77e5\u548c\u591a\u6a21\u6001\u52a8\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5bf9\u9053\u8def\u5730\u5f62\u7684\u611f\u77e5\u80fd\u529b\u3002", "relevance": 30.0}}
{"id": "2505.10822", "pdf": "https://arxiv.org/pdf/2505.10822", "abs": "https://arxiv.org/abs/2505.10822", "authors": ["Reilly Haskins", "Benjamin Adams"], "title": "Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Knowledge distillation compresses a larger neural model (teacher) into\nsmaller, faster student models by training them to match teacher outputs.\nHowever, the internal computational transformations that occur during this\nprocess remain poorly understood. We apply techniques from mechanistic\ninterpretability to analyze how internal circuits, representations, and\nactivation patterns differ between teacher and student. Focusing on GPT2-small\nand its distilled counterpart DistilGPT2, we find that student models\nreorganize, compress, and discard teacher components, often resulting in\nstronger reliance on fewer individual components. To quantify functional\nalignment beyond output similarity, we introduce an alignment metric based on\ninfluence-weighted component similarity, validated across multiple tasks. Our\nfindings reveal that while knowledge distillation preserves broad functional\nbehaviors, it also causes significant shifts in internal computation, with\nimportant implications for the robustness and generalization capacity of\ndistilled models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u6559\u5e08\u6a21\u578b\u4e0e\u5b66\u751f\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u5b66\u751f\u6a21\u578b\u4f1a\u91cd\u7ec4\u3001\u538b\u7f29\u6216\u4e22\u5f03\u6559\u5e08\u6a21\u578b\u7684\u7ec4\u4ef6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u6559\u5e08\u6a21\u578b\u4e0e\u5b66\u751f\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u5dee\u5f02\uff0c\u4ee5\u63ed\u793a\u84b8\u998f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5e94\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5206\u6790\u6559\u5e08\u6a21\u578b\uff08GPT2-small\uff09\u548c\u5b66\u751f\u6a21\u578b\uff08DistilGPT2\uff09\u7684\u5185\u90e8\u7535\u8def\u3001\u8868\u793a\u548c\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5f71\u54cd\u529b\u52a0\u6743\u7684\u7ec4\u4ef6\u76f8\u4f3c\u6027\u5bf9\u9f50\u5ea6\u91cf\u3002", "result": "\u5b66\u751f\u6a21\u578b\u4f1a\u91cd\u7ec4\u3001\u538b\u7f29\u6216\u4e22\u5f03\u6559\u5e08\u6a21\u578b\u7684\u7ec4\u4ef6\uff0c\u5bfc\u81f4\u5bf9\u66f4\u5c11\u7ec4\u4ef6\u7684\u4f9d\u8d56\uff1b\u77e5\u8bc6\u84b8\u998f\u4fdd\u7559\u4e86\u5e7f\u6cdb\u7684\u529f\u80fd\u884c\u4e3a\uff0c\u4f46\u5185\u90e8\u8ba1\u7b97\u53d1\u751f\u4e86\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u867d\u7136\u4fdd\u7559\u4e86\u529f\u80fd\u884c\u4e3a\uff0c\u4f46\u5185\u90e8\u8ba1\u7b97\u7684\u53d8\u5316\u53ef\u80fd\u5f71\u54cd\u84b8\u998f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10781", "pdf": "https://arxiv.org/pdf/2505.10781", "abs": "https://arxiv.org/abs/2505.10781", "authors": ["David Minkwan Kim", "Soeun Lee", "Byeongkeun Kang"], "title": "Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages", "summary": "This work addresses the task of completely weakly supervised\nclass-incremental learning for semantic segmentation to learn segmentation for\nboth base and additional novel classes using only image-level labels. While\nclass-incremental semantic segmentation (CISS) is crucial for handling diverse\nand newly emerging objects in the real world, traditional CISS methods require\nexpensive pixel-level annotations for training. To overcome this limitation,\npartially weakly-supervised approaches have recently been proposed. However, to\nthe best of our knowledge, this is the first work to introduce a completely\nweakly-supervised method for CISS. To achieve this, we propose to generate\nrobust pseudo-labels by combining pseudo-labels from a localizer and a sequence\nof foundation models based on their uncertainty. Moreover, to mitigate\ncatastrophic forgetting, we introduce an exemplar-guided data augmentation\nmethod that generates diverse images containing both previous and novel classes\nwith guidance. Finally, we conduct experiments in three common experimental\nsettings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint\nand overlap. The experimental results demonstrate that our completely weakly\nsupervised method outperforms even partially weakly supervised methods in the\n15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the\nCOCO-to-VOC setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5f31\u76d1\u7763\u7684\u7c7b\u589e\u91cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u8bad\u7ec3\u57fa\u7840\u548c\u65b0\u7c7b\u522b\u5206\u5272\u3002\u901a\u8fc7\u7ed3\u5408\u5b9a\u4f4d\u5668\u548c\u57fa\u7840\u6a21\u578b\u7684\u4f2a\u6807\u7b7e\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u9c81\u68d2\u4f2a\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u793a\u4f8b\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u4ee5\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u90e8\u5206\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7c7b\u589e\u91cf\u8bed\u4e49\u5206\u5272\uff08CISS\uff09\u9700\u8981\u6602\u8d35\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u9996\u6b21\u63d0\u51fa\u5b8c\u5168\u5f31\u76d1\u7763\u7684CISS\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5b9a\u4f4d\u5668\u548c\u57fa\u7840\u6a21\u578b\u7684\u4f2a\u6807\u7b7e\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u9c81\u68d2\u4f2a\u6807\u7b7e\uff0c\u4f7f\u7528\u793a\u4f8b\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u4ee5\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u572815-5 VOC\u548c10-10 VOC\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u90e8\u5206\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5728COCO-to-VOC\u8bbe\u7f6e\u4e2d\u8868\u73b0\u7ade\u4e89\u6027\u3002", "conclusion": "\u5b8c\u5168\u5f31\u76d1\u7763\u7684CISS\u65b9\u6cd5\u53ef\u884c\u4e14\u6709\u6548\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.11008", "pdf": "https://arxiv.org/pdf/2505.11008", "abs": "https://arxiv.org/abs/2505.11008", "authors": ["Ye Kyaw Thu", "Thazin Myint Oo"], "title": "Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "14 pages, 2 figures, 6 tables, 1 listing", "summary": "This paper explores syllable sequence prediction in Abugida languages using\nTransformer-based models, focusing on six languages: Bengali, Hindi, Khmer,\nLao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We\ninvestigate the reconstruction of complete syllable sequences from various\nincomplete input types, including consonant sequences, vowel sequences, partial\nsyllables (with random character deletions), and masked syllables (with fixed\nsyllable deletions). Our experiments reveal that consonant sequences play a\ncritical role in accurate syllable prediction, achieving high BLEU scores,\nwhile vowel sequences present a significantly greater challenge. The model\ndemonstrates robust performance across tasks, particularly in handling partial\nand masked syllable reconstruction, with strong results for tasks involving\nconsonant information and syllable masking. This study advances the\nunderstanding of sequence prediction for Abugida languages and provides\npractical insights for applications such as text prediction, spelling\ncorrection, and data augmentation in these scripts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728Abugida\u8bed\u8a00\u4e2d\u7684\u97f3\u8282\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u516d\u79cd\u4e9a\u6d32\u8bed\u8a00\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8f85\u97f3\u5e8f\u5217\u5728\u97f3\u8282\u9884\u6d4b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u800c\u5143\u97f3\u5e8f\u5217\u66f4\u5177\u6311\u6218\u6027\u3002", "motivation": "\u63a2\u7d22Abugida\u8bed\u8a00\u4e2d\u97f3\u8282\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5b8c\u6574\u8f93\u5165\u6761\u4ef6\u4e0b\u7684\u91cd\u5efa\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u6587\u672c\u9884\u6d4b\u3001\u62fc\u5199\u6821\u6b63\u7b49\u5e94\u7528\u3002", "method": "\u4f7f\u7528Transformer\u6a21\u578b\uff0c\u4ece\u8f85\u97f3\u5e8f\u5217\u3001\u5143\u97f3\u5e8f\u5217\u3001\u90e8\u5206\u97f3\u8282\u548c\u63a9\u7801\u97f3\u8282\u7b49\u4e0d\u5b8c\u6574\u8f93\u5165\u4e2d\u91cd\u5efa\u5b8c\u6574\u97f3\u8282\u5e8f\u5217\u3002", "result": "\u8f85\u97f3\u5e8f\u5217\u5bf9\u97f3\u8282\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0cBLEU\u5f97\u5206\u9ad8\uff1b\u5143\u97f3\u5e8f\u5217\u9884\u6d4b\u66f4\u5177\u6311\u6218\u6027\u3002\u6a21\u578b\u5728\u90e8\u5206\u548c\u63a9\u7801\u97f3\u8282\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u7814\u7a76\u4e3aAbugida\u8bed\u8a00\u7684\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u652f\u6301\u76f8\u5173\u5e94\u7528\u5f00\u53d1\u3002", "relevance": 40.0}}
{"id": "2505.11086", "pdf": "https://arxiv.org/pdf/2505.11086", "abs": "https://arxiv.org/abs/2505.11086", "authors": ["Keita Kinjo"], "title": "Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data", "categories": ["cs.AI", "cs.CY"], "comment": "19 pages, 7 figures", "summary": "Recently, the proliferation of omni-channel platforms has attracted interest\nin customer journeys, particularly regarding their role in developing marketing\nstrategies. However, few efforts have been taken to quantitatively study or\ncomprehensively analyze them owing to the sequential nature of their data and\nthe complexity involved in analysis. In this study, we propose a novel approach\ncomprising three steps for analyzing customer journeys. First, the distance\nbetween sequential data is defined and used to identify and visualize\nrepresentative sequences. Second, the likelihood of purchase is predicted based\non this distance. Third, if a sequence suggests no purchase, counterfactual\nsequences are recommended to increase the probability of a purchase using a\nproposed method, which extracts counterfactual explanations for sequential\ndata. A survey was conducted, and the data were analyzed; the results revealed\nthat typical sequences could be extracted, and the parts of those sequences\nimportant for purchase could be detected. We believe that the proposed approach\ncan support improvements in various marketing activities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u5ba2\u6237\u65c5\u7a0b\u7684\u4e09\u6b65\u65b9\u6cd5\uff0c\u5305\u62ec\u5b9a\u4e49\u5e8f\u5217\u6570\u636e\u8ddd\u79bb\u3001\u9884\u6d4b\u8d2d\u4e70\u6982\u7387\u548c\u63a8\u8350\u53cd\u4e8b\u5b9e\u5e8f\u5217\u4ee5\u63d0\u9ad8\u8d2d\u4e70\u6982\u7387\u3002", "motivation": "\u591a\u5e73\u53f0\u5ba2\u6237\u65c5\u7a0b\u7684\u5b9a\u91cf\u7814\u7a76\u8f83\u5c11\uff0c\u56e0\u5176\u6570\u636e\u5e8f\u5217\u6027\u548c\u5206\u6790\u590d\u6742\u6027\u3002", "method": "1. \u5b9a\u4e49\u5e8f\u5217\u6570\u636e\u8ddd\u79bb\u5e76\u53ef\u89c6\u5316\u4ee3\u8868\u6027\u5e8f\u5217\uff1b2. \u57fa\u4e8e\u8ddd\u79bb\u9884\u6d4b\u8d2d\u4e70\u6982\u7387\uff1b3. \u63a8\u8350\u53cd\u4e8b\u5b9e\u5e8f\u5217\u4ee5\u63d0\u9ad8\u8d2d\u4e70\u6982\u7387\u3002", "result": "\u6210\u529f\u63d0\u53d6\u5178\u578b\u5e8f\u5217\u5e76\u8bc6\u522b\u5bf9\u8d2d\u4e70\u91cd\u8981\u7684\u90e8\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u652f\u6301\u8425\u9500\u6d3b\u52a8\u6539\u8fdb\u3002", "relevance": 10.0}}
{"id": "2505.10833", "pdf": "https://arxiv.org/pdf/2505.10833", "abs": "https://arxiv.org/abs/2505.10833", "authors": ["Yifei He", "Siqi Zeng", "Yuzheng Hu", "Rui Yang", "Tong Zhang", "Han Zhao"], "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Model merging provides a scalable alternative to multi-task training by\ncombining specialized finetuned models through parameter arithmetic, enabling\nefficient deployment without the need for joint training or access to all task\ndata. While recent methods have shown promise, existing evaluations are limited\nin both model scale and task diversity, leaving open questions about their\napplicability to large, domain-specialized LLMs. To tackle the challenges, we\nintroduce MergeBench, a comprehensive evaluation suite designed to assess model\nmerging at scale. MergeBench builds on state-of-the-art open-source language\nmodels, including Llama and Gemma families at 2B to 9B scales, and covers five\nkey domains: instruction following, mathematics, multilingual understanding,\ncoding and safety. We standardize finetuning and evaluation protocols, and\nassess eight representative merging methods across multi-task performance,\nforgetting and runtime efficiency. Based on extensive experiments, we provide\npractical guidelines for algorithm selection and share insights showing that\nmodel merging tends to perform better on stronger base models, with techniques\nsuch as merging coefficient tuning and sparsification improving knowledge\nretention. However, several challenges remain, including the computational cost\non large models, the gap for in-domain performance compared to multi-task\nmodels, and the underexplored role of model merging in standard LLM training\npipelines. We hope MergeBench provides a foundation for future research to\nadvance the understanding and practical application of model merging. We open\nsource our code at\n\\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.", "AI": {"tldr": "MergeBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6a21\u578b\u5408\u5e76\u7684\u7efc\u5408\u5957\u4ef6\uff0c\u9488\u5bf9\u5927\u89c4\u6a21\u3001\u9886\u57df\u4e13\u4e1a\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u6a21\u578b\u89c4\u6a21\u548c\u4efb\u52a1\u591a\u6837\u6027\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u8bc4\u4f30\u5176\u5728\u5927\u578bLLMs\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama\u548cGemma\uff09\uff0c\u5728\u4e94\u4e2a\u5173\u952e\u9886\u57df\uff08\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u3001\u591a\u8bed\u8a00\u7406\u89e3\u3001\u7f16\u7801\u548c\u5b89\u5168\uff09\u4e0a\u8bc4\u4f30\u516b\u79cd\u4ee3\u8868\u6027\u5408\u5e76\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5408\u5e76\u5bf9\u66f4\u5f3a\u7684\u57fa\u7840\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u5408\u5e76\u7cfb\u6570\u8c03\u6574\u548c\u7a00\u758f\u5316\u7b49\u6280\u672f\u6709\u52a9\u4e8e\u77e5\u8bc6\u4fdd\u7559\uff0c\u4f46\u4ecd\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9886\u57df\u5185\u6027\u80fd\u5dee\u8ddd\u7b49\u95ee\u9898\u3002", "conclusion": "MergeBench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f46\u6a21\u578b\u5408\u5e76\u5728\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "relevance": 85.0}}
{"id": "2505.10784", "pdf": "https://arxiv.org/pdf/2505.10784", "abs": "https://arxiv.org/abs/2505.10784", "authors": ["Qiushi Guo", "Jason Rambach"], "title": "SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Detecting potential obstacles in railway environments is critical for\npreventing serious accidents. Identifying a broad range of obstacle categories\nunder complex conditions requires large-scale datasets with precisely\nannotated, high-quality images. However, existing publicly available datasets\nfail to meet these requirements, thereby hindering progress in railway safety\nresearch. To address this gap, we introduce SynRailObs, a high-fidelity\nsynthetic dataset designed to represent a diverse range of weather conditions\nand geographical features. Furthermore, diffusion models are employed to\ngenerate rare and difficult-to-capture obstacles that are typically challenging\nto obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs,\nwe perform experiments in real-world railway environments, testing on both\nballasted and ballastless tracks across various weather conditions. The results\ndemonstrate that SynRailObs holds substantial potential for advancing obstacle\ndetection in railway safety applications. Models trained on this dataset show\nconsistent performance across different distances and environmental conditions.\nMoreover, the model trained on SynRailObs exhibits zero-shot capabilities,\nwhich are essential for applications in security-sensitive domains. The data is\navailable in https://www.kaggle.com/datasets/qiushi910/synrailobs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SynRailObs\uff0c\u4e00\u4e2a\u7528\u4e8e\u94c1\u8def\u969c\u788d\u7269\u68c0\u6d4b\u7684\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u7f55\u89c1\u969c\u788d\u7269\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u94c1\u8def\u5b89\u5168\u7814\u7a76\u4e2d\u590d\u6742\u6761\u4ef6\u4e0b\u591a\u7c7b\u522b\u969c\u788d\u7269\u68c0\u6d4b\u7684\u9700\u6c42\uff0c\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u63d0\u51faSynRailObs\u5408\u6210\u6570\u636e\u96c6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7f55\u89c1\u969c\u788d\u7269\uff0c\u5e76\u5728\u771f\u5b9e\u94c1\u8def\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eSynRailObs\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4e0d\u540c\u8ddd\u79bb\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u8868\u73b0\u4e00\u81f4\uff0c\u5e76\u5177\u5907\u96f6\u6837\u672c\u80fd\u529b\u3002", "conclusion": "SynRailObs\u5728\u94c1\u8def\u5b89\u5168\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u53ef\u63a8\u52a8\u969c\u788d\u7269\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002", "relevance": 30.0}}
{"id": "2505.11010", "pdf": "https://arxiv.org/pdf/2505.11010", "abs": "https://arxiv.org/abs/2505.11010", "authors": ["Jiangxu Wu", "Cong Wang", "TianHuang Su", "Jun Yang", "Haozhi Lin", "Chao Zhang", "Ming Peng", "Kai Shi", "SongPan Yang", "BinQing Pan", "ZiXian Li", "Ni Yang", "ZhenYu Yang"], "title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL2025 Accepted", "summary": "The effectiveness of large language models (LLMs) in conversational AI is\nhindered by their reliance on single-turn supervised fine-tuning (SFT) data,\nwhich limits contextual coherence in multi-turn dialogues. Existing methods for\ngenerating multi-turn dialogue data struggle to ensure both diversity and\nquality in instructions. To address this, we propose Review-Instruct, a novel\nframework that synthesizes multi-turn conversations through an iterative\n\"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple\nReviewers, and a Chairman. The framework iteratively refines instructions by\nincorporating Reviewer feedback, enhancing dialogue diversity and difficulty.\nWe construct a multi-turn dataset using the Alpaca dataset and fine-tune the\nLLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate\nsignificant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\%\non MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.\nAblation studies confirm the critical role of the Review stage and the use of\nmultiple Reviewers in boosting instruction diversity and difficulty. Our work\nhighlights the potential of review-driven, multi-agent frameworks for\ngenerating high-quality conversational data at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86Review-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u8fed\u4ee3\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u8f6e\u76d1\u7763\u5fae\u8c03\u6570\u636e\u9650\u5236\u4e86\u591a\u8f6e\u5bf9\u8bdd\u7684\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u591a\u6837\u6027\u4e0e\u8d28\u91cf\u3002", "method": "\u63d0\u51faReview-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u201cAsk-Respond-Review\u201d\u6d41\u7a0b\u548c\u591a\u89d2\u8272\uff08\u5019\u9009\u8005\u3001\u8bc4\u5ba1\u8005\u3001\u4e3b\u5e2d\uff09\u8fed\u4ee3\u4f18\u5316\u6307\u4ee4\u3002", "result": "\u5728MMLU-Pro\u548cMT-Bench\u4e0a\u5206\u522b\u53d6\u5f972.9%\u548c2%\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u8bc4\u5ba1\u9636\u6bb5\u548c\u591a\u8bc4\u5ba1\u8005\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u591a\u4ee3\u7406\u6846\u67b6\u80fd\u89c4\u6a21\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u6570\u636e\uff0c\u63d0\u5347LLM\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.11107", "pdf": "https://arxiv.org/pdf/2505.11107", "abs": "https://arxiv.org/abs/2505.11107", "authors": ["Chan-Jan Hsu", "Davide Buffelli", "Jamie McGowan", "Feng-Ting Liao", "Yi-Chang Chen", "Sattar Vakili", "Da-shan Shiu"], "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGroup Think\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e2aLLM\u6a21\u62df\u591a\u4e2a\u5e76\u53d1\u63a8\u7406\u4ee3\u7406\uff0c\u5b9e\u73b0\u52a8\u6001\u534f\u4f5c\uff0c\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u4ee3\u7406\u901a\u5e38\u4ee5\u8f6e\u6d41\u65b9\u5f0f\u4ea4\u4e92\uff0c\u727a\u7272\u5ef6\u8fdf\u6362\u53d6\u8d28\u91cf\u63d0\u5347\uff0c\u800cGroup Think\u65e8\u5728\u901a\u8fc7\u5e76\u53d1\u63a8\u7406\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "Group Think\u8ba9\u5355\u4e2aLLM\u4f5c\u4e3a\u591a\u4e2a\u5e76\u53d1\u63a8\u7406\u4ee3\u7406\u8fd0\u884c\uff0c\u901a\u8fc7\u5171\u4eab\u90e8\u5206\u751f\u6210\u8fdb\u5ea6\u5b9e\u73b0\u52a8\u6001\u534f\u4f5c\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u901a\u7528\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u73b0\u6709LLM\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cGroup Think\u5728\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8d28\u91cf\uff0c\u5e76\u6709\u6548\u5229\u7528\u4e86\u7a7a\u95f2\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "Group Think\u4e3a\u672a\u6765LLM\u5b9e\u73b0\u66f4\u9ad8\u6548\u534f\u4f5c\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.10838", "pdf": "https://arxiv.org/pdf/2505.10838", "abs": "https://arxiv.org/abs/2505.10838", "authors": ["Ran Li", "Hao Wang", "Chengzhi Mao"], "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization.", "AI": {"tldr": "LARGO\u662f\u4e00\u79cd\u65b0\u578b\u7684\u68af\u5ea6\u4f18\u5316\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728LLM\u7684\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f18\u5316\u5bf9\u6297\u6027\u5411\u91cf\uff0c\u751f\u6210\u6d41\u7545\u7684\u8d8a\u72f1\u63d0\u793a\uff0c\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9LLM\u7684\u653b\u51fb\u65b9\u6cd5\u5728\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u4e2d\u6548\u679c\u6709\u9650\uff0c\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u53d1\u6325\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63ed\u793aLLM\u7684\u6f0f\u6d1e\u3002", "method": "LARGO\u901a\u8fc7\u5728LLM\u7684\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f18\u5316\u5bf9\u6297\u6027\u5411\u91cf\uff0c\u5e76\u9012\u5f52\u8c03\u7528LLM\u5c06\u5176\u89e3\u7801\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u751f\u6210\u6d41\u7545\u4e14\u9690\u853d\u7684\u8d8a\u72f1\u63d0\u793a\u3002", "result": "\u5728AdvBench\u548cJailbreakBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLARGO\u7684\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff08\u5982AutoDAN\uff09\u9ad8\u51fa44\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LARGO\u5c55\u793a\u4e86\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u653b\u51fbLLM\u5185\u90e8\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.10787", "pdf": "https://arxiv.org/pdf/2505.10787", "abs": "https://arxiv.org/abs/2505.10787", "authors": ["Jianlin Guo", "Haihong Xiao", "Wenxiong Kang"], "title": "EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes", "categories": ["cs.CV"], "comment": null, "summary": "Efficient scene representations are essential for many real-world\napplications, especially those involving spatial measurement. Although current\nNeRF-based methods have achieved impressive results in reconstructing\nbuilding-scale scenes, they still suffer from slow training and inference\nspeeds due to time-consuming stochastic sampling. Recently, 3D Gaussian\nSplatting (3DGS) has demonstrated excellent performance with its high-quality\nrendering and real-time speed, especially for objects and small-scale scenes.\nHowever, in outdoor scenes, its point-based explicit representation lacks an\neffective adjustment mechanism, and the millions of Gaussian points required\noften lead to memory constraints during training. To address these challenges,\nwe propose EA-3DGS, a high-quality real-time rendering method designed for\noutdoor scenes. First, we introduce a mesh structure to regulate the\ninitialization of Gaussian components by leveraging an adaptive tetrahedral\nmesh that partitions the grid and initializes Gaussian components on each face,\neffectively capturing geometric structures in low-texture regions. Second, we\npropose an efficient Gaussian pruning strategy that evaluates each 3D\nGaussian's contribution to the view and prunes accordingly. To retain\ngeometry-critical Gaussian points, we also present a structure-aware\ndensification strategy that densifies Gaussian points in low-curvature regions.\nAdditionally, we employ vector quantization for parameter quantization of\nGaussian components, significantly reducing disk space requirements with only a\nminimal impact on rendering quality. Extensive experiments on 13 scenes,\nincluding eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \\&\nTemples, WHU) and five self-collected scenes acquired through UAV\nphotogrammetry measurement from SCUT-CA and plateau regions, further\ndemonstrate the superiority of our method.", "AI": {"tldr": "EA-3DGS\u662f\u4e00\u79cd\u9488\u5bf9\u6237\u5916\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7f51\u683c\u7ed3\u6784\u548c\u9ad8\u6548\u7684\u9ad8\u65af\u526a\u679d\u7b56\u7565\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u6237\u5916\u573a\u666f\u4e2d\u7684\u5185\u5b58\u548c\u8c03\u6574\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3DGS\u65b9\u6cd5\u5728\u6237\u5916\u573a\u666f\u4e2d\u56e0\u70b9\u57fa\u663e\u5f0f\u8868\u793a\u7f3a\u4e4f\u6709\u6548\u8c03\u6574\u673a\u5236\u548c\u9ad8\u5185\u5b58\u9700\u6c42\u800c\u53d7\u9650\uff0cEA-3DGS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u81ea\u9002\u5e94\u56db\u9762\u4f53\u7f51\u683c\u521d\u59cb\u5316\u9ad8\u65af\u7ec4\u4ef6\uff1b2. \u63d0\u51fa\u9ad8\u6548\u9ad8\u65af\u526a\u679d\u7b56\u7565\uff1b3. \u7ed3\u6784\u611f\u77e5\u7684\u5bc6\u96c6\u5316\u7b56\u7565\uff1b4. \u9ad8\u65af\u7ec4\u4ef6\u7684\u53c2\u6570\u91cf\u5316\u3002", "result": "\u572813\u4e2a\u573a\u666f\uff08\u5305\u62ec\u516c\u5f00\u548c\u81ea\u91c7\u96c6\u6570\u636e\u96c6\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "EA-3DGS\u663e\u8457\u63d0\u5347\u4e86\u6237\u5916\u573a\u666f\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.11026", "pdf": "https://arxiv.org/pdf/2505.11026", "abs": "https://arxiv.org/abs/2505.11026", "authors": ["Maria Dziuba", "Valentin Malykh"], "title": "StRuCom: A Novel Dataset of Structured Code Comments in Russian", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Structured code comments in docstring format are essential for code\ncomprehension and maintenance, but existing machine learning models for their\ngeneration perform poorly for Russian compared to English. To bridge this gap,\nwe present StRuCom - the first large-scale dataset (153K examples) specifically\ndesigned for Russian code documentation. Unlike machine-translated English\ndatasets that distort terminology (e.g., technical loanwords vs. literal\ntranslations) and docstring structures, StRuCom combines human-written comments\nfrom Russian GitHub repositories with synthetically generated ones, ensuring\ncompliance with Python, Java, JavaScript, C#, and Go standards through\nautomated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom\nshows statistically significant improvements of chrf++ and BERTScore over\nbaseline models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86StRuCom\uff0c\u4e00\u4e2a\u9488\u5bf9\u4fc4\u8bed\u4ee3\u7801\u6587\u6863\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03Qwen2.5-Coder\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4fc4\u8bed\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4fc4\u8bed\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cStRuCom\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u907f\u514d\u4e86\u673a\u5668\u7ffb\u8bd1\u5e26\u6765\u7684\u672f\u8bed\u548c\u7ed3\u6784\u5931\u771f\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4fc4\u8bedGitHub\u4ed3\u5e93\u4e2d\u7684\u4eba\u5de5\u7f16\u5199\u6ce8\u91ca\u548c\u5408\u6210\u751f\u6210\u7684\u6ce8\u91ca\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9a8c\u8bc1\u786e\u4fdd\u7b26\u5408\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u6807\u51c6\uff0c\u5e76\u5fae\u8c03Qwen2.5-Coder\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728chrf++\u548cBERTScore\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "StRuCom\u4e3a\u4fc4\u8bed\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "relevance": 30.0}}
{"id": "2505.11119", "pdf": "https://arxiv.org/pdf/2505.11119", "abs": "https://arxiv.org/abs/2505.11119", "authors": ["Jiabei Cheng", "Zhen-Qun Yang", "Jiannong Cao", "Yu Yang", "Xinzhe Zheng"], "title": "Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Timely prediction of students at high risk of dropout is critical for early\nintervention and improving educational outcomes. However, in offline\neducational settings, poor data quality, limited scale, and high heterogeneity\noften hinder the application of advanced machine learning models. Furthermore,\nwhile educational theories provide valuable insights into dropout phenomena,\nthe lack of quantifiable metrics for key indicators limits their use in\ndata-driven modeling. Through data analysis and a review of educational\nliterature, we identified abrupt changes in student behavior as key early\nsignals of dropout risk. To address this, we propose the Dual-Modal Multiscale\nSliding Window (DMSW) Model, which integrates academic performance and\nbehavioral data to dynamically capture behavior patterns using minimal data.\nThe DMSW model improves prediction accuracy by 15% compared to traditional\nmethods, enabling educators to identify high-risk students earlier, provide\ntimely support, and foster a more inclusive learning environment. Our analysis\nhighlights key behavior patterns, offering practical insights for preventive\nstrategies and tailored support. These findings bridge the gap between theory\nand practice in dropout prediction, giving educators an innovative tool to\nenhance student retention and outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u6001\u591a\u5c3a\u5ea6\u6ed1\u52a8\u7a97\u53e3\uff08DMSW\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u52a8\u6001\u6355\u6349\u5b66\u751f\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u9884\u6d4b\u8f8d\u5b66\u98ce\u9669\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534715%\u3002", "motivation": "\u5728\u79bb\u7ebf\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u6570\u636e\u8d28\u91cf\u5dee\u3001\u89c4\u6a21\u6709\u9650\u548c\u9ad8\u5ea6\u5f02\u8d28\u6027\u9650\u5236\u4e86\u9ad8\u7ea7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6559\u80b2\u7406\u8bba\u7f3a\u4e4f\u53ef\u91cf\u5316\u6307\u6807\uff0c\u96be\u4ee5\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u6570\u636e\u5206\u6790\u548c\u6559\u80b2\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u5b66\u751f\u884c\u4e3a\u7684\u7a81\u53d8\u4f5c\u4e3a\u8f8d\u5b66\u98ce\u9669\u7684\u65e9\u671f\u4fe1\u53f7\uff0c\u63d0\u51faDMSW\u6a21\u578b\uff0c\u6574\u5408\u5b66\u4e1a\u8868\u73b0\u548c\u884c\u4e3a\u6570\u636e\uff0c\u52a8\u6001\u6355\u6349\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "DMSW\u6a21\u578b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad815%\uff0c\u80fd\u66f4\u65e9\u8bc6\u522b\u9ad8\u98ce\u9669\u5b66\u751f\uff0c\u63d0\u4f9b\u53ca\u65f6\u652f\u6301\u3002", "conclusion": "DMSW\u6a21\u578b\u586b\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u589e\u5f3a\u5b66\u751f\u4fdd\u7559\u7387\u548c\u6210\u679c\u7684\u521b\u65b0\u5de5\u5177\u3002", "relevance": 20.0}}
{"id": "2505.10845", "pdf": "https://arxiv.org/pdf/2505.10845", "abs": "https://arxiv.org/abs/2505.10845", "authors": ["Hanyu Duan", "Yi Yang", "Ahmed Abbasi", "Kar Yan Tam"], "title": "Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces Ready2Unlearn, a learning-time optimization approach\ndesigned to facilitate future unlearning processes. Unlike the majority of\nexisting unlearning efforts that focus on designing unlearning algorithms,\nwhich are typically implemented reactively when an unlearning request is made\nduring the model deployment phase, Ready2Unlearn shifts the focus to the\ntraining phase, adopting a \"forward-looking\" perspective. Building upon\nwell-established meta-learning principles, Ready2Unlearn proactively trains\nmachine learning models with unlearning readiness, such that they are well\nprepared and can handle future unlearning requests in a more efficient and\nprincipled manner. Ready2Unlearn is model-agnostic and compatible with any\ngradient ascent-based machine unlearning algorithms. We evaluate the method on\nboth vision and language tasks under various unlearning settings, including\nclass-wise unlearning and random data unlearning. Experimental results show\nthat by incorporating such preparedness at training time, Ready2Unlearn\nproduces an unlearning-ready model state, which offers several key advantages\nwhen future unlearning is required, including reduced unlearning time, improved\nretention of overall model capability, and enhanced resistance to the\ninadvertent recovery of forgotten data. We hope this work could inspire future\nefforts to explore more proactive strategies for equipping machine learning\nmodels with built-in readiness towards more reliable and principled machine\nunlearning.", "AI": {"tldr": "Ready2Unlearn\u662f\u4e00\u79cd\u5728\u8bad\u7ec3\u9636\u6bb5\u4f18\u5316\u6a21\u578b\u4ee5\u652f\u6301\u672a\u6765\u9057\u5fd8\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u539f\u5219\u63d0\u5347\u9057\u5fd8\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u5728\u90e8\u7f72\u9636\u6bb5\u88ab\u52a8\u5904\u7406\u9057\u5fd8\u8bf7\u6c42\uff0cReady2Unlearn\u63d0\u51fa\u5728\u8bad\u7ec3\u9636\u6bb5\u4e3b\u52a8\u4f18\u5316\u6a21\u578b\uff0c\u4f7f\u5176\u5177\u5907\u9057\u5fd8\u51c6\u5907\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5143\u5b66\u4e60\u539f\u5219\uff0cReady2Unlearn\u5728\u8bad\u7ec3\u9636\u6bb5\u4e3b\u52a8\u4f18\u5316\u6a21\u578b\uff0c\u4f7f\u5176\u517c\u5bb9\u68af\u5ea6\u4e0a\u5347\u7c7b\u9057\u5fd8\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReady2Unlearn\u80fd\u51cf\u5c11\u9057\u5fd8\u65f6\u95f4\u3001\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u5bf9\u9057\u5fd8\u6570\u636e\u7684\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "Ready2Unlearn\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e3b\u52a8\u7684\u9057\u5fd8\u51c6\u5907\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 60.0}}
{"id": "2505.10810", "pdf": "https://arxiv.org/pdf/2505.10810", "abs": "https://arxiv.org/abs/2505.10810", "authors": ["Gabriel Maldonado", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation", "categories": ["cs.CV"], "comment": "11 pages, 5 figures, 2 tables. Presented at the CVPR 2025 Human\n  Motion Generation (HuMoGen) Workshop. Introduces MoCLIP, a CLIP-based\n  fine-tuning strategy for motion generation, with results on HumanML3D dataset\n  and ablation studies", "summary": "Human motion generation is essential for fields such as animation, robotics,\nand virtual reality, requiring models that effectively capture motion dynamics\nfrom text descriptions. Existing approaches often rely on Contrastive\nLanguage-Image Pretraining (CLIP)-based text encoders, but their training on\ntext-image pairs constrains their ability to understand temporal and kinematic\nstructures inherent in motion and motion generation. This work introduces\nMoCLIP, a fine-tuned CLIP model with an additional motion encoding head,\ntrained on motion sequences using contrastive learning and tethering loss. By\nexplicitly incorporating motion-aware representations, MoCLIP enhances motion\nfidelity while remaining compatible with existing CLIP-based pipelines and\nseamlessly integrating into various CLIP-based methods. Experiments demonstrate\nthat MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining\ncompetitive FID, leading to improved text-to-motion alignment results. These\nresults highlight MoCLIP's versatility and effectiveness, establishing it as a\nrobust framework for enhancing motion generation.", "AI": {"tldr": "MoCLIP\u662f\u4e00\u79cd\u6539\u8fdb\u7684CLIP\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u52a0\u8fd0\u52a8\u7f16\u7801\u5934\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u6587\u672c\u7f16\u7801\u5668\u5728\u7406\u89e3\u8fd0\u52a8\u52a8\u6001\u548c\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0cMoCLIP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5fae\u8c03CLIP\u6a21\u578b\u5e76\u589e\u52a0\u8fd0\u52a8\u7f16\u7801\u5934\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548ctethering\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "MoCLIP\u63d0\u9ad8\u4e86Top-1\u3001Top-2\u548cTop-3\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684FID\u5206\u6570\u3002", "conclusion": "MoCLIP\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u5347\u8fd0\u52a8\u751f\u6210\u7684\u8d28\u91cf\u548c\u517c\u5bb9\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11031", "pdf": "https://arxiv.org/pdf/2505.11031", "abs": "https://arxiv.org/abs/2505.11031", "authors": ["Xiao Zhang", "Huiyuan Lai", "Qianru Meng", "Johan Bos"], "title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning", "categories": ["cs.CL"], "comment": "Paper submitted to NeruoIPS 2025 dataset and benchmark track", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing tasks, yet their ability to process\nstructured symbolic knowledge remains underexplored. To address this gap, we\npropose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the\nfirst comprehensive benchmark designed to systematically evaluate LLMs'\nproficiency in handling ontologies -- formal, symbolic representations of\ndomain knowledge through concepts, relationships, and instances. Based on the\nproposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n58,981 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing proficiency in understanding\nontological knowledge but substantial weaknesses in reasoning and learning\ntasks. These findings highlight fundamental limitations in LLMs' capability to\nprocess symbolic knowledge and establish OntoURL as a critical benchmark for\nadvancing the integration of LLMs with formal knowledge representations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OntoURL\uff0c\u9996\u4e2a\u5168\u9762\u8bc4\u4f30LLMs\u5904\u7406\u672c\u4f53\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u7b26\u53f7\u77e5\u8bc6\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5904\u7406\u7ed3\u6784\u5316\u7b26\u53f7\u77e5\u8bc6\uff08\u5982\u672c\u4f53\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faLLMs\u672c\u4f53\u80fd\u529b\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1OntoURL\u57fa\u51c6\uff0c\u5305\u542b15\u4e2a\u4efb\u52a1\u300158,981\u4e2a\u95ee\u9898\uff0c\u8986\u76d68\u4e2a\u9886\u57df\u768440\u4e2a\u672c\u4f53\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u672c\u4f53\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u63a8\u7406\u548c\u5b66\u4e60\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "OntoURL\u4e3aLLMs\u4e0e\u5f62\u5f0f\u77e5\u8bc6\u8868\u793a\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLMs\u5904\u7406\u7b26\u53f7\u77e5\u8bc6\u7684\u6839\u672c\u9650\u5236\u3002", "relevance": 85.0}}
{"id": "2505.11122", "pdf": "https://arxiv.org/pdf/2505.11122", "abs": "https://arxiv.org/abs/2505.11122", "authors": ["Yu Shi", "Yitong Duan", "Jian Li"], "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining", "categories": ["cs.AI"], "comment": "30 pages", "summary": "Alpha factor mining is pivotal in quantitative investment for identifying\npredictive signals from complex financial data. While traditional formulaic\nalpha mining relies on human expertise, contemporary automated methods, such as\nthose based on genetic programming or reinforcement learning, often suffer from\nsearch inefficiency or yield poorly interpretable alpha factors. This paper\nintroduces a novel framework that integrates Large Language Models (LLMs) with\nMonte Carlo Tree Search (MCTS) to overcome these limitations. Our approach\nleverages the LLM's instruction-following and reasoning capability to\niteratively generate and refine symbolic alpha formulas within an MCTS-driven\nexploration. A key innovation is the guidance of MCTS exploration by rich,\nquantitative feedback from financial backtesting of each candidate factor,\nenabling efficient navigation of the vast search space. Furthermore, a frequent\nsubtree avoidance mechanism is introduced to bolster search efficiency and\nalpha factor performance. Experimental results on real-world stock market data\ndemonstrate that our LLM-based framework outperforms existing methods by mining\nalphas with superior predictive accuracy, trading performance, and improved\ninterpretability, while offering a more efficient solution for formulaic alpha\nmining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6316\u6398\u53ef\u89e3\u91ca\u7684\u91cf\u5316\u6295\u8d44Alpha\u56e0\u5b50\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u6295\u8d44Alpha\u56e0\u5b50\u6316\u6398\u4f9d\u8d56\u4eba\u5de5\u7ecf\u9a8c\u6216\u81ea\u52a8\u5316\u65b9\u6cd5\uff08\u5982\u9057\u4f20\u7f16\u7a0b\u6216\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u4f46\u5b58\u5728\u641c\u7d22\u6548\u7387\u4f4e\u6216\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528LLM\u7684\u6307\u4ee4\u9075\u5faa\u548c\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408MCTS\u9a71\u52a8\u7684\u63a2\u7d22\uff0c\u901a\u8fc7\u91d1\u878d\u56de\u6d4b\u53cd\u9988\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7b26\u53f7\u5316Alpha\u516c\u5f0f\u3002", "result": "\u5728\u771f\u5b9e\u80a1\u7968\u5e02\u573a\u6570\u636e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u4ea4\u6613\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u516c\u5f0f\u5316Alpha\u6316\u6398\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.10846", "pdf": "https://arxiv.org/pdf/2505.10846", "abs": "https://arxiv.org/abs/2505.10846", "authors": ["Jiacheng Liang", "Tanqiu Jiang", "Yuhui Wang", "Rongyi Zhu", "Fenglong Ma", "Ting Wang"], "title": "AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models", "categories": ["cs.LG", "cs.CR"], "comment": "9 pages", "summary": "This paper presents AutoRAN, the first automated, weak-to-strong jailbreak\nattack framework targeting large reasoning models (LRMs). At its core, AutoRAN\nleverages a weak, less-aligned reasoning model to simulate the target model's\nhigh-level reasoning structures, generates narrative prompts, and iteratively\nrefines candidate prompts by incorporating the target model's intermediate\nreasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including\nGPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets\n(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN\nachieves remarkable success rates (approaching 100%) within one or a few turns\nacross different LRMs, even when judged by a robustly aligned external model.\nThis work reveals that leveraging weak reasoning models can effectively exploit\nthe critical vulnerabilities of much more capable reasoning models,\nhighlighting the need for improved safety measures specifically designed for\nreasoning-based models. The code for replicating AutoRAN and running records\nare available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:\nthis paper contains potentially harmful content generated by LRMs.)", "AI": {"tldr": "AutoRAN\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u5f31\u5230\u5f3a\u7684\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\uff0c\u901a\u8fc7\u5f31\u5bf9\u9f50\u6a21\u578b\u6a21\u62df\u76ee\u6807\u6a21\u578b\u7684\u9ad8\u5c42\u63a8\u7406\u7ed3\u6784\uff0c\u751f\u6210\u5e76\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u6210\u529f\u7387\u8fbe\u5230\u63a5\u8fd1100%\u3002", "motivation": "\u63ed\u793a\u5229\u7528\u5f31\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u653b\u51fb\u66f4\u5f3a\u63a8\u7406\u6a21\u578b\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u9700\u8981\u9488\u5bf9\u63a8\u7406\u6a21\u578b\u8bbe\u8ba1\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u63aa\u65bd\u3002", "method": "\u5229\u7528\u5f31\u5bf9\u9f50\u6a21\u578b\u6a21\u62df\u76ee\u6807\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u6784\uff0c\u751f\u6210\u53d9\u4e8b\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAutoRAN\u5bf9GPT-o3/o4-mini\u548cGemini-2.5-Flash\u7b49LRMs\u7684\u6210\u529f\u7387\u63a5\u8fd1100%\u3002", "conclusion": "\u5f31\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u9ad8\u6548\u653b\u51fb\u66f4\u5f3a\u63a8\u7406\u6a21\u578b\uff0c\u51f8\u663e\u4e86\u63a8\u7406\u6a21\u578b\u5b89\u5168\u63aa\u65bd\u7684\u4e0d\u8db3\u3002", "relevance": 85.0}}
{"id": "2505.10823", "pdf": "https://arxiv.org/pdf/2505.10823", "abs": "https://arxiv.org/abs/2505.10823", "authors": ["Xue Li", "Jameson Merkow", "Noel C. F. Codella", "Alberto Santamaria-Pang", "Naiteek Sangani", "Alexander Ersoy", "Christopher Burt", "John W. Garrett", "Richard J. Bruce", "Joshua D. Warner", "Tyler Bradshaw", "Ivan Tarapov", "Matthew P. Lungren", "Alan B. McMillan"], "title": "From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Foundation models, pretrained on extensive datasets, have significantly\nadvanced machine learning by providing robust and transferable embeddings\napplicable to various domains, including medical imaging diagnostics. This\nstudy evaluates the utility of embeddings derived from both general-purpose and\nmedical domain-specific foundation models for training lightweight adapter\nmodels in multi-class radiography classification, focusing specifically on tube\nplacement assessment. A dataset comprising 8842 radiographs classified into\nseven distinct categories was employed to extract embeddings using six\nfoundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,\nRad-DINO, and CXR-Foundation. Adapter models were subsequently trained using\nclassical machine learning algorithms. Among these combinations,\nMedImageInsight embeddings paired with an support vector machine adapter\nyielded the highest mean area under the curve (mAUC) at 93.8%, followed closely\nby Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and\nDenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,\nrespectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.\nNotably, most adapter models demonstrated computational efficiency, achieving\ntraining within one minute and inference within seconds on CPU, underscoring\ntheir practicality for clinical applications. Furthermore, fairness analyses on\nadapters trained on MedImageInsight-derived embeddings indicated minimal\ndisparities, with gender differences in performance within 2% and standard\ndeviations across age groups not exceeding 3%. These findings confirm that\nfoundation model embeddings-especially those from MedImageInsight-facilitate\naccurate, computationally efficient, and equitable diagnostic classification\nusing lightweight adapters for radiographic image analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u901a\u7528\u548c\u533b\u5b66\u4e13\u7528\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u5728\u653e\u5c04\u5b66\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0MedImageInsight\u5d4c\u5165\u4e0e\u652f\u6301\u5411\u91cf\u673a\u9002\u914d\u5668\u7ec4\u5408\u6548\u679c\u6700\u4f73\uff0c\u540c\u65f6\u5177\u6709\u9ad8\u6548\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u5728\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u7684\u8868\u73b0\u548c\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u7528\u516d\u79cd\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u5d4c\u5165\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8fdb\u884c\u591a\u7c7b\u653e\u5c04\u5b66\u5206\u7c7b\uff0c\u8bc4\u4f30\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "result": "MedImageInsight\u5d4c\u5165\u8868\u73b0\u6700\u4f73\uff08mAUC 93.8%\uff09\uff0c\u9002\u914d\u5668\u8bad\u7ec3\u548c\u63a8\u7406\u9ad8\u6548\uff0c\u516c\u5e73\u6027\u5206\u6790\u663e\u793a\u5dee\u5f02\u5c0f\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5d4c\u5165\uff08\u5c24\u5176\u662fMedImageInsight\uff09\u652f\u6301\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u516c\u5e73\u7684\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u3002", "relevance": 60.0}}
{"id": "2505.11051", "pdf": "https://arxiv.org/pdf/2505.11051", "abs": "https://arxiv.org/abs/2505.11051", "authors": ["Iwona Christop", "Maciej Czajka"], "title": "CAMEO: Collection of Multilingual Emotional Speech Corpora", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Under review at NeurIPS", "summary": "This paper presents CAMEO -- a curated collection of multilingual emotional\nspeech datasets designed to facilitate research in emotion recognition and\nother speech-related tasks. The main objectives were to ensure easy access to\nthe data, to allow reproducibility of the results, and to provide a\nstandardized benchmark for evaluating speech emotion recognition (SER) systems\nacross different emotional states and languages. The paper describes the\ndataset selection criteria, the curation and normalization process, and\nprovides performance results for several models. The collection, along with\nmetadata, and a leaderboard, is publicly available via the Hugging Face\nplatform.", "AI": {"tldr": "CAMEO\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6\u96c6\u5408\uff0c\u65e8\u5728\u4fc3\u8fdb\u60c5\u611f\u8bc6\u522b\u548c\u5176\u4ed6\u8bed\u97f3\u76f8\u5173\u4efb\u52a1\u7684\u7814\u7a76\u3002", "motivation": "\u63d0\u4f9b\u6613\u4e8e\u8bbf\u95ee\u7684\u6570\u636e\u3001\u786e\u4fdd\u7ed3\u679c\u53ef\u590d\u73b0\uff0c\u5e76\u4e3a\u8de8\u8bed\u8a00\u548c\u60c5\u611f\u72b6\u6001\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u63cf\u8ff0\u4e86\u6570\u636e\u96c6\u9009\u62e9\u6807\u51c6\u3001\u6574\u7406\u548c\u6807\u51c6\u5316\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u7ed3\u679c\u3002", "result": "\u6570\u636e\u96c6\u3001\u5143\u6570\u636e\u548c\u6392\u884c\u699c\u5df2\u901a\u8fc7Hugging Face\u5e73\u53f0\u516c\u5f00\u3002", "conclusion": "CAMEO\u4e3a\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u548c\u591a\u8bed\u8a00\u7684\u6570\u636e\u652f\u6301\u3002", "relevance": 30.0}}
{"id": "2505.11135", "pdf": "https://arxiv.org/pdf/2505.11135", "abs": "https://arxiv.org/abs/2505.11135", "authors": ["Patrick St\u00f6ckermann", "Henning S\u00fcdfeld", "Alessandro Immordino", "Thomas Altenm\u00fcller", "Marc Wegmann", "Martin Gebser", "Konstantin Schekotihin", "Georg Seidel", "Chew Wye Chan", "Fei Fei Zhang"], "title": "Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets", "categories": ["cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Benchmark datasets are crucial for evaluating approaches to scheduling or\ndispatching in the semiconductor industry during the development and deployment\nphases. However, commonly used benchmark datasets like the Minifab or SMT2020\nlack the complex details and constraints found in real-world scenarios. To\nmitigate this shortcoming, we compare open-source simulation models with a real\nindustry dataset to evaluate how optimization methods scale with different\nlevels of complexity. Specifically, we focus on Reinforcement Learning methods,\nperforming optimization based on policy-gradient and Evolution Strategies. Our\nresearch provides insights into the effectiveness of these optimization methods\nand their applicability to realistic semiconductor frontend fab simulations. We\nshow that our proposed Evolution Strategies-based method scales much better\nthan a comparable policy-gradient-based approach. Moreover, we identify the\nselection and combination of relevant bottleneck tools to control by the agent\nas crucial for an efficient optimization. For the generalization across\ndifferent loading scenarios and stochastic tool failure patterns, we achieve\nadvantages when utilizing a diverse training dataset. While the overall\napproach is computationally expensive, it manages to scale well with the number\nof CPU cores used for training. For the real industry dataset, we achieve an\nimprovement of up to 4% regarding tardiness and up to 1% regarding throughput.\nFor the less complex open-source models Minifab and SMT2020, we observe\ndouble-digit percentage improvement in tardiness and single digit percentage\nimprovement in throughput by use of Evolution Strategies.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u7b56\u7565\u68af\u5ea6\u548c\u8fdb\u5316\u7b56\u7565\uff09\u5728\u534a\u5bfc\u4f53\u884c\u4e1a\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8fdb\u5316\u7b56\u7565\u5728\u590d\u6742\u6027\u548c\u6269\u5c55\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982Minifab\u6216SMT2020\uff09\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u9700\u8bc4\u4f30\u4f18\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u7b56\u7565\u68af\u5ea6\u548c\u8fdb\u5316\u7b56\u7565\uff09\u4f18\u5316\u534a\u5bfc\u4f53\u524d\u7aef\u5de5\u5382\u6a21\u62df\uff0c\u5e76\u5206\u6790\u5176\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "result": "\u8fdb\u5316\u7b56\u7565\u5728\u590d\u6742\u6027\u548c\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u7b56\u7565\u68af\u5ea6\uff0c\u5bf9\u771f\u5b9e\u6570\u636e\u96c6\u63d0\u53474%\u7684\u5ef6\u8fdf\u548c1%\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u8fdb\u5316\u7b56\u7565\u5728\u590d\u6742\u8c03\u5ea6\u95ee\u9898\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "relevance": 40.0}}
{"id": "2505.10848", "pdf": "https://arxiv.org/pdf/2505.10848", "abs": "https://arxiv.org/abs/2505.10848", "authors": ["Justin Sanders", "Melih Yilmaz", "Jacob H. Russell", "Wout Bittremieux", "William E. Fondrie", "Nicholas M. Riley", "Sewoong Oh", "William Stafford Noble"], "title": "Foundation model for mass spectrometry proteomics", "categories": ["cs.LG"], "comment": null, "summary": "Mass spectrometry is the dominant technology in the field of proteomics,\nenabling high-throughput analysis of the protein content of complex biological\nsamples. Due to the complexity of the instrumentation and resulting data,\nsophisticated computational methods are required for the processing and\ninterpretation of acquired mass spectra. Machine learning has shown great\npromise to improve the analysis of mass spectrometry data, with numerous\npurpose-built methods for improving specific steps in the data acquisition and\nanalysis pipeline reaching widespread adoption. Here, we propose unifying\nvarious spectrum prediction tasks under a single foundation model for mass\nspectra. To this end, we pre-train a spectrum encoder using de novo sequencing\nas a pre-training task. We then show that using these pre-trained spectrum\nrepresentations improves our performance on the four downstream tasks of\nspectrum quality prediction, chimericity prediction, phosphorylation\nprediction, and glycosylation status prediction. Finally, we perform multi-task\nfine-tuning and find that this approach improves the performance on each task\nindividually. Overall, our work demonstrates that a foundation model for tandem\nmass spectrometry proteomics trained on de novo sequencing learns generalizable\nrepresentations of spectra, improves performance on downstream tasks where\ntraining data is limited, and can ultimately enhance data acquisition and\nanalysis in proteomics experiments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d28\u8c31\u6570\u636e\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u8d28\u8c31\u6570\u636e\u7684\u590d\u6742\u6027\u9700\u8981\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u673a\u5668\u5b66\u4e60\u5728\u6b64\u9886\u57df\u663e\u793a\u51fa\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u7b80\u5316\u5e76\u6539\u8fdb\u8d28\u8c31\u6570\u636e\u5206\u6790\u3002", "method": "\u9884\u8bad\u7ec3\u4e00\u4e2a\u8d28\u8c31\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u4ece\u5934\u6d4b\u5e8f\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u5728\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u9884\u8bad\u7ec3\u7684\u8d28\u8c31\u8868\u793a\u63d0\u5347\u4e86\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u591a\u4efb\u52a1\u5fae\u8c03\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u7ed3\u679c\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u901a\u7528\u7684\u8d28\u8c31\u8868\u793a\uff0c\u63d0\u5347\u6570\u636e\u6709\u9650\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f18\u5316\u8d28\u8c31\u5b9e\u9a8c\u7684\u6570\u636e\u83b7\u53d6\u4e0e\u5206\u6790\u3002", "relevance": 20.0}}
{"id": "2505.10825", "pdf": "https://arxiv.org/pdf/2505.10825", "abs": "https://arxiv.org/abs/2505.10825", "authors": ["Jinke Li", "Yue Wu", "Xiaoyan Yang"], "title": "A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation", "categories": ["cs.CV", "cs.LG"], "comment": "This manuscript has been accepted for publication in the\n  International Journal for Housing Science and Its Applications (IJHSA), 2025", "summary": "Thermal Infrared (TIR) technology involves the use of sensors to detect and\nmeasure infrared radiation emitted by objects, and it is widely utilized across\na broad spectrum of applications. The advancements in object detection methods\nutilizing TIR images have sparked significant research interest. However, most\ntraditional methods lack the capability to effectively extract and fuse\nlocal-global information, which is crucial for TIR-domain feature attention. In\nthis study, we present a novel and efficient thermal infrared object detection\nframework, known as CRT-YOLO, that is based on centralized feature regulation,\nenabling the establishment of global-range interaction on TIR information. Our\nproposed model integrates efficient multi-scale attention (EMA) modules, which\nadeptly capture long-range dependencies while incurring minimal computational\noverhead. Additionally, it leverages the Centralized Feature Pyramid (CFP)\nnetwork, which offers global regulation of TIR features. Extensive experiments\nconducted on two benchmark datasets demonstrate that our CRT-YOLO model\nsignificantly outperforms conventional methods for TIR image object detection.\nFurthermore, the ablation study provides compelling evidence of the\neffectiveness of our proposed modules, reinforcing the potential impact of our\napproach on advancing the field of thermal infrared object detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u4e2d\u7279\u5f81\u8c03\u8282\u7684\u65b0\u578b\u70ed\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\u6846\u67b6CRT-YOLO\uff0c\u901a\u8fc7\u9ad8\u6548\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\u548c\u96c6\u4e2d\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u70ed\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u63d0\u53d6\u548c\u878d\u5408\u5c40\u90e8-\u5168\u5c40\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u7279\u5f81\u6ce8\u610f\u529b\u6548\u679c\u3002", "method": "\u63d0\u51faCRT-YOLO\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u6548\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\uff08EMA\uff09\u548c\u96c6\u4e2d\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08CFP\uff09\uff0c\u5b9e\u73b0\u5168\u5c40\u8303\u56f4\u7684\u70ed\u7ea2\u5916\u4fe1\u606f\u4ea4\u4e92\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRT-YOLO\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "CRT-YOLO\u901a\u8fc7\u5168\u5c40\u7279\u5f81\u8c03\u8282\u548c\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u70ed\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 20.0}}
{"id": "2505.11080", "pdf": "https://arxiv.org/pdf/2505.11080", "abs": "https://arxiv.org/abs/2505.11080", "authors": ["Yapei Chang", "Yekyung Kim", "Michael Krumdick", "Amir Zadeh", "Chuan Li", "Chris Tanner", "Mohit Iyyer"], "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 11 figures, 15 tables", "summary": "Reward models are central to aligning LLMs with human preferences, but they\nare costly to train, requiring large-scale human-labeled preference data and\npowerful pretrained LLM backbones. Meanwhile, the increasing availability of\nhigh-quality synthetic instruction-following datasets raises the question: can\nsimpler, reference-based metrics serve as viable alternatives to reward models\nduring RL-based alignment? In this paper, we show first that BLEU, a basic\nstring-matching metric, surprisingly matches strong reward models in agreement\nwith human preferences on general instruction-following datasets. Based on this\ninsight, we develop BLEUBERI, a method that first identifies challenging\ninstructions and then applies Group Relative Policy Optimization (GRPO) using\nBLEU directly as the reward function. We demonstrate that BLEUBERI-trained\nmodels are competitive with models trained via reward model-guided RL across\nfour challenging instruction-following benchmarks and three different base\nlanguage models. A human evaluation further supports that the quality of\nBLEUBERI model outputs is on par with those from reward model-aligned models.\nMoreover, BLEUBERI models generate outputs that are more factually grounded\nthan competing methods. Overall, we show that given access to high-quality\nreference outputs (easily obtained via existing instruction-following datasets\nor synthetic data generation), string matching-based metrics are cheap yet\neffective proxies for reward models during alignment. We release our code and\ndata at https://github.com/lilakk/BLEUBERI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBLEU\u6307\u6807\u7684\u66ff\u4ee3\u65b9\u6cd5BLEUBERI\uff0c\u7528\u4e8eLLM\u5bf9\u9f50\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u800c\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u7684\u53ef\u7528\u6027\u589e\u52a0\uff0c\u4fc3\u4f7f\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u7528\u66f4\u7b80\u5355\u7684\u53c2\u8003\u6307\u6807\u66ff\u4ee3\u5956\u52b1\u6a21\u578b\u3002", "method": "\u4f7f\u7528BLEU\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408GRPO\u65b9\u6cd5\uff0c\u8bc6\u522b\u6311\u6218\u6027\u6307\u4ee4\u5e76\u4f18\u5316\u6a21\u578b\u3002", "result": "BLEUBERI\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u5956\u52b1\u6a21\u578b\u5bf9\u9f50\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u751f\u6210\u5185\u5bb9\u66f4\u4e8b\u5b9e\u51c6\u786e\u3002", "conclusion": "\u57fa\u4e8e\u5b57\u7b26\u4e32\u5339\u914d\u7684\u6307\u6807\u662f\u5956\u52b1\u6a21\u578b\u7684\u6709\u6548\u5ec9\u4ef7\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.11136", "pdf": "https://arxiv.org/pdf/2505.11136", "abs": "https://arxiv.org/abs/2505.11136", "authors": ["Janik Bischoff", "Alexandru Rinciog", "Anne Meyer"], "title": "Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design", "categories": ["cs.AI", "cs.RO"], "comment": "Under review LION19: The 19th Learning and Intelligent OptimizatioN\n  Conference", "summary": "We propose a novel reinforcement learning (RL) design to optimize the\ncharging strategy for autonomous mobile robots in large-scale block stacking\nwarehouses. RL design involves a wide array of choices that can mostly only be\nevaluated through lengthy experimentation. Our study focuses on how different\nreward and action space configurations, ranging from flexible setups to more\nguided, domain-informed design configurations, affect the agent performance.\nUsing heuristic charging strategies as a baseline, we demonstrate the\nsuperiority of flexible, RL-based approaches in terms of service times.\nFurthermore, our findings highlight a trade-off: While more open-ended designs\nare able to discover well-performing strategies on their own, they may require\nlonger convergence times and are less stable, whereas guided configurations\nlead to a more stable learning process but display a more limited\ngeneralization potential. Our contributions are threefold. First, we extend\nSLAPStack, an open-source, RL-compatible simulation-framework to accommodate\ncharging strategies. Second, we introduce a novel RL design for tackling the\ncharging strategy problem. Finally, we introduce several novel adaptive\nbaseline heuristics and reproducibly evaluate the design using a Proximal\nPolicy Optimization agent and varying different design configurations, with a\nfocus on reward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bbe\u8ba1\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u5757\u5806\u53e0\u4ed3\u5e93\u4e2d\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5145\u7535\u7b56\u7565\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u5956\u52b1\u548c\u52a8\u4f5c\u7a7a\u95f4\u914d\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u901a\u8fc7RL\u4f18\u5316\u673a\u5668\u4eba\u5145\u7535\u7b56\u7565\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9700\u8981\u901a\u8fc7\u5197\u957f\u5b9e\u9a8c\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6269\u5c55SLAPStack\u4eff\u771f\u6846\u67b6\u4ee5\u652f\u6301\u5145\u7535\u7b56\u7565\uff0c\u63d0\u51fa\u65b0\u7684RL\u8bbe\u8ba1\uff0c\u5e76\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u4ee3\u7406\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7075\u6d3b\u7684RL\u65b9\u6cd5\u5728\u670d\u52a1\u65f6\u95f4\u4e0a\u4f18\u4e8e\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u4f46\u5b58\u5728\u6536\u655b\u65f6\u95f4\u957f\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff1b\u800c\u5f15\u5bfc\u5f0f\u914d\u7f6e\u5219\u66f4\u7a33\u5b9a\u4f46\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u7ed3\u8bba\u662fRL\u5728\u5145\u7535\u7b56\u7565\u4f18\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6743\u8861\u7075\u6d3b\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "relevance": 40.0}}
{"id": "2505.10856", "pdf": "https://arxiv.org/pdf/2505.10856", "abs": "https://arxiv.org/abs/2505.10856", "authors": ["Mengxuan Li", "Ke Liu", "Jialong Guo", "Jiajun Bu", "Hongwei Wang", "Haishuai Wang"], "title": "ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Healthcare data frequently contain a substantial proportion of missing\nvalues, necessitating effective time series imputation to support downstream\ndisease diagnosis tasks. However, existing imputation methods focus on discrete\ndata points and are unable to effectively model sparse data, resulting in\nparticularly poor performance for imputing substantial missing values. In this\npaper, we propose a novel approach, ImputeINR, for time series imputation by\nemploying implicit neural representations (INR) to learn continuous functions\nfor time series. ImputeINR leverages the merits of INR in that the continuous\nfunctions are not coupled to sampling frequency and have infinite sampling\nfrequency, allowing ImputeINR to generate fine-grained imputations even on\nextremely sparse observed values. Extensive experiments conducted on eight\ndatasets with five ratios of masked values show the superior imputation\nperformance of ImputeINR, especially for high missing ratios in time series\ndata. Furthermore, we validate that applying ImputeINR to impute missing values\nin healthcare data enhances the performance of downstream disease diagnosis\ntasks. Codes are available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aImputeINR\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u7eed\u51fd\u6570\uff0c\u6709\u6548\u5904\u7406\u9ad8\u7f3a\u5931\u7387\u6570\u636e\u3002", "motivation": "\u533b\u7597\u6570\u636e\u4e2d\u7f3a\u5931\u503c\u6bd4\u4f8b\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u7a00\u758f\u6570\u636e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u7eed\u51fd\u6570\uff0c\u4e0d\u4f9d\u8d56\u91c7\u6837\u9891\u7387\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea6\u586b\u8865\u3002", "result": "\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86ImputeINR\u7684\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u5728\u9ad8\u7f3a\u5931\u7387\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "ImputeINR\u4e3a\u65f6\u95f4\u5e8f\u5217\u586b\u8865\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u533b\u7597\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "relevance": 30.0}}
{"id": "2505.10827", "pdf": "https://arxiv.org/pdf/2505.10827", "abs": "https://arxiv.org/abs/2505.10827", "authors": ["Nail Ibrahimli", "Julian F. P. Kooij", "Liangliang Nan"], "title": "NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits", "categories": ["cs.CV"], "comment": null, "summary": "Implicit surface representations are valued for their compactness and\ncontinuity, but they pose significant challenges for editing. Despite recent\nadvancements, existing methods often fail to preserve identity and maintain\ngeometric consistency during editing. To address these challenges, we present\nNeuSEditor, a novel method for text-guided editing of neural implicit surfaces\nderived from multi-view images. NeuSEditor introduces an identity-preserving\narchitecture that efficiently separates scenes into foreground and background,\nenabling precise modifications without altering the scene-specific elements.\nOur geometry-aware distillation loss significantly enhances rendering and\ngeometric quality. Our method simplifies the editing workflow by eliminating\nthe need for continuous dataset updates and source prompting. NeuSEditor\noutperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF,\ndelivering superior quantitative and qualitative results. For more visual\nresults, visit: neuseditor.github.io.", "AI": {"tldr": "NeuSEditor\u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u7f16\u8f91\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8eab\u4efd\u4fdd\u7559\u67b6\u6784\u548c\u51e0\u4f55\u611f\u77e5\u84b8\u998f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u9690\u5f0f\u8868\u9762\u8868\u793a\u65b9\u6cd5\u5728\u7f16\u8f91\u65f6\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0cNeuSEditor\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "NeuSEditor\u91c7\u7528\u8eab\u4efd\u4fdd\u7559\u67b6\u6784\u5206\u79bb\u524d\u666f\u548c\u80cc\u666f\uff0c\u5e76\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u84b8\u998f\u635f\u5931\uff0c\u65e0\u9700\u6301\u7eed\u66f4\u65b0\u6570\u636e\u96c6\u6216\u6e90\u63d0\u793a\u3002", "result": "NeuSEditor\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u4f18\u4e8ePDS\u548cInstructNeRF2NeRF\u7b49\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "NeuSEditor\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u8868\u9762\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.11095", "pdf": "https://arxiv.org/pdf/2505.11095", "abs": "https://arxiv.org/abs/2505.11095", "authors": ["Lekang Jiang", "Pascal A Scherz", "Stephan Goetz"], "title": "Towards Better Evaluation for Generated Patent Claims", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. 14 pages, 8 tables", "summary": "Patent claims define the scope of protection and establish the legal\nboundaries of an invention. Drafting these claims is a complex and\ntime-consuming process that usually requires the expertise of skilled patent\nattorneys, which can form a large access barrier for many small enterprises. To\nsolve these challenges, researchers have investigated the use of large language\nmodels (LLMs) for automating patent claim generation. However, existing studies\nhighlight inconsistencies between automated evaluation metrics and human expert\nassessments. To bridge this gap, we introduce Patent-CE, the first\ncomprehensive benchmark for evaluating patent claims. Patent-CE includes\ncomparative claim evaluations annotated by patent experts, focusing on five key\ncriteria: feature completeness, conceptual clarity, terminology consistency,\nlogical linkage, and overall quality. Additionally, we propose PatClaimEval, a\nnovel multi-dimensional evaluation method specifically designed for patent\nclaims. Our experiments demonstrate that PatClaimEval achieves the highest\ncorrelation with human expert evaluations across all assessment criteria among\nall tested metrics. This research provides the groundwork for more accurate\nevaluations of automated patent claim generation systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPatent-CE\u57fa\u51c6\u548cPatClaimEval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4e13\u5229\u6743\u5229\u8981\u6c42\u751f\u6210\u7684\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u4e13\u5229\u6743\u5229\u8981\u6c42\u8d77\u8349\u590d\u6742\u4e14\u8017\u65f6\uff0c\u73b0\u6709LLM\u81ea\u52a8\u5316\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8bc4\u4f30\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5f15\u5165Patent-CE\u57fa\u51c6\uff08\u4e13\u5bb6\u6807\u6ce8\u7684\u4e94\u7ef4\u8bc4\u4f30\uff09\u548cPatClaimEval\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "PatClaimEval\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\u4e2d\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u76f8\u5173\u6027\u6700\u9ad8\u3002", "conclusion": "\u4e3a\u81ea\u52a8\u5316\u4e13\u5229\u6743\u5229\u8981\u6c42\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u57fa\u7840\u3002", "relevance": 50.0}}
{"id": "2505.11181", "pdf": "https://arxiv.org/pdf/2505.11181", "abs": "https://arxiv.org/abs/2505.11181", "authors": ["Jae Myung Kim", "Stephan Alaniz", "Cordelia Schmid", "Zeynep Akata"], "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning", "categories": ["cs.AI"], "comment": "ECCV Workshop in OOD-CV, 2024", "summary": "Humans can easily tell if an attribute (also called state) is realistic,\ni.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In\nOpen-World Compositional Zero-Shot Learning, when all possible state-object\ncombinations are considered as unseen classes, zero-shot predictors tend to\nperform poorly. Our work focuses on using external auxiliary knowledge to\ndetermine the feasibility of state-object combinations. Our Feasibility with\nLanguage Model (FLM) is a simple and effective approach that leverages Large\nLanguage Models (LLMs) to better comprehend the semantic relationships between\nstates and objects. FLM involves querying an LLM about the feasibility of a\ngiven pair and retrieving the output logit for the positive answer. To mitigate\npotential misguidance of the LLM given that many of the state-object\ncompositions are rare or completely infeasible, we observe that the in-context\nlearning ability of LLMs is essential. We present an extensive study\nidentifying Vicuna and ChatGPT as best performing, and we demonstrate that our\nFLM consistently improves OW-CZSL performance across all three benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5224\u65ad\u72b6\u6001-\u5bf9\u8c61\u7ec4\u5408\u53ef\u884c\u6027\u7684\u65b9\u6cd5\uff08FLM\uff09\uff0c\u901a\u8fc7\u67e5\u8be2LLMs\u5e76\u5229\u7528\u5176\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\uff08OW-CZSL\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u96f6\u6837\u672c\u9884\u6d4b\u5668\u5bf9\u72b6\u6001-\u5bf9\u8c61\u7ec4\u5408\u7684\u53ef\u884c\u6027\u5224\u65ad\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u501f\u52a9\u5916\u90e8\u77e5\u8bc6\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faFLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u67e5\u8be2LLMs\u83b7\u53d6\u72b6\u6001-\u5bf9\u8c61\u7ec4\u5408\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5229\u7528\u5176\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u4f18\u5316\u7ed3\u679c\u3002", "result": "FLM\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86OW-CZSL\u7684\u6027\u80fd\uff0c\u5176\u4e2dVicuna\u548cChatGPT\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "FLM\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5229\u7528LLMs\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u663e\u8457\u6539\u5584\u4e86\u72b6\u6001-\u5bf9\u8c61\u7ec4\u5408\u7684\u53ef\u884c\u6027\u5224\u65ad\u3002", "relevance": 70.0}}
{"id": "2505.10860", "pdf": "https://arxiv.org/pdf/2505.10860", "abs": "https://arxiv.org/abs/2505.10860", "authors": ["Huy Nguyen", "Thong T. Doan", "Quang Pham", "Nghi D. Q. Bui", "Nhat Ho", "Alessandro Rinaldo"], "title": "On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating", "categories": ["cs.LG", "stat.ML"], "comment": "100 pages", "summary": "Mixture of experts (MoE) methods are a key component in most large language\nmodel architectures, including the recent series of DeepSeek models. Compared\nto other MoE implementations, DeepSeekMoE stands out because of two unique\nfeatures: the deployment of a shared expert strategy and of the normalized\nsigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the\nsuccess of the DeepSeek series of models, there have been only a few attempts\nto justify theoretically the value of the shared expert strategy, while its\nnormalized sigmoid gating has remained unexplored. To bridge this gap, we\nundertake a comprehensive theoretical study of these two features of\nDeepSeekMoE from a statistical perspective. We perform a convergence analysis\nof the expert estimation task to highlight the gains in sample efficiency for\nboth the shared expert strategy and the normalized sigmoid gating, offering\nuseful insights into the design of expert and gating structures. To verify\nempirically our theoretical findings, we carry out several experiments on both\nsynthetic data and real-world datasets for (vision) language modeling tasks.\nFinally, we conduct an extensive empirical analysis of the router behaviors,\nranging from router saturation, router change rate, to expert utilization.", "AI": {"tldr": "\u672c\u6587\u5bf9DeepSeekMoE\u4e2d\u7684\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u548c\u5f52\u4e00\u5316Sigmoid\u95e8\u63a7\u673a\u5236\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6837\u672c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u8def\u7531\u884c\u4e3a\u5206\u6790\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "motivation": "\u5c3d\u7ba1DeepSeekMoE\u5728\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5176\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u548c\u5f52\u4e00\u5316Sigmoid\u95e8\u63a7\u673a\u5236\u7684\u7406\u8bba\u4ef7\u503c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7edf\u8ba1\u89c6\u89d2\u5bf9\u5171\u4eab\u4e13\u5bb6\u7b56\u7565\u548c\u5f52\u4e00\u5316Sigmoid\u95e8\u63a7\u8fdb\u884c\u6536\u655b\u5206\u6790\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8fd9\u4e24\u79cd\u673a\u5236\u80fd\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u5b9e\u9a8c\u548c\u8def\u7531\u884c\u4e3a\u5206\u6790\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e13\u5bb6\u548c\u95e8\u63a7\u7ed3\u6784\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u9a8c\u8bc1\u4e86DeepSeekMoE\u7684\u4f18\u8d8a\u6027\u3002", "relevance": 85.0}}
{"id": "2505.10841", "pdf": "https://arxiv.org/pdf/2505.10841", "abs": "https://arxiv.org/abs/2505.10841", "authors": ["Jaeguk Kim", "Jaewoo Park", "Keuntek Lee", "Nam Ik Cho"], "title": "RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Estimating the 6D pose of unseen objects from monocular RGB images remains a\nchallenging problem, especially due to the lack of prior object-specific\nknowledge. To tackle this issue, we propose RefPose, an innovative approach to\nobject pose estimation that leverages a reference image and geometric\ncorrespondence as guidance. RefPose first predicts an initial pose by using\nobject templates to render the reference image and establish the geometric\ncorrespondence needed for the refinement stage. During the refinement stage,\nRefPose estimates the geometric correspondence of the query based on the\ngenerated references and iteratively refines the pose through a\nrender-and-compare approach. To enhance this estimation, we introduce a\ncorrelation volume-guided attention mechanism that effectively captures\ncorrelations between the query and reference images. Unlike traditional methods\nthat depend on pre-defined object models, RefPose dynamically adapts to new\nobject shapes by leveraging a reference image and geometric correspondence.\nThis results in robust performance across previously unseen objects. Extensive\nevaluation on the BOP benchmark datasets shows that RefPose achieves\nstate-of-the-art results while maintaining a competitive runtime.", "AI": {"tldr": "RefPose\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u548c\u51e0\u4f55\u5bf9\u5e94\u76846D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u65b0\u7269\u4f53\u5f62\u72b6\uff0c\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5355\u76eeRGB\u56fe\u50cf\u4e2d\u672a\u89c1\u7269\u4f536D\u59ff\u6001\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7269\u4f53\u6a21\u578b\uff0c\u800cRefPose\u901a\u8fc7\u53c2\u8003\u56fe\u50cf\u548c\u51e0\u4f55\u5bf9\u5e94\u52a8\u6001\u9002\u5e94\u65b0\u7269\u4f53\u3002", "method": "RefPose\u5206\u4e3a\u521d\u59cb\u59ff\u6001\u9884\u6d4b\u548c\u7ec6\u5316\u9636\u6bb5\uff0c\u5229\u7528\u7269\u4f53\u6a21\u677f\u6e32\u67d3\u53c2\u8003\u56fe\u50cf\u5efa\u7acb\u51e0\u4f55\u5bf9\u5e94\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3-\u6bd4\u8f83\u65b9\u6cd5\u8fed\u4ee3\u4f18\u5316\u59ff\u6001\uff0c\u5f15\u5165\u76f8\u5173\u6027\u4f53\u79ef\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728BOP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "RefPose\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u65b0\u7269\u4f53\u5f62\u72b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u7269\u4f53\u76846D\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.11140", "pdf": "https://arxiv.org/pdf/2505.11140", "abs": "https://arxiv.org/abs/2505.11140", "authors": ["Mike Zhang", "Johannes Bjerva", "Russa Biswas"], "title": "Scaling Reasoning can Improve Factuality in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u957f\u63a8\u7406\u94fe\u662f\u5426\u63d0\u5347LLM\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u84b8\u998f\u63a8\u7406\u8f68\u8ff9\u548c\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\uff0c\u53d1\u73b0\u5c0f\u6a21\u578b\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4e14\u6d4b\u8bd5\u65f6\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u957f\u63a8\u7406\u94fe\u662f\u5426\u5728\u6570\u5b66\u63a8\u7406\u4e4b\u5916\u7684\u4efb\u52a1\u4e2d\u63d0\u5347LLM\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u57df\u95ee\u7b54\u573a\u666f\u3002", "method": "\u84b8\u998f\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\uff0c\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5c0f\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u6d4b\u8bd5\u65f6\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd2-8%\u3002", "conclusion": "\u957f\u63a8\u7406\u94fe\u548c\u6d4b\u8bd5\u65f6\u8d44\u6e90\u6269\u5c55\u80fd\u6709\u6548\u63d0\u5347\u5f00\u653e\u57df\u95ee\u7b54\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002", "relevance": 85.0}}
{"id": "2505.11189", "pdf": "https://arxiv.org/pdf/2505.11189", "abs": "https://arxiv.org/abs/2505.11189", "authors": ["Francesco Sovrano"], "title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Generative AI systems can help spread information but also misinformation and\nbiases, potentially undermining the UN Sustainable Development Goals (SDGs).\nExplainable AI (XAI) aims to reveal the inner workings of AI systems and expose\nmisbehaviours or biases. However, current XAI tools, built for simpler models,\nstruggle to handle the non-numerical nature of large language models (LLMs).\nThis paper examines the effectiveness of global XAI methods, such as\nrule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we\nfirst show a text-to-ordinal mapping strategy to convert non-numerical\ninputs/outputs into numerical features, enabling these tools to identify (some)\nmisinformation-related biases in LLM-generated content. Then, we inject\nnon-linear biases of varying complexity (univariate, conjunctive, and\nnon-convex) into widespread LLMs like ChatGPT and Llama via system\ninstructions, using global XAI methods to detect them. This way, we found that\nRuleFit struggles with conjunctive and non-convex biases, while SHAP can\napproximate conjunctive biases but cannot express them as actionable rules.\nHence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP\nand RuleFit to detect more non-univariate biases, improving injected bias\ndetection over RuleFit by +94% (MRR@1) on average.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5168\u5c40XAI\u65b9\u6cd5\u68c0\u6d4bLLM\u4e2d\u7684\u504f\u89c1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u672c\u5230\u5e8f\u6570\u7684\u6620\u5c04\u7b56\u7565\uff0c\u5e76\u5f15\u5165RuleSHAP\u7b97\u6cd5\u4ee5\u6539\u8fdb\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7cfb\u7edf\u53ef\u80fd\u4f20\u64ad\u504f\u89c1\u548c\u9519\u8bef\u4fe1\u606f\uff0c\u800c\u73b0\u6709XAI\u5de5\u5177\u96be\u4ee5\u5904\u7406LLM\u7684\u975e\u6570\u503c\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6587\u672c\u5230\u5e8f\u6570\u7684\u6620\u5c04\u7b56\u7565\u5c06\u975e\u6570\u503c\u8f93\u5165/\u8f93\u51fa\u8f6c\u6362\u4e3a\u6570\u503c\u7279\u5f81\uff0c\u5e76\u6ce8\u5165\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u504f\u89c1\uff0c\u4f7f\u7528RuleFit\u548cSHAP\u7b49\u5de5\u5177\u68c0\u6d4b\uff0c\u6700\u7ec8\u63d0\u51faRuleSHAP\u7b97\u6cd5\u3002", "result": "RuleFit\u5728\u68c0\u6d4b\u590d\u6742\u504f\u89c1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cSHAP\u80fd\u8fd1\u4f3c\u68c0\u6d4b\u4f46\u65e0\u6cd5\u751f\u6210\u53ef\u64cd\u4f5c\u89c4\u5219\uff0cRuleSHAP\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\uff08MRR@1\u5e73\u5747\u63d0\u534794%\uff09\u3002", "conclusion": "RuleSHAP\u7ed3\u5408\u4e86SHAP\u548cRuleFit\u7684\u4f18\u52bf\uff0c\u80fd\u66f4\u6709\u6548\u5730\u68c0\u6d4bLLM\u4e2d\u7684\u975e\u5355\u53d8\u91cf\u504f\u89c1\uff0c\u4e3aXAI\u5de5\u5177\u5728LLM\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.10861", "pdf": "https://arxiv.org/pdf/2505.10861", "abs": "https://arxiv.org/abs/2505.10861", "authors": ["Thang Duong", "Minglai Yang", "Chicheng Zhang"], "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM", "categories": ["cs.LG"], "comment": "31 pages (9 for the main paper), 27 figures, NeurIPS 25 submission", "summary": "We investigate the usage of Large Language Model (LLM) in collecting\nhigh-quality data to warm-start Reinforcement Learning (RL) algorithms for\nlearning in some classical Markov Decision Process (MDP) environments. In this\nwork, we focus on using LLM to generate an off-policy dataset that sufficiently\ncovers state-actions visited by optimal policies, then later using an RL\nalgorithm to explore the environment and improve the policy suggested by the\nLLM. Our algorithm, LORO, can both converge to an optimal policy and have a\nhigh sample efficiency thanks to the LLM's good starting policy. On multiple\nOpenAI Gym environments, such as CartPole and Pendulum, we empirically\ndemonstrate that LORO outperforms baseline algorithms such as pure LLM-based\npolicies, pure RL, and a naive combination of the two, achieving up to $4\n\\times$ the cumulative rewards of the pure RL baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLORO\u7b97\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u9884\u70edRL\u7b97\u6cd5\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u4f18\u4e8e\u7eafLLM\u3001\u7eafRL\u53ca\u7b80\u5355\u7ec4\u5408\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u4ee5\u63d0\u5347RL\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u8986\u76d6\u6700\u4f18\u7b56\u7565\u72b6\u6001-\u52a8\u4f5c\u7684\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u7ed3\u5408RL\u7b97\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u7b56\u7565\u3002", "result": "LORO\u5728CartPole\u548cPendulum\u7b49\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7d2f\u79ef\u5956\u52b1\u6700\u9ad8\u8fbe\u7eafRL\u76844\u500d\u3002", "conclusion": "LORO\u7ed3\u5408LLM\u548cRL\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.10869", "pdf": "https://arxiv.org/pdf/2505.10869", "abs": "https://arxiv.org/abs/2505.10869", "authors": ["Go Fukino", "Kanta Tachibana"], "title": "A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination", "categories": ["cs.CV", "cs.HC"], "comment": "7 pages, 13 figures, 3 tables", "summary": "This study focuses on the velocity patterns of various body parts during\nwalking and proposes a method for evaluating gait symmetry. Traditional motion\nanalysis studies have assessed gait symmetry based on differences in\nelectromyographic (EMG) signals or acceleration between the left and right\nsides. In contrast, this paper models intersegmental coordination using an LTI\nsystem and proposes a dissimilarity metric to evaluate symmetry. The method was\ntested on five subjects with both symmetric and asymmetric gait.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u65f6\u4e0d\u53d8\uff08LTI\uff09\u7cfb\u7edf\u7684\u6b65\u6001\u5bf9\u79f0\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u8282\u6bb5\u95f4\u534f\u8c03\u6027\u5e76\u8bbe\u8ba1\u5dee\u5f02\u5ea6\u91cf\u6765\u8bc4\u4f30\u5bf9\u79f0\u6027\u3002", "motivation": "\u4f20\u7edf\u6b65\u6001\u5bf9\u79f0\u6027\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u808c\u7535\u56fe\uff08EMG\uff09\u4fe1\u53f7\u6216\u52a0\u901f\u5ea6\u5dee\u5f02\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LTI\u7cfb\u7edf\u5efa\u6a21\u8282\u6bb5\u95f4\u534f\u8c03\u6027\uff0c\u5e76\u63d0\u51fa\u5dee\u5f02\u5ea6\u91cf\u6765\u8bc4\u4f30\u5bf9\u79f0\u6027\u3002\u65b9\u6cd5\u5728\u4e94\u540d\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u6b65\u6001\u53d7\u8bd5\u8005\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u533a\u5206\u4e86\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u6b65\u6001\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684LTI\u7cfb\u7edf\u65b9\u6cd5\u4e3a\u6b65\u6001\u5bf9\u79f0\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 10.0}}
{"id": "2505.11166", "pdf": "https://arxiv.org/pdf/2505.11166", "abs": "https://arxiv.org/abs/2505.11166", "authors": ["Huashan Sun", "Shengyi Liao", "Yansen Han", "Yu Bai", "Yang Gao", "Cheng Fu", "Weizhou Shen", "Fanqi Wan", "Ming Yan", "Ji Zhang", "Fei Huang"], "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSoLoPO\u6846\u67b6\uff0c\u901a\u8fc7\u77ed\u4e0a\u4e0b\u6587\u504f\u597d\u4f18\u5316\u548c\u77ed\u5230\u957f\u5956\u52b1\u5bf9\u9f50\uff0c\u63d0\u5347LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u6570\u636e\u8d28\u91cf\u3001\u8bad\u7ec3\u6548\u7387\u548c\u4f18\u5316\u76ee\u6807\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5c06\u957f\u4e0a\u4e0b\u6587\u504f\u597d\u4f18\u5316\u5206\u89e3\u4e3a\u77ed\u4e0a\u4e0b\u6587\u504f\u597d\u4f18\u5316\u548c\u77ed\u5230\u957f\u5956\u52b1\u5bf9\u9f50\uff08SoLo-RA\uff09\u3002", "result": "SoLoPO\u663e\u8457\u63d0\u5347\u4e86\u957f\u5ea6\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "SoLoPO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5bf9\u9f50\u95ee\u9898\uff0c\u517c\u5bb9\u4e3b\u6d41\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.11191", "pdf": "https://arxiv.org/pdf/2505.11191", "abs": "https://arxiv.org/abs/2505.11191", "authors": ["Kasra Borazjani", "Payam Abdisarabshali", "Fardis Nadimi", "Naji Khosravan", "Minghui Liwang", "Xianbin Wang", "Yiguang Hong", "Seyyedali Hosseinalipour"], "title": "Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration", "categories": ["cs.AI", "cs.RO"], "comment": "10 pages, 3 figures, 3 tables", "summary": "As embodied AI systems become increasingly multi-modal, personalized, and\ninteractive, they must learn effectively from diverse sensory inputs, adapt\ncontinually to user preferences, and operate safely under resource and privacy\nconstraints. These challenges expose a pressing need for machine learning\nmodels capable of swift, context-aware adaptation while balancing model\ngeneralization and personalization. Here, two methods emerge as suitable\ncandidates, each offering parts of these capabilities: Foundation Models (FMs)\nprovide a pathway toward generalization across tasks and modalities, whereas\nFederated Learning (FL) offers the infrastructure for distributed,\nprivacy-preserving model updates and user-level model personalization. However,\nwhen used in isolation, each of these approaches falls short of meeting the\ncomplex and diverse capability requirements of real-world embodied\nenvironments. In this vision paper, we introduce Federated Foundation Models\n(FFMs) for embodied AI, a new paradigm that unifies the strengths of\nmulti-modal multi-task (M3T) FMs with the privacy-preserving distributed nature\nof FL, enabling intelligent systems at the wireless edge. We collect critical\ndeployment dimensions of FFMs in embodied AI ecosystems under a unified\nframework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness\nand imbalance, Bandwidth and compute constraints, On-device continual learning,\nDistributed control and autonomy, and Yielding safety, privacy, and\npersonalization. For each, we identify concrete challenges and envision\nactionable research directions. We also present an evaluation framework for\ndeploying FFMs in embodied AI systems, along with the associated trade-offs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFederated Foundation Models (FFMs)\u7684\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u4e86Foundation Models (FMs)\u7684\u6cdb\u5316\u80fd\u529b\u548cFederated Learning (FL)\u7684\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u4e2a\u6027\u5316AI\u7cfb\u7edf\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u53d8\u5f97\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u548c\u4ea4\u4e92\u5316\uff0c\u9700\u8981\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u3001\u5e73\u8861\u6cdb\u5316\u4e0e\u4e2a\u6027\u5316\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u548c\u8d44\u6e90\u7ea6\u675f\u3002", "method": "\u63d0\u51faFFMs\uff0c\u7ed3\u5408FMs\u548cFL\u7684\u4f18\u52bf\uff0c\u5e76\u5f15\u5165EMBODY\u6846\u67b6\uff0c\u5206\u6790\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u7ef4\u5ea6\uff08\u5982\u5f02\u6784\u6027\u3001\u6a21\u6001\u4e30\u5bcc\u6027\u7b49\uff09\u3002", "result": "\u63d0\u51fa\u4e86FFMs\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u5177\u4f53\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "FFMs\u4e3a\u591a\u6a21\u6001\u4e2a\u6027\u5316AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u95ee\u9898\u3002", "relevance": 70.0}}
{"id": "2505.10873", "pdf": "https://arxiv.org/pdf/2505.10873", "abs": "https://arxiv.org/abs/2505.10873", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Hashing for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Image Analysis and Processing\n  (ICIAP 2023)", "summary": "We focus on the problem of identifying samples in a set that do not conform\nto structured patterns represented by low-dimensional manifolds. An effective\nway to solve this problem is to embed data in a high dimensional space, called\nPreference Space, where anomalies can be identified as the most isolated\npoints. In this work, we employ Locality Sensitive Hashing to avoid explicit\ncomputation of distances in high dimensions and thus improve Anomaly Detection\nefficiency. Specifically, we present an isolation-based anomaly detection\ntechnique designed to work in the Preference Space which achieves\nstate-of-the-art performance at a lower computational cost. Code is publicly\navailable at\nhttps://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7a7a\u95f4\u7684\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u907f\u514d\u9ad8\u7ef4\u8ddd\u79bb\u8ba1\u7b97\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8bc6\u522b\u4e0d\u7b26\u5408\u4f4e\u7ef4\u6d41\u5f62\u7ed3\u6784\u6a21\u5f0f\u7684\u6837\u672c\u95ee\u9898\uff0c\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u6280\u672f\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u9ad8\u7ef4\u8ddd\u79bb\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9694\u79bb\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5728\u504f\u597d\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "relevance": 30.0}}
{"id": "2505.10875", "pdf": "https://arxiv.org/pdf/2505.10875", "abs": "https://arxiv.org/abs/2505.10875", "authors": ["Alexey Magay", "Dhurba Tripathi", "Yu Hao", "Yi Fang"], "title": "A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision", "categories": ["cs.CV"], "comment": "Project website and code: https://dktpt44.github.io/LV-GPT/", "summary": "People with blindness and low vision (pBLV) face significant challenges,\nstruggling to navigate environments and locate objects due to limited visual\ncues. Spatial reasoning is crucial for these individuals, as it enables them to\nunderstand and interpret the spatial relationships in their surroundings,\nenhancing their ability to navigate and interact more safely and independently.\nCurrent multi-modal large language (MLLM) models for low vision people lack the\nspatial reasoning capabilities needed to effectively assist in these tasks.\nMoreover, there is a notable absence of lightweight, easy-to-use systems that\nallow pBLV to effectively perceive and interact with their surrounding\nenvironment. In this paper, we propose a novel spatial enhanced multi-modal\nlarge language model based approach for visually impaired individuals. By\nfine-tuning the MLLM to incorporate spatial reasoning capabilities, our method\nsignificantly improves the understanding of environmental context, which is\ncritical for navigation and object recognition. The innovation extends to a\nhardware component, designed as an attachment for glasses, ensuring increased\naccessibility and ease of use. This integration leverages advanced VLMs to\ninterpret visual data and provide real-time, spatially aware feedback to the\nuser. Our approach aims to bridge the gap between advanced machine learning\nmodels and practical, user-friendly assistive devices, offering a robust\nsolution for visually impaired users to navigate their surroundings more\neffectively and independently. The paper includes an in-depth evaluation using\nthe VizWiz dataset, demonstrating substantial improvements in accuracy and user\nexperience. Additionally, we design a comprehensive dataset to evaluate our\nmethod's effectiveness in realworld situations, demonstrating substantial\nimprovements in accuracy and user experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u66f4\u597d\u5730\u7406\u89e3\u548c\u5bfc\u822a\u73af\u5883\uff0c\u7ed3\u5408\u786c\u4ef6\u8bbe\u5907\u63d0\u5347\u5b9e\u7528\u6027\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u5728\u5bfc\u822a\u548c\u7269\u4f53\u5b9a\u4f4d\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709MLLM\u7f3a\u4e4f\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u7f3a\u4e4f\u8f7b\u91cf\u6613\u7528\u7684\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5fae\u8c03MLLM\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408\u773c\u955c\u9644\u4ef6\u786c\u4ef6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u3002", "result": "\u5728VizWiz\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5bfc\u822a\u548c\u4ea4\u4e92\u5de5\u5177\uff0c\u586b\u8865\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u5b9e\u7528\u8bbe\u5907\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "relevance": 40.0}}
{"id": "2505.11177", "pdf": "https://arxiv.org/pdf/2505.11177", "abs": "https://arxiv.org/abs/2505.11177", "authors": ["Hrishit Madhavi", "Jacob Cherian", "Yuvraj Khamkar", "Dhananjay Bhagat"], "title": "Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline", "categories": ["cs.CL", "cs.AI", "68T50 (Natural language processing), 68U10 (Image processing)"], "comment": "8 pages, 7 figures, direct arXiv submission", "summary": "This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u591a\u8bed\u8a00\u4fe1\u606f\u63d0\u53d6\u548c\u5904\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408OCR\u3001\u5927\u8bed\u8a00\u6a21\u578bAPI\u548c\u5176\u4ed6\u6a21\u5757\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u7ffb\u8bd1\u3001\u6458\u8981\u751f\u6210\u7b49\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u6587\u6863\u4e2d\u7684\u591a\u8bed\u8a00\u4fe1\u606f\u63d0\u53d6\u548c\u5904\u7406\u95ee\u9898\uff0c\u7f29\u5c0f\u8bed\u8a00\u5dee\u8ddd\uff0c\u63d0\u5347\u4fe1\u606f\u83b7\u53d6\u6548\u7387\u3002", "method": "\u4f7f\u7528Tesseract\u8fdb\u884cOCR\u6587\u672c\u63d0\u53d6\uff0c\u7ed3\u5408Gemini API\u8fdb\u884c\u8de8\u8bed\u8a00\u7ffb\u8bd1\u548c\u6458\u8981\u751f\u6210\uff0c\u5e76\u6dfb\u52a0\u60c5\u611f\u5206\u6790\u3001\u4e3b\u9898\u5206\u7c7b\u7b49\u6a21\u5757\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7Gradio\u754c\u9762\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u63d0\u5347\u56fe\u50cf\u6587\u6863\u4fe1\u606f\u8bbf\u95ee\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u8bed\u8a00\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u7ed3\u5408\u4e86\u591a\u79cd\u6280\u672f\u548c\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.11208", "pdf": "https://arxiv.org/pdf/2505.11208", "abs": "https://arxiv.org/abs/2505.11208", "authors": ["Dongjun Kim", "Junwoo Park", "Chaehyeon Shin", "Jaeheon Jung", "Kyungho Shin", "Seungheon Baek", "Sanghyuk Heo", "Woongrae Kim", "Inchul Jeong", "Joohwan Cho", "Jongsun Park"], "title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning", "categories": ["cs.AI", "cs.CE", "cs.ET", "cs.LG"], "comment": "Accepted for DAC 2025", "summary": "Analog/mixed-signal circuit design encounters significant challenges due to\nperformance degradation from process, voltage, and temperature (PVT)\nvariations. To achieve commercial-grade reliability, iterative manual design\nrevisions and extensive statistical simulations are required. While several\nstudies have aimed to automate variation aware analog design to reduce\ntime-to-market, the substantial mismatches in real-world wafers have not been\nthoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing\nframework that effectively manages the impact of diverse random mismatches to\nimprove robustness against PVT variations. In the proposed approach,\nrisk-sensitive reinforcement learning is leveraged to account for the\nreliability bound affected by PVT variations, and ensemble-based critic is\nintroduced to achieve sample-efficient learning. For design verification, we\nalso propose $\\mu$-$\\sigma$ evaluation and simulation reordering method to\nreduce simulation costs of identifying failed designs. GLOVA supports\nverification through industrial-level PVT variation evaluation methods,\nincluding corner simulation as well as global and local Monte Carlo (MC)\nsimulations. Compared to previous state-of-the-art variation-aware analog\nsizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample\nefficiency and 76.0$\\times$ reduction in time.", "AI": {"tldr": "GLOVA\u662f\u4e00\u4e2a\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u96c6\u6210\u6279\u8bc4\u5668\u63d0\u9ad8\u5bf9PVT\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u6a21\u62df/\u6df7\u5408\u4fe1\u53f7\u7535\u8def\u8bbe\u8ba1\u56e0PVT\u53d8\u5316\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u5b9e\u9645\u6676\u5706\u4e2d\u7684\u5931\u914d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u98ce\u9669\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u96c6\u6210\u6279\u8bc4\u5668\uff0c\u7ed3\u5408\u03bc-\u03c3\u8bc4\u4f30\u548c\u4eff\u771f\u91cd\u6392\u5e8f\u65b9\u6cd5\u964d\u4f4e\u4eff\u771f\u6210\u672c\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cGLOVA\u5728\u6837\u672c\u6548\u7387\u548c\u65f6\u95f4\u4e0a\u5206\u522b\u63d0\u5347\u4e8680.5\u500d\u548c76.0\u500d\u3002", "conclusion": "GLOVA\u6709\u6548\u63d0\u5347\u4e86\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "relevance": 10.0}}
{"id": "2505.10874", "pdf": "https://arxiv.org/pdf/2505.10874", "abs": "https://arxiv.org/abs/2505.10874", "authors": ["Luca Magri", "Filippo Leveni", "Giacomo Boracchi"], "title": "MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at Computer Vision and Pattern Recognition (CVPR 2021)", "summary": "We address the problem of recovering multiple structures of different classes\nin a dataset contaminated by noise and outliers. In particular, we consider\ngeometric structures defined by a mixture of underlying parametric models (e.g.\nplanes and cylinders, homographies and fundamental matrices), and we tackle the\nrobust fitting problem by preference analysis and clustering. We present a new\nalgorithm, termed MultiLink, that simultaneously deals with multiple classes of\nmodels. MultiLink combines on-the-fly model fitting and model selection in a\nnovel linkage scheme that determines whether two clusters are to be merged. The\nresulting method features many practical advantages with respect to methods\nbased on preference analysis, being faster, less sensitive to the inlier\nthreshold, and able to compensate limitations deriving from hypotheses\nsampling. Experiments on several public datasets demonstrate that Multi-Link\nfavourably compares with state of the art alternatives, both in multi-class and\nsingle-class problems. Code is publicly made available for download.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultiLink\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u548c\u5f02\u5e38\u503c\u6c61\u67d3\u7684\u6570\u636e\u96c6\u4e2d\u6062\u590d\u591a\u7c7b\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u89e3\u51b3\u5728\u566a\u58f0\u548c\u5f02\u5e38\u503c\u6c61\u67d3\u7684\u6570\u636e\u96c6\u4e2d\u540c\u65f6\u6062\u590d\u591a\u7c7b\u51e0\u4f55\u7ed3\u6784\uff08\u5982\u5e73\u9762\u3001\u5706\u67f1\u4f53\u3001\u5355\u5e94\u77e9\u9635\u7b49\uff09\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u504f\u597d\u5206\u6790\u548c\u805a\u7c7b\uff0c\u7ed3\u5408\u52a8\u6001\u6a21\u578b\u62df\u5408\u548c\u6a21\u578b\u9009\u62e9\uff0c\u63d0\u51faMultiLink\u7b97\u6cd5\uff0c\u51b3\u5b9a\u662f\u5426\u5408\u5e76\u4e24\u4e2a\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMultiLink\u5728\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u7c7b\u548c\u5355\u7c7b\u95ee\u9898\u3002", "conclusion": "MultiLink\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u7c7b\u51e0\u4f55\u7ed3\u6784\u6062\u590d\u3002", "relevance": 40.0}}
{"id": "2505.10888", "pdf": "https://arxiv.org/pdf/2505.10888", "abs": "https://arxiv.org/abs/2505.10888", "authors": ["Saad Manzur", "Bryan Vela", "Brandon Vela", "Aditya Agrawal", "Lan-Anh Dang-Vu", "David Li", "Wayne Hayes"], "title": "PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": "https://github.com/bryanjvela/PoseLab3D/tree/submission_branch", "summary": "Reliable three-dimensional human pose estimation is becoming increasingly\nimportant for real-world applications, yet much of prior work has focused\nsolely on the performance within a single dataset. In practice, however,\nsystems must adapt to diverse viewpoints, environments, and camera setups --\nconditions that differ significantly from those encountered during training,\nwhich is often the case in real-world scenarios. To address these challenges,\nwe present a standardized testing environment in which each method is evaluated\non a variety of datasets, ensuring consistent and fair cross-dataset\ncomparisons -- allowing for the analysis of methods on previously unseen data.\nTherefore, we propose PoseBench3D, a unified framework designed to\nsystematically re-evaluate prior and future models across four of the most\nwidely used datasets for human pose estimation -- with the framework able to\nsupport novel and future datasets as the field progresses. Through a unified\ninterface, our framework provides datasets in a pre-configured yet easily\nmodifiable format, ensuring compatibility with diverse model architectures. We\nre-evaluated the work of 18 methods, either trained or gathered from existing\nliterature, and reported results using both Mean Per Joint Position Error\n(MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE)\nmetrics, yielding more than 100 novel cross-dataset evaluation results.\nAdditionally, we analyze performance differences resulting from various\npre-processing techniques and dataset preparation parameters -- offering\nfurther insight into model generalization capabilities.", "AI": {"tldr": "PoseBench3D\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f303D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u652f\u6301\u8de8\u6570\u636e\u96c6\u6bd4\u8f83\u548c\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5728\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPoseBench3D\u6846\u67b6\uff0c\u7edf\u4e00\u8bc4\u4f3018\u79cd\u65b9\u6cd5\u57284\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u652f\u6301\u65b0\u6570\u636e\u96c6\u6269\u5c55\u3002", "result": "\u751f\u6210\u4e86100\u591a\u9879\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u7ed3\u679c\uff0c\u5206\u6790\u4e86\u9884\u5904\u7406\u6280\u672f\u548c\u6570\u636e\u96c6\u53c2\u6570\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "conclusion": "PoseBench3D\u4e3a3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "relevance": 30.0}}
{"id": "2505.11199", "pdf": "https://arxiv.org/pdf/2505.11199", "abs": "https://arxiv.org/abs/2505.11199", "authors": ["Chris K\u00f6cher", "Alexander Kozachinskiy", "Anthony Widjaja Lin", "Marco S\u00e4lzer", "Georg Zetzsche"], "title": "NoPE: The Counting Power of Transformers with No Positional Encodings", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": null, "summary": "Positional Encodings (PEs) seem to be indispensable for ensuring\nexpressiveness of transformers; without them attention transformers reduce to a\nbag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard\nattention mechanisms were very recently shown to only be able to express\nregular languages, i.e., with limited counting ability. This paper shows that,\nwith average hard attention mechanisms, NoPE-transformers are still\nsurprisingly expressive: they can express counting languages corresponding to\nnonnegative integer solutions to multivariate polynomial equations (i.e.\nDiophantine equations), reasoning about which is well-known to be undecidable.\nIn fact, we provide a precise characterization of languages expressible by\nAverage Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond\nprecisely to what we call \\emph{semi-algebraic sets}, i.e., finite unions of\nsets of nonnegative integer solutions to systems of multivariate polynomial\ninequations. We obtain several interesting consequences of our\ncharacterization. Firstly, NoPE-transformers can express counting properties\nthat are far more complex than established models like simplified counter\nmachines and Petri nets, but cannot express a very simple counting property of\nPARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,\ne.g., whether a given NoPE transformer classifies all input strings in one\nclass. To complement our results, we exhibit a counting language that is not\nexpressible by average hard attention transformers even with arbitrary PEs but\nis expressible in the circuit complexity class TC$^0$, answering an open\nproblem.", "AI": {"tldr": "NoPE-transformers\uff08\u65e0\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\uff09\u5728\u5e73\u5747\u786c\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u89e3\u51b3\u590d\u6742\u7684\u8ba1\u6570\u8bed\u8a00\u95ee\u9898\uff0c\u4f46\u5176\u5206\u6790\u95ee\u9898\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002", "motivation": "\u7814\u7a76\u65e0\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\u5728\u5e73\u5747\u786c\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u5bf9\u8fd9\u7c7b\u6a21\u578b\u7406\u89e3\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u523b\u753b\u4e86NoPE-AHATs\uff08\u65e0\u4f4d\u7f6e\u7f16\u7801\u7684\u5e73\u5747\u786c\u6ce8\u610f\u529bTransformer\uff09\u80fd\u591f\u8868\u8fbe\u7684\u8bed\u8a00\u7c7b\u522b\uff0c\u5373\u534a\u4ee3\u6570\u96c6\u3002", "result": "NoPE-AHATs\u80fd\u591f\u8868\u8fbe\u590d\u6742\u7684\u8ba1\u6570\u8bed\u8a00\uff08\u5982Diophantine\u65b9\u7a0b\u7684\u89e3\uff09\uff0c\u4f46\u5176\u5206\u6790\u95ee\u9898\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002\u540c\u65f6\uff0c\u53d1\u73b0\u67d0\u4e9b\u8ba1\u6570\u8bed\u8a00\u5373\u4f7f\u6709\u4f4d\u7f6e\u7f16\u7801\u4e5f\u65e0\u6cd5\u8868\u8fbe\u3002", "conclusion": "\u65e0\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\u5728\u7279\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5176\u590d\u6742\u6027\u548c\u4e0d\u53ef\u5224\u5b9a\u6027\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "relevance": 85.0}}
{"id": "2505.11227", "pdf": "https://arxiv.org/pdf/2505.11227", "abs": "https://arxiv.org/abs/2505.11227", "authors": ["Zhangying Feng", "Qianglong Chen", "Ning Lu", "Yongqian Li", "Siqi Cheng", "Shuangmu Peng", "Duyu Tang", "Shengcai Liu", "Zhirui Zhang"], "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The development of reasoning capabilities represents a critical frontier in\nlarge language models (LLMs) research, where reinforcement learning (RL) and\nprocess reward models (PRMs) have emerged as predominant methodological\nframeworks. Contrary to conventional wisdom, empirical evidence from\nDeepSeek-R1 demonstrates that pure RL training focused on mathematical\nproblem-solving can progressively enhance reasoning abilities without PRM\nintegration, challenging the perceived necessity of process supervision. In\nthis study, we conduct a systematic investigation of the relationship between\nRL training and PRM capabilities. Our findings demonstrate that problem-solving\nproficiency and process supervision capabilities represent complementary\ndimensions of reasoning that co-evolve synergistically during pure RL training.\nIn particular, current PRMs underperform simple baselines like majority voting\nwhen applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To\naddress this limitation, we propose Self-PRM, an introspective framework in\nwhich models autonomously evaluate and rerank their generated solutions through\nself-reward mechanisms. Although Self-PRM consistently improves the accuracy of\nthe benchmark (particularly with larger sample sizes), analysis exposes\npersistent challenges: The approach exhibits low precision (<10\\%) on difficult\nproblems, frequently misclassifying flawed solutions as valid. These analyses\nunderscore the need for continued RL scaling to improve reward alignment and\nintrospective accuracy. Overall, our findings suggest that PRM may not be\nessential for enhancing complex reasoning, as pure RL not only improves\nproblem-solving skills but also inherently fosters robust PRM capabilities. We\nhope these findings provide actionable insights for building more reliable and\nself-aware complex reasoning models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7eaf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u53ef\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u7701\u6846\u67b6Self-PRM\u4ee5\u6539\u8fdb\u7ed3\u679c\u3002", "motivation": "\u63a2\u7d22RL\u8bad\u7ec3\u4e0ePRM\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u6311\u6218PRM\u5728\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e2d\u7684\u5fc5\u8981\u6027\u3002", "method": "\u901a\u8fc7\u7eafRL\u8bad\u7ec3\u548cSelf-PRM\u6846\u67b6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7eafRL\u8bad\u7ec3\u80fd\u540c\u65f6\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u548cPRM\u80fd\u529b\uff0c\u4f46Self-PRM\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u7cbe\u5ea6\u4f4e\u3002", "conclusion": "PRM\u53ef\u80fd\u975e\u5fc5\u9700\uff0c\u7eafRL\u8bad\u7ec3\u80fd\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u5956\u52b1\u5bf9\u9f50\u3002", "relevance": 85.0}}
{"id": "2505.10876", "pdf": "https://arxiv.org/pdf/2505.10876", "abs": "https://arxiv.org/abs/2505.10876", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Preference Isolation Forest for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Submitted to Pattern Recognition", "summary": "We address the problem of detecting anomalies as samples that do not conform\nto structured patterns represented by low-dimensional manifolds. To this end,\nwe conceive a general anomaly detection framework called Preference Isolation\nForest (PIF), that combines the benefits of adaptive isolation-based methods\nwith the flexibility of preference embedding. The key intuition is to embed the\ndata into a high-dimensional preference space by fitting low-dimensional\nmanifolds, and to identify anomalies as isolated points. We propose three\nisolation approaches to identify anomalies: $i$) Voronoi-iForest, the most\ngeneral solution, $ii$) RuzHash-iForest, that avoids explicit computation of\ndistances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a\nlocality prior to improve efficiency and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u504f\u597d\u9694\u79bb\u68ee\u6797\uff08PIF\uff09\u7684\u901a\u7528\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u9694\u79bb\u65b9\u6cd5\u548c\u504f\u597d\u5d4c\u5165\u7684\u7075\u6d3b\u6027\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5d4c\u5165\u9ad8\u7ef4\u504f\u597d\u7a7a\u95f4\u6765\u8bc6\u522b\u5f02\u5e38\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u4f4e\u7ef4\u6d41\u5f62\u8868\u793a\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u7ed3\u5408\u9694\u79bb\u65b9\u6cd5\u548c\u504f\u597d\u5d4c\u5165\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u9694\u79bb\u65b9\u6cd5\uff1aVoronoi-iForest\uff08\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff09\u3001RuzHash-iForest\uff08\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u8ddd\u79bb\uff09\u3001Sliding-PIF\uff08\u5229\u7528\u5c40\u90e8\u6027\u5148\u9a8c\u63d0\u9ad8\u6548\u7387\uff09\u3002", "result": "\u901a\u8fc7\u9ad8\u7ef4\u504f\u597d\u7a7a\u95f4\u5d4c\u5165\u548c\u9694\u79bb\u65b9\u6cd5\u6709\u6548\u8bc6\u522b\u5f02\u5e38\u3002", "conclusion": "PIF\u6846\u67b6\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10902", "pdf": "https://arxiv.org/pdf/2505.10902", "abs": "https://arxiv.org/abs/2505.10902", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Rong Wang", "Li Zhang"], "title": "Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach", "categories": ["cs.CV", "cs.HC", "92C50", "I.3.8; I.6.8"], "comment": "34 pages, 24 figures", "summary": "Background and Objective: Precise preoperative planning and effective\nphysician training for coronary interventions are increasingly important.\nDespite advances in medical imaging technologies, transforming static or\nlimited dynamic imaging data into comprehensive dynamic cardiac models remains\nchallenging. Existing training systems lack accurate simulation of cardiac\nphysiological dynamics. This study develops a comprehensive dynamic cardiac\nmodel research framework based on 4D-CTA, integrating digital twin technology,\ncomputer vision, and physical model manufacturing to provide precise,\npersonalized tools for interventional cardiology. Methods: Using 4D-CTA data\nfrom a 60-year-old female with three-vessel coronary stenosis, we segmented\ncardiac chambers and coronary arteries, constructed dynamic models, and\nimplemented skeletal skinning weight computation to simulate vessel deformation\nacross 20 cardiac phases. Transparent vascular physical models were\nmanufactured using medical-grade silicone. We developed cardiac output analysis\nand virtual angiography systems, implemented guidewire 3D reconstruction using\nbinocular stereo vision, and evaluated the system through angiography\nvalidation and CABG training applications. Results: Morphological consistency\nbetween virtual and real angiography reached 80.9%. Dice similarity\ncoefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory\nerrors below 1.1 mm. The transparent model demonstrated advantages in CABG\ntraining, allowing direct visualization while simulating beating heart\nchallenges. Conclusion: Our patient-specific digital-physical twin approach\neffectively reproduces both anatomical structures and dynamic characteristics\nof coronary vasculature, offering a dynamic environment with visual and tactile\nfeedback valuable for education and clinical planning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e4D-CTA\u7684\u52a8\u6001\u5fc3\u810f\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u4e3a\u51a0\u72b6\u52a8\u8109\u4ecb\u5165\u63d0\u4f9b\u7cbe\u786e\u7684\u4e2a\u6027\u5316\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bad\u7ec3\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5fc3\u810f\u751f\u7406\u52a8\u6001\u51c6\u786e\u6a21\u62df\u7684\u95ee\u9898\uff0c\u63d0\u5347\u51a0\u72b6\u52a8\u8109\u4ecb\u5165\u7684\u672f\u524d\u89c4\u5212\u548c\u533b\u5e08\u57f9\u8bad\u6548\u679c\u3002", "method": "\u5229\u75284D-CTA\u6570\u636e\uff0c\u6784\u5efa\u52a8\u6001\u5fc3\u810f\u6a21\u578b\uff0c\u5236\u9020\u900f\u660e\u8840\u7ba1\u7269\u7406\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u5fc3\u810f\u8f93\u51fa\u5206\u6790\u548c\u865a\u62df\u8840\u7ba1\u9020\u5f71\u7cfb\u7edf\u3002", "result": "\u865a\u62df\u4e0e\u771f\u5b9e\u8840\u7ba1\u9020\u5f71\u5f62\u6001\u4e00\u81f4\u6027\u8fbe80.9%\uff0c\u5bfc\u4e1d\u8fd0\u52a8\u8f68\u8ff9\u8bef\u5dee\u4f4e\u4e8e1.1 mm\uff0c\u900f\u660e\u6a21\u578b\u5728CABG\u8bad\u7ec3\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u60a3\u8005\u7279\u5f02\u6027\u6570\u5b57-\u7269\u7406\u5b6a\u751f\u65b9\u6cd5\u6709\u6548\u6a21\u62df\u51a0\u72b6\u52a8\u8109\u89e3\u5256\u7ed3\u6784\u548c\u52a8\u6001\u7279\u6027\uff0c\u4e3a\u6559\u80b2\u548c\u4e34\u5e8a\u89c4\u5212\u63d0\u4f9b\u52a8\u6001\u73af\u5883\u3002", "relevance": 20.0}}
{"id": "2505.11225", "pdf": "https://arxiv.org/pdf/2505.11225", "abs": "https://arxiv.org/abs/2505.11225", "authors": ["Chengyu Huang", "Zhengxin Zhang", "Claire Cardie"], "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While scaling the length of responses at test-time has been shown to markedly\nimprove the reasoning abilities and performance of large language models\n(LLMs), it often results in verbose outputs and increases inference cost. Prior\napproaches for efficient test-time scaling, typically using universal budget\nconstraints or query-level length optimization, do not leverage historical\ninformation from previous encounters with the same problem during training. We\nhypothesize that this limits their ability to progressively make solutions more\nconcise over time. To address this, we present History-Aware Policy\nOptimization (HAPO), which keeps track of a history state (e.g., the minimum\nlength over previously generated correct responses) for each problem. HAPO\nemploys a novel length reward function based on this history state to\nincentivize the discovery of correct solutions that are more concise than those\npreviously found. Crucially, this reward structure avoids overly penalizing\nshorter incorrect responses with the goal of facilitating exploration towards\nmore efficient solutions. By combining this length reward with a correctness\nreward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to\ntrain DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and\nQwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span\nvarious difficulty levels. Experiment results demonstrate that HAPO effectively\ninduces LLMs' concise reasoning abilities, producing length reductions of\n33-59% with accuracy drops of only 2-5%.", "AI": {"tldr": "HAPO\u662f\u4e00\u79cd\u5386\u53f2\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb0\u5f55\u5386\u53f2\u72b6\u6001\u548c\u8bbe\u8ba1\u65b0\u9896\u7684\u957f\u5ea6\u5956\u52b1\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316LLM\u7684\u6b63\u786e\u6027\u548c\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u957f\u5ea6\u65f6\u65e0\u6cd5\u5229\u7528\u5386\u53f2\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4ee5\u9010\u6b65\u751f\u6210\u66f4\u7b80\u6d01\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHAPO\uff0c\u8bb0\u5f55\u5386\u53f2\u72b6\u6001\uff08\u5982\u6700\u5c0f\u6b63\u786e\u54cd\u5e94\u957f\u5ea6\uff09\uff0c\u5e76\u8bbe\u8ba1\u957f\u5ea6\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u6b63\u786e\u6027\u5956\u52b1\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHAPO\u5c06\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1133-59%\uff0c\u51c6\u786e\u6027\u4ec5\u4e0b\u964d2-5%\u3002", "conclusion": "HAPO\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u7b80\u6d01\u63a8\u7406\u80fd\u529b\uff0c\u5e73\u8861\u4e86\u6548\u7387\u548c\u6b63\u786e\u6027\u3002", "relevance": 85.0}}
{"id": "2505.11247", "pdf": "https://arxiv.org/pdf/2505.11247", "abs": "https://arxiv.org/abs/2505.11247", "authors": ["Mingxing Peng", "Yuting Xie", "Xusen Guo", "Ruoyu Yao", "Hai Yang", "Jun Ma"], "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "13 pages, 5 figures", "summary": "Ensuring the safety and robustness of autonomous driving systems necessitates\na comprehensive evaluation in safety-critical scenarios. However, these\nsafety-critical scenarios are rare and difficult to collect from real-world\ndriving data, posing significant challenges to effectively assessing the\nperformance of autonomous vehicles. Typical existing methods often suffer from\nlimited controllability and lack user-friendliness, as extensive expert\nknowledge is essentially required. To address these challenges, we propose\nLD-Scene, a novel framework that integrates Large Language Models (LLMs) with\nLatent Diffusion Models (LDMs) for user-controllable adversarial scenario\ngeneration through natural language. Our approach comprises an LDM that\ncaptures realistic driving trajectory distributions and an LLM-based guidance\nmodule that translates user queries into adversarial loss functions,\nfacilitating the generation of scenarios aligned with user queries. The\nguidance module integrates an LLM-based Chain-of-Thought (CoT) code generator\nand an LLM-based code debugger, enhancing the controllability and robustness in\ngenerating guidance functions. Extensive experiments conducted on the nuScenes\ndataset demonstrate that LD-Scene achieves state-of-the-art performance in\ngenerating realistic, diverse, and effective adversarial scenarios.\nFurthermore, our framework provides fine-grained control over adversarial\nbehaviors, thereby facilitating more effective testing tailored to specific\ndriving scenarios.", "AI": {"tldr": "LD-Scene\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7528\u6237\u53ef\u63a7\u7684\u5bf9\u6297\u6027\u9a7e\u9a76\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u73b0\u6709\u65b9\u6cd5\u53ef\u63a7\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408LDM\u6355\u6349\u771f\u5b9e\u9a7e\u9a76\u8f68\u8ff9\u5206\u5e03\uff0c\u5229\u7528LLM\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u5316\u4e3a\u5bf9\u6297\u6027\u635f\u5931\u51fd\u6570\uff0c\u5e76\u901a\u8fc7CoT\u4ee3\u7801\u751f\u6210\u5668\u548c\u8c03\u8bd5\u5668\u589e\u5f3a\u53ef\u63a7\u6027\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cLD-Scene\u5728\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u6709\u6548\u7684\u5bf9\u6297\u6027\u573a\u666f\u65b9\u9762\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "LD-Scene\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u5bf9\u6297\u884c\u4e3a\u63a7\u5236\uff0c\u4e3a\u7279\u5b9a\u9a7e\u9a76\u573a\u666f\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.10877", "pdf": "https://arxiv.org/pdf/2505.10877", "abs": "https://arxiv.org/abs/2505.10877", "authors": ["Mathieu Alain", "So Takao", "Xiaowen Dong", "Bastian Rieck", "Emmanuel Noutahi"], "title": "Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Predicting the labels of graph-structured data is crucial in scientific\napplications and is often achieved using graph neural networks (GNNs). However,\nwhen data is scarce, GNNs suffer from overfitting, leading to poor performance.\nRecently, Gaussian processes (GPs) with graph-level inputs have been proposed\nas an alternative. In this work, we extend the Gaussian process framework to\nsimplicial complexes (SCs), enabling the handling of edge-level attributes and\nattributes supported on higher-order simplices. We further augment the\nresulting SC representations by considering their Hodge decompositions,\nallowing us to account for homological information, such as the number of\nholes, in the SC. We demonstrate that our framework enhances the predictions\nacross various applications, paving the way for GPs to be more widely used for\ngraph and SC-level predictions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u7a00\u7f3a\u6570\u636e\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u5355\u7eaf\u590d\u5f62\uff08SCs\uff09\u4ee5\u652f\u6301\u9ad8\u9636\u5c5e\u6027\u3002\u901a\u8fc7Hodge\u5206\u89e3\uff0c\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u9ad8\u65af\u8fc7\u7a0b\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u6269\u5c55\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\u81f3\u5355\u7eaf\u590d\u5f62\uff0c\u7ed3\u5408Hodge\u5206\u89e3\u4ee5\u5229\u7528\u540c\u8c03\u4fe1\u606f\uff08\u5982\u201c\u5b54\u201d\u7684\u6570\u91cf\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u5728\u56fe\u548c\u5355\u7eaf\u590d\u5f62\u7ea7\u522b\u7684\u9884\u6d4b\u4e2d\u66f4\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002", "relevance": 30.0}}
{"id": "2505.10917", "pdf": "https://arxiv.org/pdf/2505.10917", "abs": "https://arxiv.org/abs/2505.10917", "authors": ["Mingxiao Li", "Na Su", "Fang Qu", "Zhizhou Zhong", "Ziyang Chen", "Zhaopeng Tu", "Xiaolong Li"], "title": "VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross-Modal Mutual Information Maximization", "categories": ["cs.CV"], "comment": null, "summary": "Current multimodal large language models (MLLMs) face a critical challenge in\nmodality alignment, often exhibiting a bias towards textual information at the\nexpense of other modalities like vision. This paper conducts a systematic\ninformation-theoretic analysis of the widely used cross-entropy loss in MLLMs,\nuncovering its implicit alignment objective. Our theoretical investigation\nreveals that this implicit objective has inherent limitations, leading to a\ndegradation of cross-modal alignment as text sequence length increases, thereby\nhindering effective multimodal information fusion. To overcome these drawbacks,\nwe propose Vision-Text Alignment (VISTA), a novel approach guided by our\ntheoretical insights. VISTA introduces an explicit alignment objective designed\nto maximize cross-modal mutual information, preventing the degradation of\nvisual alignment. Notably, VISTA enhances the visual understanding capabilities\nof existing MLLMs without requiring any additional trainable modules or extra\ntraining data, making it both efficient and practical. Our method significantly\noutperforms baseline models across more than a dozen benchmark datasets,\nincluding VQAv2, MMStar, and MME, paving the way for new directions in MLLM\nmodal alignment research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVISTA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u9690\u542b\u5bf9\u9f50\u76ee\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5bf9\u9f50\u76ee\u6807\u4ee5\u4f18\u5316\u8de8\u6a21\u6001\u4fe1\u606f\u878d\u5408\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u6a21\u6001\u5bf9\u9f50\u4e2d\u5b58\u5728\u504f\u5411\u6587\u672c\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u89c6\u89c9\u7b49\u5176\u4ed6\u6a21\u6001\u7684\u4fe1\u606f\u878d\u5408\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6539\u8fdb\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u9690\u542b\u5bf9\u9f50\u76ee\u6807\uff0c\u63d0\u51faVISTA\u65b9\u6cd5\uff0c\u5f15\u5165\u663e\u5f0f\u5bf9\u9f50\u76ee\u6807\u4ee5\u6700\u5927\u5316\u8de8\u6a21\u6001\u4e92\u4fe1\u606f\u3002", "result": "VISTA\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982VQAv2\u3001MMStar\u548cMME\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "VISTA\u4e3aMLLMs\u7684\u6a21\u6001\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6a21\u5757\u6216\u6570\u636e\uff0c\u9ad8\u6548\u5b9e\u7528\u3002", "relevance": 85.0}}
{"id": "2505.11271", "pdf": "https://arxiv.org/pdf/2505.11271", "abs": "https://arxiv.org/abs/2505.11271", "authors": ["Camille Couturier", "Spyros Mastorakis", "Haiying Shen", "Saravan Rajmohan", "Victor R\u00fchle"], "title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.7"], "comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings", "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7f13\u5b58\u7684LLM\u95ee\u7b54\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b9750-60%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u3001\u5185\u5b58\u548c\u5e26\u5bbd\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u7f13\u5b58\u5b58\u50a8\u548c\u590d\u7528\u4e2d\u95f4\u4e0a\u4e0b\u6587\u6458\u8981\uff0c\u4f18\u5316LLM\u95ee\u7b54\u6d41\u7a0b\u3002", "result": "\u5728NaturalQuestions\u3001TriviaQA\u548cArXiv\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b9750-60%\uff0c\u51c6\u786e\u6027\u63a5\u8fd1\u5b8c\u6574\u6587\u6863\u5904\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u54cd\u5e94\u8d28\u91cf\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6AI\u52a9\u624b\u3002", "relevance": 85.0}}
{"id": "2505.11274", "pdf": "https://arxiv.org/pdf/2505.11274", "abs": "https://arxiv.org/abs/2505.11274", "authors": ["Zheng Li", "Qingxiu Dong", "Jingyuan Ma", "Di Zhang", "Zhifang Sui"], "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.", "AI": {"tldr": "SelfBudgeter\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u53ef\u63a7\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u8bad\u7ec3\uff08\u9884\u4f30\u8ba1\u63a8\u7406\u6210\u672c\u548c\u9884\u7b97\u5f15\u5bfc\u7684GPRO\u5f3a\u5316\u5b66\u4e60\uff09\u4f18\u5316\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u7b80\u5355\u548c\u590d\u6742\u67e5\u8be2\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u5ef6\u8fdf\u589e\u52a0\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u9884\u4f30\u8ba1\u67e5\u8be2\u96be\u5ea6\u5bf9\u5e94\u7684\u63a8\u7406\u6210\u672c\uff1b2) \u9884\u7b97\u5f15\u5bfc\u7684GPRO\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u54cd\u5e94\u957f\u5ea6\u538b\u7f29\u8fbe74.47%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "SelfBudgeter\u80fd\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u5408\u7406\u5206\u914d\u9884\u7b97\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.10880", "pdf": "https://arxiv.org/pdf/2505.10880", "abs": "https://arxiv.org/abs/2505.10880", "authors": ["Guoji Fu", "Wee Sun Lee"], "title": "Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions", "categories": ["cs.LG", "stat.ML"], "comment": "94 pages", "summary": "This paper studies the approximation and generalization abilities of\nscore-based neural network generative models (SGMs) in estimating an unknown\ndistribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming\nmerely that $P_0$ is $\\alpha$-sub-Gaussian, we prove that for any time step $t\n\\in [t_0, n^{O(1)}]$, where $t_0 \\geq O(\\alpha^2n^{-2/d}\\log n)$, there exists\na deep ReLU neural network with width $\\leq O(\\log^3n)$ and depth $\\leq\nO(n^{3/d}\\log_2n)$ that can approximate the scores with $\\tilde{O}(n^{-1})$\nmean square error and achieve a nearly optimal rate of\n$\\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score\nmatching loss. Our framework is universal and can be used to establish\nconvergence rates for SGMs under milder assumptions than previous work. For\nexample, assuming further that the target density function $p_0$ lies in\nSobolev or Besov classes, with an appropriately early stopping strategy, we\ndemonstrate that neural network-based SGMs can attain nearly minimax\nconvergence rates up to logarithmic factors. Our analysis removes several\ncrucial assumptions, such as Lipschitz continuity of the score function or a\nstrictly positive lower bound on the target density.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u5728\u4f30\u8ba1\u672a\u77e5\u5206\u5e03\u65f6\u7684\u8fd1\u4f3c\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5728\u8f83\u5f31\u7684\u5047\u8bbe\u4e0b\uff0cSGMs\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u7814\u7a76SGMs\u5728\u5206\u5e03\u4f30\u8ba1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8f83\u5f31\u7684\u5047\u8bbe\u4e0b\uff0c\u586b\u8865\u73b0\u6709\u5de5\u4f5c\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6ReLU\u795e\u7ecf\u7f51\u7edc\uff0c\u5206\u6790\u5176\u5728\u5206\u6570\u4f30\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u65e9\u671f\u505c\u6b62\u7b56\u7565\u3002", "result": "\u8bc1\u660e\u4e86SGMs\u5728\u8f83\u5f31\u7684\u5047\u8bbe\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6536\u655b\u901f\u7387\uff0c\u4e14\u53bb\u9664\u4e86\u5173\u952e\u5047\u8bbe\uff08\u5982\u5206\u6570\u51fd\u6570\u7684Lipschitz\u8fde\u7eed\u6027\uff09\u3002", "conclusion": "SGMs\u5728\u5206\u5e03\u4f30\u8ba1\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.10921", "pdf": "https://arxiv.org/pdf/2505.10921", "abs": "https://arxiv.org/abs/2505.10921", "authors": ["Junyi Yuan", "Jian Zhang", "Fangyu Wu", "Dongming Lu", "Huanda Lu", "Qiufeng Wang"], "title": "Towards Cross-modal Retrieval in Chinese Cultural Heritage Documents: Dataset and Solution", "categories": ["cs.CV"], "comment": null, "summary": "China has a long and rich history, encompassing a vast cultural heritage that\nincludes diverse multimodal information, such as silk patterns, Dunhuang\nmurals, and their associated historical narratives. Cross-modal retrieval plays\na pivotal role in understanding and interpreting Chinese cultural heritage by\nbridging visual and textual modalities to enable accurate text-to-image and\nimage-to-text retrieval. However, despite the growing interest in multimodal\nresearch, there is a lack of specialized datasets dedicated to Chinese cultural\nheritage, limiting the development and evaluation of cross-modal learning\nmodels in this domain. To address this gap, we propose a multimodal dataset\nnamed CulTi, which contains 5,726 image-text pairs extracted from two series of\nprofessional documents, respectively related to ancient Chinese silk and\nDunhuang murals. Compared to existing general-domain multimodal datasets, CulTi\npresents a challenge for cross-modal retrieval: the difficulty of local\nalignment between intricate decorative motifs and specialized textual\ndescriptions. To address this challenge, we propose LACLIP, a training-free\nlocal alignment strategy built upon a fine-tuned Chinese-CLIP. LACLIP enhances\nthe alignment of global textual descriptions with local visual regions by\ncomputing weighted similarity scores during inference. Experimental results on\nCulTi demonstrate that LACLIP significantly outperforms existing models in\ncross-modal retrieval, particularly in handling fine-grained semantic\nassociations within Chinese cultural heritage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCulTi\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u4e2d\u56fd\u6587\u5316\u9057\u4ea7\u7684\u8de8\u6a21\u6001\u68c0\u7d22\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c40\u90e8\u5bf9\u9f50\u7b56\u7565LACLIP\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u68c0\u7d22\u7814\u7a76\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u4e2d\u56fd\u6587\u5316\u9057\u4ea7\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u6a21\u578b\u7684\u53d1\u5c55\u4e0e\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u5fae\u8c03\u7684\u4e2d\u6587CLIP\u6a21\u578b\uff0c\u63d0\u51faLACLIP\u7b56\u7565\uff0c\u901a\u8fc7\u52a0\u6743\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5b9e\u73b0\u5c40\u90e8\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLACLIP\u5728CulTi\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5173\u8054\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CulTi\u6570\u636e\u96c6\u548cLACLIP\u7b56\u7565\u586b\u8865\u4e86\u4e2d\u56fd\u6587\u5316\u9057\u4ea7\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277", "abs": "https://arxiv.org/abs/2505.11277", "authors": ["Yaorui Shi", "Shihan Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.", "AI": {"tldr": "AutoRefine\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u641c\u7d22-\u7cbe\u70bc-\u601d\u8003\u201d\u8303\u5f0f\u63d0\u5347LLM\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u50a8\u5907\u6709\u9650\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5e38\u56e0\u68c0\u7d22\u65e0\u5173\u6216\u566a\u58f0\u4fe1\u606f\u800c\u5f71\u54cd\u63a8\u7406\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faAutoRefine\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5165\u77e5\u8bc6\u7cbe\u70bc\u6b65\u9aa4\u548c\u68c0\u7d22\u7279\u5b9a\u5956\u52b1\uff0c\u4f18\u5316\u68c0\u7d22\u4e0e\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u5355\u8df3\u548c\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e2d\u3002", "conclusion": "AutoRefine\u901a\u8fc7\u9ad8\u6548\u641c\u7d22\u548c\u8bc1\u636e\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u7684\u8d28\u91cf\u3002", "relevance": 85.0}}
{"id": "2505.11289", "pdf": "https://arxiv.org/pdf/2505.11289", "abs": "https://arxiv.org/abs/2505.11289", "authors": ["Reginald McLean", "Evangelos Chatzaroulas", "Luc McCutcheon", "Frank R\u00f6der", "Tianhe Yu", "Zhanpeng He", "K. R. Zentner", "Ryan Julian", "J K Terry", "Isaac Woungang", "Nariman Farsad", "Pablo Samuel Castro"], "title": "Meta-World+: An Improved, Standardized, RL Benchmark", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Meta-World is widely used for evaluating multi-task and meta-reinforcement\nlearning agents, which are challenged to master diverse skills simultaneously.\nSince its introduction however, there have been numerous undocumented changes\nwhich inhibit a fair comparison of algorithms. This work strives to\ndisambiguate these results from the literature, while also leveraging the past\nversions of Meta-World to provide insights into multi-task and\nmeta-reinforcement learning benchmark design. Through this process we release a\nnew open-source version of Meta-World\n(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility\nof past results, is more technically ergonomic, and gives users more control\nover the tasks that are included in a task set.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Meta-World\u57fa\u51c6\u6d4b\u8bd5\u7684\u672a\u8bb0\u5f55\u53d8\u5316\u5bf9\u7b97\u6cd5\u516c\u5e73\u6bd4\u8f83\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u5f00\u6e90\u7248\u672c\uff0c\u786e\u4fdd\u7ed3\u679c\u53ef\u590d\u73b0\u6027\u3001\u6280\u672f\u6613\u7528\u6027\u548c\u7528\u6237\u5bf9\u4efb\u52a1\u96c6\u7684\u66f4\u591a\u63a7\u5236\u3002", "motivation": "Meta-World\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u591a\u4efb\u52a1\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u4f46\u672a\u8bb0\u5f55\u7684\u53d8\u66f4\u5f71\u54cd\u4e86\u7b97\u6cd5\u7684\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u5206\u6790Meta-World\u7684\u5386\u53f2\u7248\u672c\uff0c\u63d0\u51fa\u6539\u8fdb\u57fa\u51c6\u8bbe\u8ba1\u7684\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u65b0\u7248\u672c\u3002", "result": "\u65b0\u7248\u672c\u7684Meta-World\u5177\u6709\u66f4\u597d\u7684\u53ef\u590d\u73b0\u6027\u3001\u6280\u672f\u6613\u7528\u6027\u548c\u7528\u6237\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u65b0\u7248\u672cMeta-World\u89e3\u51b3\u4e86\u5386\u53f2\u95ee\u9898\uff0c\u4e3a\u591a\u4efb\u52a1\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.10881", "pdf": "https://arxiv.org/pdf/2505.10881", "abs": "https://arxiv.org/abs/2505.10881", "authors": ["Donghyeon Ki", "JunHyeok Oh", "Seong-Woong Shim", "Byung-Jun Lee"], "title": "Prior-Guided Diffusion Planning for Offline Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have recently gained prominence in offline reinforcement\nlearning due to their ability to effectively learn high-performing,\ngeneralizable policies from static datasets. Diffusion-based planners\nfacilitate long-horizon decision-making by generating high-quality trajectories\nthrough iterative denoising, guided by return-maximizing objectives. However,\nexisting guided sampling strategies such as Classifier Guidance,\nClassifier-Free Guidance, and Monte Carlo Sample Selection either produce\nsuboptimal multi-modal actions, struggle with distributional drift, or incur\nprohibitive inference-time costs. To address these challenges, we propose Prior\nGuidance (PG), a novel guided sampling framework that replaces the standard\nGaussian prior of a behavior-cloned diffusion model with a learnable\ndistribution, optimized via a behavior-regularized objective. PG directly\ngenerates high-value trajectories without costly reward optimization of the\ndiffusion model itself, and eliminates the need to sample multiple candidates\nat inference for sample selection. We present an efficient training strategy\nthat applies behavior regularization in latent space, and empirically\ndemonstrate that PG outperforms state-of-the-art diffusion policies and\nplanners across diverse long-horizon offline RL benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrior Guidance\uff08PG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6269\u6563\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u66ff\u6362\u6807\u51c6\u9ad8\u65af\u5148\u9a8c\u4e3a\u53ef\u5b66\u4e60\u7684\u5206\u5e03\uff0c\u4f18\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u4e0d\u7406\u60f3\u3001\u5206\u5e03\u6f02\u79fb\u6216\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0cPG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "PG\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u5206\u5e03\u66ff\u4ee3\u6807\u51c6\u9ad8\u65af\u5148\u9a8c\uff0c\u901a\u8fc7\u884c\u4e3a\u6b63\u5219\u5316\u76ee\u6807\u4f18\u5316\uff0c\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u9ad8\u4ef7\u503c\u8f68\u8ff9\uff0c\u907f\u514d\u6602\u8d35\u7684\u5956\u52b1\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPG\u5728\u591a\u6837\u5316\u7684\u957f\u89c6\u91ce\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u4e2d\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u7b56\u7565\u548c\u89c4\u5212\u5668\u3002", "conclusion": "PG\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6269\u6563\u6a21\u578b\u5f15\u5bfc\u91c7\u6837\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "relevance": 60.0}}
{"id": "2505.10931", "pdf": "https://arxiv.org/pdf/2505.10931", "abs": "https://arxiv.org/abs/2505.10931", "authors": ["Chao Wang", "Wei Lu", "Xiang Li", "Jian Yang", "Lei Luo"], "title": "M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Single-source remote sensing object detection using optical or SAR images\nstruggles in complex environments. Optical images offer rich textural details\nbut are often affected by low-light, cloud-obscured, or low-resolution\nconditions, reducing the detection performance. SAR images are robust to\nweather, but suffer from speckle noise and limited semantic expressiveness.\nOptical and SAR images provide complementary advantages, and fusing them can\nsignificantly improve the detection accuracy. However, progress in this field\nis hindered by the lack of large-scale, standardized datasets. To address these\nchallenges, we propose the first comprehensive dataset for optical-SAR fusion\nobject detection, named Multi-resolution, Multi-polarization, Multi-scene,\nMulti-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image\npairs and nearly one million labeled instances with arbitrary orientations,\nspanning six key categories. To enable standardized evaluation, we develop a\nunified benchmarking toolkit that integrates six state-of-the-art multi-source\nfusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end\nmulti-source fusion detection framework that mitigates cross-domain\ndiscrepancies and establishes a robust baseline for future studies. Extensive\nexperiments on M4-SAR demonstrate that fusing optical and SAR data can improve\n$mAP$ by 5.7\\% over single-source inputs, with particularly significant gains\nin complex environments. The dataset and code are publicly available at\nhttps://github.com/wchao0601/M4-SAR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5149\u5b66-SAR\u878d\u5408\u76ee\u6807\u68c0\u6d4b\u7684\u7efc\u5408\u6570\u636e\u96c6M4-SAR\uff0c\u5e76\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u57fa\u51c6\u5de5\u5177\u5305\u548c\u7aef\u5230\u7aef\u878d\u5408\u68c0\u6d4b\u6846\u67b6E2E-OSDet\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5149\u5b66\u548cSAR\u56fe\u50cf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5355\u6e90\u68c0\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u878d\u5408\u4e92\u8865\u4f18\u52bf\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faM4-SAR\u6570\u636e\u96c6\uff08112,184\u5bf9\u56fe\u50cf\uff09\u3001\u7edf\u4e00\u57fa\u51c6\u5de5\u5177\u5305\u548cE2E-OSDet\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6e90\u878d\u5408\u68c0\u6d4b\u3002", "result": "\u878d\u5408\u5149\u5b66\u548cSAR\u6570\u636e\u4f7fmAP\u63d0\u53475.7%\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u679c\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "M4-SAR\u548cE2E-OSDet\u4e3a\u591a\u6e90\u878d\u5408\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6570\u636e\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.11280", "pdf": "https://arxiv.org/pdf/2505.11280", "abs": "https://arxiv.org/abs/2505.11280", "authors": ["Horacio Thompson", "Esa\u00fa Villatoro-Tello", "Manuel Montes-y-G\u00f3mez", "Marcelo Errecalde"], "title": "Temporal fine-tuning for early risk detection", "categories": ["cs.CL"], "comment": "In: Proceedings of the 53rd JAIIO / 50th CLEI - ASAID, 2024, p. 137.\n  ISSN: 2451-7496", "summary": "Early Risk Detection (ERD) on the Web aims to identify promptly users facing\nsocial and health issues. Users are analyzed post-by-post, and it is necessary\nto guarantee correct and quick answers, which is particularly challenging in\ncritical scenarios. ERD involves optimizing classification precision and\nminimizing detection delay. Standard classification metrics may not suffice,\nresorting to specific metrics such as ERDE(theta) that explicitly consider\nprecision and delay. The current research focuses on applying a multi-objective\napproach, prioritizing classification performance and establishing a separate\ncriterion for decision time. In this work, we propose a completely different\nstrategy, temporal fine-tuning, which allows tuning transformer-based models by\nexplicitly incorporating time within the learning process. Our method allows us\nto analyze complete user post histories, tune models considering different\ncontexts, and evaluate training performance using temporal metrics. We\nevaluated our proposal in the depression and eating disorders tasks for the\nSpanish language, achieving competitive results compared to the best models of\nMentalRiskES 2023. We found that temporal fine-tuning optimized decisions\nconsidering context and time progress. In this way, by properly taking\nadvantage of the power of transformers, it is possible to address ERD by\ncombining precision and speed as a single objective.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\uff08ERD\uff09\u4e2d\u7684\u5206\u7c7b\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\uff08ERD\uff09\u9700\u8981\u5728\u4fdd\u8bc1\u5206\u7c7b\u7cbe\u5ea6\u7684\u540c\u65f6\u6700\u5c0f\u5316\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u8fd9\u4e24\u70b9\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u5fae\u8c03\u7b56\u7565\uff0c\u5c06\u65f6\u95f4\u56e0\u7d20\u663e\u5f0f\u7eb3\u5165Transformer\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5206\u6790\u7528\u6237\u5b8c\u6574\u53d1\u5e16\u5386\u53f2\u3002", "result": "\u5728\u897f\u73ed\u7259\u8bed\u7684\u6291\u90c1\u548c\u996e\u98df\u969c\u788d\u4efb\u52a1\u4e2d\uff0c\u53d6\u5f97\u4e86\u4e0eMentalRiskES 2023\u6700\u4f73\u6a21\u578b\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "\u65f6\u95f4\u5fae\u8c03\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u8fdb\u5c55\u4f18\u5316\u51b3\u7b56\uff0c\u5c06\u7cbe\u5ea6\u548c\u901f\u5ea6\u7edf\u4e00\u4e3a\u4e00\u4e2a\u76ee\u6807\u3002", "relevance": 40.0}}
{"id": "2505.11451", "pdf": "https://arxiv.org/pdf/2505.11451", "abs": "https://arxiv.org/abs/2505.11451", "authors": ["Lee Harris", "James Bentham", "Philippe De Wilde"], "title": "Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps", "categories": ["cs.AI"], "comment": null, "summary": "Dates often contribute towards highly impactful medical decisions, but it is\nrarely clear how to extract this data. AI has only just begun to be used\ntranscribe such documents, and common methods are either to trust that the\noutput produced by a complex AI model, or to parse the text using regular\nexpressions. Recent work has established that regular expressions are an\nexplainable form of logic, but it is difficult to decompose these into the\ncomponent parts that are required to construct precise UNIX timestamps. First,\nwe test publicly-available regular expressions, and we found that these were\nunable to capture a significant number of our dates. Next, we manually created\neasily-decomposable regular expressions, and we found that these were able to\ndetect the majority of real dates, but also a lot of sequences of text that\nlook like dates. Finally, we used regular expression synthesis to automatically\nidentify regular expressions from the reverse-engineered UNIX timestamps that\nwe created. We find that regular expressions created by regular expression\nsynthesis detect far fewer sequences of text that look like dates than those\nthat were manually created, at the cost of a slight increase to the number of\nmissed dates. Overall, our results show that regular expressions can be created\nthrough regular expression synthesis to identify complex dates and date ranges\nin text transcriptions. To our knowledge, our proposed way of learning\ndeterministic logic by reverse-engineering several many-one mappings and\nfeeding these into a regular expression synthesiser is a new approach.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4ece\u6587\u672c\u8f6c\u5f55\u4e2d\u63d0\u53d6\u590d\u6742\u65e5\u671f\u548c\u65e5\u671f\u8303\u56f4\uff0c\u6bd4\u8f83\u4e86\u624b\u52a8\u521b\u5efa\u548c\u6b63\u5219\u8868\u8fbe\u5f0f\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u540e\u8005\u5728\u51cf\u5c11\u8bef\u62a5\u65b9\u9762\u66f4\u4f18\u3002", "motivation": "\u65e5\u671f\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u590d\u6742AI\u6a21\u578b\u6216\u6b63\u5219\u8868\u8fbe\u5f0f\uff09\u96be\u4ee5\u7cbe\u786e\u63d0\u53d6\u3002", "method": "\u6d4b\u8bd5\u516c\u5f00\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u624b\u52a8\u521b\u5efa\u53ef\u5206\u89e3\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u5e76\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u5408\u6210\u81ea\u52a8\u751f\u6210\u6b63\u5219\u8868\u8fbe\u5f0f\u3002", "result": "\u6b63\u5219\u8868\u8fbe\u5f0f\u5408\u6210\u751f\u6210\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u8bef\u62a5\u66f4\u5c11\uff0c\u4f46\u6f0f\u62a5\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u6b63\u5219\u8868\u8fbe\u5f0f\u5408\u6210\u662f\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4ece\u6587\u672c\u4e2d\u8bc6\u522b\u590d\u6742\u65e5\u671f\u548c\u65e5\u671f\u8303\u56f4\u3002", "relevance": 30.0}}
{"id": "2505.10882", "pdf": "https://arxiv.org/pdf/2505.10882", "abs": "https://arxiv.org/abs/2505.10882", "authors": ["Alex Saad-Falcon", "Brighton Ancelin", "Justin Romberg"], "title": "Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper addresses the challenge of efficient principal component analysis\n(PCA) in high-dimensional spaces by analyzing a compressively sampled variant\nof Oja's algorithm with adaptive sensing. Traditional PCA methods incur\nsubstantial computational costs that scale poorly with data dimensionality,\nwhereas subspace tracking algorithms like Oja's offer more efficient\nalternatives but typically require full-dimensional observations. We analyze a\nvariant where, at each iteration, only two compressed measurements are taken:\none in the direction of the current estimate and one in a random orthogonal\ndirection. We prove that this adaptive sensing approach achieves global\nconvergence in the presence of noise when tracking the leading eigenvector of a\ndatastream with eigengap $\\Delta=\\lambda_1-\\lambda_2$. Our theoretical analysis\ndemonstrates that the algorithm experiences two phases: (1) a warmup phase\nrequiring $O(\\lambda_1\\lambda_2d^2/\\Delta^2)$ iterations to achieve a\nconstant-level alignment with the true eigenvector, followed by (2) a local\nconvergence phase where the sine alignment error decays at a rate of\n$O(\\lambda_1\\lambda_2d^2/\\Delta^2 t)$ for iterations $t$. The guarantee aligns\nwith existing minimax lower bounds with an added factor of $d$ due to the\ncompressive sampling. This work provides the first convergence guarantees in\nadaptive sensing for subspace tracking with noise. Our proof technique is also\nconsiderably simpler than those in prior works. The results have important\nimplications for applications where acquiring full-dimensional samples is\nchallenging or costly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u538b\u7f29\u91c7\u6837\u7684Oja\u7b97\u6cd5\u53d8\u4f53\uff0c\u7528\u4e8e\u9ad8\u6548\u9ad8\u7ef4PCA\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u5b9e\u73b0\u5168\u5c40\u6536\u655b\u3002", "motivation": "\u4f20\u7edfPCA\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u5b50\u7a7a\u95f4\u8ddf\u8e2a\u7b97\u6cd5\u5982Oja\u7b97\u6cd5\u867d\u9ad8\u6548\u4f46\u901a\u5e38\u9700\u8981\u5168\u7ef4\u5ea6\u89c2\u6d4b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u538b\u7f29\u91c7\u6837\u53d8\u4f53\u7684Oja\u7b97\u6cd5\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u91c7\u96c6\u4e24\u4e2a\u538b\u7f29\u6d4b\u91cf\uff1a\u4e00\u4e2a\u6cbf\u5f53\u524d\u4f30\u8ba1\u65b9\u5411\uff0c\u4e00\u4e2a\u6cbf\u968f\u673a\u6b63\u4ea4\u65b9\u5411\u3002", "result": "\u7b97\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u80fd\u5168\u5c40\u6536\u655b\uff0c\u5206\u4e3a\u9884\u70ed\u9636\u6bb5\u548c\u5c40\u90e8\u6536\u655b\u9636\u6bb5\uff0c\u6536\u655b\u901f\u7387\u4e0e\u73b0\u6709\u6781\u5c0f\u6781\u5927\u4e0b\u754c\u4e00\u81f4\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u81ea\u9002\u5e94\u611f\u77e5\u5b50\u7a7a\u95f4\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u6536\u655b\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u5168\u7ef4\u5ea6\u91c7\u6837\u56f0\u96be\u6216\u6602\u8d35\u7684\u573a\u666f\u3002", "relevance": 30.0}}
{"id": "2505.10996", "pdf": "https://arxiv.org/pdf/2505.10996", "abs": "https://arxiv.org/abs/2505.10996", "authors": ["Yunkang Cao", "Yuqi Cheng", "Xiaohao Xu", "Yiheng Zhang", "Yihan Sun", "Yuxiang Tan", "Yuxin Zhang", "Xiaonan Huang", "Weiming Shen"], "title": "Visual Anomaly Detection under Complex View-Illumination Interplay: A Large-Scale Benchmark", "categories": ["cs.CV"], "comment": "Homgepage: https://hustcyq.github.io/M2AD/. Yunkang Cao and Yuqi\n  Cheng contribute equally to this work", "summary": "The practical deployment of Visual Anomaly Detection (VAD) systems is\nhindered by their sensitivity to real-world imaging variations, particularly\nthe complex interplay between viewpoint and illumination which drastically\nalters defect visibility. Current benchmarks largely overlook this critical\nchallenge. We introduce Multi-View Multi-Illumination Anomaly Detection (M2AD),\na new large-scale benchmark comprising 119,880 high-resolution images designed\nexplicitly to probe VAD robustness under such interacting conditions. By\nsystematically capturing 999 specimens across 10 categories using 12\nsynchronized views and 10 illumination settings (120 configurations total),\nM2AD enables rigorous evaluation. We establish two evaluation protocols:\nM2AD-Synergy tests the ability to fuse information across diverse\nconfigurations, and M2AD-Invariant measures single-image robustness against\nrealistic view-illumination effects. Our extensive benchmarking shows that\nstate-of-the-art VAD methods struggle significantly on M2AD, demonstrating the\nprofound challenge posed by view-illumination interplay. This benchmark serves\nas an essential tool for developing and validating VAD methods capable of\novercoming real-world complexities. Our full dataset and test suite will be\nreleased at https://hustcyq.github.io/M2AD to facilitate the field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aM2AD\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u7cfb\u7edf\u5728\u89c6\u89d2\u548c\u5149\u7167\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5305\u542b119,880\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "motivation": "\u5f53\u524dVAD\u7cfb\u7edf\u5bf9\u771f\u5b9e\u4e16\u754c\u6210\u50cf\u53d8\u5316\uff08\u5c24\u5176\u662f\u89c6\u89d2\u548c\u5149\u7167\u7684\u590d\u6742\u4ea4\u4e92\uff09\u7684\u654f\u611f\u6027\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\uff0c\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5145\u5206\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6355\u83b710\u4e2a\u7c7b\u522b\u7684999\u4e2a\u6837\u672c\uff0c\u572812\u4e2a\u540c\u6b65\u89c6\u89d2\u548c10\u79cd\u5149\u7167\u8bbe\u7f6e\u4e0b\u751f\u6210120\u79cd\u914d\u7f6e\uff0c\u6784\u5efaM2AD\u57fa\u51c6\u3002\u63d0\u51fa\u4e24\u79cd\u8bc4\u4f30\u534f\u8bae\uff1aM2AD-Synergy\u6d4b\u8bd5\u8de8\u914d\u7f6e\u4fe1\u606f\u878d\u5408\u80fd\u529b\uff0cM2AD-Invariant\u6d4b\u8bd5\u5355\u56fe\u50cf\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709VAD\u65b9\u6cd5\u5728M2AD\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u4e86\u89c6\u89d2-\u5149\u7167\u4ea4\u4e92\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "M2AD\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684VAD\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002", "relevance": 30.0}}
{"id": "2505.11297", "pdf": "https://arxiv.org/pdf/2505.11297", "abs": "https://arxiv.org/abs/2505.11297", "authors": ["Gal Astrach", "Yuval Pinter"], "title": "Probing Subphonemes in Morphology Models", "categories": ["cs.CL"], "comment": null, "summary": "Transformers have achieved state-of-the-art performance in morphological\ninflection tasks, yet their ability to generalize across languages and\nmorphological rules remains limited. One possible explanation for this behavior\ncan be the degree to which these models are able to capture implicit phenomena\nat the phonological and subphonemic levels. We introduce a language-agnostic\nprobing method to investigate phonological feature encoding in transformers\ntrained directly on phonemes, and perform it across seven morphologically\ndiverse languages. We show that phonological features which are local, such as\nfinal-obstruent devoicing in Turkish, are captured well in phoneme embeddings,\nwhereas long-distance dependencies like vowel harmony are better represented in\nthe transformer's encoder. Finally, we discuss how these findings inform\nempirical strategies for training morphological models, particularly regarding\nthe role of subphonemic feature acquisition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u5f62\u6001\u5b66\u4efb\u52a1\u4e2d\u5bf9\u97f3\u97f5\u7279\u5f81\u7684\u7f16\u7801\u80fd\u529b\uff0c\u53d1\u73b0\u5c40\u90e8\u7279\u5f81\uff08\u5982\u571f\u8033\u5176\u8bed\u7684\u5c3e\u97f3\u6e05\u5316\uff09\u5728\u97f3\u7d20\u5d4c\u5165\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u800c\u957f\u8ddd\u79bb\u4f9d\u8d56\uff08\u5982\u5143\u97f3\u548c\u8c10\uff09\u5219\u5728\u7f16\u7801\u5668\u4e2d\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22Transformer\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u5f62\u6001\u5b66\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5176\u5bf9\u97f3\u97f5\u548c\u6b21\u97f3\u4f4d\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u63a2\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8e\u97f3\u7d20\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u4e2d\u7684\u97f3\u97f5\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u5728\u4e03\u79cd\u5f62\u6001\u591a\u6837\u7684\u8bed\u8a00\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5c40\u90e8\u97f3\u97f5\u7279\u5f81\u5728\u97f3\u7d20\u5d4c\u5165\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u800c\u957f\u8ddd\u79bb\u4f9d\u8d56\u7279\u5f81\u5728Transformer\u7684\u7f16\u7801\u5668\u4e2d\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f62\u6001\u5b66\u6a21\u578b\u7684\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5f3a\u8c03\u4e86\u6b21\u97f3\u4f4d\u7279\u5f81\u83b7\u53d6\u7684\u91cd\u8981\u6027\u3002", "relevance": 60.0}}
{"id": "2505.11478", "pdf": "https://arxiv.org/pdf/2505.11478", "abs": "https://arxiv.org/abs/2505.11478", "authors": ["Mingxuan Li", "Junzhe Zhang", "Elias Bareinboim"], "title": "Automatic Reward Shaping from Confounded Offline Data", "categories": ["cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u9488\u5bf9\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u504f\u5dee\u7684\u9ad8\u7ef4\u590d\u6742\u9886\u57df\uff0c\u4f18\u5316\u4e86\u6807\u51c6DQN\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u504f\u5dee\u7684\u9ad8\u7ef4\u590d\u6742\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u6539\u8fdbDQN\u7b97\u6cd5\u5b9e\u73b0\u66f4\u4f18\u7684\u51b3\u7b56\u7b56\u7565\u3002", "method": "\u57fa\u4e8eDQN\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5bfb\u627e\u4e0e\u89c2\u6d4b\u6570\u636e\u517c\u5bb9\u7684\u6700\u574f\u60c5\u51b5\u4e0b\u5b89\u5168\u7b56\u7565\u3002", "result": "\u572812\u4e2a\u5b58\u5728\u6df7\u6742\u504f\u5dee\u7684Atari\u6e38\u620f\u4e2d\uff0c\u65b0\u7b97\u6cd5\u5728\u6240\u6709\u8f93\u5165\u884c\u4e3a\u4e0e\u76ee\u6807\u7b56\u7565\u4e0d\u5339\u914d\u7684\u6e38\u620f\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6DQN\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u5904\u7406\u672a\u89c2\u6d4b\u6df7\u6742\u504f\u5dee\u65f6\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u9ad8\u7ef4\u73af\u5883\u3002", "relevance": 70.0}}
{"id": "2505.10892", "pdf": "https://arxiv.org/pdf/2505.10892", "abs": "https://arxiv.org/abs/2505.10892", "authors": ["Akhil Agnihotri", "Rahul Jain", "Deepak Ramachandran", "Zheng Wen"], "title": "Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2406.18853 by other authors", "summary": "Post-training of LLMs with RLHF, and subsequently preference optimization\nalgorithms such as DPO, IPO, etc., made a big difference in improving human\nalignment. However, all such techniques can only work with a single (human)\nobjective. In practice, human users have multiple objectives, such as\nhelpfulness and harmlessness, and there is no natural way to aggregate them\ninto a single objective. In this paper, we address the multi-objective\npreference-alignment problem, where a policy must optimize several, potentially\nconflicting, objectives. We introduce the Multi-Objective Preference\nOptimization (MOPO) algorithm, which frames alignment as a constrained\nKL-regularized optimization: the primary objective is maximized while secondary\nobjectives are lower-bounded by tunable safety thresholds. Unlike prior work,\nMOPO operates directly on pairwise preference data, requires no point-wise\nreward assumption, and avoids heuristic prompt-context engineering. The method\nrecovers policies on the Pareto front whenever the front is attainable;\npractically, it reduces to simple closed-form iterative updates suitable for\nlarge-scale training. On synthetic benchmarks with diverse canonical preference\nstructures, we show that MOPO approximates the Pareto front. When fine-tuning a\n1.3B-parameter language model on real-world human-preference datasets, MOPO\nattains higher rewards and yields policies that Pareto-dominate baselines;\nablation studies confirm optimization stability and robustness to\nhyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u76ee\u6807\u504f\u597d\u4f18\u5316\u7b97\u6cd5MOPO\uff0c\u89e3\u51b3LLM\u5728RLHF\u540e\u8bad\u7ec3\u4e2d\u591a\u76ee\u6807\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u65e0\u9700\u5355\u76ee\u6807\u805a\u5408\u3002", "motivation": "\u73b0\u6709RLHF\u548c\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u4e00\u76ee\u6807\uff0c\u800c\u5b9e\u9645\u4e2d\u7528\u6237\u6709\u591a\u76ee\u6807\u9700\u6c42\uff08\u5982\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\uff09\uff0c\u7f3a\u4e4f\u81ea\u7136\u805a\u5408\u65b9\u6cd5\u3002", "method": "MOPO\u5c06\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3a\u5e26\u7ea6\u675f\u7684KL\u6b63\u5219\u5316\u4f18\u5316\uff0c\u4e3b\u76ee\u6807\u6700\u5927\u5316\uff0c\u6b21\u76ee\u6807\u901a\u8fc7\u5b89\u5168\u9608\u503c\u7ea6\u675f\u3002\u76f4\u63a5\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u65e0\u9700\u70b9\u5f0f\u5956\u52b1\u5047\u8bbe\u6216\u542f\u53d1\u5f0f\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOPO\u903c\u8fd1Pareto\u524d\u6cbf\uff1b\u57281.3B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cMOPO\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u4e14Pareto\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "MOPO\u6709\u6548\u89e3\u51b3\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u95ee\u9898\uff0c\u5177\u6709\u4f18\u5316\u7a33\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u3002", "relevance": 90.0}}
{"id": "2505.10999", "pdf": "https://arxiv.org/pdf/2505.10999", "abs": "https://arxiv.org/abs/2505.10999", "authors": ["Weilai Xiang", "Hongyu Yang", "Di Huang", "Yunhong Wang"], "title": "DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have gained prominence in image synthesis, their\ngenerative pre-training has been shown to yield discriminative representations,\npaving the way towards unified visual generation and understanding. However,\ntwo key questions remain: 1) Can these representations be leveraged to improve\nthe training of diffusion models themselves, rather than solely benefiting\ndownstream tasks? 2) Can the feature quality be enhanced to rival or even\nsurpass modern self-supervised learners, without compromising generative\ncapability? This work addresses these questions by introducing\nself-conditioning, a straightforward yet effective mechanism that internally\nleverages the rich semantics inherent in denoising network to guide its own\ndecoding layers, forming a tighter bottleneck that condenses high-level\nsemantics to improve generation. Results are compelling: our method boosts both\ngeneration FID and recognition accuracy with 1% computational overhead and\ngeneralizes across diverse diffusion architectures. Crucially,\nself-conditioning facilitates an effective integration of discriminative\ntechniques, such as contrastive self-distillation, directly into diffusion\nmodels without sacrificing generation quality. Extensive experiments on\npixel-space and latent-space datasets show that in linear evaluations, our\nenhanced diffusion models, particularly UViT and DiT, serve as strong\nrepresentation learners, surpassing various self-supervised models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6761\u4ef6\u673a\u5236\uff0c\u901a\u8fc7\u5229\u7528\u53bb\u566a\u7f51\u7edc\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u6765\u6307\u5bfc\u89e3\u7801\u5c42\uff0c\u4ece\u800c\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u548c\u8868\u5f81\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9884\u8bad\u7ec3\u4e2d\u83b7\u5f97\u7684\u8868\u5f81\u662f\u5426\u80fd\u7528\u4e8e\u6539\u8fdb\u6a21\u578b\u81ea\u8eab\u8bad\u7ec3\uff0c\u5e76\u63d0\u5347\u7279\u5f81\u8d28\u91cf\u4ee5\u5ab2\u7f8e\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u5f15\u5165\u81ea\u6761\u4ef6\u673a\u5236\uff0c\u5229\u7528\u53bb\u566a\u7f51\u7edc\u7684\u8bed\u4e49\u4fe1\u606f\u6307\u5bfc\u89e3\u7801\u5c42\uff0c\u5f62\u6210\u66f4\u7d27\u5bc6\u7684\u74f6\u9888\u4ee5\u63d0\u5347\u751f\u6210\u548c\u8868\u5f81\u80fd\u529b\u3002", "result": "\u5728\u8ba1\u7b97\u5f00\u9500\u4ec5\u589e\u52a01%\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210FID\u548c\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u7ebf\u6027\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u591a\u79cd\u81ea\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u81ea\u6761\u4ef6\u673a\u5236\u6709\u6548\u6574\u5408\u4e86\u5224\u522b\u6027\u6280\u672f\uff08\u5982\u5bf9\u6bd4\u81ea\u84b8\u998f\uff09\uff0c\u5728\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\u3002", "relevance": 60.0}}
{"id": "2505.11336", "pdf": "https://arxiv.org/pdf/2505.11336", "abs": "https://arxiv.org/abs/2505.11336", "authors": ["Nuo Chen", "Andre Lin HuiKai", "Jiaying Wu", "Junyi Hou", "Zining Zhang", "Qian Wang", "Xidong Wang", "Bingsheng He"], "title": "XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision", "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4eba\u7c7b-AI\u534f\u4f5c\u6846\u67b6XtraGPT\uff0c\u7528\u4e8e\u5b66\u672f\u8bba\u6587\u4fee\u8ba2\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5f00\u6e90LLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u5199\u4f5c\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u5728\u79d1\u5b66\u5199\u4f5c\u652f\u6301\u4e0a\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7814\u7a76\u6c9f\u901a\u7684\u6df1\u5c42\u9700\u6c42\uff0c\u5982\u8de8\u7ae0\u8282\u7684\u6982\u5ff5\u8fde\u8d2f\u6027\u3002\u5b66\u672f\u5199\u4f5c\u7684\u8fed\u4ee3\u6027\u548c\u4fee\u8ba2\u9a71\u52a8\u7279\u6027\u4e5f\u672a\u88ab\u73b0\u6709\u76f4\u63a5\u63d0\u793a\u8303\u5f0f\u5145\u5206\u652f\u6301\u3002", "method": "\u6784\u5efa\u5305\u542b7,040\u7bc7\u7814\u7a76\u8bba\u6587\u548c140,000\u6761\u6307\u4ee4-\u54cd\u5e94\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u5f00\u6e90LLM XtraGPT\uff081.5B\u81f314B\u53c2\u6570\uff09\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5199\u4f5c\u8f85\u52a9\u3002", "result": "XtraGPT\u663e\u8457\u4f18\u4e8e\u540c\u89c4\u6a21\u57fa\u7ebf\uff0c\u63a5\u8fd1\u4e13\u6709\u7cfb\u7edf\u8d28\u91cf\uff0c\u81ea\u52a8\u504f\u597d\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u5747\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "XtraGPT\u6846\u67b6\u4e3a\u5b66\u672f\u5199\u4f5c\u4fee\u8ba2\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709LLM\u5728\u79d1\u5b66\u5199\u4f5c\u652f\u6301\u4e0a\u7684\u7a7a\u767d\u3002", "relevance": 85.0}}
{"id": "2505.11481", "pdf": "https://arxiv.org/pdf/2505.11481", "abs": "https://arxiv.org/abs/2505.11481", "authors": ["Alayt Issak", "Jeba Rezwana", "Casper Harteveld"], "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation", "categories": ["cs.AI"], "comment": null, "summary": "Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMOSAAIC\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4eba\u7c7b\u4e0eAI\u534f\u540c\u521b\u4f5c\u4e2d\u5e73\u8861\u63a7\u5236\u6743\uff0c\u6db5\u76d6\u81ea\u4e3b\u6027\u3001\u4e3b\u52a8\u6027\u548c\u6743\u5a01\u6027\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u9002\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u5728\u534f\u540c\u521b\u4f5c\u4e2d\u7684\u63a7\u5236\u6743\u5e73\u8861\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u5408\u4f5c\u52a8\u6001\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08172\u7bc7\u8bba\u6587\uff09\u63d0\u51faMOSAAIC\u6846\u67b6\uff0c\u5e76\u5206\u6790\u516d\u4e2a\u6848\u4f8b\u7814\u7a76\u3002", "result": "MOSAAIC\u6846\u67b6\u6210\u529f\u8bc6\u522b\u63a7\u5236\u6743\u7684\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6848\u4f8b\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "MOSAAIC\u4e3a\u534f\u540c\u521b\u4f5c\u4e2d\u7684\u63a7\u5236\u6743\u5206\u914d\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u4eba\u673a\u5408\u4f5c\u3002", "relevance": 40.0}}
{"id": "2505.10894", "pdf": "https://arxiv.org/pdf/2505.10894", "abs": "https://arxiv.org/abs/2505.10894", "authors": ["Yishuo Wang", "Feng Zhou", "Muping Zhou", "Qicheng Meng", "Zhijun Hu", "Yi Wang"], "title": "CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper proposes CTP, a novel deep learning framework that integrates\nconvolutional neural network(CNN), Transformer architectures, and\nphysics-informed neural network(PINN) for ocean front prediction. Ocean fronts,\nas dynamic interfaces between distinct water masses, play critical roles in\nmarine biogeochemical and physical processes. Existing methods such as LSTM,\nConvLSTM, and AttentionConv often struggle to maintain spatial continuity and\nphysical consistency over multi-step forecasts. CTP addresses these challenges\nby combining localized spatial encoding, long-range temporal attention, and\nphysical constraint enforcement. Experimental results across south China\nsea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP\nachieves state-of-the-art(SOTA) performance in both single-step and multi-step\npredictions, significantly outperforming baseline models in accuracy, $F_1$\nscore, and temporal stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u3001Transformer\u548cPINN\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6CTP\uff0c\u7528\u4e8e\u6d77\u6d0b\u950b\u9762\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982LSTM\u3001ConvLSTM\u7b49\uff09\u5728\u6d77\u6d0b\u950b\u9762\u9884\u6d4b\u4e2d\u96be\u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0cCTP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CTP\u6574\u5408\u4e86\u5c40\u90e8\u7a7a\u95f4\u7f16\u7801\u3001\u957f\u7a0b\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u7269\u7406\u7ea6\u675f\uff0c\u7ed3\u5408\u4e86CNN\u3001Transformer\u548cPINN\u7684\u4f18\u52bf\u3002", "result": "\u5728\u5357\u6d77\u548c\u9ed1\u6f6e\u533a\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cCTP\u5728\u5355\u6b65\u548c\u591a\u6b65\u9884\u6d4b\u4e2d\u5747\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "CTP\u901a\u8fc7\u591a\u6a21\u6001\u67b6\u6784\u548c\u7269\u7406\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d77\u6d0b\u950b\u9762\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11003", "pdf": "https://arxiv.org/pdf/2505.11003", "abs": "https://arxiv.org/abs/2505.11003", "authors": ["Bo Du", "Xuekang Zhu", "Xiaochen Ma", "Chenfan Qu", "Kaiwen Feng", "Zhe Yang", "Chi-Man Pun", "Jian Liu", "Jizhe Zhou"], "title": "ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization", "categories": ["cs.CV"], "comment": "Technical report. Code available at:\n  https://github.com/scu-zjz/ForensicHub", "summary": "The field of Fake Image Detection and Localization (FIDL) is highly\nfragmented, encompassing four domains: deepfake detection (Deepfake), image\nmanipulation detection and localization (IMDL), artificial\nintelligence-generated image detection (AIGC), and document image manipulation\nlocalization (Doc). Although individual benchmarks exist in some domains, a\nunified benchmark for all domains in FIDL remains blank. The absence of a\nunified benchmark results in significant domain silos, where each domain\nindependently constructs its datasets, models, and evaluation protocols without\ninteroperability, preventing cross-domain comparisons and hindering the\ndevelopment of the entire FIDL field. To close the domain silo barrier, we\npropose ForensicHub, the first unified benchmark & codebase for all-domain fake\nimage detection and localization. Considering drastic variations on dataset,\nmodel, and evaluation configurations across all domains, as well as the\nscarcity of open-sourced baseline models and the lack of individual benchmarks\nin some domains, ForensicHub: i) proposes a modular and configuration-driven\narchitecture that decomposes forensic pipelines into interchangeable components\nacross datasets, transforms, models, and evaluators, allowing flexible\ncomposition across all domains; ii) fully implements 10 baseline models, 6\nbackbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing\nbenchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii)\nconducts indepth analysis based on the ForensicHub, offering 8 key actionable\ninsights into FIDL model architecture, dataset characteristics, and evaluation\nstandards. ForensicHub represents a significant leap forward in breaking the\ndomain silos in the FIDL field and inspiring future breakthroughs.", "AI": {"tldr": "ForensicHub\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u5047\u56fe\u50cf\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u57fa\u51c6\u548c\u4ee3\u7801\u5e93\uff0c\u89e3\u51b3\u4e86FIDL\u9886\u57df\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "FIDL\u9886\u57df\u56e0\u7f3a\u4e4f\u7edf\u4e00\u57fa\u51c6\u5bfc\u81f4\u9886\u57df\u5b64\u5c9b\uff0c\u963b\u788d\u8de8\u9886\u57df\u6bd4\u8f83\u548c\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5206\u89e3\u6cd5\u533b\u6d41\u7a0b\u4e3a\u53ef\u4e92\u6362\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u7075\u6d3b\u7ec4\u5408\uff1b\u5b9e\u73b010\u4e2a\u57fa\u7ebf\u6a21\u578b\u30016\u4e2a\u4e3b\u5e72\u7f51\u7edc\u30012\u4e2a\u65b0\u57fa\u51c6\uff0c\u5e76\u96c6\u6210\u73b0\u6709\u57fa\u51c6\u3002", "result": "\u63d0\u4f9b\u4e868\u4e2a\u5173\u952e\u89c1\u89e3\uff0c\u63a8\u52a8FIDL\u9886\u57df\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u7279\u6027\u548c\u8bc4\u4f30\u6807\u51c6\u7684\u6539\u8fdb\u3002", "conclusion": "ForensicHub\u6253\u7834\u4e86FIDL\u9886\u57df\u7684\u5b64\u5c9b\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.11341", "pdf": "https://arxiv.org/pdf/2505.11341", "abs": "https://arxiv.org/abs/2505.11341", "authors": ["Banca Calvo Figueras", "Rodrigo Agerri"], "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose assumptions and\nchallenge the reasoning in arguments. Despite growing interest in this area,\nprogress has been hindered by the lack of suitable datasets and automatic\nevaluation standards. This work presents a comprehensive approach to support\nthe development and benchmarking of systems for this task. We construct the\nfirst large-scale manually-annotated dataset. We also investigate automatic\nevaluation methods and identify a reference-based technique using large\nlanguage models (LLMs) as the strategy that best correlates with human\njudgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline\nwhile showcasing the difficulty of the task. Data, code, and a public\nleaderboard are provided to encourage further research not only in terms of\nmodel performance, but also to explore the practical benefits of CQs-Gen for\nboth automated reasoning and human critical thinking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65b9\u6cd5\u6765\u652f\u6301\u5173\u952e\u95ee\u9898\u751f\u6210\uff08CQs-Gen\uff09\u4efb\u52a1\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u7814\u7a76\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9002\u5408\u7684\u6570\u636e\u96c6\u548c\u81ea\u52a8\u8bc4\u4f30\u6807\u51c6\uff0c\u963b\u788d\u4e86\u5173\u952e\u95ee\u9898\u751f\u6210\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u7814\u7a76\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u57fa\u4e8eLLM\u7684\u53c2\u8003\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u6700\u9ad8\uff0c\u5e76\u5bf911\u4e2aLLM\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u6570\u636e\u3001\u4ee3\u7801\u548c\u516c\u5f00\u6392\u884c\u699c\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u6a21\u578b\u6027\u80fd\u53caCQs-Gen\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 60.0}}
{"id": "2503.09243", "pdf": "https://arxiv.org/pdf/2503.09243", "abs": "https://arxiv.org/abs/2503.09243", "authors": ["Ruihai Wu", "Ziyu Zhu", "Yuran Wang", "Yue Chen", "Jiarui Wang", "Hao Dong"], "title": "GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Cluttered garments manipulation poses significant challenges due to the\ncomplex, deformable nature of garments and intricate garment relations. Unlike\nsingle-garment manipulation, cluttered scenarios require managing complex\ngarment entanglements and interactions, while maintaining garment cleanliness\nand manipulation stability. To address these demands, we propose to learn\npoint-level affordance, the dense representation modeling the complex space and\nmulti-modal manipulation candidates, while being aware of garment geometry,\nstructure, and inter-object relations. Additionally, as it is difficult to\ndirectly retrieve a garment in some extremely entangled clutters, we introduce\nan adaptation module, guided by learned affordance, to reorganize\nhighly-entangled garments into states plausible for manipulation. Our framework\ndemonstrates effectiveness over environments featuring diverse garment types\nand pile configurations in both simulation and the real world. Project page:\nhttps://garmentpile.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u70b9\u7ea7affordance\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u6742\u4e71\u8863\u7269\u64cd\u4f5c\u4e2d\u7684\u590d\u6742\u51e0\u4f55\u548c\u4ea4\u4e92\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u9002\u5e94\u6a21\u5757\u4ee5\u91cd\u7ec4\u9ad8\u5ea6\u7ea0\u7f20\u7684\u8863\u7269\u72b6\u6001\u3002", "motivation": "\u6742\u4e71\u8863\u7269\u64cd\u4f5c\u56e0\u8863\u7269\u7684\u590d\u6742\u53d8\u5f62\u6027\u548c\u4ea4\u4e92\u5173\u7cfb\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u89e3\u51b3\u7ea0\u7f20\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u70b9\u7ea7affordance\u5b66\u4e60\uff0c\u5efa\u6a21\u5bc6\u96c6\u8868\u793a\u548c\u591a\u6a21\u6001\u64cd\u4f5c\u5019\u9009\uff0c\u5e76\u7ed3\u5408\u9002\u5e94\u6a21\u5757\u91cd\u7ec4\u7ea0\u7f20\u8863\u7269\u3002", "result": "\u6846\u67b6\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u5bf9\u591a\u79cd\u8863\u7269\u7c7b\u578b\u548c\u5806\u53e0\u914d\u7f6e\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60affordance\u548c\u9002\u5e94\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6742\u4e71\u8863\u7269\u64cd\u4f5c\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.10913", "pdf": "https://arxiv.org/pdf/2505.10913", "abs": "https://arxiv.org/abs/2505.10913", "authors": ["Muntasir Hoq", "Ananya Rao", "Reisha Jaishankar", "Krish Piryani", "Nithya Janapati", "Jessica Vandenberg", "Bradford Mott", "Narges Norouzi", "James Lester", "Bita Akram"], "title": "Automated Identification of Logical Errors in Programs: Advancing Scalable Analysis of Student Misconceptions", "categories": ["cs.LG", "K.3.1"], "comment": "Accepted for publication at the 18th International Conference on\n  Educational Data Mining (EDM), 2025", "summary": "In Computer Science (CS) education, understanding factors contributing to\nstudents' programming difficulties is crucial for effective learning support.\nBy identifying specific issues students face, educators can provide targeted\nassistance to help them overcome obstacles and improve learning outcomes. While\nidentifying sources of struggle, such as misconceptions, in real-time can be\nchallenging in current educational practices, analyzing logical errors in\nstudents' code can offer valuable insights. This paper presents a scalable\nframework for automatically detecting logical errors in students' programming\nsolutions. Our framework is based on an explainable Abstract Syntax Tree (AST)\nembedding model, the Subtree-based Attention Neural Network (SANN), that\nidentifies the structural components of programs containing logical errors. We\nconducted a series of experiments to evaluate its effectiveness, and the\nresults suggest that our framework can accurately capture students' logical\nerrors and, more importantly, provide us with deeper insights into their\nlearning processes, offering a valuable tool for enhancing programming\neducation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u5d4c\u5165\u6a21\u578b\uff08SANN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5b66\u751f\u7f16\u7a0b\u4f5c\u4e1a\u4e2d\u7684\u903b\u8f91\u9519\u8bef\uff0c\u5e76\u5e2e\u52a9\u7406\u89e3\u5b66\u751f\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u4e2d\uff0c\u8bc6\u522b\u5b66\u751f\u7f16\u7a0b\u4e2d\u7684\u903b\u8f91\u9519\u8bef\u5bf9\u4e8e\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u5b66\u4e60\u652f\u6301\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5b9e\u65f6\u68c0\u6d4b\u8fd9\u4e9b\u9519\u8bef\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5b50\u6811\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\uff08SANN\uff09\u7684AST\u5d4c\u5165\u6a21\u578b\uff0c\u5206\u6790\u7a0b\u5e8f\u7ed3\u6784\u4ee5\u5b9a\u4f4d\u903b\u8f91\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u51c6\u786e\u6355\u6349\u903b\u8f91\u9519\u8bef\uff0c\u5e76\u4e3a\u7406\u89e3\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u6df1\u5c42\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "relevance": 20.0}}
{"id": "2505.11013", "pdf": "https://arxiv.org/pdf/2505.11013", "abs": "https://arxiv.org/abs/2505.11013", "authors": ["Zongye Zhang", "Bohan Kong", "Qingjie Liu", "Yunhong Wang"], "title": "Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion", "categories": ["cs.CV", "cs.MM", "I.3.8"], "comment": "10 pages, 6 figures, 5 tables", "summary": "Generating 3D human motion from text descriptions remains challenging due to\nthe diverse and complex nature of human motion. While existing methods excel\nwithin the training distribution, they often struggle with out-of-distribution\nmotions, limiting their applicability in real-world scenarios. Existing\nVQVAE-based methods often fail to represent novel motions faithfully using\ndiscrete tokens, which hampers their ability to generalize beyond seen data.\nMeanwhile, diffusion-based methods operating on continuous representations\noften lack fine-grained control over individual frames. To address these\nchallenges, we propose a robust motion generation framework MoMADiff, which\ncombines masked modeling with diffusion processes to generate motion using\nframe-level continuous representations. Our model supports flexible\nuser-provided keyframe specification, enabling precise control over both\nspatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong\ngeneralization capability on novel text-to-motion datasets with sparse\nkeyframes as motion prompts. Extensive experiments on two held-out datasets and\ntwo standard benchmarks show that our method consistently outperforms\nstate-of-the-art models in motion quality, instruction fidelity, and keyframe\nadherence.", "AI": {"tldr": "MoMADiff\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u62103D\u4eba\u4f53\u8fd0\u52a8\u7684\u6846\u67b6\uff0c\u652f\u6301\u7528\u6237\u63d0\u4f9b\u5173\u952e\u5e27\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u5206\u5e03\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u5916\u8fd0\u52a8\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u8fde\u7eed\u8868\u793a\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "method": "\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u6269\u6563\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5e27\u7ea7\u8fde\u7eed\u8868\u793a\u751f\u6210\u8fd0\u52a8\uff0c\u652f\u6301\u7528\u6237\u63d0\u4f9b\u5173\u952e\u5e27\u3002", "result": "\u5728\u4e24\u4e2a\u4fdd\u7559\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\uff0cMoMADiff\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u6307\u4ee4\u5fe0\u5b9e\u5ea6\u548c\u5173\u952e\u5e27\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MoMADiff\u901a\u8fc7\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5bf93D\u4eba\u4f53\u8fd0\u52a8\u7684\u9c81\u68d2\u751f\u6210\u548c\u7cbe\u786e\u63a7\u5236\u3002", "relevance": 40.0}}
{"id": "2505.11352", "pdf": "https://arxiv.org/pdf/2505.11352", "abs": "https://arxiv.org/abs/2505.11352", "authors": ["Rao Ma", "Tongzhou Chen", "Kartik Audhkhasi", "Bhuvana Ramabhadran"], "title": "LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recently, large-scale pre-trained speech encoders and Large Language Models\n(LLMs) have been released, which show state-of-the-art performance on a range\nof spoken language processing tasks including Automatic Speech Recognition\n(ASR). To effectively combine both models for better performance, continuous\nspeech prompts, and ASR error correction have been adopted. However, these\nmethods are prone to suboptimal performance or are inflexible. In this paper,\nwe propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using\nthe ASR posterior matrices. The speech encoder is trained to generate\nConnectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,\nwhich are used to reconstruct pseudo-audio embeddings by computing a weighted\nsum of the LLM input embeddings. These embeddings are concatenated with text\nembeddings in the LLM input space. Using the well-performing USM and Gemma\nmodels as an example, we demonstrate that our proposed LegoSLM method yields\ngood performance on both ASR and speech translation tasks. By connecting USM\nwith Gemma models, we can get an average of 49% WERR over the USM-CTC baseline\non 8 MLS testsets. The trained model also exhibits modularity in a range of\nsettings -- after fine-tuning the Gemma model weights, the speech encoder can\nbe switched and combined with the LLM in a zero-shot fashion. Additionally, we\npropose to control the decode-time influence of the USM and LLM using a softmax\ntemperature, which shows effectiveness in domain adaptation.", "AI": {"tldr": "LegoSLM\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7ASR\u540e\u9a8c\u77e9\u9635\u8fde\u63a5\u8bed\u97f3\u7f16\u7801\u5668\u548cLLM\uff0c\u63d0\u5347\u4e86ASR\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u8bed\u97f3\u7f16\u7801\u5668\u548cLLM\u65f6\u6027\u80fd\u4e0d\u4f73\u6216\u4e0d\u591f\u7075\u6d3b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u751f\u6210CTC\u540e\u9a8c\uff0c\u91cd\u6784\u4f2a\u97f3\u9891\u5d4c\u5165\u5e76\u4e0eLLM\u6587\u672c\u5d4c\u5165\u62fc\u63a5\u3002", "result": "\u57288\u4e2aMLS\u6d4b\u8bd5\u96c6\u4e0a\u5e73\u5747WERR\u63d0\u534749%\uff0c\u652f\u6301\u96f6\u6837\u672c\u6a21\u5757\u5316\u7ec4\u5408\u3002", "conclusion": "LegoSLM\u5728\u6027\u80fd\u548c\u7075\u6d3b\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9886\u57df\u9002\u5e94\u7684\u89e3\u7801\u63a7\u5236\u3002", "relevance": 75.0}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472", "abs": "https://arxiv.org/abs/2505.10472", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u901a\u7528\u548c\u533b\u5b66\u4e13\u7528LLM\u5728\u751f\u6210\u51c6\u786e\u3001\u5b89\u5168\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u901a\u7528LLM\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u60c5\u611f\u8868\u8fbe\u4e0a\u66f4\u4f18\uff0c\u800c\u533b\u5b66LLM\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5b58\u5728\u66f4\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u548c\u504f\u89c1\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u764c\u75c7\u6c9f\u901a\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u586b\u8865\u516c\u4f17\u5bf9\u764c\u75c7\u9884\u9632\u3001\u7b5b\u67e5\u548c\u6cbb\u7597\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u5e76\u652f\u6301\u60a3\u8005\u7406\u89e3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u5b9a\u91cf\u6307\u6807\u3001\u5b9a\u6027\u4e13\u5bb6\u8bc4\u5206\u53ca\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5\uff08Welch's ANOVA, Games-Howell, Hedges' g\uff09\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u901a\u7528\u548c\u4e09\u79cd\u533b\u5b66LLM\u3002", "result": "\u901a\u7528LLM\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u60c5\u611f\u8868\u8fbe\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u533b\u5b66LLM\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u4e0a\u66f4\u597d\uff0c\u4f46\u6f5c\u5728\u5371\u5bb3\u548c\u504f\u89c1\u66f4\u9ad8\u3002", "conclusion": "\u9700\u9488\u5bf9\u6027\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c11\u5371\u5bb3\u548c\u504f\u89c1\u3001\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u60c5\u611f\u8868\u8fbe\u65b9\u9762\uff0c\u4ee5\u63d0\u5347AI\u751f\u6210\u5065\u5eb7\u5185\u5bb9\u7684\u8d28\u91cf\u3002", "relevance": 70.0}}
{"id": "2505.10928", "pdf": "https://arxiv.org/pdf/2505.10928", "abs": "https://arxiv.org/abs/2505.10928", "authors": ["Xiao Han", "Dayan Pan", "Xiangyu Zhao", "Xuyuan Hu", "Zhaolin Deng", "Xiangjie Kong", "Guojiang Shen"], "title": "A Dataset for Spatiotemporal-Sensitive POI Question Answering", "categories": ["cs.LG"], "comment": "Under Review", "summary": "Spatiotemporal relationships are critical in data science, as many prediction\nand reasoning tasks require analysis across both spatial and temporal\ndimensions--for instance, navigating an unfamiliar city involves planning\nitineraries that sequence locations and timing cultural experiences. However,\nexisting Question-Answering (QA) datasets lack sufficient\nspatiotemporal-sensitive questions, making them inadequate benchmarks for\nevaluating models' spatiotemporal reasoning capabilities. To address this gap,\nwe introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on\nPoint of Interest (POI), constructed through three key steps: mining and\naligning open-source vehicle trajectory data from GAIA with high-precision\ngeographic POI data, rigorous manual validation of noisy spatiotemporal facts,\nand generating bilingual (Chinese/English) QA pairs that reflect\nhuman-understandable spatiotemporal reasoning tasks. Our dataset challenges\nmodels to parse complex spatiotemporal dependencies, and evaluations of\nstate-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark\nlimitations: even the top-performing model (Qwen2.5-7B fine-tuned with\nRAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,\nfar below human performance at 0.56. This underscores persistent weaknesses in\nLLMs' ability to perform consistent spatiotemporal reasoning, while\nhighlighting POI-QA as a robust benchmark to advance algorithms sensitive to\nspatiotemporal dynamics. The dataset is publicly available at\nhttps://www.kaggle.com/ds/7394666.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86POI-QA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709LLMs\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709QA\u6570\u636e\u96c6\u7f3a\u4e4f\u65f6\u7a7a\u654f\u611f\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6316\u6398\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u3001\u5730\u7406POI\u6570\u636e\u5bf9\u9f50\u3001\u4eba\u5de5\u9a8c\u8bc1\u548c\u751f\u6210\u53cc\u8bedQA\u5bf9\u6784\u5efaPOI-QA\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\uff08Qwen2.5-7B\uff09\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u7684HR@10\u4ec5\u4e3a0.41\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff080.56\uff09\u3002", "conclusion": "POI-QA\u662f\u4e00\u4e2a\u6709\u6548\u7684\u65f6\u7a7a\u63a8\u7406\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u65f6\u7a7a\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11015", "pdf": "https://arxiv.org/pdf/2505.11015", "abs": "https://arxiv.org/abs/2505.11015", "authors": ["An-Lan Wang", "Jingqun Tang", "Liao Lei", "Hao Feng", "Qi Liu", "Xiang Fei", "Jinghui Lu", "Han Wang", "Weiwei Liu", "Hao Liu", "Yuliang Liu", "Xiang Bai", "Can Huang"], "title": "WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced capabilities in Document Understanding. However,\nprevailing benchmarks like DocVQA and ChartQA predominantly comprise\n\\textit{scanned or digital} documents, inadequately reflecting the intricate\nchallenges posed by diverse real-world scenarios, such as variable illumination\nand physical distortions. This paper introduces WildDoc, the inaugural\nbenchmark designed specifically for assessing document understanding in natural\nenvironments. WildDoc incorporates a diverse set of manually captured document\nimages reflecting real-world conditions and leverages document sources from\nestablished benchmarks to facilitate comprehensive comparisons with digital or\nscanned documents. Further, to rigorously evaluate model robustness, each\ndocument is captured four times under different conditions. Evaluations of\nstate-of-the-art MLLMs on WildDoc expose substantial performance declines and\nunderscore the models' inadequate robustness compared to traditional\nbenchmarks, highlighting the unique challenges posed by real-world document\nunderstanding. Our project homepage is available at\nhttps://bytedance.github.io/WildDoc.", "AI": {"tldr": "WildDoc\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u81ea\u7136\u73af\u5883\u4e2d\u6587\u6863\u7406\u89e3\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6587\u6863\u56fe\u50cf\uff0c\u5e76\u63ed\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6587\u6863\u7406\u89e3\u57fa\u51c6\uff08\u5982DocVQA\u548cChartQA\uff09\u4e3b\u8981\u57fa\u4e8e\u626b\u63cf\u6216\u6570\u5b57\u6587\u6863\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u6311\u6218\uff08\u5982\u5149\u7167\u53d8\u5316\u548c\u7269\u7406\u53d8\u5f62\uff09\u3002WildDoc\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "WildDoc\u901a\u8fc7\u624b\u52a8\u6355\u83b7\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u6587\u6863\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u73b0\u6709\u57fa\u51c6\u7684\u6587\u6863\u6765\u6e90\u8fdb\u884c\u5bf9\u6bd4\u3002\u6bcf\u4e2a\u6587\u6863\u5728\u56db\u79cd\u4e0d\u540c\u6761\u4ef6\u4e0b\u62cd\u6444\u4ee5\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs\u5728WildDoc\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u7a81\u663e\u4e86\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7406\u89e3\u7684\u72ec\u7279\u6311\u6218\u3002", "conclusion": "WildDoc\u4e3a\u8bc4\u4f30\u6587\u6863\u7406\u89e3\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "relevance": 60.0}}
{"id": "2505.11368", "pdf": "https://arxiv.org/pdf/2505.11368", "abs": "https://arxiv.org/abs/2505.11368", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GuideBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9075\u5faa\u9886\u57df\u5bfc\u5411\u6307\u5357\u65b9\u9762\u6027\u80fd\u7684\u7efc\u5408\u57fa\u51c6\u3002", "motivation": "\u968f\u7740LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u9886\u57df\u5bfc\u5411\u4ee3\u7406\u90e8\u7f72\uff0c\u5176\u9075\u5faa\u9886\u57df\u89c4\u5219\u7684\u80fd\u529b\u9762\u4e34\u6311\u6218\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "GuideBench\u4ece\u4e09\u4e2a\u65b9\u9762\u8bc4\u4f30LLMs\uff1a\u89c4\u5219\u591a\u6837\u6027\u9075\u5faa\u3001\u89c4\u5219\u66f4\u65b0\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728\u9075\u5faa\u9886\u57df\u5bfc\u5411\u6307\u5357\u65b9\u9762\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "GuideBench\u586b\u8865\u4e86\u8bc4\u4f30LLMs\u9886\u57df\u6307\u5357\u9075\u5faa\u80fd\u529b\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.10577", "pdf": "https://arxiv.org/pdf/2505.10577", "abs": "https://arxiv.org/abs/2505.10577", "authors": ["Yutong Guo"], "title": "GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by 2023 IEEE International Conference on Multimedia and Expo\n  (ICME 2023)", "summary": "Modern video super-resolution (VSR) systems based on convolutional neural\nnetworks (CNNs) require huge computational costs. The problem of feature\nredundancy is present in most models in many domains, but is rarely discussed\nin VSR. We experimentally observe that many features in VSR models are also\nsimilar to each other, so we propose to use \"Ghost features\" to reduce this\nredundancy. We also analyze the so-called \"gradient disappearance\" phenomenon\ngenerated by the conventional recurrent convolutional network (RNN) model, and\ncombine the Ghost module with RNN to complete the modeling on time series. The\ncurrent frame is used as input to the model together with the next frame, the\noutput of the previous frame and the hidden state. Extensive experiments on\nseveral benchmark models and datasets show that the PSNR and SSIM of our\nproposed modality are improved to some extent. Some texture details in the\nvideo are also better preserved.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201cGhost\u7279\u5f81\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u5197\u4f59\uff0c\u5e76\u7ed3\u5408RNN\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728PSNR\u548cSSIM\u4e0a\u6709\u6240\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7cfb\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5b58\u5728\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u4f46\u8be5\u95ee\u9898\u5728VSR\u9886\u57df\u8f83\u5c11\u88ab\u8ba8\u8bba\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u201cGhost\u7279\u5f81\u201d\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u7ed3\u5408RNN\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u8f93\u5165\u5305\u62ec\u5f53\u524d\u5e27\u3001\u4e0b\u4e00\u5e27\u3001\u524d\u4e00\u5e27\u8f93\u51fa\u548c\u9690\u85cf\u72b6\u6001\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u548cSSIM\u6709\u6240\u63d0\u5347\uff0c\u89c6\u9891\u7eb9\u7406\u7ec6\u8282\u4fdd\u7559\u66f4\u597d\u3002", "conclusion": "Ghost\u7279\u5f81\u548cRNN\u7684\u7ed3\u5408\u6709\u6548\u51cf\u5c11\u4e86\u5197\u4f59\u5e76\u63d0\u5347\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.10930", "pdf": "https://arxiv.org/pdf/2505.10930", "abs": "https://arxiv.org/abs/2505.10930", "authors": ["Congcong Zhu", "Xiaoyan Xu", "Jiayue Han", "Jingrun Chen"], "title": "Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models", "categories": ["cs.LG", "35Q68", "G.1.8"], "comment": "Accepted as a conference paper in ICML2025", "summary": "Auto-regressive partial differential equation (PDE) foundation models have\nshown great potential in handling time-dependent data. However, these models\nsuffer from the shortcut problem deeply rooted in auto-regressive prediction,\ncausing error accumulation. The challenge becomes particularly evident for\nout-of-distribution data, as the pretraining performance may approach random\nmodel initialization for downstream tasks with long-term dynamics. To deal with\nthis problem, we propose physics-informed temporal alignment (PITA), a\nself-supervised learning framework inspired by inverse problem solving.\nSpecifically, PITA aligns the physical dynamics discovered at different time\nsteps on each given PDE trajectory by integrating physics-informed constraints\ninto the self-supervision signal. The alignment is derived from observation\ndata without relying on known physics priors, indicating strong generalization\nability to the out-of-distribution data. Extensive experiments show that PITA\nsignificantly enhances the accuracy and robustness of existing foundation\nmodels on diverse time-dependent PDE data. The code is available at\nhttps://github.com/SCAILab-USTC/PITA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6PITA\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u89e3\u51b3\u81ea\u56de\u5f52PDE\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u56de\u5f52PDE\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\u6570\u636e\u65f6\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPITA\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u5bf9\u9f50\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u7269\u7406\u52a8\u6001\uff0c\u65e0\u9700\u5df2\u77e5\u7269\u7406\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePITA\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u65f6\u95f4\u4f9d\u8d56PDE\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "PITA\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11018", "pdf": "https://arxiv.org/pdf/2505.11018", "abs": "https://arxiv.org/abs/2505.11018", "authors": ["Pengchen Zhang", "Alan J. X. Guo", "Sipin Luo", "Zhe Han", "Lin Guo"], "title": "Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised medical image segmentation has attracted significant\nattention due to its potential to reduce manual annotation costs. The mean\nteacher (MT) strategy, commonly understood as introducing smoothed, temporally\nlagged consistency regularization, has demonstrated strong performance across\nvarious tasks in this field. In this work, we reinterpret the MT strategy on\nsupervised data as a form of self-paced learning, regulated by the output\nagreement between the temporally lagged teacher model and the ground truth\nlabels. This idea is further extended to incorporate agreement between a\ntemporally lagged model and a cross-architectural model, which offers greater\nflexibility in regulating the learning pace and enables application to\nunlabeled data. Specifically, we propose dual teacher-student learning (DTSL),\na framework that introduces two groups of teacher-student models with different\narchitectures. The output agreement between the cross-group teacher and student\nmodels is used as pseudo-labels, generated via a Jensen-Shannon\ndivergence-based consensus label generator (CLG). Extensive experiments on\npopular datasets demonstrate that the proposed method consistently outperforms\nexisting state-of-the-art approaches. Ablation studies further validate the\neffectiveness of the proposed modules.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5e08\u751f\u5b66\u4e60\u6846\u67b6\uff08DTSL\uff09\uff0c\u901a\u8fc7\u8de8\u67b6\u6784\u6a21\u578b\u95f4\u7684\u8f93\u51fa\u4e00\u81f4\u6027\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u624b\u52a8\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u63d0\u9ad8\u534a\u76d1\u7763\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u5f15\u5165\u53cc\u5e08\u751f\u5b66\u4e60\u6846\u67b6\uff08DTSL\uff09\uff0c\u5229\u7528\u8de8\u67b6\u6784\u6a21\u578b\u95f4\u7684\u8f93\u51fa\u4e00\u81f4\u6027\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7Jensen-Shannon\u6563\u5ea6\u8fdb\u884c\u5171\u8bc6\u6807\u7b7e\u751f\u6210\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "DTSL\u901a\u8fc7\u8de8\u67b6\u6784\u6a21\u578b\u4e00\u81f4\u6027\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.11379", "pdf": "https://arxiv.org/pdf/2505.11379", "abs": "https://arxiv.org/abs/2505.11379", "authors": ["Alicia Gonz\u00e1lez Mart\u00ednez"], "title": "A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography", "categories": ["cs.CL"], "comment": null, "summary": "Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic\nnotation that can be traced back to the early stages of Islam, when the Quran\nwas mainly oral in nature and the first written renderings of it served as\nmemory aids for this oral tradition. The early systems of diacritical marks\ncreated on top of the Quranic Consonantal Text (QCT) motivated the creation and\nfurther development of a fine-grained system of phonetic notation that\nrepresented tajwid-the rules of recitation. We explored the systematicity of\nthe rules of tajwid, as they are encountered in the Cairo Quran, using a fully\nand accurately encoded digital edition of the Quranic text. For this purpose,\nwe developed a python module that can remove or add the orthographic layer of\ntajwid from a Quranic text in CQO. The interesting characteristic of these two\nsets of rules is that they address the complete Quranic text of the Cairo\nQuran, so they can be used as precise witnesses to study its phonetic and\nprosodic processes. From a computational point of view, the text of the Cairo\nQuran can be used as a linchpin to align and compare Quranic manuscripts, due\nto its richness and completeness. This will let us create a very powerful\nframework to work with the Arabic script, not just within an isolated text, but\nautomatically exploring a specific textual phenomenon in other connected\nmanuscripts. Having all the texts mapped among each other can serve as a\npowerful tool to study the nature of the notation systems of diacritics added\nto the consonantal skeleton.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53e4\u5170\u7ecf\u5f53\u4ee3\u6b63\u5b57\u6cd5\uff08CQO\uff09\u7684\u7cfb\u7edf\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2aPython\u6a21\u5757\u7528\u4e8e\u6dfb\u52a0\u6216\u79fb\u9664tajwid\u5c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u7528\u4e8e\u5bf9\u9f50\u548c\u6bd4\u8f83\u53e4\u5170\u7ecf\u624b\u7a3f\u3002", "motivation": "\u7814\u7a76\u53e4\u5170\u7ecf\u6b63\u5b57\u6cd5\u7684\u7cfb\u7edf\u6027\u53ca\u5176\u5728\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662ftajwid\u89c4\u5219\u5728CQO\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aPython\u6a21\u5757\uff0c\u7528\u4e8e\u5904\u7406\u53e4\u5170\u7ecf\u6587\u672c\u4e2d\u7684tajwid\u5c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u7528\u4e8e\u5bf9\u9f50\u548c\u6bd4\u8f83\u624b\u7a3f\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u53e4\u5170\u7ecf\u6587\u672c\u7684\u7cbe\u786e\u7f16\u7801\u548c\u5904\u7406\uff0c\u4e3a\u7814\u7a76\u963f\u62c9\u4f2f\u6587\u5b57\u548c\u624b\u7a3f\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "conclusion": "CQO\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u4e3a\u963f\u62c9\u4f2f\u6587\u5b57\u7684\u8ba1\u7b97\u8bed\u8a00\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u624b\u7a3f\u5bf9\u9f50\u548c\u6bd4\u8f83\u65b9\u9762\u3002", "relevance": 30.0}}
{"id": "2505.10588", "pdf": "https://arxiv.org/pdf/2505.10588", "abs": "https://arxiv.org/abs/2505.10588", "authors": ["Manisha Mehta", "Fausto Giunchiglia"], "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "I.2; I.2.7; K.4.2"], "comment": "Accepted to ACM FAccT 2025. To be presented in Athens, June 2025, and\n  published in the conference proceedings. Preprint version; final version will\n  appear in the ACM Digital Library", "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86AI\u7cfb\u7edf\u5bf9Alpha\u4e16\u4ee3\uff082010-2024\u5e74\u51fa\u751f\uff09\u6570\u5b57\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709AI\u6a21\u578b\u5728\u68c0\u6d4b\u9690\u85cf\u9a9a\u6270\u548c\u64cd\u7eb5\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6846\u67b6\u3002", "motivation": "Alpha\u4e16\u4ee3\u662f\u9996\u4e2a\u4e0eAI\u5171\u540c\u6210\u957f\u7684\u7fa4\u4f53\uff0c\u5176\u72ec\u7279\u7684\u6570\u5b57\u8bed\u8a00\u53ef\u80fd\u5bfc\u81f4\u73b0\u6709\u5b89\u5168\u5de5\u5177\u5931\u6548\uff0c\u4e9f\u9700\u7814\u7a76\u5982\u4f55\u6539\u8fdbAI\u7cfb\u7edf\u4ee5\u4fdd\u62a4\u9752\u5c11\u5e74\u5728\u7ebf\u5b89\u5168\u3002", "method": "\u8bc4\u4f30\u4e86GPT-4\u3001Claude\u3001Gemini\u548cLlama 3\u56db\u79cdAI\u6a21\u578b\u5bf9Alpha\u4e16\u4ee3\u8bed\u8a00\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4f7f\u7528\u4e86100\u6761\u6765\u81ea\u6e38\u620f\u5e73\u53f0\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u89c6\u9891\u5185\u5bb9\u7684\u8868\u8fbe\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u4eba\u7c7b\u5ba1\u6838\u5458\u3001\u5bb6\u957f\u548cAlpha\u4e16\u4ee3\u7814\u7a76\u5458\u7684\u53cd\u9988\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u6a21\u578b\u5728\u7406\u89e3Alpha\u4e16\u4ee3\u8bed\u8a00\u65f6\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5bfc\u81f4\u5728\u7ebf\u5b89\u5168\u98ce\u9669\u589e\u52a0\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u9002\u5e94\u9752\u5c11\u5e74\u8bed\u8a00\u7684\u5b89\u5168\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u591a\u65b9\u89c6\u89d2\uff08AI\u3001\u4eba\u7c7b\u3001\u9752\u5c11\u5e74\uff09\u4ee5\u63d0\u5347\u4fdd\u62a4\u6548\u679c\u3002", "relevance": 60.0}}
{"id": "2505.10941", "pdf": "https://arxiv.org/pdf/2505.10941", "abs": "https://arxiv.org/abs/2505.10941", "authors": ["Ozan \u00d6zdenizci", "Elmar Rueckert", "Robert Legenstein"], "title": "Privacy-Aware Lifelong Learning", "categories": ["cs.LG"], "comment": null, "summary": "Lifelong learning algorithms enable models to incrementally acquire new\nknowledge without forgetting previously learned information. Contrarily, the\nfield of machine unlearning focuses on explicitly forgetting certain previous\nknowledge from pretrained models when requested, in order to comply with data\nprivacy regulations on the right-to-be-forgotten. Enabling efficient lifelong\nlearning with the capability to selectively unlearn sensitive information from\nmodels presents a critical and largely unaddressed challenge with contradicting\nobjectives. We address this problem from the perspective of simultaneously\npreventing catastrophic forgetting and allowing forward knowledge transfer\nduring task-incremental learning, while ensuring exact task unlearning and\nminimizing memory requirements, based on a single neural network model to be\nadapted. Our proposed solution, privacy-aware lifelong learning (PALL),\ninvolves optimization of task-specific sparse subnetworks with parameter\nsharing within a single architecture. We additionally utilize an episodic\nmemory rehearsal mechanism to facilitate exact unlearning without performance\ndegradations. We empirically demonstrate the scalability of PALL across various\narchitectures in image classification, and provide a state-of-the-art solution\nthat uniquely integrates lifelong learning and privacy-aware unlearning\nmechanisms for responsible AI applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u611f\u77e5\u7684\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\uff08PALL\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u7279\u5b9a\u7684\u7a00\u758f\u5b50\u7f51\u7edc\u548c\u53c2\u6570\u5171\u4eab\uff0c\u540c\u65f6\u5b9e\u73b0\u7ec8\u8eab\u5b66\u4e60\u548c\u9009\u62e9\u6027\u9057\u5fd8\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u7ec8\u8eab\u5b66\u4e60\u4e2d\u5982\u4f55\u540c\u65f6\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u548c\u9009\u62e9\u6027\u9057\u5fd8\u654f\u611f\u4fe1\u606f\u7684\u77db\u76fe\u9700\u6c42\uff0c\u4ee5\u7b26\u5408\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u7a00\u758f\u5b50\u7f51\u7edc\u4f18\u5316\u548c\u53c2\u6570\u5171\u4eab\uff0c\u7ed3\u5408\u60c5\u666f\u8bb0\u5fc6\u590d\u8ff0\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u548c\u7cbe\u786e\u9057\u5fd8\u3002", "result": "PALL\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u4e0d\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u786e\u9057\u5fd8\u3002", "conclusion": "PALL\u4e3a\u8d1f\u8d23\u4efbAI\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ec8\u8eab\u5b66\u4e60\u548c\u9690\u79c1\u611f\u77e5\u9057\u5fd8\u7684\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.11020", "pdf": "https://arxiv.org/pdf/2505.11020", "abs": "https://arxiv.org/abs/2505.11020", "authors": ["Yi-Lu Jiang", "Wen-Chang Chang", "Ching-Lin Wang", "Kung-Liang Hsu", "Chih-Yi Chiu"], "title": "Classifying Shelf Life Quality of Pineapples by Combining Audio and Visual Features", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Determining the shelf life quality of pineapples using non-destructive\nmethods is a crucial step to reduce waste and increase income. In this paper, a\nmultimodal and multiview classification model was constructed to classify\npineapples into four quality levels based on audio and visual characteristics.\nFor research purposes, we compiled and released the PQC500 dataset consisting\nof 500 pineapples with two modalities: one was tapping pineapples to record\nsounds by multiple microphones and the other was taking pictures by multiple\ncameras at different locations, providing multimodal and multi-view audiovisual\nfeatures. We modified the contrastive audiovisual masked autoencoder to train\nthe cross-modal-based classification model by abundant combinations of audio\nand visual pairs. In addition, we proposed to sample a compact size of training\ndata for efficient computation. The experiments were evaluated under various\ndata and model configurations, and the results demonstrated that the proposed\ncross-modal model trained using audio-major sampling can yield 84% accuracy,\noutperforming the unimodal models of only audio and only visual by 6% and 18%,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u548c\u591a\u89c6\u89d2\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u5bf9\u83e0\u841d\u7684\u8d27\u67b6\u671f\u8d28\u91cf\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u53d1\u5e03\u4e86PQC500\u6570\u636e\u96c6\u3002", "motivation": "\u51cf\u5c11\u6d6a\u8d39\u5e76\u589e\u52a0\u6536\u5165\uff0c\u901a\u8fc7\u975e\u7834\u574f\u6027\u65b9\u6cd5\u8bc4\u4f30\u83e0\u841d\u7684\u8d27\u67b6\u671f\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u4e86\u591a\u6a21\u6001\u548c\u591a\u89c6\u89d2\u5206\u7c7b\u6a21\u578b\uff0c\u4f7f\u7528\u5bf9\u6bd4\u6027\u89c6\u542c\u63a9\u7801\u81ea\u7f16\u7801\u5668\u8bad\u7ec3\u8de8\u6a21\u6001\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u8ba1\u7b97\u7684\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u91c7\u6837\u65b9\u6cd5\u3002", "result": "\u8de8\u6a21\u6001\u6a21\u578b\u5728\u97f3\u9891\u4e3b\u5bfc\u91c7\u6837\u4e0b\u8fbe\u523084%\u7684\u51c6\u786e\u7387\uff0c\u5206\u522b\u6bd4\u5355\u6a21\u6001\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u578b\u9ad86%\u548c18%\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u83e0\u841d\u8d28\u91cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u4e14\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\u6709\u52a9\u4e8e\u8ba1\u7b97\u6548\u7387\u3002", "relevance": 10.0}}
{"id": "2505.11413", "pdf": "https://arxiv.org/pdf/2505.11413", "abs": "https://arxiv.org/abs/2505.11413", "authors": ["Sijia Chen", "Xiaomin Li", "Mengxue Zhang", "Eric Hanchen Jiang", "Qingcheng Zeng", "Chen-Hsiang Yu"], "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in medical contexts,\nraising critical concerns about safety, alignment, and susceptibility to\nadversarial manipulation. While prior benchmarks assess model refusal\ncapabilities for harmful prompts, they often lack clinical specificity, graded\nharmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES\n(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for\nevaluating LLM safety in healthcare. CARES includes over 18,000 prompts\nspanning eight medical safety principles, four harm levels, and four prompting\nstyles: direct, indirect, obfuscated, and role-play, to simulate both malicious\nand benign use cases. We propose a three-way response evaluation protocol\n(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess\nmodel behavior. Our analysis reveals that many state-of-the-art LLMs remain\nvulnerable to jailbreaks that subtly rephrase harmful prompts, while also\nover-refusing safe but atypically phrased queries. Finally, we propose a\nmitigation strategy using a lightweight classifier to detect jailbreak attempts\nand steer models toward safer behavior via reminder-based conditioning. CARES\nprovides a rigorous framework for testing and improving medical LLM safety\nunder adversarial and ambiguous conditions.", "AI": {"tldr": "CARES\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u533b\u7597\u9886\u57dfLLM\u5b89\u5168\u6027\u7684\u57fa\u51c6\uff0c\u5305\u542b18,000\u591a\u4e2a\u63d0\u793a\uff0c\u8986\u76d6\u591a\u79cd\u533b\u7597\u5b89\u5168\u539f\u5219\u548c\u653b\u51fb\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u533b\u7597\u9886\u57dfLLM\u7684\u5b89\u5168\u6027\u3001\u5bf9\u9f50\u6027\u548c\u5bf9\u6297\u6027\u64cd\u7eb5\u7684\u8106\u5f31\u6027\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u4e34\u5e8a\u7279\u5f02\u6027\u548c\u591a\u7ea7\u5371\u5bb3\u8bc4\u4f30\u3002", "method": "CARES\u57fa\u51c6\u5305\u542b\u591a\u7ea7\u5371\u5bb3\u63d0\u793a\u548c\u56db\u79cd\u63d0\u793a\u98ce\u683c\uff0c\u63d0\u51fa\u4e09\u5411\u54cd\u5e94\u8bc4\u4f30\u534f\u8bae\u548c\u7ec6\u7c92\u5ea6\u5b89\u5168\u8bc4\u5206\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u5bf9\u63d0\u793a\u7684\u5fae\u5999\u6539\u5199\u4ecd\u6613\u53d7\u653b\u51fb\uff0c\u540c\u65f6\u5bf9\u975e\u5178\u578b\u5b89\u5168\u67e5\u8be2\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "CARES\u4e3a\u533b\u7597LLM\u5728\u5bf9\u6297\u6027\u548c\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e25\u683c\u6d4b\u8bd5\u6846\u67b6\u3002", "relevance": 85.0}}
{"id": "2505.10947", "pdf": "https://arxiv.org/pdf/2505.10947", "abs": "https://arxiv.org/abs/2505.10947", "authors": ["Kehan Long", "Jorge Cort\u00e9s", "Nikolay Atanasov"], "title": "Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We study the problem of certifying the stability of closed-loop systems under\ncontrol policies derived from optimal control or reinforcement learning (RL).\nClassical Lyapunov methods require a strict step-wise decrease in the Lyapunov\nfunction but such a certificate is difficult to construct for a learned control\npolicy. The value function associated with an RL policy is a natural Lyapunov\nfunction candidate but it is not clear how it should be modified. To gain\nintuition, we first study the linear quadratic regulator (LQR) problem and make\ntwo key observations. First, a Lyapunov function can be obtained from the value\nfunction of an LQR policy by augmenting it with a residual term related to the\nsystem dynamics and stage cost. Second, the classical Lyapunov decrease\nrequirement can be relaxed to a generalized Lyapunov condition requiring only\ndecrease on average over multiple time steps. Using this intuition, we consider\nthe nonlinear setting and formulate an approach to learn generalized Lyapunov\nfunctions by augmenting RL value functions with neural network residual terms.\nOur approach successfully certifies the stability of RL policies trained on\nGymnasium and DeepMind Control benchmarks. We also extend our method to jointly\ntrain neural controllers and stability certificates using a multi-step Lyapunov\nloss, resulting in larger certified inner approximations of the region of\nattraction compared to the classical Lyapunov approach. Overall, our\nformulation enables stability certification for a broad class of systems with\nlearned policies by making certificates easier to construct, thereby bridging\nclassical control theory and modern learning-based methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u5e7f\u4e49Lyapunov\u51fd\u6570\u6765\u8ba4\u8bc1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u548c\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4e3a\u4ece\u6700\u4f18\u63a7\u5236\u6216\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u5bfc\u51fa\u7684\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u7a33\u5b9a\u6027\u8ba4\u8bc1\uff0c\u514b\u670d\u7ecf\u5178Lyapunov\u65b9\u6cd5\u5728\u6784\u9020\u8bc1\u4e66\u4e0a\u7684\u56f0\u96be\u3002", "method": "\u9996\u5148\u4ece\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u95ee\u9898\u4e2d\u83b7\u53d6\u76f4\u89c9\uff0c\u63d0\u51fa\u901a\u8fc7\u589e\u5f3aRL\u503c\u51fd\u6570\u4e0e\u795e\u7ecf\u7f51\u7edc\u6b8b\u5dee\u9879\u6765\u5b66\u4e60\u5e7f\u4e49Lyapunov\u51fd\u6570\u3002", "result": "\u6210\u529f\u8ba4\u8bc1\u4e86\u5728Gymnasium\u548cDeepMind Control\u57fa\u51c6\u4e0a\u8bad\u7ec3\u7684RL\u7b56\u7565\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u6b65Lyapunov\u635f\u5931\u8054\u5408\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5668\u548c\u7a33\u5b9a\u6027\u8bc1\u4e66\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e7f\u6cdb\u7684\u5b66\u4e60\u7b56\u7565\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5b9a\u6027\u8ba4\u8bc1\uff0c\u7b80\u5316\u4e86\u8bc1\u4e66\u6784\u9020\uff0c\u8fde\u63a5\u4e86\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u4e0e\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u3002", "relevance": 70.0}}
{"id": "2505.11034", "pdf": "https://arxiv.org/pdf/2505.11034", "abs": "https://arxiv.org/abs/2505.11034", "authors": ["Fabian Gr\u00f6ger", "Simone Lionetti", "Philippe Gottfrois", "Alvaro Gonzalez-Jimenez", "Ludovic Amruthalingam", "Elisabeth Victoria Goessinger", "Hanna Lindemann", "Marie Bargiela", "Marie Hofbauer", "Omar Badri", "Philipp Tschandl", "Arash Koochek", "Matthew Groh", "Alexander A. Navarini", "Marc Pouly"], "title": "CleanPatrick: A Benchmark for Image Data Cleaning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust machine learning depends on clean data, yet current image data\ncleaning benchmarks rely on synthetic noise or narrow human studies, limiting\ncomparison and real-world relevance. We introduce CleanPatrick, the first\nlarge-scale benchmark for data cleaning in the image domain, built upon the\npublicly available Fitzpatrick17k dermatology dataset. We collect 496,377\nbinary annotations from 933 medical crowd workers, identify off-topic samples\n(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation\nmodel inspired by item-response theory followed by expert review to derive\nhigh-quality ground truth. CleanPatrick formalizes issue detection as a ranking\ntask and adopts typical ranking metrics mirroring real audit workflows.\nBenchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident\nLearning, NoiseRank, and SelfClean, we find that, on CleanPatrick,\nself-supervised representations excel at near-duplicate detection, classical\nmethods achieve competitive off-topic detection under constrained review\nbudgets, and label-error detection remains an open challenge for fine-grained\nmedical classification. By releasing both the dataset and the evaluation\nframework, CleanPatrick enables a systematic comparison of image-cleaning\nstrategies and paves the way for more reliable data-centric artificial\nintelligence.", "AI": {"tldr": "CleanPatrick\u662f\u9996\u4e2a\u9488\u5bf9\u56fe\u50cf\u6570\u636e\u6e05\u7406\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eFitzpatrick17k\u76ae\u80a4\u79d1\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4f17\u5305\u6807\u6ce8\u548c\u4e13\u5bb6\u5ba1\u6838\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6570\u636e\u6e05\u7406\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u6570\u636e\u6e05\u7406\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u5408\u6210\u566a\u58f0\u6216\u6709\u9650\u4eba\u5de5\u7814\u7a76\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u53ef\u6bd4\u6027\uff0cCleanPatrick\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6536\u96c6496,377\u4e2a\u4f17\u5305\u6807\u6ce8\uff0c\u8bc6\u522b\u79bb\u9898\u6837\u672c\u3001\u8fd1\u91cd\u590d\u548c\u6807\u7b7e\u9519\u8bef\uff0c\u91c7\u7528\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u7684\u805a\u5408\u6a21\u578b\u548c\u4e13\u5bb6\u5ba1\u6838\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u3002", "result": "\u81ea\u76d1\u7763\u8868\u793a\u5728\u8fd1\u91cd\u590d\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ecf\u5178\u65b9\u6cd5\u5728\u6709\u9650\u9884\u7b97\u4e0b\u5bf9\u79bb\u9898\u68c0\u6d4b\u6709\u6548\uff0c\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u4ecd\u662f\u6311\u6218\u3002", "conclusion": "CleanPatrick\u4e3a\u56fe\u50cf\u6e05\u7406\u7b56\u7565\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6bd4\u8f83\u5de5\u5177\uff0c\u63a8\u52a8\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684AI\u53ef\u9760\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11421", "pdf": "https://arxiv.org/pdf/2505.11421", "abs": "https://arxiv.org/abs/2505.11421", "authors": ["Phan Tran Minh Dat", "Vo Hoang Nhat Khang", "Quan Thanh Tho"], "title": "Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model", "categories": ["cs.CL"], "comment": null, "summary": "This work explores the journey towards achieving Bahnaric-Vietnamese\ntranslation for the sake of culturally bridging the two ethnic groups in\nVietnam. However, translating from Bahnaric to Vietnamese also encounters some\ndifficulties. The most prominent challenge is the lack of available original\nBahnaric resources source language, including vocabulary, grammar, dialogue\npatterns and bilingual corpus, which hinders the data collection process for\ntraining. To address this, we leverage a transfer learning approach using\nsequence-to-sequence pre-training language model. First of all, we leverage a\npre-trained Vietnamese language model to capture the characteristics of this\nlanguage. Especially, to further serve the purpose of machine translation, we\naim for a sequence-to-sequence model, not encoder-only like BERT or\ndecoder-only like GPT. Taking advantage of significant similarity between the\ntwo languages, we continue training the model with the currently limited\nbilingual resources of Vietnamese-Bahnaric text to perform the transfer\nlearning from language model to machine translation. Thus, this approach can\nhelp to handle the problem of imbalanced resources between two languages, while\nalso optimizing the training and computational processes. Additionally, we also\nenhanced the datasets using data augmentation to generate additional resources\nand defined some heuristic methods to help the translation more precise. Our\napproach has been validated to be highly effective for the Bahnaric-Vietnamese\ntranslation model, contributing to the expansion and preservation of languages,\nand facilitating better mutual understanding between the two ethnic people.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5df4\u62ff\u8bed-\u8d8a\u5357\u8bed\u7ffb\u8bd1\u4e2d\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f18\u5316\u4e86\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u4fc3\u8fdb\u8d8a\u5357\u4e24\u4e2a\u6c11\u65cf\u4e4b\u95f4\u7684\u6587\u5316\u4ea4\u6d41\uff0c\u4f46\u9762\u4e34\u5df4\u62ff\u8bed\u8d44\u6e90\u532e\u4e4f\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9884\u8bad\u7ec3\u8d8a\u5357\u8bed\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f18\u5316\u5df4\u62ff\u8bed-\u8d8a\u5357\u8bed\u7ffb\u8bd1\u3002", "result": "\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u8bed\u8a00\u4fdd\u62a4\u548c\u8de8\u6587\u5316\u7406\u89e3\u3002", "relevance": 40.0}}
{"id": "2505.10590", "pdf": "https://arxiv.org/pdf/2505.10590", "abs": "https://arxiv.org/abs/2505.10590", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "Anchoring AI Capabilities in Market Valuations: The Capability Realization Rate Model and Valuation Misalignment Risk", "categories": ["cs.CY", "cs.AI"], "comment": "11 pages, 3 figures, NeurIPS", "summary": "Recent breakthroughs in artificial intelligence (AI) have triggered surges in\nmarket valuations for AI-related companies, often outpacing the realization of\nunderlying capabilities. We examine the anchoring effect of AI capabilities on\nequity valuations and propose a Capability Realization Rate (CRR) model to\nquantify the gap between AI potential and realized performance. Using data from\nthe 2023--2025 generative AI boom, we analyze sector-level sensitivity and\nconduct case studies (OpenAI, Adobe, NVIDIA, Meta, Microsoft, Goldman Sachs) to\nillustrate patterns of valuation premium and misalignment. Our findings\nindicate that AI-native firms commanded outsized valuation premiums anchored to\nfuture potential, while traditional companies integrating AI experienced\nre-ratings subject to proof of tangible returns. We argue that CRR can help\nidentify valuation misalignment risk-where market prices diverge from realized\nAI-driven value. We conclude with policy recommendations to improve\ntransparency, mitigate speculative bubbles, and align AI innovation with\nsustainable market value.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u529b\u5b9e\u73b0\u7387\uff08CRR\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u91cf\u5316AI\u6f5c\u529b\u4e0e\u5b9e\u9645\u8868\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5206\u6790\u4e86AI\u80fd\u529b\u5bf9\u80a1\u7968\u4f30\u503c\u7684\u951a\u5b9a\u6548\u5e94\u3002", "motivation": "\u7814\u7a76AI\u80fd\u529b\u5bf9\u5e02\u573a\u4f30\u503c\u7684\u951a\u5b9a\u6548\u5e94\uff0c\u63ed\u793aAI\u6f5c\u529b\u4e0e\u5b9e\u9645\u8868\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u5e2e\u52a9\u8bc6\u522b\u4f30\u503c\u504f\u5dee\u98ce\u9669\u3002", "method": "\u4f7f\u75282023-2025\u5e74\u751f\u6210\u5f0fAI\u7e41\u8363\u7684\u6570\u636e\uff0c\u5206\u6790\u884c\u4e1a\u654f\u611f\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u5982OpenAI\u3001Adobe\u7b49\uff09\u5c55\u793a\u4f30\u503c\u6ea2\u4ef7\u548c\u504f\u5dee\u6a21\u5f0f\u3002", "result": "AI\u539f\u751f\u4f01\u4e1a\u56e0\u672a\u6765\u6f5c\u529b\u83b7\u5f97\u9ad8\u4f30\u503c\u6ea2\u4ef7\uff0c\u800c\u4f20\u7edf\u4f01\u4e1a\u9700\u8bc1\u660eAI\u5e26\u6765\u7684\u5b9e\u9645\u56de\u62a5\u624d\u80fd\u91cd\u65b0\u8bc4\u7ea7\u3002", "conclusion": "CRR\u6a21\u578b\u53ef\u5e2e\u52a9\u8bc6\u522b\u4f30\u503c\u504f\u5dee\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u653f\u7b56\u5efa\u8bae\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u3001\u51cf\u5c11\u6295\u673a\u6ce1\u6cab\u3002", "relevance": 30.0}}
{"id": "2505.10949", "pdf": "https://arxiv.org/pdf/2505.10949", "abs": "https://arxiv.org/abs/2505.10949", "authors": ["Chenhui Xu", "Dancheng Liu", "Amir Nassereldine", "Jinjun Xiong"], "title": "FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Physics Informed Neural Networks (PINNs) often exhibit failure modes in which\nthe PDE residual loss converges while the solution error stays large, a\nphenomenon traditionally blamed on local optima separated from the true\nsolution by steep loss barriers. We challenge this understanding by demonstrate\nthat the real culprit is insufficient arithmetic precision: with standard FP32,\nthe LBFGS optimizer prematurely satisfies its convergence test, freezing the\nnetwork in a spurious failure phase. Simply upgrading to FP64 rescues\noptimization, enabling vanilla PINNs to solve PDEs without any failure modes.\nThese results reframe PINN failure modes as precision induced stalls rather\nthan inescapable local minima and expose a three stage training dynamic\nunconverged, failure, success whose boundaries shift with numerical precision.\nOur findings emphasize that rigorous arithmetic precision is the key to\ndependable PDE solving with neural networks.", "AI": {"tldr": "PINNs\u7684\u5931\u8d25\u6a21\u5f0f\u901a\u5e38\u5f52\u56e0\u4e8e\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b9e\u9645\u539f\u56e0\u662f\u7b97\u672f\u7cbe\u5ea6\u4e0d\u8db3\uff08FP32\uff09\u3002\u6539\u7528FP64\u540e\uff0c\u4f18\u5316\u6210\u529f\uff0c\u89e3\u51b3\u4e86PDE\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8PINNs\u5728\u89e3\u51b3PDE\u65f6\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u4e3a\u7684\u5c40\u90e8\u6700\u4f18\u89e3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4FP32\u548cFP64\u7684\u7b97\u672f\u7cbe\u5ea6\uff0c\u5206\u6790LBFGS\u4f18\u5316\u5668\u7684\u6536\u655b\u884c\u4e3a\u3002", "result": "FP32\u5bfc\u81f4\u4f18\u5316\u8fc7\u65e9\u6536\u655b\uff0cFP64\u6210\u529f\u89e3\u51b3\u4e86PDE\u95ee\u9898\u3002", "conclusion": "\u7b97\u672f\u7cbe\u5ea6\u662fPINNs\u53ef\u9760\u89e3\u51b3PDE\u7684\u5173\u952e\u56e0\u7d20\u3002", "relevance": 30.0}}
{"id": "2505.11046", "pdf": "https://arxiv.org/pdf/2505.11046", "abs": "https://arxiv.org/abs/2505.11046", "authors": ["Tim Alpherts", "Sennay Ghebreab", "Nanne van Noord"], "title": "Artifacts of Idiosyncracy in Global Street View Data", "categories": ["cs.CV"], "comment": "Published at FAccT '25", "summary": "Street view data is increasingly being used in computer vision applications\nin recent years. Machine learning datasets are collected for these applications\nusing simple sampling techniques. These datasets are assumed to be a systematic\nrepresentation of cities, especially when densely sampled. Prior works however,\nshow that there are clear gaps in coverage, with certain cities or regions\nbeing covered poorly or not at all. Here we demonstrate that a cities'\nidiosyncracies, such as city layout, may lead to biases in street view data for\n28 cities across the globe, even when they are densely covered. We\nquantitatively uncover biases in the distribution of coverage of street view\ndata and propose a method for evaluation of such distributions to get better\ninsight in idiosyncracies in a cities' coverage. In addition, we perform a case\nstudy of Amsterdam with semi-structured interviews, showing how idiosyncracies\nof the collection process impact representation of cities and regions and\nallowing us to address biases at their source.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5168\u740328\u4e2a\u57ce\u5e02\u7684\u8857\u666f\u6570\u636e\u8986\u76d6\u504f\u5dee\uff0c\u53d1\u73b0\u57ce\u5e02\u5e03\u5c40\u7b49\u56e0\u7d20\u4f1a\u5bfc\u81f4\u6570\u636e\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3002\u963f\u59c6\u65af\u7279\u4e39\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u8857\u666f\u6570\u636e\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u53ef\u80fd\u5b58\u5728\u8986\u76d6\u504f\u5dee\uff0c\u5f71\u54cd\u5176\u4ee3\u8868\u6027\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5e76\u91cf\u5316\u8fd9\u79cd\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u5206\u679028\u4e2a\u57ce\u5e02\u7684\u8857\u666f\u6570\u636e\u8986\u76d6\u5206\u5e03\uff0c\u63d0\u51fa\u8bc4\u4f30\u65b9\u6cd5\uff1b\u5e76\u901a\u8fc7\u963f\u59c6\u65af\u7279\u4e39\u7684\u6848\u4f8b\u7814\u7a76\uff08\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff09\u5206\u6790\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u7684\u504f\u5dee\u6765\u6e90\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u57ce\u5e02\u5e03\u5c40\u7b49\u56e0\u7d20\u4f1a\u5bfc\u81f4\u8857\u666f\u6570\u636e\u8986\u76d6\u504f\u5dee\uff0c\u5373\u4f7f\u5728\u9ad8\u5bc6\u5ea6\u91c7\u6837\u4e0b\u4e5f\u5b58\u5728\u95ee\u9898\u3002\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u7684\u5177\u4f53\u504f\u5dee\u6765\u6e90\u3002", "conclusion": "\u8857\u666f\u6570\u636e\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u9700\u6539\u8fdb\u6570\u636e\u6536\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u4ee3\u8868\u6027\u3002", "relevance": 30.0}}
{"id": "2505.11423", "pdf": "https://arxiv.org/pdf/2505.11423", "abs": "https://arxiv.org/abs/2505.11423", "authors": ["Xiaomin Li", "Zhou Yu", "Zhiwei Zhang", "Xupeng Chen", "Ziji Zhang", "Yingying Zhuang", "Narayanan Sadagopan", "Anurag Beniwal"], "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u4f1a\u663e\u8457\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6307\u4ee4\u9075\u5faa\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u56db\u79cd\u7b56\u7565\uff08\u5982\u5206\u7c7b\u5668\u9009\u62e9\u6027\u63a8\u7406\uff09\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u663e\u5f0f\u63a8\u7406\u5bf9LLM\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u6b64\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5728IFEval\u548cComplexBench\u4e24\u4e2a\u57fa\u51c6\u4e0a\u8bc4\u4f3015\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u56db\u79cd\u7f13\u89e3\u7b56\u7565\u3002", "result": "CoT\u63a8\u7406\u4f1a\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b\u9009\u62e9\u6027\u63a8\u7406\u7b56\u7565\uff08\u5c24\u5176\u662f\u5206\u7c7b\u5668\u9009\u62e9\u6027\u63a8\u7406\uff09\u80fd\u663e\u8457\u6062\u590d\u6027\u80fd\u3002", "conclusion": "\u663e\u5f0f\u63a8\u7406\u53ef\u80fd\u635f\u5bb3\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f46\u9009\u62e9\u6027\u63a8\u7406\u7b56\u7565\u53ef\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "relevance": 85.0}}
{"id": "2505.10593", "pdf": "https://arxiv.org/pdf/2505.10593", "abs": "https://arxiv.org/abs/2505.10593", "authors": ["Shanhui Zhao", "Hao Wen", "Wenjie Du", "Cheng Liang", "Yunxin Liu", "Xiaozhou Ye", "Ye Ouyang", "Yuanchun Li"], "title": "LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted by MobiCom 2025", "summary": "Large language models (LLMs) have opened new opportunities for automated\nmobile app exploration, an important and challenging problem that used to\nsuffer from the difficulty of generating meaningful UI interactions. However,\nexisting LLM-based exploration approaches rely heavily on LLMs to generate\nactions in almost every step, leading to a huge cost of token fees and\ncomputational resources. We argue that such extensive usage of LLMs is neither\nnecessary nor effective, since many actions during exploration do not require,\nor may even be biased by the abilities of LLMs. Further, based on the insight\nthat a precise and compact knowledge plays the central role for effective\nexploration, we introduce LLM-Explorer, a new exploration agent designed for\nefficiency and affordability. LLM-Explorer uses LLMs primarily for maintaining\nthe knowledge instead of generating actions, and knowledge is used to guide\naction generation in a LLM-less manner. Based on a comparison with 5 strong\nbaselines on 20 typical apps, LLM-Explorer was able to achieve the fastest and\nhighest coverage among all automated app explorers, with over 148x lower cost\nthan the state-of-the-art LLM-based approach.", "AI": {"tldr": "LLM-Explorer\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u79fb\u52a8\u5e94\u7528\u63a2\u7d22\u4ee3\u7406\uff0c\u901a\u8fc7\u51cf\u5c11\u5bf9LLM\u7684\u76f4\u63a5\u4f9d\u8d56\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5e94\u7528\u63a2\u7d22\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u8bb8\u591a\u64cd\u4f5c\u65e0\u9700LLM\u53c2\u4e0e\uff0c\u751a\u81f3\u53ef\u80fd\u53d7\u5176\u80fd\u529b\u5f71\u54cd\u3002", "method": "LLM-Explorer\u4e3b\u8981\u5229\u7528LLM\u7ef4\u62a4\u77e5\u8bc6\u800c\u975e\u751f\u6210\u64cd\u4f5c\uff0c\u77e5\u8bc6\u7528\u4e8e\u4ee5\u65e0LLM\u7684\u65b9\u5f0f\u6307\u5bfc\u64cd\u4f5c\u751f\u6210\u3002", "result": "\u572820\u4e2a\u5178\u578b\u5e94\u7528\u4e0a\u4e0e5\u4e2a\u57fa\u7ebf\u6bd4\u8f83\uff0cLLM-Explorer\u5b9e\u73b0\u4e86\u6700\u5feb\u7684\u8986\u76d6\u7387\u548c\u6700\u4f4e\u7684\u6210\u672c\uff08\u6bd4\u73b0\u6709LLM\u65b9\u6cd5\u4f4e148\u500d\uff09\u3002", "conclusion": "LLM-Explorer\u5c55\u793a\u4e86\u5728\u51cf\u5c11LLM\u4f7f\u7528\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a2\u7d22\u7684\u53ef\u884c\u6027\u3002", "relevance": 75.0}}
{"id": "2505.10950", "pdf": "https://arxiv.org/pdf/2505.10950", "abs": "https://arxiv.org/abs/2505.10950", "authors": ["Tianshuo Zhang", "Gao Jia", "Wenzhe Zhai", "Rui Yann", "Xianglei Xing"], "title": "Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography", "categories": ["cs.LG"], "comment": null, "summary": "Data steganography aims to conceal information within visual content, yet\nexisting spatial- and frequency-domain approaches suffer from trade-offs\nbetween security, capacity, and perceptual quality. Recent advances in\ngenerative models, particularly diffusion models, offer new avenues for\nadaptive image synthesis, but integrating precise information embedding into\nthe generative process remains challenging. We introduce Shackled Dancing\nDiffusion, or SD$^2$, a plug-and-play generative steganography method that\ncombines bit-position locking with diffusion sampling injection to enable\ncontrollable information embedding within the generative trajectory. SD$^2$\nleverages the expressive power of diffusion models to synthesize diverse\ncarrier images while maintaining full message recovery with $100\\%$ accuracy.\nOur method achieves a favorable balance between randomness and constraint,\nenhancing robustness against steganalysis without compromising image fidelity.\nExtensive experiments show that SD$^2$ substantially outperforms prior methods\nin security, embedding capacity, and stability. This algorithm offers new\ninsights into controllable generation and opens promising directions for secure\nvisual communication.", "AI": {"tldr": "SD\u00b2\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u9690\u5199\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4d\u9501\u5b9a\u548c\u6269\u6563\u91c7\u6837\u6ce8\u5165\u5b9e\u73b0\u53ef\u63a7\u4fe1\u606f\u5d4c\u5165\uff0c\u5e73\u8861\u5b89\u5168\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9690\u5199\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u5bb9\u91cf\u548c\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u6269\u6563\u6a21\u578b\u4e3a\u81ea\u9002\u5e94\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u7ed3\u5408\u4f4d\u9501\u5b9a\u548c\u6269\u6563\u91c7\u6837\u6ce8\u5165\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u5408\u6210\u8f7d\u4f53\u56fe\u50cf\uff0c\u540c\u65f6\u786e\u4fdd\u4fe1\u606f\u5b8c\u5168\u6062\u590d\u3002", "result": "SD\u00b2\u5728\u5b89\u5168\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0100%\u4fe1\u606f\u6062\u590d\u3002", "conclusion": "SD\u00b2\u4e3a\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u4e3a\u5b89\u5168\u89c6\u89c9\u901a\u4fe1\u5f00\u8f9f\u4e86\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.11060", "pdf": "https://arxiv.org/pdf/2505.11060", "abs": "https://arxiv.org/abs/2505.11060", "authors": ["David M\u00e9ndez", "Gianpaolo Bontempo", "Elisa Ficarra", "Roberto Confalonieri", "Natalia D\u00edaz-Rodr\u00edguez"], "title": "CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.4; I.5.2"], "comment": "8 pages, 3 figures, 5 tables. Accepted at IJCNN 2025; to appear in\n  IEEE Xplore", "summary": "Deep vision models often rely on biases learned from spurious correlations in\ndatasets. To identify these biases, methods that interpret high-level,\nhuman-understandable concepts are more effective than those relying primarily\non low-level features like heatmaps. A major challenge for these concept-based\nmethods is the lack of image annotations indicating potentially bias-inducing\nconcepts, since creating such annotations requires detailed labeling for each\ndataset and concept, which is highly labor-intensive. We present CUBIC (Concept\nembeddings for Unsupervised Bias IdentifiCation), a novel method that\nautomatically discovers interpretable concepts that may bias classifier\nbehavior. Unlike existing approaches, CUBIC does not rely on predefined bias\ncandidates or examples of model failures tied to specific biases, as such\ninformation is not always available. Instead, it leverages image-text latent\nspace and linear classifier probes to examine how the latent representation of\na superclass label$\\unicode{x2014}$shared by all instances in the\ndataset$\\unicode{x2014}$is influenced by the presence of a given concept. By\nmeasuring these shifts against the normal vector to the classifier's decision\nboundary, CUBIC identifies concepts that significantly influence model\npredictions. Our experiments demonstrate that CUBIC effectively uncovers\npreviously unknown biases using Vision-Language Models (VLMs) without requiring\nthe samples in the dataset where the classifier underperforms or prior\nknowledge of potential biases.", "AI": {"tldr": "CUBIC\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u6f5c\u5728\u7a7a\u95f4\u548c\u7ebf\u6027\u5206\u7c7b\u5668\u63a2\u9488\u81ea\u52a8\u53d1\u73b0\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u7684\u504f\u89c1\u6982\u5ff5\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7684\u504f\u89c1\u5019\u9009\u6216\u5931\u8d25\u793a\u4f8b\u3002", "motivation": "\u6df1\u5ea6\u89c6\u89c9\u6a21\u578b\u5e38\u4f9d\u8d56\u6570\u636e\u96c6\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u5b66\u4e60\u504f\u89c1\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u9884\u5b9a\u4e49\u7684\u504f\u89c1\u4fe1\u606f\u3002CUBIC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u81ea\u52a8\u53d1\u73b0\u6f5c\u5728\u7684\u504f\u89c1\u6982\u5ff5\u3002", "method": "CUBIC\u5229\u7528\u56fe\u50cf-\u6587\u672c\u6f5c\u5728\u7a7a\u95f4\u548c\u7ebf\u6027\u5206\u7c7b\u5668\u63a2\u9488\uff0c\u901a\u8fc7\u5206\u6790\u8d85\u7c7b\u6807\u7b7e\u6f5c\u5728\u8868\u793a\u7684\u53d8\u5316\u6765\u8bc6\u522b\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u7684\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCUBIC\u80fd\u6709\u6548\u53d1\u73b0\u672a\u77e5\u504f\u89c1\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u5931\u8d25\u6837\u672c\u6216\u5148\u9a8c\u504f\u89c1\u77e5\u8bc6\u3002", "conclusion": "CUBIC\u4e3a\u65e0\u76d1\u7763\u504f\u89c1\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.11436", "pdf": "https://arxiv.org/pdf/2505.11436", "abs": "https://arxiv.org/abs/2505.11436", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art", "categories": ["cs.CL", "cs.AI"], "comment": "69 pages, 66 figures, accepted by ACL 2025", "summary": "Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGODBench\u57fa\u51c6\u548cRipple of Thought (RoT)\u6846\u67b6\uff0c\u8bc4\u4f30\u548c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u8bc4\u8bba\u827a\u672f\u521b\u4f5c\u4e2d\u7684\u521b\u9020\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u548cCoT\u65b9\u6cd5\u5728\u751f\u6210\u521b\u610f\u8868\u8fbe\uff08\u5982\u5e7d\u9ed8\u548c\u8bbd\u523a\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u591a\u6a21\u6001\u548c\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165GODBench\u57fa\u51c6\uff08\u89c6\u9891\u548c\u6587\u672c\u6a21\u6001\uff09\u548cRoT\u591a\u6b65\u63a8\u7406\u6846\u67b6\uff08\u53d7\u7269\u7406\u6ce2\u4f20\u64ad\u542f\u53d1\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709MLLMs\u548cCoT\u65b9\u6cd5\u5728\u521b\u610f\u8bc4\u8bba\u751f\u6210\u4e0a\u4ecd\u6709\u6311\u6218\uff0c\u800cRoT\u663e\u8457\u63d0\u5347\u521b\u9020\u529b\u3002", "conclusion": "RoT\u4e3aMLLM\u7684\u521b\u610f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0cGODBench\u4e3a\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 60.0}}
{"id": "2505.10594", "pdf": "https://arxiv.org/pdf/2505.10594", "abs": "https://arxiv.org/abs/2505.10594", "authors": ["Ningxin Gui", "Qianghuai Jia", "Feijun Jiang", "Yuling Jiao", "dechun wang", "Jerry Zhijian Yang"], "title": "CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "We introduce CRPE (Code Reasoning Process Enhancer), an innovative\nthree-stage framework for data synthesis and model training that advances the\ndevelopment of sophisticated code reasoning capabilities in large language\nmodels (LLMs). Building upon existing system-1 models, CRPE addresses the\nfundamental challenge of enhancing LLMs' analytical and logical processing in\ncode generation tasks. Our framework presents a methodologically rigorous yet\nimplementable approach to cultivating advanced code reasoning abilities in\nlanguage models. Through the implementation of CRPE, we successfully develop an\nenhanced COT-Coder that demonstrates marked improvements in code generation\ntasks. Evaluation results on LiveCodeBench (20240701-20240901) demonstrate that\nour COT-Coder-7B-StepDPO, derived from Qwen2.5-Coder-7B-Base, with a pass@1\naccuracy of 21.88, exceeds all models with similar or even larger sizes.\nFurthermore, our COT-Coder-32B-StepDPO, based on Qwen2.5-Coder-32B-Base,\nexhibits superior performance with a pass@1 accuracy of 35.08, outperforming\nGPT4O on the benchmark. Overall, CRPE represents a comprehensive, open-source\nmethod that encompasses the complete pipeline from instruction data acquisition\nthrough expert code reasoning data synthesis, culminating in an autonomous\nreasoning enhancement mechanism.", "AI": {"tldr": "CRPE\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5206\u6790\u548c\u903b\u8f91\u5904\u7406\u80fd\u529b\u3002\u901a\u8fc7CRPE\u8bad\u7ec3\u7684COT-Coder\u6a21\u578b\u5728LiveCodeBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u540c\u7c7b\u751a\u81f3\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5206\u6790\u548c\u903b\u8f91\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCRPE\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u57fa\u4e8eQwen2.5-Coder\u6a21\u578b\u8fdb\u884c\u6539\u8fdb\u3002", "result": "COT-Coder-7B\u548c32B\u5728LiveCodeBench\u4e0a\u7684pass@1\u51c6\u786e\u7387\u5206\u522b\u4e3a21.88\u548c35.08\uff0c\u8d85\u8d8a\u4e86GPT4O\u7b49\u6a21\u578b\u3002", "conclusion": "CRPE\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5f00\u6e90\u65b9\u6cd5\uff0c\u4ece\u6570\u636e\u5408\u6210\u5230\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10951", "pdf": "https://arxiv.org/pdf/2505.10951", "abs": "https://arxiv.org/abs/2505.10951", "authors": ["Qiuyu Zhu", "Liang Zhang", "Qianxiong Xu", "Cheng Long", "Jie Zhang"], "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache", "categories": ["cs.LG"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).", "AI": {"tldr": "SubGCache\u901a\u8fc7\u805a\u7c7b\u67e5\u8be2\u5e76\u91cd\u7528\u9884\u8ba1\u7b97\u7684\u5173\u952e\u503c\u7f13\u5b58\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u9488\u5bf9\u4e0d\u540c\u67e5\u8be2\u53ef\u80fd\u68c0\u7d22\u5230\u76f8\u4f3c\u5b50\u56fe\u7684\u95ee\u9898\uff0c\u63d0\u51faSubGCache\u4ee5\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\uff0c\u63d0\u5347\u6548\u7387\u3002", "method": "SubGCache\u901a\u8fc7\u5b50\u56fe\u5d4c\u5165\u805a\u7c7b\u67e5\u8be2\uff0c\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u6784\u5efa\u4ee3\u8868\u6027\u5b50\u56fe\u5e76\u9884\u8ba1\u7b97\u5176KV\u7f13\u5b58\uff0c\u4ece\u800c\u5728\u67e5\u8be2\u65f6\u91cd\u7528\u7f13\u5b58\u4ee5\u8282\u7701\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSubGCache\u5728\u591a\u4e2aLLM\u548cRAG\u6846\u67b6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe6.68\u500d\u7684TTFT\u51cf\u5c11\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "SubGCache\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u56fe\u7684RAG\u7cfb\u7edf\u7684\u63a8\u7406\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.11062", "pdf": "https://arxiv.org/pdf/2505.11062", "abs": "https://arxiv.org/abs/2505.11062", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "HSRMamba: Efficient Wavelet Stripe State Space Model for Hyperspectral Image Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Single hyperspectral image super-resolution (SHSR) aims to restore\nhigh-resolution images from low-resolution hyperspectral images. Recently, the\nVisual Mamba model has achieved an impressive balance between performance and\ncomputational efficiency. However, due to its 1D scanning paradigm, the model\nmay suffer from potential artifacts during image generation. To address this\nissue, we propose HSRMamba. While maintaining the computational efficiency of\nVisual Mamba, we introduce a strip-based scanning scheme to effectively reduce\nartifacts from global unidirectional scanning. Additionally, HSRMamba uses\nwavelet decomposition to alleviate modal conflicts between high-frequency\nspatial features and low-frequency spectral features, further improving\nsuper-resolution performance. Extensive experiments show that HSRMamba not only\nexcels in reducing computational load and model size but also outperforms\nexisting methods, achieving state-of-the-art results.", "AI": {"tldr": "HSRMamba\u6539\u8fdbVisual Mamba\u6a21\u578b\uff0c\u901a\u8fc7\u6761\u5e26\u626b\u63cf\u548c\u5c0f\u6ce2\u5206\u89e3\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u8fbe\u5230SOTA\u6548\u679c\u3002", "motivation": "\u89e3\u51b3Visual Mamba\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u56e01D\u626b\u63cf\u5bfc\u81f4\u7684\u6f5c\u5728\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6761\u5e26\u626b\u63cf\u65b9\u6848\u548c\u5c0f\u6ce2\u5206\u89e3\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u3002", "result": "HSRMamba\u5728\u8ba1\u7b97\u8d1f\u8f7d\u548c\u6a21\u578b\u5927\u5c0f\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HSRMamba\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u5e73\u8861\u3002", "relevance": 30.0}}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441", "abs": "https://arxiv.org/abs/2505.11441", "authors": ["Xianzhen Luo", "Shijie Xuyang", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "title": "Is Compression Really Linear with Code Intelligence?", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6570\u636e\u538b\u7f29\u4e0e\u4ee3\u7801LLMs\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u65b9\u6cd5Format Annealing\uff0c\u5e76\u63ed\u793a\u4e86\u4ee3\u7801\u667a\u80fd\u4e0e\u538b\u7f29\u6548\u7387\u4e4b\u95f4\u7684\u5bf9\u6570\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u538b\u7f29\u4e0e\u4ee3\u7801LLMs\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u3001\u591a\u4efb\u52a1\u4ee3\u7801\u9886\u57df\u7684\u516c\u5e73\u8bc4\u4f30\u95ee\u9898\u3002", "method": "\u5f15\u5165Format Annealing\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u8bed\u8a00\u3001\u591a\u4efb\u52a1\u4ee3\u7801\u57fa\u51c6\u8bc4\u4f30\u5f00\u6e90Code LLMs\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u5927\u89c4\u6a21\u4ee3\u7801\u9a8c\u8bc1\u96c6\u6d4b\u91cf\u538b\u7f29\u6548\u7387\uff08BPC\uff09\u3002", "result": "\u53d1\u73b0\u4ee3\u7801\u667a\u80fd\u4e0eBPC\u4e4b\u95f4\u5b58\u5728\u5bf9\u6570\u5173\u7cfb\uff0c\u4fee\u6b63\u4e86\u6b64\u524d\u7ebf\u6027\u5173\u7cfb\u7684\u5047\u8bbe\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u538b\u7f29\u5728\u4ee3\u7801\u667a\u80fd\u53d1\u5c55\u4e2d\u4f5c\u7528\u7684\u66f4\u7ec6\u81f4\u7406\u89e3\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u4ee3\u7801\u9886\u57df\u8bc4\u4f30\u6846\u67b6\u3002", "relevance": 85.0}}
{"id": "2505.10596", "pdf": "https://arxiv.org/pdf/2505.10596", "abs": "https://arxiv.org/abs/2505.10596", "authors": ["Retno Larasati"], "title": "Inclusivity of AI Speech in Healthcare: A Decade Look Back", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The integration of AI speech recognition technologies into healthcare has the\npotential to revolutionize clinical workflows and patient-provider\ncommunication. However, this study reveals significant gaps in inclusivity,\nwith datasets and research disproportionately favouring high-resource\nlanguages, standardized accents, and narrow demographic groups. These biases\nrisk perpetuating healthcare disparities, as AI systems may misinterpret speech\nfrom marginalized groups. This paper highlights the urgent need for inclusive\ndataset design, bias mitigation research, and policy frameworks to ensure\nequitable access to AI speech technologies in healthcare.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u53ca\u5176\u6f5c\u5728\u7684\u504f\u89c1\u95ee\u9898\uff0c\u6307\u51fa\u6570\u636e\u96c6\u548c\u7814\u7a76\u504f\u5411\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u7279\u5b9a\u4eba\u7fa4\uff0c\u53ef\u80fd\u5bfc\u81f4\u533b\u7597\u4e0d\u5e73\u7b49\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793aAI\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u5728\u533b\u7597\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u5f71\u54cd\uff0c\u4ee5\u63a8\u52a8\u66f4\u5305\u5bb9\u7684\u6280\u672f\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6570\u636e\u96c6\u548c\u7814\u7a76\uff0c\u8bc6\u522b\u51fa\u8bed\u8a00\u3001\u53e3\u97f3\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7684\u504f\u89c1\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u7279\u5b9a\u4eba\u7fa4\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u53ef\u80fd\u4ea7\u751f\u8bef\u5224\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u8bbe\u8ba1\u5305\u5bb9\u6027\u6570\u636e\u96c6\u3001\u7814\u7a76\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\uff0c\u5e76\u5236\u5b9a\u653f\u7b56\u6846\u67b6\u4ee5\u786e\u4fddAI\u8bed\u97f3\u6280\u672f\u7684\u516c\u5e73\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10954", "pdf": "https://arxiv.org/pdf/2505.10954", "abs": "https://arxiv.org/abs/2505.10954", "authors": ["Koki Iwai", "Yusuke Kumagae", "Yuki Koyama", "Masahiro Hamasaki", "Masataka Goto"], "title": "Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design", "categories": ["cs.LG", "cs.AI", "cs.GR", "cs.HC"], "comment": "17 pages, 15 figures", "summary": "Preferential Bayesian optimization (PBO) is a variant of Bayesian\noptimization that observes relative preferences (e.g., pairwise comparisons)\ninstead of direct objective values, making it especially suitable for\nhuman-in-the-loop scenarios. However, real-world optimization tasks often\ninvolve inequality constraints, which existing PBO methods have not yet\naddressed. To fill this gap, we propose constrained preferential Bayesian\noptimization (CPBO), an extension of PBO that incorporates inequality\nconstraints for the first time. Specifically, we present a novel acquisition\nfunction for this purpose. Our technical evaluation shows that our CPBO method\nsuccessfully identifies optimal solutions by focusing on exploring feasible\nregions. As a practical application, we also present a designer-in-the-loop\nsystem for banner ad design using CPBO, where the objective is the designer's\nsubjective preference, and the constraint ensures a target predicted\nclick-through rate. We conducted a user study with professional ad designers,\ndemonstrating the potential benefits of our approach in guiding creative design\nunder real-world constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ea6\u675f\u504f\u597d\u8d1d\u53f6\u65af\u4f18\u5316\uff08CPBO\uff09\uff0c\u9996\u6b21\u5c06\u4e0d\u7b49\u5f0f\u7ea6\u675f\u5f15\u5165\u504f\u597d\u8d1d\u53f6\u65af\u4f18\u5316\uff08PBO\uff09\uff0c\u9002\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u3002", "motivation": "\u73b0\u6709PBO\u65b9\u6cd5\u672a\u8003\u8651\u73b0\u5b9e\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u96c6\u51fd\u6570\uff0c\u6269\u5c55PBO\u4ee5\u652f\u6301\u7ea6\u675f\u4f18\u5316\u3002", "result": "CPBO\u6210\u529f\u63a2\u7d22\u53ef\u884c\u533a\u57df\u5e76\u627e\u5230\u6700\u4f18\u89e3\uff0c\u5e94\u7528\u4e8e\u5e7f\u544a\u8bbe\u8ba1\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CPBO\u586b\u8865\u4e86PBO\u5728\u7ea6\u675f\u4f18\u5316\u4e2d\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u6307\u5bfc\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11070", "pdf": "https://arxiv.org/pdf/2505.11070", "abs": "https://arxiv.org/abs/2505.11070", "authors": ["Renjie Chen", "Wenfeng Lin", "Yichen Zhang", "Jiangchuan Wei", "Boyuan Liu", "Chao Feng", "Jiao Ran", "Mingyu Guo"], "title": "Towards Self-Improvement of Diffusion Models via Group Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aligning text-to-image (T2I) diffusion models with Direct Preference\nOptimization (DPO) has shown notable improvements in generation quality.\nHowever, applying DPO to T2I faces two challenges: the sensitivity of DPO to\npreference pairs and the labor-intensive process of collecting and annotating\nhigh-quality data. In this work, we demonstrate that preference pairs with\nmarginal differences can degrade DPO performance. Since DPO relies exclusively\non relative ranking while disregarding the absolute difference of pairs, it may\nmisclassify losing samples as wins, or vice versa. We empirically show that\nextending the DPO from pairwise to groupwise and incorporating reward\nstandardization for reweighting leads to performance gains without explicit\ndata selection. Furthermore, we propose Group Preference Optimization (GPO), an\neffective self-improvement method that enhances performance by leveraging the\nmodel's own capabilities without requiring external data. Extensive experiments\ndemonstrate that GPO is effective across various diffusion models and tasks.\nSpecifically, combining with widely used computer vision models, such as YOLO\nand OCR, the GPO improves the accurate counting and text rendering capabilities\nof the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a\nplug-and-play method, no extra overhead is introduced during inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGroup Preference Optimization (GPO)\uff0c\u901a\u8fc7\u5c06DPO\u4ece\u6210\u5bf9\u6269\u5c55\u5230\u7ec4\u522b\u5e76\u7ed3\u5408\u5956\u52b1\u6807\u51c6\u5316\uff0c\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u3002", "motivation": "\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u504f\u597d\u5bf9\u7684\u654f\u611f\u6027\u548c\u6570\u636e\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u6269\u5c55DPO\u4e3a\u7ec4\u522b\u4f18\u5316\uff08GPO\uff09\uff0c\u7ed3\u5408\u5956\u52b1\u6807\u51c6\u5316\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u80fd\u529b\u8fdb\u884c\u81ea\u6211\u6539\u8fdb\u3002", "result": "GPO\u5728Stable Diffusion 3.5 Medium\u4e0a\u63d0\u5347\u4e8620%\u7684\u8ba1\u6570\u548c\u6587\u672c\u6e32\u67d3\u80fd\u529b\uff0c\u4e14\u63a8\u7406\u65f6\u65e0\u989d\u5916\u5f00\u9500\u3002", "conclusion": "GPO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u548c\u4efb\u52a1\u3002", "relevance": 75.0}}
{"id": "2505.11462", "pdf": "https://arxiv.org/pdf/2505.11462", "abs": "https://arxiv.org/abs/2505.11462", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u5206\u4e3a\u63a8\u7406\u548c\u77e5\u8bc6\u5b50\u96c6\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u751f\u7269\u533b\u5b66\u548c\u901a\u7528\u9886\u57df\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578bBioMed-R1\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6df7\u5408\u4e86\u63a8\u7406\u548c\u4e8b\u5b9e\u56de\u5fc6\uff0c\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528PubMedBERT\u5206\u7c7b\u5668\u5c0611\u4e2a\u751f\u7269\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u5206\u4e3a\u63a8\u7406\u548c\u77e5\u8bc6\u5b50\u96c6\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3BioMed-R1\u3002", "result": "\u4ec532.8%\u7684\u95ee\u9898\u9700\u8981\u590d\u6742\u63a8\u7406\uff1b\u751f\u7269\u533b\u5b66\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u901a\u7528\u6a21\u578b\u66f4\u7a33\u5065\uff1bBioMed-R1\u5728\u540c\u7c7b\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u4e34\u5e8a\u6848\u4f8b\u548c\u5bf9\u6297\u6027\u573a\u666f\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "relevance": 85.0}}
{"id": "2505.10960", "pdf": "https://arxiv.org/pdf/2505.10960", "abs": "https://arxiv.org/abs/2505.10960", "authors": ["Vijay Prakash Dwivedi", "Sri Jaladi", "Yangyi Shen", "Federico L\u00f3pez", "Charilaos I. Kanatsoulis", "Rishi Puri", "Matthias Fey", "Jure Leskovec"], "title": "Relational Graph Transformer", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": "Code: https://github.com/snap-stanford/relgt", "summary": "Relational Deep Learning (RDL) is a promising approach for building\nstate-of-the-art predictive models on multi-table relational data by\nrepresenting it as a heterogeneous temporal graph. However, commonly used Graph\nNeural Network models suffer from fundamental limitations in capturing complex\nstructural patterns and long-range dependencies that are inherent in relational\ndata. While Graph Transformers have emerged as powerful alternatives to GNNs on\ngeneral graphs, applying them to relational entity graphs presents unique\nchallenges: (i) Traditional positional encodings fail to generalize to massive,\nheterogeneous graphs; (ii) existing architectures cannot model the temporal\ndynamics and schema constraints of relational data; (iii) existing tokenization\nschemes lose critical structural information. Here we introduce the Relational\nGraph Transformer (RelGT), the first graph transformer architecture designed\nspecifically for relational tables. RelGT employs a novel multi-element\ntokenization strategy that decomposes each node into five components (features,\ntype, hop distance, time, and local structure), enabling efficient encoding of\nheterogeneity, temporality, and topology without expensive precomputation. Our\narchitecture combines local attention over sampled subgraphs with global\nattention to learnable centroids, incorporating both local and database-wide\nrepresentations. Across 21 tasks from the RelBench benchmark, RelGT\nconsistently matches or outperforms GNN baselines by up to 18%, establishing\nGraph Transformers as a powerful architecture for Relational Deep Learning.", "AI": {"tldr": "RelGT\u662f\u4e00\u79cd\u4e13\u4e3a\u5173\u7cfb\u8868\u8bbe\u8ba1\u7684\u56feTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5143\u7d20\u6807\u8bb0\u5316\u7b56\u7565\u548c\u5c40\u90e8\u4e0e\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u7cfb\u6570\u636e\u7684\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfGNN\u548c\u56feTransformer\u5728\u5173\u7cfb\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7ed3\u6784\u6a21\u5f0f\u3001\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u65f6\u95f4\u52a8\u6001\u548c\u6a21\u5f0f\u7ea6\u675f\u3002", "method": "\u63d0\u51faRelGT\u67b6\u6784\uff0c\u91c7\u7528\u591a\u5143\u7d20\u6807\u8bb0\u5316\u7b56\u7565\uff08\u7279\u5f81\u3001\u7c7b\u578b\u3001\u8df3\u6570\u8ddd\u79bb\u3001\u65f6\u95f4\u3001\u5c40\u90e8\u7ed3\u6784\uff09\uff0c\u7ed3\u5408\u5c40\u90e8\u5b50\u56fe\u6ce8\u610f\u529b\u548c\u5168\u5c40\u53ef\u5b66\u4e60\u4e2d\u5fc3\u6ce8\u610f\u529b\u3002", "result": "\u5728RelBench\u57fa\u51c6\u6d4b\u8bd5\u768421\u4e2a\u4efb\u52a1\u4e2d\uff0cRelGT\u6027\u80fd\u4f18\u4e8eGNN\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u9ad8\u63d0\u534718%\u3002", "conclusion": "RelGT\u8bc1\u660e\u4e86\u56feTransformer\u5728\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5173\u7cfb\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.11075", "pdf": "https://arxiv.org/pdf/2505.11075", "abs": "https://arxiv.org/abs/2505.11075", "authors": ["Jianghang Lin", "Yilin Lu", "Yunhang Shen", "Chaoyang Zhu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Pseudo-Label Quality Decoupling and Correction for Semi-Supervised Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semi-Supervised Instance Segmentation (SSIS) involves classifying and\ngrouping image pixels into distinct object instances using limited labeled\ndata. This learning paradigm usually faces a significant challenge of unstable\nperformance caused by noisy pseudo-labels of instance categories and pixel\nmasks. We find that the prevalent practice of filtering instance pseudo-labels\nassessing both class and mask quality with a single score threshold, frequently\nleads to compromises in the trade-off between the qualities of class and mask\nlabels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and\nCorrection (PL-DC) framework for SSIS to tackle the above challenges. Firstly,\nat the instance level, a decoupled dual-threshold filtering mechanism is\ndesigned to decouple class and mask quality estimations for instance-level\npseudo-labels, thereby independently controlling pixel classifying and grouping\nqualities. Secondly, at the category level, we introduce a dynamic instance\ncategory correction module to dynamically correct the pseudo-labels of instance\ncategories, effectively alleviating category confusion. Lastly, we introduce a\npixel-level mask uncertainty-aware mechanism at the pixel level to re-weight\nthe mask loss for different pixels, thereby reducing the impact of noise\nintroduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO\nand Cityscapes datasets demonstrate that the proposed PL-DC achieves\nsignificant performance improvements, setting new state-of-the-art results for\nSSIS. Notably, our PL-DC shows substantial gains even with minimal labeled\ndata, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and\n+15.5 mAP with 5% Cityscapes labeled data. The code will be public.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u6846\u67b6PL-DC\uff0c\u901a\u8fc7\u89e3\u8026\u4f2a\u6807\u7b7e\u7684\u7c7b\u522b\u548c\u63a9\u7801\u8d28\u91cf\u8bc4\u4f30\uff0c\u52a8\u6001\u4fee\u6b63\u7c7b\u522b\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u50cf\u7d20\u7ea7\u63a9\u7801\u4e0d\u786e\u5b9a\u6027\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u534a\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u4e2d\u4f2a\u6807\u7b7e\u7684\u566a\u58f0\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7c7b\u522b\u548c\u63a9\u7801\u8d28\u91cf\u8bc4\u4f30\u4e0a\u5b58\u5728\u59a5\u534f\u3002", "method": "\u8bbe\u8ba1\u4e86\u5b9e\u4f8b\u7ea7\u7684\u89e3\u8026\u53cc\u9608\u503c\u8fc7\u6ee4\u673a\u5236\u3001\u7c7b\u522b\u7ea7\u7684\u52a8\u6001\u4fee\u6b63\u6a21\u5757\u548c\u50cf\u7d20\u7ea7\u7684\u63a9\u7801\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u673a\u5236\u3002", "result": "\u5728COCO\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "PL-DC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u534a\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.11470", "pdf": "https://arxiv.org/pdf/2505.11470", "abs": "https://arxiv.org/abs/2505.11470", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "title": "No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies", "categories": ["cs.CL"], "comment": null, "summary": "We introduce two reference-free metrics for quality evaluation of taxonomies.\nThe first metric evaluates robustness by calculating the correlation between\nsemantic and taxonomic similarity, covering a type of error not handled by\nexisting metrics. The second uses Natural Language Inference to assess logical\nadequacy. Both metrics are tested on five taxonomies and are shown to correlate\nwell with F1 against gold-standard taxonomies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u53c2\u8003\u6307\u6807\u7528\u4e8e\u8bc4\u4f30\u5206\u7c7b\u6cd5\u7684\u8d28\u91cf\uff0c\u5206\u522b\u57fa\u4e8e\u8bed\u4e49\u4e0e\u5206\u7c7b\u76f8\u4f3c\u6027\u7684\u76f8\u5173\u6027\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u6307\u6807\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u5206\u7c7b\u6cd5\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u8bed\u4e49\u548c\u903b\u8f91\u65b9\u9762\u7684\u9519\u8bef\u3002", "method": "\u7b2c\u4e00\u79cd\u6307\u6807\u901a\u8fc7\u8ba1\u7b97\u8bed\u4e49\u4e0e\u5206\u7c7b\u76f8\u4f3c\u6027\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u9c81\u68d2\u6027\uff1b\u7b2c\u4e8c\u79cd\u6307\u6807\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8bc4\u4f30\u903b\u8f91\u5408\u7406\u6027\u3002", "result": "\u4e24\u79cd\u6307\u6807\u5728\u4e94\u4e2a\u5206\u7c7b\u6cd5\u4e0a\u6d4b\u8bd5\uff0c\u4e0e\u9ec4\u91d1\u6807\u51c6\u5206\u7c7b\u6cd5\u7684F1\u5206\u6570\u76f8\u5173\u6027\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u6307\u6807\u80fd\u6709\u6548\u8865\u5145\u73b0\u6709\u5206\u7c7b\u6cd5\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u548c\u903b\u8f91\u65b9\u9762\u3002", "relevance": 40.0}}
{"id": "2505.10978", "pdf": "https://arxiv.org/pdf/2505.10978", "abs": "https://arxiv.org/abs/2505.10978", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "title": "Group-in-Group Policy Optimization for LLM Agent Training", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGiGPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u957f\u5e8f\u5217LLM\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u7cbe\u7ec6\u4fe1\u7528\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u5e8f\u5217LLM\u4ee3\u7406\u8bad\u7ec3\u4e2d\u4fe1\u7528\u5206\u914d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u7684\u95ee\u9898\u3002", "method": "GiGPO\u91c7\u7528\u4e24\u7ea7\u7ed3\u6784\u4f30\u8ba1\u76f8\u5bf9\u4f18\u52bf\uff1a\u5b8f\u89c2\u8f68\u8ff9\u7ec4\u548c\u5fae\u89c2\u72b6\u6001\u7ec4\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u6eda\u52a8\u3002", "result": "\u5728ALFWorld\u548cWebShop\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5206\u522b\u63d0\u534712%\u548c9%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684GPU\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "GiGPO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684RL\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u957f\u5e8f\u5217LLM\u4ee3\u7406\u8bad\u7ec3\u3002", "relevance": 85.0}}
{"id": "2505.11099", "pdf": "https://arxiv.org/pdf/2505.11099", "abs": "https://arxiv.org/abs/2505.11099", "authors": ["Bin Liu", "Chunyang Wang", "Xuelian Liu", "Guan Xi", "Ge Zhang", "Ziteng Yao", "Mengxue Dong"], "title": "Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification", "categories": ["cs.CV"], "comment": null, "summary": "The point cloud classification tasks face the dual challenge of efficiently\nextracting local geometric features while maintaining model complexity. The\nMamba architecture utilizes the linear complexity advantage of state space\nmodels (SSMs) to overcome the computational bottleneck of Transformers while\nbalancing global modeling capabilities. However, the inherent contradiction\nbetween its unidirectional dependency and the unordered nature of point clouds\nimpedes modeling spatial correlation in local neighborhoods, thus constraining\ngeometric feature extraction. This paper proposes Hybrid-Emba3D, a\nbidirectional Mamba model enhanced by geometry-feature coupling and cross-path\nfeature hybridization. The Local geometric pooling with geometry-feature\ncoupling mechanism significantly enhances local feature discriminative power\nvia coordinated propagation and dynamic aggregation of geometric information\nbetween local center points and their neighborhoods, without introducing\nadditional parameters. The designed Collaborative feature enhancer adopts\ndual-path hybridization, effectively handling local mutations and sparse key\nsignals, breaking through the limitations of traditional SSM long-range\nmodeling. Experimental results demonstrate that the proposed model achieves a\nnew SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M\nadditional.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHybrid-Emba3D\uff0c\u4e00\u79cd\u53cc\u5411Mamba\u6a21\u578b\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u8026\u5408\u548c\u8de8\u8def\u5f84\u7279\u5f81\u6df7\u5408\u4f18\u5316\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u5e73\u8861\u7684\u6311\u6218\uff0c\u540c\u65f6\u514b\u670dMamba\u67b6\u6784\u5728\u65e0\u5e8f\u70b9\u4e91\u6570\u636e\u4e2d\u5efa\u6a21\u7a7a\u95f4\u76f8\u5173\u6027\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faHybrid-Emba3D\uff0c\u7ed3\u5408\u51e0\u4f55\u7279\u5f81\u8026\u5408\u673a\u5236\u548c\u53cc\u8def\u5f84\u7279\u5f81\u6df7\u5408\uff0c\u589e\u5f3a\u5c40\u90e8\u7279\u5f81\u5224\u522b\u80fd\u529b\u5e76\u5904\u7406\u7a81\u53d8\u548c\u7a00\u758f\u4fe1\u53f7\u3002", "result": "\u5728ModelNet40\u4e0a\u8fbe\u523095.99%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4ec5\u589e\u52a00.03M\u53c2\u6570\u3002", "conclusion": "Hybrid-Emba3D\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u8026\u5408\u548c\u8de8\u8def\u5f84\u6df7\u5408\u663e\u8457\u63d0\u5347\u70b9\u4e91\u5206\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "relevance": 60.0}}
{"id": "2505.11475", "pdf": "https://arxiv.org/pdf/2505.11475", "abs": "https://arxiv.org/abs/2505.11475", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "38 pages, 2 figures", "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference", "AI": {"tldr": "\u4ecb\u7ecd\u4e86HelpSteer3-Preference\u6570\u636e\u96c6\uff0c\u7528\u4e8eRLHF\u8bad\u7ec3\uff0c\u5305\u542b40,000\u591a\u6837\u672c\uff0c\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u516c\u5f00\u504f\u597d\u6570\u636e\u96c6\u8d28\u91cf\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301RLHF\u8bad\u7ec3\u3002", "method": "\u6536\u96c6\u5e76\u6807\u6ce8\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u504f\u597d\u6570\u636e\uff0c\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5956\u52b1\u6a21\u578b\u5728RM-Bench\u548cJudgeBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u7ea610%\u3002", "conclusion": "HelpSteer3-Preference\u6570\u636e\u96c6\u6709\u6548\u652f\u6301RLHF\u8bad\u7ec3\u548c\u6a21\u578b\u5bf9\u9f50\u3002", "relevance": 85.0}}
{"id": "2505.10983", "pdf": "https://arxiv.org/pdf/2505.10983", "abs": "https://arxiv.org/abs/2505.10983", "authors": ["Haozheng Luo", "Chenghao Qiu", "Yimin Wang", "Shang Wu", "Jiahao Yu", "Han Liu", "Binghui Wang", "Yan Chen"], "title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,\nGenoArmory offers the first comprehensive evaluation framework to\nsystematically assess the vulnerability of GFMs to adversarial attacks.\nMethodologically, we evaluate the adversarial robustness of five\nstate-of-the-art GFMs using four widely adopted attack algorithms and three\ndefense strategies. Importantly, our benchmark provides an accessible and\ncomprehensive framework to analyze GFM vulnerabilities with respect to model\narchitecture, quantization schemes, and training datasets. Additionally, we\nintroduce GenoAdv, a new adversarial sample dataset designed to improve GFM\nsafety. Empirically, classification models exhibit greater robustness to\nadversarial perturbations compared to generative models, highlighting the\nimpact of task type on model vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features.", "AI": {"tldr": "GenoArmory\u662f\u9996\u4e2a\u9488\u5bf9\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\uff08GFMs\uff09\u7684\u7edf\u4e00\u5bf9\u6297\u653b\u51fb\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4e94\u79cdGFMs\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709GFM\u57fa\u51c6\u7f3a\u4e4f\u5bf9\u6297\u653b\u51fb\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0cGenoArmory\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u65e8\u5728\u63d0\u5347GFM\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u653b\u51fb\u7b97\u6cd5\u548c\u4e09\u79cd\u9632\u5fa1\u7b56\u7565\u8bc4\u4f30\u4e94\u79cdGFMs\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5e76\u5206\u6790\u6a21\u578b\u67b6\u6784\u3001\u91cf\u5316\u65b9\u6848\u548c\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u6bd4\u751f\u6210\u6a21\u578b\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5bf9\u6297\u653b\u51fb\u5e38\u9488\u5bf9\u751f\u7269\u5b66\u91cd\u8981\u533a\u57df\u3002", "conclusion": "GenoArmory\u4e3aGFM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "relevance": 40.0}}
{"id": "2505.11109", "pdf": "https://arxiv.org/pdf/2505.11109", "abs": "https://arxiv.org/abs/2505.11109", "authors": ["Florinel-Alin Croitoru", "Vlad Hondru", "Marius Popescu", "Radu Tudor Ionescu", "Fahad Shahbaz Khan", "Mubarak Shah"], "title": "MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "15 pages", "summary": "We present the first large-scale open-set benchmark for multilingual\naudio-video deepfake detection. Our dataset comprises over 250 hours of real\nand fake videos across eight languages, with 60% of data being generated. For\neach language, the fake videos are generated with seven distinct deepfake\ngeneration models, selected based on the quality of the generated content. We\norganize the training, validation and test splits such that only a subset of\nthe chosen generative models and languages are available during training, thus\ncreating several challenging open-set evaluation setups. We perform experiments\nwith various pre-trained and fine-tuned deepfake detectors proposed in recent\nliterature. Our results show that state-of-the-art detectors are not currently\nable to maintain their performance levels when tested in our open-set\nscenarios. We publicly release our data and code at:\nhttps://huggingface.co/datasets/unibuc-cs/MAVOS-DD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u97f3\u9891-\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u5927\u89c4\u6a21\u5f00\u653e\u96c6\u57fa\u51c6\uff0c\u5305\u542b8\u79cd\u8bed\u8a00\u7684250\u5c0f\u65f6\u771f\u5b9e\u4e0e\u4f2a\u9020\u89c6\u9891\uff0c60%\u4e3a\u751f\u6210\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u4e3a\u591a\u8bed\u8a00\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u5f00\u653e\u96c6\u57fa\u51c6\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b8\u79cd\u8bed\u8a00\u30017\u79cd\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u5f00\u653e\u96c6\u8bc4\u4f30\u573a\u666f\uff0c\u6d4b\u8bd5\u591a\u79cd\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u68c0\u6d4b\u5668\u3002", "result": "\u73b0\u6709\u5148\u8fdb\u68c0\u6d4b\u5668\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5f00\u653e\u96c6\u591a\u8bed\u8a00\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u3002", "relevance": 30.0}}
{"id": "2505.11480", "pdf": "https://arxiv.org/pdf/2505.11480", "abs": "https://arxiv.org/abs/2505.11480", "authors": ["Anjiang Wei", "Tarun Suresh", "Huanmi Tan", "Yinglun Xu", "Gagandeep Singh", "Ke Wang", "Alex Aiken"], "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.PF", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6c47\u7f16\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u7528PPO\u8bad\u7ec3LLMs\uff0c\u5e76\u5728\u771f\u5b9e\u7a0b\u5e8f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u672a\u5f00\u53d1\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6c47\u7f16\u4ee3\u7801\u5c42\u9762\uff0c\u4ee5\u5b9e\u73b0\u6bd4\u9ad8\u7ea7\u8bed\u8a00\u66f4\u7ec6\u7c92\u5ea6\u7684\u6027\u80fd\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08PPO\uff09\uff0c\u7ed3\u5408\u529f\u80fd\u6b63\u786e\u6027\u548c\u6267\u884c\u6027\u80fd\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8bad\u7ec3LLM\u6a21\u578bQwen2.5-Coder-7B-PPO\u3002", "result": "\u6a21\u578b\u57288,072\u4e2a\u771f\u5b9e\u7a0b\u5e8f\u4e0a\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7387\u4e3a96.0%\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53471.47\u500d\uff0c\u4f18\u4e8e\u5305\u62ecClaude-3.7-sonnet\u5728\u5185\u768420\u4e2a\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u91ca\u653eLLMs\u5728\u6c47\u7f16\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.10603", "pdf": "https://arxiv.org/pdf/2505.10603", "abs": "https://arxiv.org/abs/2505.10603", "authors": ["Jorge Machado"], "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5f00\u6e90\u548c\u4e13\u6709\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u516c\u5171\u548c\u5b89\u5168\u7684Gen AI\u6846\u67b6\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u5f00\u6e90\u548c\u4e13\u6709\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5168\u9762\u6bd4\u8f83\u7814\u7a76\uff0c\u4ee5\u53ca\u5982\u4f55\u6784\u5efa\u53ef\u4fe1\u8d56\u7684Gen AI\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u3001\u6279\u5224\u6027\u5206\u6790\u548c\u6bd4\u8f83\u5206\u6790\u7684\u65b9\u6cd5\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u66f4\u5177\u900f\u660e\u5ea6\u548c\u7075\u6d3b\u6027\uff0c\u800c\u4e13\u6709\u7cfb\u7edf\u6280\u672f\u652f\u6301\u66f4\u597d\u4f46\u7f3a\u4e4f\u95ee\u8d23\u5236\u3002", "conclusion": "\u591a\u5229\u76ca\u76f8\u5173\u65b9\u6cbb\u7406\u548c\u76d1\u7ba1\u6846\u67b6\u5bf9\u8d1f\u8d23\u4efb\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "relevance": 75.0}}
{"id": "2505.10992", "pdf": "https://arxiv.org/pdf/2505.10992", "abs": "https://arxiv.org/abs/2505.10992", "authors": ["Feiran You", "Hongyang Du"], "title": "ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent\nmanagement due to the diverse user requirements and time-varying wireless\nconditions. These factors introduce significant decision complexity, which\nlimits the adaptability of existing Deep Reinforcement Learning (DRL) methods.\nIn many DRL algorithms, especially those involving value-based or actor-critic\nstructures, the critic component plays a key role in guiding policy learning by\nestimating value functions. However, conventional critic models often use\nshallow architectures that map observations directly to scalar estimates,\nlimiting their ability to handle multi-task complexity. In contrast, recent\nprogress in inference-time scaling of Large Language Models (LLMs) has shown\nthat generating intermediate reasoning steps can significantly improve decision\nquality. Motivated by this, we propose ReaCritic, a large reasoning\ntransformer-based criticmodel scaling scheme that brings reasoning ability into\nDRL. ReaCritic performs horizontal reasoning over parallel state-action inputs\nand vertical reasoning through deep transformer stacks. It is compatible with a\nbroad range of value-based and actor-critic DRL algorithms and enhances\ngeneralization in dynamic wireless environments. Extensive experiments\ndemonstrate that ReaCritic improves convergence speed and final performance\nacross various HetNet settings and standard OpenAI Gym control tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReaCritic\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u63a8\u7406Transformer\u7684\u6279\u8bc4\u6a21\u578b\uff0c\u901a\u8fc7\u6c34\u5e73\u548c\u5782\u76f4\u63a8\u7406\u589e\u5f3aDRL\u5728\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f02\u6784\u7f51\u7edc\u7684\u591a\u6837\u6027\u548c\u65f6\u53d8\u6027\u5bfc\u81f4\u73b0\u6709DRL\u65b9\u6cd5\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u6279\u8bc4\u6a21\u578b\u7684\u6d45\u5c42\u67b6\u6784\u9650\u5236\u4e86\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002\u53d7LLM\u63a8\u7406\u80fd\u529b\u542f\u53d1\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "ReaCritic\u7ed3\u5408\u6c34\u5e73\uff08\u5e76\u884c\u72b6\u6001-\u52a8\u4f5c\u8f93\u5165\uff09\u548c\u5782\u76f4\uff08\u6df1\u5c42Transformer\u5806\u6808\uff09\u63a8\u7406\uff0c\u517c\u5bb9\u591a\u79cd\u57fa\u4e8e\u503c\u548c\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7684DRL\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReaCritic\u5728\u5f02\u6784\u7f51\u7edc\u548c\u6807\u51c6\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "ReaCritic\u901a\u8fc7\u5f15\u5165\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u589e\u5f3a\u4e86DRL\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11110", "pdf": "https://arxiv.org/pdf/2505.11110", "abs": "https://arxiv.org/abs/2505.11110", "authors": ["Massimiliano Cassia", "Luca Guarnera", "Mirko Casu", "Ignazio Zangara", "Sebastiano Battiato"], "title": "Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic media generated by Generative Adversarial Networks (GANs) pose\nsignificant challenges in verifying authenticity and tracing dataset origins,\nraising critical concerns in copyright enforcement, privacy protection, and\nlegal compliance. This paper introduces a novel forensic framework for\nidentifying the training dataset (e.g., CelebA or FFHQ) of GAN-generated images\nthrough interpretable feature analysis. By integrating spectral transforms\n(Fourier/DCT), color distribution metrics, and local feature descriptors\n(SIFT), our pipeline extracts discriminative statistical signatures embedded in\nsynthetic outputs. Supervised classifiers (Random Forest, SVM, XGBoost) achieve\n98-99% accuracy in binary classification (real vs. synthetic) and multi-class\ndataset attribution across diverse GAN architectures (StyleGAN, AttGAN, GDWCT,\nStarGAN, and StyleGAN2). Experimental results highlight the dominance of\nfrequency-domain features (DCT/FFT) in capturing dataset-specific artifacts,\nsuch as upsampling patterns and spectral irregularities, while color histograms\nreveal implicit regularization strategies in GAN training. We further examine\nlegal and ethical implications, showing how dataset attribution can address\ncopyright infringement, unauthorized use of personal data, and regulatory\ncompliance under frameworks like GDPR and California's AB 602. Our framework\nadvances accountability and governance in generative modeling, with\napplications in digital forensics, content moderation, and intellectual\nproperty litigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc6\u522bGAN\u751f\u6210\u56fe\u50cf\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u53d6\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u5206\u6790\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6cd5\u5f8b\u548c\u4f26\u7406\u5f71\u54cd\u3002", "motivation": "GAN\u751f\u6210\u7684\u5408\u6210\u5a92\u4f53\u5728\u771f\u5b9e\u6027\u9a8c\u8bc1\u548c\u6570\u636e\u6eaf\u6e90\u65b9\u9762\u5e26\u6765\u6311\u6218\uff0c\u6d89\u53ca\u7248\u6743\u3001\u9690\u79c1\u548c\u6cd5\u5f8b\u5408\u89c4\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u9891\u8c31\u53d8\u6362\u3001\u989c\u8272\u5206\u5e03\u5ea6\u91cf\u548c\u5c40\u90e8\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u63d0\u53d6\u5408\u6210\u56fe\u50cf\u4e2d\u7684\u7edf\u8ba1\u7279\u5f81\uff0c\u4f7f\u7528\u76d1\u7763\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u4e8c\u5143\u5206\u7c7b\u548c\u591a\u7c7b\u6570\u636e\u96c6\u5f52\u5c5e\u4efb\u52a1\u4e2d\u8fbe\u523098-99%\u7684\u51c6\u786e\u7387\uff0c\u9891\u7387\u57df\u7279\u5f81\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u95ee\u8d23\u548c\u6cbb\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u53d6\u8bc1\u3001\u5185\u5bb9\u5ba1\u6838\u548c\u77e5\u8bc6\u4ea7\u6743\u8bc9\u8bbc\u3002", "relevance": 40.0}}
{"id": "2505.11484", "pdf": "https://arxiv.org/pdf/2505.11484", "abs": "https://arxiv.org/abs/2505.11484", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "categories": ["cs.CL"], "comment": "14 pages", "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.", "AI": {"tldr": "SoftCoT++\u901a\u8fc7\u6270\u52a8\u6f5c\u5728\u601d\u7ef4\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6269\u5c55\u4e86SoftCoT\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\uff08\u5982SoftCoT\uff09\u7531\u4e8e\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\u9650\u5236\u4e86\u591a\u6837\u5316\u63a2\u7d22\uff0c\u800cSoftCoT++\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u591a\u4e2a\u4e13\u7528\u521d\u59cb\u4ee4\u724c\u6270\u52a8\u6f5c\u5728\u601d\u7ef4\uff0c\u5e76\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u4fc3\u8fdb\u8f6f\u601d\u7ef4\u8868\u793a\u7684\u591a\u6837\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u548c\u4e24\u79cdLLM\u67b6\u6784\u4e0a\uff0cSoftCoT++\u663e\u8457\u4f18\u4e8eSoftCoT\uff0c\u4e14\u4e0e\u4f20\u7edf\u6269\u5c55\u6280\u672f\uff08\u5982\u81ea\u4e00\u81f4\u6027\uff09\u517c\u5bb9\u3002", "conclusion": "SoftCoT++\u4e3a\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u63a2\u7d22\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.11017", "pdf": "https://arxiv.org/pdf/2505.11017", "abs": "https://arxiv.org/abs/2505.11017", "authors": ["Wenjie Ou", "Zhishuo Zhao", "Dongyue Guo", "Yi Lin"], "title": "Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting is critical across multiple domains, where time\nseries data exhibits both local patterns and global dependencies. While\nTransformer-based methods effectively capture global dependencies, they often\noverlook short-term local variations in time series. Recent methods that adapt\nlarge language models (LLMs) into time series forecasting inherit this\nlimitation by treating LLMs as black-box encoders, relying solely on the\nfinal-layer output and underutilizing hierarchical representations. To address\nthis limitation, we propose Logo-LLM, a novel LLM-based framework that\nexplicitly extracts and models multi-scale temporal features from different\nlayers of a pre-trained LLM. Through empirical analysis, we show that shallow\nlayers of LLMs capture local dynamics in time series, while deeper layers\nencode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and\nGlobal-Mixer modules to align and integrate features with the temporal input\nacross layers. Extensive experiments demonstrate that Logo-LLM achieves\nsuperior performance across diverse benchmarks, with strong generalization in\nfew-shot and zero-shot settings while maintaining low computational overhead.", "AI": {"tldr": "Logo-LLM\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u9884\u8bad\u7ec3LLM\u4e0d\u540c\u5c42\u6b21\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6a21\u5757\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6700\u7ec8\u5c42\u8f93\u51fa\uff0c\u5ffd\u7565\u4e86LLM\u5c42\u6b21\u5316\u8868\u793a\u4e2d\u7684\u5c40\u90e8\u52a8\u6001\u548c\u5168\u5c40\u8d8b\u52bf\u3002", "method": "Logo-LLM\u4ece\u9884\u8bad\u7ec3LLM\u7684\u4e0d\u540c\u5c42\u6b21\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u5f15\u5165Local-Mixer\u548cGlobal-Mixer\u6a21\u5757\u5bf9\u9f50\u548c\u6574\u5408\u8fd9\u4e9b\u7279\u5f81\u3002", "result": "Logo-LLM\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "Logo-LLM\u901a\u8fc7\u5145\u5206\u5229\u7528LLM\u7684\u5c42\u6b21\u5316\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.11121", "pdf": "https://arxiv.org/pdf/2505.11121", "abs": "https://arxiv.org/abs/2505.11121", "authors": ["Mathis J\u00fcrgen Adler", "Leonard Hackel", "Gencer Sumbul", "Beg\u00fcm Demir"], "title": "Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2025. Our code is available at\n  https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm", "summary": "The development of foundation models through pretraining of vision-language\nmodels (VLMs) has recently attracted great attention in remote sensing (RS).\nVLM pretraining aims to learn image and language alignments from a large number\nof image-text pairs. Each pretraining image is often associated with multiple\ncaptions containing redundant information due to repeated or semantically\nsimilar phrases, resulting in increased pretraining and inference time. To\novercome this, we introduce a weighted feature aggregation (WFA) strategy for\nVLM pretraining in RS. Our strategy aims to extract and exploit complementary\ninformation from multiple captions per image while reducing redundancies\nthrough feature aggregation with importance weighting. To calculate adaptive\nimportance weights for different captions of each image, we propose two\ntechniques: (i) non-parametric uniqueness and (ii) learning-based attention. In\nthe first technique, importance weights are calculated based on the bilingual\nevaluation understudy (BLEU) scores of the captions to emphasize unique\nsentences and reduce the influence of repetitive ones. In the second technique,\nimportance weights are learned through an attention mechanism instead of\nrelying on hand-crafted features. The effectiveness of the proposed WFA\nstrategy with the two techniques is analyzed in terms of downstream performance\non text-to-image retrieval in RS. Experimental results show that the proposed\nstrategy enables efficient and effective pretraining of VLMs in RS. Based on\nthe experimental analysis, we derive guidelines for selecting appropriate\ntechniques depending on downstream task requirements and resource constraints.\nThe code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u7279\u5f81\u805a\u5408\uff08WFA\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u9065\u611f\uff08RS\uff09\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9884\u8bad\u7ec3\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\u5e76\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3VLM\u9884\u8bad\u7ec3\u4e2d\u56e0\u591a\u6807\u9898\u5197\u4f59\u4fe1\u606f\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6280\u672f\uff1a\u57fa\u4e8eBLEU\u5206\u6570\u7684\u975e\u53c2\u6570\u552f\u4e00\u6027\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u8ba1\u7b97\u6807\u9898\u7684\u91cd\u8981\u6027\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWFA\u7b56\u7565\u80fd\u9ad8\u6548\u4e14\u6709\u6548\u5730\u9884\u8bad\u7ec3VLM\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u6839\u636e\u4efb\u52a1\u9700\u6c42\u548c\u8d44\u6e90\u9650\u5236\uff0c\u9009\u62e9\u5408\u9002\u7684\u6280\u672f\u53ef\u4f18\u5316VLM\u9884\u8bad\u7ec3\u6548\u679c\u3002", "relevance": 40.0}}
{"id": "2505.11485", "pdf": "https://arxiv.org/pdf/2505.11485", "abs": "https://arxiv.org/abs/2505.11485", "authors": ["Bruno Bianchi", "Ferm\u00edn Travi", "Juan E. Kamienkowski"], "title": "Modeling cognitive processes of natural reading with transformer-based Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08GPT2\u3001LLaMA-7B\u548cLLaMA2-7B\uff09\u5728\u89e3\u91ca\u897f\u73ed\u7259\u8bed\u8bfb\u8005\u773c\u52a8\u884c\u4e3a\uff08\u51dd\u89c6\u65f6\u95f4\uff09\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u4f18\u4e8e\u65e9\u671f\u6a21\u578b\uff0c\u4f46\u4ecd\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u4eba\u7c7b\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\uff08\u5982Transformer\uff09\u662f\u5426\u80fd\u66f4\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u773c\u52a8\u884c\u4e3a\u3002", "method": "\u4f7f\u7528GPT2\u3001LLaMA-7B\u548cLLaMA2-7B\u6a21\u578b\uff0c\u5206\u6790\u5176\u9884\u6d4b\u80fd\u529b\u4e0e\u4eba\u7c7b\u773c\u52a8\u6570\u636e\uff08\u51dd\u89c6\u65f6\u95f4\uff09\u7684\u5173\u7cfb\u3002", "result": "Transformer\u6a21\u578b\u4f18\u4e8e\u65e9\u671f\u6a21\u578b\uff0c\u4f46\u4ecd\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u4eba\u7c7b\u9884\u6d4b\u80fd\u529b\uff0c\u8868\u660e\u5176\u9884\u6d4b\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "conclusion": "\u5c3d\u7ba1\u6280\u672f\u8fdb\u6b65\uff0c\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u6027\u3002", "relevance": 60.0}}
{"id": "2505.11023", "pdf": "https://arxiv.org/pdf/2505.11023", "abs": "https://arxiv.org/abs/2505.11023", "authors": ["Kutalm\u0131\u015f Co\u015fkun", "Ivo Kavisanczki", "Amin Mirzaei", "Tom Siegl", "Bjarne C. Hiller", "Stefan L\u00fcdtke", "Martin Becker"], "title": "Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs", "categories": ["cs.LG"], "comment": "10 pages, 7 figures", "summary": "In complex and low-data domains such as biomedical research, incorporating\nbackground knowledge (BK) graphs, such as protein-protein interaction (PPI)\nnetworks, into graph-based machine learning pipelines is a promising research\ndirection. However, while BK is often assumed to improve model performance, its\nactual contribution and the impact of imperfect knowledge remain poorly\nunderstood. In this work, we investigate the role of BK in an important\nreal-world task: cancer subtype classification. Surprisingly, we find that (i)\nstate-of-the-art GNNs using BK perform no better than uninformed models like\nlinear regression, and (ii) their performance remains largely unchanged even\nwhen the BK graph is heavily perturbed. To understand these unexpected results,\nwe introduce an evaluation framework, which employs (i) a synthetic setting\nwhere the BK is clearly informative and (ii) a set of perturbations that\nsimulate various imperfections in BK graphs. With this, we test the robustness\nof BK-aware models in both synthetic and real-world biomedical settings. Our\nfindings reveal that careful alignment of GNN architectures and BK\ncharacteristics is necessary but holds the potential for significant\nperformance improvements.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u80cc\u666f\u77e5\u8bc6\uff08BK\uff09\u56fe\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u56fe\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u73b0\u6709GNN\u6a21\u578b\u4f7f\u7528BK\u5e76\u672a\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u6a21\u578b\uff0c\u4e14\u5bf9BK\u56fe\u7684\u6270\u52a8\u4e0d\u654f\u611f\u3002", "motivation": "\u63a2\u7a76BK\u56fe\u5728\u5b9e\u9645\u4efb\u52a1\uff08\u5982\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\uff09\u4e2d\u7684\u8d21\u732e\u53ca\u5176\u4e0d\u5b8c\u7f8e\u77e5\u8bc6\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u5408\u6210\u8bbe\u7f6e\u548cBK\u56fe\u6270\u52a8\u6d4b\u8bd5\uff0c\u5206\u6790GNN\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0BK\u611f\u77e5\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u4e8eGNN\u67b6\u6784\u4e0eBK\u7279\u6027\u7684\u7cbe\u7ec6\u5bf9\u9f50\u3002", "conclusion": "BK\u7684\u6709\u6548\u5229\u7528\u9700\u8981\u6a21\u578b\u8bbe\u8ba1\u4e0eBK\u7279\u6027\u7684\u5339\u914d\uff0c\u4f46\u5177\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11129", "pdf": "https://arxiv.org/pdf/2505.11129", "abs": "https://arxiv.org/abs/2505.11129", "authors": ["Makoto Yamada", "Kian Ming A. Chai", "Ayoub Rhim", "Satoki Ishikawa", "Mohammad Sabokrou", "Yao-Hung Hubert Tsai"], "title": "PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.14650", "summary": "Recent advances in self-supervised learning (SSL) have revolutionized\ncomputer vision through innovative architectures and learning objectives, yet\nthey have not fully leveraged insights from biological visual processing\nsystems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is\nbased on a ResNet backbone and operates on static image inputs with strong\naugmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based\narchitecture that processes temporal visual input (that is, sequences of\nimages) without relying on strong augmentation. Our model leverages variational\ninference to learn robust visual representations from continuous input streams,\nsimilar to human visual processing. Through extensive experimentation, we\ndemonstrate that PhiNet v2 achieves competitive performance compared to\nstate-of-the-art vision foundation models, while maintaining the ability to\nlearn from sequential input without strong data augmentation. This work\nrepresents a significant step toward more biologically plausible computer\nvision systems that process visual information in a manner more closely aligned\nwith human cognitive processes.", "AI": {"tldr": "PhiNet v2\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5904\u7406\u65f6\u5e8f\u89c6\u89c9\u8f93\u5165\uff0c\u65e0\u9700\u5f3a\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u5b66\u4e60\u9c81\u68d2\u89c6\u89c9\u8868\u793a\u3002", "motivation": "\u63a2\u7d22\u66f4\u63a5\u8fd1\u4eba\u7c7b\u89c6\u89c9\u5904\u7406\u7684\u751f\u7269\u542f\u53d1\u5f0f\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u5f3a\u6570\u636e\u589e\u5f3a\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faPhiNet v2\uff0c\u57fa\u4e8eTransformer\u5904\u7406\u56fe\u50cf\u5e8f\u5217\uff0c\u5229\u7528\u53d8\u5206\u63a8\u7406\u5b66\u4e60\u8868\u793a\u3002", "result": "PhiNet v2\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4e14\u80fd\u4ece\u65f6\u5e8f\u8f93\u5165\u4e2d\u5b66\u4e60\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u7684\u751f\u7269\u542f\u53d1\u5f0f\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u3002", "relevance": 40.0}}
{"id": "2505.11024", "pdf": "https://arxiv.org/pdf/2505.11024", "abs": "https://arxiv.org/abs/2505.11024", "authors": ["Wolfgang Rannetbauer", "Simon Hubmer", "Carina Hambrock", "Ronny Ramlau"], "title": "Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "29 pages, 7 figures", "summary": "The implementation of thermally sprayed components in steel manufacturing\npresents challenges for production and plant maintenance. While enhancing\nperformance through specialized surface properties, these components may\nencounter difficulties in meeting modified requirements due to standardization\nin the refurbishment process. This article proposes updating the established\ncoating process for thermally spray coated components for steel manufacturing\n(TCCSM) by integrating real-time data analytics and predictive quality\nmanagement. Two essential components--the data aggregator and the quality\npredictor--are designed through continuous process monitoring and the\napplication of data-driven methodologies to meet the dynamic demands of the\nevolving steel landscape. The quality predictor is powered by the simple and\neffective multiple kernel learning strategy with the goal of realizing\npredictive quality. The data aggregator, designed with sensors, flow meters,\nand intelligent data processing for the thermal spray coating process, is\nproposed to facilitate real-time analytics. The performance of this combination\nwas verified using small-scale tests that enabled not only the accurate\nprediction of coating quality based on the collected data but also proactive\nnotification to the operator as soon as significant deviations are identified.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u5206\u6790\u548c\u9884\u6d4b\u8d28\u91cf\u7ba1\u7406\u66f4\u65b0\u70ed\u55b7\u6d82\u6d82\u5c42\u5de5\u827a\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u94a2\u94c1\u5236\u9020\u4e2d\u7684\u52a8\u6001\u9700\u6c42\u3002", "motivation": "\u70ed\u55b7\u6d82\u7ec4\u4ef6\u5728\u94a2\u94c1\u5236\u9020\u4e2d\u9762\u4e34\u751f\u4ea7\u548c\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u6807\u51c6\u5316\u4fee\u590d\u8fc7\u7a0b\u96be\u4ee5\u6ee1\u8db3\u52a8\u6001\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u6570\u636e\u805a\u5408\u5668\u548c\u8d28\u91cf\u9884\u6d4b\u5668\uff0c\u91c7\u7528\u591a\u6838\u5b66\u4e60\u7b56\u7565\u548c\u4f20\u611f\u5668\u6570\u636e\u5b9e\u65f6\u5206\u6790\u3002", "result": "\u5c0f\u89c4\u6a21\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u51c6\u786e\u9884\u6d4b\u6d82\u5c42\u8d28\u91cf\u5e76\u4e3b\u52a8\u901a\u77e5\u64cd\u4f5c\u5458\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u94a2\u94c1\u5236\u9020\u4e2d\u7684\u70ed\u55b7\u6d82\u5de5\u827a\u63d0\u4f9b\u4e86\u52a8\u6001\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 10.0}}
{"id": "2505.11131", "pdf": "https://arxiv.org/pdf/2505.11131", "abs": "https://arxiv.org/abs/2505.11131", "authors": ["Feiran Li", "Qianqian Xu", "Shilong Bao", "Zhiyong Yang", "Xiaochun Cao", "Qingming Huang"], "title": "One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepeted to ICML 2025. Not Final Version", "summary": "Concept erasing has recently emerged as an effective paradigm to prevent\ntext-to-image diffusion models from generating visually undesirable or even\nharmful content. However, current removal methods heavily rely on manually\ncrafted text prompts, making it challenging to achieve a high erasure\n(efficacy) while minimizing the impact on other benign concepts (usability). In\nthis paper, we attribute the limitations to the inherent gap between the text\nand image modalities, which makes it hard to transfer the intricately entangled\nconcept knowledge from text prompts to the image generation process. To address\nthis, we propose a novel solution by directly integrating visual supervision\ninto the erasure process, introducing the first text-image Collaborative\nConcept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the\nconcept jointly by text prompts and the corresponding undesirable images\ninduced by the prompts, and then reduces the generating probability of the\ntarget concept through negative guidance. This approach effectively bypasses\nthe knowledge gap between text and image, significantly enhancing erasure\nefficacy. Additionally, we design a text-guided image concept refinement\nstrategy that directs the model to focus on visual features most relevant to\nthe specified text concept, minimizing disruption to other benign concepts.\nFinally, comprehensive experiments suggest that Co-Erasing outperforms\nstate-of-the-art erasure approaches significantly with a better trade-off\nbetween efficacy and usability. Codes are available at\nhttps://github.com/Ferry-Li/Co-Erasing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u672c-\u56fe\u50cf\u534f\u540c\u6982\u5ff5\u64e6\u9664\uff08Co-Erasing\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u76d1\u7763\u63d0\u5347\u6982\u5ff5\u64e6\u9664\u6548\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u5176\u4ed6\u826f\u6027\u6982\u5ff5\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u6587\u672c\u63d0\u793a\uff0c\u96be\u4ee5\u5728\u64e6\u9664\u6548\u679c\u548c\u53ef\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6587\u672c\u4e0e\u56fe\u50cf\u6a21\u6001\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u6587\u672c\u63d0\u793a\u548c\u5bf9\u5e94\u4e0d\u826f\u56fe\u50cf\u7684\u8054\u5408\u63cf\u8ff0\uff0c\u5f15\u5165\u8d1f\u5411\u6307\u5bfc\u964d\u4f4e\u76ee\u6807\u6982\u5ff5\u7684\u751f\u6210\u6982\u7387\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u6982\u5ff5\u7ec6\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCo-Erasing\u5728\u64e6\u9664\u6548\u679c\u548c\u53ef\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Co-Erasing\u901a\u8fc7\u89c6\u89c9\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u4e0e\u56fe\u50cf\u6a21\u6001\u95f4\u7684\u77e5\u8bc6\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6982\u5ff5\u64e6\u9664\u7684\u6548\u679c\u3002", "relevance": 60.0}}
{"id": "2505.10586", "pdf": "https://arxiv.org/pdf/2505.10586", "abs": "https://arxiv.org/abs/2505.10586", "authors": ["Poli A. Nemkova", "Suleyman O. Polat", "Rafid I. Jahan", "Sagnik Ray Choudhury", "Sun-joo Lee", "Shouryadipta Sarkar", "Mark V. Albert"], "title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Timely and accurate situation awareness is vital for decision-making in\nhumanitarian response, conflict monitoring, and early warning and early action.\nHowever, the manual analysis of vast and heterogeneous data sources often\nresults in delays, limiting the effectiveness of interventions. This paper\nintroduces a dynamic Retrieval-Augmented Generation (RAG) system that\nautonomously generates situation awareness reports by integrating real-time\ndata from diverse sources, including news articles, conflict event databases,\nand economic indicators. Our system constructs query-specific knowledge bases\non demand, ensuring timely, relevant, and accurate insights.\n  To ensure the quality of generated reports, we propose a three-level\nevaluation framework that combines semantic similarity metrics, factual\nconsistency checks, and expert feedback. The first level employs automated NLP\nmetrics to assess coherence and factual accuracy. The second level involves\nhuman expert evaluation to verify the relevance and completeness of the\nreports. The third level utilizes LLM-as-a-Judge, where large language models\nprovide an additional layer of assessment to ensure robustness. The system is\ntested across multiple real-world scenarios, demonstrating its effectiveness in\nproducing coherent, insightful, and actionable reports. By automating report\ngeneration, our approach reduces the burden on human analysts and accelerates\ndecision-making processes. To promote reproducibility and further research, we\nopenly share our code and evaluation tools with the community via GitHub.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b9e\u65f6\u60c5\u5883\u611f\u77e5\u62a5\u544a\uff0c\u7ed3\u5408\u591a\u6e90\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e09\u7ea7\u8bc4\u4f30\u6846\u67b6\u786e\u4fdd\u62a5\u544a\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u5206\u6790\u6d77\u91cf\u5f02\u6784\u6570\u636e\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u51b3\u7b56\u6548\u7387\u3002", "method": "\u52a8\u6001RAG\u7cfb\u7edf\u6574\u5408\u5b9e\u65f6\u6570\u636e\uff0c\u6784\u5efa\u67e5\u8be2\u7279\u5b9a\u77e5\u8bc6\u5e93\uff1b\u4e09\u7ea7\u8bc4\u4f30\u6846\u67b6\uff08\u81ea\u52a8NLP\u6307\u6807\u3001\u4e13\u5bb6\u53cd\u9988\u3001LLM\u8bc4\u4f30\uff09\u786e\u4fdd\u62a5\u544a\u8d28\u91cf\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u4e2a\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u751f\u6210\u8fde\u8d2f\u3001\u6709\u6d1e\u5bdf\u529b\u4e14\u53ef\u64cd\u4f5c\u7684\u62a5\u544a\u3002", "conclusion": "\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u51cf\u8f7b\u4eba\u5de5\u8d1f\u62c5\uff0c\u52a0\u901f\u51b3\u7b56\uff1b\u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.10609", "pdf": "https://arxiv.org/pdf/2505.10609", "abs": "https://arxiv.org/abs/2505.10609", "authors": ["Ken Huang", "Vineeth Sai Narajala", "Idan Habler", "Akram Sheriff"], "title": "Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability", "categories": ["cs.CR", "cs.AI", "cs.MA", "cs.NI"], "comment": "15 pages, 6 figures, 6 code listings, Supported and endorsed by OWASP\n  GenAI ASI Project", "summary": "The proliferation of AI agents requires robust mechanisms for secure\ndiscovery. This paper introduces the Agent Name Service (ANS), a novel\narchitecture based on DNS addressing the lack of a public agent discovery\nframework. ANS provides a protocol-agnostic registry infrastructure that\nleverages Public Key Infrastructure (PKI) certificates for verifiable agent\nidentity and trust. The architecture features several key innovations: a\nformalized agent registration and renewal mechanism for lifecycle management;\nDNS-inspired naming conventions with capability-aware resolution; a modular\nProtocol Adapter Layer supporting diverse communication standards (A2A, MCP,\nACP etc.); and precisely defined algorithms for secure resolution. We implement\nstructured communication using JSON Schema and conduct a comprehensive threat\nanalysis of our proposal. The result is a foundational directory service\naddressing the core challenges of secured discovery and interaction in\nmulti-agent systems, paving the way for future interoperable, trustworthy, and\nscalable agent ecosystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDNS\u7684\u65b0\u578b\u67b6\u6784Agent Name Service (ANS)\uff0c\u7528\u4e8e\u89e3\u51b3AI\u4ee3\u7406\u7684\u5b89\u5168\u53d1\u73b0\u548c\u4ea4\u4e92\u95ee\u9898\uff0c\u652f\u6301\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u516c\u5171\u7684\u4ee3\u7406\u53d1\u73b0\u6846\u67b6\uff0c\u5bfc\u81f4\u5b89\u5168\u548c\u4fe1\u4efb\u95ee\u9898\u3002ANS\u65e8\u5728\u901a\u8fc7PKI\u8bc1\u4e66\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ANS\u91c7\u7528DNS\u542f\u53d1\u7684\u547d\u540d\u7ea6\u5b9a\u3001\u534f\u8bae\u9002\u914d\u5c42\u548cJSON Schema\u7ed3\u6784\u5316\u901a\u4fe1\uff0c\u5b9e\u73b0\u4ee3\u7406\u6ce8\u518c\u3001\u751f\u547d\u5468\u671f\u7ba1\u7406\u548c\u5b89\u5168\u89e3\u6790\u3002", "result": "ANS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u4ee3\u7406\u8eab\u4efd\u548c\u4fe1\u4efb\u7684\u76ee\u5f55\u670d\u52a1\uff0c\u652f\u6301\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u53d1\u73b0\u548c\u4ea4\u4e92\u3002", "conclusion": "ANS\u4e3a\u672a\u6765\u7684\u53ef\u4e92\u64cd\u4f5c\u3001\u53ef\u4fe1\u8d56\u548c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.11029", "pdf": "https://arxiv.org/pdf/2505.11029", "abs": "https://arxiv.org/abs/2505.11029", "authors": ["Li Ju", "Max Andersson", "Stina Fredriksson", "Edward Gl\u00f6ckner", "Andreas Hellander", "Ekta Vats", "Prashant Singh"], "title": "Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere", "categories": ["cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) as foundation models have significantly\nenhanced performance across a wide range of visual and textual tasks, without\nrequiring large-scale training from scratch for downstream tasks. However,\nthese deterministic VLMs fail to capture the inherent ambiguity and uncertainty\nin natural language and visual data. Recent probabilistic post-hoc adaptation\nmethods address this by mapping deterministic embeddings onto probability\ndistributions; however, existing approaches do not account for the asymmetric\nuncertainty structure of the modalities, and the constraint that meaningful\ndeterministic embeddings reside on a unit hypersphere, potentially leading to\nsuboptimal performance. In this paper, we address the asymmetric uncertainty\nstructure inherent in textual and visual data, and propose AsymVLM to build\nprobabilistic embeddings from pre-trained VLMs on the unit hypersphere,\nenabling uncertainty quantification. We validate the effectiveness of the\nprobabilistic embeddings on established benchmarks, and present comprehensive\nablation studies demonstrating the inherent nature of asymmetry in the\nuncertainty structure of textual and visual data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAsymVLM\uff0c\u4e00\u79cd\u5728\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u57fa\u7840\u4e0a\u6784\u5efa\u6982\u7387\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6a21\u6001\u95f4\u4e0d\u5bf9\u79f0\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u73b0\u6709\u786e\u5b9a\u6027VLM\u65e0\u6cd5\u6355\u6349\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u73b0\u6709\u6982\u7387\u540e\u9002\u5e94\u65b9\u6cd5\u672a\u8003\u8651\u6a21\u6001\u95f4\u4e0d\u5bf9\u79f0\u6027\u53ca\u5355\u4f4d\u8d85\u7403\u9762\u7ea6\u675f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faAsymVLM\uff0c\u5728\u9884\u8bad\u7ec3VLM\u57fa\u7840\u4e0a\u6784\u5efa\u6982\u7387\u5d4c\u5165\uff0c\u8003\u8651\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u7684\u4e0d\u5bf9\u79f0\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\uff0c\u5e76\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6982\u7387\u5d4c\u5165\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u4e86\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "conclusion": "AsymVLM\u80fd\u591f\u6709\u6548\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u6a21\u6001\u95f4\u4e0d\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.11141", "pdf": "https://arxiv.org/pdf/2505.11141", "abs": "https://arxiv.org/abs/2505.11141", "authors": ["Yansheng Qiu", "Li Xiao", "Zhaopan Xu", "Pengfei Zhou", "Zheng Wang", "Kaipeng Zhang"], "title": "Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of achieving Artificial General Intelligence (AGI) is to imitate\nhumans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have\ndemonstrated that large language models (LLMs) with human-like reasoning\ncapabilities exhibit exceptional performance and are being gradually integrated\ninto multimodal large language models (MLLMs). However, whether these models\npossess capabilities comparable to humans in handling reasoning tasks remains\nunclear at present. In this paper, we propose Human-Aligned Bench, a benchmark\nfor fine-grained alignment of multimodal reasoning with human performance.\nSpecifically, we collected 9,794 multimodal questions that solely rely on\ncontextual reasoning, including bilingual (Chinese and English) multimodal\nquestions and pure text-based questions, encompassing four question types:\nvisual reasoning, definition judgment, analogical reasoning, and logical\njudgment. More importantly, each question is accompanied by human success rates\nand options that humans are prone to choosing incorrectly. Extensive\nexperiments on the Human-Aligned Bench reveal notable differences between the\nperformance of current MLLMs in multimodal reasoning and human performance. The\nfindings on our benchmark provide insights into the development of the\nnext-generation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHuman-Aligned Bench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u80fd\u529b\u7684\u5dee\u8ddd\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6536\u96c6\u4e869,794\u4e2a\u591a\u6a21\u6001\u95ee\u9898\uff0c\u6db5\u76d6\u56db\u79cd\u95ee\u9898\u7c7b\u578b\uff0c\u5e76\u6807\u6ce8\u4eba\u7c7b\u6210\u529f\u7387\u53ca\u6613\u9519\u9009\u9879\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dMLLMs\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u4e0b\u4e00\u4ee3\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.10640", "pdf": "https://arxiv.org/pdf/2505.10640", "abs": "https://arxiv.org/abs/2505.10640", "authors": ["Kirill Vasilevski", "Benjamin Rombaut", "Gopi Krishnan Rajbahadur", "Gustavo A. Oliva", "Keheliya Gallaba", "Filipe R. Cogo", "Jiahuei", "Lin", "Dayi Lin", "Haoxiang Zhang", "Bouyan Chen", "Kishanthan Thangarajah", "Ahmed E. Hassan", "Zhen Ming", "Jiang"], "title": "The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware)", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Foundation Models (FMs) such as Large Language Models (LLMs) are reshaping\nthe software industry by enabling FMware, systems that integrate these FMs as\ncore components. In this KDD 2025 tutorial, we present a comprehensive\nexploration of FMware that combines a curated catalogue of challenges with\nreal-world production concerns. We first discuss the state of research and\npractice in building FMware. We further examine the difficulties in selecting\nsuitable models, aligning high-quality domain-specific data, engineering robust\nprompts, and orchestrating autonomous agents. We then address the complex\njourney from impressive demos to production-ready systems by outlining issues\nin system testing, optimization, deployment, and integration with legacy\nsoftware. Drawing on our industrial experience and recent research in the area,\nwe provide actionable insights and a technology roadmap for overcoming these\nchallenges. Attendees will gain practical strategies to enable the creation of\ntrustworthy FMware in the evolving technology landscape.", "AI": {"tldr": "\u8be5\u6559\u7a0b\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u57fa\u7840\u6a21\u578b\uff08\u5982LLMs\uff09\u6574\u5408\u4e3a\u8f6f\u4ef6\u6838\u5fc3\u7ec4\u4ef6\uff08FMware\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ece\u7814\u7a76\u5230\u751f\u4ea7\u5b9e\u8df5\u7684\u5168\u9762\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u5728\u8f6f\u4ef6\u884c\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u5c06\u5176\u6709\u6548\u6574\u5408\u4e3a\u751f\u4ea7\u7ea7\u7cfb\u7edf\uff08FMware\uff09\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u6a21\u578b\u9009\u62e9\u3001\u6570\u636e\u5bf9\u9f50\u3001\u63d0\u793a\u5de5\u7a0b\u7b49\u3002", "method": "\u7ed3\u5408\u5de5\u4e1a\u7ecf\u9a8c\u548c\u6700\u65b0\u7814\u7a76\uff0c\u6559\u7a0b\u63d0\u4f9b\u4e86FMware\u7684\u6311\u6218\u76ee\u5f55\u3001\u6280\u672f\u8def\u7ebf\u56fe\u548c\u5b9e\u7528\u7b56\u7565\u3002", "result": "\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u7684FMware\u63d0\u4f9b\u4e86\u4ece\u7814\u7a76\u5230\u751f\u4ea7\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6559\u7a0b\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u514b\u670dFMware\u6311\u6218\u7684\u5b9e\u7528\u5de5\u5177\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.11035", "pdf": "https://arxiv.org/pdf/2505.11035", "abs": "https://arxiv.org/abs/2505.11035", "authors": ["Kihun Hong", "Sejun Park", "Ganguk Hwang"], "title": "Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios", "categories": ["cs.LG"], "comment": "9 pages + appendix, 8 figures, 18 tables", "summary": "Federated learning (FL) has attracted significant attention for enabling\ncollaborative learning without exposing private data. Among the primary\nvariants of FL, vertical federated learning (VFL) addresses feature-partitioned\ndata held by multiple institutions, each holding complementary information for\nthe same set of users. However, existing VFL methods often impose restrictive\nassumptions such as a small number of participating parties, fully aligned\ndata, or only using labeled data. In this work, we reinterpret alignment gaps\nin VFL as missing data problems and propose a unified framework that\naccommodates both training and inference under arbitrary alignment and labeling\nscenarios, while supporting diverse missingness mechanisms. In the experiments\non 168 configurations spanning four benchmark datasets, six training-time\nmissingness patterns, and seven testing-time missingness patterns, our method\noutperforms all baselines in 160 cases with an average gap of 9.6 percentage\npoints over the next-best competitors. To the best of our knowledge, this is\nthe first VFL framework to jointly handle arbitrary data alignment, unlabeled\ndata, and multi-party collaboration all at once.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5bf9\u9f50\u3001\u672a\u6807\u8bb0\u6570\u636e\u548c\u591a\u673a\u6784\u534f\u4f5c\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709VFL\u65b9\u6cd5\u5bf9\u6570\u636e\u5bf9\u9f50\u3001\u6807\u8bb0\u6570\u636e\u548c\u53c2\u4e0e\u65b9\u6570\u91cf\u6709\u4e25\u683c\u9650\u5236\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06VFL\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u91cd\u65b0\u89e3\u91ca\u4e3a\u7f3a\u5931\u6570\u636e\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u5bf9\u9f50\u548c\u6807\u8bb0\u573a\u666f\u3002", "result": "\u5728168\u79cd\u5b9e\u9a8c\u914d\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728160\u79cd\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e73\u5747\u9886\u51489.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u80fd\u540c\u65f6\u5904\u7406\u4efb\u610f\u6570\u636e\u5bf9\u9f50\u3001\u672a\u6807\u8bb0\u6570\u636e\u548c\u591a\u673a\u6784\u534f\u4f5c\u7684VFL\u6846\u67b6\u3002", "relevance": 30.0}}
{"id": "2505.11152", "pdf": "https://arxiv.org/pdf/2505.11152", "abs": "https://arxiv.org/abs/2505.11152", "authors": ["Daniel Sungho Jung", "Kyoung Mu Lee"], "title": "Learning Dense Hand Contact Estimation from Imbalanced Data", "categories": ["cs.CV"], "comment": "Project page: http://haco-release.github.io", "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u624b\u90e8\u63a5\u89e6\u4f30\u8ba1\u4e2d\u7c7b\u522b\u548c\u7a7a\u95f4\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u6846\u67b6HACO\uff0c\u901a\u8fc7\u5e73\u8861\u63a5\u89e6\u91c7\u6837\u548c\u9876\u70b9\u7ea7\u7c7b\u522b\u5e73\u8861\u635f\u5931\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u624b\u90e8\u63a5\u89e6\u7406\u89e3\u5bf9\u4eba\u7c7b\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u7c7b\u522b\u548c\u7a7a\u95f4\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u5e73\u8861\u63a5\u89e6\u91c7\u6837\u548c\u9876\u70b9\u7ea7\u7c7b\u522b\u5e73\u8861\u635f\u5931\uff08VCB\uff09\u6765\u89e3\u51b3\u7c7b\u522b\u548c\u7a7a\u95f4\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6709\u6548\u5b66\u4e60\u4e86\u5927\u89c4\u6a21\u624b\u90e8\u63a5\u89e6\u6570\u636e\u7684\u5bc6\u96c6\u4f30\u8ba1\uff0c\u907f\u514d\u4e86\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "HACO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u624b\u90e8\u63a5\u89e6\u4f30\u8ba1\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002", "relevance": 10.0}}
{"id": "2505.11040", "pdf": "https://arxiv.org/pdf/2505.11040", "abs": "https://arxiv.org/abs/2505.11040", "authors": ["Zhexiang Li", "Haoyu Wang", "Yutong Bao", "David Woodruff"], "title": "Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in transformer architectures deeply enhance long-context\nlanguage modeling. Among them, HyperAttention achieves competitive efficiency\nby combining a single-level LSH-based clustering with uniform residual\nsampling. However,such a sampling limits crucial keys' capturing, which in turn\nraises the overall perplexity. In this paper, we propose a pre-scoring\nmechanism to assist HyperAttention to prioritize significant keys.\nSpecifically, we introduce three scoring methods: K-means clustering, K-median\nclustering, and leverage score-based ranking (inspired by LevAttention) to\nfilter keys effectively. We further replace HyperAttention's original uniform\nresidual sampling entirely, relying exclusively on our pre-scoring mechanism.\nExperiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,\nwhich outperforms standard HyperAttention. Moreover, when running on the\nVision-Transformer (ViT), our method shows that it can guarantee similar\naccuracy compared with LevAttention, and will surpass LevAttention given\nspecific parameters. Although this method introduces computational overhead,\nits combination with HyperAttention remains 20 times faster than\nFlashAttention, providing a balanced trade-off between speed and modeling\naccuracy. Our results highlight the effectiveness of integrating pre-scoring\ninto hierarchical attention mechanisms, significantly improving Transformer's\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bc4\u5206\u673a\u5236\uff0c\u6539\u8fdbHyperAttention\u5bf9\u5173\u952e\u952e\u7684\u6355\u83b7\u80fd\u529b\uff0c\u901a\u8fc7K-means\u3001K-median\u805a\u7c7b\u548c\u6760\u6746\u8bc4\u5206\u6392\u5e8f\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\uff0c\u5e76\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "HyperAttention\u7684\u5747\u5300\u6b8b\u5dee\u91c7\u6837\u9650\u5236\u4e86\u5173\u952e\u952e\u7684\u6355\u83b7\uff0c\u5bfc\u81f4\u56f0\u60d1\u5ea6\u8f83\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9884\u8bc4\u5206\u673a\u5236\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u9884\u8bc4\u5206\u65b9\u6cd5\uff08K-means\u3001K-median\u805a\u7c7b\u548c\u6760\u6746\u8bc4\u5206\u6392\u5e8f\uff09\uff0c\u5b8c\u5168\u66ff\u4ee3HyperAttention\u7684\u5747\u5300\u6b8b\u5dee\u91c7\u6837\u3002", "result": "\u5728ChatGLM2\uff08131k token\u4e0a\u4e0b\u6587\uff09\u4e0a\uff0c\u56f0\u60d1\u5ea6\u4ece12\u964d\u81f38.3\uff1b\u5728ViT\u4e0a\uff0c\u4e0eLevAttention\u76f8\u6bd4\uff0c\u7279\u5b9a\u53c2\u6570\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u9884\u8bc4\u5206\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u7684\u6548\u7387\uff0c\u4e3aTransformer\u6a21\u578b\u63d0\u4f9b\u4e86\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "relevance": 85.0}}
{"id": "2505.11168", "pdf": "https://arxiv.org/pdf/2505.11168", "abs": "https://arxiv.org/abs/2505.11168", "authors": ["Xinran Li", "Yu Liu", "Xiujuan Xu", "Xiaowei Zhao"], "title": "CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer", "categories": ["cs.CV", "cs.AI"], "comment": "BIBM", "summary": "The automatic diagnosis of chest diseases is a popular and challenging task.\nMost current methods are based on convolutional neural networks (CNNs), which\nfocus on local features while neglecting global features. Recently,\nself-attention mechanisms have been introduced into the field of computer\nvision, demonstrating superior performance. Therefore, this paper proposes an\neffective model, CheX-DS, for classifying long-tail multi-label data in the\nmedical field of chest X-rays. The model is based on the excellent CNN model\nDenseNet for medical imaging and the newly popular Swin Transformer model,\nutilizing ensemble deep learning techniques to combine the two models and\nleverage the advantages of both CNNs and Transformers. The loss function of\nCheX-DS combines weighted binary cross-entropy loss with asymmetric loss,\neffectively addressing the issue of data imbalance. The NIH ChestX-ray14\ndataset is selected to evaluate the model's effectiveness. The model\noutperforms previous studies with an excellent average AUC score of 83.76\\%,\ndemonstrating its superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cTransformer\u7684\u6a21\u578bCheX-DS\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728NIH ChestX-ray14\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u5728\u80f8\u90e8\u75be\u75c5\u81ea\u52a8\u8bca\u65ad\u4e2d\u5ffd\u7565\u4e86\u5168\u5c40\u7279\u5f81\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u6b64\u7ed3\u5408CNN\u548cTransformer\u7684\u4f18\u52bf\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u57fa\u4e8eDenseNet\u548cSwin Transformer\uff0c\u91c7\u7528\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7ed3\u5408\u4e24\u79cd\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u52a0\u6743\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u975e\u5bf9\u79f0\u635f\u5931\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728NIH ChestX-ray14\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5e73\u5747AUC\u8fbe\u523083.76%\uff0c\u4f18\u4e8e\u5148\u524d\u7814\u7a76\u3002", "conclusion": "CheX-DS\u6a21\u578b\u901a\u8fc7\u7ed3\u5408CNN\u548cTransformer\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80f8\u90e8X\u5149\u7247\u5206\u7c7b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.11044", "pdf": "https://arxiv.org/pdf/2505.11044", "abs": "https://arxiv.org/abs/2505.11044", "authors": ["Zhirui Fang", "Kai Yang", "Jian Tao", "Jiafei Lyu", "Lusong Li", "Li Shen", "Xiu Li"], "title": "Exploration by Random Distribution Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Exploration remains a critical challenge in online reinforcement learning, as\nan agent must effectively explore unknown environments to achieve high returns.\nCurrently, the main exploration algorithms are primarily count-based methods\nand curiosity-based methods, with prediction-error methods being a prominent\nexample. In this paper, we propose a novel method called \\textbf{R}andom\n\\textbf{D}istribution \\textbf{D}istillation (RDD), which samples the output of\na target network from a normal distribution. RDD facilitates a more extensive\nexploration by explicitly treating the difference between the prediction\nnetwork and the target network as an intrinsic reward. Furthermore, by\nintroducing randomness into the output of the target network for a given state\nand modeling it as a sample from a normal distribution, intrinsic rewards are\nbounded by two key components: a pseudo-count term ensuring proper exploration\ndecay and a discrepancy term accounting for predictor convergence. We\ndemonstrate that RDD effectively unifies both count-based and prediction-error\napproaches. It retains the advantages of prediction-error methods in\nhigh-dimensional spaces, while also implementing an intrinsic reward decay mode\nakin to the pseudo-count method. In the experimental section, RDD is compared\nwith more advanced methods in a series of environments. Both theoretical\nanalysis and experimental results confirm the effectiveness of our approach in\nimproving online exploration for reinforcement learning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRDD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u7f51\u7edc\u7684\u8f93\u51fa\u5efa\u6a21\u4e3a\u6b63\u6001\u5206\u5e03\u6837\u672c\uff0c\u7edf\u4e00\u4e86\u57fa\u4e8e\u8ba1\u6570\u548c\u9884\u6d4b\u8bef\u5dee\u7684\u63a2\u7d22\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u63a2\u7d22\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u8ba1\u6570\u548c\u9884\u6d4b\u8bef\u5dee\u7684\u65b9\u6cd5\uff09\u5404\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "RDD\u65b9\u6cd5\u901a\u8fc7\u91c7\u6837\u76ee\u6807\u7f51\u7edc\u7684\u8f93\u51fa\u4f5c\u4e3a\u6b63\u6001\u5206\u5e03\u6837\u672c\uff0c\u5c06\u9884\u6d4b\u7f51\u7edc\u4e0e\u76ee\u6807\u7f51\u7edc\u7684\u5dee\u5f02\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u7ed3\u5408\u4f2a\u8ba1\u6570\u9879\u548c\u5dee\u5f02\u9879\u5b9e\u73b0\u63a2\u7d22\u8870\u51cf\u548c\u9884\u6d4b\u5668\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRDD\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RDD\u6210\u529f\u7edf\u4e00\u4e86\u4e24\u79cd\u63a2\u7d22\u65b9\u6cd5\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4fdd\u6301\u4e86\u9884\u6d4b\u8bef\u5dee\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4f2a\u8ba1\u6570\u65b9\u6cd5\u7684\u63a2\u7d22\u8870\u51cf\u3002", "relevance": 75.0}}
{"id": "2505.11178", "pdf": "https://arxiv.org/pdf/2505.11178", "abs": "https://arxiv.org/abs/2505.11178", "authors": ["Yixin Wan", "Kai-Wei Chang"], "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CompAlign\u57fa\u51c6\u548cCompQuest\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u7ec4\u5408\u573a\u666f\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002CompAlign\u5305\u542b900\u4e2a\u590d\u6742\u591a\u4e3b\u4f53\u751f\u6210\u4efb\u52a1\uff0cCompQuest\u901a\u8fc7\u5206\u89e3\u63d0\u793a\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\u3002\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u590d\u67423D\u7a7a\u95f4\u5173\u7cfb\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u53cd\u9988\u5bf9\u9f50\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u5728\u751f\u6210\u7ec4\u5408\u573a\u666f\uff08\u591a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51faCompAlign\u57fa\u51c6\uff0c\u5305\u542b900\u4e2a\u590d\u6742\u591a\u4e3b\u4f53\u751f\u6210\u4efb\u52a1\uff1b2. \u8bbe\u8ba1CompQuest\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u63d0\u793a\u5e76\u4f7f\u7528MLLM\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\uff1b3. \u5229\u7528\u53cd\u9988\u4fe1\u53f7\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u3002", "result": "1. \u6a21\u578b\u5728\u590d\u67423D\u7a7a\u95f4\u5173\u7cfb\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff1b2. \u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff1b3. \u5bf9\u9f50\u540e\u6a21\u578b\u5728\u7ec4\u5408\u51c6\u786e\u6027\u4e0a\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CompAlign\u548cCompQuest\u4e3a\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u5de5\u5177\uff0c\u5bf9\u9f50\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.11050", "pdf": "https://arxiv.org/pdf/2505.11050", "abs": "https://arxiv.org/abs/2505.11050", "authors": ["Jeroen Bollen", "Jan Van den Bussche", "Stijn Vansummeren", "Jonni Virtema"], "title": "Halting Recurrent GNNs and the Graded $\u03bc$-Calculus", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "Graph Neural Networks (GNNs) are a class of machine-learning models that\noperate on graph-structured data. Their expressive power is intimately related\nto logics that are invariant under graded bisimilarity. Current proposals for\nrecurrent GNNs either assume that the graph size is given to the model, or\nsuffer from a lack of termination guarantees. In this paper, we propose a\nhalting mechanism for recurrent GNNs. We prove that our halting model can\nexpress all node classifiers definable in graded modal mu-calculus, even for\nthe standard GNN variant that is oblivious to the graph size. A recent\nbreakthrough in the study of the expressivity of graded modal mu-calculus in\nthe finite suggests that conversely, restricted to node classifiers definable\nin monadic second-order logic, recurrent GNNs can express only node classifiers\ndefinable in graded modal mu-calculus. To prove our main result, we develop a\nnew approximate semantics for graded mu-calculus, which we believe to be of\nindependent interest. We leverage this new semantics into a new model-checking\nalgorithm, called the counting algorithm, which is oblivious to the graph size.\nIn a final step we show that the counting algorithm can be implemented on a\nhalting recurrent GNN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5faa\u73af\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u505c\u6b62\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u8be5\u673a\u5236\u53ef\u4ee5\u8868\u8fbe\u6240\u6709\u5728\u5206\u7ea7\u6a21\u6001\u03bc\u6f14\u7b97\u4e2d\u5b9a\u4e49\u7684\u8282\u70b9\u5206\u7c7b\u5668\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u56fe\u7684\u5927\u5c0f\u3002", "motivation": "\u5f53\u524d\u5faa\u73afGNN\u8981\u4e48\u5047\u8bbe\u56fe\u7684\u5927\u5c0f\u5df2\u77e5\uff0c\u8981\u4e48\u7f3a\u4e4f\u7ec8\u6b62\u4fdd\u8bc1\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u505c\u6b62\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7ea7\u03bc\u6f14\u7b97\u8fd1\u4f3c\u8bed\u4e49\uff0c\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u56fe\u5927\u5c0f\u7684\u8ba1\u6570\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u505c\u6b62\u673a\u5236\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8ba1\u6570\u7b97\u6cd5\u53ef\u4ee5\u5728\u505c\u6b62\u5faa\u73afGNN\u4e0a\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u4e3a\u5faa\u73afGNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u56fe\u5927\u5c0f\u7684\u7ec8\u6b62\u673a\u5236\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "relevance": 50.0}}
{"id": "2505.11182", "pdf": "https://arxiv.org/pdf/2505.11182", "abs": "https://arxiv.org/abs/2505.11182", "authors": ["Yuzhuo Dai", "Jiaqi Jin", "Zhibin Dong", "Siwei Wang", "Xinwang Liu", "En Zhu", "Xihong Yang", "Xinbiao Gan", "Yu Feng"], "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning", "categories": ["cs.CV", "cs.AI"], "comment": "The paper has been accepted by the 42nd CVPR 2025. The main text has\n  9 pages, including 8 figures and 4 tables. The appendix has 8 pages, with 10\n  figures and 6 tables. The reference list has 3 pages", "summary": "In incomplete multi-view clustering (IMVC), missing data induce prototype\nshifts within views and semantic inconsistencies across views. A feasible\nsolution is to explore cross-view consistency in paired complete observations,\nfurther imputing and aligning the similarity relationships inherently shared\nacross views. Nevertheless, existing methods are constrained by two-tiered\nlimitations: (1) Neither instance- nor cluster-level consistency learning\nconstruct a semantic space shared across views to learn consensus semantics.\nThe former enforces cross-view instances alignment, and wrongly regards\nunpaired observations with semantic consistency as negative pairs; the latter\nfocuses on cross-view cluster counterparts while coarsely handling fine-grained\nintra-cluster relationships within views. (2) Excessive reliance on consistency\nresults in unreliable imputation and alignment without incorporating\nview-specific cluster information. Thus, we propose an IMVC framework,\nimputation- and alignment-free for consensus semantics learning (FreeCSL). To\nbridge semantic gaps across all observations, we learn consensus prototypes\nfrom available data to discover a shared space, where semantically similar\nobservations are pulled closer for consensus semantics learning. To capture\nsemantic relationships within specific views, we design a heuristic graph\nclustering based on modularity to recover cluster structure with intra-cluster\ncompactness and inter-cluster separation for cluster semantics enhancement.\nExtensive experiments demonstrate, compared to state-of-the-art competitors,\nFreeCSL achieves more confident and robust assignments on IMVC task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFreeCSL\u7684\u4e0d\u5b8c\u6574\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u8bc6\u539f\u578b\u5b66\u4e60\u548c\u542f\u53d1\u5f0f\u56fe\u805a\u7c7b\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u89c6\u56fe\u7279\u5b9a\u4fe1\u606f\u5229\u7528\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4e0d\u5b8c\u6574\u591a\u89c6\u56fe\u805a\u7c7b\u65b9\u6cd5\u5728\u5b9e\u4f8b\u7ea7\u548c\u96c6\u7fa4\u7ea7\u4e00\u81f4\u6027\u5b66\u4e60\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u4e00\u81f4\u6027\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u586b\u8865\u548c\u5bf9\u9f50\u3002", "method": "FreeCSL\u901a\u8fc7\u5b66\u4e60\u5171\u8bc6\u539f\u578b\u6784\u5efa\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u542f\u53d1\u5f0f\u56fe\u805a\u7c7b\u589e\u5f3a\u89c6\u56fe\u5185\u96c6\u7fa4\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeCSL\u5728\u4e0d\u5b8c\u6574\u591a\u89c6\u56fe\u805a\u7c7b\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "FreeCSL\u901a\u8fc7\u5171\u8bc6\u8bed\u4e49\u5b66\u4e60\u548c\u89c6\u56fe\u7279\u5b9a\u4fe1\u606f\u6574\u5408\uff0c\u63d0\u5347\u4e86\u805a\u7c7b\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "relevance": 30.0}}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831", "abs": "https://arxiv.org/abs/2505.10831", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "title": "Creating General User Models from Computer Use", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture that user knowledge and preferences. GUMs can infer that a user is\npreparing for a wedding they're attending from messages with a friend. Or\nrecognize that a user is struggling with a collaborator's feedback on a draft\nby observing multiple stalled edits and a switch to reading related work. GUMs\nintroduce an architecture that infers new propositions about a user from\nmultimodal observations, retrieves related propositions for context, and\ncontinuously revises existing propositions. To illustrate the breadth of\napplications that GUMs enable, we demonstrate how they augment chat-based\nassistants with context, manage OS notifications to selectively surface\nimportant information, and enable interactive agents that adapt to preferences\nacross apps. We also instantiate proactive assistants (GUMBOs) that discover\nand execute useful suggestions on a user's behalf using their GUM. In our\nevaluations, we find that GUMs make calibrated and accurate inferences about\nusers, and that assistants built on GUMs proactively identify and perform\nactions that users wouldn't think to request explicitly. Altogether, GUMs\nintroduce methods that leverage multimodal models to understand unstructured\ncontext, enabling long-standing visions of HCI and entirely new interactive\nsystems that anticipate user needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7528\u6237\u6a21\u578b\uff08GUM\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u89c2\u5bdf\u7528\u6237\u4e0e\u8ba1\u7b97\u673a\u7684\u4ea4\u4e92\uff0c\u5b66\u4e60\u7528\u6237\u77e5\u8bc6\u548c\u504f\u597d\uff0c\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406\u548c\u52a8\u6001\u66f4\u65b0\u3002", "motivation": "\u5f53\u524d\u7528\u6237\u6a21\u578b\u5c40\u9650\u4e8e\u7279\u5b9a\u5e94\u7528\uff0c\u7f3a\u4e4f\u7075\u6d3b\u63a8\u7406\u80fd\u529b\u3002GUM\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u89c2\u5bdf\u5b9e\u73b0\u901a\u7528\u7528\u6237\u7406\u89e3\uff0c\u652f\u6301\u66f4\u667a\u80fd\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "GUM\u901a\u8fc7\u8f93\u5165\u975e\u7ed3\u6784\u5316\u7528\u6237\u89c2\u5bdf\uff08\u5982\u8bbe\u5907\u622a\u56fe\uff09\uff0c\u751f\u6210\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u7528\u6237\u547d\u9898\uff0c\u5e76\u52a8\u6001\u66f4\u65b0\u3002\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\u3002", "result": "GUM\u80fd\u51c6\u786e\u63a8\u65ad\u7528\u6237\u9700\u6c42\uff0c\u5e76\u7528\u4e8e\u589e\u5f3a\u804a\u5929\u52a9\u624b\u3001\u7ba1\u7406\u901a\u77e5\u548c\u6784\u5efa\u4e3b\u52a8\u4ee3\u7406\uff08GUMBO\uff09\u3002\u8bc4\u4f30\u663e\u793a\u5176\u63a8\u65ad\u6821\u51c6\u4e14\u51c6\u786e\u3002", "conclusion": "GUM\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u7406\u89e3\u975e\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u7684\u4eba\u673a\u4ea4\u4e92\u613f\u666f\uff0c\u652f\u6301\u65b0\u578b\u667a\u80fd\u7cfb\u7edf\u3002", "relevance": 60.0}}
{"id": "2505.11054", "pdf": "https://arxiv.org/pdf/2505.11054", "abs": "https://arxiv.org/abs/2505.11054", "authors": ["M\u00e9lodie Monod", "Alessandro Micheli", "Samir Bhatt"], "title": "NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce NeuralSurv, the first deep survival model to incorporate\nBayesian uncertainty quantification. Our non-parametric, architecture-agnostic\nframework flexibly captures time-varying covariate-risk relationships in\ncontinuous time via a novel two-stage data-augmentation scheme, for which we\nestablish theoretical guarantees. For efficient posterior inference, we\nintroduce a mean-field variational algorithm with coordinate-ascent updates\nthat scale linearly in model size. By locally linearizing the Bayesian neural\nnetwork, we obtain full conjugacy and derive all coordinate updates in closed\nform. In experiments, NeuralSurv delivers superior calibration compared to\nstate-of-the-art deep survival models, while matching or exceeding their\ndiscriminative performance across both synthetic benchmarks and real-world\ndatasets. Our results demonstrate the value of Bayesian principles in\ndata-scarce regimes by enhancing model calibration and providing robust,\nwell-calibrated uncertainty estimates for the survival function.", "AI": {"tldr": "NeuralSurv\u662f\u4e00\u4e2a\u7ed3\u5408\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6570\u636e\u589e\u5f3a\u65b9\u6848\u7075\u6d3b\u6355\u6349\u65f6\u95f4\u53d8\u5316\u7684\u534f\u53d8\u91cf-\u98ce\u9669\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u800c\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u80fd\u63d0\u5347\u6a21\u578b\u6821\u51c6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u975e\u53c2\u6570\u3001\u67b6\u6784\u65e0\u5173\u7684\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u5e76\u8bbe\u8ba1\u7ebf\u6027\u53ef\u6269\u5c55\u7684\u53d8\u5206\u63a8\u65ad\u7b97\u6cd5\u3002", "result": "NeuralSurv\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cNeuralSurv\u4e3a\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u6821\u51c6\u826f\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "relevance": 40.0}}
{"id": "2505.11192", "pdf": "https://arxiv.org/pdf/2505.11192", "abs": "https://arxiv.org/abs/2505.11192", "authors": ["Myunsoo Kim", "Seong-Woong Shim", "Byung-Jun Lee"], "title": "FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "False negatives pose a critical challenge in vision-language pretraining\n(VLP) due to the many-to-many correspondence between images and texts in\nlarge-scale datasets. These false negatives introduce conflicting supervision\nsignals that degrade the learned embedding space and diminish the effectiveness\nof hard negative sampling. In this paper, we propose FALCON (False-negative\nAware Learning of COntrastive Negatives), a learning-based mini-batch\nconstruction strategy that adaptively balances the trade-off between hard and\nfalse negatives during VLP. Rather than relying on fixed heuristics, FALCON\nemploys a negative mining scheduler that dynamically selects negative samples\nof appropriate hardness for each anchor instance during mini-batch\nconstruction, guided by a proxy for cross-modal alignment improvement.\nExperimental results demonstrate that FALCON significantly improves performance\nacross two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of\ndownstream tasks and evaluation settings, underscoring its effectiveness and\nrobustness in mitigating the impact of false negatives.", "AI": {"tldr": "FALCON\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5e73\u8861\u786c\u8d1f\u6837\u672c\u548c\u5047\u8d1f\u6837\u672c\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff08VLP\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8d1f\u6837\u672c\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5047\u8d1f\u6837\u672c\u5728VLP\u4e2d\u5bfc\u81f4\u51b2\u7a81\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u5f71\u54cd\u5d4c\u5165\u7a7a\u95f4\u8d28\u91cf\u548c\u786c\u8d1f\u91c7\u6837\u6548\u679c\u3002", "method": "\u63d0\u51faFALCON\uff0c\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684mini-batch\u6784\u5efa\u7b56\u7565\uff0c\u52a8\u6001\u9009\u62e9\u8d1f\u6837\u672c\uff0c\u901a\u8fc7\u4ee3\u7406\u6307\u6807\u6307\u5bfc\u8de8\u6a21\u6001\u5bf9\u9f50\u6539\u8fdb\u3002", "result": "FALCON\u663e\u8457\u63d0\u5347\u4e86ALBEF\u548cBLIP-2\u6846\u67b6\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u548c\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "FALCON\u6709\u6548\u7f13\u89e3\u5047\u8d1f\u6837\u672c\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u4e86VLP\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "relevance": 60.0}}
{"id": "2505.10681", "pdf": "https://arxiv.org/pdf/2505.10681", "abs": "https://arxiv.org/abs/2505.10681", "authors": ["\u00d6nder G\u00fcrcan", "Vanja Falck", "Markus G. Rousseau", "Larissa L. Lima"], "title": "Towards an LLM-powered Social Digital Twinning Platform", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "13 pages, 3 figures, 23rd International Conference on Practical\n  applications of Agents and Multi-Agent Systems (PAAMS 2025)", "summary": "We present Social Digital Twinner, an innovative social simulation tool for\nexploring plausible effects of what-if scenarios in complex adaptive social\nsystems. The architecture is composed of three seamlessly integrated parts: a\ndata infrastructure featuring real-world data and a multi-dimensionally\nrepresentative synthetic population of citizens, an LLM-enabled agent-based\nsimulation engine, and a user interface that enable intuitive, natural language\ninteractions with the simulation engine and the artificial agents (i.e.\ncitizens). Social Digital Twinner facilitates real-time engagement and empowers\nstakeholders to collaboratively design, test, and refine intervention measures.\nThe approach is promoting a data-driven and evidence-based approach to societal\nproblem-solving. We demonstrate the tool's interactive capabilities by\naddressing the critical issue of youth school dropouts in Kragero, Norway,\nshowcasing its ability to create and execute a dedicated social digital twin\nusing natural language.", "AI": {"tldr": "Social Digital Twinner\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u793e\u4ea4\u6a21\u62df\u5de5\u5177\uff0c\u7528\u4e8e\u63a2\u7d22\u590d\u6742\u81ea\u9002\u5e94\u793e\u4ea4\u7cfb\u7edf\u4e2d\u5047\u8bbe\u573a\u666f\u7684\u53ef\u80fd\u5f71\u54cd\u3002\u5b83\u7ed3\u5408\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u3001LLM\u9a71\u52a8\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u5f15\u64ce\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u754c\u9762\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u65b9\u6cd5\u89e3\u51b3\u793e\u4f1a\u95ee\u9898\uff0c\u4fc3\u8fdb\u5229\u76ca\u76f8\u5173\u8005\u534f\u4f5c\u8bbe\u8ba1\u548c\u6d4b\u8bd5\u5e72\u9884\u63aa\u65bd\u3002", "method": "\u5de5\u5177\u67b6\u6784\u5305\u62ec\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u3001LLM\u9a71\u52a8\u7684\u6a21\u62df\u5f15\u64ce\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u754c\u9762\uff0c\u652f\u6301\u5b9e\u65f6\u53c2\u4e0e\u3002", "result": "\u901a\u8fc7\u632a\u5a01Kragero\u7684\u9752\u5c11\u5e74\u8f8d\u5b66\u95ee\u9898\u5c55\u793a\u4e86\u5de5\u5177\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u80fd\u591f\u521b\u5efa\u548c\u6267\u884c\u4e13\u7528\u7684\u793e\u4ea4\u6570\u5b57\u5b6a\u751f\u3002", "conclusion": "Social Digital Twinner\u4e3a\u590d\u6742\u793e\u4f1a\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5b9e\u65f6\u534f\u4f5c\u3002", "relevance": 40.0}}
{"id": "2505.11067", "pdf": "https://arxiv.org/pdf/2505.11067", "abs": "https://arxiv.org/abs/2505.11067", "authors": ["Omobayode Fagbohungbe", "Corey Lammie", "Malte J. Rasch", "Takashi Ando", "Tayfun Gokmen", "Vijay Narayanan"], "title": "Assessing the Performance of Analog Training for Transfer Learning", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.DC", "cs.NE"], "comment": null, "summary": "Analog in-memory computing is a next-generation computing paradigm that\npromises fast, parallel, and energy-efficient deep learning training and\ntransfer learning (TL). However, achieving this promise has remained elusive\ndue to a lack of suitable training algorithms. Analog memory devices exhibit\nasymmetric and non-linear switching behavior in addition to device-to-device\nvariation, meaning that most, if not all, of the current off-the-shelf training\nalgorithms cannot achieve good training outcomes. Also, recently introduced\nalgorithms have enjoyed limited attention, as they require bi-directionally\nswitching devices of unrealistically high symmetry and precision and are highly\nsensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which\nleverages the chopped technique to address many of the challenges mentioned\nabove. In this paper, we assess the performance of the c-TTv2 algorithm for\nanalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also\ninvestigate the robustness of our algorithm to changes in some device\nspecifications, including weight transfer noise, symmetry point skew, and\nsymmetry point variability", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ac-TTv2\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u4e2d\u8bad\u7ec3\u7b97\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728Swin-ViT\u6a21\u578b\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "motivation": "\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u56e0\u5176\u9ad8\u6548\u6027\u6210\u4e3a\u4e0b\u4e00\u4ee3\u8ba1\u7b97\u8303\u5f0f\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u8bbe\u5907\u975e\u7ebf\u6027\u548c\u4e0d\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6f5c\u529b\u3002", "method": "\u63d0\u51fac-TTv2\u7b97\u6cd5\uff0c\u5229\u7528chopped\u6280\u672f\u89e3\u51b3\u8bbe\u5907\u975e\u7ebf\u6027\u548c\u4e0d\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u5e76\u5728Swin-ViT\u6a21\u578b\u548cCIFAR100\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "c-TTv2\u7b97\u6cd5\u5728\u6a21\u62df\u8fc1\u79fb\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5bf9\u8bbe\u5907\u89c4\u683c\u53d8\u5316\uff08\u5982\u6743\u91cd\u4f20\u8f93\u566a\u58f0\u3001\u5bf9\u79f0\u70b9\u504f\u79fb\uff09\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "c-TTv2\u7b97\u6cd5\u4e3a\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002", "relevance": 30.0}}
{"id": "2505.11196", "pdf": "https://arxiv.org/pdf/2505.11196", "abs": "https://arxiv.org/abs/2505.11196", "authors": ["Yuang Ai", "Qihang Fan", "Xuefeng Hu", "Zhenheng Yang", "Ran He", "Huaibo Huang"], "title": "DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling", "categories": ["cs.CV"], "comment": "27 pages, 29 figures, 9 tables", "summary": "Diffusion Transformer (DiT), a promising diffusion model for visual\ngeneration, demonstrates impressive performance but incurs significant\ncomputational overhead. Intriguingly, analysis of pre-trained DiT models\nreveals that global self-attention is often redundant, predominantly capturing\nlocal patterns-highlighting the potential for more efficient alternatives. In\nthis paper, we revisit convolution as an alternative building block for\nconstructing efficient and expressive diffusion models. However, naively\nreplacing self-attention with convolution typically results in degraded\nperformance. Our investigations attribute this performance gap to the higher\nchannel redundancy in ConvNets compared to Transformers. To resolve this, we\nintroduce a compact channel attention mechanism that promotes the activation of\nmore diverse channels, thereby enhancing feature diversity. This leads to\nDiffusion ConvNet (DiCo), a family of diffusion models built entirely from\nstandard ConvNet modules, offering strong generative performance with\nsignificant efficiency gains. On class-conditional ImageNet benchmarks, DiCo\noutperforms previous diffusion models in both image quality and generation\nspeed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53\nat 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively.\nFurthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID\nof 1.90 on ImageNet 256x256-without any additional supervision during training.\nCode: https://github.com/shallowdream204/DiCo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiffusion ConvNet (DiCo)\uff0c\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u7684\u9ad8\u6548\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u7d27\u51d1\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\uff0c\u5728ImageNet\u4e0a\u8d85\u8d8aDiT\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002", "motivation": "DiT\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5206\u6790\u53d1\u73b0\u5176\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u5197\u4f59\uff0c\u63a2\u7d22\u5377\u79ef\u4f5c\u4e3a\u66ff\u4ee3\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51faDiCo\uff0c\u7528\u6807\u51c6\u5377\u79ef\u6a21\u5757\u6784\u5efa\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u7d27\u51d1\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u51cf\u5c11\u901a\u9053\u5197\u4f59\u5e76\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "DiCo\u5728ImageNet\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u751f\u6210\u901f\u5ea6\uff0cDiCo-XL FID\u4e3a2.05\uff08256x256\uff09\u548c2.53\uff08512x512\uff09\uff0c\u901f\u5ea6\u63d0\u53472.7x\u548c3.1x\u3002", "conclusion": "DiCo\u5c55\u793a\u4e86\u5377\u79ef\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 75.0}}
{"id": "2505.10691", "pdf": "https://arxiv.org/pdf/2505.10691", "abs": "https://arxiv.org/abs/2505.10691", "authors": ["Wanying Dou", "Gorkem Durak", "Koushik Biswas", "Ziliang Hong", "Andrea Mia Bejar", "Elif Keles", "Kaan Akin", "Sukru Mehmet Erturk", "Alpay Medetalibeyoglu", "Marc Sala", "Alexander Misharin", "Hatice Savas", "Mary Salvatore", "Sachin Jambawalikar", "Drew Torigian", "Jayaram K. Udupa", "Ulas Bagci"], "title": "Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "While the acute phase of the COVID-19 pandemic has subsided, its long-term\neffects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known\nas Long COVID. There remains substantial uncertainty regarding both its\nduration and optimal management strategies. PASC manifests as a diverse array\nof persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and\nneurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and\nmusculoskeletal abnormalities--that extend beyond the acute infection phase.\nThis heterogeneous presentation poses substantial challenges for clinical\nassessment, diagnosis, and treatment planning. In this paper, we focus on\nimaging findings that may suggest fibrotic damage in the lungs, a critical\nmanifestation characterized by scarring of lung tissue, which can potentially\naffect long-term respiratory function in patients with PASC. This study\nintroduces a novel multi-center chest CT analysis framework that combines deep\nlearning and radiomics for fibrosis prediction. Our approach leverages\nconvolutional neural networks (CNNs) and interpretable feature extraction,\nachieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate\nthe effectiveness of Grad-CAM visualization and radiomics-based feature\nanalysis in providing clinically relevant insights for PASC-related lung\nfibrosis prediction. Our findings highlight the potential of deep\nlearning-driven computational methods for early detection and risk assessment\nof PASC-related lung fibrosis--presented for the first time in the literature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u653e\u5c04\u7ec4\u5b66\u7684\u591a\u4e2d\u5fc3\u80f8\u90e8CT\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bCOVID-19\u540e\u9057\u75c7\uff08PASC\uff09\u76f8\u5173\u7684\u80ba\u7ea4\u7ef4\u5316\uff0c\u51c6\u786e\u7387\u8fbe82.2%\uff0cAUC\u4e3a85.5%\u3002", "motivation": "PASC\u7684\u5f02\u8d28\u6027\u8868\u73b0\u5bf9\u4e34\u5e8a\u8bc4\u4f30\u548c\u6cbb\u7597\u89c4\u5212\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u80ba\u7ea4\u7ef4\u5316\u53ef\u80fd\u5f71\u54cd\u957f\u671f\u547c\u5438\u529f\u80fd\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8ba1\u7b97\u533b\u5b66\u65b9\u6cd5\u65e9\u671f\u68c0\u6d4b\u548c\u8bc4\u4f30\u98ce\u9669\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u7ed3\u5408Grad-CAM\u53ef\u89c6\u5316\u548c\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5206\u6790\u3002", "result": "\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523082.2%\u7684\u51c6\u786e\u7387\u548c85.5%\u7684AUC\uff0c\u9a8c\u8bc1\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728PASC\u76f8\u5173\u80ba\u7ea4\u7ef4\u5316\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728PASC\u76f8\u5173\u80ba\u7ea4\u7ef4\u5316\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u98ce\u9669\u8bc4\u4f30\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "relevance": 20.0}}
{"id": "2505.11076", "pdf": "https://arxiv.org/pdf/2505.11076", "abs": "https://arxiv.org/abs/2505.11076", "authors": ["Vladim\u00edr Bo\u017ea", "Vladim\u00edr Macko"], "title": "Addition is almost all you need: Compressing neural networks with double binary factorization", "categories": ["cs.LG"], "comment": null, "summary": "Binary quantization approaches, which replace weight matrices with binary\nmatrices and substitute costly multiplications with cheaper additions, offer a\ncomputationally efficient approach to address the increasing computational and\nstorage requirements of Large Language Models (LLMs). However, the severe\nquantization constraint ($\\pm1$) can lead to significant accuracy degradation.\nIn this paper, we propose Double Binary Factorization (DBF), a novel method\nthat factorizes dense weight matrices into products of two binary (sign)\nmatrices, each accompanied by scaling vectors. DBF preserves the efficiency\nadvantages of binary representations while achieving compression rates that are\ncompetitive with or superior to state-of-the-art methods. Specifically, in a\n1-bit per weight range, DBF is better than existing binarization approaches. In\na 2-bit per weight range, DBF is competitive with the best quantization methods\nlike QuIP\\# and QTIP. Unlike most existing compression techniques, which offer\nlimited compression level choices, DBF allows fine-grained control over\ncompression ratios by adjusting the factorization's intermediate dimension.\nBased on this advantage, we further introduce an algorithm for estimating\nnon-uniform layer-wise compression ratios for DBF, based on previously\ndeveloped channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u4e8c\u8fdb\u5236\u5206\u89e3\uff08DBF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7a20\u5bc6\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e8c\u8fdb\u5236\u77e9\u9635\u53ca\u5176\u7f29\u653e\u5411\u91cf\u7684\u4e58\u79ef\uff0c\u5728\u4fdd\u6301\u4e8c\u8fdb\u5236\u8868\u793a\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u4e8c\u503c\u5316\u65b9\u6cd5\u7684\u538b\u7f29\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4e8c\u503c\u91cf\u5316\u65b9\u6cd5\u901a\u8fc7\u7528\u4e8c\u8fdb\u5236\u77e9\u9635\u66ff\u6362\u6743\u91cd\u77e9\u9635\u5e76\u51cf\u5c11\u4e58\u6cd5\u8fd0\u7b97\u6765\u63d0\u5347\u6548\u7387\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u4e25\u91cd\u91cf\u5316\u7ea6\u675f\u4e0b\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "method": "DBF\u5c06\u7a20\u5bc6\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e8c\u8fdb\u5236\u77e9\u9635\u53ca\u5176\u7f29\u653e\u5411\u91cf\u7684\u4e58\u79ef\uff0c\u901a\u8fc7\u8c03\u6574\u5206\u89e3\u7684\u4e2d\u95f4\u7ef4\u5ea6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u538b\u7f29\u6bd4\u63a7\u5236\uff0c\u5e76\u57fa\u4e8e\u901a\u9053\u526a\u679d\u6807\u51c6\u4f30\u8ba1\u975e\u5747\u5300\u5c42\u95f4\u538b\u7f29\u6bd4\u3002", "result": "\u57281\u6bd4\u7279/\u6743\u91cd\u8303\u56f4\u5185\uff0cDBF\u4f18\u4e8e\u73b0\u6709\u4e8c\u503c\u5316\u65b9\u6cd5\uff1b\u57282\u6bd4\u7279/\u6743\u91cd\u8303\u56f4\u5185\uff0c\u4e0eQuIP#\u548cQTIP\u7b49\u6700\u4f73\u91cf\u5316\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "DBF\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u538b\u7f29\u6bd4\u63a7\u5236\uff0c\u4e3aLLMs\u7684\u91cf\u5316\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.11216", "pdf": "https://arxiv.org/pdf/2505.11216", "abs": "https://arxiv.org/abs/2505.11216", "authors": ["Shibin Mei", "Hang Wang", "Bingbing Ni"], "title": "GeoMM: On Geodesic Perspective for Multi-modal Learning", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, accepted by CVPR2025", "summary": "Geodesic distance serves as a reliable means of measuring distance in\nnonlinear spaces, and such nonlinear manifolds are prevalent in the current\nmultimodal learning. In these scenarios, some samples may exhibit high\nsimilarity, yet they convey different semantics, making traditional distance\nmetrics inadequate for distinguishing between positive and negative samples.\nThis paper introduces geodesic distance as a novel distance metric in\nmulti-modal learning for the first time, to mine correlations between samples,\naiming to address the limitations of common distance metric. Our approach\nincorporates a comprehensive series of strategies to adapt geodesic distance\nfor the current multimodal learning. Specifically, we construct a graph\nstructure to represent the adjacency relationships among samples by\nthresholding distances between them and then apply the shortest-path algorithm\nto obtain geodesic distance within this graph. To facilitate efficient\ncomputation, we further propose a hierarchical graph structure through\nclustering and combined with incremental update strategies for dynamic status\nupdates. Extensive experiments across various downstream tasks validate the\neffectiveness of our proposed method, demonstrating its capability to capture\ncomplex relationships between samples and improve the performance of multimodal\nlearning models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u6d4b\u5730\u8ddd\u79bb\u5f15\u5165\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u901a\u8fc7\u6784\u5efa\u56fe\u7ed3\u6784\u548c\u5c42\u6b21\u5316\u7b56\u7565\u4f18\u5316\u8ba1\u7b97\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ddd\u79bb\u5ea6\u91cf\u5728\u975e\u7ebf\u6027\u7a7a\u95f4\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u975e\u7ebf\u6027\u7a7a\u95f4\u4e2d\u4f20\u7edf\u8ddd\u79bb\u5ea6\u91cf\u65e0\u6cd5\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u4f46\u8bed\u4e49\u4e0d\u540c\u7684\u6837\u672c\uff0c\u6d4b\u5730\u8ddd\u79bb\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u6837\u672c\u95f4\u590d\u6742\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u6837\u672c\u90bb\u63a5\u56fe\uff0c\u5e94\u7528\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u8ba1\u7b97\u6d4b\u5730\u8ddd\u79bb\uff0c\u5e76\u63d0\u51fa\u5c42\u6b21\u5316\u56fe\u7ed3\u6784\u548c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u6d4b\u5730\u8ddd\u79bb\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u89e3\u51b3\u4f20\u7edf\u5ea6\u91cf\u7684\u5c40\u9650\u6027\u3002", "relevance": 40.0}}
{"id": "2505.10852", "pdf": "https://arxiv.org/pdf/2505.10852", "abs": "https://arxiv.org/abs/2505.10852", "authors": ["Siyu Liu", "Jiamin Xu", "Beilin Ye", "Bo Hu", "David J. Srolovitz", "Tongqi Wen"], "title": "MatTools: Benchmarking Large Language Models for Materials Science Tools", "categories": ["cond-mat.mtrl-sci", "cs.CL", "cs.DB"], "comment": "27 pages, 23 figures", "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MatTools\uff0c\u4e00\u4e2a\u8bc4\u4f30LLMs\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542bQA\u548c\u5b9e\u9645\u5de5\u5177\u4f7f\u7528\u4e24\u90e8\u5206\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u80fd\u529b\uff0c\u7279\u522b\u662f\u751f\u6210\u548c\u6267\u884c\u57fa\u4e8e\u7269\u7406\u8ba1\u7b97\u4ee3\u7801\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u52a8\u5316\u65b9\u6cd5\u6536\u96c6\u6750\u6599\u79d1\u5b66\u5de5\u5177\u4f7f\u7528\u6848\u4f8b\uff0c\u6784\u5efa\u4e86\u5305\u542b69,225 QA\u5bf9\u7684QA\u57fa\u51c6\u548c49\u4e2a\u5b9e\u9645\u4efb\u52a1\u7684\u57fa\u51c6\u3002", "result": "\u53d1\u73b0\u901a\u7528\u6a21\u578b\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0cAI\u66f4\u4e86\u89e3AI\uff0c\u7b80\u5355\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "MatTools\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u3002", "relevance": 60.0}}
{"id": "2505.10695", "pdf": "https://arxiv.org/pdf/2505.10695", "abs": "https://arxiv.org/abs/2505.10695", "authors": ["Julian Wolter", "Amr Gomaa"], "title": "Predicting Human Behavior in Autonomous Systems: A Collaborative Machine Teaching Approach for Reducing Transfer of Control Events", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "As autonomous systems become integral to various industries, effective\nstrategies for fault handling are essential to ensure reliability and\nefficiency. Transfer of Control (ToC), a traditional approach for interrupting\nautomated processes during faults, is often triggered unnecessarily in\nnon-critical situations. To address this, we propose a data-driven method that\nuses human interaction data to train AI models capable of preemptively\nidentifying and addressing issues or assisting users in resolution. Using an\ninteractive tool simulating an industrial vacuum cleaner, we collected data and\ndeveloped an LSTM-based model to predict user behavior. Our findings reveal\nthat even data from non-experts can effectively train models to reduce\nunnecessary ToC events, enhancing the system's robustness. This approach\nhighlights the potential of AI to learn directly from human problem-solving\nbehaviors, complementing sensor data to improve industrial automation and\nhuman-AI collaboration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3AI\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u4e0d\u5fc5\u8981\u7684\u63a7\u5236\u8f6c\u79fb\uff08ToC\uff09\u4e8b\u4ef6\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u8f6c\u79fb\u65b9\u6cd5\u5728\u975e\u5173\u952e\u60c5\u51b5\u4e0b\u9891\u7e41\u89e6\u53d1\uff0c\u5f71\u54cd\u7cfb\u7edf\u6548\u7387\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u5b66\u4e60\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u884c\u4e3a\uff0c\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u5de5\u4e1a\u5438\u5c18\u5668\u7684\u4ea4\u4e92\u5de5\u5177\u6536\u96c6\u6570\u636e\uff0c\u5f00\u53d1\u57fa\u4e8eLSTM\u7684\u6a21\u578b\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u975e\u4e13\u5bb6\u6570\u636e\u4e5f\u80fd\u6709\u6548\u8bad\u7ec3\u6a21\u578b\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684ToC\u4e8b\u4ef6\u3002", "conclusion": "AI\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u884c\u4e3a\u53ef\u63d0\u5347\u5de5\u4e1a\u81ea\u52a8\u5316\u4e0e\u4eba\u673a\u534f\u4f5c\u7684\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.11081", "pdf": "https://arxiv.org/pdf/2505.11081", "abs": "https://arxiv.org/abs/2505.11081", "authors": ["Pierre Clavier", "Nathan Grinsztajn", "Raphael Avalos", "Yannis Flet-Berliac", "Irem Ergun", "Omar D. Domingues", "Eugene Tarassov", "Olivier Pietquin", "Pierre H. Richemond", "Florian Strub", "Matthieu Geist"], "title": "ShiQ: Bringing back Bellman to LLMs", "categories": ["cs.LG"], "comment": null, "summary": "The fine-tuning of pre-trained large language models (LLMs) using\nreinforcement learning (RL) is generally formulated as direct policy\noptimization. This approach was naturally favored as it efficiently improves a\npretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning\nmethods, has received far less attention in the LLM community while\ndemonstrating major success in various non-LLM RL tasks. In particular,\nQ-learning effectiveness comes from its sample efficiency and ability to learn\noffline, which is particularly valuable given the high computational cost of\nsampling with LLMs. However, naively applying a Q-learning-style update to the\nmodel's logits is ineffective due to the specificity of LLMs. Our core\ncontribution is to derive theoretically grounded loss functions from Bellman\nequations to adapt Q-learning methods to LLMs. To do so, we carefully adapt\ninsights from the RL literature to account for LLM-specific characteristics,\nensuring that the logits become reliable Q-value estimates. We then use this\nloss to build a practical algorithm, ShiQ for Shifted-Q, that supports\noff-policy, token-wise learning while remaining simple to implement. Finally,\nwe evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,\nUltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn\nand multi-turn LLM settings", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQ\u5b66\u4e60\u7684LLM\u5fae\u8c03\u65b9\u6cd5ShiQ\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u9002\u5e94LLM\u7279\u6027\uff0c\u652f\u6301\u79bb\u7ebf\u5b66\u4e60\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "Q\u5b66\u4e60\u5728\u975eLLM\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8eLLM\u6548\u679c\u4e0d\u4f73\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347LLM\u5fae\u8c03\u7684\u6837\u672c\u6548\u7387\u548c\u79bb\u7ebf\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u4ece\u8d1d\u5c14\u66fc\u65b9\u7a0b\u63a8\u5bfc\u7406\u8bba\u635f\u5931\u51fd\u6570\uff0c\u9002\u5e94LLM\u7279\u6027\uff0c\u63d0\u51faShiQ\u7b97\u6cd5\u652f\u6301\u79bb\u7ebf\u5b66\u4e60\u548c\u5206\u6b65\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u57fa\u51c6\uff08\u5982UltraFeedback\u548cBFCL-V3\uff09\u4e0a\u9a8c\u8bc1\u4e86ShiQ\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u5355\u8f6e\u548c\u591a\u8f6eLLM\u4efb\u52a1\u3002", "conclusion": "ShiQ\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684Q\u5b66\u4e60\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709RL\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "relevance": 90.0}}
{"id": "2505.11232", "pdf": "https://arxiv.org/pdf/2505.11232", "abs": "https://arxiv.org/abs/2505.11232", "authors": ["Haiyu Li", "Charith Abhayaratne"], "title": "AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras, which capture brightness changes with high temporal\nresolution, inherently generate a significant amount of redundant and noisy\ndata beyond essential object structures. The primary challenge in event-based\nobject recognition lies in effectively removing this noise without losing\ncritical spatial-temporal information. To address this, we propose an Adaptive\nGraph-based Noisy Data Removal framework for Event-based Object Recognition.\nSpecifically, our approach integrates adaptive event segmentation based on\nnormalized density analysis, a multifactorial edge-weighting mechanism, and\nadaptive graph-based denoising strategies. These innovations significantly\nenhance the integration of spatiotemporal information, effectively filtering\nnoise while preserving critical structural features for robust recognition.\nExperimental evaluations on four challenging datasets demonstrate that our\nmethod achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and\n96.89%, surpassing existing graph-based methods by up to 8.79%, and improving\nnoise reduction performance by up to 19.57%, with an additional accuracy gain\nof 6.26% compared to traditional Euclidean-based techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u56fe\u7684\u4e8b\u4ef6\u6570\u636e\u53bb\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u4e2d\u7684\u7269\u4f53\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u7cbe\u5ea6\u548c\u566a\u58f0\u53bb\u9664\u6548\u679c\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u751f\u6210\u7684\u6570\u636e\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u548c\u566a\u58f0\uff0c\u5982\u4f55\u5728\u53bb\u566a\u7684\u540c\u65f6\u4fdd\u7559\u5173\u952e\u65f6\u7a7a\u4fe1\u606f\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u4e8b\u4ef6\u5206\u5272\u3001\u591a\u56e0\u7d20\u8fb9\u6743\u91cd\u673a\u5236\u548c\u81ea\u9002\u5e94\u56fe\u53bb\u566a\u7b56\u7565\uff0c\u6709\u6548\u6574\u5408\u65f6\u7a7a\u4fe1\u606f\u5e76\u53bb\u566a\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f9783.77%\u300176.79%\u300199.30%\u548c96.89%\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e8b\u4ef6\u76f8\u673a\u7269\u4f53\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u53bb\u9664\u548c\u8bc6\u522b\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.10872", "pdf": "https://arxiv.org/pdf/2505.10872", "abs": "https://arxiv.org/abs/2505.10872", "authors": ["Chenxi Jiang", "Chuhao Zhou", "Jianfei Yang"], "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Submitted to CoRL 2025, under review", "summary": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6a21\u7cca\u6307\u4ee3\u8868\u8fbe\uff08REs\uff09\u5bf9\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u4efb\u52a1\u5bfc\u5411\u4e0a\u4e0b\u6587\u8ba4\u77e5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u975e\u4e13\u5bb6\u7528\u6237\u7684\u6307\u4ee4\u5f80\u5f80\u5305\u542b\u6a21\u7cca\u6027\uff0c\u5c24\u5176\u662f\u8001\u5e74\u4eba\u548c\u513f\u7ae5\uff0c\u8fd9\u5bf9\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5305\u542b\u6a21\u7ccaREs\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u57fa\u51c6\uff08REI-Bench\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4efb\u52a1\u5bfc\u5411\u4e0a\u4e0b\u6587\u8ba4\u77e5\u65b9\u6cd5\u4ee5\u751f\u6210\u6e05\u6670\u6307\u4ee4\u3002", "result": "\u6a21\u7ccaREs\u5bfc\u81f4\u4efb\u52a1\u89c4\u5212\u6210\u529f\u7387\u4e0b\u964d\u9ad8\u8fbe77.9%\uff0c\u800c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u4efb\u52a1\u5bfc\u5411\u4e0a\u4e0b\u6587\u8ba4\u77e5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u7ccaREs\u95ee\u9898\uff0c\u4f7f\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u66f4\u5b9e\u7528\u3002", "relevance": 70.0}}
{"id": "2505.11083", "pdf": "https://arxiv.org/pdf/2505.11083", "abs": "https://arxiv.org/abs/2505.11083", "authors": ["Guangqiang Li", "M. Amine Atoui", "Xiangshun Li"], "title": "Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 11 figures", "summary": "Deep learning methods have shown promising performance in fault diagnosis for\nmultimode process. Most existing studies assume that the collected health state\ncategories from different operating modes are identical. However, in real\nindustrial scenarios, these categories typically exhibit only partial overlap.\nThe incompleteness of the available data and the large distributional\ndifferences between the operating modes pose a significant challenge to\nexisting fault diagnosis methods. To address this problem, a novel fault\ndiagnosis model named self-adaptive temporal-spatial attention network\n(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy\ncategory data to generate multimode samples. To enrich the diversity of the\nfault data, interpolation is performed between healthy and fault samples.\nSubsequently, the fault diagnosis model is trained using real and generated\ndata. The self-adaptive instance normalization is established to suppress\nirrelevant information while retaining essential statistical features for\ndiagnosis. In addition, a temporal-spatial attention mechanism is constructed\nto focus on the key features, thus enhancing the generalization ability of the\nmodel. The extensive experiments demonstrate that the proposed model\nsignificantly outperforms the state-of-the-art methods. The code will be\navailable on Github at https://github.com/GuangqiangLi/TSA-SAN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u7f51\u7edc\uff08TSA-SAN\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u5f0f\u8fc7\u7a0b\u4e2d\u6545\u969c\u8bca\u65ad\u7684\u6570\u636e\u4e0d\u5b8c\u6574\u548c\u5206\u5e03\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\uff0c\u4e0d\u540c\u64cd\u4f5c\u6a21\u5f0f\u7684\u5065\u5eb7\u72b6\u6001\u7c7b\u522b\u901a\u5e38\u53ea\u6709\u90e8\u5206\u91cd\u53e0\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u6570\u636e\u4e0d\u5b8c\u6574\u548c\u5206\u5e03\u5dee\u5f02\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u6a21\u5f0f\u95f4\u6620\u5c04\u751f\u6210\u591a\u6a21\u5f0f\u6837\u672c\uff0c\u5229\u7528\u63d2\u503c\u4e30\u5bcc\u6545\u969c\u6570\u636e\u591a\u6837\u6027\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTSA-SAN\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TSA-SAN\u5728\u591a\u6a21\u5f0f\u6545\u969c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.11245", "pdf": "https://arxiv.org/pdf/2505.11245", "abs": "https://arxiv.org/abs/2505.11245", "authors": ["Fu-Yun Wang", "Yunhao Shui", "Jingtan Piao", "Keqiang Sun", "Hongsheng Li"], "title": "Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "Diffusion models have made substantial advances in image generation, yet\nmodels trained on large, unfiltered datasets often yield outputs misaligned\nwith human preferences. Numerous methods have been proposed to fine-tune\npre-trained diffusion models, achieving notable improvements in aligning\ngenerated outputs with human preferences. However, we argue that existing\npreference alignment methods neglect the critical role of handling\nunconditional/negative-conditional outputs, leading to a diminished capacity to\navoid generating undesirable outcomes. This oversight limits the efficacy of\nclassifier-free guidance~(CFG), which relies on the contrast between\nconditional generation and unconditional/negative-conditional generation to\noptimize output quality. In response, we propose a straightforward but\nversatile effective approach that involves training a model specifically\nattuned to negative preferences. This method does not require new training\nstrategies or datasets but rather involves minor modifications to existing\ntechniques. Our approach integrates seamlessly with models such as SD1.5, SDXL,\nvideo diffusion models and models that have undergone preference optimization,\nconsistently enhancing their alignment with human preferences.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u65e0\u6761\u4ef6/\u8d1f\u6761\u4ef6\u8f93\u51fa\u7684\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e13\u95e8\u9488\u5bf9\u8d1f\u9762\u504f\u597d\u7684\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5bf9\u65e0\u6761\u4ef6/\u8d1f\u6761\u4ef6\u8f93\u51fa\u7684\u5904\u7406\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u53ef\u80fd\u4e0d\u7b26\u5408\u4eba\u7c7b\u504f\u597d\uff0c\u9650\u5236\u4e86\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\uff08CFG\uff09\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u8d1f\u9762\u504f\u597d\u7684\u6a21\u578b\uff0c\u65e0\u9700\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u6216\u6570\u636e\u96c6\uff0c\u4ec5\u9700\u5bf9\u73b0\u6709\u6280\u672f\u8fdb\u884c\u5fae\u5c0f\u4fee\u6539\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u4e0eSD1.5\u3001SDXL\u3001\u89c6\u9891\u6269\u6563\u6a21\u578b\u7b49\u65e0\u7f1d\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5904\u7406\u65e0\u6761\u4ef6/\u8d1f\u6761\u4ef6\u8f93\u51fa\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u7ed3\u679c\u7684\u8d28\u91cf\u548c\u504f\u597d\u5bf9\u9f50\u80fd\u529b\u3002", "relevance": 60.0}}
{"id": "2505.11085", "pdf": "https://arxiv.org/pdf/2505.11085", "abs": "https://arxiv.org/abs/2505.11085", "authors": ["Oliver Schacht", "Biwei Huang"], "title": "A Fast Kernel-based Conditional Independence test with Application to Causal Discovery", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "9 pages, 5 figures", "summary": "Kernel-based conditional independence (KCI) testing is a powerful\nnonparametric method commonly employed in causal discovery tasks. Despite its\nflexibility and statistical reliability, cubic computational complexity limits\nits application to large datasets. To address this computational bottleneck, we\npropose \\textit{FastKCI}, a scalable and parallelizable kernel-based\nconditional independence test that utilizes a mixture-of-experts approach\ninspired by embarrassingly parallel inference techniques for Gaussian\nprocesses. By partitioning the dataset based on a Gaussian mixture model over\nthe conditioning variables, FastKCI conducts local KCI tests in parallel,\naggregating the results using an importance-weighted sampling scheme.\nExperiments on synthetic datasets and benchmarks on real-world production data\nvalidate that FastKCI maintains the statistical power of the original KCI test\nwhile achieving substantial computational speedups. FastKCI thus represents a\npractical and efficient solution for conditional independence testing in causal\ninference on large-scale data.", "AI": {"tldr": "FastKCI\u662f\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u548c\u5206\u533a\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7edf\u8ba1\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u7684KCI\u6d4b\u8bd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0cFastKCI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5bf9\u6570\u636e\u96c6\u5206\u533a\uff0c\u5e76\u884c\u6267\u884c\u5c40\u90e8KCI\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u91cd\u8981\u6027\u52a0\u6743\u91c7\u6837\u805a\u5408\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastKCI\u5728\u4fdd\u6301\u7edf\u8ba1\u529f\u6548\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u3002", "conclusion": "FastKCI\u4e3a\u5927\u89c4\u6a21\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.11246", "pdf": "https://arxiv.org/pdf/2505.11246", "abs": "https://arxiv.org/abs/2505.11246", "authors": ["Nirjhor Datta", "Afroza Akther", "M. Sohel Rahman"], "title": "Entropy-Driven Genetic Optimization for Deep-Feature-Guided Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Image enhancement methods often prioritize pixel level information,\noverlooking the semantic features. We propose a novel, unsupervised,\nfuzzy-inspired image enhancement framework guided by NSGA-II algorithm that\noptimizes image brightness, contrast, and gamma parameters to achieve a balance\nbetween visual quality and semantic fidelity. Central to our proposed method is\nthe use of a pre trained deep neural network as a feature extractor. To find\nthe best enhancement settings, we use a GPU-accelerated NSGA-II algorithm that\nbalances multiple objectives, namely, increasing image entropy, improving\nperceptual similarity, and maintaining appropriate brightness. We further\nimprove the results by applying a local search phase to fine-tune the top\ncandidates from the genetic algorithm. Our approach operates entirely without\npaired training data making it broadly applicable across domains with limited\nor noisy labels. Quantitatively, our model achieves excellent performance with\naverage BRISQUE and NIQE scores of 19.82 and 3.652, respectively, in all\nunpaired datasets. Qualitatively, enhanced images by our model exhibit\nsignificantly improved visibility in shadowed regions, natural balance of\ncontrast and also preserve the richer fine detail without introducing noticable\nartifacts. This work opens new directions for unsupervised image enhancement\nwhere semantic consistency is critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNSGA-II\u7b97\u6cd5\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u548c\u4f3d\u9a6c\u53c2\u6570\uff0c\u5e73\u8861\u89c6\u89c9\u8d28\u91cf\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u591a\u5173\u6ce8\u50cf\u7d20\u7ea7\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u7279\u5f81\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408GPU\u52a0\u901f\u7684NSGA-II\u7b97\u6cd5\u4f18\u5316\u591a\u76ee\u6807\uff08\u56fe\u50cf\u71b5\u3001\u611f\u77e5\u76f8\u4f3c\u6027\u3001\u4eae\u5ea6\uff09\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u641c\u7d22\u5fae\u8c03\u7ed3\u679c\u3002", "result": "\u5728\u65e0\u914d\u5bf9\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5e73\u5747BRISQUE\u548cNIQE\u5206\u6570\u5206\u522b\u4e3a19.82\u548c3.652\uff0c\u589e\u5f3a\u56fe\u50cf\u5728\u9634\u5f71\u533a\u57df\u53ef\u89c1\u6027\u3001\u5bf9\u6bd4\u5ea6\u548c\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u76d1\u7763\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bed\u4e49\u4e00\u81f4\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.11079", "pdf": "https://arxiv.org/pdf/2505.11079", "abs": "https://arxiv.org/abs/2505.11079", "authors": ["Hao Gu", "Jiangyan Yi", "Chenglong Wang", "Jianhua Tao", "Zheng Lian", "Jiayi He", "Yong Ren", "Yujie Chen", "Zhengqi Wen"], "title": "$\\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Audio deepfake detection (ADD) has grown increasingly important due to the\nrise of high-fidelity audio generative models and their potential for misuse.\nGiven that audio large language models (ALLMs) have made significant progress\nin various audio processing tasks, a heuristic question arises: Can ALLMs be\nleveraged to solve ADD?. In this paper, we first conduct a comprehensive\nzero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in\ndetecting fake audio. To enhance their performance, we propose\n$\\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we\nreformulate ADD task as an audio question answering problem, prompting the\nmodel with the question: \"Is this audio fake or real?\". We then perform\nsupervised fine-tuning to enable the ALLM to assess the authenticity of query\naudio. Extensive experiments are conducted to demonstrate that our ALLM-based\nmethod can achieve superior performance in fake audio detection, particularly\nin data-scarce scenarios. As a pioneering study, we anticipate that this work\nwill inspire the research community to leverage ALLMs to develop more effective\nADD systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLM\uff09\u8fdb\u884c\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08ADD\uff09\u7684\u53ef\u80fd\u6027\uff0c\u63d0\u51fa\u4e86ALLM4ADD\u6846\u67b6\uff0c\u901a\u8fc7\u5c06ADD\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u97f3\u9891\u95ee\u7b54\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u9ad8\u4fdd\u771f\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u5174\u8d77\uff0c\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08ADD\uff09\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLM\uff09\u5728ADD\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u8bba\u6587\u9996\u5148\u5bf9ALLM\u5728ADD\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u6548\u679c\u4e0d\u4f73\u3002\u968f\u540e\u63d0\u51fa\u4e86ALLM4ADD\u6846\u67b6\uff0c\u5c06ADD\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u97f3\u9891\u95ee\u7b54\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cALLM4ADD\u6846\u67b6\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5229\u7528ALLM\u5f00\u53d1\u66f4\u6709\u6548\u7684ADD\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f00\u521b\u6027\u7814\u7a76\uff0c\u5e76\u671f\u5f85\u6fc0\u53d1\u66f4\u591a\u76f8\u5173\u7814\u7a76\u3002", "relevance": 70.0}}
{"id": "2505.11100", "pdf": "https://arxiv.org/pdf/2505.11100", "abs": "https://arxiv.org/abs/2505.11100", "authors": ["Lang Feng", "Jiahao Lin", "Dong Xing", "Li Zhang", "De Ma", "Gang Pan"], "title": "Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Population-population generalization is a challenging problem in multi-agent\nreinforcement learning (MARL), particularly when agents encounter unseen\nco-players. However, existing self-play-based methods are constrained by the\nlimitation of inside-space generalization. In this study, we propose\nBidirectional Distillation (BiDist), a novel mixed-play framework, to overcome\nthis limitation in MARL. BiDist leverages knowledge distillation in two\nalternating directions: forward distillation, which emulates the historical\npolicies' space and creates an implicit self-play, and reverse distillation,\nwhich systematically drives agents towards novel distributions outside the\nknown policy space in a non-self-play manner. In addition, BiDist operates as a\nconcise and efficient solution without the need for the complex and costly\nstorage of past policies. We provide both theoretical analysis and empirical\nevidence to support BiDist's effectiveness. Our results highlight its\nremarkable generalization ability across a variety of cooperative, competitive,\nand social dilemma tasks, and reveal that BiDist significantly diversifies the\npolicy distribution space. We also present comprehensive ablation studies to\nreinforce BiDist's effectiveness and key success factors. Source codes are\navailable in the supplementary material.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBiDist\u7684\u53cc\u5411\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7fa4\u4f53\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u6b63\u5411\u548c\u53cd\u5411\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u667a\u80fd\u4f53\u9762\u5bf9\u672a\u89c1\u5408\u4f5c\u8005\u65f6\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u5bf9\u5f08\u65b9\u6cd5\u5c40\u9650\u4e8e\u5185\u90e8\u7a7a\u95f4\u6cdb\u5316\u3002", "method": "BiDist\u6846\u67b6\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\uff08\u6b63\u5411\u84b8\u998f\u6a21\u62df\u5386\u53f2\u7b56\u7565\u7a7a\u95f4\uff0c\u53cd\u5411\u84b8\u998f\u9a71\u52a8\u7b56\u7565\u63a2\u7d22\u65b0\u5206\u5e03\uff09\u5b9e\u73b0\u6cdb\u5316\u63d0\u5347\u3002", "result": "\u5728\u5408\u4f5c\u3001\u7ade\u4e89\u548c\u793e\u4f1a\u56f0\u5883\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u8457\u6269\u5c55\u4e86\u7b56\u7565\u5206\u5e03\u7a7a\u95f4\u3002", "conclusion": "BiDist\u662f\u4e00\u79cd\u7b80\u6d01\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u590d\u6742\u7684\u5386\u53f2\u7b56\u7565\u5b58\u50a8\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "relevance": 70.0}}
{"id": "2505.11257", "pdf": "https://arxiv.org/pdf/2505.11257", "abs": "https://arxiv.org/abs/2505.11257", "authors": ["Giulia Bertazzini", "Daniele Baracchi", "Dasara Shullani", "Isao Echizen", "Alessandro Piva"], "title": "DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The remarkable ease of use of diffusion models for image generation has led\nto a proliferation of synthetic content online. While these models are often\nemployed for legitimate purposes, they are also used to generate fake images\nthat support misinformation and hate speech. Consequently, it is crucial to\ndevelop robust tools capable of detecting whether an image has been generated\nby such models. Many current detection methods, however, require large volumes\nof sample images for training. Unfortunately, due to the rapid evolution of the\nfield, existing datasets often cover only a limited range of models and quickly\nbecome outdated. In this work, we introduce DRAGON, a comprehensive dataset\ncomprising images from 25 diffusion models, spanning both recent advancements\nand older, well-established architectures. The dataset contains a broad variety\nof images representing diverse subjects. To enhance image realism, we propose a\nsimple yet effective pipeline that leverages a large language model to expand\ninput prompts, thereby generating more diverse and higher-quality outputs, as\nevidenced by improvements in standard quality metrics. The dataset is provided\nin multiple sizes (ranging from extra-small to extra-large) to accomodate\ndifferent research scenarios. DRAGON is designed to support the forensic\ncommunity in developing and evaluating detection and attribution techniques for\nsynthetic content. Additionally, the dataset is accompanied by a dedicated test\nset, intended to serve as a benchmark for assessing the performance of newly\ndeveloped methods.", "AI": {"tldr": "DRAGON\u662f\u4e00\u4e2a\u5305\u542b25\u79cd\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u5408\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u548c\u6eaf\u6e90\u6280\u672f\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u5185\u5bb9\u53ef\u80fd\u88ab\u7528\u4e8e\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u68c0\u6d4b\u5de5\u5177\u3002\u73b0\u6709\u6570\u636e\u96c6\u8986\u76d6\u8303\u56f4\u6709\u9650\u4e14\u6613\u8fc7\u65f6\uff0cDRAGON\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u8f93\u5165\u63d0\u793a\u7684\u6d41\u7a0b\uff0c\u751f\u6210\u66f4\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u5e76\u6784\u5efa\u5305\u542b\u591a\u79cd\u6a21\u578b\u548c\u4e3b\u9898\u7684\u6570\u636e\u96c6\u3002", "result": "DRAGON\u6570\u636e\u96c6\u652f\u6301\u4e0d\u540c\u7814\u7a76\u573a\u666f\uff0c\u5e76\u9644\u5e26\u4e13\u7528\u6d4b\u8bd5\u96c6\u4f5c\u4e3a\u65b0\u65b9\u6cd5\u7684\u57fa\u51c6\u3002", "conclusion": "DRAGON\u4e3a\u5408\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u548c\u6eaf\u6e90\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.11154", "pdf": "https://arxiv.org/pdf/2505.11154", "abs": "https://arxiv.org/abs/2505.11154", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Yu Liu", "Wenbo Jiang", "Wenshu Fan", "Qingchuan Zhao", "Guowen Xu"], "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MCP\u534f\u8bae\u4e2d\u7684\u5b89\u5168\u5a01\u80c1MPMA\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\uff08DPMA\u548cGAPMA\uff09\uff0c\u63ed\u793a\u4e86MCP\u751f\u6001\u7cfb\u7edf\u7684\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740MCP\u534f\u8bae\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u7b2c\u4e09\u65b9\u5b9a\u5236\u670d\u52a1\u5668\u53ef\u80fd\u5f15\u53d1\u5b89\u5168\u95ee\u9898\uff0c\u5982MPMA\u653b\u51fb\uff0c\u5f71\u54cdLLM\u5de5\u5177\u751f\u6001\u7684\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\uff1a\u76f4\u63a5\u7684DPMA\u548c\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684GAPMA\uff0c\u540e\u8005\u901a\u8fc7\u4f18\u5316\u63cf\u8ff0\u589e\u5f3a\u9690\u853d\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGAPMA\u5728\u653b\u51fb\u6548\u679c\u548c\u9690\u853d\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u63ed\u793a\u4e86MCP\u751f\u6001\u7684\u6f5c\u5728\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86MCP\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u547c\u5401\u5f00\u53d1\u9632\u5fa1\u673a\u5236\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u3002", "relevance": 85.0}}
{"id": "2505.11106", "pdf": "https://arxiv.org/pdf/2505.11106", "abs": "https://arxiv.org/abs/2505.11106", "authors": ["Thanadej Rattanakornphan", "Piyanon Charoenpoonpanich", "Chainarong Amornbunchornvej"], "title": "Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series", "categories": ["cs.LG", "cs.AI", "cs.DB", "stat.ME"], "comment": "Under review", "summary": "Finding the most similar subsequences between two multidimensional time\nseries has many applications: e.g. capturing dependency in stock market or\ndiscovering coordinated movement of baboons. Considering one pattern occurring\nin one time series, we might be wondering whether the same pattern occurs in\nanother time series with some distortion that might have a different length.\nNevertheless, to the best of our knowledge, there is no efficient framework\nthat deals with this problem yet. In this work, we propose an algorithm that\nprovides the exact solution of finding the most similar multidimensional\nsubsequences between time series where there is a difference in length both\nbetween time series and between subsequences. The algorithm is built based on\ntheoretical guarantee of correctness and efficiency. The result in simulation\ndatasets illustrated that our approach not just only provided correct solution,\nbut it also utilized running time only quarter of time compared against the\nbaseline approaches. In real-world datasets, it extracted the most similar\nsubsequences even faster (up to 20 times faster against baseline methods) and\nprovided insights regarding the situation in stock market and following\nrelations of multidimensional time series of baboon movement. Our approach can\nbe used for any time series. The code and datasets of this work are provided\nfor the public use.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u4e2d\u627e\u5230\u6700\u76f8\u4f3c\u7684\u5b50\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u957f\u5ea6\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u4e2d\u5b50\u5e8f\u5217\u957f\u5ea6\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u91d1\u878d\u548c\u751f\u6001\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u7406\u8bba\u4fdd\u8bc1\u7684\u7b97\u6cd5\uff0c\u786e\u4fdd\u6b63\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e2d\u8fd0\u884c\u65f6\u95f4\u4ec5\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u7684\u56db\u5206\u4e4b\u4e00\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u5feb\u8fbe20\u500d\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u80a1\u7968\u5e02\u573a\u548c\u72d2\u72d2\u8fd0\u52a8\u5206\u6790\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002", "relevance": 30.0}}
{"id": "2505.11264", "pdf": "https://arxiv.org/pdf/2505.11264", "abs": "https://arxiv.org/abs/2505.11264", "authors": ["Mohamed Ali Chebbi", "Ewelina Rupnik", "Paul Lopes", "Marc Pierrot-Deseilligny"], "title": "Multi-view dense image matching with similarity learning and geometry priors", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks\ndesigned for multi-view similarity learning, leveraging epipolar geometry for\ntraining. Our approach incorporates an online geometry prior to characterize\npixel relationships, either along the epipolar line or through homography\nrectification. This enables the generation of geometry-aware features from\nnative images, which are then projected across candidate depth hypotheses using\nplane sweeping. Our method geometric preconditioning effectively adapts\nepipolar-based features for enhanced multi-view reconstruction, without\nrequiring the laborious multi-view training dataset creation. By aggregating\nlearned similarities, we construct and regularize the cost volume, leading to\nimproved multi-view surface reconstruction over traditional dense matching\napproaches. MV-DeepSimNets demonstrates superior performance against leading\nsimilarity learning networks and end-to-end regression models, especially in\nterms of generalization capabilities across both aerial and satellite imagery\nwith varied ground sampling distances. Our pipeline is integrated into MicMac\nsoftware and can be readily adopted in standard multi-resolution image matching\npipelines.", "AI": {"tldr": "MV-DeepSimNets\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u89c6\u56fe\u76f8\u4f3c\u6027\u5b66\u4e60\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5957\u4ef6\uff0c\u5229\u7528\u6781\u7ebf\u51e0\u4f55\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u7e41\u7410\u7684\u591a\u89c6\u56fe\u6570\u636e\u96c6\u521b\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u56fe\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u89c6\u56fe\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u5339\u914d\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0cMV-DeepSimNets\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u548c\u5728\u7ebf\u51e0\u4f55\u5148\u9a8c\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u6781\u7ebf\u51e0\u4f55\u548c\u5355\u5e94\u6027\u6821\u6b63\u751f\u6210\u51e0\u4f55\u611f\u77e5\u7279\u5f81\uff0c\u901a\u8fc7\u5e73\u9762\u626b\u63cf\u6295\u5f71\u5230\u5019\u9009\u6df1\u5ea6\u5047\u8bbe\uff0c\u805a\u5408\u76f8\u4f3c\u6027\u5e76\u6b63\u5219\u5316\u6210\u672c\u4f53\u79ef\u3002", "result": "\u5728\u822a\u7a7a\u548c\u536b\u661f\u56fe\u50cf\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u76f8\u4f3c\u6027\u5b66\u4e60\u548c\u7aef\u5230\u7aef\u56de\u5f52\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MV-DeepSimNets\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u591a\u89c6\u56fe\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u5339\u914d\u6d41\u7a0b\u3002", "relevance": 20.0}}
{"id": "2505.11165", "pdf": "https://arxiv.org/pdf/2505.11165", "abs": "https://arxiv.org/abs/2505.11165", "authors": ["Haiqing Hao", "Nikola Zubi\u0107", "Weihua He", "Zhipeng Sui", "Davide Scaramuzza", "Wenhui Wang"], "title": "Maximizing Asynchronicity in Event-based Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 figures, 9 tables", "summary": "Event cameras deliver visual data with high temporal resolution, low latency,\nand minimal redundancy, yet their asynchronous, sparse sequential nature\nchallenges standard tensor-based machine learning (ML). While the recent\nasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by\nasynchronously encoding events into learned representations for ML pipelines,\nexisting A2S approaches often sacrifice representation expressivity and\ngeneralizability compared to dense, synchronous methods. This paper introduces\nEVA (EVent Asynchronous representation learning), a novel A2S framework to\ngenerate highly expressive and generalizable event-by-event representations.\nInspired by the analogy between events and language, EVA uniquely adapts\nadvances from language modeling in linear attention and self-supervised\nlearning for its construction. In demonstration, EVA outperforms prior A2S\nmethods on recognition tasks (DVS128-Gesture and N-Cars), and represents the\nfirst A2S framework to successfully master demanding detection tasks, achieving\na remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's\ntransformative potential for advancing real-time event-based vision\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEVA\u7684\u5f02\u6b65\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u501f\u9274\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\u4f7f\u5176\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5f02\u6b65\u7a00\u758f\u6570\u636e\u96be\u4ee5\u4e0e\u6807\u51c6ML\u65b9\u6cd5\u517c\u5bb9\u3002\u73b0\u6709A2S\u65b9\u6cd5\u5728\u8868\u793a\u80fd\u529b\u548c\u6cdb\u5316\u6027\u4e0a\u4e0d\u8db3\u3002", "method": "EVA\u6846\u67b6\u501f\u9274\u8bed\u8a00\u5efa\u6a21\u6280\u672f\uff0c\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u751f\u6210\u9ad8\u8868\u8fbe\u6027\u548c\u6cdb\u5316\u6027\u7684\u4e8b\u4ef6\u8868\u793a\u3002", "result": "EVA\u5728\u8bc6\u522b\u4efb\u52a1\uff08DVS128-Gesture\u548cN-Cars\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709A2S\u65b9\u6cd5\uff0c\u5e76\u5728\u68c0\u6d4b\u4efb\u52a1\uff08Gen1\u6570\u636e\u96c6\uff09\u4e0a\u8fbe\u523047.7 mAP\u3002", "conclusion": "EVA\u4e3a\u5b9e\u65f6\u4e8b\u4ef6\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.10732", "pdf": "https://arxiv.org/pdf/2505.10732", "abs": "https://arxiv.org/abs/2505.10732", "authors": ["Jia Hui Chin", "Pu Zhang", "Yu Xin Cheong", "Jonathan Pan"], "title": "Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "In the current rapidly changing digital environment, businesses are under\nconstant stress to ensure that their systems are secured. Security audits help\nto maintain a strong security posture by ensuring that policies are in place,\ncontrols are implemented, gaps are identified for cybersecurity risks\nmitigation. However, audits are usually manual, requiring much time and costs.\nThis paper looks at the possibility of developing a framework to leverage Large\nLanguage Models (LLMs) as an autonomous agent to execute part of the security\naudit, namely with the field audit. password policy compliance for Windows\noperating system. Through the conduct of an exploration experiment of using\nGPT-4 with Langchain, the agent executed the audit tasks by accurately flagging\npassword policy violations and appeared to be more efficient than traditional\nmanual audits. Despite its potential limitations in operational consistency in\ncomplex and dynamic environment, the framework suggests possibilities to extend\nfurther to real-time threat monitoring and compliance checks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528LLM\uff08\u5982GPT-4\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u6267\u884c\u90e8\u5206\u5b89\u5168\u5ba1\u8ba1\u4efb\u52a1\uff08\u5982\u5bc6\u7801\u7b56\u7565\u5408\u89c4\u6027\u5ba1\u8ba1\uff09\u7684\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u7387\u4f18\u4e8e\u4f20\u7edf\u624b\u52a8\u5ba1\u8ba1\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u5ba1\u8ba1\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u5728\u81ea\u52a8\u5316\u5ba1\u8ba1\u4e2d\u7684\u6f5c\u529b\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528GPT-4\u4e0eLangchain\u6784\u5efa\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728Windows\u5bc6\u7801\u7b56\u7565\u5408\u89c4\u6027\u5ba1\u8ba1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u4ee3\u7406\u80fd\u51c6\u786e\u6807\u8bb0\u5bc6\u7801\u7b56\u7565\u8fdd\u89c4\uff0c\u6548\u7387\u9ad8\u4e8e\u624b\u52a8\u5ba1\u8ba1\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u80fd\u5b58\u5728\u64cd\u4f5c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4f46\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5b9e\u65f6\u5a01\u80c1\u76d1\u63a7\u548c\u5408\u89c4\u68c0\u67e5\u3002", "relevance": 60.0}}
{"id": "2505.11111", "pdf": "https://arxiv.org/pdf/2505.11111", "abs": "https://arxiv.org/abs/2505.11111", "authors": ["Lin Zhu", "Yijun Bian", "Lei You"], "title": "FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "3 figures, 15 pages", "summary": "Ensuring fairness in machine learning models is critical, particularly in\nhigh-stakes domains where biased decisions can lead to serious societal\nconsequences. Existing preprocessing approaches generally lack transparent\nmechanisms for identifying which features or instances are responsible for\nunfairness. This obscures the rationale behind data modifications. We introduce\nFairSHAP, a novel pre-processing framework that leverages Shapley value\nattribution to improve both individual and group fairness. FairSHAP identifies\nfairness-critical instances in the training data using an interpretable measure\nof feature importance, and systematically modifies them through instance-level\nmatching across sensitive groups. This process reduces discriminative risk - an\nindividual fairness metric - while preserving data integrity and model\naccuracy. We demonstrate that FairSHAP significantly improves demographic\nparity and equality of opportunity across diverse tabular datasets, achieving\nfairness gains with minimal data perturbation and, in some cases, improved\npredictive performance. As a model-agnostic and transparent method, FairSHAP\nintegrates seamlessly into existing machine learning pipelines and provides\nactionable insights into the sources of bias.Our code is on\nhttps://github.com/youlei202/FairSHAP.", "AI": {"tldr": "FairSHAP\u662f\u4e00\u4e2a\u57fa\u4e8eShapley\u503c\u7684\u9884\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u900f\u660e\u5730\u8bc6\u522b\u548c\u4fee\u6539\u4e0d\u516c\u5e73\u7684\u5b9e\u4f8b\u548c\u6570\u636e\u7279\u5f81\uff0c\u63d0\u5347\u4e2a\u4f53\u548c\u7fa4\u4f53\u516c\u5e73\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9884\u5904\u7406\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u96be\u4ee5\u8bc6\u522b\u4e0d\u516c\u5e73\u7684\u6765\u6e90\u3002", "method": "FairSHAP\u5229\u7528Shapley\u503c\u8bc6\u522b\u4e0d\u516c\u5e73\u5b9e\u4f8b\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u5339\u914d\u4fee\u6539\u6570\u636e\uff0c\u51cf\u5c11\u6b67\u89c6\u6027\u98ce\u9669\u3002", "result": "\u5728\u591a\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e2d\uff0cFairSHAP\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u53e3\u5747\u7b49\u548c\u673a\u4f1a\u5747\u7b49\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u5b8c\u6574\u6027\u548c\u6a21\u578b\u51c6\u786e\u6027\u3002", "conclusion": "FairSHAP\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u4e14\u900f\u660e\u7684\u65b9\u6cd5\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u504f\u89c1\u6765\u6e90\u5206\u6790\u3002", "relevance": 75.0}}
{"id": "2505.11267", "pdf": "https://arxiv.org/pdf/2505.11267", "abs": "https://arxiv.org/abs/2505.11267", "authors": ["Wuzhou Quan", "Mingqiang Wei", "Jinhui Tang"], "title": "Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Hyperspectral image (HSI) representation is fundamentally challenged by\npervasive non-uniformity, where spectral dependencies, spatial continuity, and\nfeature efficiency exhibit complex and often conflicting behaviors. Most\nexisting models rely on a unified processing paradigm that assumes homogeneity\nacross dimensions, leading to suboptimal performance and biased\nrepresentations. To address this, we propose FairHyp, a fairness-directed\nframework that explicitly disentangles and resolves the threefold\nnon-uniformity through cooperative yet specialized modules. We introduce a\nRunge-Kutta-inspired spatial variability adapter to restore spatial coherence\nunder resolution discrepancies, a multi-receptive field convolution module with\nsparse-aware refinement to enhance discriminative features while respecting\ninherent sparsity, and a spectral-context state space model that captures\nstable and long-range spectral dependencies via bidirectional Mamba scanning\nand statistical aggregation. Unlike one-size-fits-all solutions, FairHyp\nachieves dimension-specific adaptation while preserving global consistency and\nmutual reinforcement. This design is grounded in the view that non-uniformity\narises from the intrinsic structure of HSI representations, rather than any\nparticular task setting. To validate this, we apply FairHyp across four\nrepresentative tasks including classification, denoising, super-resolution, and\ninpaintin, demonstrating its effectiveness in modeling a shared structural\nflaw. Extensive experiments show that FairHyp consistently outperforms\nstate-of-the-art methods under varied imaging conditions. Our findings redefine\nfairness as a structural necessity in HSI modeling and offer a new paradigm for\nbalancing adaptability, efficiency, and fidelity in high-dimensional vision\ntasks.", "AI": {"tldr": "FairHyp\u662f\u4e00\u4e2a\u9488\u5bf9\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u8868\u793a\u4e2d\u975e\u5747\u5300\u6027\u95ee\u9898\u7684\u516c\u5e73\u6027\u5bfc\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u6a21\u5757\u89e3\u51b3\u7a7a\u95f4\u3001\u5149\u8c31\u548c\u7279\u5f81\u7684\u51b2\u7a81\u884c\u4e3a\u3002", "motivation": "HSI\u8868\u793a\u4e2d\u5b58\u5728\u590d\u6742\u7684\u975e\u5747\u5300\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5047\u8bbe\u7ef4\u5ea6\u540c\u8d28\u6027\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0cFairHyp\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "FairHyp\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aRunge-Kutta\u542f\u53d1\u7684\u7a7a\u95f4\u9002\u914d\u5668\u3001\u591a\u611f\u53d7\u91ce\u5377\u79ef\u6a21\u5757\u548c\u53cc\u5411Mamba\u626b\u63cf\u7684\u5149\u8c31\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u5728\u5206\u7c7b\u3001\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\uff0cFairHyp\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FairHyp\u91cd\u65b0\u5b9a\u4e49\u4e86HSI\u5efa\u6a21\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u4e3a\u9ad8\u7ef4\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "relevance": 40.0}}
{"id": "2505.10746", "pdf": "https://arxiv.org/pdf/2505.10746", "abs": "https://arxiv.org/abs/2505.10746", "authors": ["Matthew Stoffolano", "Ayush Rout", "Justin M. Pelletier"], "title": "ChestyBot: Detecting and Disrupting Chinese Communist Party Influence Stratagems", "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.SI"], "comment": "Presented at USCYBERCOM Cyber Recon Symposium 2023 at DreamPort in\n  Columbia, MD on April 20, 2023", "summary": "Foreign information operations conducted by Russian and Chinese actors\nexploit the United States' permissive information environment. These campaigns\nthreaten democratic institutions and the broader Westphalian model. Yet,\nexisting detection and mitigation strategies often fail to identify active\ninformation campaigns in real time. This paper introduces ChestyBot, a\npragmatics-based language model that detects unlabeled foreign malign influence\ntweets with up to 98.34% accuracy. The model supports a novel framework to\ndisrupt foreign influence operations in their formative stages.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ChestyBot\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bed\u7528\u5b66\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u672a\u6807\u8bb0\u7684\u5916\u56fd\u6076\u610f\u5f71\u54cd\u63a8\u6587\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe98.34%\u3002", "motivation": "\u4fc4\u7f57\u65af\u548c\u4e2d\u56fd\u7b49\u5916\u56fd\u884c\u4e3a\u8005\u5229\u7528\u7f8e\u56fd\u5bbd\u677e\u7684\u4fe1\u606f\u73af\u5883\u8fdb\u884c\u4fe1\u606f\u64cd\u4f5c\uff0c\u5a01\u80c1\u6c11\u4e3b\u5236\u5ea6\u548c\u5a01\u65af\u7279\u4f10\u5229\u4e9a\u6a21\u5f0f\u3002\u73b0\u6709\u68c0\u6d4b\u548c\u7f13\u89e3\u7b56\u7565\u65e0\u6cd5\u5b9e\u65f6\u8bc6\u522b\u8fd9\u4e9b\u6d3b\u52a8\u3002", "method": "\u63d0\u51faChestyBot\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bed\u7528\u5b66\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u5916\u56fd\u6076\u610f\u5f71\u54cd\u63a8\u6587\u3002", "result": "ChestyBot\u68c0\u6d4b\u672a\u6807\u8bb0\u63a8\u6587\u7684\u51c6\u786e\u7387\u8fbe\u523098.34%\u3002", "conclusion": "ChestyBot\u652f\u6301\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u53ef\u5728\u5916\u56fd\u5f71\u54cd\u64cd\u4f5c\u5f62\u6210\u9636\u6bb5\u8fdb\u884c\u5e72\u6270\u3002", "relevance": 30.0}}
{"id": "2505.11117", "pdf": "https://arxiv.org/pdf/2505.11117", "abs": "https://arxiv.org/abs/2505.11117", "authors": ["Chenhong Zhou", "Jie Chen", "Zaifeng Yang", "Ching Eng Png"], "title": "Dual-Balancing for Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "Accepted at IJCAI 2025 (34th International Joint Conference on\n  Artificial Intelligence)", "summary": "Physics-informed neural networks (PINNs) have emerged as a new learning\nparadigm for solving partial differential equations (PDEs) by enforcing the\nconstraints of physical equations, boundary conditions (BCs), and initial\nconditions (ICs) into the loss function. Despite their successes, vanilla PINNs\nstill suffer from poor accuracy and slow convergence due to the intractable\nmulti-objective optimization issue. In this paper, we propose a novel\nDual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by\nintegrating inter-balancing and intra-balancing to alleviate two imbalance\nissues in PINNs. Inter-balancing aims to mitigate the gradient imbalance\nbetween PDE residual loss and condition-fitting losses by determining an\naggregated weight that offsets their gradient distribution discrepancies.\nIntra-balancing acts on condition-fitting losses to tackle the imbalance in\nfitting difficulty across diverse conditions. By evaluating the fitting\ndifficulty based on the loss records, intra-balancing can allocate the\naggregated weight proportionally to each condition loss according to its\nfitting difficulty levels. We further introduce a robust weight update strategy\nto prevent abrupt spikes and arithmetic overflow in instantaneous weight values\ncaused by large loss variances, enabling smooth weight updating and stable\ntraining. Extensive experiments demonstrate that DB-PINN achieves significantly\nsuperior performance than those popular gradient-based weighting methods in\nterms of convergence speed and prediction accuracy. Our code and supplementary\nmaterial are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u5e73\u8861PINN\uff08DB-PINN\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86PINN\u7684\u6536\u655b\u901f\u5ea6\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfPINN\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u5b58\u5728\u7cbe\u5ea6\u4f4e\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DB-PINN\u901a\u8fc7\u96c6\u6210\u5185\u5e73\u8861\u548c\u5916\u5e73\u8861\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u5916\u5e73\u8861\u89e3\u51b3PDE\u6b8b\u5dee\u635f\u5931\u4e0e\u6761\u4ef6\u62df\u5408\u635f\u5931\u4e4b\u95f4\u7684\u68af\u5ea6\u4e0d\u5e73\u8861\uff0c\u5185\u5e73\u8861\u89e3\u51b3\u4e0d\u540c\u6761\u4ef6\u62df\u5408\u96be\u5ea6\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDB-PINN\u5728\u6536\u655b\u901f\u5ea6\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u68af\u5ea6\u7684\u6743\u91cd\u8c03\u6574\u65b9\u6cd5\u3002", "conclusion": "DB-PINN\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u8c03\u6574\u6709\u6548\u89e3\u51b3\u4e86PINN\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.11282", "pdf": "https://arxiv.org/pdf/2505.11282", "abs": "https://arxiv.org/abs/2505.11282", "authors": ["Shrutarv Awasthi", "Anas Gouda", "Sven Franke", "J\u00e9r\u00f4me Rutinowski", "Frank Hoffmann", "Moritz Roidl"], "title": "MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025 Workshop on Event-based Vision", "summary": "Mobile robots are reaching unprecedented speeds, with platforms like Unitree\nB2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s.\nHowever, effectively utilizing such speeds remains a challenge due to the\nlimitations of RGB cameras, which suffer from motion blur and fail to provide\nreal-time responsiveness. Event cameras, with their asynchronous operation, and\nlow-latency sensing, offer a promising alternative for high-speed robotic\nperception. In this work, we introduce MTevent, a dataset designed for 6D pose\nestimation and moving object detection in highly dynamic environments with\nlarge detection distances. Our setup consists of a stereo-event camera and an\nRGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16\nunique objects under challenging conditions such as extreme viewing angles,\nvarying lighting, and occlusions. MTevent is the first dataset to combine\nhigh-speed motion, long-range perception, and real-world object interactions,\nmaking it a valuable resource for advancing event-based vision in robotics. To\nestablish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's\nFoundationPose on RGB images, achieving an Average Recall of 0.22 with\nground-truth masks, highlighting the limitations of RGB-based approaches in\nsuch dynamic settings. With MTevent, we provide a novel resource to improve\nperception models and foster further research in high-speed robotic vision. The\ndataset is available for download\nhttps://huggingface.co/datasets/anas-gouda/MTevent", "AI": {"tldr": "MTevent\u6570\u636e\u96c6\u4e13\u6ce8\u4e8e\u9ad8\u901f\u673a\u5668\u4eba\u611f\u77e5\uff0c\u7ed3\u5408\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\uff0c\u4e3a6D\u59ff\u6001\u4f30\u8ba1\u548c\u52a8\u6001\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u8d44\u6e90\uff0c\u586b\u8865\u4e86RGB\u76f8\u673a\u5728\u9ad8\u901f\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u9ad8\u901f\u79fb\u52a8\u673a\u5668\u4eba\u9762\u4e34RGB\u76f8\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff08\u5982\u8fd0\u52a8\u6a21\u7cca\u548c\u5ef6\u8fdf\uff09\uff0c\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u5f02\u6b65\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6784\u5efaMTevent\u6570\u636e\u96c6\uff0c\u5305\u542b75\u4e2a\u573a\u666f\uff0c\u7ed3\u5408\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\uff0c\u6355\u6349\u9ad8\u901f\u8fd0\u52a8\u548c\u957f\u8ddd\u79bb\u611f\u77e5\u3002", "result": "\u57286D\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0cRGB\u65b9\u6cd5\uff08FoundationPose\uff09\u7684\u5e73\u5747\u53ec\u56de\u7387\u4ec5\u4e3a0.22\uff0c\u51f8\u663e\u5176\u5c40\u9650\u6027\u3002", "conclusion": "MTevent\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u9ad8\u901f\u673a\u5668\u4eba\u89c6\u89c9\u4e2d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u63a8\u52a8\u611f\u77e5\u6a21\u578b\u7684\u6539\u8fdb\u3002", "relevance": 30.0}}
{"id": "2505.11183", "pdf": "https://arxiv.org/pdf/2505.11183", "abs": "https://arxiv.org/abs/2505.11183", "authors": ["Jacob Trauger", "Ambuj Tewari"], "title": "On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms", "categories": ["stat.ML", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "Probabilistic next-token prediction trained using cross-entropy loss is the\nbasis of most large language models. Given a sequence of previous values,\nnext-token prediction assigns a probability to each possible next value in the\nvocabulary. There are many ways to use next-token prediction to output token\nsequences. This paper examines a few of these algorithms (greedy, lookahead,\nrandom sampling, and temperature-scaled random sampling) and studies their\nconsistency with respect to various goals encoded as loss functions. Although\nconsistency of surrogate losses with respect to a target loss function is a\nwell researched topic, we are the first to study it in the context of LLMs (to\nthe best of our knowledge). We find that, so long as next-token prediction\nconverges to its true probability distribution, random sampling is consistent\nwith outputting sequences that mimic sampling from the true probability\ndistribution. For the other goals, such as minimizing the 0-1 loss on the\nentire sequence, we show no polynomial-time algorithm is optimal for all\nprobability distributions and all decoding algorithms studied are only optimal\nfor a subset of probability distributions. When analyzing these results, we see\nthat there is a dichotomy created between the goals of information retrieval\nand creative generation for the decoding algorithms. This shows that choosing\nthe correct decoding algorithm based on the desired goal is extremely important\nand many of the ones used are lacking theoretical grounding in numerous\nscenarios.", "AI": {"tldr": "\u7814\u7a76\u4e86LLM\u4e2d\u4e0d\u540c\u89e3\u7801\u7b97\u6cd5\uff08\u8d2a\u5a6a\u3001\u524d\u77bb\u3001\u968f\u673a\u91c7\u6837\u3001\u6e29\u5ea6\u7f29\u653e\u968f\u673a\u91c7\u6837\uff09\u4e0e\u76ee\u6807\u635f\u5931\u51fd\u6570\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u968f\u673a\u91c7\u6837\u5728\u6a21\u62df\u771f\u5b9e\u6982\u7387\u5206\u5e03\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5176\u4ed6\u7b97\u6cd5\u4ec5\u5bf9\u90e8\u5206\u5206\u5e03\u6700\u4f18\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u89e3\u7801\u7b97\u6cd5\u5728LLM\u4e2d\u4e0e\u76ee\u6807\u635f\u5931\u51fd\u6570\u7684\u4e00\u81f4\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5206\u6790\u591a\u79cd\u89e3\u7801\u7b97\u6cd5\uff08\u8d2a\u5a6a\u3001\u524d\u77bb\u3001\u968f\u673a\u91c7\u6837\u3001\u6e29\u5ea6\u7f29\u653e\u968f\u673a\u91c7\u6837\uff09\u5728\u4e0d\u540c\u76ee\u6807\u635f\u5931\u51fd\u6570\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u968f\u673a\u91c7\u6837\u5728\u6a21\u62df\u771f\u5b9e\u6982\u7387\u5206\u5e03\u65f6\u4e00\u81f4\uff0c\u5176\u4ed6\u7b97\u6cd5\u4ec5\u5bf9\u90e8\u5206\u5206\u5e03\u6700\u4f18\u3002\u89e3\u7801\u7b97\u6cd5\u7684\u9009\u62e9\u9700\u57fa\u4e8e\u76ee\u6807\uff08\u5982\u4fe1\u606f\u68c0\u7d22\u6216\u521b\u610f\u751f\u6210\uff09\u3002", "conclusion": "\u89e3\u7801\u7b97\u6cd5\u7684\u9009\u62e9\u5bf9LLM\u8f93\u51fa\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u7406\u8bba\u57fa\u7840\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "relevance": 85.0}}
{"id": "2505.10770", "pdf": "https://arxiv.org/pdf/2505.10770", "abs": "https://arxiv.org/abs/2505.10770", "authors": ["Ebasa Temesgen", "Mario Jerez", "Greta Brown", "Graham Wilson", "Sree Ganesh Lalitaditya Divakarla", "Sarah Boelter", "Oscar Nelson", "Robert McPherson", "Maria Gini"], "title": "Geofenced Unmanned Aerial Robotic Defender for Deer Detection and Deterrence (GUARD)", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "Wildlife-induced crop damage, particularly from deer, threatens agricultural\nproductivity. Traditional deterrence methods often fall short in scalability,\nresponsiveness, and adaptability to diverse farmland environments. This paper\npresents an integrated unmanned aerial vehicle (UAV) system designed for\nautonomous wildlife deterrence, developed as part of the Farm Robotics\nChallenge. Our system combines a YOLO-based real-time computer vision module\nfor deer detection, an energy-efficient coverage path planning algorithm for\nefficient field monitoring, and an autonomous charging station for continuous\noperation of the UAV. In collaboration with a local Minnesota farmer, the\nsystem is tailored to address practical constraints such as terrain,\ninfrastructure limitations, and animal behavior. The solution is evaluated\nthrough a combination of simulation and field testing, demonstrating robust\ndetection accuracy, efficient coverage, and extended operational time. The\nresults highlight the feasibility and effectiveness of drone-based wildlife\ndeterrence in precision agriculture, offering a scalable framework for future\ndeployment and extension.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u91ce\u751f\u52a8\u7269\u9a71\u8d76\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u9ad8\u6548\u8def\u5f84\u89c4\u5212\u548c\u81ea\u4e3b\u5145\u7535\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u91ce\u751f\u52a8\u7269\u9a71\u8d76\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u54cd\u5e94\u6027\u548c\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5a01\u80c1\u519c\u4e1a\u751f\u4ea7\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86YOLO\u5b9e\u65f6\u68c0\u6d4b\u3001\u9ad8\u6548\u8def\u5f84\u89c4\u5212\u548c\u81ea\u4e3b\u5145\u7535\u7ad9\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u5c55\u793a\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3001\u9ad8\u6548\u8986\u76d6\u548c\u957f\u7eed\u822a\u80fd\u529b\u3002", "conclusion": "\u65e0\u4eba\u673a\u9a71\u8d76\u7cfb\u7edf\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u53ef\u884c\u4e14\u6709\u6548\uff0c\u4e3a\u672a\u6765\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002", "relevance": 20.0}}
{"id": "2505.11125", "pdf": "https://arxiv.org/pdf/2505.11125", "abs": "https://arxiv.org/abs/2505.11125", "authors": ["Enjun Du", "Siyi Liu", "Yongqi Zhang"], "title": "GraphOracle: A Foundation Model for Knowledge Graph Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Foundation models have demonstrated remarkable capabilities across various\ndomains, but developing analogous models for knowledge graphs presents unique\nchallenges due to their dynamic nature and the need for cross-domain reasoning.\nTo address these issues, we introduce \\textbf{\\textsc{GraphOracle}}, a\nrelation-centric foundation model that unifies reasoning across knowledge\ngraphs by converting them into Relation-Dependency Graphs (RDG), explicitly\nencoding compositional patterns with fewer edges than prior methods. A\nquery-dependent attention mechanism is further developed to learn inductive\nrepresentations for both relations and entities. Pre-training on diverse\nknowledge graphs, followed by minutes-level fine-tuning, enables effective\ngeneralization to unseen entities, relations, and entire graphs. Through\ncomprehensive experiments on 31 diverse benchmarks spanning transductive,\ninductive, and cross-domain settings, we demonstrate consistent\nstate-of-the-art performance with minimal adaptation, improving the prediction\nperformance by up to 35\\% compared to the strongest baselines.", "AI": {"tldr": "GraphOracle\u662f\u4e00\u4e2a\u9762\u5411\u77e5\u8bc6\u56fe\u8c31\u7684\u5173\u7cfb\u4e2d\u5fc3\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5173\u7cfb\u4f9d\u8d56\u56fe\uff08RDG\uff09\u7edf\u4e00\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u7684\u52a8\u6001\u6027\u548c\u8de8\u9886\u57df\u63a8\u7406\u9700\u6c42\u4f7f\u5f97\u5f00\u53d1\u7c7b\u4f3c\u57fa\u7840\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0cGraphOracle\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\u4e3a\u5173\u7cfb\u4f9d\u8d56\u56fe\uff08RDG\uff09\uff0c\u5f00\u53d1\u67e5\u8be2\u4f9d\u8d56\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9884\u8bad\u7ec3\u540e\u5feb\u901f\u5fae\u8c03\u3002", "result": "\u572831\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe35%\u3002", "conclusion": "GraphOracle\u5728\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11293", "pdf": "https://arxiv.org/pdf/2505.11293", "abs": "https://arxiv.org/abs/2505.11293", "authors": ["Raghuveer Thirukovalluru", "Rui Meng", "Ye Liu", "Karthikeyan K", "Mingyi Su", "Ping Nie", "Semih Yavuz", "Yingbo Zhou", "Wenhu Chen", "Bhuwan Dhingra"], "title": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining", "categories": ["cs.CV"], "comment": "14 pages, 4 figures", "summary": "Contrastive learning (CL) is a prevalent technique for training embedding\nmodels, which pulls semantically similar examples (positives) closer in the\nrepresentation space while pushing dissimilar ones (negatives) further apart. A\nkey source of negatives are 'in-batch' examples, i.e., positives from other\nexamples in the batch. Effectiveness of such models is hence strongly\ninfluenced by the size and quality of training batches. In this work, we\npropose 'Breaking the Batch Barrier' (B3), a novel batch construction strategy\ndesigned to curate high-quality batches for CL. Our approach begins by using a\npretrained teacher embedding model to rank all examples in the dataset, from\nwhich a sparse similarity graph is constructed. A community detection algorithm\nis then applied to this graph to identify clusters of examples that serve as\nstrong negatives for one another. The clusters are then used to construct\nbatches that are rich in in-batch negatives. Empirical results on the MMEB\nmultimodal embedding benchmark (36 tasks) demonstrate that our method sets a\nnew state of the art, outperforming previous best methods by +1.3 and +2.9\npoints at the 7B and 2B model scales, respectively. Notably, models trained\nwith B3 surpass existing state-of-the-art results even with a batch size as\nsmall as 64, which is 4-16x smaller than that required by other methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Breaking the Batch Barrier'\uff08B3\uff09\u7684\u65b0\u6279\u6b21\u6784\u5efa\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u548c\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u6279\u6b21\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u7684\u6548\u679c\u53d7\u6279\u6b21\u5927\u5c0f\u548c\u8d28\u91cf\u5f71\u54cd\u8f83\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u6279\u6b21\u624d\u80fd\u53d6\u5f97\u597d\u6548\u679c\u3002B3\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6279\u6b21\u6784\u5efa\uff0c\u51cf\u5c11\u5bf9\u5927\u6279\u6b21\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u5bf9\u6570\u636e\u96c6\u4e2d\u7684\u6837\u672c\u8fdb\u884c\u6392\u5e8f\uff0c\u6784\u5efa\u7a00\u758f\u76f8\u4f3c\u56fe\uff0c\u5e76\u901a\u8fc7\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\u8bc6\u522b\u6837\u672c\u7c07\uff0c\u7528\u4e8e\u6784\u5efa\u5bcc\u542b\u8d1f\u6837\u672c\u7684\u6279\u6b21\u3002", "result": "\u5728MMEB\u591a\u6a21\u6001\u5d4c\u5165\u57fa\u51c6\uff0836\u4e2a\u4efb\u52a1\uff09\u4e0a\uff0cB3\u65b9\u6cd5\u57287B\u548c2B\u6a21\u578b\u89c4\u6a21\u4e0a\u5206\u522b\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa1.3\u548c2.9\u5206\uff0c\u4e14\u4ec5\u970064\u7684\u5c0f\u6279\u6b21\u3002", "conclusion": "B3\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u6279\u6b21\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6bd4\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u6279\u6b21\u7684\u4f9d\u8d56\u3002", "relevance": 60.0}}
{"id": "2505.11200", "pdf": "https://arxiv.org/pdf/2505.11200", "abs": "https://arxiv.org/abs/2505.11200", "authors": ["Xihuai Wang", "Ziyi Zhao", "Siyu Ren", "Shao Zhang", "Song Li", "Xiaoyu Li", "Ziwen Wang", "Lin Qiu", "Guanglu Wan", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang"], "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "comment": "Under Review", "summary": "Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAudio Turing Test (ATT)\u7684\u591a\u7ef4\u4e2d\u6587\u8bed\u6599\u5e93\u6570\u636e\u96c6ATT-Corpus\uff0c\u7528\u4e8e\u6539\u8fdbTTS\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177Auto-ATT\u3002", "motivation": "\u73b0\u6709\u7684TTS\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982MOS\uff09\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u6587TTS\u8bc4\u4f30\u4e2d\u7f3a\u4e4f\u591a\u7ef4\u8bbe\u8ba1\u3002", "method": "\u5f15\u5165ATT-Corpus\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u56fe\u7075\u6d4b\u8bd5\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u540c\u65f6\u5fae\u8c03Qwen2-Audio-Instruct\u6a21\u578b\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177Auto-ATT\u3002", "result": "ATT\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u5728\u7279\u5b9a\u80fd\u529b\u7ef4\u5ea6\u7684\u8868\u73b0\uff0cAuto-ATT\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "ATT\u548cAuto-ATT\u4e3aTTS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u5feb\u901f\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "relevance": 60.0}}
{"id": "2505.11126", "pdf": "https://arxiv.org/pdf/2505.11126", "abs": "https://arxiv.org/abs/2505.11126", "authors": ["Shokichi Takakura", "Seng Pei Liew", "Satoshi Hasegawa"], "title": "FedDuA: Doubly Adaptive Federated Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Federated learning is a distributed learning framework where clients\ncollaboratively train a global model without sharing their raw data. FedAvg is\na popular algorithm for federated learning, but it often suffers from slow\nconvergence due to the heterogeneity of local datasets and anisotropy in the\nparameter space. In this work, we formalize the central server optimization\nprocedure through the lens of mirror descent and propose a novel framework,\ncalled FedDuA, which adaptively selects the global learning rate based on both\ninter-client and coordinate-wise heterogeneity in the local updates. We prove\nthat our proposed doubly adaptive step-size rule is minimax optimal and provide\na convergence analysis for convex objectives. Although the proposed method does\nnot require additional communication or computational cost on clients,\nextensive numerical experiments show that our proposed framework outperforms\nbaselines in various settings and is robust to the choice of hyperparameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedDuA\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u955c\u9762\u4e0b\u964d\u89c6\u89d2\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5168\u5c40\u5b66\u4e60\u7387\u9009\u62e9\uff0c\u89e3\u51b3\u4e86FedAvg\u56e0\u6570\u636e\u5f02\u6784\u6027\u548c\u53c2\u6570\u7a7a\u95f4\u5404\u5411\u5f02\u6027\u5bfc\u81f4\u7684\u6536\u655b\u6162\u95ee\u9898\u3002", "motivation": "FedAvg\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u672c\u5730\u6570\u636e\u96c6\u7684\u5f02\u6784\u6027\u548c\u53c2\u6570\u7a7a\u95f4\u7684\u5404\u5411\u5f02\u6027\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u6162\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5168\u5c40\u5b66\u4e60\u7387\u9009\u62e9\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u955c\u9762\u4e0b\u964d\u89c6\u89d2\u5f62\u5f0f\u5316\u4e2d\u5fc3\u670d\u52a1\u5668\u4f18\u5316\u8fc7\u7a0b\uff0c\u63d0\u51faFedDuA\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u5730\u57fa\u4e8e\u5ba2\u6237\u7aef\u95f4\u548c\u5750\u6807\u95f4\u5f02\u6784\u6027\u9009\u62e9\u5168\u5c40\u5b66\u4e60\u7387\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u63d0\u51fa\u7684\u53cc\u91cd\u81ea\u9002\u5e94\u6b65\u957f\u89c4\u5219\u662f\u6781\u5c0f\u6781\u5927\u6700\u4f18\u7684\uff0c\u5e76\u5728\u51f8\u76ee\u6807\u4e0b\u63d0\u4f9b\u6536\u655b\u5206\u6790\u3002\u5b9e\u9a8c\u8868\u660eFedDuA\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u4e14\u5bf9\u8d85\u53c2\u6570\u9009\u62e9\u9c81\u68d2\u3002", "conclusion": "FedDuA\u5728\u4e0d\u589e\u52a0\u5ba2\u6237\u7aef\u901a\u4fe1\u6216\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6536\u655b\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11314", "pdf": "https://arxiv.org/pdf/2505.11314", "abs": "https://arxiv.org/abs/2505.11314", "authors": ["Christoph Leiter", "Yuki M. Asano", "Margret Keuper", "Steffen Eger"], "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks", "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCROC\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6bd4\u6d4b\u8bd5\u6848\u4f8b\u91cf\u5316\u6307\u6807\u9c81\u68d2\u6027\uff0c\u5e76\u8bad\u7ec3\u65b0\u6307\u6807CROCScore\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u7a00\u7f3a\u3002", "method": "\u63d0\u51faCROC\u6846\u67b6\uff0c\u751f\u6210\u5bf9\u6bd4\u6d4b\u8bd5\u6848\u4f8b\uff08CROC$^{syn}$\u6570\u636e\u96c6\uff09\uff0c\u8bad\u7ec3\u65b0\u6307\u6807CROCScore\uff0c\u5e76\u5f15\u5165\u4eba\u5de5\u76d1\u7763\u57fa\u51c6\uff08CROC$^{hum}$\uff09\u3002", "result": "CROCScore\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u73b0\u6709\u6307\u6807\u5728\u5426\u5b9a\u63d0\u793a\u548c\u8eab\u4f53\u90e8\u4f4d\u8bc6\u522b\u7b49\u4efb\u52a1\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u95ee\u9898\u3002", "conclusion": "CROC\u6846\u67b6\u4e3a\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u9a8c\u8bc1\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6307\u6807\u7684\u4e0d\u8db3\u3002", "relevance": 60.0}}
{"id": "2505.11128", "pdf": "https://arxiv.org/pdf/2505.11128", "abs": "https://arxiv.org/abs/2505.11128", "authors": ["Simone Azeglio", "Arianna Di Bernardo"], "title": "What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have demonstrated their remarkable\nability to capture complex image distributions, but the geometric properties of\nthe learned data manifold remain poorly understood. We address this gap by\nintroducing a score-based Riemannian metric that leverages the Stein score\nfunction from diffusion models to characterize the intrinsic geometry of the\ndata manifold without requiring explicit parameterization. Our approach defines\na metric tensor in the ambient space that stretches distances perpendicular to\nthe manifold while preserving them along tangential directions, effectively\ncreating a geometry where geodesics naturally follow the manifold's contours.\nWe develop efficient algorithms for computing these geodesics and demonstrate\ntheir utility for both interpolation between data points and extrapolation\nbeyond the observed data distribution. Through experiments on synthetic data\nwith known geometry, Rotated MNIST, and complex natural images via Stable\nDiffusion, we show that our score-based geodesics capture meaningful\ntransformations that respect the underlying data distribution. Our method\nconsistently outperforms baseline approaches on perceptual metrics (LPIPS) and\ndistribution-level metrics (FID, KID), producing smoother, more realistic image\ntransitions. These results reveal the implicit geometric structure learned by\ndiffusion models and provide a principled way to navigate the manifold of\nnatural images through the lens of Riemannian geometry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u9ece\u66fc\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4e2d\u7684Stein\u5206\u6570\u51fd\u6570\u6765\u8868\u5f81\u6570\u636e\u6d41\u5f62\u7684\u5185\u5728\u51e0\u4f55\u7279\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u53c2\u6570\u5316\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u548c\u5206\u5e03\u7ea7\u522b\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6355\u6349\u590d\u6742\u56fe\u50cf\u5206\u5e03\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u7279\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5206\u6570\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684Stein\u5206\u6570\u51fd\u6570\u5b9a\u4e49\u73af\u5883\u7a7a\u95f4\u4e2d\u7684\u5ea6\u91cf\u5f20\u91cf\uff0c\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u8ba1\u7b97\u6d4b\u5730\u7ebf\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u3001Rotated MNIST\u548c\u81ea\u7136\u56fe\u50cf\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728LPIPS\u3001FID\u548cKID\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u66f4\u771f\u5b9e\u7684\u56fe\u50cf\u8fc7\u6e21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7684\u9690\u5f0f\u51e0\u4f55\u7ed3\u6784\uff0c\u4e3a\u901a\u8fc7\u9ece\u66fc\u51e0\u4f55\u5bfc\u822a\u81ea\u7136\u56fe\u50cf\u6d41\u5f62\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.11326", "pdf": "https://arxiv.org/pdf/2505.11326", "abs": "https://arxiv.org/abs/2505.11326", "authors": ["Keunwoo Peter Yu", "Joyce Chai"], "title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTGLG\u7684\u65b0\u57fa\u51c6\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5b9e\u65f6\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u611f\u77e5\u66f4\u65b0\u548c\u5e94\u6025\u610f\u8bc6\u80fd\u529b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86VLM-TSI\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u540c\u6b65\u4ea4\u9519\u89c6\u89c9\u548c\u8bed\u8a00\u4ee4\u724c\u5b9e\u73b0\u5b9e\u65f6\u8bed\u8a00\u751f\u6210\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u79bb\u7ebf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u65f6\u4ea4\u4e92\u73af\u5883\u4e2d\u9700\u8981\u751f\u6210\u8bed\u4e49\u51c6\u786e\u4e14\u65f6\u95f4\u7cbe\u786e\u7684\u54cd\u5e94\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86TGLG\u57fa\u51c6\u4efb\u52a1\u548cTRACE\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u4e86VLM-TSI\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u540c\u6b65\u4ea4\u9519\u89c6\u89c9\u548c\u8bed\u8a00\u4ee4\u724c\u5b9e\u73b0\u5b9e\u65f6\u751f\u6210\u3002", "result": "VLM-TSI\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4ecd\u6709\u9650\uff0c\u8868\u660eTGLG\u4efb\u52a1\u7684\u6311\u6218\u6027\u3002", "conclusion": "TGLG\u4efb\u52a1\u548cVLM-TSI\u6a21\u578b\u4e3a\u5b9e\u65f6VLMs\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "relevance": 60.0}}
{"id": "2505.11132", "pdf": "https://arxiv.org/pdf/2505.11132", "abs": "https://arxiv.org/abs/2505.11132", "authors": ["Feng Xiao", "Xiaoying Tang", "Jicong Fan"], "title": "Fairness-aware Anomaly Detection via Fair Projection", "categories": ["cs.LG"], "comment": null, "summary": "Unsupervised anomaly detection is a critical task in many high-social-impact\napplications such as finance, healthcare, social media, and cybersecurity,\nwhere demographics involving age, gender, race, disease, etc, are used\nfrequently. In these scenarios, possible bias from anomaly detection systems\ncan lead to unfair treatment for different groups and even exacerbate social\nbias. In this work, first, we thoroughly analyze the feasibility and necessary\nassumptions for ensuring group fairness in unsupervised anomaly detection.\nSecond, we propose a novel fairness-aware anomaly detection method FairAD. From\nthe normal training data, FairAD learns a projection to map data of different\ndemographic groups to a common target distribution that is simple and compact,\nand hence provides a reliable base to estimate the density of the data. The\ndensity can be directly used to identify anomalies while the common target\ndistribution ensures fairness between different groups. Furthermore, we propose\na threshold-free fairness metric that provides a global view for model's\nfairness, eliminating dependence on manual threshold selection. Experiments on\nreal-world benchmarks demonstrate that our method achieves an improved\ntrade-off between detection accuracy and fairness under both balanced and\nskewed data across different groups.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u6027\u611f\u77e5\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5FairAD\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7ec4\u7684\u6570\u636e\u6620\u5c04\u5230\u5171\u540c\u76ee\u6807\u5206\u5e03\u6765\u786e\u4fdd\u516c\u5e73\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u9608\u503c\u7684\u516c\u5e73\u6027\u5ea6\u91cf\u3002", "motivation": "\u5728\u9ad8\u793e\u4f1a\u5f71\u54cd\u5e94\u7528\u4e2d\uff0c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u53ef\u80fd\u56e0\u6570\u636e\u504f\u5dee\u5bfc\u81f4\u4e0d\u516c\u5e73\uff0c\u56e0\u6b64\u9700\u8981\u786e\u4fdd\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u6027\u3002", "method": "FairAD\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u5c06\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7ec4\u7684\u6570\u636e\u6620\u5c04\u5230\u5171\u540c\u76ee\u6807\u5206\u5e03\uff0c\u5e76\u57fa\u4e8e\u5bc6\u5ea6\u4f30\u8ba1\u8bc6\u522b\u5f02\u5e38\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFairAD\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "FairAD\u4e3a\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.11334", "pdf": "https://arxiv.org/pdf/2505.11334", "abs": "https://arxiv.org/abs/2505.11334", "authors": ["Y. B. Wang", "S Wang", "J. N. Zhang", "J. F. Wu", "Q. D. He", "C. C. Fu", "C. J. Wang", "Y. Liu"], "title": "MARRS: Masked Autoregressive Unit-based Reaction Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This work aims at a challenging task: human action-reaction synthesis, i.e.,\ngenerating human reactions based on the action sequence of the other as\nconditions. Currently, autoregressive modeling approaches have achieved\nremarkable performance in motion generation tasks, e.g. text-to-motion.\nHowever, vector quantization (VQ) accompanying autoregressive generation has\ninherent disadvantages, including loss of quantization information, low\ncodebook utilization, etc. Moreover, unlike text-to-motion, which focuses\nsolely on the movement of body joints, human action-reaction synthesis also\nencompasses fine-grained hand movements. In this work, we propose MARRS, a\nnovel framework designed to generate coordinated and fine-grained reaction\nmotions in continuous representations. Initially, we present the\nUnit-distinguished Motion Variational AutoEncoder (UD-VAE), which segments the\nentire body into distinct body and hand units, encoding them independently.\nSubsequently, we propose Action-Conditioned Fusion (ACF), which involves\nrandomly masking a subset of reactive tokens and extracting specific\ninformation about the body and hands from the active tokens. Furthermore, we\nintroduce Adaptive Unit Modulation (AUM) to facilitate interaction between body\nand hand units by using the information from one unit to adaptively modulate\nthe other. Finally, for the diffusion model, we employ a compact MLP as a noise\npredictor for each distinct body unit and incorporate the diffusion loss to\nmodel the probability distribution of each token. Quantitative and qualitative\nresults demonstrate that our method achieves superior performance. The code\nwill be released upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMARRS\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u534f\u8c03\u4e14\u7ec6\u7c92\u5ea6\u7684\u4eba\u7c7b\u53cd\u5e94\u52a8\u4f5c\u3002\u901a\u8fc7UD-VAE\u3001ACF\u548cAUM\u7b49\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u635f\u5931\u548c\u4ee3\u7801\u672c\u5229\u7528\u7387\u4f4e\u7b49\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u52a8\u4f5c-\u53cd\u5e94\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUD-VAE\u72ec\u7acb\u7f16\u7801\u8eab\u4f53\u548c\u624b\u90e8\u5355\u5143\uff0c\u7ed3\u5408ACF\u548cAUM\u5b9e\u73b0\u5355\u5143\u95f4\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6982\u7387\u5206\u5e03\u5efa\u6a21\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0cMARRS\u5728\u751f\u6210\u534f\u8c03\u4e14\u7ec6\u7c92\u5ea6\u7684\u53cd\u5e94\u52a8\u4f5c\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MARRS\u6846\u67b6\u5728\u4eba\u7c7b\u52a8\u4f5c-\u53cd\u5e94\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11365", "pdf": "https://arxiv.org/pdf/2505.11365", "abs": "https://arxiv.org/abs/2505.11365", "authors": ["Pierre Le Jeune", "Beno\u00eet Mal\u00e9sieux", "Weixuan Xiao", "Matteo Dora"], "title": "Phare: A Safety Probe for Large Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.", "AI": {"tldr": "Phare\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5e7b\u89c9\u4e0e\u53ef\u9760\u6027\u3001\u793e\u4f1a\u504f\u89c1\u548c\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u6027\u80fd\u800c\u975e\u5931\u8d25\u6a21\u5f0f\uff0c\u9700\u66f4\u5168\u9762\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165Phare\u6846\u67b6\uff0c\u8bc4\u4f3017\u79cd\u5148\u8fdbLLM\u5728\u4e09\u4e2a\u5b89\u5168\u7ef4\u5ea6\u7684\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff08\u5982\u5949\u627f\u3001\u63d0\u793a\u654f\u611f\u6027\u548c\u523b\u677f\u5370\u8c61\u518d\u73b0\uff09\u3002", "conclusion": "Phare\u4e3a\u6784\u5efa\u66f4\u5065\u58ee\u3001\u5bf9\u9f50\u4e14\u53ef\u4fe1\u8d56\u7684LLM\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002", "relevance": 90.0}}
{"id": "2505.10790", "pdf": "https://arxiv.org/pdf/2505.10790", "abs": "https://arxiv.org/abs/2505.10790", "authors": ["Liu Zhang", "Yiran Yao", "Danping Shi", "Dongchen Chai", "Jian Guo", "Zilong Wang"], "title": "Neural-Inspired Advances in Integral Cryptanalysis", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The study by Gohr et.al at CRYPTO 2019 and sunsequent related works have\nshown that neural networks can uncover previously unused features, offering\nnovel insights into cryptanalysis. Motivated by these findings, we employ\nneural networks to learn features specifically related to integral properties\nand integrate the corresponding insights into optimized search frameworks.\nThese findings validate the framework of using neural networks for feature\nexploration, providing researchers with novel insights that advance established\ncryptanalysis methods.\n  Neural networks have inspired the development of more precise integral search\nmodels. By comparing the integral distinguishers obtained via neural networks\nwith those identified by classical methods, we observe that existing automated\nsearch models often fail to find optimal distinguishers. To address this issue,\nwe develop a meet in the middle search framework that balances model accuracy\nand computational efficiency. As a result, we reduce the number of active\nplaintext bits required for an 11 rounds integral distinguisher on SKINNY64/64,\nand further identify a 12 rounds key dependent integral distinguisher achieving\none additional round over the previous best-known result.\n  The integral distinguishers discovered by neural networks enable key recovery\nattacks on more rounds. We identify a 7 rounds key independent integral\ndistinguisher from neural networks with even only one active plaintext cell,\nwhich is based on linear combinations of bits. This distinguisher enables a 15\nrounds key recovery attack on SKINNYn/n, improving upon the previous record by\none round. Additionally, we discover an 8 rounds key dependent integral\ndistinguisher using neural network that further reduces the time complexity of\nkey recovery attacks against SKINNY.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u79ef\u5206\u7279\u6027\uff0c\u4f18\u5316\u641c\u7d22\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u5bc6\u7801\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u53d7\u795e\u7ecf\u7f51\u7edc\u5728\u5bc6\u7801\u5206\u6790\u4e2d\u63ed\u793a\u65b0\u7279\u5f81\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u79ef\u5206\u641c\u7d22\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u201c\u4e2d\u95f4\u76f8\u9047\u201d\u641c\u7d22\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u6027\u5b66\u4e60\u80fd\u529b\uff0c\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u51cf\u5c11\u4e86SKINNY64/64\u768411\u8f6e\u79ef\u5206\u533a\u5206\u5668\u6240\u9700\u7684\u6d3b\u52a8\u660e\u6587\u6bd4\u7279\u6570\uff0c\u5e76\u53d1\u73b0\u4e8612\u8f6e\u5bc6\u94a5\u4f9d\u8d56\u79ef\u5206\u533a\u5206\u5668\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u53d1\u73b0\u7684\u79ef\u5206\u533a\u5206\u5668\u63d0\u5347\u4e86\u5bc6\u94a5\u6062\u590d\u653b\u51fb\u7684\u6548\u7387\uff0c\u6269\u5c55\u4e86\u5bc6\u7801\u5206\u6790\u7684\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11134", "pdf": "https://arxiv.org/pdf/2505.11134", "abs": "https://arxiv.org/abs/2505.11134", "authors": ["Desong Zhang", "Jia Hu", "Geyong Min"], "title": "Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) process information via discrete spikes,\nenabling them to operate at remarkably low energy levels. However, our\nexperimental observations reveal a striking vulnerability when SNNs are trained\nusing the mainstream method--direct encoding combined with backpropagation\nthrough time (BPTT): even a single backward pass on data drawn from a slightly\ndifferent distribution can lead to catastrophic network collapse. Our\ntheoretical analysis attributes this vulnerability to the repeated inputs\ninherent in direct encoding and the gradient accumulation characteristic of\nBPTT, which together produce an exceptional large Hessian spectral radius. To\naddress this challenge, we develop a hyperparameter-free method called Dominant\nEigencomponent Projection (DEP). By orthogonally projecting gradients to\nprecisely remove their dominant components, DEP effectively reduces the Hessian\nspectral radius, thereby preventing SNNs from settling into sharp minima.\nExtensive experiments demonstrate that DEP not only mitigates the vulnerability\nof SNNs to heterogeneous data poisoning, but also significantly enhances\noverall robustness compared to key baselines, providing strong support for\nsafer and more reliable SNN deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEP\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3SNNs\u5728\u76f4\u63a5\u7f16\u7801\u548cBPTT\u8bad\u7ec3\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3b\u6d41SNN\u8bad\u7ec3\u65b9\u6cd5\uff08\u76f4\u63a5\u7f16\u7801+BPTT\uff09\u5728\u9762\u5bf9\u5206\u5e03\u504f\u79fb\u6570\u636e\u65f6\u8868\u73b0\u51fa\u707e\u96be\u6027\u5d29\u6e83\u7684\u8106\u5f31\u6027\uff0c\u9700\u8981\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDominant Eigencomponent Projection (DEP)\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u79fb\u9664\u68af\u5ea6\u7684\u4e3b\u5bfc\u6210\u5206\uff0c\u964d\u4f4eHessian\u8c31\u534a\u5f84\u3002", "result": "DEP\u4e0d\u4ec5\u7f13\u89e3\u4e86SNNs\u5bf9\u5f02\u6784\u6570\u636e\u6c61\u67d3\u7684\u8106\u5f31\u6027\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53\u9c81\u68d2\u6027\u3002", "conclusion": "DEP\u4e3aSNNs\u7684\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "relevance": 30.0}}
{"id": "2505.11344", "pdf": "https://arxiv.org/pdf/2505.11344", "abs": "https://arxiv.org/abs/2505.11344", "authors": ["Chenyu Huang", "Peng Ye", "Shenghe Zheng", "Xiaohui Wang", "Lei Bai", "Tao Chen", "Wanli Ouyang"], "title": "Dynamic Base model Shift for Delta Compression", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "Transformer-based models with the pretrain-finetune paradigm bring about\nsignificant progress, along with the heavy storage and deployment costs of\nfinetuned models on multiple tasks. Delta compression attempts to lower the\ncosts by reducing the redundancy of delta parameters (i.e., the difference\nbetween the finetuned and pre-trained model weights) through pruning or\nquantization. However, existing methods by default employ the pretrained model\nas the base model and compress the delta parameters for every task, which may\ncauses significant performance degradation, especially when the compression\nrate is extremely high. To tackle this issue, we investigate the impact of\ndifferent base models on the performance of delta compression and find that the\npre-trained base model can hardly be optimal. To this end, we propose Dynamic\nBase Model Shift (DBMS), which dynamically adapts the base model to the target\ntask before performing delta compression. Specifically, we adjust two\nparameters, which respectively determine the magnitude of the base model shift\nand the overall scale of delta compression, to boost the compression\nperformance on each task. Through low-cost learning of these two parameters,\nour DBMS can maintain most of the finetuned model's performance even under an\nextremely high compression ratio setting, significantly surpassing existing\nmethods. Moreover, our DBMS is orthogonal and can be integrated with a variety\nof other methods, and it has been evaluated across different types of models\nincluding language, vision transformer, and multi-modal models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u57fa\u6a21\u578b\u8f6c\u79fb\uff08DBMS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u57fa\u6a21\u578b\u548c\u538b\u7f29\u53c2\u6570\u6765\u4f18\u5316Delta\u538b\u7f29\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684Delta\u538b\u7f29\u65b9\u6cd5\u9ed8\u8ba4\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u57fa\u6a21\u578b\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u3002\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u57fa\u6a21\u578b\u5bf9\u538b\u7f29\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u57fa\u6a21\u578b\u901a\u5e38\u4e0d\u662f\u6700\u4f18\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u57fa\u6a21\u578b\u8f6c\u79fb\uff08DBMS\uff09\uff0c\u52a8\u6001\u8c03\u6574\u57fa\u6a21\u578b\u4ee5\u9002\u5e94\u76ee\u6807\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u4e24\u4e2a\u53c2\u6570\uff08\u57fa\u6a21\u578b\u8f6c\u79fb\u5e45\u5ea6\u548c\u538b\u7f29\u89c4\u6a21\uff09\u6765\u4f18\u5316\u538b\u7f29\u6027\u80fd\u3002", "result": "DBMS\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5fae\u8c03\u6a21\u578b\u7684\u5927\u90e8\u5206\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\uff08\u8bed\u8a00\u3001\u89c6\u89c9Transformer\u548c\u591a\u6a21\u6001\u6a21\u578b\uff09\u3002", "conclusion": "DBMS\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684Delta\u538b\u7f29\u65b9\u6cd5\uff0c\u53ef\u4e0e\u5176\u4ed6\u6280\u672f\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.11405", "pdf": "https://arxiv.org/pdf/2505.11405", "abs": "https://arxiv.org/abs/2505.11405", "authors": ["Bohao Xing", "Xin Liu", "Guoying Zhao", "Chengyu Liu", "Xiaolan Fu", "Heikki K\u00e4lvi\u00e4inen"], "title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Emotion understanding is a critical yet challenging task. Recent advances in\nMultimodal Large Language Models (MLLMs) have significantly enhanced their\ncapabilities in this area. However, MLLMs often suffer from hallucinations,\ngenerating irrelevant or nonsensical content. To the best of our knowledge,\ndespite the importance of this issue, there has been no dedicated effort to\nevaluate emotion-related hallucinations in MLLMs. In this work, we introduce\nEmotionHallucer, the first benchmark for detecting and analyzing emotion\nhallucinations in MLLMs. Unlike humans, whose emotion understanding stems from\nthe interplay of biology and social learning, MLLMs rely solely on data-driven\nlearning and lack innate emotional instincts. Fortunately, emotion psychology\nprovides a solid foundation of knowledge about human emotions. Building on\nthis, we assess emotion hallucinations from two dimensions: emotion psychology\nknowledge and real-world multimodal perception. To support robust evaluation,\nwe utilize an adversarial binary question-answer (QA) framework, which employs\ncarefully crafted basic and hallucinated pairs to assess the emotion\nhallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on\nEmotionHallucer, we reveal that: i) most current models exhibit substantial\nissues with emotion hallucinations; ii) closed-source models outperform\nopen-source ones in detecting emotion hallucinations, and reasoning capability\nprovides additional advantages; iii) existing models perform better in emotion\npsychology knowledge than in multimodal emotion perception. As a byproduct,\nthese findings inspire us to propose the PEP-MEK framework, which yields an\naverage improvement of 9.90% in emotion hallucination detection across selected\nmodels. Resources will be available at\nhttps://github.com/xxtars/EmotionHallucer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EmotionHallucer\uff0c\u9996\u4e2a\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u60c5\u611f\u5e7b\u89c9\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u60c5\u611f\u5e7b\u89c9\u65b9\u9762\u7684\u663e\u8457\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6846\u67b6PEP-MEK\u3002", "motivation": "\u60c5\u611f\u7406\u89e3\u662f\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u800cMLLMs\u5728\u60c5\u611f\u5e7b\u89c9\u65b9\u9762\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u60c5\u611f\u5fc3\u7406\u5b66\u77e5\u8bc6\u548c\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u611f\u77e5\uff0c\u91c7\u7528\u5bf9\u6297\u6027\u4e8c\u5143QA\u6846\u67b6\u8bc4\u4f3038\u4e2aLLMs\u548cMLLMs\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5b58\u5728\u60c5\u611f\u5e7b\u89c9\u95ee\u9898\uff0c\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u989d\u5916\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u4e86PEP-MEK\u6846\u67b6\uff0c\u5e73\u5747\u6539\u8fdb9.90%\u7684\u60c5\u611f\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.10791", "pdf": "https://arxiv.org/pdf/2505.10791", "abs": "https://arxiv.org/abs/2505.10791", "authors": ["N Harsha Vardhan", "Ponnurangam Kumaraguru", "Kiran Garimella"], "title": "Analyzing Patterns and Influence of Advertising in Print Newspapers", "categories": ["cs.CY", "cs.AI", "cs.SI"], "comment": "Accepted at COMPASS 2025", "summary": "This paper investigates advertising practices in print newspapers across\nIndia using a novel data-driven approach. We develop a pipeline employing image\nprocessing and OCR techniques to extract articles and advertisements from\ndigital versions of print newspapers with high accuracy. Applying this\nmethodology to five popular newspapers that span multiple regions and three\nlanguages, English, Hindi, and Telugu, we assembled a dataset of more than\n12,000 editions containing several hundred thousand advertisements.\nCollectively, these newspapers reach a readership of over 100 million people.\nUsing this extensive dataset, we conduct a comprehensive analysis to answer key\nquestions about print advertising: who advertises, what they advertise, when\nthey advertise, where they place their ads, and how they advertise. Our\nfindings reveal significant patterns, including the consistent level of print\nadvertising over the past six years despite declining print circulation, the\noverrepresentation of company ads on prominent pages, and the disproportionate\nrevenue contributed by government ads. Furthermore, we examine whether\nadvertising in a newspaper influences the coverage an advertiser receives.\nThrough regression analyses on coverage volume and sentiment, we find strong\nevidence supporting this hypothesis for corporate advertisers. The results\nindicate a clear trend where increased advertising correlates with more\nfavorable and extensive media coverage, a relationship that remains robust over\ntime and across different levels of advertiser popularity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u56fe\u50cf\u5904\u7406\u548cOCR\u6280\u672f\u6784\u5efa\u4e86\u4e00\u4e2a\u5370\u5ea6\u5370\u5237\u62a5\u7eb8\u5e7f\u544a\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u5e7f\u544a\u6a21\u5f0f\u53ca\u5176\u5bf9\u5a92\u4f53\u62a5\u9053\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5370\u5ea6\u5370\u5237\u62a5\u7eb8\u4e2d\u7684\u5e7f\u544a\u5b9e\u8df5\uff0c\u63ed\u793a\u5e7f\u544a\u4e0e\u5a92\u4f53\u62a5\u9053\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u5904\u7406\u548cOCR\u6280\u672f\u63d0\u53d6\u5e7f\u544a\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u5206\u6790\u7814\u7a76\u5e7f\u544a\u4e0e\u5a92\u4f53\u62a5\u9053\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5e7f\u544a\u91cf\u4e0e\u5a92\u4f53\u62a5\u9053\u7684\u6b63\u9762\u6027\u548c\u5e7f\u6cdb\u6027\u5448\u6b63\u76f8\u5173\uff0c\u653f\u5e9c\u5e7f\u544a\u8d21\u732e\u4e86\u4e0d\u6210\u6bd4\u4f8b\u7684\u6536\u76ca\u3002", "conclusion": "\u5e7f\u544a\u663e\u8457\u5f71\u54cd\u5a92\u4f53\u62a5\u9053\uff0c\u5c24\u5176\u662f\u5bf9\u4f01\u4e1a\u5e7f\u544a\u4e3b\u3002", "relevance": 20.0}}
{"id": "2505.11139", "pdf": "https://arxiv.org/pdf/2505.11139", "abs": "https://arxiv.org/abs/2505.11139", "authors": ["Om Roy", "Yashar Moshfeghi", "Keith Smith"], "title": "Covariance Density Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph neural networks have re-defined how we model and predict on network\ndata but there lacks a consensus on choosing the correct underlying graph\nstructure on which to model signals. CoVariance Neural Networks (VNN) address\nthis issue by using the sample covariance matrix as a Graph Shift Operator\n(GSO). Here, we improve on the performance of VNNs by constructing a Density\nMatrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of\nthe system in the space of random variables. Crucially, using this density\nmatrix as the GSO allows components of the data to be extracted at different\nscales, allowing enhanced discriminability and performance. We show that this\napproach allows explicit control of the stability-discriminability trade-off of\nthe network, provides enhanced robustness to noise compared to VNNs, and\noutperforms them in useful real-life applications where the underlying\ncovariance matrix is informative. In particular, we show that our model can\nachieve strong performance in subject-independent Brain Computer Interface EEG\nmotor imagery classification, outperforming EEGnet while being faster. This\nshows how covariance density neural networks provide a basis for the\nnotoriously difficult task of transferability of BCIs when evaluated on unseen\nindividuals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u77e9\u9635\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6837\u672c\u534f\u65b9\u5dee\u77e9\u9635\u89c6\u4e3a\u51c6\u54c8\u5bc6\u987f\u91cf\uff0c\u6539\u8fdb\u4e86\u534f\u65b9\u5dee\u795e\u7ecf\u7f51\u7edc\uff08VNN\uff09\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u5224\u522b\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7f3a\u4e4f\u5171\u8bc6\u7684\u56fe\u7ed3\u6784\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u63d0\u5347VNN\u7684\u6027\u80fd\u3002", "method": "\u5c06\u6837\u672c\u534f\u65b9\u5dee\u77e9\u9635\u6784\u9020\u4e3a\u5bc6\u5ea6\u77e9\u9635\u4f5c\u4e3a\u56fe\u79fb\u4f4d\u7b97\u5b50\uff08GSO\uff09\uff0c\u5b9e\u73b0\u6570\u636e\u591a\u5c3a\u5ea6\u63d0\u53d6\u3002", "result": "\u5728\u8111\u673a\u63a5\u53e3EEG\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8eEEGnet\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8111\u673a\u63a5\u53e3\u7684\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\u3002", "relevance": 50.0}}
{"id": "2505.11383", "pdf": "https://arxiv.org/pdf/2505.11383", "abs": "https://arxiv.org/abs/2505.11383", "authors": ["Zihan Wang", "Seungjun Lee", "Gim Hee Lee"], "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) is a core task where embodied agents\nleverage their spatial mobility to navigate in 3D environments toward\ndesignated destinations based on natural language instructions. Recently,\nvideo-language large models (Video-VLMs) with strong generalization\ncapabilities and rich commonsense knowledge have shown remarkable performance\nwhen applied to VLN tasks. However, these models still encounter the following\nchallenges when applied to real-world 3D navigation: 1) Insufficient\nunderstanding of 3D geometry and spatial semantics; 2) Limited capacity for\nlarge-scale exploration and long-term environmental memory; 3) Poor\nadaptability to dynamic and changing environments.To address these limitations,\nwe propose Dynam3D, a dynamic layered 3D representation model that leverages\nlanguage-aligned, generalizable, and hierarchical 3D representations as visual\ninput to train 3D-VLM in navigation action prediction. Given posed RGB-D\nimages, our Dynam3D projects 2D CLIP features into 3D space and constructs\nmulti-level 3D patch-instance-zone representations for 3D geometric and\nsemantic understanding with a dynamic and layer-wise update strategy. Our\nDynam3D is capable of online encoding and localization of 3D instances, and\ndynamically updates them in changing environments to provide large-scale\nexploration and long-term memory capabilities for navigation. By leveraging\nlarge-scale 3D-language pretraining and task-specific adaptation, our Dynam3D\nsets new state-of-the-art performance on VLN benchmarks including R2R-CE,\nREVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for\npre-exploration, lifelong memory, and real-world robot validate the\neffectiveness of practical deployment.", "AI": {"tldr": "Dynam3D\u662f\u4e00\u79cd\u52a8\u6001\u5206\u5c42\u76843D\u8868\u793a\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u8a00\u5bf9\u9f50\u7684\u3001\u53ef\u6cdb\u5316\u7684\u5206\u5c423D\u8868\u793a\u4f5c\u4e3a\u89c6\u89c9\u8f93\u5165\uff0c\u63d0\u53473D-VLM\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u5927\u6a21\u578b\u57283D\u5bfc\u822a\u4e2d\u7684\u51e0\u4f55\u7406\u89e3\u3001\u5927\u89c4\u6a21\u63a2\u7d22\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u5927\u6a21\u578b\uff08Video-VLMs\uff09\u57283D\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b58\u5728\u5bf93D\u51e0\u4f55\u548c\u7a7a\u95f4\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u3001\u5927\u89c4\u6a21\u63a2\u7d22\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u5bf9\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDynam3D\u6a21\u578b\uff0c\u5c062D CLIP\u7279\u5f81\u6295\u5f71\u52303D\u7a7a\u95f4\uff0c\u6784\u5efa\u591a\u5c42\u6b21\u76843D\u8868\u793a\uff08patch-instance-zone\uff09\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u5206\u5c42\u66f4\u65b0\u7b56\u7565\u3002\u652f\u6301\u5728\u7ebf3D\u5b9e\u4f8b\u7f16\u7801\u4e0e\u5b9a\u4f4d\uff0c\u52a8\u6001\u66f4\u65b0\u4ee5\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "result": "\u5728R2R-CE\u3001REVERIE-CE\u548cNavRAG-CE\u7b49VLN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u9884\u63a2\u7d22\u3001\u957f\u671f\u8bb0\u5fc6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "Dynam3D\u901a\u8fc7\u5206\u5c423D\u8868\u793a\u548c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e863D-VLM\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u5927\u89c4\u6a21\u73af\u5883\u3002", "relevance": 60.0}}
{"id": "2505.11406", "pdf": "https://arxiv.org/pdf/2505.11406", "abs": "https://arxiv.org/abs/2505.11406", "authors": ["Jenny Xiyu Fu", "Brennan Antone", "Kowe Kadoma", "Malte Jung"], "title": "Large Language Model Use Impact Locus of Control", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0eAI\u5171\u540c\u5199\u4f5c\u5bf9\u4eba\u4eec\u63a7\u5236\u70b9\u5fc3\u7406\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c31\u4e1a\u72b6\u6001\u662f\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22AI\u5de5\u5177\u5982\u4f55\u901a\u8fc7\u5171\u540c\u5199\u4f5c\u5f71\u54cd\u4eba\u4eec\u7684\u5fc3\u7406\u63a7\u5236\u70b9\uff0c\u5c24\u5176\u662f\u5c31\u4e1a\u72b6\u6001\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7462\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u7ed3\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5c31\u4e1a\u8005\u66f4\u4f9d\u8d56AI\u4e14\u63a7\u5236\u70b9\u5185\u5316\uff0c\u5931\u4e1a\u8005\u5219\u4e2a\u4eba\u80fd\u52a8\u6027\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u5f15\u53d1\u5173\u4e8eAI\u5982\u4f55\u5851\u9020\u4e2a\u4eba\u80fd\u52a8\u6027\u548c\u8eab\u4efd\u7684\u5e7f\u6cdb\u8ba8\u8bba\u3002", "relevance": 30.0}}
{"id": "2505.11153", "pdf": "https://arxiv.org/pdf/2505.11153", "abs": "https://arxiv.org/abs/2505.11153", "authors": ["Ashok Arora", "Neetesh Kumar"], "title": "Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes", "categories": ["cs.LG"], "comment": null, "summary": "In real-world reinforcement learning (RL) scenarios, agents often encounter\npartial observability, where incomplete or noisy information obscures the true\nstate of the environment. Partially Observable Markov Decision Processes\n(POMDPs) are commonly used to model these environments, but effective\nperformance requires memory mechanisms to utilise past observations. While\nrecurrence networks have traditionally addressed this need, transformer-based\nmodels have recently shown improved sample efficiency in RL tasks. However,\ntheir application to POMDPs remains underdeveloped, and their real-world\ndeployment is constrained due to the high parameter count. This work introduces\na novel bi-recurrent model architecture that improves sample efficiency and\nreduces model parameter count in POMDP scenarios. The architecture replaces the\nmultiple feed forward layers with a single layer of bi-directional recurrence\nunit to better capture and utilize sequential dependencies and contextual\ninformation. This approach improves the model's ability to handle partial\nobservability and increases sample efficiency, enabling effective learning from\ncomparatively fewer interactions. To evaluate the performance of the proposed\nmodel architecture, experiments were conducted on a total of 23 POMDP\nenvironments. The proposed model architecture outperforms existing\ntransformer-based, attention-based, and recurrence-based methods by a margin\nranging from 87.39% to 482.04% on average across the 23 POMDP environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53cc\u5411\u5faa\u73af\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3POMDPs\u4e2d\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u548c\u9ad8\u53c2\u6570\u6570\u91cf\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728RL\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u5411\u5faa\u73af\u5355\u5143\u66ff\u4ee3\u4f20\u7edf\u524d\u9988\u5c42\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u5e8f\u5217\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u572823\u4e2aPOMDP\u73af\u5883\u4e2d\uff0c\u65b0\u6a21\u578b\u5e73\u5747\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd587.39%\u81f3482.04%\u3002", "conclusion": "\u65b0\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86POMDPs\u4e2d\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645RL\u573a\u666f\u3002", "relevance": 85.0}}
{"id": "2505.11386", "pdf": "https://arxiv.org/pdf/2505.11386", "abs": "https://arxiv.org/abs/2505.11386", "authors": ["Zifan Wang", "Jingwei Li", "Yitang Li", "Yunze Liu"], "title": "MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field\n(NeRF) performance under limited samples using Mutual Information Theory. While\nNeRF excels in 3D scene synthesis, challenges arise with limited data and\nexisting methods that aim to introduce prior knowledge lack theoretical support\nin a unified framework. We introduce a simple but theoretically robust concept,\nMutual Information, as a metric to uniformly measure the correlation between\nimages, considering both macro (semantic) and micro (pixel) levels.\n  For sparse view sampling, we strategically select additional viewpoints\ncontaining more non-overlapping scene information by minimizing mutual\ninformation without knowing ground truth images beforehand. Our framework\nemploys a greedy algorithm, offering a near-optimal solution.\n  For few-shot view synthesis, we maximize the mutual information between\ninferred images and ground truth, expecting inferred images to gain more\nrelevant information from known images. This is achieved by incorporating\nefficient, plug-and-play regularization terms.\n  Experiments under limited samples show consistent improvement over\nstate-of-the-art baselines in different settings, affirming the efficacy of our\nframework.", "AI": {"tldr": "MutualNeRF\u5229\u7528\u4e92\u4fe1\u606f\u7406\u8bba\u63d0\u5347NeRF\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6216\u6700\u5927\u5316\u4e92\u4fe1\u606f\u4f18\u5316\u89c6\u89d2\u9009\u62e9\u548c\u5408\u6210\u6548\u679c\u3002", "motivation": "NeRF\u57283D\u573a\u666f\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6709\u9650\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d2a\u5a6a\u7b97\u6cd5\u9009\u62e9\u89c6\u89d2\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u9879\u6700\u5927\u5316\u5408\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684\u4e92\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMutualNeRF\u5728\u6709\u9650\u6837\u672c\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MutualNeRF\u4e3a\u6709\u9650\u6837\u672c\u4e0b\u7684NeRF\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u6846\u67b6\u3002", "relevance": 30.0}}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan Vuli\u0107"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u89c4\u5212\u8303\u5f0f\uff08Visual Planning\uff09\uff0c\u901a\u8fc7\u7eaf\u89c6\u89c9\u8868\u793a\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u975e\u4f9d\u8d56\u6587\u672c\u3002\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u6269\u5c55\uff08MLLMs\uff09\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u8bed\u8a00\u53ef\u80fd\u4e0d\u662f\u5904\u7406\u7a7a\u95f4\u548c\u51e0\u4f55\u4fe1\u606f\u4efb\u52a1\u7684\u6700\u6709\u6548\u6a21\u6001\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u89c4\u5212\u8303\u5f0f\uff08Visual Planning\uff09\uff0c\u901a\u8fc7\u56fe\u50cf\u5e8f\u5217\u6267\u884c\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08VPRL\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728FrozenLake\u3001Maze\u548cMiniBehavior\u7b49\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u89c6\u89c9\u89c4\u5212\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u89c9\u89c4\u5212\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 75.0}}
{"id": "2505.11157", "pdf": "https://arxiv.org/pdf/2505.11157", "abs": "https://arxiv.org/abs/2505.11157", "authors": ["Boris Bonev", "Max Rietmann", "Andrea Paris", "Alberto Carpentieri", "Thorsten Kurth"], "title": "Attention on the Sphere", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a generalized attention mechanism for spherical domains,\nenabling Transformer architectures to natively process data defined on the\ntwo-dimensional sphere - a critical need in fields such as atmospheric physics,\ncosmology, and robotics, where preserving spherical symmetries and topology is\nessential for physical accuracy. By integrating numerical quadrature weights\ninto the attention mechanism, we obtain a geometrically faithful spherical\nattention that is approximately rotationally equivariant, providing strong\ninductive biases and leading to better performance than Cartesian approaches.\nTo further enhance both scalability and model performance, we propose\nneighborhood attention on the sphere, which confines interactions to geodesic\nneighborhoods. This approach reduces computational complexity and introduces\nthe additional inductive bias for locality, while retaining the symmetry\nproperties of our method. We provide optimized CUDA kernels and\nmemory-efficient implementations to ensure practical applicability. The method\nis validated on three diverse tasks: simulating shallow water equations on the\nrotating sphere, spherical image segmentation, and spherical depth estimation.\nAcross all tasks, our spherical Transformers consistently outperform their\nplanar counterparts, highlighting the advantage of geometric priors for\nlearning on spherical domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u7403\u9762\u57df\u7684\u5e7f\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7fTransformer\u80fd\u591f\u76f4\u63a5\u5904\u7406\u7403\u9762\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u5fe0\u5b9e\u6027\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u6c14\u8c61\u5b66\u3001\u5b87\u5b99\u5b66\u548c\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\uff0c\u7403\u9762\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u7ed3\u6784\u7684\u4fdd\u6301\u5bf9\u7269\u7406\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u7403\u9762\u6570\u636e\u7684Transformer\u67b6\u6784\u3002", "method": "\u901a\u8fc7\u5c06\u6570\u503c\u79ef\u5206\u6743\u91cd\u96c6\u6210\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u63d0\u51fa\u51e0\u4f55\u5fe0\u5b9e\u7684\u7403\u9762\u6ce8\u610f\u529b\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f15\u5165\u5c40\u90e8\u6ce8\u610f\u529b\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u6a21\u62df\u65cb\u8f6c\u7403\u9762\u4e0a\u7684\u6d45\u6c34\u65b9\u7a0b\u3001\u7403\u9762\u56fe\u50cf\u5206\u5272\u548c\u7403\u9762\u6df1\u5ea6\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e2d\uff0c\u7403\u9762Transformer\u6027\u80fd\u4f18\u4e8e\u5e73\u9762\u65b9\u6cd5\u3002", "conclusion": "\u51e0\u4f55\u5148\u9a8c\u5bf9\u4e8e\u7403\u9762\u57df\u5b66\u4e60\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7403\u9762Transformer\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "relevance": 40.0}}
{"id": "2505.11404", "pdf": "https://arxiv.org/pdf/2505.11404", "abs": "https://arxiv.org/abs/2505.11404", "authors": ["Wenchuan Zhang", "Penghao Zhang", "Jingru Guo", "Tao Cheng", "Jie Chen", "Shuwan Zhang", "Zhang Zhang", "Yuhao Yi", "Hong Bu"], "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose PathoCLIP, trained on the same figure-caption corpus used for continued\npretraining. Comprehensive experimental results demonstrate that both PathoCLIP\nand Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPatho-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u7684\u75c5\u7406\u5b66\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u63a8\u7406\u5408\u7406\u6027\u3002", "motivation": "\u5f53\u524d\u75c5\u7406\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u63a8\u7406\u5408\u7406\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u96c6\u7f3a\u4e4f\u6df1\u5ea6\u548c\u7ed3\u6784\u5316\u8bca\u65ad\u8303\u5f0f\u3002", "method": "1) \u7ee7\u7eed\u9884\u8bad\u7ec33.5\u767e\u4e07\u56fe\u50cf-\u6587\u672c\u5bf9\uff1b2) \u76d1\u7763\u5fae\u8c0350\u4e07\u9ad8\u8d28\u91cf\u94fe\u5f0f\u63a8\u7406\u6837\u672c\uff1b3) \u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6a21\u6001\u63a8\u7406\u8d28\u91cf\u3002", "result": "Patho-R1\u548cPathoCLIP\u5728\u591a\u9879\u75c5\u7406\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u5b66\u6a21\u578b\u7684\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.11170", "pdf": "https://arxiv.org/pdf/2505.11170", "abs": "https://arxiv.org/abs/2505.11170", "authors": ["Myeonghwan Ahn", "Sungjoo Yoo"], "title": "Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training", "categories": ["cs.LG"], "comment": null, "summary": "Ever-growing scale of large language models (LLMs) is pushing for improved\nefficiency, favoring fully quantized training (FQT) over BF16. While FQT\naccelerates training, it faces consistency challenges and requires searching\nover an exponential number of cases, each needing over 200B tokens to ensure\nstability.\n  Pseudo-quantization training (PQT) addresses the issues of FQT, although it\nis not well-studied. We explore the practical implications of PQT in detail and\npropose a noise distribution $R$ that is floating-point (FP)-friendly, with\nideal properties including stochastic precision annealing. As a result, the\nproposed method serves as an effective theoretical foundation for low-precision\nFP parameters through PQT, utilizing efficient fake quantization via an\naddition and subsequent FP casting.\n  We demonstrate that Gaussian weight sampling is (1) scalable: supports\nlow-precision FP parameters down to FP6 and high-precision noise up to 9-bit\nwith BF16 operator. The proposed method is (2) efficient: incurring\ncomputational overhead as low as 1.40\\% on the A100 GPU in terms of Llama2\ntraining tokens per second, and requiring 2 bytes per parameter in GPU memory.\nWe demonstrate that PQT with Gaussian weight sampling is (3) stable: closely\nfollowing or even surpassing performance of the BF16 baseline while\npre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f2a\u91cf\u5316\u8bad\u7ec3\uff08PQT\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5168\u91cf\u5316\u8bad\u7ec3\uff08FQT\uff09\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u9ad8\u65af\u6743\u91cd\u91c7\u6837\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5b9a\u7684\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u53c2\u6570\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c4\u6a21\u7684\u4e0d\u65ad\u6269\u5927\uff0c\u5168\u91cf\u5316\u8bad\u7ec3\uff08FQT\uff09\u7684\u6548\u7387\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u5176\u9762\u4e34\u4e00\u81f4\u6027\u6311\u6218\u548c\u641c\u7d22\u7a7a\u95f4\u7206\u70b8\u7684\u95ee\u9898\u3002\u4f2a\u91cf\u5316\u8bad\u7ec3\uff08PQT\uff09\u867d\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d6e\u70b9\u53cb\u597d\u7684\u566a\u58f0\u5206\u5e03R\uff0c\u5177\u6709\u968f\u673a\u7cbe\u5ea6\u9000\u706b\u7b49\u7406\u60f3\u7279\u6027\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4f2a\u91cf\u5316\uff08\u52a0\u6cd5\u540e\u6d6e\u70b9\u8f6c\u6362\uff09\u5b9e\u73b0\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u53c2\u6570\u7684\u7406\u8bba\u57fa\u7840\u3002", "result": "\u9ad8\u65af\u6743\u91cd\u91c7\u6837\u5177\u6709\u53ef\u6269\u5c55\u6027\uff08\u652f\u6301\u4f4e\u81f3FP6\u7684\u53c2\u6570\u548c\u9ad8\u81f39\u4f4d\u7684\u566a\u58f0\uff09\u3001\u9ad8\u6548\u6027\uff08A100 GPU\u4e0a\u8ba1\u7b97\u5f00\u9500\u4f4e\u81f31.40%\uff09\u548c\u7a33\u5b9a\u6027\uff08\u5728GPT2\u548cLlama2\u4e0a\u6027\u80fd\u63a5\u8fd1\u6216\u8d85\u8fc7BF16\u57fa\u7ebf\uff09\u3002", "conclusion": "PQT\u7ed3\u5408\u9ad8\u65af\u6743\u91cd\u91c7\u6837\u4e3a\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u53c2\u6570\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u3002", "relevance": 85.0}}
{"id": "2505.11424", "pdf": "https://arxiv.org/pdf/2505.11424", "abs": "https://arxiv.org/abs/2505.11424", "authors": ["Rana Poureskandar", "Shiva Razzagzadeh"], "title": "Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study evaluated the performance of a YOLOv8-based segmentation model for\ndetecting and segmenting wrinkles in facial images.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u57fa\u4e8eYOLOv8\u7684\u5206\u5272\u6a21\u578b\u5728\u9762\u90e8\u56fe\u50cf\u4e2d\u68c0\u6d4b\u548c\u5206\u5272\u76b1\u7eb9\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u9762\u90e8\u76b1\u7eb9\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u53ef\u80fd\u4e3a\u7f8e\u5bb9\u6216\u533b\u7597\u9886\u57df\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u4f7f\u7528YOLOv8\u67b6\u6784\u7684\u5206\u5272\u6a21\u578b\u5bf9\u9762\u90e8\u56fe\u50cf\u4e2d\u7684\u76b1\u7eb9\u8fdb\u884c\u68c0\u6d4b\u548c\u5206\u5272\u3002", "result": "\u5c55\u793a\u4e86\u6a21\u578b\u5728\u76b1\u7eb9\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "YOLOv8\u5206\u5272\u6a21\u578b\u5728\u9762\u90e8\u76b1\u7eb9\u68c0\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 20.0}}
{"id": "2505.11185", "pdf": "https://arxiv.org/pdf/2505.11185", "abs": "https://arxiv.org/abs/2505.11185", "authors": ["Francesco Madeddu", "Lucia Testa", "Gianluca De Carlo", "Michele Pieroni", "Andrea Mastropietro", "Aris Anagnostopoulos", "Paolo Tieri", "Sergio Barbarossa"], "title": "VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks", "categories": ["cs.LG"], "comment": "9 pages of main text, 4 figures", "summary": "The intrinsic complexity of human biology presents ongoing challenges to\nscientific understanding. Researchers collaborate across disciplines to expand\nour knowledge of the biological interactions that define human life. AI\nmethodologies have emerged as powerful tools across scientific domains,\nparticularly in computational biology, where graph data structures effectively\nmodel biological entities such as protein-protein interaction (PPI) networks\nand gene functional networks. Those networks are used as datasets for paramount\nnetwork medicine tasks, such as gene-disease association prediction, drug\nrepurposing, and polypharmacy side effect studies. Reliable predictions from\nmachine learning models require high-quality foundational data. In this work,\nwe present a comprehensive multi-purpose biological knowledge graph constructed\nby integrating and refining multiple publicly available datasets. Building upon\nthe Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with\na) cleaning inconsistencies and redundancies present in DRKG, b) coalescing\ninformation from the main available public data sources, and c) enriching the\ngraph nodes with expressive feature vectors such as molecular fingerprints and\ngene ontologies. Biologically and chemically relevant features improve the\ncapacity of machine learning models to generate accurate and well-structured\nembedding spaces. The resulting resource represents a coherent and reliable\nbiological knowledge graph that serves as a state-of-the-art platform to\nadvance research in computational biology and precision medicine. Moreover, it\noffers the opportunity to benchmark graph-based machine learning and network\nmedicine models on relevant tasks. We demonstrate the effectiveness of the\nproposed dataset by benchmarking it against the task of drug repurposing, PPI\nprediction, and side-effect prediction, modeled as link prediction problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u591a\u7528\u9014\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u6574\u5408\u548c\u4f18\u5316\u516c\u5f00\u6570\u636e\u96c6\u6784\u5efa\uff0c\u7528\u4e8e\u8ba1\u7b97\u751f\u7269\u5b66\u548c\u7cbe\u51c6\u533b\u5b66\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\u4e2d\u6570\u636e\u4e0d\u4e00\u81f4\u548c\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u57fa\u56e0-\u75be\u75c5\u5173\u8054\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7ba1\u9053\uff0c\u6e05\u7406DRKG\u4e2d\u7684\u4e0d\u4e00\u81f4\u548c\u5197\u4f59\uff0c\u6574\u5408\u516c\u5171\u6570\u636e\u6e90\uff0c\u5e76\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\uff08\u5982\u5206\u5b50\u6307\u7eb9\u548c\u57fa\u56e0\u672c\u4f53\uff09\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u53ef\u9760\u7684\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\uff0c\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u751f\u7269\u5b66\u548c\u7f51\u7edc\u533b\u5b66\u6a21\u578b\u7684\u57fa\u51c6\u5e73\u53f0\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u4e3a\u8ba1\u7b97\u751f\u7269\u5b66\u548c\u7cbe\u51c6\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5148\u8fdb\u5de5\u5177\uff0c\u5e76\u5728\u836f\u7269\u91cd\u5b9a\u4f4d\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "relevance": 30.0}}
{"id": "2505.11425", "pdf": "https://arxiv.org/pdf/2505.11425", "abs": "https://arxiv.org/abs/2505.11425", "authors": ["Michal Podstawski", "Malgorzata Kudelska", "Haohong Wang"], "title": "Face Consistency Benchmark for GenAI Video", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Video generation driven by artificial intelligence has advanced\nsignificantly, enabling the creation of dynamic and realistic content. However,\nmaintaining character consistency across video sequences remains a major\nchallenge, with current models struggling to ensure coherence in appearance and\nattributes. This paper introduces the Face Consistency Benchmark (FCB), a\nframework for evaluating and comparing the consistency of characters in\nAI-generated videos. By providing standardized metrics, the benchmark\nhighlights gaps in existing solutions and promotes the development of more\nreliable approaches. This work represents a crucial step toward improving\ncharacter consistency in AI video generation technologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Face Consistency Benchmark (FCB)\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u89c6\u9891\u4e2d\u89d2\u8272\u4e00\u81f4\u6027\u7684\u6846\u67b6\uff0c\u65e8\u5728\u586b\u8865\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dAI\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165FCB\u6846\u67b6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u6307\u6807\u8bc4\u4f30\u89d2\u8272\u4e00\u81f4\u6027\u3002", "result": "FCB\u63ed\u793a\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u66f4\u53ef\u9760\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u662f\u63d0\u5347AI\u89c6\u9891\u751f\u6210\u4e2d\u89d2\u8272\u4e00\u81f4\u6027\u7684\u91cd\u8981\u4e00\u6b65\u3002", "relevance": 40.0}}
{"id": "2505.11197", "pdf": "https://arxiv.org/pdf/2505.11197", "abs": "https://arxiv.org/abs/2505.11197", "authors": ["Zhenyi Zhang", "Zihan Wang", "Yuhao Sun", "Tiejun Li", "Peijie Zhou"], "title": "Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr\u00f6dinger Bridge", "categories": ["cs.LG", "math.OC", "q-bio.QM"], "comment": null, "summary": "Modeling the dynamics from sparsely time-resolved snapshot data is crucial\nfor understanding complex cellular processes and behavior. Existing methods\nleverage optimal transport, Schr\\\"odinger bridge theory, or their variants to\nsimultaneously infer stochastic, unbalanced dynamics from snapshot data.\nHowever, these approaches remain limited in their ability to account for\ncell-cell interactions. This integration is essential in real-world scenarios\nsince intercellular communications are fundamental life processes and can\ninfluence cell state-transition dynamics. To address this challenge, we\nformulate the Unbalanced Mean-Field Schr\\\"odinger Bridge (UMFSB) framework to\nmodel unbalanced stochastic interaction dynamics from snapshot data. Inspired\nby this framework, we further propose CytoBridge, a deep learning algorithm\ndesigned to approximate the UMFSB problem. By explicitly modeling cellular\ntransitions, proliferation, and interactions through neural networks,\nCytoBridge offers the flexibility to learn these processes directly from data.\nThe effectiveness of our method has been extensively validated using both\nsynthetic gene regulatory data and real scRNA-seq datasets. Compared to\nexisting methods, CytoBridge identifies growth, transition, and interaction\npatterns, eliminates false transitions, and reconstructs the developmental\nlandscape with greater accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUMFSB\u7684\u6846\u67b6\u548cCytoBridge\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u65f6\u95f4\u5206\u8fa8\u5feb\u7167\u6570\u636e\u4e2d\u5efa\u6a21\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u4e0d\u5e73\u8861\u968f\u673a\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u7ec6\u80de\u95f4\u901a\u8baf\u5bf9\u7ec6\u80de\u72b6\u6001\u8f6c\u6362\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faUMFSB\u6846\u67b6\u548cCytoBridge\u7b97\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u663e\u5f0f\u5efa\u6a21\u7ec6\u80de\u8f6c\u6362\u3001\u589e\u6b96\u548c\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u5408\u6210\u57fa\u56e0\u8c03\u63a7\u6570\u636e\u548c\u771f\u5b9escRNA-seq\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCytoBridge\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u751f\u957f\u3001\u8f6c\u6362\u548c\u76f8\u4e92\u4f5c\u7528\u6a21\u5f0f\u3002", "conclusion": "CytoBridge\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u7cbe\u786e\u5730\u91cd\u5efa\u53d1\u80b2\u666f\u89c2\u3002", "relevance": 30.0}}
{"id": "2505.11439", "pdf": "https://arxiv.org/pdf/2505.11439", "abs": "https://arxiv.org/abs/2505.11439", "authors": ["Utsav Rai", "Haozheng Xu", "Stamatia Giannarou"], "title": "SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "To be published in 2025 International Conference on Robotics and\n  Automation (ICRA)", "summary": "Accurate pose estimation of surgical tools in Robot-assisted Minimally\nInvasive Surgery (RMIS) is essential for surgical navigation and robot control.\nWhile traditional marker-based methods offer accuracy, they face challenges\nwith occlusions, reflections, and tool-specific designs. Similarly, supervised\nlearning methods require extensive training on annotated datasets, limiting\ntheir adaptability to new tools. Despite their success in other domains,\nzero-shot pose estimation models remain unexplored in RMIS for pose estimation\nof surgical instruments, creating a gap in generalising to unseen surgical\ntools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation\npipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D\nmodels like the FoundationPose and SAM-6D. We advanced these models by\nincorporating vision-based depth estimation using the RAFT-Stereo method, for\nrobust depth estimation in reflective and textureless environments.\nAdditionally, we enhanced SAM-6D by replacing its instance segmentation module,\nSegment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly\nboosting segmentation accuracy in occluded and complex conditions. Extensive\nvalidation reveals that our enhanced SAM-6D surpasses FoundationPose in\nzero-shot pose estimation of unseen surgical instruments, setting a new\nbenchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the\ngeneralisability of pose estimation for unseen objects and pioneers the\napplication of RGB-D zero-shot methods in RMIS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u6837\u672cRGB-D\u6a21\u578b\u76846\u81ea\u7531\u5ea6\u624b\u672f\u5de5\u5177\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408RAFT-Stereo\u6df1\u5ea6\u4f30\u8ba1\u548c\u6539\u8fdb\u7684SAM-6D\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u8fc7\u7684\u624b\u672f\u5de5\u5177\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6807\u8bb0\u6cd5\u548c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u624b\u672f\u5de5\u5177\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u96f6\u6837\u672c\u65b9\u6cd5\u5728RMIS\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408FoundationPose\u548cSAM-6D\u7b49\u96f6\u6837\u672cRGB-D\u6a21\u578b\uff0c\u6539\u8fdb\u6df1\u5ea6\u4f30\u8ba1\u548c\u5206\u5272\u6a21\u5757\uff08\u5982\u7528Mask R-CNN\u66ff\u6362SAM\uff09\u3002", "result": "\u6539\u8fdb\u7684SAM-6D\u5728\u96f6\u6837\u672c\u59ff\u6001\u4f30\u8ba1\u4e2d\u4f18\u4e8eFoundationPose\uff0c\u4e3aRMIS\u4e2d\u7684\u96f6\u6837\u672cRGB-D\u65b9\u6cd5\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u96f6\u6837\u672c\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u9996\u6b21\u5c06RGB-D\u96f6\u6837\u672c\u65b9\u6cd5\u5e94\u7528\u4e8eRMIS\u3002", "relevance": 40.0}}
{"id": "2505.11204", "pdf": "https://arxiv.org/pdf/2505.11204", "abs": "https://arxiv.org/abs/2505.11204", "authors": ["Hangyu Zhou", "Aaron Gokaslan", "Volodymyr Kuleshov", "Bharath Hariharan"], "title": "RanDeS: Randomized Delta Superposition for Multi-Model Compression", "categories": ["cs.LG", "cs.AI"], "comment": "https://github.com/Zhou-Hangyu/randes", "summary": "From a multi-model compression perspective, model merging enables\nmemory-efficient serving of multiple models fine-tuned from the same base, but\nsuffers from degraded performance due to interference among their task-specific\nparameter adjustments (i.e., deltas). In this paper, we reformulate model\nmerging as a compress-and-retrieve scheme, revealing that the task interference\narises from the summation of irrelevant deltas during model retrieval. To\naddress this issue, we use random orthogonal transformations to decorrelate\nthese vectors into self-cancellation. We show that this approach drastically\nreduces interference, improving performance across both vision and language\ntasks. Since these transformations are fully defined by random seeds, adding\nnew models requires no extra memory. Further, their data- and model-agnostic\nnature enables easy addition or removal of models with minimal compute\noverhead, supporting efficient and flexible multi-model serving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6b63\u4ea4\u53d8\u6362\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u8c03\u6574\u4ee5\u51cf\u5c11\u5e72\u6270\uff0c\u63d0\u5347\u591a\u6a21\u578b\u670d\u52a1\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u591a\u6a21\u578b\u538b\u7f29\u80cc\u666f\u4e0b\uff0c\u6a21\u578b\u5408\u5e76\u56e0\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u8c03\u6574\u7684\u5e72\u6270\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u5b9e\u73b0\u9ad8\u6548\u591a\u6a21\u578b\u670d\u52a1\u3002", "method": "\u5c06\u6a21\u578b\u5408\u5e76\u91cd\u65b0\u5b9a\u4e49\u4e3a\u538b\u7f29-\u68c0\u7d22\u65b9\u6848\uff0c\u4f7f\u7528\u968f\u673a\u6b63\u4ea4\u53d8\u6362\u89e3\u8026\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u5411\u91cf\u4ee5\u51cf\u5c11\u5e72\u6270\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5e72\u6270\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u5747\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5185\u5b58\u5373\u53ef\u6dfb\u52a0\u65b0\u6a21\u578b\u3002", "conclusion": "\u968f\u673a\u6b63\u4ea4\u53d8\u6362\u662f\u4e00\u79cd\u6570\u636e\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u591a\u6a21\u578b\u670d\u52a1\u3002", "relevance": 75.0}}
{"id": "2505.11454", "pdf": "https://arxiv.org/pdf/2505.11454", "abs": "https://arxiv.org/abs/2505.11454", "authors": ["Shaina Raza", "Aravind Narayanan", "Vahid Reza Khazaie", "Ashmal Vayani", "Mukund S. Chettiar", "Amandeep Singh", "Mubarak Shah", "Deval Pandya"], "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large multimodal models (LMMs) now excel on many vision language benchmarks,\nhowever, they still struggle with human centered criteria such as fairness,\nethics, empathy, and inclusivity, key to aligning with human values. We\nintroduce HumaniBench, a holistic benchmark of 32K real-world image question\npairs, annotated via a scalable GPT4o assisted pipeline and exhaustively\nverified by domain experts. HumaniBench evaluates seven Human Centered AI\n(HCAI) principles: fairness, ethics, understanding, reasoning, language\ninclusivity, empathy, and robustness, across seven diverse tasks, including\nopen and closed ended visual question answering (VQA), multilingual QA, visual\ngrounding, empathetic captioning, and robustness tests. Benchmarking 15 state\nof the art LMMs (open and closed source) reveals that proprietary models\ngenerally lead, though robustness and visual grounding remain weak points. Some\nopen-source models also struggle to balance accuracy with adherence to\nhuman-aligned principles. HumaniBench is the first benchmark purpose built\naround HCAI principles. It provides a rigorous testbed for diagnosing alignment\ngaps and guiding LMMs toward behavior that is both accurate and socially\nresponsible. Dataset, annotation prompts, and evaluation code are available at:\nhttps://vectorinstitute.github.io/HumaniBench", "AI": {"tldr": "HumaniBench\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u4eba\u7c7b\u4e2d\u5fc3AI\u539f\u5219\uff08\u5982\u516c\u5e73\u6027\u3001\u4f26\u7406\u3001\u540c\u7406\u5fc3\u7b49\uff09\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b32K\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u95ee\u9898\u5bf9\uff0c\u8986\u76d67\u4e2a\u4efb\u52a1\u3002", "motivation": "\u5f53\u524dLMMs\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4eba\u7c7b\u4e2d\u5fc3\u4ef7\u503c\u89c2\uff08\u5982\u516c\u5e73\u6027\u3001\u4f26\u7406\u7b49\uff09\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7GPT4\u8f85\u52a9\u7684\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa32K\u56fe\u50cf\u95ee\u9898\u5bf9\uff0c\u5e76\u7531\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u8bc4\u4f307\u9879HCAI\u539f\u5219\uff0c\u6db5\u76d67\u79cd\u4efb\u52a1\u3002", "result": "\u6d4b\u8bd515\u4e2aLMMs\u663e\u793a\uff0c\u4e13\u6709\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5f00\u653e\u6a21\u578b\u5728\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u539f\u5219\u65b9\u9762\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "HumaniBench\u4e3a\u8bca\u65adLMMs\u7684\u5bf9\u9f50\u5dee\u8ddd\u548c\u63a8\u52a8\u793e\u4f1a\u8d23\u4efb\u611f\u63d0\u4f9b\u4e86\u9996\u4e2aHCAI\u57fa\u51c6\u6d4b\u8bd5\u3002", "relevance": 75.0}}
{"id": "2505.10871", "pdf": "https://arxiv.org/pdf/2505.10871", "abs": "https://arxiv.org/abs/2505.10871", "authors": ["Joonhyuk Ko", "Juba Ziani", "Ferdinando Fioretto"], "title": "Optimal Allocation of Privacy Budget on Hierarchical Data Release", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "Releasing useful information from datasets with hierarchical structures while\npreserving individual privacy presents a significant challenge. Standard\nprivacy-preserving mechanisms, and in particular Differential Privacy, often\nrequire careful allocation of a finite privacy budget across different levels\nand components of the hierarchy. Sub-optimal allocation can lead to either\nexcessive noise, rendering the data useless, or to insufficient protections for\nsensitive information. This paper addresses the critical problem of optimal\nprivacy budget allocation for hierarchical data release. It formulates this\nchallenge as a constrained optimization problem, aiming to maximize data\nutility subject to a total privacy budget while considering the inherent\ntrade-offs between data granularity and privacy loss. The proposed approach is\nsupported by theoretical analysis and validated through comprehensive\nexperiments on real hierarchical datasets. These experiments demonstrate that\noptimal privacy budget allocation significantly enhances the utility of the\nreleased data and improves the performance of downstream tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u9690\u79c1\u9884\u7b97\u5206\u914d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5206\u5c42\u6570\u636e\u53d1\u5e03\u4e2d\u5e73\u8861\u6570\u636e\u6548\u7528\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5728\u5206\u5c42\u6570\u636e\u53d1\u5e03\u4e2d\uff0c\u9690\u79c1\u9884\u7b97\u7684\u5206\u914d\u5bf9\u6570\u636e\u6548\u7528\u548c\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5b9e\u73b0\u6700\u4f18\u5e73\u8861\u3002", "method": "\u5c06\u9690\u79c1\u9884\u7b97\u5206\u914d\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5927\u5316\u6570\u636e\u6548\u7528\uff0c\u540c\u65f6\u8003\u8651\u6570\u636e\u7c92\u5ea6\u548c\u9690\u79c1\u635f\u5931\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u7684\u9690\u79c1\u9884\u7b97\u5206\u914d\u663e\u8457\u63d0\u9ad8\u4e86\u53d1\u5e03\u6570\u636e\u7684\u6548\u7528\uff0c\u5e76\u6539\u5584\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5206\u5c42\u6570\u636e\u53d1\u5e03\u4e2d\u7684\u9690\u79c1\u9884\u7b97\u5206\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.11210", "pdf": "https://arxiv.org/pdf/2505.11210", "abs": "https://arxiv.org/abs/2505.11210", "authors": ["Anders Gj\u00f8lbye", "Stefan Haufe", "Lars Kai Hansen"], "title": "Minimizing False-Positive Attributions in Explanations of Non-Linear Models", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint. Under review", "summary": "Suppressor variables can influence model predictions without being dependent\non the target outcome and they pose a significant challenge for Explainable AI\n(XAI) methods. These variables may cause false-positive feature attributions,\nundermining the utility of explanations. Although effective remedies exist for\nlinear models, their extension to non-linear models and to instance-based\nexplanations has remained limited. We introduce PatternLocal, a novel XAI\ntechnique that addresses this gap. PatternLocal begins with a locally linear\nsurrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the\nresulting discriminative model weights into a generative representation,\nthereby suppressing the influence of suppressor variables while preserving\nlocal fidelity. In extensive hyperparameter optimization on the XAI-TRIS\nbenchmark, PatternLocal consistently outperformed other XAI methods and reduced\nfalse-positive attributions when explaining non-linear tasks, thereby enabling\nmore reliable and actionable insights.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatternLocal\u7684\u65b0XAI\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u7ebf\u6027\u6a21\u578b\u4e2d\u6291\u5236\u53d8\u91cf\u5bf9\u7279\u5f81\u5f52\u56e0\u7684\u5f71\u54cd\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5047\u9633\u6027\u5f52\u56e0\u3002", "motivation": "\u6291\u5236\u53d8\u91cf\u53ef\u80fd\u5728\u4e0d\u4f9d\u8d56\u76ee\u6807\u7ed3\u679c\u7684\u60c5\u51b5\u4e0b\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\uff0c\u5bfc\u81f4XAI\u65b9\u6cd5\u7684\u7279\u5f81\u5f52\u56e0\u51fa\u73b0\u5047\u9633\u6027\uff0c\u964d\u4f4e\u4e86\u89e3\u91ca\u7684\u5b9e\u7528\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7ebf\u6027\u6a21\u578b\uff0c\u5bf9\u975e\u7ebf\u6027\u6a21\u578b\u548c\u5b9e\u4f8b\u89e3\u91ca\u7684\u6269\u5c55\u6709\u9650\u3002", "method": "PatternLocal\u4ece\u5c40\u90e8\u7ebf\u6027\u4ee3\u7406\uff08\u5982LIME\u3001KernelSHAP\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff09\u51fa\u53d1\uff0c\u5c06\u5224\u522b\u6a21\u578b\u6743\u91cd\u8f6c\u6362\u4e3a\u751f\u6210\u8868\u793a\uff0c\u4ece\u800c\u6291\u5236\u6291\u5236\u53d8\u91cf\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u4fdd\u771f\u5ea6\u3002", "result": "\u5728XAI-TRIS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPatternLocal\u5728\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6XAI\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u975e\u7ebf\u6027\u4efb\u52a1\u4e2d\u7684\u5047\u9633\u6027\u5f52\u56e0\u3002", "conclusion": "PatternLocal\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u548c\u53ef\u64cd\u4f5c\u7684XAI\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u548c\u5b9e\u4f8b\u89e3\u91ca\u3002", "relevance": 70.0}}
{"id": "2505.11468", "pdf": "https://arxiv.org/pdf/2505.11468", "abs": "https://arxiv.org/abs/2505.11468", "authors": ["Dingbang Huang", "Wenbo Li", "Yifei Zhao", "Xinyu Pan", "Yanhong Zeng", "Bo Dai"], "title": "PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/dingbang777/PSDiffusion/", "summary": "Diffusion models have made remarkable advancements in generating high-quality\nimages from textual descriptions. Recent works like LayerDiffuse have extended\nthe previous single-layer, unified image generation paradigm to transparent\nimage layer generation. However, existing multi-layer generation methods fail\nto handle the interactions among multiple layers such as rational global\nlayout, physics-plausible contacts and visual effects like shadows and\nreflections while maintaining high alpha quality. To solve this problem, we\npropose PSDiffusion, a unified diffusion framework for simultaneous multi-layer\ntext-to-image generation. Our model can automatically generate multi-layer\nimages with one RGB background and multiple RGBA foregrounds through a single\nfeed-forward process. Unlike existing methods that combine multiple tools for\npost-decomposition or generate layers sequentially and separately, our method\nintroduces a global-layer interactive mechanism that generates layered-images\nconcurrently and collaboratively, ensuring not only high quality and\ncompleteness for each layer, but also spatial and visual interactions among\nlayers for global coherence.", "AI": {"tldr": "PSDiffusion\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u751f\u6210\u591a\u5c42\u6587\u672c\u5230\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c42\u4ea4\u4e92\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u591a\u5c42\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5c42\u95f4\u4ea4\u4e92\uff08\u5982\u5168\u5c40\u5e03\u5c40\u3001\u7269\u7406\u63a5\u89e6\u548c\u89c6\u89c9\u6548\u679c\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "method": "\u63d0\u51faPSDiffusion\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5c42\u4ea4\u4e92\u673a\u5236\uff0c\u5355\u6b21\u524d\u9988\u8fc7\u7a0b\u751f\u6210\u591a\u5c42\u56fe\u50cf\uff08RGB\u80cc\u666f\u548c\u591a\u4e2aRGBA\u524d\u666f\uff09\u3002", "result": "\u6a21\u578b\u80fd\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b8c\u6574\u7684\u5404\u5c42\u56fe\u50cf\uff0c\u5e76\u786e\u4fdd\u5c42\u95f4\u7684\u7a7a\u95f4\u548c\u89c6\u89c9\u4ea4\u4e92\u4ee5\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u3002", "conclusion": "PSDiffusion\u4e3a\u591a\u5c42\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.11211", "pdf": "https://arxiv.org/pdf/2505.11211", "abs": "https://arxiv.org/abs/2505.11211", "authors": ["Francisco Madaleno", "Pernille Julie Viuff Sand", "Francisco C. Pereira", "Sergio Hernan Garrido Mejia"], "title": "Bayesian Hierarchical Invariant Prediction", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": null, "summary": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u7684\u56e0\u679c\u9884\u6d4b\u65b9\u6cd5BHIP\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfICP\u7684\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5229\u7528\u5148\u9a8c\u4fe1\u606f\u63d0\u9ad8\u4e86\u56e0\u679c\u7279\u5f81\u7684\u8bc6\u522b\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edfICP\u65b9\u6cd5\u5728\u5f02\u6784\u6570\u636e\u4e0b\u56e0\u679c\u673a\u5236\u4e0d\u53d8\u6027\u6d4b\u8bd5\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cBHIP\u901a\u8fc7\u5206\u5c42\u8d1d\u53f6\u65af\u6846\u67b6\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BHIP\u901a\u8fc7\u5206\u5c42\u8d1d\u53f6\u65af\u6846\u67b6\u663e\u5f0f\u6d4b\u8bd5\u56e0\u679c\u673a\u5236\u7684\u4e0d\u53d8\u6027\uff0c\u5e76\u5229\u7528\u9a6c\u523a\u5148\u9a8c\u548c\u9489\u677f\u5148\u9a8c\u7b49\u7a00\u758f\u8bf1\u5bfc\u5148\u9a8c\u8bc6\u522b\u56e0\u679c\u7279\u5f81\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBHIP\u5728\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u548c\u56e0\u679c\u7279\u5f81\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfICP\u65b9\u6cd5\u3002", "conclusion": "BHIP\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3ICP\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u548c\u5927\u89c4\u6a21\u9884\u6d4b\u53d8\u91cf\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "relevance": 40.0}}
{"id": "2505.11482", "pdf": "https://arxiv.org/pdf/2505.11482", "abs": "https://arxiv.org/abs/2505.11482", "authors": ["Shirin Shoushtari", "Edward P. Chandler", "Yuanhao Wang", "M. Salman Asif", "Ulugbek S. Kamilov"], "title": "Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models are widely used as priors in imaging inverse problems.\nHowever, their performance often degrades under distribution shifts between the\ntraining and test-time images. Existing methods for identifying and quantifying\ndistribution shifts typically require access to clean test images, which are\nalmost never available while solving inverse problems (at test time). We\npropose a fully unsupervised metric for estimating distribution shifts using\nonly indirect (corrupted) measurements and score functions from diffusion\nmodels trained on different datasets. We theoretically show that this metric\nestimates the KL divergence between the training and test image distributions.\nEmpirically, we show that our score-based metric, using only corrupted\nmeasurements, closely approximates the KL divergence computed from clean\nimages. Motivated by this result, we show that aligning the out-of-distribution\nscore with the in-distribution score -- using only corrupted measurements --\nreduces the KL divergence and leads to improved reconstruction quality across\nmultiple inverse problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5229\u7528\u95f4\u63a5\u6d4b\u91cf\u548c\u6269\u6563\u6a21\u578b\u7684\u5206\u6570\u51fd\u6570\u4f30\u8ba1\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u5206\u6570\u6539\u5584\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u56fe\u50cf\u5206\u5e03\u504f\u79fb\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5e72\u51c0\u7684\u6d4b\u8bd5\u56fe\u50cf\uff0c\u800c\u9006\u95ee\u9898\u4e2d\u901a\u5e38\u65e0\u6cd5\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u95f4\u63a5\u6d4b\u91cf\u548c\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6269\u6563\u6a21\u578b\u5206\u6570\u51fd\u6570\u6765\u4f30\u8ba1\u5206\u5e03\u504f\u79fb\u3002\u7406\u8bba\u8bc1\u660e\u8be5\u5ea6\u91cf\u4f30\u8ba1\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u56fe\u50cf\u5206\u5e03\u95f4\u7684KL\u6563\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u57fa\u4e8e\u5206\u6570\u7684\u5ea6\u91cf\u4ec5\u4f7f\u7528\u635f\u574f\u6d4b\u91cf\u5373\u53ef\u8fd1\u4f3c\u5e72\u51c0\u56fe\u50cf\u7684KL\u6563\u5ea6\u3002\u901a\u8fc7\u5bf9\u9f50\u5206\u6570\uff0c\u51cf\u5c11\u4e86KL\u6563\u5ea6\u5e76\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5e72\u51c0\u6d4b\u8bd5\u56fe\u50cf\u5373\u53ef\u6709\u6548\u4f30\u8ba1\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u901a\u8fc7\u5206\u6570\u5bf9\u9f50\u63d0\u5347\u9006\u95ee\u9898\u4e2d\u7684\u91cd\u5efa\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.11221", "pdf": "https://arxiv.org/pdf/2505.11221", "abs": "https://arxiv.org/abs/2505.11221", "authors": ["Donghoon Lee", "Tung M. Luu", "Younghwan Lee", "Chang D. Yoo"], "title": "Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation", "categories": ["cs.LG"], "comment": "5 pages, ICASSP 2025. The first two authors are equally contributed", "summary": "Recent research highlights the potential of multimodal foundation models in\ntackling complex decision-making challenges. However, their large parameters\nmake real-world deployment resource-intensive and often impractical for\nconstrained systems. Reinforcement learning (RL) shows promise for\ntask-specific agents but suffers from high sample complexity, limiting\npractical applications. To address these challenges, we introduce LVLM to\nPolicy (LVLM2P), a novel framework that distills knowledge from large\nvision-language models (LVLM) into more efficient RL agents. Our approach\nleverages the LVLM as a teacher, providing instructional actions based on\ntrajectories collected by the RL agent, which helps reduce less meaningful\nexploration in the early stages of learning, thereby significantly accelerating\nthe agent's learning progress. Additionally, by leveraging the LVLM to suggest\nactions directly from visual observations, we eliminate the need for manual\ntextual descriptors of the environment, enhancing applicability across diverse\ntasks. Experiments show that LVLM2P significantly enhances the sample\nefficiency of baseline RL algorithms.", "AI": {"tldr": "LVLM2P\u6846\u67b6\u901a\u8fc7\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u7684\u6837\u672c\u6548\u7387\uff0c\u51cf\u5c11\u65e9\u671f\u63a2\u7d22\u7684\u65e0\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u90e8\u7f72\u8d44\u6e90\u5bc6\u96c6\u548cRL\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528LVLM\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u57fa\u4e8eRL\u4ee3\u7406\u6536\u96c6\u7684\u8f68\u8ff9\u63d0\u4f9b\u6307\u5bfc\u52a8\u4f5c\uff0c\u76f4\u63a5\u4ece\u89c6\u89c9\u89c2\u5bdf\u751f\u6210\u52a8\u4f5c\u5efa\u8bae\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebfRL\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "LVLM2P\u4e3a\u9ad8\u6548RL\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u3002", "relevance": 75.0}}
{"id": "2505.11493", "pdf": "https://arxiv.org/pdf/2505.11493", "abs": "https://arxiv.org/abs/2505.11493", "authors": ["Yusu Qian", "Jiasen Lu", "Tsu-Jui Fu", "Xinze Wang", "Chen Chen", "Yinfei Yang", "Wenze Hu", "Zhe Gan"], "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Editing images using natural language instructions has become a natural and\nexpressive way to modify visual content; yet, evaluating the performance of\nsuch models remains challenging. Existing evaluation approaches often rely on\nimage-text similarity metrics like CLIP, which lack precision. In this work, we\nintroduce a new benchmark designed to evaluate text-guided image editing models\nin a more grounded manner, along two critical dimensions: (i) functional\ncorrectness, assessed via automatically generated multiple-choice questions\nthat verify whether the intended change was successfully applied; and (ii)\nimage content preservation, which ensures that non-targeted regions of the\nimage remain visually consistent using an object-aware masking technique and\npreservation scoring. The benchmark includes over 1000 high-quality editing\nexamples across 20 diverse content categories, each annotated with detailed\nediting instructions, evaluation questions, and spatial object masks. We\nconduct a large-scale study comparing GPT-Image-1, the latest flagship in the\ntext-guided image editing space, against several state-of-the-art editing\nmodels, and validate our automatic metrics against human ratings. Results show\nthat GPT-Image-1 leads in instruction-following accuracy, but often\nover-modifies irrelevant image regions, highlighting a key trade-off in the\ncurrent model behavior. GIE-Bench provides a scalable, reproducible framework\nfor advancing more accurate evaluation of text-guided image editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5GIE-Bench\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\u548c\u56fe\u50cf\u5185\u5bb9\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982CLIP\uff09\u7f3a\u4e4f\u7cbe\u786e\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u591a\u9009\u9898\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5e76\u4f7f\u7528\u5bf9\u8c61\u611f\u77e5\u63a9\u7801\u6280\u672f\u8bc4\u4f30\u56fe\u50cf\u5185\u5bb9\u4fdd\u7559\u3002", "result": "GPT-Image-1\u5728\u6307\u4ee4\u8ddf\u968f\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u4fee\u6539\u975e\u76ee\u6807\u533a\u57df\u7684\u95ee\u9898\u3002", "conclusion": "GIE-Bench\u4e3a\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "relevance": 40.0}}
{"id": "2505.11230", "pdf": "https://arxiv.org/pdf/2505.11230", "abs": "https://arxiv.org/abs/2505.11230", "authors": ["Oskar Bohn Lassen", "Serio Agriesti", "Mohamed Eldafrawi", "Daniele Gammelli", "Guido Cantelmo", "Guido Gentile", "Francisco Camara Pereira"], "title": "Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment", "categories": ["cs.LG"], "comment": null, "summary": "The Traffic Assignment Problem is a fundamental, yet computationally\nexpensive, task in transportation modeling, especially for large-scale\nnetworks. Traditional methods require iterative simulations to reach\nequilibrium, making real-time or large-scale scenario analysis challenging. In\nthis paper, we propose a learning-based approach using Message-Passing Neural\nNetworks as a metamodel to approximate the equilibrium flow of the Stochastic\nUser Equilibrium assignment. Our model is designed to mimic the algorithmic\nstructure used in conventional traffic simulators allowing it to better capture\nthe underlying process rather than just the data. We benchmark it against other\nconventional deep learning techniques and evaluate the model's robustness by\ntesting its ability to predict traffic flows on input data outside the domain\non which it was trained. This approach offers a promising solution for\naccelerating out-of-distribution scenario assessments, reducing computational\ncosts in large-scale transportation planning, and enabling real-time\ndecision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u968f\u673a\u7528\u6237\u5747\u8861\u5206\u914d\u7684\u4ea4\u901a\u6d41\uff0c\u4ee5\u52a0\u901f\u5927\u89c4\u6a21\u4ea4\u901a\u89c4\u5212\u7684\u8bc4\u4f30\u548c\u5b9e\u65f6\u51b3\u7b56\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u5206\u914d\u95ee\u9898\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6216\u5927\u89c4\u6a21\u573a\u666f\u5206\u6790\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u5143\u6a21\u578b\uff0c\u6a21\u62df\u4f20\u7edf\u4ea4\u901a\u6a21\u62df\u5668\u7684\u7b97\u6cd5\u7ed3\u6784\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u5e95\u5c42\u8fc7\u7a0b\u3002", "result": "\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u80fd\u591f\u52a0\u901f\u4ea4\u901a\u6d41\u9884\u6d4b\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u4ea4\u901a\u89c4\u5212\u548c\u5b9e\u65f6\u51b3\u7b56\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.11497", "pdf": "https://arxiv.org/pdf/2505.11497", "abs": "https://arxiv.org/abs/2505.11497", "authors": ["Yushi Huang", "Ruihao Gong", "Jing Liu", "Yifu Ding", "Chengtao Lv", "Haotong Qin", "Jun Zhang"], "title": "QVGen: Pushing the Limit of Quantized Video Generative Models", "categories": ["cs.CV"], "comment": "Our code will be released upon acceptance", "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of $\\Phi$, we propose a\nrank-decay strategy that progressively eliminates $\\Phi$. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization $\\mathbf{\\gamma}$ to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and\n$+8.43$ in Scene Consistency on VBench.", "AI": {"tldr": "QVGen\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\uff08\u59824\u4f4d\u6216\u66f4\u4f4e\uff09\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faQVGen\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u6a21\u5757\uff08\u03a6\uff09\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u5e76\u91c7\u7528\u79e9\u8870\u51cf\u7b56\u7565\u9010\u6b65\u6d88\u9664\u03a6\uff0c\u4ee5\u4fdd\u6301\u6027\u80fd\u5e76\u6d88\u9664\u63a8\u7406\u5f00\u9500\u3002", "result": "\u57284\u4f4d\u91cf\u5316\u4e0b\uff0cQVGen\u9996\u6b21\u8fbe\u5230\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u76f8\u5f53\u7684\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "QVGen\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "relevance": 70.0}}
{"id": "2505.11235", "pdf": "https://arxiv.org/pdf/2505.11235", "abs": "https://arxiv.org/abs/2505.11235", "authors": ["Fei Wu", "Jia Hu", "Geyong Min", "Shiqiang Wang"], "title": "Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation", "categories": ["cs.LG"], "comment": null, "summary": "Driven by the relentless growth in model parameters, which renders full\nfine-tuning prohibitively expensive for large-scale deployment,\nparameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for\nrapidly adapting large models to a wide range of downstream tasks. Among the\nPEFT family, orthogonal fine-tuning and its variants have demonstrated\nremarkable performance by preserving hyperspherical energy, which encodes\npairwise angular similarity between neurons. However, these methods are\ninherently memory-inefficient due to the need to store intermediate activations\nfrom multiple full-dimensional sparse matrices. To address this limitation, we\npropose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace\nadaptation. Specifically, we first establish a theoretical condition under\nwhich orthogonal transformations within a low-rank subspace preserve\nhyperspherical energy. Based on this insight, we constrain orthogonal\nfine-tuning to the principal subspace defined by the top-r components obtained\nthrough singular value decomposition and impose an additional constraint on the\nprojection matrix to satisfy the preservation condition. To enhance MOFT's\nflexibility across tasks, we relax strict orthogonality by introducing two\nlearnable scaling vectors. Extensive experiments on 37 diverse tasks and four\nmodels across NLP and CV demonstrate that MOFT consistently outperforms key\nbaselines while significantly reducing the memory footprint of orthogonal\nfine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08MOFT\uff09\uff0c\u901a\u8fc7\u4e3b\u6210\u5206\u5b50\u7a7a\u95f4\u9002\u5e94\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u53c2\u6570\u589e\u957f\uff0c\u5168\u53c2\u6570\u5fae\u8c03\u6210\u672c\u8fc7\u9ad8\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6210\u4e3a\u5173\u952e\u3002\u6b63\u4ea4\u5fae\u8c03\u867d\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5185\u5b58\u6548\u7387\u4f4e\u3002", "method": "MOFT\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\u5c06\u6b63\u4ea4\u53d8\u6362\u9650\u5236\u5728\u4e3b\u6210\u5206\u5b50\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u53ef\u5b66\u4e60\u7f29\u653e\u5411\u91cf\u589e\u5f3a\u7075\u6d3b\u6027\u3002", "result": "\u572837\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u548c4\u4e2a\u6a21\u578b\u4e2d\uff0cMOFT\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u4e14\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "MOFT\u4e3a\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u5185\u5b58\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "relevance": 75.0}}
{"id": "2505.11239", "pdf": "https://arxiv.org/pdf/2505.11239", "abs": "https://arxiv.org/abs/2505.11239", "authors": ["Wilson Wongso", "Hao Xue", "Flora D. Salim"], "title": "Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks", "categories": ["cs.LG"], "comment": null, "summary": "Understanding human mobility through Point-of-Interest (POI) recommendation\nis increasingly important for applications such as urban planning, personalized\nservices, and generative agent simulation. However, progress in this field is\nhindered by two key challenges: the over-reliance on older datasets from\n2012-2013 and the lack of reproducible, city-level check-in datasets that\nreflect diverse global regions. To address these gaps, we present Massive-STEPS\n(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,\npublicly available benchmark dataset built upon the Semantic Trails dataset and\nenriched with semantic POI metadata. Massive-STEPS spans 12 geographically and\nculturally diverse cities and features more recent (2017-2018) and\nlonger-duration (24 months) check-in data than prior datasets. We benchmarked a\nwide range of POI recommendation models on Massive-STEPS using both supervised\nand zero-shot approaches, and evaluated their performance across multiple urban\ncontexts. By releasing Massive-STEPS, we aim to facilitate reproducible and\nequitable research in human mobility and POI recommendation. The dataset and\nbenchmarking code are available at:\nhttps://github.com/cruiseresearchgroup/Massive-STEPS", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Massive-STEPS\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3POI\u63a8\u8350\u9886\u57df\u7684\u6570\u636e\u9648\u65e7\u548c\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u5728\u591a\u57ce\u5e02\u80cc\u666f\u4e0b\u8bc4\u4f30\u4e86\u591a\u79cd\u63a8\u8350\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3POI\u63a8\u8350\u9886\u57df\u5bf9\u8001\u65e7\u6570\u636e\u96c6\uff082012-2013\uff09\u7684\u4f9d\u8d56\u548c\u7f3a\u4e4f\u5168\u7403\u591a\u6837\u6027\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86Massive-STEPS\u6570\u636e\u96c6\uff0c\u8986\u76d612\u4e2a\u591a\u6837\u5316\u57ce\u5e02\uff0c\u5305\u542b2017-2018\u5e74\u768424\u4e2a\u6708\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u4e86\u76d1\u7763\u548c\u96f6\u6837\u672c\u63a8\u8350\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4ee3\u7801\uff0c\u652f\u6301\u53ef\u91cd\u590d\u548c\u516c\u5e73\u7684\u7814\u7a76\u3002", "conclusion": "Massive-STEPS\u6709\u52a9\u4e8e\u63a8\u52a8\u4eba\u7c7b\u79fb\u52a8\u6027\u548cPOI\u63a8\u8350\u9886\u57df\u7684\u7814\u7a76\u3002", "relevance": 30.0}}
{"id": "2505.10578", "pdf": "https://arxiv.org/pdf/2505.10578", "abs": "https://arxiv.org/abs/2505.10578", "authors": ["Yunji Feng", "Chengpu Yu", "Fengrui Ran", "Zhi Yang", "Yinni Liu"], "title": "ExploreGS: a vision-based low overhead framework for 3D scene reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper proposes a low-overhead, vision-based 3D scene reconstruction\nframework for drones, named ExploreGS. By using RGB images, ExploreGS replaces\ntraditional lidar-based point cloud acquisition process with a vision model,\nachieving a high-quality reconstruction at a lower cost. The framework\nintegrates scene exploration and model reconstruction, and leverags a\nBag-of-Words(BoW) model to enable real-time processing capabilities, therefore,\nthe 3D Gaussian Splatting (3DGS) training can be executed on-board.\nComprehensive experiments in both simulation and real-world environments\ndemonstrate the efficiency and applicability of the ExploreGS framework on\nresource-constrained devices, while maintaining reconstruction quality\ncomparable to state-of-the-art methods.", "AI": {"tldr": "ExploreGS\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u4f4e\u5f00\u95003D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\uff0c\u66ff\u4ee3\u4f20\u7edf\u6fc0\u5149\u96f7\u8fbe\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u91c7\u96c6\u6210\u672c\u9ad8\uff0c\u63a2\u7d22\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u573a\u666f\u63a2\u7d22\u548c\u6a21\u578b\u91cd\u5efa\uff0c\u5229\u7528BoW\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\uff0c\u652f\u6301\u673a\u8f7d3D\u9ad8\u65af\u6e85\u5c04\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9ad8\u6548\u6027\u548c\u9002\u7528\u6027\uff0c\u91cd\u5efa\u8d28\u91cf\u4e0e\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ExploreGS\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4f4e\u6210\u672c3D\u91cd\u5efa\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "relevance": 20.0}}
{"id": "2505.10885", "pdf": "https://arxiv.org/pdf/2505.10885", "abs": "https://arxiv.org/abs/2505.10885", "authors": ["Istiaq Ahmed Fahad", "Kamruzzaman Asif", "Sifat Sikder"], "title": "BanglaFake: Constructing and Evaluating a Specialized Bengali Deepfake Audio Dataset", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 page", "summary": "Deepfake audio detection is challenging for low-resource languages like\nBengali due to limited datasets and subtle acoustic features. To address this,\nwe introduce BangalFake, a Bengali Deepfake Audio Dataset with 12,260 real and\n13,260 deepfake utterances. Synthetic speech is generated using SOTA\nText-to-Speech (TTS) models, ensuring high naturalness and quality. We evaluate\nthe dataset through both qualitative and quantitative analyses. Mean Opinion\nScore (MOS) from 30 native speakers shows Robust-MOS of 3.40 (naturalness) and\n4.01 (intelligibility). t-SNE visualization of MFCCs highlights real vs. fake\ndifferentiation challenges. This dataset serves as a crucial resource for\nadvancing deepfake detection in Bengali, addressing the limitations of\nlow-resource language research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BangalFake\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\uff0c\u5305\u542b12,260\u771f\u5b9e\u548c13,260\u4f2a\u9020\u8bed\u97f3\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u8bc4\u4f30\u5176\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u56e0\u6570\u636e\u96c6\u6709\u9650\u4e14\u58f0\u5b66\u7279\u5f81\u7ec6\u5fae\u3002", "method": "\u4f7f\u7528SOTA TTS\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u8bed\u97f3\uff0c\u901a\u8fc7MOS\u8bc4\u5206\u548ct-SNE\u53ef\u89c6\u5316\u5206\u6790\u6570\u636e\u96c6\u3002", "result": "MOS\u663e\u793a\u81ea\u7136\u5ea6\u4e3a3.40\uff0c\u53ef\u61c2\u5ea6\u4e3a4.01\uff1bt-SNE\u63ed\u793a\u4e86\u771f\u5b9e\u4e0e\u4f2a\u9020\u97f3\u9891\u7684\u533a\u5206\u96be\u70b9\u3002", "conclusion": "BangalFake\u6570\u636e\u96c6\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002", "relevance": 30.0}}
{"id": "2505.11243", "pdf": "https://arxiv.org/pdf/2505.11243", "abs": "https://arxiv.org/abs/2505.11243", "authors": ["Elliot L. Epstein", "Apaar Sadhwani", "Kay Giesecke"], "title": "A Set-Sequence Model for Time Series", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.6"], "comment": "Presented at the Workshop on Financial AI at ICLR 2025", "summary": "In many financial prediction problems, the behavior of individual units (such\nas loans, bonds, or stocks) is influenced by observable unit-level factors and\nmacroeconomic variables, as well as by latent cross-sectional effects.\nTraditional approaches attempt to capture these latent effects via handcrafted\nsummary features. We propose a Set-Sequence model that eliminates the need for\nhandcrafted features. The Set model first learns a shared cross-sectional\nsummary at each period. The Sequence model then ingests the summary-augmented\ntime series for each unit independently to predict its outcome. Both components\nare learned jointly over arbitrary sets sampled during training. Our approach\nharnesses the set nature of the cross-section and is computationally efficient,\ngenerating set summaries in linear time relative to the number of units. It is\nalso flexible, allowing the use of existing sequence models and accommodating a\nvariable number of units at inference. Empirical evaluations demonstrate that\nour Set-Sequence model significantly outperforms benchmarks on stock return\nprediction and mortgage behavior tasks. Code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdSet-Sequence\u6a21\u578b\uff0c\u7528\u4e8e\u91d1\u878d\u9884\u6d4b\u95ee\u9898\uff0c\u65e0\u9700\u624b\u5de5\u7279\u5f81\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8de8\u622a\u9762\u548c\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u6355\u6349\u8de8\u622a\u9762\u6548\u5e94\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "method": "Set\u6a21\u578b\u5b66\u4e60\u5171\u4eab\u8de8\u622a\u9762\u6458\u8981\uff0cSequence\u6a21\u578b\u72ec\u7acb\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4e24\u8005\u8054\u5408\u5b66\u4e60\u3002", "result": "\u5728\u80a1\u7968\u6536\u76ca\u9884\u6d4b\u548c\u62b5\u62bc\u8d37\u6b3e\u884c\u4e3a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "Set-Sequence\u6a21\u578b\u9ad8\u6548\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u91d1\u878d\u9884\u6d4b\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.10672", "pdf": "https://arxiv.org/pdf/2505.10672", "abs": "https://arxiv.org/abs/2505.10672", "authors": ["Hania Ghouse", "Muzammil Behzad"], "title": "MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Efficient and accurate multi-organ segmentation from abdominal CT volumes is\na fundamental challenge in medical image analysis. Existing 3D segmentation\napproaches are computationally and memory intensive, often processing entire\nvolumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods\nsuffer from class imbalance and lack cross-view contextual awareness. To\naddress these limitations, we propose a novel, anatomically-aware slice\nselector pipeline that reduces input volume prior to segmentation. Our unified\nframework introduces a vision-language model (VLM) for cross-view organ\npresence detection using fused tri-slice (2.5D) representations from axial,\nsagittal, and coronal planes. Our proposed model acts as an \"expert\" in\nanatomical localization, reasoning over multi-view representations to\nselectively retain slices with high structural relevance. This enables\nspatially consistent filtering across orientations while preserving contextual\ncues. More importantly, since standard segmentation metrics such as Dice or IoU\nfail to measure the spatial precision of such slice selection, we introduce a\nnovel metric, Slice Localization Concordance (SLC), which jointly captures\nanatomical coverage and spatial alignment with organ-centric reference slices.\nUnlike segmentation-specific metrics, SLC provides a model-agnostic evaluation\nof localization fidelity. Our model offers substantial improvement gains\nagainst several baselines across all organs, demonstrating both accurate and\nreliable organ-focused slice filtering. These results show that our method\nenables efficient and spatially consistent organ filtering, thereby\nsignificantly reducing downstream segmentation cost while maintaining high\nanatomical fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5207\u7247\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u591a\u5668\u5b98\u5206\u5272\uff0c\u901a\u8fc7\u51cf\u5c11\u8f93\u5165\u6570\u636e\u91cf\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807SLC\u3002", "motivation": "\u73b0\u67093D\u5206\u5272\u65b9\u6cd5\u8ba1\u7b97\u548c\u5185\u5b58\u6d88\u8017\u9ad8\uff0c2D\u65b9\u6cd5\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408VLM\u7684\u5207\u7247\u9009\u62e9\u6846\u67b6\uff0c\u5229\u7528\u4e09\u89c6\u56fe\u878d\u5408\uff082.5D\uff09\u8fdb\u884c\u5668\u5b98\u5b9a\u4f4d\uff0c\u5e76\u5f15\u5165SLC\u6307\u6807\u8bc4\u4f30\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u5668\u5b98\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a7a\u95f4\u4e00\u81f4\u7684\u5668\u5b98\u5207\u7247\u8fc7\u6ee4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5206\u5272\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u89e3\u5256\u5b66\u4fdd\u771f\u5ea6\u3002", "relevance": 30.0}}
{"id": "2505.10900", "pdf": "https://arxiv.org/pdf/2505.10900", "abs": "https://arxiv.org/abs/2505.10900", "authors": ["Wenqing Zheng", "Noah Fatsi", "Daniel Barcklow", "Dmitri Kalaev", "Steven Yao", "Owen Reinert", "C. Bayan Bruss", "Daniele Rosa"], "title": "Explain What You Mean: Intent Augmented Knowledge Graph Recommender Built With LLM", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Interaction sparsity is the primary obstacle for recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nalso is found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this sparsity issue shifts the performance\nbottleneck to other areas in the computational pipeline. Those that focus on\nenriching sparse representations with connectivity data from other external\nsources propose methods that are resource demanding and require careful domain\nexpert aided addition of this newly introduced data. Others that turn to Large\nLanguage Model (LLM) based recommenders will quickly encounter limitations\nsurrounding data quality and availability. In this work, we propose LLM-based\nIntent Knowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR learns latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u610f\u56fe\u77e5\u8bc6\u56fe\u8c31\u63a8\u8350\u6846\u67b6\uff08IKGR\uff09\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u7f16\u7801\u65b9\u6cd5\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7a00\u758f\u6027\u95ee\u9898\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4ea4\u4e92\u7a00\u758f\u6027\u662f\u4e3b\u8981\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8d44\u6e90\u5bc6\u96c6\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u6570\u636e\u8d28\u91cf\u548c\u53ef\u7528\u6027\u3002IKGR\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u548c\u7a20\u5bc6\u5316\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "IKGR\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u7f16\u7801\u65b9\u6cd5\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5b66\u4e60\u7528\u6237-\u7269\u54c1\u7684\u6f5c\u5728\u4eb2\u548c\u529b\uff0c\u5e76\u901a\u8fc7\u610f\u56fe\u8fde\u901a\u6027\u8fdb\u4e00\u6b65\u7a20\u5bc6\u5316\u56fe\u8c31\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIKGR\u5728\u516c\u5f00\u548c\u5185\u90e8\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IKGR\u901a\u8fc7\u610f\u56fe\u77e5\u8bc6\u56fe\u8c31\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u3002", "relevance": 50.0}}
{"id": "2505.11250", "pdf": "https://arxiv.org/pdf/2505.11250", "abs": "https://arxiv.org/abs/2505.11250", "authors": ["Xvyuan Liu", "Xiangfei Qiu", "Xingjian Wu", "Zhengyu Li", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "title": "Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline", "categories": ["cs.LG"], "comment": null, "summary": "The forecasting of irregular multivariate time series (IMTS) is crucial in\nkey areas such as healthcare, biomechanics, climate science, and astronomy.\nHowever, achieving accurate and practical predictions is challenging due to two\nmain factors. First, the inherent irregularity and data missingness in\nirregular time series make modeling difficult. Second, most existing methods\nare typically complex and resource-intensive. In this study, we propose a\ngeneral framework called APN to address these challenges. Specifically, we\ndesign a novel Time-Aware Patch Aggregation (TAPA) module that achieves\nadaptive patching. By learning dynamically adjustable patch boundaries and a\ntime-aware weighted averaging strategy, TAPA transforms the original irregular\nsequences into high-quality, regularized representations in a\nchannel-independent manner. Additionally, we use a simple query module to\neffectively integrate historical information while maintaining the model's\nefficiency. Finally, predictions are made by a shallow MLP. Experimental\nresults on multiple real-world datasets show that APN outperforms existing\nstate-of-the-art methods in both efficiency and accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPN\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff08IMTS\uff09\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u7684\u8865\u4e01\u805a\u5408\u6a21\u5757\uff08TAPA\uff09\u548c\u7b80\u5355\u67e5\u8be2\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u9884\u6d4b\u3002", "motivation": "\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff08IMTS\uff09\u9884\u6d4b\u5728\u533b\u7597\u3001\u751f\u7269\u529b\u5b66\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u96be\u4ee5\u5e94\u5bf9\u6570\u636e\u7f3a\u5931\u548c\u4e0d\u89c4\u5219\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u65f6\u95f4\u611f\u77e5\u8865\u4e01\u805a\u5408\u6a21\u5757\uff08TAPA\uff09\uff0c\u52a8\u6001\u8c03\u6574\u8865\u4e01\u8fb9\u754c\u548c\u65f6\u95f4\u611f\u77e5\u52a0\u6743\u5e73\u5747\u7b56\u7565\uff0c\u5c06\u4e0d\u89c4\u5219\u5e8f\u5217\u8f6c\u5316\u4e3a\u89c4\u5219\u5316\u8868\u793a\uff1b\u4f7f\u7528\u7b80\u5355\u67e5\u8be2\u6a21\u5757\u6574\u5408\u5386\u53f2\u4fe1\u606f\uff1b\u6700\u7ec8\u901a\u8fc7\u6d45\u5c42MLP\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cAPN\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "APN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684IMTS\u9884\u6d4b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.10687", "pdf": "https://arxiv.org/pdf/2505.10687", "abs": "https://arxiv.org/abs/2505.10687", "authors": ["Sayed Mehedi Azim", "Brian Corbett", "Iman Dehzangi"], "title": "ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "The hippocampus, a critical brain structure involved in memory processing and\nvarious neurodegenerative and psychiatric disorders, comprises three key\nsubregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3\n(CA3). Accurate segmentation of these subregions from histological tissue\nimages is essential for advancing our understanding of disease mechanisms,\ndevelopmental dynamics, and therapeutic interventions. However, no existing\nmethods address the automated segmentation of hippocampal subregions from\ntissue images, particularly from immunohistochemistry (IHC) images. To bridge\nthis gap, we introduce a novel set of four comprehensive murine hippocampal IHC\ndatasets featuring distinct staining modalities: cFos, NeuN, and multiplexed\nstains combining cFos, NeuN, and either {\\Delta}FosB or GAD67, capturing\nstructural, neuronal activity, and plasticity associated information.\nAdditionally, we propose ROIsGAN, a region-guided U-Net-based generative\nadversarial network tailored for hippocampal subregion segmentation. By\nleveraging adversarial learning, ROIsGAN enhances boundary delineation and\nstructural detail refinement through a novel region-guided discriminator loss\ncombining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3\nsubregions, ROIsGAN consistently outperforms conventional segmentation models,\nachieving performance gains ranging from 1-10% in Dice score and up to 11% in\nIntersection over Union (IoU), particularly under challenging staining\nconditions. Our work establishes foundational datasets and methods for\nautomated hippocampal segmentation, enabling scalable, high-precision analysis\nof tissue images in neuroscience research. Our generated datasets, proposed\nmodel as a standalone tool, and its corresponding source code are publicly\navailable at: https://github.com/MehediAzim/ROIsGAN", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aROIsGAN\u7684\u533a\u57df\u5f15\u5bfc\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5206\u5272\u6d77\u9a6c\u4f53\u4e9a\u533a\uff0c\u5e76\u63d0\u4f9b\u4e86\u56db\u4e2a\u65b0\u7684\u5c0f\u9f20\u6d77\u9a6c\u4f53IHC\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u81ea\u52a8\u5316\u5206\u5272\u6d77\u9a6c\u4f53\u4e9a\u533a\uff0c\u7279\u522b\u662f\u5728IHC\u56fe\u50cf\u4e2d\uff0c\u963b\u788d\u4e86\u5bf9\u75be\u75c5\u673a\u5236\u548c\u6cbb\u7597\u7684\u6df1\u5165\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8eU-Net\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08ROIsGAN\uff09\uff0c\u7ed3\u5408\u533a\u57df\u5f15\u5bfc\u5224\u522b\u5668\u635f\u5931\uff08Dice\u548c\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\uff09\u4f18\u5316\u8fb9\u754c\u548c\u7ed3\u6784\u7ec6\u8282\u3002", "result": "ROIsGAN\u5728DG\u3001CA1\u548cCA3\u4e9a\u533a\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0cDice\u5f97\u5206\u63d0\u53471-10%\uff0cIoU\u63d0\u5347\u8fbe11%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u81ea\u52a8\u5316\u7ec4\u7ec7\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002", "relevance": 10.0}}
{"id": "2505.10903", "pdf": "https://arxiv.org/pdf/2505.10903", "abs": "https://arxiv.org/abs/2505.10903", "authors": ["Ping He", "Yuhao Mao", "Changjiang Li", "Lorenzo Cavallaro", "Ting Wang", "Shouling Ji"], "title": "On the Security Risks of ML-based Malware Detection Systems: A Survey", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Malware presents a persistent threat to user privacy and data integrity. To\ncombat this, machine learning-based (ML-based) malware detection (MD) systems\nhave been developed. However, these systems have increasingly been attacked in\nrecent years, undermining their effectiveness in practice. While the security\nrisks associated with ML-based MD systems have garnered considerable attention,\nthe majority of prior works is limited to adversarial malware examples, lacking\na comprehensive analysis of practical security risks. This paper addresses this\ngap by utilizing the CIA principles to define the scope of security risks. We\nthen deconstruct ML-based MD systems into distinct operational stages, thus\ndeveloping a stage-based taxonomy. Utilizing this taxonomy, we summarize the\ntechnical progress and discuss the gaps in the attack and defense proposals\nrelated to the ML-based MD systems within each stage. Subsequently, we conduct\ntwo case studies, using both inter-stage and intra-stage analyses according to\nthe stage-based taxonomy to provide new empirical insights. Based on these\nanalyses and insights, we suggest potential future directions from both\ninter-stage and intra-stage perspectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7CIA\u539f\u5219\u5b9a\u4e49\u5b89\u5168\u98ce\u9669\u8303\u56f4\uff0c\u5c06\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u5206\u89e3\u4e3a\u4e0d\u540c\u64cd\u4f5c\u9636\u6bb5\uff0c\u63d0\u51fa\u9636\u6bb5\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u6280\u672f\u8fdb\u5c55\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u6027\u6076\u610f\u8f6f\u4ef6\u793a\u4f8b\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u5b89\u5168\u98ce\u9669\u7684\u5168\u9762\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528CIA\u539f\u5219\u5b9a\u4e49\u5b89\u5168\u98ce\u9669\u8303\u56f4\uff0c\u5c06\u7cfb\u7edf\u5206\u89e3\u4e3a\u4e0d\u540c\u64cd\u4f5c\u9636\u6bb5\uff0c\u63d0\u51fa\u9636\u6bb5\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u5206\u6790\u3002", "result": "\u603b\u7ed3\u4e86\u6280\u672f\u8fdb\u5c55\uff0c\u8ba8\u8bba\u4e86\u653b\u51fb\u4e0e\u9632\u5fa1\u63d0\u6848\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4ece\u9636\u6bb5\u5185\u548c\u9636\u6bb5\u95f4\u89c6\u89d2\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "relevance": 30.0}}
{"id": "2505.11254", "pdf": "https://arxiv.org/pdf/2505.11254", "abs": "https://arxiv.org/abs/2505.11254", "authors": ["Jeffrey Willette", "Heejun Lee", "Sung Ju Hwang"], "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction", "categories": ["cs.LG"], "comment": null, "summary": "The attention mechanism of a transformer has a quadratic complexity, leading\nto high inference costs and latency for long sequences. However, attention\nmatrices are mostly sparse, which implies that many entries may be omitted from\ncomputation for efficient inference. Sparse attention inference methods aim to\nreduce this computational burden; however, they also come with a troublesome\nperformance degradation. We discover that one reason for this degradation is\nthat the sparse calculation induces a distributional shift in the attention\noutputs. The distributional shift causes decoding-time queries to fail to align\nwell with the appropriate keys from the prefill stage, leading to a drop in\nperformance. We propose a simple, novel, and effective procedure for correcting\nthis distributional shift, bringing the distribution of sparse attention\noutputs closer to that of quadratic attention. Our method can be applied on top\nof any sparse attention method, and results in an average 36%pt performance\nincrease, recovering 88% of quadratic attention accuracy on the 131K RULER\nbenchmark when applied on top of sliding window attention with sink tokens\nwhile only adding a small overhead. Our method can maintain approximately 98.5%\nsparsity over full quadratic attention, making our model 32 times faster than\nFlash Attention 2 when processing 1M token prefills.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea0\u6b63\u7a00\u758f\u6ce8\u610f\u529b\u8f93\u51fa\u5206\u5e03\u504f\u79fb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u3002", "motivation": "Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u957f\u5e8f\u5217\u63a8\u7406\u6210\u672c\u9ad8\u3002\u7a00\u758f\u6ce8\u610f\u529b\u867d\u80fd\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u4f1a\u5e26\u6765\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\u662f\u7a00\u758f\u8ba1\u7b97\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ea0\u6b63\u7a00\u758f\u6ce8\u610f\u529b\u8f93\u51fa\u7684\u5206\u5e03\u504f\u79fb\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u4e8c\u6b21\u6ce8\u610f\u529b\u7684\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "result": "\u5728131K RULER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534736%\uff0c\u6062\u590d88%\u7684\u4e8c\u6b21\u6ce8\u610f\u529b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u630198.5%\u7684\u7a00\u758f\u6027\uff0c\u4f7f\u6a21\u578b\u6bd4Flash Attention 2\u5feb32\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "relevance": 90.0}}
{"id": "2505.10909", "pdf": "https://arxiv.org/pdf/2505.10909", "abs": "https://arxiv.org/abs/2505.10909", "authors": ["Chiyue Wei", "Bowen Duan", "Cong Guo", "Jingyang Zhang", "Qingyue Song", "Hai \"Helen\" Li", "Yiran Chen"], "title": "Phi: Leveraging Pattern-based Hierarchical Sparsity for High-Efficiency Spiking Neural Networks", "categories": ["cs.AR", "cs.AI"], "comment": "ISCA 2025", "summary": "Spiking Neural Networks (SNNs) are gaining attention for their energy\nefficiency and biological plausibility, utilizing 0-1 activation sparsity\nthrough spike-driven computation. While existing SNN accelerators exploit this\nsparsity to skip zero computations, they often overlook the unique distribution\npatterns inherent in binary activations. In this work, we observe that\nparticular patterns exist in spike activations, which we can utilize to reduce\nthe substantial computation of SNN models. Based on these findings, we propose\na novel \\textbf{pattern-based hierarchical sparsity} framework, termed\n\\textbf{\\textit{Phi}}, to optimize computation.\n  \\textit{Phi} introduces a two-level sparsity hierarchy: Level 1 exhibits\nvector-wise sparsity by representing activations with pre-defined patterns,\nallowing for offline pre-computation with weights and significantly reducing\nmost runtime computation. Level 2 features element-wise sparsity by\ncomplementing the Level 1 matrix, using a highly sparse matrix to further\nreduce computation while maintaining accuracy. We present an algorithm-hardware\nco-design approach. Algorithmically, we employ a k-means-based pattern\nselection method to identify representative patterns and introduce a\npattern-aware fine-tuning technique to enhance Level 2 sparsity.\nArchitecturally, we design \\textbf{\\textit{Phi}}, a dedicated hardware\narchitecture that efficiently processes the two levels of \\textit{Phi} sparsity\non the fly. Extensive experiments demonstrate that \\textit{Phi} achieves a\n$3.45\\times$ speedup and a $4.93\\times$ improvement in energy efficiency\ncompared to state-of-the-art SNN accelerators, showcasing the effectiveness of\nour framework in optimizing SNN computation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhi\u7684\u6a21\u5f0f\u5206\u5c42\u7a00\u758f\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u7ea7\u7a00\u758f\u5c42\u6b21\u4f18\u5316\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u4e863.45\u500d\u7684\u52a0\u901f\u548c4.93\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u73b0\u6709SNN\u52a0\u901f\u5668\u867d\u7136\u5229\u7528\u4e86\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u4e8c\u8fdb\u5236\u6fc0\u6d3b\u7684\u72ec\u7279\u5206\u5e03\u6a21\u5f0f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u5f0f\u5206\u5c42\u7a00\u758f\u6846\u67b6\u8fdb\u4e00\u6b65\u4f18\u5316\u8ba1\u7b97\u3002", "method": "\u63d0\u51faPhi\u6846\u67b6\uff0c\u5305\u542b\u4e24\u7ea7\u7a00\u758f\u5c42\u6b21\uff1aLevel 1\u5229\u7528\u9884\u5b9a\u4e49\u6a21\u5f0f\u5b9e\u73b0\u5411\u91cf\u7ea7\u7a00\u758f\uff0cLevel 2\u8865\u5145\u7a00\u758f\u77e9\u9635\u3002\u91c7\u7528k-means\u6a21\u5f0f\u9009\u62e9\u548c\u6a21\u5f0f\u611f\u77e5\u5fae\u8c03\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u7528\u786c\u4ef6\u67b6\u6784\u3002", "result": "Phi\u5b9e\u73b0\u4e863.45\u500d\u7684\u52a0\u901f\u548c4.93\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709SNN\u52a0\u901f\u5668\u3002", "conclusion": "Phi\u6846\u67b6\u901a\u8fc7\u6a21\u5f0f\u5206\u5c42\u7a00\u758f\u548c\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86SNN\u7684\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548\u3002", "relevance": 40.0}}
{"id": "2505.11261", "pdf": "https://arxiv.org/pdf/2505.11261", "abs": "https://arxiv.org/abs/2505.11261", "authors": ["Jingyang Li", "Jiuqian Shang", "Yang Chen"], "title": "Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Tensor completion is crucial in many scientific domains with missing data\nproblems. Traditional low-rank tensor models, including CP, Tucker, and\nTensor-Train, exploit low-dimensional structures to recover missing data.\nHowever, these methods often treat all tensor modes symmetrically, failing to\ncapture the unique spatiotemporal patterns inherent in scientific data, where\nthe temporal component exhibits both low-frequency stability and high-frequency\nvariations. To address this, we propose a novel model, \\underline{F}ourier\n\\underline{Lo}w-rank and \\underline{S}parse \\underline{T}ensor (FLoST), which\ndecomposes the tensor along the temporal dimension using a Fourier transform.\nThis approach captures low-frequency components with low-rank matrices and\nhigh-frequency fluctuations with sparsity, resulting in a hybrid structure that\nefficiently models both smooth and localized variations. Compared to the\nwell-known tubal-rank model, which assumes low-rankness across all frequency\ncomponents, FLoST requires significantly fewer parameters, making it\ncomputationally more efficient, particularly when the time dimension is large.\nThrough theoretical analysis and empirical experiments, we demonstrate that\nFLoST outperforms existing tensor completion models in terms of both accuracy\nand computational efficiency, offering a more interpretable solution for\nspatiotemporal data reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFLoST\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u5206\u89e3\u65f6\u95f4\u7ef4\u5ea6\uff0c\u7ed3\u5408\u4f4e\u79e9\u77e9\u9635\u548c\u7a00\u758f\u6027\uff0c\u9ad8\u6548\u5efa\u6a21\u65f6\u7a7a\u6570\u636e\u4e2d\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u53d8\u5316\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u5f20\u91cf\u6a21\u578b\u5bf9\u79f0\u5904\u7406\u6240\u6709\u7ef4\u5ea6\uff0c\u65e0\u6cd5\u6355\u6349\u65f6\u7a7a\u6570\u636e\u4e2d\u65f6\u95f4\u7ef4\u5ea6\u7684\u72ec\u7279\u6a21\u5f0f\uff08\u4f4e\u9891\u7a33\u5b9a\u6027\u548c\u9ad8\u9891\u53d8\u5316\uff09\u3002", "method": "FLoST\u6a21\u578b\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u5206\u89e3\u65f6\u95f4\u7ef4\u5ea6\uff0c\u4f4e\u9891\u90e8\u5206\u7528\u4f4e\u79e9\u77e9\u9635\u5efa\u6a21\uff0c\u9ad8\u9891\u90e8\u5206\u7528\u7a00\u758f\u6027\u8868\u793a\u3002", "result": "FLoST\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u5f20\u91cf\u8865\u5168\u6a21\u578b\uff0c\u53c2\u6570\u66f4\u5c11\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u65f6\u95f4\u7ef4\u5ea6\u573a\u666f\u3002", "conclusion": "FLoST\u4e3a\u65f6\u7a7a\u6570\u636e\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.10922", "pdf": "https://arxiv.org/pdf/2505.10922", "abs": "https://arxiv.org/abs/2505.10922", "authors": ["Binwen Liu", "Jiexi Ge", "Jiamin Wang"], "title": "Vaiage: A Multi-Agent Solution to Personalized Travel Planning", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Planning trips is a cognitively intensive task involving conflicting user\npreferences, dynamic external information, and multi-step temporal-spatial\noptimization. Traditional platforms often fall short - they provide static\nresults, lack contextual adaptation, and fail to support real-time interaction\nor intent refinement.\n  Our approach, Vaiage, addresses these challenges through a graph-structured\nmulti-agent framework built around large language models (LLMs) that serve as\nboth goal-conditioned recommenders and sequential planners. LLMs infer user\nintent, suggest personalized destinations and activities, and synthesize\nitineraries that align with contextual constraints such as budget, timing,\ngroup size, and weather. Through natural language interaction, structured tool\nuse, and map-based feedback loops, Vaiage enables adaptive, explainable, and\nend-to-end travel planning grounded in both symbolic reasoning and\nconversational understanding.\n  To evaluate Vaiage, we conducted human-in-the-loop experiments using\nrubric-based GPT-4 assessments and qualitative feedback. The full system\nachieved an average score of 8.5 out of 10, outperforming the no-strategy (7.2)\nand no-external-API (6.8) variants, particularly in feasibility. Qualitative\nanalysis indicated that agent coordination - especially the Strategy and\nInformation Agents - significantly improved itinerary quality by optimizing\ntime use and integrating real-time context. These results demonstrate the\neffectiveness of combining LLM reasoning with symbolic agent coordination in\nopen-ended, real-world planning tasks.", "AI": {"tldr": "Vaiage\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u65c5\u884c\u89c4\u5212\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u7b26\u53f7\u63a8\u7406\u4f18\u5316\u884c\u7a0b\u3002", "motivation": "\u4f20\u7edf\u65c5\u884c\u89c4\u5212\u5e73\u53f0\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u4ea4\u4e92\u80fd\u529b\uff0cVaiage\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56fe\u7ed3\u6784\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0cLLM\u4f5c\u4e3a\u76ee\u6807\u9a71\u52a8\u7684\u63a8\u8350\u5668\u548c\u987a\u5e8f\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5de5\u5177\u4f7f\u7528\u3002", "result": "\u5728\u4eba\u7c7b\u5b9e\u9a8c\u4e2d\uff0cVaiage\u5e73\u5747\u5f97\u52068.5/10\uff0c\u4f18\u4e8e\u5176\u4ed6\u53d8\u4f53\uff0c\u667a\u80fd\u4f53\u534f\u8c03\u663e\u8457\u63d0\u5347\u884c\u7a0b\u8d28\u91cf\u3002", "conclusion": "LLM\u63a8\u7406\u4e0e\u7b26\u53f7\u667a\u80fd\u4f53\u534f\u8c03\u7ed3\u5408\u5728\u5f00\u653e\u89c4\u5212\u4efb\u52a1\u4e2d\u6709\u6548\u3002", "relevance": 70.0}}
{"id": "2505.11269", "pdf": "https://arxiv.org/pdf/2505.11269", "abs": "https://arxiv.org/abs/2505.11269", "authors": ["Shengjia Chang", "Xianshuo Yue"], "title": "Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach", "categories": ["cs.LG"], "comment": "10 pages, 6 figures, 7 tables", "summary": "This study proposes a dynamically weighted ARIMA-RF-HW hybrid model\nintegrating ARIMA for seasonality and trends, Random Forest for nonlinear\nfeatures, and Holt-Winters smoothing for seasonal adjustment to improve China's\npet population forecasting accuracy. Using 2005-2023 data with nine economic,\nsocial, and policy indicators (urban income, consumption, aging ratio, policy\nquantity, new veterinary drug approvals), data were preprocessed via Z-score\nnormalization and missing value imputation. The results show that key drivers\nof pet populations include urban income (19.48% for cats, 17.15% for dogs),\nconsumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for\ndogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization\namplifying the demand for pets. Forecasts show steady cat growth and\nfluctuating dog numbers, reflecting cats' adaptability to urban environments.\nThis research supports policymakers in optimizing pet health management and\nguides enterprises in developing differentiated services, advancing sustainable\nindustry growth.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u52a0\u6743\u7684ARIMA-RF-HW\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u4e2d\u56fd\u5ba0\u7269\u6570\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u7ed3\u5408\u4e86ARIMA\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u6790\u3001\u968f\u673a\u68ee\u6797\u7684\u975e\u7ebf\u6027\u7279\u5f81\u4ee5\u53caHolt-Winters\u7684\u5b63\u8282\u8c03\u6574\u3002", "motivation": "\u5ba0\u7269\u6570\u91cf\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u653f\u7b56\u5236\u5b9a\u548c\u4f01\u4e1a\u670d\u52a1\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u7ecf\u6d4e\u3001\u793e\u4f1a\u548c\u653f\u7b56\u56e0\u7d20\u3002", "method": "\u4f7f\u75282005-2023\u5e74\u7684\u6570\u636e\uff0c\u7ed3\u54089\u4e2a\u7ecf\u6d4e\u3001\u793e\u4f1a\u548c\u653f\u7b56\u6307\u6807\uff0c\u901a\u8fc7Z-score\u6807\u51c6\u5316\u548c\u7f3a\u5931\u503c\u586b\u8865\u9884\u5904\u7406\u6570\u636e\uff0c\u6784\u5efa\u52a8\u6001\u52a0\u6743\u7684ARIMA-RF-HW\u6df7\u5408\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57ce\u5e02\u6536\u5165\u3001\u6d88\u8d39\u548c\u653f\u7b56\u6570\u91cf\u662f\u5ba0\u7269\u6570\u91cf\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u732b\u7684\u6570\u91cf\u5448\u73b0\u7a33\u5b9a\u589e\u957f\uff0c\u800c\u72d7\u7684\u6570\u91cf\u6ce2\u52a8\u8f83\u5927\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u4f18\u5316\u5ba0\u7269\u5065\u5eb7\u7ba1\u7406\u548c\u4f01\u4e1a\u5f00\u53d1\u5dee\u5f02\u5316\u670d\u52a1\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u884c\u4e1a\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "relevance": 20.0}}
{"id": "2505.10696", "pdf": "https://arxiv.org/pdf/2505.10696", "abs": "https://arxiv.org/abs/2505.10696", "authors": ["Manthan Patel", "Fan Yang", "Yuheng Qiu", "Cesar Cadena", "Sebastian Scherer", "Marco Hutter", "Wenshan Wang"], "title": "TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Under review for IEEE conference", "summary": "We present TartanGround, a large-scale, multi-modal dataset to advance the\nperception and autonomy of ground robots operating in diverse environments.\nThis dataset, collected in various photorealistic simulation environments\nincludes multiple RGB stereo cameras for 360-degree coverage, along with depth,\noptical flow, stereo disparity, LiDAR point clouds, ground truth poses,\nsemantic segmented images, and occupancy maps with semantic labels. Data is\ncollected using an integrated automatic pipeline, which generates trajectories\nmimicking the motion patterns of various ground robot platforms, including\nwheeled and legged robots. We collect 910 trajectories across 70 environments,\nresulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM\ntasks reveal that state-of-the-art methods trained on existing datasets\nstruggle to generalize across diverse scenes. TartanGround can serve as a\ntestbed for training and evaluation of a broad range of learning-based tasks,\nincluding occupancy prediction, SLAM, neural scene representation,\nperception-based navigation, and more, enabling advancements in robotic\nperception and autonomy towards achieving robust models generalizable to more\ndiverse scenarios. The dataset and codebase for data collection will be made\npublicly available upon acceptance. Webpage: https://tartanair.org/tartanground", "AI": {"tldr": "TartanGround\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u5730\u9762\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u611f\u77e5\u548c\u81ea\u4e3b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0cTartanGround\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u611f\u77e5\u548c\u81ea\u4e3b\u6027\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u81ea\u52a8\u7ba1\u9053\u5728\u591a\u79cd\u4eff\u771f\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u5305\u62ecRGB\u7acb\u4f53\u76f8\u673a\u3001\u6df1\u5ea6\u3001\u5149\u6d41\u3001LiDAR\u70b9\u4e91\u7b49\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u6536\u96c6\u4e86910\u6761\u8f68\u8ff9\u548c1.5\u767e\u4e07\u6837\u672c\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "TartanGround\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u5b66\u4e60\u4efb\u52a1\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u611f\u77e5\u548c\u81ea\u4e3b\u6027\u7684\u53d1\u5c55\u3002", "relevance": 20.0}}
{"id": "2505.11276", "pdf": "https://arxiv.org/pdf/2505.11276", "abs": "https://arxiv.org/abs/2505.11276", "authors": ["Francesco Marchetti", "Edoardo Legnaro", "Sabrina Guastavino"], "title": "Multiclass threshold-based classification", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we introduce a threshold-based framework for multiclass\nclassification that generalizes the standard argmax rule. This is done by\nreplacing the probabilistic interpretation of softmax outputs with a geometric\none on the multidimensional simplex, where the classification depends on a\nmultidimensional threshold. This change of perspective enables for any trained\nclassification network an a posteriori optimization of the classification score\nby means of threshold tuning, as usually carried out in the binary setting.\nThis allows a further refinement of the prediction capability of any network.\nMoreover, this multidimensional threshold-based setting makes it possible to\ndefine score-oriented losses, which are based on the interpretation of the\nthreshold as a random variable. Our experiments show that the multidimensional\nthreshold tuning yields consistent performance improvements across various\nnetworks and datasets, and that the proposed multiclass score-oriented losses\nare competitive with standard loss functions, resembling the advantages\nobserved in the binary case.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9608\u503c\u7684\u591a\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u89c6\u89d2\u66ff\u4ee3\u4f20\u7edf\u7684softmax\u6982\u7387\u89e3\u91ca\uff0c\u652f\u6301\u540e\u9a8c\u9608\u503c\u4f18\u5316\u548c\u9762\u5411\u5206\u6570\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u591a\u7c7b\u5206\u7c7b\u4f9d\u8d56argmax\u89c4\u5219\uff0c\u7f3a\u4e4f\u7c7b\u4f3c\u4e8c\u5206\u7c7b\u4e2d\u7684\u9608\u503c\u4f18\u5316\u80fd\u529b\uff0c\u9650\u5236\u4e86\u7f51\u7edc\u9884\u6d4b\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u5c06softmax\u8f93\u51fa\u7684\u6982\u7387\u89e3\u91ca\u8f6c\u6362\u4e3a\u591a\u7ef4\u5355\u7eaf\u5f62\u4e0a\u7684\u51e0\u4f55\u89e3\u91ca\uff0c\u5f15\u5165\u591a\u7ef4\u9608\u503c\uff0c\u652f\u6301\u540e\u9a8c\u4f18\u5316\u548c\u9762\u5411\u5206\u6570\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u7ef4\u9608\u503c\u8c03\u4f18\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u63d0\u51fa\u7684\u9762\u5411\u5206\u6570\u635f\u5931\u51fd\u6570\u4e0e\u6807\u51c6\u635f\u5931\u51fd\u6570\u7ade\u4e89\u529b\u76f8\u5f53\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u7c7b\u5206\u7c7b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u4f18\u5316\u624b\u6bb5\uff0c\u6269\u5c55\u4e86\u4e8c\u5206\u7c7b\u4e2d\u7684\u9608\u503c\u4f18\u5316\u4f18\u52bf\u3002", "relevance": 40.0}}
{"id": "2505.10729", "pdf": "https://arxiv.org/pdf/2505.10729", "abs": "https://arxiv.org/abs/2505.10729", "authors": ["NingFeng Que", "Xiaofei Wang", "Jingjing Chen", "Yixuan Jiang", "Chao Li"], "title": "Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "Early accepted by MICCAI 2025", "summary": "Spatial transcriptomics (ST) is a promising technique that characterizes the\nspatial gene profiling patterns within the tissue context. Comprehensive ST\nanalysis depends on consecutive slices for 3D spatial insights, whereas the\nmissing intermediate tissue sections and high costs limit the practical\nfeasibility of generating multi-slice ST. In this paper, we propose C2-STi, the\nfirst attempt for interpolating missing ST slices at arbitrary intermediate\npositions between adjacent ST slices. Despite intuitive, effective ST\ninterpolation presents significant challenges, including 1) limited continuity\nacross heterogeneous tissue sections, 2) complex intrinsic correlation across\ngenes, and 3) intricate cellular structures and biological semantics within\neach tissue section. To mitigate these challenges, in C2-STi, we design 1) a\ndistance-aware local structural modulation module to adaptively capture\ncross-slice deformations and enhance positional correlations between ST slices,\n2) a pyramid gene co-expression correlation module to capture multi-scale\nbiological associations among genes, and 3) a cross-modal alignment module that\nintegrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter\nand align the essential cellular features across ST and H\\&E images. Extensive\nexperiments on the public dataset demonstrate our superiority over\nstate-of-the-art approaches on both single-slice and multi-slice ST\ninterpolation. Codes are available at\nhttps://github.com/XiaofeiWang2018/C2-STi.", "AI": {"tldr": "C2-STi\u662f\u4e00\u79cd\u7528\u4e8e\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u4e2d\u7f3a\u5931\u5207\u7247\u63d2\u503c\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddd\u79bb\u611f\u77e5\u6a21\u5757\u3001\u57fa\u56e0\u5171\u8868\u8fbe\u6a21\u5757\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u89e3\u51b3ST\u63d2\u503c\u7684\u6311\u6218\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u7684\u591a\u5207\u7247\u5206\u6790\u53d7\u9650\u4e8e\u7f3a\u5931\u5207\u7247\u548c\u9ad8\u6210\u672c\uff0cC2-STi\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u6280\u672f\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u8ddd\u79bb\u611f\u77e5\u5c40\u90e8\u7ed3\u6784\u8c03\u5236\u6a21\u5757\u3001\u91d1\u5b57\u5854\u57fa\u56e0\u5171\u8868\u8fbe\u76f8\u5173\u6a21\u5757\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "C2-STi\u4e3aST\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63d2\u503c\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.11283", "pdf": "https://arxiv.org/pdf/2505.11283", "abs": "https://arxiv.org/abs/2505.11283", "authors": ["Tom Siegl", "Kutalm\u0131\u015f Co\u015fkun", "Bjarne Hiller", "Amin Mirzaei", "Florian Lemmerich", "Martin Becker"], "title": "SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers", "categories": ["cs.LG"], "comment": "49 pages, 8 figures", "summary": "Machine learning (ML) is increasingly employed in real-world applications\nlike medicine or economics, thus, potentially affecting large populations.\nHowever, ML models often do not perform homogeneously across such populations\nresulting in subgroups of the population (e.g., sex=female AND\nmarital_status=married) where the model underperforms or, conversely, is\nparticularly accurate. Identifying and describing such subgroups can support\npractical decisions on which subpopulation a model is safe to deploy or where\nmore training data is required. The potential of identifying and analyzing such\nsubgroups has been recognized, however, an efficient and coherent framework for\neffective search is missing. Consequently, we introduce SubROC, an open-source,\neasy-to-use framework based on Exceptional Model Mining for reliably and\nefficiently finding strengths and weaknesses of classification models in the\nform of interpretable population subgroups. SubROC incorporates common\nevaluation measures (ROC and PR AUC), efficient search space pruning for fast\nexhaustive subgroup search, control for class imbalance, adjustment for\nredundant patterns, and significance testing. We illustrate the practical\nbenefits of SubROC in case studies as well as in comparative analyses across\nmultiple datasets.", "AI": {"tldr": "SubROC\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u5206\u7c7b\u6a21\u578b\u5728\u4e0d\u540c\u5b50\u7fa4\u4f53\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u652f\u6301ROC\u548cPR AUC\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u663e\u8457\u6027\u6d4b\u8bd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u5b50\u7fa4\u4f53\u4e2d\u6027\u80fd\u4e0d\u5747\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u6846\u67b6\u6765\u8bc6\u522b\u548c\u5206\u6790\u8fd9\u4e9b\u5dee\u5f02\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u90e8\u7f72\u548c\u6570\u636e\u6536\u96c6\u3002", "method": "\u57fa\u4e8eExceptional Model Mining\uff0cSubROC\u901a\u8fc7\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u63a7\u5236\u548c\u5197\u4f59\u6a21\u5f0f\u8c03\u6574\uff0c\u9ad8\u6548\u641c\u7d22\u53ef\u89e3\u91ca\u7684\u5b50\u7fa4\u4f53\u3002", "result": "SubROC\u5728\u6848\u4f8b\u7814\u7a76\u548c\u591a\u6570\u636e\u96c6\u6bd4\u8f83\u4e2d\u5c55\u793a\u4e86\u5176\u8bc6\u522b\u6a21\u578b\u5f3a\u5f31\u70b9\u7684\u80fd\u529b\u3002", "conclusion": "SubROC\u4e3a\u6a21\u578b\u5b50\u7fa4\u4f53\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.10824", "pdf": "https://arxiv.org/pdf/2505.10824", "abs": "https://arxiv.org/abs/2505.10824", "authors": ["Kaifa Yang", "Qi Yang", "Zhu Li", "Yiling Xu"], "title": "Textured mesh Quality Assessment using Geometry and Color Field Similarity", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "15 pages main content, 4 pages supplementary material. Submitted to\n  IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) for\n  review", "summary": "Textured mesh quality assessment (TMQA) is critical for various 3D mesh\napplications. However, existing TMQA methods often struggle to provide accurate\nand robust evaluations. Motivated by the effectiveness of fields in\nrepresenting both 3D geometry and color information, we propose a novel\npoint-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes\nsigned distance fields and a newly proposed color field named nearest surface\npoint color field to realize effective mesh feature description. Four features\nrelated to visual perception are extracted from the geometry and color fields:\ngeometry similarity, geometry gradient similarity, space color distribution\nsimilarity, and space color gradient similarity. Experimental results on three\nbenchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA)\nTMQA metrics. Furthermore, FMQM exhibits low computational complexity, making\nit a practical and efficient solution for real-world applications in 3D\ngraphics and visualization. Our code is publicly available at:\nhttps://github.com/yyyykf/FMQM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u7684\u7eb9\u7406\u7f51\u683c\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5FMQM\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u989c\u8272\u573a\u63d0\u53d6\u7279\u5f81\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7eb9\u7406\u7f51\u683c\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u53d7\u573a\u8868\u793a3D\u51e0\u4f55\u548c\u989c\u8272\u4fe1\u606f\u7684\u542f\u53d1\uff0c\u63d0\u51faFMQM\u3002", "method": "\u5229\u7528\u6709\u7b26\u53f7\u8ddd\u79bb\u573a\u548c\u65b0\u7684\u989c\u8272\u573a\u63d0\u53d6\u51e0\u4f55\u76f8\u4f3c\u6027\u3001\u51e0\u4f55\u68af\u5ea6\u76f8\u4f3c\u6027\u3001\u7a7a\u95f4\u989c\u8272\u5206\u5e03\u76f8\u4f3c\u6027\u548c\u7a7a\u95f4\u989c\u8272\u68af\u5ea6\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "conclusion": "FMQM\u662f\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u7eb9\u7406\u7f51\u683c\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e3D\u56fe\u5f62\u548c\u53ef\u89c6\u5316\u5e94\u7528\u3002", "relevance": 20.0}}
{"id": "2505.11294", "pdf": "https://arxiv.org/pdf/2505.11294", "abs": "https://arxiv.org/abs/2505.11294", "authors": ["Juan D. Guerra", "Thomas Garbay", "Guillaume Lajoie", "Marco Bonizzato"], "title": "Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Hierarchical Gaussian Process (H-GP) models divide problems into different\nsubtasks, allowing for different models to address each part, making them\nwell-suited for problems with inherent hierarchical structure. However, typical\nH-GP models do not fully take advantage of this structure, only sending\ninformation up or down the hierarchy. This one-way coupling limits sample\nefficiency and slows convergence. We propose Bidirectional Information Flow\n(BIF), an efficient H-GP framework that establishes bidirectional information\nexchange between parent and child models in H-GPs for online training. BIF\nretains the modular structure of hierarchical models - the parent combines\nsubtask knowledge from children GPs - while introducing top-down feedback to\ncontinually refine children models during online learning. This mutual exchange\nimproves sample efficiency, enables robust training, and allows modular reuse\nof learned subtask models. BIF outperforms conventional H-GP Bayesian\nOptimization methods, achieving up to 85% and 5x higher $R^2$ scores for the\nparent and children respectively, on synthetic and real-world neurostimulation\noptimization tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u4fe1\u606f\u6d41\uff08BIF\uff09\u7684\u5c42\u6b21\u9ad8\u65af\u8fc7\u7a0b\uff08H-GP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfH-GP\u6a21\u578b\u4ec5\u5355\u5411\u4f20\u9012\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6837\u672c\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\uff0cBIF\u901a\u8fc7\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BIF\u6846\u67b6\u5728H-GP\u4e2d\u5efa\u7acb\u7236\u6a21\u578b\u4e0e\u5b50\u6a21\u578b\u7684\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\uff0c\u652f\u6301\u5728\u7ebf\u8bad\u7ec3\u548c\u6a21\u5757\u5316\u590d\u7528\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u795e\u7ecf\u523a\u6fc0\u4f18\u5316\u4efb\u52a1\u4e2d\uff0cBIF\u6bd4\u4f20\u7edfH-GP\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u7236\u6a21\u578b\u548c\u5b50\u6a21\u578b\u7684R\u00b2\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e8685%\u548c5\u500d\u3002", "conclusion": "BIF\u663e\u8457\u63d0\u5347\u4e86H-GP\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.10940", "pdf": "https://arxiv.org/pdf/2505.10940", "abs": "https://arxiv.org/abs/2505.10940", "authors": ["Qing Yu", "Xiaobei Wang", "Shuchang Liu", "Yandong Bai", "Xiaoyu Yang", "Xueliang Wang", "Chang Meng", "Shanshan Wu", "Hailan Yang", "Huihui Xiao", "Xiang Li", "Fan Yang", "Xiaoqiang Feng", "Lantao Hu", "Han Li", "Kun Gai", "Lixin Zou"], "title": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, the exploitation of the LLM's\nworld knowledge and logic inference ability produces a virtual logic graph that\nreveals dynamic and expressive knowledge of users, augmenting the\nrecommendation performance. On the other hand, the user role aligns the user\nbehavioral logic with the observed user feedback, refining our understanding of\nuser behaviors. Additionally, we also show that the extracted user-item logic\ngraph is empirically a general knowledge that can benefit a wide range of\nrecommendation tasks, and conduct experiments on industrial and several public\ndatasets as verification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTagCF\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u63a8\u8350\u7cfb\u7edf\uff0c\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u89d2\u8272\u548c\u884c\u4e3a\u903b\u8f91\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u9879\u76ee\u4e3b\u9898\u548c\u7528\u6237\u504f\u597d\uff0c\u4f46\u5ffd\u7565\u4e86\u7528\u6237\u7279\u5f81\u548c\u793e\u4f1a\u89d2\u8272\u7684\u5efa\u6a21\uff0c\u8fd9\u4e9b\u662f\u5f71\u54cd\u7528\u6237\u504f\u597d\u7684\u6f5c\u5728\u56e0\u7d20\u3002", "method": "\u5f15\u5165\u7528\u6237\u89d2\u8272\u8bc6\u522b\u548c\u884c\u4e3a\u903b\u8f91\u5efa\u6a21\u4efb\u52a1\uff0c\u5229\u7528LLM\u7684\u4e16\u754c\u77e5\u8bc6\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u751f\u6210\u865a\u62df\u903b\u8f91\u56fe\uff0c\u589e\u5f3a\u63a8\u8350\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTagCF\u5728\u5de5\u4e1a\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u53d6\u7684\u7528\u6237-\u9879\u76ee\u903b\u8f91\u56fe\u5177\u6709\u666e\u9002\u6027\u3002", "conclusion": "\u7ed3\u5408LLM\u548c\u63a8\u8350\u7cfb\u7edf\u80fd\u6709\u6548\u5efa\u6a21\u7528\u6237\u89d2\u8272\u548c\u884c\u4e3a\u903b\u8f91\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 50.0}}
{"id": "2505.11298", "pdf": "https://arxiv.org/pdf/2505.11298", "abs": "https://arxiv.org/abs/2505.11298", "authors": ["Sohir Maskey", "Raffaele Paolino", "Fabian Jogl", "Gitta Kutyniok", "Johannes F. Lutzeyer"], "title": "Graph Representational Learning: When Does More Expressivity Hurt Generalization?", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) are powerful tools for learning on structured\ndata, yet the relationship between their expressivity and predictive\nperformance remains unclear. We introduce a family of premetrics that capture\ndifferent degrees of structural similarity between graphs and relate these\nsimilarities to generalization, and consequently, the performance of expressive\nGNNs. By considering a setting where graph labels are correlated with\nstructural features, we derive generalization bounds that depend on the\ndistance between training and test graphs, model complexity, and training set\nsize. These bounds reveal that more expressive GNNs may generalize worse unless\ntheir increased complexity is balanced by a sufficiently large training set or\nreduced distance between training and test graphs. Our findings relate\nexpressivity and generalization, offering theoretical insights supported by\nempirical results.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u6cdb\u5316\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5ea6\u91cf\u7ed3\u6784\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u8868\u8fbe\u80fd\u529b\u5f3a\u7684GNNs\u5728\u8bad\u7ec3\u96c6\u4e0d\u8db3\u6216\u6d4b\u8bd5\u56fe\u4e0e\u8bad\u7ec3\u56fe\u5dee\u5f02\u5927\u65f6\u53ef\u80fd\u6cdb\u5316\u8f83\u5dee\u3002", "motivation": "\u63a2\u7d22GNNs\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u5176\u5b9e\u9645\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684GNNs\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u65cf\u5ea6\u91cf\u7ed3\u6784\u76f8\u4f3c\u6027\u7684\u9884\u5ea6\u91cf\uff0c\u7ed3\u5408\u56fe\u6807\u7b7e\u4e0e\u7ed3\u6784\u7279\u5f81\u7684\u76f8\u5173\u6027\uff0c\u63a8\u5bfc\u4e86\u6cdb\u5316\u8fb9\u754c\u3002", "result": "\u53d1\u73b0\u8868\u8fbe\u80fd\u529b\u5f3a\u7684GNNs\u5728\u8bad\u7ec3\u96c6\u4e0d\u8db3\u6216\u6d4b\u8bd5\u56fe\u4e0e\u8bad\u7ec3\u56fe\u5dee\u5f02\u5927\u65f6\u6cdb\u5316\u6027\u80fd\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e3aGNNs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u4e0e\u6cdb\u5316\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "relevance": 40.0}}
{"id": "2505.10855", "pdf": "https://arxiv.org/pdf/2505.10855", "abs": "https://arxiv.org/abs/2505.10855", "authors": ["Aneesh Rangnekar", "Nikhil Mankuzhy", "Jonas Willmann", "Chloe Choi", "Abraham Wu", "Maria Thor", "Andreas Rimner", "Harini Veeraraghavan"], "title": "Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "AI automated segmentations for radiation treatment planning (RTP) can\ndeteriorate when applied in clinical cases with different characteristics than\ntraining dataset. Hence, we refined a pretrained transformer into a hybrid\ntransformer convolutional network (HTN) to segment cardiac substructures lung\nand breast cancer patients acquired with varying imaging contrasts and patient\nscan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124\nnon-contrast CT (NCCT) scans from patients with non-small cell lung cancers\nacquired in supine position, was used to create oracle with all 180 training\ncases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models were\nevaluated on a held-out validation set of 60 cohort I patients and 66 patients\nwith breast cancer from cohort II acquired in supine (n=45) and prone (n=21)\npositions. Accuracy was measured using DSC, HD95, and dose metrics. Publicly\navailable TotalSegmentator served as the benchmark. The oracle and balanced\nmodels were similarly accurate (DSC Cohort I: 0.80 \\pm 0.10 versus 0.81 \\pm\n0.10; Cohort II: 0.77 \\pm 0.13 versus 0.80 \\pm 0.12), outperforming\nTotalSegmentator. The balanced model, using half the training cases as oracle,\nproduced similar dose metrics as manual delineations for all cardiac\nsubstructures. This model was robust to CT contrast in 6 out of 8 substructures\nand patient scan position variations in 5 out of 8 substructures and showed low\ncorrelations of accuracy to patient size and age. A HTN demonstrated robustly\naccurate (geometric and dose metrics) cardiac substructures segmentation from\nCTs with varying imaging and patient characteristics, one key requirement for\nclinical use. Moreover, the model combining pretraining with balanced\ndistribution of NCCT and CECT scans was able to provide reliably accurate\nsegmentations under varied conditions with far fewer labeled datasets compared\nto an oracle model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408Transformer\u5377\u79ef\u7f51\u7edc\uff08HTN\uff09\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u6210\u50cf\u5bf9\u6bd4\u548c\u60a3\u8005\u626b\u63cf\u4f4d\u7f6e\u4e0b\u5206\u5272\u5fc3\u810f\u4e9a\u7ed3\u6784\uff0c\u8868\u73b0\u4f18\u4e8e\u516c\u5f00\u57fa\u51c6TotalSegmentator\u3002", "motivation": "\u89e3\u51b3AI\u81ea\u52a8\u5206\u5272\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u6570\u636e\u7279\u5f81\u4e0d\u540c\u800c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684Transformer\u6539\u8fdb\u4e3aHTN\uff0c\u5e76\u5728\u5e73\u8861\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002", "result": "HTN\u5728\u51e0\u4f55\u548c\u5242\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u4e14\u5bf9\u60a3\u8005\u626b\u63cf\u4f4d\u7f6e\u548c\u6210\u50cf\u5bf9\u6bd4\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "HTN\u7ed3\u5408\u9884\u8bad\u7ec3\u548c\u5e73\u8861\u6570\u636e\u5206\u5e03\uff0c\u80fd\u5728\u8f83\u5c11\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u3002", "relevance": 60.0}}
{"id": "2505.11304", "pdf": "https://arxiv.org/pdf/2505.11304", "abs": "https://arxiv.org/abs/2505.11304", "authors": ["Shudi Weng", "Chao Ren", "Ming Xiao", "Mikael Skoglund"], "title": "Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) commonly involves clients with diverse communication\nand computational capabilities. Such heterogeneity can significantly distort\nthe optimization dynamics and lead to objective inconsistency, where the global\nmodel converges to an incorrect stationary point potentially far from the\npursued optimum. Despite its critical impact, the joint effect of communication\nand computation heterogeneity has remained largely unexplored, due to the\nintrinsic complexity of their interaction. In this paper, we reveal the\nfundamentally distinct mechanisms through which heterogeneous communication and\ncomputation drive inconsistency in FL. To the best of our knowledge, this is\nthe first unified theoretical analysis of general heterogeneous FL, offering a\nprincipled understanding of how these two forms of heterogeneity jointly\ndistort the optimization trajectory under arbitrary choices of local solvers.\nMotivated by these insights, we propose Federated Heterogeneity-Aware Client\nSampling, FedACS, a universal method to eliminate all types of objective\ninconsistency. We theoretically prove that FedACS converges to the correct\noptimum at a rate of $O(1/\\sqrt{R})$, even in dynamic heterogeneous\nenvironments. Extensive experiments across multiple datasets show that FedACS\noutperforms state-of-the-art and category-specific baselines by 4.3%-36%, while\nreducing communication costs by 22%-89% and computation loads by 14%-105%,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u901a\u4fe1\u548c\u8ba1\u7b97\u5f02\u8d28\u6027\u5bf9\u4f18\u5316\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86FedACS\u65b9\u6cd5\u4ee5\u6d88\u9664\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u5f02\u8d28\u6027\u4f1a\u663e\u8457\u626d\u66f2\u4f18\u5316\u52a8\u6001\uff0c\u5bfc\u81f4\u5168\u5c40\u6a21\u578b\u6536\u655b\u5230\u9519\u8bef\u7684\u5e73\u7a33\u70b9\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u5176\u673a\u5236\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Federated Heterogeneity-Aware Client Sampling (FedACS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "FedACS\u5728\u52a8\u6001\u5f02\u8d28\u73af\u5883\u4e2d\u4ee5$O(1/\\sqrt{R})$\u7684\u901f\u7387\u6536\u655b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedACS\u662f\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.11306", "pdf": "https://arxiv.org/pdf/2505.11306", "abs": "https://arxiv.org/abs/2505.11306", "authors": ["Xinyan Wang", "Rui Dai", "Kaikui Liu", "Xiangxiang Chu"], "title": "Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion", "categories": ["cs.LG"], "comment": null, "summary": "We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel\nprobabilistic framework for time series forecasting. First, we introduce the\nDiffusion Model for Residual Regression (DMRR) framework, which unifies\ndiffusion-based probabilistic regression methods. Within this framework, FALDA\nleverages Fourier-based decomposition to incorporate a component-specific\narchitecture, enabling tailored modeling of individual temporal components. A\nconditional diffusion model is utilized to estimate the future noise term,\nwhile our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN),\nconditions on the historical noise term to enhance denoising performance.\nThrough mathematical analysis and empirical validation, we demonstrate that\nFALDA effectively reduces epistemic uncertainty, allowing probabilistic\nlearning to primarily focus on aleatoric uncertainty. Experiments on six\nreal-world benchmarks demonstrate that FALDA consistently outperforms existing\nprobabilistic forecasting approaches across most datasets for long-term time\nseries forecasting while achieving enhanced computational efficiency without\ncompromising accuracy. Notably, FALDA also achieves superior overall\nperformance compared to state-of-the-art (SOTA) point forecasting approaches,\nwith improvements of up to 9%.", "AI": {"tldr": "FALDA\u662f\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b0\u578b\u6982\u7387\u6846\u67b6\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u5206\u89e3\u548c\u8f7b\u91cf\u7ea7\u53bb\u566a\u5668\uff0c\u663e\u8457\u964d\u4f4e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002FALDA\u65e8\u5728\u901a\u8fc7\u6982\u7387\u6846\u67b6\u548c\u5085\u91cc\u53f6\u5206\u89e3\u6539\u8fdb\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faDMRR\u6846\u67b6\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u5206\u89e3\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53bb\u566a\u5668DEMA\u4f18\u5316\u53bb\u566a\u6027\u80fd\u3002", "result": "FALDA\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6982\u7387\u548c\u70b9\u9884\u6d4b\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u7cbe\u5ea6\u4e0d\u964d\u3002", "conclusion": "FALDA\u901a\u8fc7\u6982\u7387\u5b66\u4e60\u548c\u5085\u91cc\u53f6\u5206\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.11307", "pdf": "https://arxiv.org/pdf/2505.11307", "abs": "https://arxiv.org/abs/2505.11307", "authors": ["Elsa Rizk", "Kun Yuan", "Ali H. Sayed"], "title": "Diffusion Learning with Partial Agent Participation and Local Updates", "categories": ["cs.LG"], "comment": "17 pages", "summary": "Diffusion learning is a framework that endows edge devices with advanced\nintelligence. By processing and analyzing data locally and allowing each agent\nto communicate with its immediate neighbors, diffusion effectively protects the\nprivacy of edge devices, enables real-time response, and reduces reliance on\ncentral servers. However, traditional diffusion learning relies on\ncommunication at every iteration, leading to communication overhead, especially\nwith large learning models. Furthermore, the inherent volatility of edge\ndevices, stemming from power outages or signal loss, poses challenges to\nreliable communication between neighboring agents. To mitigate these issues,\nthis paper investigates an enhanced diffusion learning approach incorporating\nlocal updates and partial agent participation. Local updates will curtail\ncommunication frequency, while partial agent participation will allow for the\ninclusion of agents based on their availability. We prove that the resulting\nalgorithm is stable in the mean-square error sense and provide a tight analysis\nof its Mean-Square-Deviation (MSD) performance. Various numerical experiments\nare conducted to illustrate our theoretical findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6269\u6563\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u66f4\u65b0\u548c\u90e8\u5206\u4ee3\u7406\u53c2\u4e0e\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u9ad8\u8fb9\u7f18\u8bbe\u5907\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u5b66\u4e60\u65b9\u6cd5\u56e0\u6bcf\u6b21\u8fed\u4ee3\u90fd\u9700\u8981\u901a\u4fe1\u800c\u4ea7\u751f\u9ad8\u5f00\u9500\uff0c\u4e14\u8fb9\u7f18\u8bbe\u5907\u7684\u6ce2\u52a8\u6027\u5bfc\u81f4\u901a\u4fe1\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u66f4\u65b0\u51cf\u5c11\u901a\u4fe1\u9891\u7387\uff0c\u5e76\u901a\u8fc7\u90e8\u5206\u4ee3\u7406\u53c2\u4e0e\u673a\u5236\u9009\u62e9\u53ef\u7528\u4ee3\u7406\u3002", "result": "\u7b97\u6cd5\u5728\u5747\u65b9\u8bef\u5dee\u610f\u4e49\u4e0b\u7a33\u5b9a\uff0c\u5e76\u63d0\u4f9b\u4e86\u7d27\u5bc6\u7684\u5747\u65b9\u504f\u5dee\uff08MSD\uff09\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u6539\u8fdb\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u9ad8\u4e86\u8fb9\u7f18\u8bbe\u5907\u5b66\u4e60\u7684\u53ef\u9760\u6027\u3002", "relevance": 40.0}}
{"id": "2505.10961", "pdf": "https://arxiv.org/pdf/2505.10961", "abs": "https://arxiv.org/abs/2505.10961", "authors": ["Ratnadira Widyasari", "Martin Weyssow", "Ivana Clairine Irsan", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "Hong Jin Kang", "David Lo"], "title": "Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection using LLM-Based Agents", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Detecting vulnerabilities in source code remains a critical yet challenging\ntask, especially when benign and vulnerable functions share significant\nsimilarities. In this work, we introduce VulTrial, a courtroom-inspired\nmulti-agent framework designed to enhance automated vulnerability detection. It\nemploys four role-specific agents, which are security researcher, code author,\nmoderator, and review board. Through extensive experiments using GPT-3.5 and\nGPT-4o we demonstrate that Vultrial outperforms single-agent and multi-agent\nbaselines. Using GPT-4o, VulTrial improves the performance by 102.39% and\n84.17% over its respective baseline. Additionally, we show that role-specific\ninstruction tuning in multi-agent with small data (50 pair samples) improves\nthe performance of VulTrial further by 139.89% and 118.30%. Furthermore, we\nanalyze the impact of increasing the number of agent interactions on VulTrial's\noverall performance. While multi-agent setups inherently incur higher costs due\nto increased token usage, our findings reveal that applying VulTrial to a\ncost-effective model like GPT-3.5 can improve its performance by 69.89%\ncompared to GPT-4o in a single-agent setting, at a lower overall cost.", "AI": {"tldr": "VulTrial\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u4ee3\u7406\u6846\u67b6\u7684\u81ea\u52a8\u5316\u6f0f\u6d1e\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u89d2\u8272\u7279\u5b9a\u7684\u4ee3\u7406\uff08\u5982\u5b89\u5168\u7814\u7a76\u5458\u3001\u4ee3\u7801\u4f5c\u8005\u7b49\uff09\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528GPT-4o\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5f53\u826f\u6027\u4ee3\u7801\u4e0e\u6f0f\u6d1e\u4ee3\u7801\u76f8\u4f3c\u65f6\u3002", "method": "\u63d0\u51faVulTrial\u6846\u67b6\uff0c\u4f7f\u7528\u56db\u4e2a\u89d2\u8272\u7279\u5b9a\u7684\u4ee3\u7406\uff08\u5b89\u5168\u7814\u7a76\u5458\u3001\u4ee3\u7801\u4f5c\u8005\u3001\u4e3b\u6301\u4eba\u548c\u8bc4\u5ba1\u59d4\u5458\u4f1a\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728GPT-3.5\u548cGPT-4o\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4f7f\u7528GPT-4o\u65f6\uff0cVulTrial\u6027\u80fd\u63d0\u5347102.39%\u548c84.17%\uff1b\u89d2\u8272\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86139.89%\u548c118.30%\u3002", "conclusion": "VulTrial\u5728\u591a\u4ee3\u7406\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u6027\u80fd\uff0c\u4e14\u901a\u8fc7\u6210\u672c\u4f18\u5316\u6a21\u578b\uff08\u5982GPT-3.5\uff09\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6548\u8868\u73b0\u3002", "relevance": 40.0}}
{"id": "2505.11308", "pdf": "https://arxiv.org/pdf/2505.11308", "abs": "https://arxiv.org/abs/2505.11308", "authors": ["Lothar Heimbach", "Sebastian Kaltenbach", "Petr Karnakov", "Francis J. Alexander", "Petros Koumoutsakos"], "title": "Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Partial Differential Equations (PDEs) describe phenomena ranging from\nturbulence and epidemics to quantum mechanics and financial markets. Despite\nrecent advances in computational science, solving such PDEs for real-world\napplications remains prohibitively expensive because of the necessity of\nresolving a broad range of spatiotemporal scales. In turn, practitioners often\nrely on coarse-grained approximations of the original PDEs, trading off\naccuracy for reduced computational resources. To mitigate the loss of detail\ninherent in such approximations, closure models are employed to represent\nunresolved spatiotemporal interactions. We present a framework for developing\nclosure models for PDEs using synthetic data acquired through the method of\nmanufactured solutions. These data are used in conjunction with reinforcement\nlearning to provide closures for coarse-grained PDEs. We illustrate the\nefficacy of our method using the one-dimensional and two-dimensional Burgers'\nequations and the two-dimensional advection equation. Moreover, we demonstrate\nthat closure models trained for inhomogeneous PDEs can be effectively\ngeneralized to homogeneous PDEs. The results demonstrate the potential for\ndeveloping accurate and computationally efficient closure models for systems\nwith scarce data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u4e3a\u7c97\u7c92\u5ea6\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u5f00\u53d1\u95ed\u5408\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2aPDE\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u5e94\u7528\u4e2dPDE\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u8fd1\u4f3c\u548c\u95ed\u5408\u6a21\u578b\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u95ed\u5408\u6a21\u578b\uff0c\u5e94\u7528\u4e8e\u4e00\u7ef4\u548c\u4e8c\u7ef4Burgers\u65b9\u7a0b\u53ca\u4e8c\u7ef4\u5e73\u6d41\u65b9\u7a0b\u3002", "result": "\u9a8c\u8bc1\u4e86\u95ed\u5408\u6a21\u578b\u5728\u7a00\u7f3a\u6570\u636e\u7cfb\u7edf\u4e2d\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u540c\u7c7bPDE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u9ad8\u6548\u95ed\u5408\u6a21\u578b\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u7cfb\u7edf\u3002", "relevance": 40.0}}
{"id": "2505.10973", "pdf": "https://arxiv.org/pdf/2505.10973", "abs": "https://arxiv.org/abs/2505.10973", "authors": ["Narayanan PP", "Sarvesh Prasanth Venkatesan", "Srinivas Kantha Reddy", "Shishir Kolathaya"], "title": "GROQLoco: Generalist and RObot-agnostic Quadruped Locomotion Control using Offline Datasets", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.9"], "comment": "18pages, 16figures, 6tables", "summary": "Recent advancements in large-scale offline training have demonstrated the\npotential of generalist policy learning for complex robotic tasks. However,\napplying these principles to legged locomotion remains a challenge due to\ncontinuous dynamics and the need for real-time adaptation across diverse\nterrains and robot morphologies. In this work, we propose GROQLoco, a scalable,\nattention-based framework that learns a single generalist locomotion policy\nacross multiple quadruped robots and terrains, relying solely on offline\ndatasets. Our approach leverages expert demonstrations from two distinct\nlocomotion behaviors - stair traversal (non-periodic gaits) and flat terrain\ntraversal (periodic gaits) - collected across multiple quadruped robots, to\ntrain a generalist model that enables behavior fusion for both behaviors.\nCrucially, our framework operates directly on proprioceptive data from all\nrobots without incorporating any robot-specific encodings. The policy is\ndirectly deployable on an Intel i7 nuc, producing low-latency control outputs\nwithout any test-time optimization. Our extensive experiments demonstrate\nstrong zero-shot transfer across highly diverse quadruped robots and terrains,\nincluding hardware deployment on the Unitree Go1, a commercially available 12kg\nrobot. Notably, we evaluate challenging cross-robot training setups where\ndifferent locomotion skills are unevenly distributed across robots, yet observe\nsuccessful transfer of both flat walking and stair traversal behaviors to all\nrobots at test time. We also show preliminary walking on Stoch 5, a 70kg\nquadruped, on flat and outdoor terrains without requiring any fine tuning.\nThese results highlight the potential for robust generalist locomotion across\ndiverse robots and terrains.", "AI": {"tldr": "GROQLoco\u662f\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u96c6\u8bad\u7ec3\u901a\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u8de8\u673a\u5668\u4eba\u548c\u5730\u5f62\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u548c\u4e0d\u540c\u5f62\u6001\u4e0b\u7684\u5b9e\u65f6\u9002\u5e94\u95ee\u9898\uff0c\u5229\u7528\u79bb\u7ebf\u8bad\u7ec3\u5b9e\u73b0\u901a\u7528\u8fd0\u52a8\u7b56\u7565\u3002", "method": "\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff08\u5305\u62ec\u5468\u671f\u6027\u6b65\u6001\u548c\u975e\u5468\u671f\u6027\u6b65\u6001\uff09\uff0c\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u901a\u7528\u6a21\u578b\uff0c\u76f4\u63a5\u5904\u7406\u672c\u4f53\u611f\u77e5\u6570\u636e\u3002", "result": "\u5728\u591a\u79cd\u56db\u8db3\u673a\u5668\u4eba\u548c\u5730\u5f62\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5305\u62ec\u5546\u4e1a\u673a\u5668\u4ebaUnitree Go1\u548c70kg\u7684Stoch 5\u3002", "conclusion": "GROQLoco\u5c55\u793a\u4e86\u8de8\u673a\u5668\u4eba\u548c\u5730\u5f62\u7684\u9c81\u68d2\u901a\u7528\u8fd0\u52a8\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11312", "pdf": "https://arxiv.org/pdf/2505.11312", "abs": "https://arxiv.org/abs/2505.11312", "authors": ["Emanuele Francazi", "Francesco Pinto", "Aurelien Lucchi", "Marco Baity-Jesi"], "title": "Where You Place the Norm Matters: From Prejudiced to Neutral Initializations", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Normalization layers, such as Batch Normalization and Layer Normalization,\nare central components in modern neural networks, widely adopted to improve\ntraining stability and generalization. While their practical effectiveness is\nwell documented, a detailed theoretical understanding of how normalization\naffects model behavior, starting from initialization, remains an important open\nquestion. In this work, we investigate how both the presence and placement of\nnormalization within hidden layers influence the statistical properties of\nnetwork predictions before training begins. In particular, we study how these\nchoices shape the distribution of class predictions at initialization, which\ncan range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a\nsubset of classes. Our analysis shows that normalization placement induces\nsystematic differences in the initial prediction behavior of neural networks,\nwhich in turn shape the dynamics of learning. By linking architectural choices\nto prediction statistics at initialization, our work provides a principled\nunderstanding of how normalization can influence early training behavior and\noffers guidance for more controlled and interpretable network design.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f52\u4e00\u5316\u5c42\uff08\u5982\u6279\u5f52\u4e00\u5316\u548c\u5c42\u5f52\u4e00\u5316\uff09\u5728\u795e\u7ecf\u7f51\u7edc\u521d\u59cb\u5316\u9636\u6bb5\u5bf9\u9884\u6d4b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u5176\u5982\u4f55\u5851\u9020\u5b66\u4e60\u52a8\u6001\u3002", "motivation": "\u5f52\u4e00\u5316\u5c42\u5728\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u521d\u59cb\u884c\u4e3a\u7684\u5f71\u54cd\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f52\u4e00\u5316\u5c42\u7684\u5b58\u5728\u548c\u4f4d\u7f6e\uff0c\u7814\u7a76\u5176\u5bf9\u7f51\u7edc\u521d\u59cb\u5316\u9636\u6bb5\u9884\u6d4b\u7edf\u8ba1\u7279\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5f52\u4e00\u5316\u4f4d\u7f6e\u4f1a\u5bfc\u81f4\u521d\u59cb\u9884\u6d4b\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4ece\u800c\u5f71\u54cd\u5b66\u4e60\u52a8\u6001\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f52\u4e00\u5316\u5c42\u5982\u4f55\u5f71\u54cd\u65e9\u671f\u8bad\u7ec3\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u6307\u5bfc\u66f4\u53ef\u63a7\u548c\u53ef\u89e3\u91ca\u7684\u7f51\u7edc\u8bbe\u8ba1\u3002", "relevance": 60.0}}
{"id": "2505.10923", "pdf": "https://arxiv.org/pdf/2505.10923", "abs": "https://arxiv.org/abs/2505.10923", "authors": ["Simeon Adebola", "Shuangyu Xie", "Chung Min Kim", "Justin Kerr", "Bart M. van Marrewijk", "Mieke van Vlaardingen", "Tim van Daalen", "Robert van Loo", "Jose Luis Susa Rincon", "Eugen Solowjow", "Rick van de Zedde", "Ken Goldberg"], "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate temporal reconstructions of plant growth are essential for plant\nphenotyping and breeding, yet remain challenging due to complex geometries,\nocclusions, and non-rigid deformations of plants. We present a novel framework\nfor building temporal digital twins of plants by combining 3D Gaussian\nSplatting with a robust sample alignment pipeline. Our method begins by\nreconstructing Gaussian Splats from multi-view camera data, then leverages a\ntwo-stage registration approach: coarse alignment through feature-based\nmatching and Fast Global Registration, followed by fine alignment with\nIterative Closest Point. This pipeline yields a consistent 4D model of plant\ndevelopment in discrete time steps. We evaluate the approach on data from the\nNetherlands Plant Eco-phenotyping Center, demonstrating detailed temporal\nreconstructions of Sequoia and Quinoa species. Videos and Images can be seen at\nhttps://berkeleyautomation.github.io/GrowSplat/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u548c\u6837\u672c\u5bf9\u9f50\u7ba1\u9053\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u690d\u7269\u7684\u65f6\u95f4\u6570\u5b57\u5b6a\u751f\u4f53\u3002", "motivation": "\u690d\u7269\u751f\u957f\u7684\u65f6\u95f4\u91cd\u5efa\u5bf9\u8868\u578b\u548c\u80b2\u79cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u906e\u6321\u548c\u975e\u521a\u6027\u53d8\u5f62\uff0c\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4ece\u591a\u89c6\u89d2\u76f8\u673a\u6570\u636e\u91cd\u5efa\u9ad8\u65af\u6cfc\u6e85\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u914d\u51c6\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7279\u5f81\u7684\u7c97\u5bf9\u9f50\u548c\u5feb\u901f\u5168\u5c40\u914d\u51c6\uff0c\u7136\u540e\u901a\u8fc7\u8fed\u4ee3\u6700\u8fd1\u70b9\u8fdb\u884c\u7cbe\u7ec6\u5bf9\u9f50\u3002", "result": "\u5728\u8377\u5170\u690d\u7269\u751f\u6001\u8868\u578b\u4e2d\u5fc3\u7684\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7ea2\u6749\u548c\u85dc\u9ea6\u7269\u79cd\u7684\u8be6\u7ec6\u65f6\u95f4\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u4e00\u81f4\u7684\u690d\u7269\u53d1\u80b24D\u6a21\u578b\uff0c\u4e3a\u690d\u7269\u8868\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 10.0}}
{"id": "2505.11321", "pdf": "https://arxiv.org/pdf/2505.11321", "abs": "https://arxiv.org/abs/2505.11321", "authors": ["Pu Yang", "J. A. Barria"], "title": "Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network\n(RWPNN) is proposed, which aims at detecting anomalies in non-stationary\nenvironments by modelling the temporal features using a nonparametric density\nestimation network. The novel framework consists of two components, a Stacked\nRecurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in\na latent space, and a Multi-Receptive-field Wavelet Probabilistic Network\n(MRWPN) that creates an ensemble probabilistic model to characterise the latent\nspace. This formulation extends the standard wavelet probabilistic networks to\nwavelet deep probabilistic networks, which can handle higher data\ndimensionality. The MRWPN module can adapt to different rates of data variation\nin different datasets without imposing strong distribution assumptions,\nresulting in a more robust and accurate detection for Time Series Anomaly\nDetection (TSAD) tasks in the non-stationary environment. We carry out the\nassessment on 45 real-world time series datasets from various domains, verify\nthe performance of RWPNN in TSAD tasks with several constraints, and show its\nability to provide early warnings for anomalous events.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5faa\u73af\u5c0f\u6ce2\u6982\u7387\u795e\u7ecf\u7f51\u7edc\uff08RWPNN\uff09\uff0c\u7528\u4e8e\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5bc6\u5ea6\u4f30\u8ba1\u7f51\u7edc\u5efa\u6a21\u65f6\u95f4\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u5806\u53e0\u5faa\u73af\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff08SREnc-Dec\uff09\u548c\u591a\u611f\u53d7\u91ce\u5c0f\u6ce2\u6982\u7387\u7f51\u7edc\uff08MRWPN\uff09\uff0c\u6269\u5c55\u6807\u51c6\u5c0f\u6ce2\u6982\u7387\u7f51\u7edc\u4ee5\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u3002", "result": "\u572845\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86RWPNN\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5f02\u5e38\u4e8b\u4ef6\u65e9\u671f\u9884\u8b66\u4e2d\u7684\u80fd\u529b\u3002", "conclusion": "RWPNN\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u7684\u53d8\u5316\u7387\uff0c\u65e0\u9700\u5f3a\u5206\u5e03\u5047\u8bbe\u3002", "relevance": 30.0}}
{"id": "2505.11335", "pdf": "https://arxiv.org/pdf/2505.11335", "abs": "https://arxiv.org/abs/2505.11335", "authors": ["Jincheng Huang", "Jie Xu", "Xiaoshuang Shi", "Ping Hu", "Lei Feng", "Xiaofeng Zhu"], "title": "The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on\ngraph-based tasks. However, their predictive confidence is often miscalibrated,\ntypically exhibiting under-confidence, which harms the reliability of their\ndecisions. Existing calibration methods for GNNs normally introduce additional\ncalibration components, which fail to capture the intrinsic relationship\nbetween the model and the prediction confidence, resulting in limited\ntheoretical guarantees and increased computational overhead. To address this\nissue, we propose a simple yet efficient graph calibration method. We establish\na unified theoretical framework revealing that model confidence is jointly\ngoverned by class-centroid-level and node-level calibration at the final layer.\nBased on this insight, we theoretically show that reducing the weight decay of\nthe final-layer parameters alleviates GNN under-confidence by acting on the\nclass-centroid level, while node-level calibration acts as a finer-grained\ncomplement to class-centroid level calibration, which encourages each test node\nto be closer to its predicted class centroid at the final-layer\nrepresentations. Extensive experiments validate the superiority of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6700\u540e\u4e00\u5c42\u53c2\u6570\u7684\u6743\u91cd\u8870\u51cf\u548c\u8282\u70b9\u7ea7\u6821\u51c6\uff0c\u89e3\u51b3\u4e86GNN\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "GNN\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u901a\u5e38\u6821\u51c6\u4e0d\u8db3\uff0c\u5f71\u54cd\u51b3\u7b56\u53ef\u9760\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6355\u6349\u6a21\u578b\u4e0e\u7f6e\u4fe1\u5ea6\u7684\u5185\u5728\u5173\u7cfb\u3002", "method": "\u5efa\u7acb\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u7f6e\u4fe1\u5ea6\u7531\u6700\u540e\u4e00\u5c42\u7684\u7c7b\u4e2d\u5fc3\u7ea7\u548c\u8282\u70b9\u7ea7\u6821\u51c6\u5171\u540c\u51b3\u5b9a\uff0c\u5e76\u901a\u8fc7\u51cf\u5c11\u6700\u540e\u4e00\u5c42\u6743\u91cd\u8870\u51cf\u548c\u8282\u70b9\u7ea7\u6821\u51c6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548c\u7b80\u5355\u8c03\u6574\uff0c\u6709\u6548\u89e3\u51b3\u4e86GNN\u7f6e\u4fe1\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u53ef\u9760\u6027\u3002", "relevance": 40.0}}
{"id": "2505.10993", "pdf": "https://arxiv.org/pdf/2505.10993", "abs": "https://arxiv.org/abs/2505.10993", "authors": ["Yuan Zhang", "Xinfeng Zhang", "Xiaoming Qi Xinyu Wu", "Feng Chen", "Guanyu Yang", "Huazhu Fu"], "title": "Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages,9 figures", "summary": "Generative modeling has emerged as a promising direction in computational\npathology, offering capabilities such as data-efficient learning, synthetic\ndata augmentation, and multimodal representation across diverse diagnostic\ntasks. This review provides a comprehensive synthesis of recent progress in the\nfield, organized into four key domains: image generation, text generation,\nmultimodal image-text generation, and other generative applications, including\nspatial simulation and molecular inference. By analyzing over 150\nrepresentative studies, we trace the evolution of generative architectures from\nearly generative adversarial networks to recent advances in diffusion models\nand foundation models with generative capabilities. We further examine the\ndatasets and evaluation protocols commonly used in this domain and highlight\nongoing limitations, including challenges in generating high-fidelity whole\nslide images, clinical interpretability, and concerns related to the ethical\nand legal implications of synthetic data. The review concludes with a\ndiscussion of open challenges and prospective research directions, with an\nemphasis on developing unified, multimodal, and clinically deployable\ngenerative systems. This work aims to provide a foundational reference for\nresearchers and practitioners developing and applying generative models in\ncomputational pathology.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u56fe\u50cf\u751f\u6210\u3001\u6587\u672c\u751f\u6210\u3001\u591a\u6a21\u6001\u751f\u6210\u7b49\u65b9\u5411\uff0c\u5e76\u8ba8\u8bba\u4e86\u6280\u672f\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5177\u6709\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u3001\u5408\u6210\u6570\u636e\u589e\u5f3a\u7b49\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u9ad8\u4fdd\u771f\u751f\u6210\u3001\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u7b49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5168\u9762\u7684\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u5206\u6790150\u591a\u9879\u4ee3\u8868\u6027\u7814\u7a76\uff0c\u68b3\u7406\u4e86\u4ece\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5230\u6269\u6563\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u67b6\u6784\u6f14\u53d8\uff0c\u5e76\u603b\u7ed3\u4e86\u5e38\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4ecd\u5b58\u5728\u6280\u672f\u9650\u5236\u548c\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u9700\u5f00\u53d1\u7edf\u4e00\u3001\u591a\u6a21\u6001\u4e14\u4e34\u5e8a\u53ef\u90e8\u7f72\u7684\u751f\u6210\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.11342", "pdf": "https://arxiv.org/pdf/2505.11342", "abs": "https://arxiv.org/abs/2505.11342", "authors": ["Andrew W. Rosemberg", "Joaquim Dias Garcia", "Russell Bent", "Pascal Van Hentenryck"], "title": "Sobolev Training of End-to-End Optimization Proxies", "categories": ["cs.LG", "math.OC"], "comment": "9 Pages, 4 Figures, 5 Tables", "summary": "Optimization proxies - machine learning models trained to approximate the\nsolution mapping of parametric optimization problems in a single forward pass -\noffer dramatic reductions in inference time compared to traditional iterative\nsolvers. This work investigates the integration of solver sensitivities into\nsuch end to end proxies via a Sobolev training paradigm and does so in two\ndistinct settings: (i) fully supervised proxies, where exact solver outputs and\nsensitivities are available, and (ii) self supervised proxies that rely only on\nthe objective and constraint structure of the underlying optimization problem.\nBy augmenting the standard training loss with directional derivative\ninformation extracted from the solver, the proxy aligns both its predicted\nsolutions and local derivatives with those of the optimizer. Under Lipschitz\ncontinuity assumptions on the true solution mapping, matching first order\nsensitivities is shown to yield uniform approximation error proportional to the\ntraining set covering radius. Empirically, different impacts are observed in\neach studied setting. On three large Alternating Current Optimal Power Flow\nbenchmarks, supervised Sobolev training cuts mean squared error by up to 56\npercent and the median worst case constraint violation by up to 400 percent\nwhile keeping the optimality gap below 0.22 percent. For a mean variance\nportfolio task trained without labeled solutions, self supervised Sobolev\ntraining halves the average optimality gap in the medium risk region (standard\ndeviation above 10 percent of budget) and matches the baseline elsewhere.\nTogether, these results highlight Sobolev training whether supervised or self\nsupervised as a path to fast reliable surrogates for safety critical large\nscale optimization workloads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7Sobolev\u8bad\u7ec3\u8303\u5f0f\u5c06\u6c42\u89e3\u5668\u654f\u611f\u6027\u96c6\u6210\u5230\u4f18\u5316\u4ee3\u7406\u4e2d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8fed\u4ee3\u6c42\u89e3\u5668\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u8f83\u957f\uff0c\u4f18\u5316\u4ee3\u7406\u6a21\u578b\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u8fd1\u4f3c\u6c42\u89e3\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6c42\u89e3\u5668\u654f\u611f\u6027\u7684\u8003\u8651\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7Sobolev\u8bad\u7ec3\u63d0\u5347\u4ee3\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528Sobolev\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\uff08\u5b8c\u5168\u76d1\u7763\u548c\u81ea\u76d1\u7763\uff09\u5c06\u6c42\u89e3\u5668\u7684\u65b9\u5411\u5bfc\u6570\u4fe1\u606f\u6574\u5408\u5230\u4ee3\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728\u5b8c\u5168\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\u51cf\u5c1156%\uff0c\u6700\u574f\u7ea6\u675f\u8fdd\u53cd\u51cf\u5c11400%\uff1b\u5728\u81ea\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u4e2d\u7b49\u98ce\u9669\u533a\u57df\u7684\u4f18\u5316\u5dee\u8ddd\u51cf\u534a\u3002", "conclusion": "Sobolev\u8bad\u7ec3\u4e3a\u5927\u89c4\u6a21\u5b89\u5168\u5173\u952e\u4f18\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u5feb\u901f\u53ef\u9760\u7684\u4ee3\u7406\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.11032", "pdf": "https://arxiv.org/pdf/2505.11032", "abs": "https://arxiv.org/abs/2505.11032", "authors": ["Yuran Wang", "Ruihai Wu", "Yue Chen", "Jiarui Wang", "Jiaqi Liang", "Ziyu Zhu", "Haoran Geng", "Jitendra Malik", "Pieter Abbeel", "Hao Dong"], "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Garment manipulation is a critical challenge due to the diversity in garment\ncategories, geometries, and deformations. Despite this, humans can effortlessly\nhandle garments, thanks to the dexterity of our hands. However, existing\nresearch in the field has struggled to replicate this level of dexterity,\nprimarily hindered by the lack of realistic simulations of dexterous garment\nmanipulation. Therefore, we propose DexGarmentLab, the first environment\nspecifically designed for dexterous (especially bimanual) garment manipulation,\nwhich features large-scale high-quality 3D assets for 15 task scenarios, and\nrefines simulation techniques tailored for garment modeling to reduce the\nsim-to-real gap. Previous data collection typically relies on teleoperation or\ntraining expert reinforcement learning (RL) policies, which are labor-intensive\nand inefficient. In this paper, we leverage garment structural correspondence\nto automatically generate a dataset with diverse trajectories using only a\nsingle expert demonstration, significantly reducing manual intervention.\nHowever, even extensive demonstrations cannot cover the infinite states of\ngarments, which necessitates the exploration of new algorithms. To improve\ngeneralization across diverse garment shapes and deformations, we propose a\nHierarchical gArment-manipuLation pOlicy (HALO). It first identifies\ntransferable affordance points to accurately locate the manipulation area, then\ngenerates generalizable trajectories to complete the task. Through extensive\nexperiments and detailed analysis of our method and baseline, we demonstrate\nthat HALO consistently outperforms existing methods, successfully generalizing\nto previously unseen instances even with significant variations in shape and\ndeformation where others fail. Our project page is available at:\nhttps://wayrise.github.io/DexGarmentLab/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DexGarmentLab\u73af\u5883\uff0c\u4e13\u6ce8\u4e8e\u7075\u5de7\uff08\u5c24\u5176\u662f\u53cc\u624b\uff09\u670d\u88c5\u64cd\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86HALO\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u5bf9\u4e0d\u540c\u670d\u88c5\u5f62\u72b6\u548c\u53d8\u5f62\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u670d\u88c5\u64cd\u4f5c\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u4f7f\u5f97\u73b0\u6709\u7814\u7a76\u96be\u4ee5\u6a21\u62df\u4eba\u7c7b\u7684\u7075\u5de7\u6027\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u3002", "method": "\u63d0\u51faDexGarmentLab\u73af\u5883\uff0c\u5305\u542b\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf3D\u8d44\u4ea7\u548c\u6539\u8fdb\u7684\u6a21\u62df\u6280\u672f\uff1b\u5229\u7528\u670d\u88c5\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u81ea\u52a8\u751f\u6210\u6570\u636e\u96c6\uff1b\u63d0\u51faHALO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u8bc6\u522b\u53ef\u8f6c\u79fb\u7684\u64cd\u63a7\u70b9\u5e76\u751f\u6210\u6cdb\u5316\u8f68\u8ff9\u3002", "result": "HALO\u7b97\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u670d\u88c5\u5f62\u72b6\u548c\u53d8\u5f62\u3002", "conclusion": "DexGarmentLab\u548cHALO\u4e3a\u670d\u88c5\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u5e76\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.10994", "pdf": "https://arxiv.org/pdf/2505.10994", "abs": "https://arxiv.org/abs/2505.10994", "authors": ["Rees Chang", "Angela Pak", "Alex Guerra", "Ni Zhan", "Nick Richardson", "Elif Ertekin", "Ryan P. Adams"], "title": "Space Group Equivariant Crystal Diffusion", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Accelerating inverse design of crystalline materials with generative models\nhas significant implications for a range of technologies. Unlike other atomic\nsystems, 3D crystals are invariant to discrete groups of isometries called the\nspace groups. Crucially, these space group symmetries are known to heavily\ninfluence materials properties. We propose SGEquiDiff, a crystal generative\nmodel which naturally handles space group constraints with space group\ninvariant likelihoods. SGEquiDiff consists of an SE(3)-invariant, telescoping\ndiscrete sampler of crystal lattices; permutation-invariant, transformer-based\nautoregressive sampling of Wyckoff positions, elements, and numbers of\nsymmetrically unique atoms; and space group equivariant diffusion of atomic\ncoordinates. We show that space group equivariant vector fields automatically\nlive in the tangent spaces of the Wyckoff positions. SGEquiDiff achieves\nstate-of-the-art performance on standard benchmark datasets as assessed by\nquantitative proxy metrics and quantum mechanical calculations.", "AI": {"tldr": "SGEquiDiff\u662f\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u901f\u6676\u4f53\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5904\u7406\u7a7a\u95f4\u7fa4\u7ea6\u675f\u548c\u7a7a\u95f4\u7fa4\u4e0d\u53d8\u4f3c\u7136\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u6676\u4f53\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\u5bf9\u591a\u79cd\u6280\u672f\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f463D\u6676\u4f53\u7684\u7a7a\u95f4\u7fa4\u5bf9\u79f0\u6027\u5bf9\u6750\u6599\u6027\u8d28\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u7136\u5904\u7406\u8fd9\u4e9b\u7ea6\u675f\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "SGEquiDiff\u7ed3\u5408\u4e86SE(3)\u4e0d\u53d8\u79bb\u6563\u91c7\u6837\u5668\u3001\u57fa\u4e8eTransformer\u7684\u81ea\u56de\u5f52\u91c7\u6837\u548c\u7a7a\u95f4\u7fa4\u7b49\u53d8\u6269\u6563\uff0c\u5904\u7406\u6676\u4f53\u7ed3\u6784\u751f\u6210\u4e2d\u7684\u5bf9\u79f0\u6027\u7ea6\u675f\u3002", "result": "\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u5b9a\u91cf\u4ee3\u7406\u6307\u6807\u548c\u91cf\u5b50\u529b\u5b66\u8ba1\u7b97\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "SGEquiDiff\u4e3a\u6676\u4f53\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5bf9\u79f0\u6027\u611f\u77e5\u7684\u751f\u6210\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.11346", "pdf": "https://arxiv.org/pdf/2505.11346", "abs": "https://arxiv.org/abs/2505.11346", "authors": ["Andreas Roth", "Thomas Liebig"], "title": "What Can We Learn From MIMO Graph Convolutions?", "categories": ["cs.LG"], "comment": "IJCAI 2025", "summary": "Most graph neural networks (GNNs) utilize approximations of the general graph\nconvolution derived in the graph Fourier domain. While GNNs are typically\napplied in the multi-input multi-output (MIMO) case, the approximations are\nperformed in the single-input single-output (SISO) case. In this work, we first\nderive the MIMO graph convolution through the convolution theorem and\napproximate it directly in the MIMO case. We find the key MIMO-specific\nproperty of the graph convolution to be operating on multiple computational\ngraphs, or equivalently, applying distinct feature transformations for each\npair of nodes. As a localized approximation, we introduce localized MIMO graph\nconvolutions (LMGCs), which generalize many linear message-passing neural\nnetworks. For almost every choice of edge weights, we prove that LMGCs with a\nsingle computational graph are injective on multisets, and the resulting\nrepresentations are linearly independent when more than one computational graph\nis used. Our experimental results confirm that an LMGC can combine the benefits\nof various methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u56fe\u5377\u79ef\u7684\u76f4\u63a5\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u79f0\u4e3a\u5c40\u90e8\u5316MIMO\u56fe\u5377\u79ef\uff08LMGC\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u591a\u8ba1\u7b97\u56fe\u4e0b\u7684\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u901a\u5e38\u5728\u5355\u8f93\u5165\u5355\u8f93\u51fa\uff08SISO\uff09\u60c5\u51b5\u4e0b\u8fd1\u4f3c\u56fe\u5377\u79ef\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u591a\u4e3a\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u76f4\u63a5\u9488\u5bf9MIMO\u60c5\u51b5\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5377\u79ef\u5b9a\u7406\u63a8\u5bfcMIMO\u56fe\u5377\u79ef\uff0c\u5e76\u63d0\u51fa\u5c40\u90e8\u5316MIMO\u56fe\u5377\u79ef\uff08LMGC\uff09\u4f5c\u4e3a\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u8ba1\u7b97\u56fe\u4e0b\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLMGC\u80fd\u591f\u7ed3\u5408\u591a\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u5728\u591a\u8ba1\u7b97\u56fe\u4e0b\u751f\u6210\u7ebf\u6027\u72ec\u7acb\u7684\u8868\u793a\u3002", "conclusion": "LMGC\u4e3aMIMO\u56fe\u5377\u79ef\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5c40\u90e8\u5316\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "relevance": 30.0}}
{"id": "2505.11347", "pdf": "https://arxiv.org/pdf/2505.11347", "abs": "https://arxiv.org/abs/2505.11347", "authors": ["Johannes Schwab", "Bryan Kelly", "Semyon Malamud", "Teng Andrea Xu"], "title": "Training NTK to Generalize with KARE", "categories": ["cs.LG"], "comment": null, "summary": "The performance of the data-dependent neural tangent kernel (NTK; Jacot et\nal. (2018)) associated with a trained deep neural network (DNN) often matches\nor exceeds that of the full network. This implies that DNN training via\ngradient descent implicitly performs kernel learning by optimizing the NTK. In\nthis paper, we propose instead to optimize the NTK explicitly. Rather than\nminimizing empirical risk, we train the NTK to minimize its generalization\nerror using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot\net al. (2020)). Our simulations and real data experiments show that NTKs\ntrained with KARE consistently match or significantly outperform the original\nDNN and the DNN- induced NTK (the after-kernel). These results suggest that\nexplicitly trained kernels can outperform traditional end-to-end DNN\noptimization in certain settings, challenging the conventional dominance of\nDNNs. We argue that explicit training of NTK is a form of over-parametrized\nfeature learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u663e\u5f0f\u4f18\u5316\u795e\u7ecf\u5207\u7ebf\u6838\uff08NTK\uff09\u6765\u66ff\u4ee3\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u8bad\u7ec3\uff0c\u4f7f\u7528\u6838\u5bf9\u9f50\u98ce\u9669\u4f30\u8ba1\u5668\uff08KARE\uff09\u6700\u5c0f\u5316\u6cdb\u5316\u8bef\u5dee\uff0c\u5b9e\u9a8c\u8868\u660e\u663e\u5f0f\u8bad\u7ec3\u7684NTK\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfDNN\u53ca\u5176\u8bf1\u5bfc\u7684NTK\u3002", "motivation": "\u4f20\u7edfDNN\u8bad\u7ec3\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u9690\u5f0f\u5b66\u4e60NTK\uff0c\u4f46\u663e\u5f0f\u4f18\u5316NTK\u53ef\u80fd\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u6311\u6218DNN\u7684\u4f20\u7edf\u4f18\u52bf\u3002", "method": "\u4f7f\u7528KARE\u663e\u5f0f\u8bad\u7ec3NTK\uff0c\u6700\u5c0f\u5316\u5176\u6cdb\u5316\u8bef\u5dee\uff0c\u800c\u975e\u76f4\u63a5\u6700\u5c0f\u5316\u7ecf\u9a8c\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u663e\u5f0f\u8bad\u7ec3\u7684NTK\u6027\u80fd\u4f18\u4e8e\u539f\u59cbDNN\u53ca\u5176\u8bf1\u5bfc\u7684NTK\u3002", "conclusion": "\u663e\u5f0f\u8bad\u7ec3NTK\u662f\u4e00\u79cd\u8fc7\u53c2\u6570\u5316\u7279\u5f81\u5b66\u4e60\u5f62\u5f0f\uff0c\u53ef\u80fd\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u8d85\u8d8a\u4f20\u7edfDNN\u4f18\u5316\u3002", "relevance": 40.0}}
{"id": "2505.11116", "pdf": "https://arxiv.org/pdf/2505.11116", "abs": "https://arxiv.org/abs/2505.11116", "authors": ["Liam Boyle", "Jonas K\u00fchne", "Nicolas Baumann", "Niklas Bastuck", "Michele Magno"], "title": "Planar Velocity Estimation for Fast-Moving Mobile Robots Using Event-Based Optical Flow", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate velocity estimation is critical in mobile robotics, particularly for\ndriver assistance systems and autonomous driving. Wheel odometry fused with\nInertial Measurement Unit (IMU) data is a widely used method for velocity\nestimation; however, it typically requires strong assumptions, such as non-slip\nsteering, or complex vehicle dynamics models that do not hold under varying\nenvironmental conditions like slippery surfaces. We introduce an approach to\nvelocity estimation that is decoupled from wheel-to-surface traction\nassumptions by leveraging planar kinematics in combination with optical flow\nfrom event cameras pointed perpendicularly at the ground. The asynchronous\nmicro-second latency and high dynamic range of event cameras make them highly\nrobust to motion blur, a common challenge in vision-based perception techniques\nfor autonomous driving. The proposed method is evaluated through in-field\nexperiments on a 1:10 scale autonomous racing platform and compared to precise\nmotion capture data, demonstrating not only performance on par with the\nstate-of-the-art Event-VIO method but also a 38.3 % improvement in lateral\nerror. Qualitative experiments at highway speeds of up to 32 m/s further\nconfirm the effectiveness of our approach, indicating significant potential for\nreal-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u548c\u5e73\u9762\u8fd0\u52a8\u5b66\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u4e2d\u7684\u901f\u5ea6\u4f30\u8ba1\uff0c\u65e0\u9700\u4f9d\u8d56\u8f6e\u5730\u63a5\u89e6\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u8f6e\u5730\u63a5\u89e6\u5047\u8bbe\u6216\u590d\u6742\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5728\u591a\u53d8\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u7684\u5149\u5b66\u6d41\u548c\u5e73\u9762\u8fd0\u52a8\u5b66\uff0c\u5b9e\u73b0\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7684\u901f\u5ea6\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u6a2a\u5411\u8bef\u5dee\u964d\u4f4e38.3%\uff0c\u5e76\u5728\u9ad8\u901f\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u53d8\u73af\u5883\u4e0b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3002", "relevance": 10.0}}
{"id": "2505.11349", "pdf": "https://arxiv.org/pdf/2505.11349", "abs": "https://arxiv.org/abs/2505.11349", "authors": ["Yuanzhao Zhang", "William Gilpin"], "title": "Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning", "categories": ["cs.LG", "nlin.CD", "physics.comp-ph"], "comment": null, "summary": "Recently-developed time series foundation models for scientific machine\nlearning exhibit emergent abilities to predict physical systems. These\nabilities include zero-shot forecasting, in which a model forecasts future\nstates of a system given only a short trajectory as context. Here, we show that\nfoundation models applied to physical systems can give accurate predictions,\nbut that they fail to develop meaningful representations of the underlying\nphysics. Instead, foundation models often forecast by context parroting, a\nsimple zero-shot forecasting strategy that copies directly from the context. As\na result, a naive direct context parroting model scores higher than\nstate-of-the-art time-series foundation models on predicting a diverse range of\ndynamical systems, at a tiny fraction of the computational cost. We draw a\nparallel between context parroting and induction heads, which explains why\nlarge language models trained on text can be repurposed for time series\nforecasting. Our dynamical systems perspective also ties the scaling between\nforecast accuracy and context length to the fractal dimension of the attractor,\nproviding insight into the previously observed in-context neural scaling laws.\nContext parroting thus serves as a simple but tough-to-beat baseline for future\ntime-series foundation models and can help identify in-context learning\nstrategies beyond parroting.", "AI": {"tldr": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9884\u6d4b\u7269\u7406\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u9884\u6d4b\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u4e0a\u4e0b\u6587\u6a21\u4eff\uff08context parroting\uff09\uff0c\u800c\u975e\u5b66\u4e60\u7269\u7406\u89c4\u5f8b\u3002", "motivation": "\u63a2\u8ba8\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b66\u4e60\u7269\u7406\u89c4\u5f8b\uff0c\u8fd8\u662f\u4ec5\u901a\u8fc7\u6a21\u4eff\u4e0a\u4e0b\u6587\u8fdb\u884c\u9884\u6d4b\u3002", "method": "\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u4e0e\u76f4\u63a5\u4e0a\u4e0b\u6587\u6a21\u4eff\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5206\u6790\u5176\u9884\u6d4b\u7b56\u7565\u3002", "result": "\u76f4\u63a5\u4e0a\u4e0b\u6587\u6a21\u4eff\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u590d\u6742\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u6a21\u4eff\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u96be\u4ee5\u8d85\u8d8a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u8bc6\u522b\u66f4\u9ad8\u7ea7\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u3002", "relevance": 60.0}}
{"id": "2505.11011", "pdf": "https://arxiv.org/pdf/2505.11011", "abs": "https://arxiv.org/abs/2505.11011", "authors": ["Darija Barak", "Miguel Costa-Gomes"], "title": "Humans expect rationality and cooperation from LLM opponents in strategic games", "categories": ["econ.GN", "cs.AI", "cs.MA", "q-fin.EC"], "comment": null, "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4eba\u7c7b\u5728\u4e0eLLM\u6216\u5176\u4ed6\u4eba\u7c7b\u5bf9\u624b\u8fdb\u884c\u591a\u73a9\u5bb6p-beauty\u7ade\u8d5b\u65f6\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba\u7c7b\u5728\u9762\u5bf9LLM\u65f6\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u8f83\u4f4e\u7684\u6570\u503c\uff0c\u5c24\u5176\u662f\u96f6\u7eb3\u4ec0\u5747\u8861\u9009\u62e9\u3002", "motivation": "\u968f\u7740LLM\u5728\u793e\u4f1a\u548c\u7ecf\u6d4e\u4e92\u52a8\u4e2d\u7684\u666e\u53ca\uff0c\u7406\u89e3\u4eba\u7c7b\u5728\u6218\u7565\u73af\u5883\u4e2d\u5bf9LLM\u7684\u53cd\u5e94\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u8d27\u5e01\u6fc0\u52b1\u7684\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\uff0c\u901a\u8fc7\u53d7\u8bd5\u8005\u5185\u8bbe\u8ba1\u6bd4\u8f83\u4e2a\u4f53\u884c\u4e3a\u3002", "result": "\u4eba\u7c7b\u5728\u4e0eLLM\u5bf9\u6218\u65f6\u9009\u62e9\u66f4\u4f4e\u6570\u503c\uff0c\u5c24\u5176\u662f\u9ad8\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u53d7\u8bd5\u8005\u66f4\u503e\u5411\u4e8e\u96f6\u7eb3\u4ec0\u5747\u8861\u9009\u62e9\uff0c\u5e76\u8ba4\u4e3aLLM\u5177\u6709\u5408\u4f5c\u503e\u5411\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eLLM\u5728\u591a\u73a9\u5bb6\u540c\u65f6\u9009\u62e9\u6e38\u620f\u4e2d\u7684\u4e92\u52a8\u5dee\u5f02\uff0c\u4e3a\u6df7\u5408\u4eba\u673a\u7cfb\u7edf\u7684\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002", "relevance": 75.0}}
{"id": "2505.11356", "pdf": "https://arxiv.org/pdf/2505.11356", "abs": "https://arxiv.org/abs/2505.11356", "authors": ["Nero Z. Li", "Xuehao Zhai", "Zhichao Shi", "Boshen Shi", "Xuhui Jiang"], "title": "Fractal Graph Contrastive Learning", "categories": ["cs.LG"], "comment": null, "summary": "While Graph Contrastive Learning (GCL) has attracted considerable attention\nin the field of graph self-supervised learning, its performance heavily relies\non data augmentations that are expected to generate semantically consistent\npositive pairs. Existing strategies typically resort to random perturbations or\nlocal structure preservation, yet lack explicit control over global structural\nconsistency between augmented views. To address this limitation, we propose\nFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that\nleverages fractal self-similarity to enforce global topological coherence.\nFractalGCL introduces two key innovations: a renormalisation-based augmentation\nthat generates structurally aligned positive views via box coverings; and a\nfractal-dimension-aware contrastive loss that aligns graph embeddings according\nto their fractal dimensions. While combining the two innovations markedly\nboosts graph-representation quality, it also adds non-trivial computational\noverhead. To mitigate the computational overhead of fractal dimension\nestimation, we derive a one-shot estimator by proving that the dimension\ndiscrepancy between original and renormalised graphs converges weakly to a\ncentred Gaussian distribution. This theoretical insight enables a reduction in\ndimension computation cost by an order of magnitude, cutting overall training\ntime by approximately 61%. The experiments show that FractalGCL not only\ndelivers state-of-the-art results on standard benchmarks but also outperforms\ntraditional baselines on traffic networks by an average margin of about\nremarkably 7%. Codes are available at\n(https://anonymous.4open.science/r/FractalGCL-0511).", "AI": {"tldr": "FractalGCL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u81ea\u76f8\u4f3c\u6027\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u62d3\u6251\u4e00\u81f4\u6027\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u6270\u52a8\u6216\u5c40\u90e8\u7ed3\u6784\u4fdd\u7559\uff0c\u7f3a\u4e4f\u5bf9\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u91cd\u5f52\u4e00\u5316\u7684\u6570\u636e\u589e\u5f3a\u548c\u5206\u5f62\u7ef4\u5ea6\u611f\u77e5\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ea4\u901a\u7f51\u7edc\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e61%\u3002", "conclusion": "FractalGCL\u901a\u8fc7\u5206\u5f62\u7406\u8bba\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.11359", "pdf": "https://arxiv.org/pdf/2505.11359", "abs": "https://arxiv.org/abs/2505.11359", "authors": ["Zihang Jia", "Zhen Zhang", "Witold Pedrycz"], "title": "LGBQPC: Local Granular-Ball Quality Peaks Clustering", "categories": ["cs.LG"], "comment": null, "summary": "The density peaks clustering (DPC) algorithm has attracted considerable\nattention for its ability to detect arbitrarily shaped clusters based on a\nsimple yet effective assumption. Recent advancements integrating granular-ball\n(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which\nimproves computational efficiency. However, GBDPC demonstrates limitations when\nhandling complex clustering tasks, particularly those involving data with\ncomplex manifold structures or non-uniform density distributions. To overcome\nthese challenges, this paper proposes the local GB quality peaks clustering\n(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB\ngeneration and clustering processes based on the principle of justifiable\ngranularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,\nis developed, which systematically refines the original GB-POJG in four key\naspects: the objective function, termination criterion for GB division,\ndefinition of abnormal GB, and granularity level adaptation strategy. GB-POJG+\nsimplifies parameter configuration by requiring only a single penalty\ncoefficient and ensures high-quality GB generation while maintaining the number\nof generated GBs within an acceptable range. In the clustering phase, two key\ninnovations are introduced based on the GB k-nearest neighbor graph: relative\nGB quality for density estimation and geodesic distance for GB distance metric.\nThese modifications substantially improve the performance of GBDPC on datasets\nwith complex manifold structures or non-uniform density distributions.\nExtensive numerical experiments on 40 benchmark datasets, including both\nsynthetic and publicly available datasets, validate the superior performance of\nthe proposed LGBQPC algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c40\u90e8GB\u8d28\u91cf\u5cf0\u503c\u805a\u7c7b\u7b97\u6cd5\uff08LGBQPC\uff09\uff0c\u901a\u8fc7\u4f18\u5316GB\u751f\u6210\u548c\u805a\u7c7b\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86GBDPC\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "GBDPC\u5728\u5904\u7406\u590d\u6742\u6d41\u5f62\u7ed3\u6784\u6216\u4e0d\u5747\u5300\u5bc6\u5ea6\u5206\u5e03\u7684\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86GB-POJG+\u65b9\u6cd5\u4f18\u5316GB\u751f\u6210\uff0c\u5e76\u5728\u805a\u7c7b\u9636\u6bb5\u5f15\u5165\u76f8\u5bf9GB\u8d28\u91cf\u548c\u6d4b\u5730\u8ddd\u79bb\u3002", "result": "\u572840\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LGBQPC\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "LGBQPC\u663e\u8457\u63d0\u5347\u4e86GBDPC\u5728\u590d\u6742\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "relevance": 20.0}}
{"id": "2505.11142", "pdf": "https://arxiv.org/pdf/2505.11142", "abs": "https://arxiv.org/abs/2505.11142", "authors": ["Guido Caccianiga", "Yarden Sharon", "Bernard Javot", "Senya Polikovsky", "G\u00f6kce Erg\u00fcn", "Ivan Capobianco", "Andr\u00e9 L. Mihaljevic", "Anton Deguet", "Katherine J. Kuchenbecker"], "title": "Open-Source Multi-Viewpoint Surgical Telerobotics", "categories": ["cs.RO", "cs.CV"], "comment": "2 pages, 2 figures, ICRA-RAMI workshop long abstract", "summary": "As robots for minimally invasive surgery (MIS) gradually become more\naccessible and modular, we believe there is a great opportunity to rethink and\nexpand the visualization and control paradigms that have characterized surgical\nteleoperation since its inception. We conjecture that introducing one or more\nadditional adjustable viewpoints in the abdominal cavity would not only unlock\nnovel visualization and collaboration strategies for surgeons but also\nsubstantially boost the robustness of machine perception toward shared\nautonomy. Immediate advantages include controlling a second viewpoint and\nteleoperating surgical tools from a different perspective, which would allow\ncollaborating surgeons to adjust their views independently and still maneuver\ntheir robotic instruments intuitively. Furthermore, we believe that capturing\nsynchronized multi-view 3D measurements of the patient's anatomy would unlock\nadvanced scene representations. Accurate real-time intraoperative 3D perception\nwill allow algorithmic assistants to directly control one or more robotic\ninstruments and/or robotic cameras. Toward these goals, we are building a\nsynchronized multi-viewpoint, multi-sensor robotic surgery system by\nintegrating high-performance vision components and upgrading the da Vinci\nResearch Kit control logic. This short paper reports a functional summary of\nour setup and elaborates on its potential impacts in research and future\nclinical practice. By fully open-sourcing our system, we will enable the\nresearch community to reproduce our setup, improve it, and develop powerful\nalgorithms, effectively boosting clinical translation of cutting-edge research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u3001\u591a\u4f20\u611f\u5668\u7684\u673a\u5668\u4eba\u624b\u672f\u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u8fdb\u5fae\u521b\u624b\u672f\u7684\u53ef\u89c6\u5316\u548c\u63a7\u5236\u8303\u5f0f\uff0c\u63d0\u5347\u673a\u5668\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u63a8\u52a8\u5171\u4eab\u81ea\u4e3b\u6743\u7684\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u5fae\u521b\u624b\u672f\u673a\u5668\u4eba\u7684\u666e\u53ca\u548c\u6a21\u5757\u5316\uff0c\u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u91cd\u65b0\u601d\u8003\u624b\u672f\u8fdc\u7a0b\u64cd\u4f5c\u7684\u53ef\u89c6\u5316\u548c\u63a7\u5236\u8303\u5f0f\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u611f\u77e5\u548c\u5171\u4eab\u81ea\u4e3b\u6743\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u9ad8\u6027\u80fd\u89c6\u89c9\u7ec4\u4ef6\u548c\u5347\u7ea7\u8fbe\u82ac\u5947\u7814\u7a76\u5957\u4ef6\u7684\u63a7\u5236\u903b\u8f91\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540c\u6b65\u591a\u89c6\u89d2\u3001\u591a\u4f20\u611f\u5668\u7684\u673a\u5668\u4eba\u624b\u672f\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u591a\u89c6\u89d2\u63a7\u5236\u548c\u5b9e\u65f63D\u611f\u77e5\uff0c\u4e3a\u7b97\u6cd5\u52a9\u624b\u76f4\u63a5\u63a7\u5236\u624b\u672f\u5de5\u5177\u6216\u6444\u50cf\u5934\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u7cfb\u7edf\uff0c\u4f5c\u8005\u5e0c\u671b\u63a8\u52a8\u7814\u7a76\u793e\u533a\u6539\u8fdb\u8be5\u7cfb\u7edf\uff0c\u52a0\u901f\u524d\u6cbf\u7814\u7a76\u7684\u4e34\u5e8a\u8f6c\u5316\u3002", "relevance": 30.0}}
{"id": "2505.11030", "pdf": "https://arxiv.org/pdf/2505.11030", "abs": "https://arxiv.org/abs/2505.11030", "authors": ["David M. Berry"], "title": "The heteronomy of algorithms: Traditional knowledge and computational knowledge", "categories": ["cs.CY", "cs.AI", "cs.MM", "K.4.0; K.4.1"], "comment": null, "summary": "If an active citizen should increasingly be a computationally enlightened\none, replacing the autonomy of reason with the heteronomy of algorithms, then I\nargue in this article that we must begin teaching the principles of critiquing\nthe computal through new notions of what we might call digital Bildung. Indeed,\nif civil society itself is mediated by computational systems and media, the\npublic use of reason must also be complemented by skills for negotiating and\nusing these computal forms to articulate such critique. Not only is there a\nneed to raise the intellectual tone regarding computation and its related\nsoftwarization processes, but there is an urgent need to attend to the likely\nepistemic challenges from computation which, as presently constituted, tends\ntowards justification through a philosophy of utility rather than through a\nphilosophy of care for the territory of the intellect. We therefore need to\ndevelop an approach to this field that uses concepts and methods drawn from\nphilosophy, politics, history, anthropology, sociology, media studies, computer\nscience, and the humanities more generally, to try to understand these issues -\nparticularly the way in which software and data increasingly penetrate our\neveryday life and the pressures and fissures that are created. We must, in\nother words, move to undertake a critical interdisciplinary research program to\nunderstand the way in which these systems are created, instantiated, and\nnormatively engendered in both specific and general contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9700\u8981\u6559\u6388\u2018\u6570\u5b57\u6559\u517b\u2019\uff08digital Bildung\uff09\u4ee5\u6279\u5224\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5f3a\u8c03\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u793e\u4f1a\u88ab\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u4ecb\u5316\uff0c\u516c\u4f17\u9700\u8981\u6279\u5224\u548c\u534f\u5546\u8ba1\u7b97\u5f62\u5f0f\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u5176\u5e26\u6765\u7684\u8ba4\u77e5\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u7ed3\u5408\u54f2\u5b66\u3001\u653f\u6cbb\u5b66\u3001\u5386\u53f2\u5b66\u3001\u4eba\u7c7b\u5b66\u3001\u793e\u4f1a\u5b66\u3001\u5a92\u4f53\u7814\u7a76\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u3002", "result": "\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u6279\u5224\u6027\u7814\u7a76\u8ba1\u5212\uff0c\u4ee5\u7406\u89e3\u8ba1\u7b97\u7cfb\u7edf\u7684\u521b\u5efa\u548c\u89c4\u8303\u751f\u6210\u3002", "conclusion": "\u547c\u5401\u901a\u8fc7\u8de8\u5b66\u79d1\u7814\u7a76\u5e94\u5bf9\u8ba1\u7b97\u7cfb\u7edf\u5bf9\u793e\u4f1a\u7684\u6e17\u900f\u548c\u5f71\u54cd\u3002", "relevance": 30.0}}
{"id": "2505.11360", "pdf": "https://arxiv.org/pdf/2505.11360", "abs": "https://arxiv.org/abs/2505.11360", "authors": ["Rares Cristian", "Pavithra Harsha", "Georgia Perakis", "Brian Quanz"], "title": "Efficient End-to-End Learning for Decision-Making: A Meta-Optimization Approach", "categories": ["cs.LG"], "comment": null, "summary": "End-to-end learning has become a widely applicable and studied problem in\ntraining predictive ML models to be aware of their impact on downstream\ndecision-making tasks. These end-to-end models often outperform traditional\nmethods that separate training from the optimization and only myopically focus\non prediction error. However, the computational complexity of end-to-end\nframeworks poses a significant challenge, particularly for large-scale\nproblems. While training an ML model using gradient descent, each time we need\nto compute a gradient we must solve an expensive optimization problem. We\npresent a meta-optimization method that learns efficient algorithms to\napproximate optimization problems, dramatically reducing computational overhead\nof solving the decision problem in general, an aspect we leverage in the\ntraining within the end-to-end framework. Our approach introduces a neural\nnetwork architecture that near-optimally solves optimization problems while\nensuring feasibility constraints through alternate projections. We prove\nexponential convergence, approximation guarantees, and generalization bounds\nfor our learning method. This method offers superior computational efficiency,\nproducing high-quality approximations faster and scaling better with problem\nsize compared to existing techniques. Our approach applies to a wide range of\noptimization problems including deterministic, single-stage as well as\ntwo-stage stochastic optimization problems. We illustrate how our proposed\nmethod applies to (1) an electricity generation problem using real data from an\nelectricity routing company coordinating the movement of electricity throughout\n13 states, (2) a shortest path problem with a computer vision task of\npredicting edge costs from terrain maps, (3) a two-stage multi-warehouse\ncross-fulfillment newsvendor problem, as well as a variety of other\nnewsvendor-like problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u9ad8\u6548\u8fd1\u4f3c\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u7aef\u5230\u7aef\u5b66\u4e60\u5728\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u65f6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u3002", "method": "\u91c7\u7528\u5143\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fd1\u4f3c\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u6295\u5f71\u786e\u4fdd\u53ef\u884c\u6027\u7ea6\u675f\u3002", "result": "\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6307\u6570\u6536\u655b\u6027\u3001\u8fd1\u4f3c\u4fdd\u8bc1\u548c\u6cdb\u5316\u8fb9\u754c\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u786e\u5b9a\u6027\u548c\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "relevance": 60.0}}
{"id": "2505.11158", "pdf": "https://arxiv.org/pdf/2505.11158", "abs": "https://arxiv.org/abs/2505.11158", "authors": ["Xing Hu", "Xiangcheng Liu", "Qianqian Duan", "Danfeng Hong", "Dawei Zhang"], "title": "Diffusion Model in Hyperspectral Image Processing and Analysis: A Review", "categories": ["eess.IV", "cs.CV"], "comment": "33 pages,20 figures", "summary": "Hyperspectral image processing and analysis has important application value\nin remote sensing, agriculture and environmental monitoring, but its high\ndimensionality, data redundancy and noise interference etc. bring great\nchallenges to the analysis. Traditional models have limitations in dealing with\nthese complex data, and it is difficult to meet the increasing demand for\nanalysis. In recent years, Diffusion Model, as an emerging generative model,\nhas shown unique advantages in hyperspectral image processing. By simulating\nthe diffusion process of data in time, the Diffusion Model can effectively\nprocess high-dimensional data, generate high-quality samples, and perform well\nin denoising and data enhancement. In this paper, we review the recent research\nadvances in diffusion modeling for hyperspectral image processing and analysis,\nand discuss its applications in tasks such as high-dimensional data processing,\nnoise removal, classification, and anomaly detection. The performance of\ndiffusion-based models on image processing is compared and the challenges are\nsummarized. It is shown that the diffusion model can significantly improve the\naccuracy and efficiency of hyperspectral image analysis, providing a new\ndirection for future research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u3001\u53bb\u566a\u548c\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u603b\u7ed3\u4e86\u6311\u6218\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5904\u7406\u5728\u9065\u611f\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u5176\u9ad8\u7ef4\u5ea6\u548c\u566a\u58f0\u95ee\u9898\uff0c\u6269\u6563\u6a21\u578b\u56e0\u5176\u72ec\u7279\u4f18\u52bf\u6210\u4e3a\u65b0\u5174\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6570\u636e\u5728\u65f6\u95f4\u4e0a\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u6570\u636e\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u5e76\u5728\u53bb\u566a\u548c\u6570\u636e\u589e\u5f3a\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "result": "\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u4e00\u4e9b\u6311\u6218\uff0c\u5982\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11370", "pdf": "https://arxiv.org/pdf/2505.11370", "abs": "https://arxiv.org/abs/2505.11370", "authors": ["Jingwei Li", "Jing Xu", "Zifan Wang", "Huishuai Zhang", "Jingzhao Zhang"], "title": "Understanding Nonlinear Implicit Bias via Region Counts in Input Space", "categories": ["cs.LG"], "comment": null, "summary": "One explanation for the strong generalization ability of neural networks is\nimplicit bias. Yet, the definition and mechanism of implicit bias in non-linear\ncontexts remains little understood. In this work, we propose to characterize\nimplicit bias by the count of connected regions in the input space with the\nsame predicted label. Compared with parameter-dependent metrics (e.g., norm or\nnormalized margin), region count can be better adapted to nonlinear,\noverparameterized models, because it is determined by the function mapping and\nis invariant to reparametrization. Empirically, we found that small region\ncounts align with geometrically simple decision boundaries and correlate well\nwith good generalization performance. We also observe that good hyper-parameter\nchoices such as larger learning rates and smaller batch sizes can induce small\nregion counts. We further establish the theoretical connections and explain how\nlarger learning rate can induce small region counts in neural networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u5f0f\u504f\u5dee\uff08implicit bias\uff09\uff0c\u63d0\u51fa\u901a\u8fc7\u8f93\u5165\u7a7a\u95f4\u4e2d\u76f8\u540c\u9884\u6d4b\u6807\u7b7e\u7684\u8fde\u901a\u533a\u57df\u6570\u91cf\u6765\u8868\u5f81\u9690\u5f0f\u504f\u5dee\uff0c\u5e76\u53d1\u73b0\u5c0f\u533a\u57df\u6570\u91cf\u4e0e\u51e0\u4f55\u7b80\u5355\u7684\u51b3\u7b56\u8fb9\u754c\u548c\u826f\u597d\u6cdb\u5316\u6027\u80fd\u76f8\u5173\u3002", "motivation": "\u7406\u89e3\u975e\u7ebf\u6027\u80cc\u666f\u4e0b\u9690\u5f0f\u504f\u5dee\u7684\u5b9a\u4e49\u548c\u673a\u5236\uff0c\u5c24\u5176\u662f\u5728\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u4e2d\uff0c\u73b0\u6709\u53c2\u6570\u4f9d\u8d56\u7684\u5ea6\u91cf\uff08\u5982\u8303\u6570\u6216\u5f52\u4e00\u5316\u8fb9\u8ddd\uff09\u96be\u4ee5\u9002\u5e94\u975e\u7ebf\u6027\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u8f93\u5165\u7a7a\u95f4\u4e2d\u76f8\u540c\u9884\u6d4b\u6807\u7b7e\u7684\u8fde\u901a\u533a\u57df\u6570\u91cf\u4f5c\u4e3a\u9690\u5f0f\u504f\u5dee\u7684\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4e0e\u6cdb\u5316\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c0f\u533a\u57df\u6570\u91cf\u4e0e\u7b80\u5355\u51b3\u7b56\u8fb9\u754c\u548c\u826f\u597d\u6cdb\u5316\u6027\u80fd\u76f8\u5173\uff0c\u4e14\u8f83\u5927\u7684\u5b66\u4e60\u7387\u548c\u8f83\u5c0f\u7684\u6279\u91cf\u5927\u5c0f\u53ef\u4ee5\u8bf1\u5bfc\u5c0f\u533a\u57df\u6570\u91cf\u3002", "conclusion": "\u533a\u57df\u6570\u91cf\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u5f0f\u504f\u5dee\u5ea6\u91cf\uff0c\u80fd\u591f\u9002\u5e94\u975e\u7ebf\u6027\u6a21\u578b\uff0c\u5e76\u4e3a\u8d85\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002", "relevance": 40.0}}
{"id": "2505.11380", "pdf": "https://arxiv.org/pdf/2505.11380", "abs": "https://arxiv.org/abs/2505.11380", "authors": ["Alejandro Moreo"], "title": "On the Interconnections of Calibration, Quantification, and Classifier Accuracy Prediction under Dataset Shift", "categories": ["cs.LG"], "comment": null, "summary": "When the distribution of the data used to train a classifier differs from\nthat of the test data, i.e., under dataset shift, well-established routines for\ncalibrating the decision scores of the classifier, estimating the proportion of\npositives in a test sample, or estimating the accuracy of the classifier,\nbecome particularly challenging. This paper investigates the interconnections\namong three fundamental problems, calibration, quantification, and classifier\naccuracy prediction, under dataset shift conditions. Specifically, we prove\ntheir equivalence through mutual reduction, i.e., we show that access to an\noracle for any one of these tasks enables the resolution of the other two.\nBased on these proofs, we propose new methods for each problem based on direct\nadaptations of well-established methods borrowed from the other disciplines.\nOur results show such methods are often competitive, and sometimes even surpass\nthe performance of dedicated approaches from each discipline. The main goal of\nthis paper is to fostering cross-fertilization among these research areas,\nencouraging the development of unified approaches and promoting synergies\nacross the fields.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u96c6\u504f\u79fb\u6761\u4ef6\u4e0b\u6821\u51c6\u3001\u91cf\u5316\u548c\u5206\u7c7b\u5668\u51c6\u786e\u6027\u9884\u6d4b\u4e09\u4e2a\u57fa\u672c\u95ee\u9898\u7684\u76f8\u4e92\u8054\u7cfb\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8de8\u5b66\u79d1\u65b9\u6cd5\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u5728\u6570\u636e\u96c6\u504f\u79fb\u4e0b\u6821\u51c6\u3001\u91cf\u5316\u548c\u5206\u7c7b\u5668\u51c6\u786e\u6027\u9884\u6d4b\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u5b66\u79d1\u65b9\u6cd5\u7684\u7edf\u4e00\u548c\u534f\u540c\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u4e09\u4e2a\u95ee\u9898\u7684\u7b49\u4ef7\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u8de8\u5b66\u79d1\u65b9\u6cd5\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u4f20\u7edf\u4e13\u7528\u65b9\u6cd5\u7ade\u4e89\uff0c\u6709\u65f6\u751a\u81f3\u8d85\u8d8a\u3002", "conclusion": "\u672c\u6587\u4fc3\u8fdb\u4e86\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u878d\u5408\uff0c\u4e3a\u7edf\u4e00\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.11217", "pdf": "https://arxiv.org/pdf/2505.11217", "abs": "https://arxiv.org/abs/2505.11217", "authors": ["Yanhao Jia", "Ji Xie", "S Jivaganesh", "Hao Li", "Xu Wu", "Mengmi Zhang"], "title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": "16 pages, 14 figures", "summary": "Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we finetune a\nstate-of-the-art model using a stereo audio-image dataset generated via 3D\nsimulations. Even with limited training data, the refined model surpasses\nexisting benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5728\u591a\u6a21\u6001\u51b2\u7a81\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4eba\u7c7b\u66f4\u4f9d\u8d56\u542c\u89c9\u4fe1\u606f\uff0c\u800cAI\u6a21\u578b\u5219\u503e\u5411\u4e8e\u89c6\u89c9\u8f93\u5165\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff0c\u6027\u80fd\u63d0\u5347\u5e76\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001AI\u5728\u6574\u5408\u89c6\u89c9\u548c\u97f3\u9891\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u8de8\u6a21\u6001\u51b2\u7a81\u4e2d\u7684\u8868\u73b0\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793aAI\u5982\u4f55\u5904\u7406\u8fd9\u4e9b\u51b2\u7a81\u53ca\u5176\u6a21\u6001\u504f\u597d\u3002", "method": "\u8bc4\u4f30\u4e86\u9886\u5148\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5fc3\u7406\u7269\u7406\u5b66\u5b9e\u9a8c\u5c06\u5176\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4\u3002\u968f\u540e\uff0c\u4f7f\u75283D\u6a21\u62df\u751f\u6210\u7684\u7acb\u4f53\u97f3\u9891\u56fe\u50cf\u6570\u636e\u96c6\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u4eba\u7c7b\u5728\u51b2\u7a81\u6216\u7f3a\u5931\u89c6\u89c9\u7ebf\u7d22\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u800cAI\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\u3002\u5fae\u8c03\u540e\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5de6\u53f3\u5b9a\u4f4d\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u611f\u5b98\u8f93\u5165\u8d28\u91cf\u548c\u7cfb\u7edf\u67b6\u6784\u5bf9\u591a\u6a21\u6001\u8868\u793a\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u6539\u8fdbAI\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "relevance": 60.0}}
{"id": "2505.11390", "pdf": "https://arxiv.org/pdf/2505.11390", "abs": "https://arxiv.org/abs/2505.11390", "authors": ["Millend Roy", "Vladimir Pyltsov", "Yinbo Hu"], "title": "IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting", "categories": ["cs.LG", "cs.SY", "econ.EM", "eess.SY"], "comment": null, "summary": "Accurate electricity load forecasting is essential for grid stability,\nresource optimization, and renewable energy integration. While\ntransformer-based deep learning models like TimeGPT have gained traction in\ntime-series forecasting, their effectiveness in long-term electricity load\nprediction remains uncertain. This study evaluates forecasting models ranging\nfrom classical regression techniques to advanced deep learning architectures\nusing data from the ESD 2025 competition. The dataset includes two years of\nhistorical electricity load data, alongside temperature and global horizontal\nirradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.\nSince actual test set load values remain undisclosed, leveraging predicted\nvalues would accumulate errors, making this a long-term forecasting challenge.\nWe employ (i) Principal Component Analysis (PCA) for dimensionality reduction\nand (ii) frame the task as a regression problem, using temperature and GHI as\ncovariates to predict load for each hour, (iii) ultimately stacking 24 models\nto generate yearly forecasts.\n  Our results reveal that deep learning models, including TimeGPT, fail to\nconsistently outperform simpler statistical and machine learning approaches due\nto the limited availability of training data and exogenous variables. In\ncontrast, XGBoost, with minimal feature engineering, delivers the lowest error\nrates across all test cases while maintaining computational efficiency. This\nhighlights the limitations of deep learning in long-term electricity\nforecasting and reinforces the importance of model selection based on dataset\ncharacteristics rather than complexity. Our study provides insights into\npractical forecasting applications and contributes to the ongoing discussion on\nthe trade-offs between traditional and modern forecasting methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4ece\u7ecf\u5178\u56de\u5f52\u5230\u6df1\u5ea6\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5728\u957f\u671f\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0XGBoost\u4f18\u4e8e\u590d\u6742\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982TimeGPT\u3002", "motivation": "\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u5bf9\u7535\u7f51\u7a33\u5b9a\u548c\u8d44\u6e90\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528PCA\u964d\u7ef4\uff0c\u5c06\u4efb\u52a1\u89c6\u4e3a\u56de\u5f52\u95ee\u9898\uff0c\u5229\u7528\u6e29\u5ea6\u548cGHI\u4f5c\u4e3a\u534f\u53d8\u91cf\uff0c\u5806\u53e024\u4e2a\u6a21\u578b\u751f\u6210\u5e74\u5ea6\u9884\u6d4b\u3002", "result": "XGBoost\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u672a\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u9009\u62e9\u5e94\u57fa\u4e8e\u6570\u636e\u7279\u6027\u800c\u975e\u590d\u6742\u5ea6\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u957f\u671f\u7535\u529b\u9884\u6d4b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11278", "pdf": "https://arxiv.org/pdf/2505.11278", "abs": "https://arxiv.org/abs/2505.11278", "authors": ["Fabian Falck", "Teodora Pandeva", "Kiarash Zahirnia", "Rachel Lawrence", "Richard Turner", "Edward Meeds", "Javier Zazo", "Sushrut Karmalkar"], "title": "A Fourier Space Perspective on Diffusion Models", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "Diffusion models are state-of-the-art generative models on data modalities\nsuch as images, audio, proteins and materials. These modalities share the\nproperty of exponentially decaying variance and magnitude in the Fourier\ndomain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)\nforward process of additive white noise, this property results in\nhigh-frequency components being corrupted faster and earlier in terms of their\nSignal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then\ngenerates low-frequency information before high-frequency details. In this\nwork, we study the inductive bias of the forward process of diffusion models in\nFourier space. We theoretically analyse and empirically demonstrate that the\nfaster noising of high-frequency components in DDPM results in violations of\nthe normality assumption in the reverse process. Our experiments show that this\nleads to degraded generation quality of high-frequency components. We then\nstudy an alternate forward process in Fourier space which corrupts all\nfrequencies at the same rate, removing the typical frequency hierarchy during\ngeneration, and demonstrate marked performance improvements on datasets where\nhigh frequencies are primary, while performing on par with DDPM on standard\nimaging benchmarks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u524d\u5411\u8fc7\u7a0b\u4e2d\u5085\u91cc\u53f6\u7a7a\u95f4\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u53d1\u73b0\u9ad8\u9891\u5206\u91cf\u5728DDPM\u4e2d\u66f4\u5feb\u88ab\u566a\u58f0\u6c61\u67d3\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u524d\u5411\u8fc7\u7a0b\uff0c\u5747\u5300\u566a\u58f0\u6240\u6709\u9891\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u9891\u4e3b\u5bfc\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u3001\u97f3\u9891\u7b49\u6a21\u6001\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6807\u51c6DDPM\u524d\u5411\u8fc7\u7a0b\u5bfc\u81f4\u9ad8\u9891\u5206\u91cf\u66f4\u5feb\u88ab\u566a\u58f0\u6c61\u67d3\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002\u7814\u7a76\u65e8\u5728\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DDPM\u524d\u5411\u8fc7\u7a0b\u4e2d\u9ad8\u9891\u5206\u91cf\u66f4\u5feb\u566a\u58f0\u5316\u7684\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5085\u91cc\u53f6\u7a7a\u95f4\u4e2d\u5747\u5300\u566a\u58f0\u6240\u6709\u9891\u7387\u7684\u66ff\u4ee3\u524d\u5411\u8fc7\u7a0b\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u9ad8\u9891\u4e3b\u5bfc\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8eDDPM\uff0c\u540c\u65f6\u5728\u6807\u51c6\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eDDPM\u76f8\u5f53\u3002", "conclusion": "\u5747\u5300\u566a\u58f0\u6240\u6709\u9891\u7387\u7684\u524d\u5411\u8fc7\u7a0b\u53ef\u4ee5\u6539\u5584\u9ad8\u9891\u5206\u91cf\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.11396", "pdf": "https://arxiv.org/pdf/2505.11396", "abs": "https://arxiv.org/abs/2505.11396", "authors": ["Dazhuo Qiu", "Jinwen Chen", "Arijit Khan", "Yan Zhao", "Francesco Bonchi"], "title": "Finding Counterfactual Evidences for Node Classification", "categories": ["cs.LG", "cs.DB"], "comment": "Accepted by KDD 2025", "summary": "Counterfactual learning is emerging as an important paradigm, rooted in\ncausality, which promises to alleviate common issues of graph neural networks\n(GNNs), such as fairness and interpretability. However, as in many real-world\napplication domains where conducting randomized controlled trials is\nimpractical, one has to rely on available observational (factual) data to\ndetect counterfactuals. In this paper, we introduce and tackle the problem of\nsearching for counterfactual evidences for the GNN-based node classification\ntask. A counterfactual evidence is a pair of nodes such that, regardless they\nexhibit great similarity both in the features and in their neighborhood\nsubgraph structures, they are classified differently by the GNN. We develop\neffective and efficient search algorithms and a novel indexing solution that\nleverages both node features and structural information to identify\ncounterfactual evidences, and generalizes beyond any specific GNN. Through\nvarious downstream applications, we demonstrate the potential of counterfactual\nevidences to enhance fairness and accuracy of GNNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u5b66\u4e60\u7684GNN\u8282\u70b9\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u641c\u7d22\u53cd\u4e8b\u5b9e\u8bc1\u636e\u63d0\u5347\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3GNN\u5728\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u8fdb\u884c\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u641c\u7d22\u7b97\u6cd5\u548c\u7d22\u5f15\u65b9\u6848\uff0c\u5229\u7528\u8282\u70b9\u7279\u5f81\u548c\u7ed3\u6784\u4fe1\u606f\u8bc6\u522b\u53cd\u4e8b\u5b9e\u8bc1\u636e\u3002", "result": "\u53cd\u4e8b\u5b9e\u8bc1\u636e\u80fd\u591f\u6709\u6548\u63d0\u5347GNN\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u5b66\u4e60\u4e3aGNN\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c24\u5176\u5728\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u3002", "relevance": 40.0}}
{"id": "2505.11394", "pdf": "https://arxiv.org/pdf/2505.11394", "abs": "https://arxiv.org/abs/2505.11394", "authors": ["Alexander Oberstrass", "Esteban Vaca", "Eric Upschulte", "Meiqi Niu", "Nicola Palomero-Gallagher", "David Graessel", "Christian Schiffer", "Markus Axer", "Katrin Amunts", "Timo Dickscheid"], "title": "From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Comprehensive assessment of the various aspects of the brain's microstructure\nrequires the use of complementary imaging techniques. This includes measuring\nthe spatial distribution of cell bodies (cytoarchitecture) and nerve fibers\n(myeloarchitecture). The gold standard for cytoarchitectonic analysis is light\nmicroscopic imaging of cell-body stained tissue sections. To reveal the 3D\norientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been\nintroduced as a reliable technique providing a resolution in the micrometer\nrange while allowing processing of series of complete brain sections. 3D-PLI\nacquisition is label-free and allows subsequent staining of sections after\nmeasurement. By post-staining for cell bodies, a direct link between fiber- and\ncytoarchitecture can potentially be established within the same section.\nHowever, inevitable distortions introduced during the staining process make a\nnonlinear and cross-modal registration necessary in order to study the detailed\nrelationships between cells and fibers in the images. In addition, the\ncomplexity of processing histological sections for post-staining only allows\nfor a limited number of samples. In this work, we take advantage of deep\nlearning methods for image-to-image translation to generate a virtual staining\nof 3D-PLI that is spatially aligned at the cellular level. In a supervised\nsetting, we build on a unique dataset of brain sections, to which Cresyl violet\nstaining has been applied after 3D-PLI measurement. To ensure high\ncorrespondence between both modalities, we address the misalignment of training\ndata using Fourier-based registration methods. In this way, registration can be\nefficiently calculated during training for local image patches of target and\npredicted staining. We demonstrate that the proposed method enables prediction\nof a Cresyl violet staining from 3D-PLI, matching individual cell instances.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece3D-PLI\u6570\u636e\u751f\u6210\u865a\u62df\u7684\u7ec6\u80de\u67d3\u8272\u56fe\u50cf\uff0c\u4ee5\u7814\u7a76\u5927\u8111\u7684\u5fae\u89c2\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76\u5927\u8111\u7684\u5fae\u89c2\u7ed3\u6784\u9700\u8981\u7ed3\u5408\u591a\u79cd\u6210\u50cf\u6280\u672f\uff0c\u4f46\u4f20\u7edf\u67d3\u8272\u65b9\u6cd5\u5b58\u5728\u53d8\u5f62\u548c\u6837\u672c\u9650\u5236\u95ee\u9898\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u57fa\u7684\u914d\u51c6\u65b9\u6cd5\u5904\u7406\u8bad\u7ec3\u6570\u636e\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u4ece3D-PLI\u9884\u6d4b\u51fa\u4e0e\u771f\u5b9e\u67d3\u8272\u5339\u914d\u7684\u7ec6\u80de\u5b9e\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7814\u7a76\u5927\u8111\u7ea4\u7ef4\u548c\u7ec6\u80de\u7ed3\u6784\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.11065", "pdf": "https://arxiv.org/pdf/2505.11065", "abs": "https://arxiv.org/abs/2505.11065", "authors": ["Changlun Li", "Yao Shi", "Chen Wang", "Qiqi Duan", "Runke Ruan", "Weijie Huang", "Haonan Long", "Lijun Huang", "Yuyu Luo", "Nan Tang"], "title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking", "categories": ["cs.CE", "cs.AI", "cs.MA"], "comment": "21 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nfinancial tasks, including financial report summarization, earnings call\ntranscript analysis, and asset classification. However, their real-world\neffectiveness in managing complex fund investment remains inadequately\nassessed. A fundamental limitation of existing benchmarks for evaluating\nLLM-driven trading strategies is their reliance on historical back-testing,\ninadvertently enabling LLMs to \"time travel\"-leveraging future information\nembedded in their training corpora, thus resulting in possible information\nleakage and overly optimistic performance estimates. To address this issue, we\nintroduce DeepFund, a live fund benchmark tool designed to rigorously evaluate\nLLM in real-time market conditions. Utilizing a multi-agent architecture,\nDeepFund connects directly with real-time stock market data-specifically data\npublished after each model pretraining cutoff-to ensure fair and leakage-free\nevaluations. Empirical tests on nine flagship LLMs from leading global\ninstitutions across multiple investment dimensions-including ticker-level\nanalysis, investment decision-making, portfolio management, and risk\ncontrol-reveal significant practical challenges. Notably, even cutting-edge\nmodels such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses\nwithin DeepFund real-time evaluation environment, underscoring the present\nlimitations of LLMs for active fund management. Our code is available at\nhttps://github.com/HKUSTDial/DeepFund.", "AI": {"tldr": "DeepFund\u662f\u4e00\u4e2a\u5b9e\u65f6\u57fa\u91d1\u57fa\u51c6\u5de5\u5177\uff0c\u7528\u4e8e\u5728\u5b9e\u65f6\u5e02\u573a\u6761\u4ef6\u4e0b\u4e25\u683c\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u4ea4\u6613\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5386\u53f2\u56de\u6d4b\u4e2d\u7684\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u57fa\u51c6\u4f9d\u8d56\u5386\u53f2\u56de\u6d4b\uff0c\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u6cc4\u6f0f\u548c\u6027\u80fd\u9ad8\u4f30\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620LLM\u5728\u5b9e\u65f6\u5e02\u573a\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u76f4\u63a5\u8fde\u63a5\u5b9e\u65f6\u80a1\u7968\u5e02\u573a\u6570\u636e\uff08\u786e\u4fdd\u6570\u636e\u5728\u6a21\u578b\u9884\u8bad\u7ec3\u622a\u6b62\u540e\u53d1\u5e03\uff09\uff0c\u5bf9\u4e5d\u79cd\u65d7\u8230LLM\u8fdb\u884c\u591a\u7ef4\u5ea6\u6295\u8d44\u6d4b\u8bd5\u3002", "result": "\u5373\u4f7f\u5728DeepSeek-V3\u548cClaude-3.7-Sonnet\u7b49\u524d\u6cbf\u6a21\u578b\u4e2d\uff0cDeepFund\u5b9e\u65f6\u8bc4\u4f30\u73af\u5883\u4e0b\u4ecd\u51fa\u73b0\u51c0\u4ea4\u6613\u4e8f\u635f\uff0c\u63ed\u793a\u4e86LLM\u5728\u4e3b\u52a8\u57fa\u91d1\u7ba1\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LLM\u5728\u5b9e\u65f6\u57fa\u91d1\u7ba1\u7406\u4e2d\u4ecd\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "relevance": 70.0}}
{"id": "2505.11411", "pdf": "https://arxiv.org/pdf/2505.11411", "abs": "https://arxiv.org/abs/2505.11411", "authors": ["Xiaotian Zhang", "Yue Shang", "Entao Yang", "Ge Zhang"], "title": "Is Grokking a Computational Glass Relaxation?", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Understanding neural network's (NN) generalizability remains a central\nquestion in deep learning research. The special phenomenon of grokking, where\nNNs abruptly generalize long after the training performance reaches a\nnear-perfect level, offers a unique window to investigate the underlying\nmechanisms of NNs' generalizability. Here we propose an interpretation for\ngrokking by framing it as a computational glass relaxation: viewing NNs as a\nphysical system where parameters are the degrees of freedom and train loss is\nthe system energy, we find memorization process resembles a rapid cooling of\nliquid into non-equilibrium glassy state at low temperature and the later\ngeneralization is like a slow relaxation towards a more stable configuration.\nThis mapping enables us to sample NNs' Boltzmann entropy (states of density)\nlandscape as a function of training loss and test accuracy. Our experiments in\ntransformers on arithmetic tasks suggests that there is NO entropy barrier in\nthe memorization-to-generalization transition of grokking, challenging previous\ntheory that defines grokking as a first-order phase transition. We identify a\nhigh-entropy advantage under grokking, an extension of prior work linking\nentropy to generalizability but much more significant. Inspired by grokking's\nfar-from-equilibrium nature, we develop a toy optimizer WanD based on\nWang-landau molecular dynamics, which can eliminate grokking without any\nconstraints and find high-norm generalizing solutions. This provides\nstrictly-defined counterexamples to theory attributing grokking solely to\nweight norm evolution towards the Goldilocks zone and also suggests new\npotential ways for optimizer design.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5c06\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684'grokking'\u73b0\u8c61\u7c7b\u6bd4\u4e3a\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u73bb\u7483\u5f1b\u8c6b\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u4f18\u5316\u5668WanD\u6765\u6d88\u9664grokking\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u7684\u673a\u5236\uff0c\u7279\u522b\u662f'grokking'\u73b0\u8c61\uff0c\u5373\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u6027\u80fd\u63a5\u8fd1\u5b8c\u7f8e\u540e\u7a81\u7136\u6cdb\u5316\u7684\u73b0\u8c61\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u89c6\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u81ea\u7531\u5ea6\uff0c\u8bad\u7ec3\u635f\u5931\u4e3a\u7cfb\u7edf\u80fd\u91cf\uff0c\u901a\u8fc7\u91c7\u6837Boltzmann\u71b5\u666f\u89c2\u5206\u6790\u6cdb\u5316\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u5728Transformer\u4e0a\u8fdb\u884c\u7b97\u672f\u4efb\u52a1\u3002", "result": "\u53d1\u73b0'grokking'\u8fc7\u7a0b\u4e2d\u4e0d\u5b58\u5728\u71b5\u58c1\u5792\uff0c\u6311\u6218\u4e86\u4e4b\u524d\u5c06\u5176\u89c6\u4e3a\u4e00\u7ea7\u76f8\u53d8\u7684\u7406\u8bba\u3002\u5f00\u53d1\u4e86\u4f18\u5316\u5668WanD\uff0c\u53ef\u6d88\u9664grokking\u73b0\u8c61\u3002", "conclusion": "'grokking'\u7684\u8fdc\u975e\u5e73\u8861\u6027\u8d28\u4e3a\u4f18\u5316\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u540c\u65f6\u5426\u5b9a\u4e86\u4ec5\u5f52\u56e0\u4e8e\u6743\u91cd\u8303\u6570\u6f14\u5316\u7684\u7406\u8bba\u3002", "relevance": 75.0}}
{"id": "2505.11467", "pdf": "https://arxiv.org/pdf/2505.11467", "abs": "https://arxiv.org/abs/2505.11467", "authors": ["Abhishek Kashyap", "Henrik Andreasson", "Todor Stoyanov"], "title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages", "summary": "Vision based robot manipulation uses cameras to capture one or more images of\na scene containing the objects to be manipulated. Taking multiple images can\nhelp if any object is occluded from one viewpoint but more visible from another\nviewpoint. However, the camera has to be moved to a sequence of suitable\npositions for capturing multiple images, which requires time and may not always\nbe possible, due to reachability constraints. So while additional images can\nproduce more accurate grasp poses due to the extra information available, the\ntime-cost goes up with the number of additional views sampled. Scene\nrepresentations like Gaussian Splatting are capable of rendering accurate\nphotorealistic virtual images from user-specified novel viewpoints. In this\nwork, we show initial results which indicate that novel view synthesis can\nprovide additional context in generating grasp poses. Our experiments on the\nGraspnet-1billion dataset show that novel views contributed force-closure\ngrasps in addition to the force-closure grasps obtained from sparsely sampled\nreal views while also improving grasp coverage. In the future we hope this work\ncan be extended to improve grasp extraction from radiance fields constructed\nwith a single input image, using for example diffusion models or generalizable\nradiance fields.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u6293\u53d6\u4e2d\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splatting\uff09\u5408\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\u4ee5\u63d0\u5347\u6293\u53d6\u51c6\u786e\u6027\u7684\u521d\u6b65\u6210\u679c\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\uff0c\u591a\u89c6\u89d2\u56fe\u50cf\u80fd\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\uff0c\u4f46\u79fb\u52a8\u6444\u50cf\u5934\u8017\u65f6\u4e14\u53ef\u80fd\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5408\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\u6765\u8865\u5145\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5408\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\uff0c\u5e76\u5728Graspnet-1billion\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u5bf9\u6293\u53d6\u59ff\u6001\u751f\u6210\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u7684\u65b0\u89c6\u89d2\u80fd\u63d0\u4f9b\u989d\u5916\u7684\u529b\u95ed\u5408\u6293\u53d6\u59ff\u6001\uff0c\u5e76\u63d0\u5347\u6293\u53d6\u8986\u76d6\u7387\u3002", "conclusion": "\u672a\u6765\u53ef\u63a2\u7d22\u901a\u8fc7\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u6784\u5efa\u8f90\u5c04\u573a\uff08\u5982\u6269\u6563\u6a21\u578b\u6216\u901a\u7528\u8f90\u5c04\u573a\uff09\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb\u6293\u53d6\u63d0\u53d6\u3002", "relevance": 40.0}}
{"id": "2505.11412", "pdf": "https://arxiv.org/pdf/2505.11412", "abs": "https://arxiv.org/abs/2505.11412", "authors": ["Ciaran Bench", "Vivek Desai", "Mohammad Moulaeifard", "Nils Strodthoff", "Philip Aston", "Andrew Thompson"], "title": "Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Photoplethysmography (PPG) signals encode information about relative changes\nin blood volume that can be used to assess various aspects of cardiac health\nnon-invasively, e.g.\\ to detect atrial fibrillation (AF) or predict blood\npressure (BP). Deep networks are well-equipped to handle the large quantities\nof data acquired from wearable measurement devices. However, they lack\ninterpretability and are prone to overfitting, leaving considerable risk for\npoor performance on unseen data and misdiagnosis. Here, we describe the use of\ntwo scalable uncertainty quantification techniques: Monte Carlo Dropout and the\nrecently proposed Improved Variational Online Newton. These techniques are used\nto assess the trustworthiness of models trained to perform AF classification\nand BP regression from raw PPG time series. We find that the choice of\nhyperparameters has a considerable effect on the predictive performance of the\nmodels and on the quality and composition of predicted uncertainties. E.g. the\nstochasticity of the model parameter sampling determines the proportion of the\ntotal uncertainty that is aleatoric, and has varying effects on predictive\nperformance and calibration quality dependent on the chosen uncertainty\nquantification technique and the chosen expression of uncertainty. We find\nsignificant discrepancy in the quality of uncertainties over the predicted\nclasses, emphasising the need for a thorough evaluation protocol that assesses\nlocal and adaptive calibration. This work suggests that the choice of\nhyperparameters must be carefully tuned to balance predictive performance and\ncalibration quality, and that the optimal parameterisation may vary depending\non the chosen expression of uncertainty.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u8499\u7279\u5361\u6d1bDropout\u548c\u6539\u8fdb\u7684\u53d8\u5206\u5728\u7ebf\u725b\u987f\u65b9\u6cd5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u63d0\u9ad8PPG\u4fe1\u53f7\u4e2d\u5fc3\u810f\u5065\u5eb7\u8bc4\u4f30\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728PPG\u4fe1\u53f7\u5206\u6790\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u8499\u7279\u5361\u6d1bDropout\u548c\u6539\u8fdb\u7684\u53d8\u5206\u5728\u7ebf\u725b\u987f\u65b9\u6cd5\uff0c\u8bc4\u4f30AF\u5206\u7c7b\u548cBP\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8d85\u53c2\u6570\u9009\u62e9\u5bf9\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u65b9\u5f0f\u9700\u8981\u4e0d\u540c\u7684\u53c2\u6570\u8c03\u4f18\u3002", "conclusion": "\u9700\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u6821\u51c6\u8d28\u91cf\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\u5e94\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u65b9\u5f0f\u7075\u6d3b\u8c03\u6574\u3002", "relevance": 40.0}}
{"id": "2505.11415", "pdf": "https://arxiv.org/pdf/2505.11415", "abs": "https://arxiv.org/abs/2505.11415", "authors": ["Yinsicheng Jiang", "Yao Fu", "Yeqi Huang", "Ping Nie", "Zhan Lu", "Leyang Xue", "Congjie He", "Man-Kit Sit", "Jilong Xue", "Li Dong", "Ziming Miao", "Dayou Du", "Tairan Xu", "Kai Zou", "Edoardo Ponti", "Luo Mai"], "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems", "categories": ["cs.LG", "cs.DC"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.07067", "summary": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MoE-CAP\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7cfb\u7edf\u5728\u6210\u672c\u3001\u51c6\u786e\u6027\u548c\u6027\u80fd\uff08CAP\uff09\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u7a00\u758fMoE\u67b6\u6784\u5728\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\u6548\u7387\u9ad8\uff0c\u4f46\u4f9d\u8d56\u4e8e\u5f02\u6784\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u5bfc\u81f4CAP\u6743\u8861\u95ee\u9898\u3002\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u51c6\u786e\u6355\u6349\u8fd9\u4e9b\u6743\u8861\uff0c\u5f71\u54cd\u5b9e\u9645\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u63d0\u51faMoE-CAP\u57fa\u51c6\u548cCAP\u96f7\u8fbe\u56fe\uff0c\u7528\u4e8e\u53ef\u89c6\u5316CAP\u6743\u8861\uff1b\u5f15\u5165\u7a00\u758f\u611f\u77e5\u6027\u80fd\u6307\u6807\uff08S-MBU\u548cS-MFU\uff09\u4ee5\u51c6\u786e\u8bc4\u4f30MoE\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5f53\u524d\u786c\u4ef6\u6761\u4ef6\u4e0b\u96be\u4ee5\u5728CAP\u4e09\u8005\u95f4\u53d6\u5f97\u6700\u4f18\u5e73\u8861\uff0c\u901a\u5e38\u53ea\u80fd\u4f18\u5316\u5176\u4e2d\u4e24\u9879\u3002", "conclusion": "MoE-CAP\u57fa\u51c6\u548c\u65b0\u6307\u6807\u4e3aMoE\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.11427", "pdf": "https://arxiv.org/pdf/2505.11427", "abs": "https://arxiv.org/abs/2505.11427", "authors": ["Adrian Robert Minut", "Tommaso Mencattini", "Andrea Santilli", "Donato Crisostomi", "Emanuele Rodol\u00e0"], "title": "Mergenetic: a Simple Evolutionary Model Merging Library", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "Link: https://github.com/tommasomncttn/mergenetic", "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.", "AI": {"tldr": "Mergenetic\u662f\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u5408\u5e76\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u4f4e\u6210\u672c\u5b9e\u9a8c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6a21\u578b\u5408\u5e76\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u7ed3\u5408\u73b0\u6709\u6a21\u578b\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u652f\u6301\u7075\u6d3b\u5b9e\u9a8c\u7684\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86Mergenetic\u5e93\uff0c\u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\u548c\u8f7b\u91cf\u7ea7\u9002\u5e94\u5ea6\u4f30\u8ba1\u5668\uff0c\u652f\u6301\u591a\u79cd\u5408\u5e76\u65b9\u6cd5\u3002", "result": "Mergenetic\u5728\u591a\u79cd\u4efb\u52a1\u548c\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4ec5\u9700\u666e\u901a\u786c\u4ef6\u3002", "conclusion": "Mergenetic\u4e3a\u8bed\u8a00\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5b9e\u9a8c\u5de5\u5177\u3002", "relevance": 75.0}}
{"id": "2505.11432", "pdf": "https://arxiv.org/pdf/2505.11432", "abs": "https://arxiv.org/abs/2505.11432", "authors": ["Chao Jin", "Ziheng Jiang", "Zhihao Bai", "Zheng Zhong", "Juncai Liu", "Xiang Li", "Ningxin Zheng", "Xi Wang", "Cong Xie", "Wen Heng", "Yiyuan Ma", "Wenlei Bao", "Size Zheng", "Yanghua Peng", "Haibin Lin", "Xuanzhe Liu", "Xin Jin", "Xin Liu"], "title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We present MegaScale-MoE, a production system tailored for the efficient\ntraining of large-scale mixture-of-experts (MoE) models. MoE emerges as a\npromising architecture to scale large language models (LLMs) to unprecedented\nsizes, thereby enhancing model performance. However, existing MoE training\nsystems experience a degradation in training efficiency, exacerbated by the\nescalating scale of MoE models and the continuous evolution of hardware.\n  Recognizing the pivotal role of efficient communication in enhancing MoE\ntraining, MegaScale-MoE customizes communication-efficient parallelism\nstrategies for attention and FFNs in each MoE layer and adopts a holistic\napproach to overlap communication with computation at both inter- and\nintra-operator levels. Additionally, MegaScale-MoE applies communication\ncompression with adjusted communication patterns to lower precision, further\nimproving training efficiency. When training a 352B MoE model on 1,440 NVIDIA\nHopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,\nimproving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our\noperational experience in accelerating MoE training and hope that by offering\nour insights in system design, this work will motivate future research in MoE\nsystems.", "AI": {"tldr": "MegaScale-MoE\u662f\u4e00\u4e2a\u4e13\u4e3a\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u8bbe\u8ba1\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u548c\u8ba1\u7b97\u91cd\u53e0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u968f\u7740MoE\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u548c\u786c\u4ef6\u7684\u6f14\u8fdb\uff0c\u73b0\u6709\u8bad\u7ec3\u7cfb\u7edf\u7684\u6548\u7387\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002", "method": "MegaScale-MoE\u5b9a\u5236\u4e86\u901a\u4fe1\u9ad8\u6548\u7684\u5e76\u884c\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u6ce8\u610f\u529b\u5c42\u548c\u524d\u9988\u7f51\u7edc\u5c42\u7684\u901a\u4fe1\uff0c\u5e76\u91c7\u7528\u901a\u4fe1\u538b\u7f29\u548c\u8ba1\u7b97\u91cd\u53e0\u6280\u672f\u3002", "result": "\u57281,440\u4e2aNVIDIA Hopper GPU\u4e0a\u8bad\u7ec3352B MoE\u6a21\u578b\u65f6\uff0c\u541e\u5410\u91cf\u8fbe\u52301.41M tokens/s\uff0c\u6548\u7387\u6bd4Megatron-LM\u63d0\u53471.88\u500d\u3002", "conclusion": "MegaScale-MoE\u4e3aMoE\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bbe\u8ba1\u7ecf\u9a8c\uff0c\u6709\u671b\u63a8\u52a8\u672a\u6765MoE\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "relevance": 85.0}}
{"id": "2505.11444", "pdf": "https://arxiv.org/pdf/2505.11444", "abs": "https://arxiv.org/abs/2505.11444", "authors": ["Xinran Song", "Tianyu Chen", "Mingyuan Zhou"], "title": "A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation", "categories": ["cs.LG", "stat.AP", "stat.ME", "stat.ML"], "comment": null, "summary": "Estimating individualized treatment effects from observational data is a\ncentral challenge in causal inference, largely due to covariate imbalance and\nconfounding bias from non-randomized treatment assignment. While inverse\nprobability weighting (IPW) is a well-established solution to this problem, its\nintegration into modern deep learning frameworks remains limited. In this work,\nwe propose Importance-Weighted Diffusion Distillation (IWDD), a novel\ngenerative framework that combines the pretraining of diffusion models with\nimportance-weighted score distillation to enable accurate and fast causal\nestimation-including potential outcome prediction and treatment effect\nestimation. We demonstrate how IPW can be naturally incorporated into the\ndistillation of pretrained diffusion models, and further introduce a\nrandomization-based adjustment that eliminates the need to compute IPW\nexplicitly-thereby simplifying computation and, more importantly, provably\nreducing the variance of gradient estimates. Empirical results show that IWDD\nachieves state-of-the-art out-of-sample prediction performance, with the\nhighest win rates compared to other baselines, significantly improving causal\nestimation and supporting the development of individualized treatment\nstrategies. We will release our PyTorch code for reproducibility and future\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIWDD\u7684\u65b0\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u91cd\u8981\u6027\u52a0\u6743\u5206\u6570\u84b8\u998f\uff0c\u7528\u4e8e\u51c6\u786e\u5feb\u901f\u7684\u56e0\u679c\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u89c2\u6d4b\u6570\u636e\u4e2d\u4e2a\u4f53\u5316\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u534f\u53d8\u91cf\u4e0d\u5e73\u8861\u548c\u975e\u968f\u673a\u6cbb\u7597\u5206\u914d\u5bfc\u81f4\u7684\u6df7\u6dc6\u504f\u5dee\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u91cd\u8981\u6027\u52a0\u6743\u5206\u6570\u84b8\u998f\uff0c\u5f15\u5165\u968f\u673a\u5316\u8c03\u6574\u4ee5\u7b80\u5316\u8ba1\u7b97\u5e76\u51cf\u5c11\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u3002", "result": "IWDD\u5728\u6837\u672c\u5916\u9884\u6d4b\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u663e\u8457\u6539\u5584\u56e0\u679c\u4f30\u8ba1\u5e76\u652f\u6301\u4e2a\u4f53\u5316\u6cbb\u7597\u7b56\u7565\u5f00\u53d1\u3002", "conclusion": "IWDD\u4e3a\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7b80\u5316\u4e86\u8ba1\u7b97\u8fc7\u7a0b\u3002", "relevance": 40.0}}
{"id": "2505.11461", "pdf": "https://arxiv.org/pdf/2505.11461", "abs": "https://arxiv.org/abs/2505.11461", "authors": ["Wesley A Suttle", "Vipul K Sharma", "Brian M Sadler"], "title": "Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks", "categories": ["cs.LG"], "comment": "7 pages, 1 figure", "summary": "Classic multi-agent reinforcement learning (MARL) methods require that agents\nenjoy global state observability, preventing development of decentralized\nalgorithms and limiting scalability. Recent work has shown that, under\nassumptions on decaying inter-agent influence, global observability can be\nreplaced by local neighborhood observability at each agent, enabling\ndecentralization and scalability. Real-world applications enjoying such decay\nproperties remain underexplored, however, despite the fact that signal power\ndecay, or signal attenuation, due to path loss is an intrinsic feature of many\nproblems in wireless communications and radar networks. In this paper, we show\nthat signal attenuation enables decentralization in MARL by considering the\nillustrative special case of performing power allocation for target detection\nin a radar network. To achieve this, we propose two new constrained multi-agent\nMarkov decision process formulations of this power allocation problem, derive\nlocal neighborhood approximations for global value function and gradient\nestimates and establish corresponding error bounds, and develop decentralized\nsaddle point policy gradient algorithms for solving the proposed problems. Our\napproach, though oriented towards the specific radar network problem we\nconsider, provides a useful model for future extensions to additional problems\nin wireless communications and radar networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u89c2\u6d4b\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u7ebf\u901a\u4fe1\u548c\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u4fe1\u53f7\u8870\u51cf\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edfMARL\u65b9\u6cd5\u9700\u8981\u5168\u5c40\u72b6\u6001\u89c2\u6d4b\uff0c\u9650\u5236\u4e86\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u5229\u7528\u4fe1\u53f7\u8870\u51cf\u7279\u6027\uff0c\u63a2\u7d22\u4e86\u5728\u96f7\u8fbe\u7f51\u7edc\u529f\u7387\u5206\u914d\u95ee\u9898\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316MARL\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u5c40\u90e8\u90bb\u57df\u8fd1\u4f3c\u5168\u5c40\u4ef7\u503c\u51fd\u6570\u548c\u68af\u5ea6\u4f30\u8ba1\uff0c\u5e76\u5f00\u53d1\u4e86\u53bb\u4e2d\u5fc3\u5316\u978d\u70b9\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u96f7\u8fbe\u7f51\u7edc\u529f\u7387\u5206\u914d\u95ee\u9898\u7684\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bef\u5dee\u754c\u9650\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u7ebf\u901a\u4fe1\u548c\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u5176\u4ed6\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u53bb\u4e2d\u5fc3\u5316MARL\u6846\u67b6\u3002", "relevance": 50.0}}
{"id": "2505.11108", "pdf": "https://arxiv.org/pdf/2505.11108", "abs": "https://arxiv.org/abs/2505.11108", "authors": ["Kartik Ramachandruni", "Sonia Chernova"], "title": "PARSEC: Preference Adaptation for Robotic Object Rearrangement from Scene Context", "categories": ["cs.RO", "cs.AI"], "comment": "Under review at ROMAN 2025", "summary": "Object rearrangement is a key task for household robots requiring\npersonalization without explicit instructions, meaningful object placement in\nenvironments occupied with objects, and generalization to unseen objects and\nnew environments. To facilitate research addressing these challenges, we\nintroduce PARSEC, an object rearrangement benchmark for learning user\norganizational preferences from observed scene context to place objects in a\npartially arranged environment. PARSEC is built upon a novel dataset of 110K\nrearrangement examples crowdsourced from 72 users, featuring 93 object\ncategories and 15 environments. We also propose ContextSortLM, an LLM-based\nrearrangement model that places objects in partially arranged environments by\nadapting to user preferences from prior and current scene context while\naccounting for multiple valid placements. We evaluate ContextSortLM and\nexisting personalized rearrangement approaches on the PARSEC benchmark and\ncomplement these findings with a crowdsourced evaluation of 108 online raters\nranking model predictions based on alignment with user preferences. Our results\nindicate that personalized rearrangement models leveraging multiple scene\ncontext sources perform better than models relying on a single context source.\nMoreover, ContextSortLM outperforms other models in placing objects to\nreplicate the target user's arrangement and ranks among the top two in all\nthree environment categories, as rated by online evaluators. Importantly, our\nevaluation highlights challenges associated with modeling environment semantics\nacross different environment categories and provides recommendations for future\nwork.", "AI": {"tldr": "PARSEC\u662f\u4e00\u4e2a\u7528\u4e8e\u5b66\u4e60\u7528\u6237\u7ec4\u7ec7\u504f\u597d\u7684\u7269\u4f53\u91cd\u6392\u57fa\u51c6\uff0c\u57fa\u4e8e11\u4e07\u6761\u4f17\u5305\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6a21\u578bContextSortLM\uff0c\u5176\u5728\u591a\u573a\u666f\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6e90\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5bb6\u5ead\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4e2a\u6027\u5316\u3001\u65e0\u660e\u786e\u6307\u4ee4\u7684\u7269\u4f53\u91cd\u6392\u95ee\u9898\uff0c\u5e76\u63a8\u5e7f\u5230\u65b0\u7269\u4f53\u548c\u65b0\u73af\u5883\u3002", "method": "\u63d0\u51faPARSEC\u57fa\u51c6\u548cContextSortLM\u6a21\u578b\uff0c\u5229\u7528\u591a\u573a\u666f\u4e0a\u4e0b\u6587\u5b66\u4e60\u7528\u6237\u504f\u597d\u3002", "result": "ContextSortLM\u5728\u591a\u6e90\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u5728\u7528\u6237\u504f\u597d\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u591a\u6e90\u4e0a\u4e0b\u6587\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u73af\u5883\u8bed\u4e49\u5efa\u6a21\u4ecd\u662f\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.11483", "pdf": "https://arxiv.org/pdf/2505.11483", "abs": "https://arxiv.org/abs/2505.11483", "authors": ["Zhaolan Huang", "Emmanuel Baccelli"], "title": "msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "AI spans from large language models to tiny models running on\nmicrocontrollers (MCUs). Extremely memory-efficient model architectures are\ndecisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,\ninference latency must remain small to fit real-time constraints. An approach\nto tackle this is patch-based fusion, which aims to optimize data flows across\nneural network layers. In this paper, we introduce msf-CNN, a novel technique\nthat efficiently finds optimal fusion settings for convolutional neural\nnetworks (CNNs) by walking through the fusion solution space represented as a\ndirected acyclic graph. Compared to previous work on CNN fusion for MCUs,\nmsf-CNN identifies a wider set of solutions. We published an implementation of\nmsf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We\nshow that msf-CNN can achieve inference using 50% less RAM compared to the\nprior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers\nadditional flexibility for system designers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3amsf-CNN\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u6709\u5411\u65e0\u73af\u56fe\u4f18\u5316\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u878d\u5408\u8bbe\u7f6e\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u9002\u7528\u4e8e\u5fae\u63a7\u5236\u5668\uff08MCU\uff09\u3002", "motivation": "\u89e3\u51b3\u5fae\u63a7\u5236\u5668\uff08MCU\uff09\u5185\u5b58\u9650\u5236\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u7684\u95ee\u9898\uff0c\u4f18\u5316CNN\u5728MCU\u4e0a\u7684\u8fd0\u884c\u6548\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8865\u4e01\u7684\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u5411\u65e0\u73af\u56fe\u63a2\u7d22\u878d\u5408\u89e3\u7a7a\u95f4\uff0c\u627e\u5230\u6700\u4f18\u878d\u5408\u8bbe\u7f6e\u3002", "result": "msf-CNN\u6bd4\u73b0\u6709\u6280\u672f\uff08MCUNetV2\u548cStreamNet\uff09\u51cf\u5c1150%\u7684RAM\u4f7f\u7528\uff0c\u5e76\u5728\u591a\u79cdMCU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "conclusion": "msf-CNN\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684MCU\u73af\u5883\u3002", "relevance": 40.0}}
{"id": "2505.11491", "pdf": "https://arxiv.org/pdf/2505.11491", "abs": "https://arxiv.org/abs/2505.11491", "authors": ["Yuan-Zheng Lei", "Yaobang Gong", "Dianwei Chen", "Yao Cheng", "Xianfeng Terry Yang"], "title": "Potential failures of physics-informed machine learning in traffic flow modeling: theoretical and experimental analysis", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "This study critically examines the performance of physics-informed machine\nlearning (PIML) approaches for traffic flow modeling, defining the failure of a\nPIML model as the scenario where it underperforms both its purely data-driven\nand purely physics-based counterparts. We analyze the loss landscape by\nperturbing trained models along the principal eigenvectors of the Hessian\nmatrix and evaluating corresponding loss values. Our results suggest that\nphysics residuals in PIML do not inherently hinder optimization, contrary to a\ncommonly assumed failure cause. Instead, successful parameter updates require\nboth ML and physics gradients to form acute angles with the quasi-true gradient\nand lie within a conical region. Given inaccuracies in both the physics models\nand the training data, satisfying this condition is often difficult.\nExperiments reveal that physical residuals can degrade the performance of LWR-\nand ARZ-based PIML models, especially under highly physics-driven settings.\nMoreover, sparse sampling and the use of temporally averaged traffic data can\nproduce misleadingly small physics residuals that fail to capture actual\nphysical dynamics, contributing to model failure. We also identify the\nCourant-Friedrichs-Lewy (CFL) condition as a key indicator of dataset\nsuitability for PIML, where successful applications consistently adhere to this\ncriterion. Lastly, we observe that higher-order models like ARZ tend to have\nlarger error lower bounds than lower-order models like LWR, which is consistent\nwith the experimental findings of existing studies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\uff08PIML\uff09\u5728\u4ea4\u901a\u6d41\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5931\u8d25\u539f\u56e0\u5e76\u975e\u7269\u7406\u6b8b\u5dee\u963b\u788d\u4f18\u5316\uff0c\u800c\u662f\u9700\u8981ML\u548c\u7269\u7406\u68af\u5ea6\u5728\u5706\u9525\u533a\u57df\u5185\u5bf9\u9f50\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u7684\u4e0d\u51c6\u786e\u6027\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u9ad8\u9636\u6a21\u578b\u7684\u8bef\u5dee\u4e0b\u9650\u66f4\u5927\u3002", "motivation": "\u63a2\u8ba8PIML\u5728\u4ea4\u901a\u6d41\u5efa\u6a21\u4e2d\u7684\u5931\u8d25\u539f\u56e0\uff0c\u6311\u6218\u4e86\u7269\u7406\u6b8b\u5dee\u963b\u788d\u4f18\u5316\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u65e8\u5728\u63ed\u793a\u66f4\u672c\u8d28\u7684\u4f18\u5316\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u6270\u52a8\u8bad\u7ec3\u6a21\u578b\u5e76\u5206\u6790Hessian\u77e9\u9635\u7684\u4e3b\u7279\u5f81\u5411\u91cf\uff0c\u8bc4\u4f30\u635f\u5931\u503c\uff0c\u7814\u7a76ML\u548c\u7269\u7406\u68af\u5ea6\u7684\u5bf9\u9f50\u6761\u4ef6\u3002", "result": "\u7269\u7406\u6b8b\u5dee\u4e0d\u76f4\u63a5\u963b\u788d\u4f18\u5316\uff0c\u4f46\u68af\u5ea6\u7684\u5bf9\u9f50\u662f\u5173\u952e\u3002\u9ad8\u9636\u6a21\u578b\uff08\u5982ARZ\uff09\u7684\u8bef\u5dee\u4e0b\u9650\u66f4\u5927\uff0c\u4e14CFL\u6761\u4ef6\u662f\u6570\u636e\u96c6\u9002\u7528\u6027\u7684\u91cd\u8981\u6307\u6807\u3002", "conclusion": "PIML\u7684\u6210\u529f\u4f9d\u8d56\u4e8eML\u548c\u7269\u7406\u68af\u5ea6\u7684\u5408\u7406\u5bf9\u9f50\uff0c\u4e14\u9700\u6ce8\u610f\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u9636\u6570\u7684\u5f71\u54cd\u3002", "relevance": 30.0}}
{"id": "2505.04514", "pdf": "https://arxiv.org/pdf/2505.04514", "abs": "https://arxiv.org/abs/2505.04514", "authors": ["Nana Liu", "Michele Minervini", "Dhrumil Patel", "Mark M. Wilde"], "title": "Quantum thermodynamics and semi-definite optimization", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.DS", "cs.LG", "math.OC"], "comment": "v2: 16 pages of main text, 15 pages of appendices, 3 figures,\n  corrections introduced", "summary": "In quantum thermodynamics, a system is described by a Hamiltonian and a list\nof non-commuting charges representing conserved quantities like particle number\nor electric charge, and an important goal is to determine the system's minimum\nenergy in the presence of these conserved charges. In optimization theory, a\nsemi-definite program (SDP) involves a linear objective function optimized over\nthe cone of positive semi-definite operators intersected with an affine space.\nThese problems arise from differing motivations in the physics and optimization\ncommunities and are phrased using very different terminology, yet they are\nessentially identical mathematically. By adopting Jaynes' mindset motivated by\nquantum thermodynamics, we observe that minimizing free energy in the\naforementioned thermodynamics problem, instead of energy, leads to an elegant\nsolution in terms of a dual chemical potential maximization problem that is\nconcave in the chemical potential parameters. As such, one can employ standard\n(stochastic) gradient ascent methods to find the optimal values of these\nparameters, and these methods are guaranteed to converge quickly. At low\ntemperature, the minimum free energy provides an excellent approximation for\nthe minimum energy. We then show how this Jaynes-inspired gradient-ascent\napproach can be used in both first- and second-order classical and hybrid\nquantum-classical algorithms for minimizing energy, and equivalently, how it\ncan be used for solving SDPs, with guarantees on the runtimes of the\nalgorithms. The approach discussed here is well grounded in quantum\nthermodynamics and, as such, provides physical motivation underpinning why\nalgorithms published fifty years after Jaynes' seminal work, including the\nmatrix multiplicative weights update method, the matrix exponentiated gradient\nupdate method, and their quantum algorithmic generalizations, perform well at\nsolving SDPs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u70ed\u529b\u5b66\u4e0e\u4f18\u5316\u7406\u8bba\u4e2d\u7684\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u95ee\u9898\u4e4b\u95f4\u7684\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eJaynes\u601d\u60f3\u7684\u68af\u5ea6\u4e0a\u5347\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7ecf\u5178\u548c\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u91cf\u5b50\u70ed\u529b\u5b66\u7684\u89c6\u89d2\uff0c\u4e3a\u4f18\u5316\u7406\u8bba\u4e2d\u7684SDP\u95ee\u9898\u63d0\u4f9b\u7269\u7406\u52a8\u673a\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u91c7\u7528Jaynes\u7684\u601d\u60f3\uff0c\u5c06\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\u95ee\u9898\u8f6c\u5316\u4e3a\u5316\u5b66\u52bf\u6700\u5927\u5316\u95ee\u9898\uff0c\u5229\u7528\u68af\u5ea6\u4e0a\u5347\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f4e\u6e29\u5ea6\u4e0b\u80fd\u6709\u6548\u8fd1\u4f3c\u6700\u5c0f\u80fd\u91cf\uff0c\u5e76\u53ef\u7528\u4e8e\u89e3\u51b3SDP\u95ee\u9898\uff0c\u7b97\u6cd5\u5177\u6709\u5feb\u901f\u6536\u655b\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7269\u7406\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u73b0\u6709\u7b97\u6cd5\uff08\u5982\u77e9\u9635\u4e58\u6cd5\u6743\u91cd\u66f4\u65b0\uff09\u7684\u9ad8\u6548\u6027\u3002", "relevance": 20.0}}
{"id": "2505.11123", "pdf": "https://arxiv.org/pdf/2505.11123", "abs": "https://arxiv.org/abs/2505.11123", "authors": ["Zibin Dong", "Yicheng Liu", "Yinchuan Li", "Hang Zhao", "Jianye Hao"], "title": "Conditioning Matters: Training Diffusion Policies is Faster Than You Think", "categories": ["cs.RO", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2505.10105", "summary": "Diffusion policies have emerged as a mainstream paradigm for building\nvision-language-action (VLA) models. Although they demonstrate strong robot\ncontrol capabilities, their training efficiency remains suboptimal. In this\nwork, we identify a fundamental challenge in conditional diffusion policy\ntraining: when generative conditions are hard to distinguish, the training\nobjective degenerates into modeling the marginal action distribution, a\nphenomenon we term loss collapse. To overcome this, we propose Cocos, a simple\nyet general solution that modifies the source distribution in the conditional\nflow matching to be condition-dependent. By anchoring the source distribution\naround semantics extracted from condition inputs, Cocos encourages stronger\ncondition integration and prevents the loss collapse. We provide theoretical\njustification and extensive empirical results across simulation and real-world\nbenchmarks. Our method achieves faster convergence and higher success rates\nthan existing approaches, matching the performance of large-scale pre-trained\nVLAs using significantly fewer gradient steps and parameters. Cocos is\nlightweight, easy to implement, and compatible with diverse policy\narchitectures, offering a general-purpose improvement to diffusion policy\ntraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCocos\u65b9\u6cd5\uff0c\u89e3\u51b3\u6761\u4ef6\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u4e2d\u7684\u635f\u5931\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u6761\u4ef6\u6d41\u5339\u914d\u4e2d\u7684\u6e90\u5206\u5e03\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u6548\u7387\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5f53\u751f\u6210\u6761\u4ef6\u96be\u4ee5\u533a\u5206\u65f6\uff0c\u8bad\u7ec3\u76ee\u6807\u4f1a\u9000\u5316\u4e3a\u5efa\u6a21\u8fb9\u9645\u52a8\u4f5c\u5206\u5e03\uff08\u635f\u5931\u5d29\u6e83\uff09\u3002", "method": "\u63d0\u51faCocos\u65b9\u6cd5\uff0c\u5c06\u6761\u4ef6\u6d41\u5339\u914d\u4e2d\u7684\u6e90\u5206\u5e03\u6539\u4e3a\u4f9d\u8d56\u6761\u4ef6\uff0c\u901a\u8fc7\u951a\u5b9a\u6761\u4ef6\u8f93\u5165\u7684\u8bed\u4e49\u63d0\u53d6\uff0c\u589e\u5f3a\u6761\u4ef6\u6574\u5408\u5e76\u9632\u6b62\u635f\u5931\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCocos\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u6027\u80fd\u5ab2\u7f8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3VLA\u6a21\u578b\uff0c\u4f46\u6240\u9700\u68af\u5ea6\u6b65\u6570\u548c\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "Cocos\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6613\u5b9e\u73b0\u4e14\u517c\u5bb9\u591a\u79cd\u7b56\u7565\u67b6\u6784\u7684\u901a\u7528\u6539\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u7b56\u7565\u7684\u8bad\u7ec3\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.10573", "pdf": "https://arxiv.org/pdf/2505.10573", "abs": "https://arxiv.org/abs/2505.10573", "authors": ["Olawale Salaudeen", "Anka Reuel", "Ahmed Ahmed", "Suhana Bedi", "Zachary Robertson", "Sudharsan Sundar", "Ben Domingue", "Angelina Wang", "Sanmi Koyejo"], "title": "Measurement to Meaning: A Validity-Centered Framework for AI Evaluation", "categories": ["cs.CY", "cs.LG"], "comment": "Corresponding author: olawale@mit.edu", "summary": "While the capabilities and utility of AI systems have advanced, rigorous\nnorms for evaluating these systems have lagged. Grand claims, such as models\nachieving general reasoning capabilities, are supported with model performance\non narrow benchmarks, like performance on graduate-level exam questions, which\nprovide a limited and potentially misleading assessment. We provide a\nstructured approach for reasoning about the types of evaluative claims that can\nbe made given the available evidence. For instance, our framework helps\ndetermine whether performance on a mathematical benchmark is an indication of\nthe ability to solve problems on math tests or instead indicates a broader\nability to reason. Our framework is well-suited for the contemporary paradigm\nin machine learning, where various stakeholders provide measurements and\nevaluations that downstream users use to validate their claims and decisions.\nAt the same time, our framework also informs the construction of evaluations\ndesigned to speak to the validity of the relevant claims. By leveraging\npsychometrics' breakdown of validity, evaluations can prioritize the most\ncritical facets for a given claim, improving empirical utility and\ndecision-making efficacy. We illustrate our framework through detailed case\nstudies of vision and language model evaluations, highlighting how explicitly\nconsidering validity strengthens the connection between evaluation evidence and\nthe claims being made.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u6709\u6548\u6027\u5206\u89e3\u6539\u8fdb\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u8bc4\u4f30\u7f3a\u4e4f\u4e25\u8c28\u6027\uff0c\u7a84\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u8bef\u5bfc\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5224\u65ad\uff0c\u9700\u8981\u66f4\u79d1\u5b66\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u6709\u6548\u6027\u5206\u89e3\uff0c\u4f18\u5316\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u80fd\u66f4\u51c6\u786e\u5730\u8fde\u63a5\u8bc4\u4f30\u8bc1\u636e\u4e0e\u6a21\u578b\u80fd\u529b\u58f0\u660e\uff0c\u63d0\u5347\u8bc4\u4f30\u7684\u5b9e\u7528\u6027\u548c\u51b3\u7b56\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u79d1\u5b66\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u5229\u76ca\u76f8\u5173\u65b9\u7684\u8bc4\u4f30\u9700\u6c42\u3002", "relevance": 85.0}}
{"id": "2505.10605", "pdf": "https://arxiv.org/pdf/2505.10605", "abs": "https://arxiv.org/abs/2505.10605", "authors": ["Frederik K\u00f6hne", "Anton Schiela"], "title": "An Exponential Averaging Process with Strong Convergence Properties", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH", "60F15 (Primary) 60G10, 60J20, 68T05, 90C15 (Secondary)"], "comment": null, "summary": "Averaging, or smoothing, is a fundamental approach to obtain stable,\nde-noised estimates from noisy observations. In certain scenarios, observations\nmade along trajectories of random dynamical systems are of particular interest.\nOne popular smoothing technique for such a scenario is exponential moving\naveraging (EMA), which assigns observations a weight that decreases\nexponentially in their age, thus giving younger observations a larger weight.\nHowever, EMA fails to enjoy strong stochastic convergence properties, which\nstems from the fact that the weight assigned to the youngest observation is\nconstant over time, preventing the noise in the averaged quantity from\ndecreasing to zero. In this work, we consider an adaptation to EMA, which we\ncall $p$-EMA, where the weights assigned to the last observations decrease to\nzero at a subharmonic rate. We provide stochastic convergence guarantees for\nthis kind of averaging under mild assumptions on the autocorrelations of the\nunderlying random dynamical system. We further discuss the implications of our\nresults for a recently introduced adaptive step size control for Stochastic\nGradient Descent (SGD), which uses $p$-EMA for averaging noisy observations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\uff08$p$-EMA\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u6743\u91cd\u5206\u914d\u65b9\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfEMA\u5728\u968f\u673a\u6536\u655b\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728SGD\u81ea\u9002\u5e94\u6b65\u957f\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfEMA\u65b9\u6cd5\u5728\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u6536\u655b\u6027\u8f83\u5f31\uff0c\u56e0\u4e3a\u5176\u8d4b\u4e88\u6700\u65b0\u89c2\u6d4b\u503c\u7684\u6743\u91cd\u6052\u5b9a\uff0c\u5bfc\u81f4\u566a\u58f0\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u3002\u672c\u6587\u65e8\u5728\u6539\u8fdbEMA\uff0c\u63d0\u51fa$p$-EMA\uff0c\u4ee5\u63d0\u5347\u6536\u655b\u6027\u3002", "method": "\u63d0\u51fa$p$-EMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u6700\u65b0\u89c2\u6d4b\u503c\u7684\u6743\u91cd\u4ee5\u6b21\u8c10\u6ce2\u901f\u7387\u9012\u51cf\u4e3a\u96f6\uff0c\u6539\u8fdbEMA\u7684\u6536\u655b\u6027\u3002\u7406\u8bba\u5206\u6790\u57fa\u4e8e\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u7684\u81ea\u76f8\u5173\u5047\u8bbe\u3002", "result": "\u5728\u6e29\u548c\u7684\u81ea\u76f8\u5173\u5047\u8bbe\u4e0b\uff0c$p$-EMA\u5177\u6709\u66f4\u5f3a\u7684\u968f\u673a\u6536\u655b\u6027\u3002", "conclusion": "$p$-EMA\u5728\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfEMA\uff0c\u5c24\u5176\u9002\u7528\u4e8eSGD\u7684\u81ea\u9002\u5e94\u6b65\u957f\u63a7\u5236\u3002", "relevance": 30.0}}
{"id": "2505.10628", "pdf": "https://arxiv.org/pdf/2505.10628", "abs": "https://arxiv.org/abs/2505.10628", "authors": ["Jonathan Garc\u00eda", "Philipp Petersen"], "title": "Minimax learning rates for estimating binary classifiers under margin conditions", "categories": ["stat.ML", "cs.LG", "math.PR", "68T05, 62C20, 41A25, 41A46"], "comment": null, "summary": "We study classification problems using binary estimators where the decision\nboundary is described by horizon functions and where the data distribution\nsatisfies a geometric margin condition. We establish upper and lower bounds for\nthe minimax learning rate over broad function classes with bounded Kolmogorov\nentropy in Lebesgue norms. A key novelty of our work is the derivation of lower\nbounds on the worst-case learning rates under a geometric margin condition -- a\nsetting that is almost universally satisfied in practice but remains\ntheoretically challenging. Moreover, our results deal with the noiseless\nsetting, where lower bounds are particularly hard to establish. We apply our\ngeneral results to classification problems with decision boundaries belonging\nto several function classes: for Barron-regular functions, and for\nH\\\"older-continuous functions with strong margins, we identify optimal rates\nclose to the fast learning rates of $\\mathcal{O}(n^{-1})$ for $n \\in\n\\mathbb{N}$ samples. Also for merely convex decision boundaries, in a strong\nmargin case optimal rates near $\\mathcal{O}(n^{-1/2})$ can be achieved.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e8c\u5143\u4f30\u8ba1\u5668\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u5728\u51e0\u4f55\u8fb9\u754c\u6761\u4ef6\u4e0b\u5e7f\u6cdb\u51fd\u6570\u7c7b\u7684\u6700\u5c0f\u6700\u5927\u5b66\u4e60\u7387\u4e0a\u4e0b\u754c\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u4e2a\u51fd\u6570\u7c7b\u3002", "motivation": "\u7814\u7a76\u5206\u7c7b\u95ee\u9898\u4e2d\u51e0\u4f55\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u5b66\u4e60\u7387\uff0c\u586b\u8865\u7406\u8bba\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u65e0\u566a\u58f0\u573a\u666f\u4e0b\u7684\u4e0b\u754c\u63a8\u5bfc\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u4f30\u8ba1\u5668\uff0c\u57fa\u4e8e\u6709\u754cKolmogorov\u71b5\u7684\u51fd\u6570\u7c7b\uff0c\u63a8\u5bfc\u5b66\u4e60\u7387\u4e0a\u4e0b\u754c\u3002", "result": "\u5728Barron\u6b63\u5219\u51fd\u6570\u548cH\u00f6lder\u8fde\u7eed\u51fd\u6570\u4e2d\uff0c\u8bc6\u522b\u51fa\u63a5\u8fd1\u5feb\u901f\u5b66\u4e60\u7387\u7684\u6700\u4f18\u901f\u7387\uff1b\u5728\u5f3a\u8fb9\u754c\u6761\u4ef6\u4e0b\uff0c\u51f8\u51b3\u7b56\u8fb9\u754c\u4e5f\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u901f\u7387\u3002", "conclusion": "\u5728\u51e0\u4f55\u8fb9\u754c\u6761\u4ef6\u4e0b\uff0c\u5206\u7c7b\u95ee\u9898\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u5b66\u4e60\u901f\u7387\u3002", "relevance": 30.0}}
{"id": "2505.11146", "pdf": "https://arxiv.org/pdf/2505.11146", "abs": "https://arxiv.org/abs/2505.11146", "authors": ["Peizhen Li", "Longbing Cao", "Xiao-Ming Wu", "Runze Yang", "Xiaohan Yu"], "title": "X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "The ability to imitate realistic facial expressions is essential for humanoid\nrobots engaged in affective human-robot communication. However, the lack of\ndatasets containing diverse humanoid facial expressions with proper annotations\nhinders progress in realistic humanoid facial expression imitation. To address\nthese challenges, we introduce X2C (Anything to Control), a dataset featuring\nnuanced facial expressions for realistic humanoid imitation. With X2C, we\ncontribute: 1) a high-quality, high-diversity, large-scale dataset comprising\n100,000 (image, control value) pairs. Each image depicts a humanoid robot\ndisplaying a diverse range of facial expressions, annotated with 30 control\nvalues representing the ground-truth expression configuration; 2) X2CNet, a\nnovel human-to-humanoid facial expression imitation framework that learns the\ncorrespondence between nuanced humanoid expressions and their underlying\ncontrol values from X2C. It enables facial expression imitation in the wild for\ndifferent human performers, providing a baseline for the imitation task,\nshowcasing the potential value of our dataset; 3) real-world demonstrations on\na physical humanoid robot, highlighting its capability to advance realistic\nhumanoid facial expression imitation. Code and Data:\nhttps://lipzh5.github.io/X2CNet/", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86X2C\u6570\u636e\u96c6\u548cX2CNet\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u9762\u90e8\u8868\u60c5\u6a21\u4eff\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u4eba\u5f62\u673a\u5668\u4eba\u9762\u90e8\u8868\u60c5\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u771f\u5b9e\u8868\u60c5\u6a21\u4eff\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "1\uff09\u6784\u5efaX2C\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07\u5f20\u5e26\u63a7\u5236\u503c\u6807\u6ce8\u7684\u56fe\u50cf\uff1b2\uff09\u63d0\u51faX2CNet\u6846\u67b6\uff0c\u5b66\u4e60\u4eba\u7c7b\u4e0e\u4eba\u5f62\u673a\u5668\u4eba\u8868\u60c5\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "X2CNet\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u4eba\u7c7b\u8868\u6f14\u8005\u7684\u9762\u90e8\u8868\u60c5\u6a21\u4eff\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u5176\u80fd\u529b\u3002", "conclusion": "X2C\u6570\u636e\u96c6\u548cX2CNet\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u9762\u90e8\u8868\u60c5\u6a21\u4eff\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.10678", "pdf": "https://arxiv.org/pdf/2505.10678", "abs": "https://arxiv.org/abs/2505.10678", "authors": ["Rebecca G. Hart", "Omkar Sudhir Patil", "Zachary I. Bell", "Warren E. Dixon"], "title": "System Identification and Control Using Lyapunov-Based Deep Neural Networks without Persistent Excitation: A Concurrent Learning Approach", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Deep Neural Networks (DNNs) are increasingly used in control applications due\nto their powerful function approximation capabilities. However, many existing\nformulations focus primarily on tracking error convergence, often neglecting\nthe challenge of identifying the system dynamics using the DNN. This paper\npresents the first result on simultaneous trajectory tracking and online system\nidentification using a DNN-based controller, without requiring persistent\nexcitation. Two new concurrent learning adaptation laws are constructed for the\nweights of all the layers of the DNN, achieving convergence of the DNN's\nparameter estimates to a neighborhood of their ideal values, provided the DNN's\nJacobian satisfies a finite-time excitation condition. A Lyapunov-based\nstability analysis is conducted to ensure convergence of the tracking error,\nweight estimation errors, and observer errors to a neighborhood of the origin.\nSimulations performed on a range of systems and trajectories, with the same\ninitial and operating conditions, demonstrated 40.5% to 73.6% improvement in\nfunction approximation performance compared to the baseline, while maintaining\na similar tracking error and control effort. Simulations evaluating function\napproximation capabilities on data points outside of the trajectory resulted in\n58.88% and 74.75% improvement in function approximation compared to the\nbaseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDNN\u7684\u63a7\u5236\u5668\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u540c\u65f6\u8fdb\u884c\u8f68\u8ff9\u8ddf\u8e2a\u548c\u5728\u7ebf\u7cfb\u7edf\u8bc6\u522b\uff0c\u65e0\u9700\u6301\u7eed\u6fc0\u52b1\u3002\u901a\u8fc7\u65b0\u7684\u5e76\u53d1\u5b66\u4e60\u9002\u5e94\u5f8b\uff0c\u6539\u8fdb\u4e86\u51fd\u6570\u903c\u8fd1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DNN\u63a7\u5236\u65b9\u6cd5\u591a\u5173\u6ce8\u8ddf\u8e2a\u8bef\u5dee\u6536\u655b\uff0c\u800c\u5ffd\u7565\u4e86\u7cfb\u7edf\u52a8\u6001\u8bc6\u522b\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u7684\u5e76\u53d1\u5b66\u4e60\u9002\u5e94\u5f8b\uff0c\u7528\u4e8e\u8c03\u6574DNN\u5404\u5c42\u6743\u91cd\uff0c\u5e76\u7ed3\u5408Lyapunov\u7a33\u5b9a\u6027\u5206\u6790\u786e\u4fdd\u6536\u655b\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u51fd\u6570\u903c\u8fd1\u6027\u80fd\u63d0\u5347\u4e8640.5%\u81f373.6%\uff0c\u4e14\u5728\u8f68\u8ff9\u5916\u6570\u636e\u70b9\u4e0a\u4e5f\u6709\u663e\u8457\u6539\u8fdb\uff0858.88%\u548c74.75%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ddf\u8e2a\u8bef\u5dee\u548c\u63a7\u5236\u52aa\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u52a8\u6001\u8bc6\u522b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.11175", "pdf": "https://arxiv.org/pdf/2505.11175", "abs": "https://arxiv.org/abs/2505.11175", "authors": ["Bo Yue", "Shuqi Guo", "Kaiyu Hu", "Chujiao Wang", "Benyou Wang", "Kui Jia", "Guiliang Liu"], "title": "Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generative skill acquisition enables embodied agents to actively learn a\nscalable and evolving repertoire of control skills, crucial for the advancement\nof large decision models. While prior approaches often rely on supervision\nsignals from generalist agents (e.g., LLMs), their effectiveness in complex 3D\nenvironments remains unclear; exhaustive evaluation incurs substantial\ncomputational costs, significantly hindering the efficiency of skill learning.\nInspired by recent successes in verification models for mathematical reasoning,\nwe propose VERGSA (Verifying Embodied Reasoning in Generative Skill\nAcquisition), a framework that systematically integrates real-time verification\nprinciples into embodied skill learning. VERGSA establishes 1) a seamless\nextension from verification of mathematical reasoning into embodied learning by\ndynamically incorporating contextually relevant tasks into prompts and defining\nsuccess metrics for both subtasks and overall tasks, and 2) an automated,\nscalable reward labeling scheme that synthesizes dense reward signals by\niteratively finalizing the contribution of scene configuration and subtask\nlearning to overall skill acquisition. To the best of our knowledge, this\napproach constitutes the first comprehensive training dataset for\nverification-driven generative skill acquisition, eliminating arduous manual\nreward engineering. Experiments validate the efficacy of our approach: 1) the\nexemplar task pool improves the average task success rates by 21%, 2) our\nverification model boosts success rates by 24% for novel tasks and 36% for\nencountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification\nquality.", "AI": {"tldr": "VERGSA\u6846\u67b6\u5c06\u6570\u5b66\u63a8\u7406\u9a8c\u8bc1\u539f\u5219\u878d\u5165\u751f\u6210\u6280\u80fd\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u96c6\u6210\u548c\u81ea\u52a8\u5316\u5956\u52b1\u6807\u6ce8\u63d0\u5347\u6280\u80fd\u83b7\u53d6\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u89e3\u51b3\u590d\u67423D\u73af\u5883\u4e2d\u751f\u6210\u6280\u80fd\u5b66\u4e60\u7684\u6548\u7387\u95ee\u9898\uff0c\u907f\u514d\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u51cf\u5c11\u4eba\u5de5\u5956\u52b1\u8bbe\u8ba1\u7684\u8d1f\u62c5\u3002", "method": "\u63d0\u51faVERGSA\u6846\u67b6\uff0c\u52a8\u6001\u96c6\u6210\u4efb\u52a1\u63d0\u793a\u548c\u5b9a\u4e49\u6210\u529f\u6307\u6807\uff0c\u81ea\u52a8\u5316\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534721%\uff0c\u9a8c\u8bc1\u6a21\u578b\u5bf9\u65b0\u9896\u548c\u5df2\u9047\u4efb\u52a1\u5206\u522b\u63d0\u534724%\u548c36%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8eLLM\u57fa\u51c6\u3002", "conclusion": "VERGSA\u901a\u8fc7\u9a8c\u8bc1\u9a71\u52a8\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u751f\u6210\u6280\u80fd\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u51b3\u7b56\u6a21\u578b\u63d0\u4f9b\u652f\u6301\u3002", "relevance": 70.0}}
{"id": "2505.11176", "pdf": "https://arxiv.org/pdf/2505.11176", "abs": "https://arxiv.org/abs/2505.11176", "authors": ["Aaron Rodrigues", "Mahmood Hegazy", "Azzam Naeem"], "title": "From Intent Discovery to Recognition with Topic Modeling and Synthetic Data", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Understanding and recognizing customer intents in AI systems is crucial,\nparticularly in domains characterized by short utterances and the cold start\nproblem, where recommender systems must include new products or services\nwithout sufficient real user data. Customer utterances are characterized by\ninfrequent word co-occurences and high term variability, which poses\nsignificant challenges for traditional methods in specifying distinct user\nneeds and preparing synthetic queries. To address this, we propose an agentic\nLLM framework for topic modeling and synthetic query generation, which\naccelerates the discovery and recognition of customer intents. We first apply\nhierarchical topic modeling and intent discovery to expand a human-curated\ntaxonomy from 36 generic user intents to 278 granular intents, demonstrating\nthe potential of LLMs to significantly enhance topic specificity and diversity.\nNext, to support newly discovered intents and address the cold start problem,\nwe generate synthetic user query data, which augments real utterances and\nreduces dependency on human annotation, especially in low-resource settings.\nTopic model experiments show substantial improvements in coherence and\nrelevance after topic expansion, while synthetic data experiments indicate that\nin-class few-shot prompting significantly improves the quality and utility of\nsynthetic queries without compromising diversity. We also show that\nLLM-generated intent descriptions and keywords can effectively substitute for\nhuman-curated versions when used as context for synthetic query generation. Our\nresearch underscores the scalability and utility of LLM agents in topic\nmodeling and highlights the strategic use of synthetic utterances to enhance\ndataset variability and coverage for intent recognition. We present a\ncomprehensive and robust framework for online discovery and recognition of new\ncustomer intents in dynamic domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e3b\u9898\u5efa\u6a21\u548c\u5408\u6210\u67e5\u8be2\u751f\u6210\uff0c\u4ee5\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u548c\u77ed\u8bdd\u8bed\u9886\u57df\u7684\u5ba2\u6237\u610f\u56fe\u8bc6\u522b\u3002\u901a\u8fc7\u5206\u5c42\u4e3b\u9898\u5efa\u6a21\u548c\u610f\u56fe\u53d1\u73b0\uff0c\u6269\u5c55\u4e86\u4eba\u5de5\u5206\u7c7b\u6cd5\uff0c\u5e76\u751f\u6210\u5408\u6210\u67e5\u8be2\u6570\u636e\u4ee5\u589e\u5f3a\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u67e5\u8be2\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u548c\u77ed\u8bdd\u8bed\u9886\u57df\u4e2d\u5ba2\u6237\u610f\u56fe\u8bc6\u522b\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4f4e\u9891\u8bcd\u5171\u73b0\u548c\u9ad8\u53d8\u5f02\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u4e3b\u9898\u5efa\u6a21\u548c\u610f\u56fe\u53d1\u73b0\u6269\u5c55\u5206\u7c7b\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u5408\u6210\u67e5\u8be2\u6570\u636e\u4ee5\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u4e3b\u9898\u6269\u5c55\u540e\u4e00\u81f4\u6027\u548c\u76f8\u5173\u6027\u663e\u8457\u63d0\u5347\uff0c\u5408\u6210\u67e5\u8be2\u8d28\u91cf\u9ad8\u4e14\u591a\u6837\u6027\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "LLM\u6846\u67b6\u5728\u4e3b\u9898\u5efa\u6a21\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u610f\u56fe\u8bc6\u522b\u7684\u6570\u636e\u96c6\u8986\u76d6\u3002", "relevance": 70.0}}
{"id": "2505.10843", "pdf": "https://arxiv.org/pdf/2505.10843", "abs": "https://arxiv.org/abs/2505.10843", "authors": ["Yuta Higuchi", "Rikuto Nagai", "Atsushi Okazaki", "Masaki Ogura", "Naoki Wakamiya"], "title": "Comparative Analysis of Black-Box Optimization Methods for Weather Intervention Design", "categories": ["physics.ao-ph", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "comment": "15 pages, 11 figures", "summary": "As climate change increases the threat of weather-related disasters, research\non weather control is gaining importance. The objective of weather control is\nto mitigate disaster risks by administering interventions with optimal timing,\nlocation, and intensity. However, the optimization process is highly\nchallenging due to the vast scale and complexity of weather phenomena, which\nintroduces two major challenges. First, obtaining accurate gradient information\nfor optimization is difficult. In addition, numerical weather prediction (NWP)\nmodels demand enormous computational resources, necessitating parameter\noptimization with minimal function evaluations. To address these challenges,\nthis study proposes a method for designing weather interventions based on\nblack-box optimization, which enables efficient exploration without requiring\ngradient information. The proposed method is evaluated in two distinct control\nscenarios: one-shot initial value intervention and sequential intervention\nbased on model predictive control. Furthermore, a comparative analysis is\nconducted among four representative black-box optimization methods in terms of\ntotal rainfall reduction. Experimental results show that Bayesian optimization\nachieves higher control effectiveness than the others, particularly in\nhigh-dimensional search spaces. These findings suggest that Bayesian\noptimization is a highly effective approach for weather intervention\ncomputation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ed1\u76d2\u4f18\u5316\u7684\u5929\u6c14\u5e72\u9884\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5929\u6c14\u63a7\u5236\u4e2d\u68af\u5ea6\u4fe1\u606f\u83b7\u53d6\u96be\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u964d\u96e8\u51cf\u5c11\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u5929\u6c14\u76f8\u5173\u707e\u5bb3\u7684\u5a01\u80c1\uff0c\u5929\u6c14\u63a7\u5236\u7814\u7a76\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u4f18\u5316\u8fc7\u7a0b\u56e0\u5929\u6c14\u73b0\u8c61\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u800c\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u8bbe\u8ba1\u5929\u6c14\u5e72\u9884\uff0c\u65e0\u9700\u68af\u5ea6\u4fe1\u606f\uff0c\u652f\u6301\u9ad8\u6548\u63a2\u7d22\u3002\u5b9e\u9a8c\u5305\u62ec\u5355\u6b21\u521d\u59cb\u503c\u5e72\u9884\u548c\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u5e8f\u5217\u5e72\u9884\u3002", "result": "\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u9ad8\u7ef4\u641c\u7d22\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u964d\u96e8\u51cf\u5c11\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u5929\u6c14\u5e72\u9884\u8ba1\u7b97\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.10879", "pdf": "https://arxiv.org/pdf/2505.10879", "abs": "https://arxiv.org/abs/2505.10879", "authors": ["Ali Sartaz Khan", "Tolulope Ogunremi", "Ahmed Attia", "Dorottya Demszky"], "title": "Multi-Stage Speaker Diarization for Noisy Classrooms", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Speaker diarization, the process of identifying \"who spoke when\" in audio\nrecordings, is essential for understanding classroom dynamics. However,\nclassroom settings present distinct challenges, including poor recording\nquality, high levels of background noise, overlapping speech, and the\ndifficulty of accurately capturing children's voices. This study investigates\nthe effectiveness of multi-stage diarization models using Nvidia's NeMo\ndiarization pipeline. We assess the impact of denoising on diarization accuracy\nand compare various voice activity detection (VAD) models, including\nself-supervised transformer-based frame-wise VAD models. We also explore a\nhybrid VAD approach that integrates Automatic Speech Recognition (ASR)\nword-level timestamps with frame-level VAD predictions. We conduct experiments\nusing two datasets from English speaking classrooms to separate teacher vs.\nstudent speech and to separate all speakers. Our results show that denoising\nsignificantly improves the Diarization Error Rate (DER) by reducing the rate of\nmissed speech. Additionally, training on both denoised and noisy datasets leads\nto substantial performance gains in noisy conditions. The hybrid VAD model\nleads to further improvements in speech detection, achieving a DER as low as\n17% in teacher-student experiments and 45% in all-speaker experiments. However,\nwe also identified trade-offs between voice activity detection and speaker\nconfusion. Overall, our study highlights the effectiveness of multi-stage\ndiarization models and integrating ASR-based information for enhancing speaker\ndiarization in noisy classroom environments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u9636\u6bb5\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6a21\u578b\u5728\u5608\u6742\u6559\u5ba4\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7ed3\u5408\u53bb\u566a\u548c\u6df7\u5408VAD\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8bf4\u8bdd\u4eba\u9519\u8bef\u7387\u3002", "motivation": "\u6559\u5ba4\u73af\u5883\u4e2d\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9762\u4e34\u5f55\u97f3\u8d28\u91cf\u5dee\u3001\u80cc\u666f\u566a\u58f0\u9ad8\u548c\u513f\u7ae5\u58f0\u97f3\u96be\u4ee5\u6355\u6349\u7b49\u6311\u6218\uff0c\u9700\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Nvidia\u7684NeMo\u65e5\u5fd7\u6d41\u6c34\u7ebf\uff0c\u8bc4\u4f30\u53bb\u566a\u6548\u679c\uff0c\u6bd4\u8f83\u4e0d\u540cVAD\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u7ed3\u5408ASR\u7684\u6df7\u5408VAD\u65b9\u6cd5\u3002", "result": "\u53bb\u566a\u663e\u8457\u964d\u4f4eDER\uff0c\u6df7\u5408VAD\u6a21\u578b\u5728\u6559\u5e08-\u5b66\u751f\u5b9e\u9a8c\u4e2dDER\u4f4e\u81f317%\uff0c\u5168\u8bf4\u8bdd\u4eba\u5b9e\u9a8c\u4e2d\u4e3a45%\u3002", "conclusion": "\u591a\u9636\u6bb5\u65e5\u5fd7\u6a21\u578b\u548cASR\u4fe1\u606f\u6574\u5408\u80fd\u6709\u6548\u63d0\u5347\u5608\u6742\u73af\u5883\u4e2d\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.10919", "pdf": "https://arxiv.org/pdf/2505.10919", "abs": "https://arxiv.org/abs/2505.10919", "authors": ["Luca Menicali", "Andrew Grace", "David H. Richter", "Stefano Castruccio"], "title": "A Physics-Informed Convolutional Long Short Term Memory Statistical Model for Fluid Thermodynamics Simulations", "categories": ["physics.flu-dyn", "cs.LG", "stat.ML"], "comment": null, "summary": "Fluid thermodynamics underpins atmospheric dynamics, climate science,\nindustrial applications, and energy systems. However, direct numerical\nsimulations (DNS) of such systems are computationally prohibitive. To address\nthis, we present a novel physics-informed spatio-temporal surrogate model for\nRayleigh-B\\'enard convection (RBC), a canonical example of convective fluid\nflow. Our approach combines convolutional neural networks for spatial feature\nextraction with an innovative recurrent architecture inspired by large language\nmodels, comprising a context builder and a sequence generator to capture\ntemporal dynamics. Inference is penalized with respect to the governing partial\ndifferential equations to ensure physical interpretability. Given the\nsensitivity of turbulent convection to initial conditions, we quantify\nuncertainty using a conformal prediction framework. This model replicates key\nfeatures of RBC dynamics while significantly reducing computational cost,\noffering a scalable alternative to DNS for long-term simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u65f6\u7a7a\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62dfRayleigh-B\u00e9nard\u5bf9\u6d41\uff08RBC\uff09\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u542f\u53d1\u7684\u5faa\u73af\u67b6\u6784\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u76f4\u63a5\u6570\u503c\u6a21\u62df\uff08DNS\uff09\u5728\u6d41\u4f53\u70ed\u529b\u5b66\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u542f\u53d1\u7684\u5faa\u73af\u67b6\u6784\uff08\u4e0a\u4e0b\u6587\u6784\u5efa\u5668\u548c\u5e8f\u5217\u751f\u6210\u5668\uff09\u6355\u6349\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u504f\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u63a8\u7406\u4ee5\u786e\u4fdd\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6a21\u578b\u6210\u529f\u590d\u73b0\u4e86RBC\u7684\u5173\u952e\u52a8\u6001\u7279\u5f81\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u957f\u671f\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684DNS\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.10942", "pdf": "https://arxiv.org/pdf/2505.10942", "abs": "https://arxiv.org/abs/2505.10942", "authors": ["Meghali Nandi", "Arash Shaghaghi", "Nazatul Haque Sultan", "Gustavo Batista", "Raymond K. Zhao", "Sanjay Jha"], "title": "Nosy Layers, Noisy Fixes: Tackling DRAs in Federated Learning Systems using Explainable AI", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted to AsiaCCS 2025", "summary": "Federated Learning (FL) has emerged as a powerful paradigm for collaborative\nmodel training while keeping client data decentralized and private. However, it\nis vulnerable to Data Reconstruction Attacks (DRA) such as \"LoKI\" and \"Robbing\nthe Fed\", where malicious models sent from the server to the client can\nreconstruct sensitive user data. To counter this, we introduce DRArmor, a novel\ndefense mechanism that integrates Explainable AI with targeted detection and\nmitigation strategies for DRA. Unlike existing defenses that focus on the\nentire model, DRArmor identifies and addresses the root cause (i.e., malicious\nlayers within the model that send gradients with malicious intent) by analyzing\ntheir contribution to the output and detecting inconsistencies in gradient\nvalues. Once these malicious layers are identified, DRArmor applies defense\ntechniques such as noise injection, pixelation, and pruning to these layers\nrather than the whole model, minimizing the attack surface and preserving\nclient data privacy. We evaluate DRArmor's performance against the advanced\nLoKI attack across diverse datasets, including MNIST, CIFAR-10, CIFAR-100, and\nImageNet, in a 200-client FL setup. Our results demonstrate DRArmor's\neffectiveness in mitigating data leakage, achieving high True Positive and True\nNegative Rates of 0.910 and 0.890, respectively. Additionally, DRArmor\nmaintains an average accuracy of 87%, effectively protecting client privacy\nwithout compromising model performance. Compared to existing defense\nmechanisms, DRArmor reduces the data leakage rate by 62.5% with datasets\ncontaining 500 samples per client.", "AI": {"tldr": "DRArmor\u662f\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u91cd\u6784\u653b\u51fb\uff08DRA\uff09\u7684\u65b0\u578b\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\u548c\u9488\u5bf9\u6027\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6613\u53d7\u6570\u636e\u91cd\u6784\u653b\u51fb\uff08\u5982LoKI\u548cRobbing the Fed\uff09\u5a01\u80c1\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u9632\u5fa1\u673a\u5236\u3002", "method": "DRArmor\u901a\u8fc7\u5206\u6790\u6076\u610f\u5c42\u5bf9\u8f93\u51fa\u7684\u8d21\u732e\u548c\u68af\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u8bc6\u522b\u5e76\u9488\u5bf9\u6027\u9632\u5fa1\uff08\u5982\u566a\u58f0\u6ce8\u5165\u3001\u50cf\u7d20\u5316\u548c\u526a\u679d\uff09\uff0c\u800c\u975e\u5168\u6a21\u578b\u9632\u5fa1\u3002", "result": "\u5728200\u5ba2\u6237\u7aef\u7684FL\u8bbe\u7f6e\u4e2d\uff0cDRArmor\u5bf9LoKI\u653b\u51fb\u6709\u6548\uff0c\u771f\u9633\u6027\u73870.910\uff0c\u771f\u9634\u6027\u73870.890\uff0c\u5e73\u5747\u51c6\u786e\u738787%\uff0c\u6570\u636e\u6cc4\u6f0f\u7387\u964d\u4f4e62.5%\u3002", "conclusion": "DRArmor\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u673a\u5236\u3002", "relevance": 40.0}}
{"id": "2505.11198", "pdf": "https://arxiv.org/pdf/2505.11198", "abs": "https://arxiv.org/abs/2505.11198", "authors": ["Jaime Ramirez Castillo", "M. Julia Flores", "Ann E. Nicholson"], "title": "User-centric Music Recommendations", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted for the 16th Bayesian Modelling Applications Workshop\n  (@UAI2022) (BMAW 2022)", "summary": "This work presents a user-centric recommendation framework, designed as a\npipeline with four distinct, connected, and customizable phases. These phases\nare intended to improve explainability and boost user engagement.\n  We have collected the historical Last.fm track playback records of a single\nuser over approximately 15 years. The collected dataset includes more than\n90,000 playbacks and approximately 14,000 unique tracks.\n  From track playback records, we have created a dataset of user temporal\ncontexts (each row is a specific moment when the user listened to certain music\ndescriptors). As music descriptors, we have used community-contributed Last.fm\ntags and Spotify audio features. They represent the music that, throughout\nyears, the user has been listening to.\n  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the\nday), we predict the Spotify audio features that best fit the user preferences\nin that particular moment. Finally, we use the predicted audio features to find\ntracks similar to these features. The final aim is to recommend (and discover)\ntracks that the user may feel like listening to at a particular moment.\n  For our initial study case, we have chosen to predict only a single audio\nfeature target: danceability. The framework, however, allows to include more\ntarget variables.\n  The ability to learn the musical habits from a single user can be quite\npowerful, and this framework could be extended to other users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u97f3\u4e50\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u53ef\u5b9a\u5236\u9636\u6bb5\u63d0\u9ad8\u89e3\u91ca\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u4e2a\u6027\u5316\u97f3\u4e50\u63a8\u8350\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528\u7528\u623715\u5e74\u7684Last.fm\u64ad\u653e\u8bb0\u5f55\uff0c\u7ed3\u5408Spotify\u97f3\u9891\u7279\u5f81\u548c\u793e\u533a\u6807\u7b7e\uff0c\u9884\u6d4b\u7528\u6237\u504f\u597d\u5e76\u63a8\u8350\u97f3\u4e50\u3002", "result": "\u6846\u67b6\u6210\u529f\u9884\u6d4b\u4e86\u7528\u6237\u504f\u597d\u7684\u97f3\u4e50\u7279\u5f81\uff08\u5982danceability\uff09\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u63a8\u8350\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u7528\u6237\uff0c\u5177\u6709\u6f5c\u529b\u63d0\u5347\u97f3\u4e50\u63a8\u8350\u7684\u4e2a\u6027\u5316\u548c\u89e3\u91ca\u6027\u3002", "relevance": 20.0}}
{"id": "2505.11006", "pdf": "https://arxiv.org/pdf/2505.11006", "abs": "https://arxiv.org/abs/2505.11006", "authors": ["Oskar Allerbo", "Thomas B. Sch\u00f6n"], "title": "Supervised Models Can Generalize Also When Trained on Random Label", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The success of unsupervised learning raises the question of whether also\nsupervised models can be trained without using the information in the output\n$y$. In this paper, we demonstrate that this is indeed possible. The key step\nis to formulate the model as a smoother, i.e. on the form $\\hat{f}=Sy$, and to\nconstruct the smoother matrix $S$ independently of $y$, e.g. by training on\nrandom labels. We present a simple model selection criterion based on the\ndistribution of the out-of-sample predictions and show that, in contrast to\ncross-validation, this criterion can be used also without access to $y$. We\ndemonstrate on real and synthetic data that $y$-free trained versions of linear\nand kernel ridge regression, smoothing splines, and neural networks perform\nsimilarly to their standard, $y$-based, versions and, most importantly,\nsignificantly better than random guessing.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.11014", "pdf": "https://arxiv.org/pdf/2505.11014", "abs": "https://arxiv.org/abs/2505.11014", "authors": ["Harsh Parikh", "Trang Quynh Nguyen", "Elizabeth A. Stuart", "Kara E. Rudolph", "Caleb H. Miles"], "title": "A Cautionary Tale on Integrating Studies with Disparate Outcome Measures for Causal Inference", "categories": ["stat.ME", "cs.LG", "econ.EM"], "comment": null, "summary": "Data integration approaches are increasingly used to enhance the efficiency\nand generalizability of studies. However, a key limitation of these methods is\nthe assumption that outcome measures are identical across datasets -- an\nassumption that often does not hold in practice. Consider the following opioid\nuse disorder (OUD) studies: the XBOT trial and the POAT study, both evaluating\nthe effect of medications for OUD on withdrawal symptom severity (not the\nprimary outcome of either trial). While XBOT measures withdrawal severity using\nthe subjective opiate withdrawal scale, POAT uses the clinical opiate\nwithdrawal scale. We analyze this realistic yet challenging setting where\noutcome measures differ across studies and where neither study records both\ntypes of outcomes. Our paper studies whether and when integrating studies with\ndisparate outcome measures leads to efficiency gains. We introduce three sets\nof assumptions -- with varying degrees of strength -- linking both outcome\nmeasures. Our theoretical and empirical results highlight a cautionary tale:\nintegration can improve asymptotic efficiency only under the strongest\nassumption linking the outcomes. However, misspecification of this assumption\nleads to bias. In contrast, a milder assumption may yield finite-sample\nefficiency gains, yet these benefits diminish as sample size increases. We\nillustrate these trade-offs via a case study integrating the XBOT and POAT\ndatasets to estimate the comparative effect of two medications for opioid use\ndisorder on withdrawal symptoms. By systematically varying the assumptions\nlinking the SOW and COW scales, we show potential efficiency gains and the\nrisks of bias. Our findings emphasize the need for careful assumption selection\nwhen fusing datasets with differing outcome measures, offering guidance for\nresearchers navigating this common challenge in modern data integration.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6570\u636e\u96c6\u6574\u5408\u4e2d\uff0c\u5f53\u7ed3\u679c\u6d4b\u91cf\u4e0d\u4e00\u81f4\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u5f3a\u5ea6\u7684\u5047\u8bbe\u94fe\u63a5\u7ed3\u679c\u6d4b\u91cf\uff0c\u4ee5\u63d0\u5347\u6548\u7387\uff0c\u4f46\u9700\u6743\u8861\u504f\u5dee\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u96c6\u6574\u5408\u4e2d\u7ed3\u679c\u6d4b\u91cf\u4e0d\u4e00\u81f4\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u4f55\u79cd\u5047\u8bbe\u4e0b\u6574\u5408\u80fd\u63d0\u5347\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u4e0d\u540c\u5f3a\u5ea6\u7684\u5047\u8bbe\u94fe\u63a5\u7ed3\u679c\u6d4b\u91cf\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u8bc4\u4f30\u6548\u7387\u548c\u504f\u5dee\u3002", "result": "\u6700\u5f3a\u5047\u8bbe\u4e0b\u6574\u5408\u53ef\u63d0\u5347\u6e10\u8fd1\u6548\u7387\uff0c\u4f46\u5047\u8bbe\u9519\u8bef\u4f1a\u5bfc\u81f4\u504f\u5dee\uff1b\u8f83\u5f31\u5047\u8bbe\u53ef\u80fd\u5728\u6709\u9650\u6837\u672c\u4e2d\u63d0\u5347\u6548\u7387\uff0c\u4f46\u968f\u6837\u672c\u589e\u5927\u6536\u76ca\u51cf\u5c11\u3002", "conclusion": "\u6574\u5408\u4e0d\u540c\u7ed3\u679c\u6d4b\u91cf\u7684\u6570\u636e\u96c6\u9700\u8c28\u614e\u9009\u62e9\u5047\u8bbe\uff0c\u6743\u8861\u6548\u7387\u4e0e\u504f\u5dee\u3002", "relevance": 30.0}}
{"id": "2505.11025", "pdf": "https://arxiv.org/pdf/2505.11025", "abs": "https://arxiv.org/abs/2505.11025", "authors": ["Naqueeb Ahmad Warsi", "Ayanava Dasgupta", "Masahito Hayashi"], "title": "Generalization Bounds for Quantum Learning via R\u00e9nyi Divergences", "categories": ["quant-ph", "cs.IT", "cs.LG", "math.IT"], "comment": "36 pages, 2 figures", "summary": "This work advances the theoretical understanding of quantum learning by\nestablishing a new family of upper bounds on the expected generalization error\nof quantum learning algorithms, leveraging the framework introduced by Caro et\nal. (2024) and a new definition for the expected true loss. Our primary\ncontribution is the derivation of these bounds in terms of quantum and\nclassical R\\'enyi divergences, utilizing a variational approach for evaluating\nquantum R\\'enyi divergences, specifically the Petz and a newly introduced\nmodified sandwich quantum R\\'enyi divergence. Analytically and numerically, we\ndemonstrate the superior performance of the bounds derived using the modified\nsandwich quantum R\\'enyi divergence compared to those based on the Petz\ndivergence. Furthermore, we provide probabilistic generalization error bounds\nusing two distinct techniques: one based on the modified sandwich quantum\nR\\'enyi divergence and classical R\\'enyi divergence, and another employing\nsmooth max R\\'enyi divergence.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5efa\u7acb\u91cf\u5b50\u5b66\u4e60\u7b97\u6cd5\u671f\u671b\u6cdb\u5316\u8bef\u5dee\u7684\u65b0\u4e0a\u754c\u5bb6\u65cf\uff0c\u63a8\u8fdb\u4e86\u91cf\u5b50\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5229\u7528\u4e86Caro\u7b49\u4eba\uff082024\uff09\u7684\u6846\u67b6\u548c\u65b0\u7684\u671f\u671b\u771f\u5b9e\u635f\u5931\u5b9a\u4e49\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\uff0c\u4ee5\u63d0\u5347\u91cf\u5b50\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5229\u7528\u91cf\u5b50\u4e0e\u7ecf\u5178R\u00e9nyi\u6563\u5ea6\uff0c\u91c7\u7528\u53d8\u5206\u65b9\u6cd5\u8bc4\u4f30\u91cf\u5b50R\u00e9nyi\u6563\u5ea6\uff08\u5305\u62ecPetz\u548c\u65b0\u5f15\u5165\u7684\u4fee\u6b63\u4e09\u660e\u6cbb\u91cf\u5b50R\u00e9nyi\u6563\u5ea6\uff09\uff0c\u63a8\u5bfc\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\u3002", "result": "\u4fee\u6b63\u4e09\u660e\u6cbb\u91cf\u5b50R\u00e9nyi\u6563\u5ea6\u5728\u5206\u6790\u548c\u6570\u503c\u4e0a\u5747\u4f18\u4e8ePetz\u6563\u5ea6\uff0c\u4e14\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4e24\u79cd\u6280\u672f\u7684\u6982\u7387\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\u3002", "conclusion": "\u4fee\u6b63\u4e09\u660e\u6cbb\u91cf\u5b50R\u00e9nyi\u6563\u5ea6\u5728\u91cf\u5b50\u5b66\u4e60\u6cdb\u5316\u8bef\u5dee\u5206\u6790\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u91cf\u5b50\u5b66\u4e60\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 10.0}}
{"id": "2505.11270", "pdf": "https://arxiv.org/pdf/2505.11270", "abs": "https://arxiv.org/abs/2505.11270", "authors": ["Chao Zhang", "Shaolei Zhang", "Quehuan Liu", "Sibei Chen", "Tong Li", "Ju Fan"], "title": "TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "The variety of data in data lakes presents significant challenges for data\nanalytics, as data scientists must simultaneously analyze multi-modal data,\nincluding structured, semi-structured, and unstructured data. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities, they still\nremain inadequate for multi-modal data analytics in terms of accuracy,\nefficiency, and freshness. First, current natural language (NL) or SQL-like\nquery languages may struggle to precisely and comprehensively capture users'\nanalytical intent. Second, relying on a single unified LLM to process diverse\ndata modalities often leads to substantial inference overhead. Third, data\nstored in data lakes may be incomplete or outdated, making it essential to\nintegrate external open-domain knowledge to generate timely and relevant\nanalytics results.\n  In this paper, we envision a new multi-modal data analytics system.\nSpecifically, we propose a novel architecture built upon the Model Context\nProtocol (MCP), an emerging paradigm that enables LLMs to collaborate with\nknowledgeable agents. First, we define a semantic operator hierarchy tailored\nfor querying multi-modal data in data lakes and develop an AI-agent-powered\nNL2Operator translator to bridge user intent and analytical execution. Next, we\nintroduce an MCP-based execution framework, in which each MCP server hosts\nspecialized foundation models optimized for specific data modalities. This\ndesign enhances both accuracy and efficiency, while supporting high scalability\nthrough modular deployment. Finally, we propose a updating mechanism by\nharnessing the deep research and machine unlearning techniques to refresh the\ndata lakes and LLM knowledges, with the goal of balancing the data freshness\nand inference efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u65b0\u578b\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7AI\u4ee3\u7406\u548c\u4e13\u4e1a\u5316\u57fa\u7840\u6a21\u578b\u534f\u4f5c\uff0c\u63d0\u5347\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u4e2dLLM\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7528\u6237\u610f\u56fe\u6355\u83b7\u548c\u63a8\u7406\u5f00\u9500\u7684\u95ee\u9898\u3002", "method": "1. \u5b9a\u4e49\u8bed\u4e49\u64cd\u4f5c\u7b26\u5c42\u6b21\u7ed3\u6784\uff0c\u5f00\u53d1AI\u4ee3\u7406\u9a71\u52a8\u7684NL2Operator\u7ffb\u8bd1\u5668\uff1b2. \u57fa\u4e8eMCP\u7684\u6267\u884c\u6846\u67b6\uff0c\u90e8\u7f72\u4e13\u4e1a\u5316\u57fa\u7840\u6a21\u578b\uff1b3. \u63d0\u51fa\u6570\u636e\u66f4\u65b0\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u7814\u7a76\u548c\u673a\u5668\u9057\u5fd8\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u652f\u6301\u6570\u636e\u65b0\u9c9c\u5ea6\u4e0e\u63a8\u7406\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "MCP\u67b6\u6784\u4e3a\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.11053", "pdf": "https://arxiv.org/pdf/2505.11053", "abs": "https://arxiv.org/abs/2505.11053", "authors": ["Adrian Kazakov", "Anna Milillo", "Alessandro Mura", "Stavro Ivanovski", "Valeria Mangano", "Alessandro Aronica", "Elisabetta De Angelis", "Pier Paolo Di Bartolomeo", "Alessandro Brin", "Luca Colasanti", "Miguel Escalona-Moran", "Francesco Lazzarotto", "Stefano Massetti", "Martina Moroni", "Raffaella Noschese", "Fabrizio Nuccilli", "Stefano Orsini", "Christina Plainaki", "Rosanna Rispoli", "Roberto Sordini", "Mirko Stumpo", "Nello Vertolli"], "title": "Conceptual framework for the application of deep neural networks to surface composition reconstruction from Mercury's exospheric data", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.LG"], "comment": "All versions of this article can be explored in the collection: DOI\n  https://doi.org/10.5281/zenodo.15394849 . This article is identical to v2.5\n  of the aforementioned collection: DOI https://doi.org/10.5281/zenodo.15425584", "summary": "Surface information derived from exospheric measurements at planetary bodies\ncomplements surface mapping provided by dedicated imagers, offering critical\ninsights into surface release processes, interactions within the planetary\nenvironment, space weathering, and planetary evolution. This study explores the\nfeasibility of deriving Mercury's regolith elemental composition from in-situ\nmeasurements of its neutral exosphere using deep neural networks (DNNs). We\npresent a supervised feed-forward DNN architecture - a multilayer perceptron\n(MLP) - that, starting from exospheric densities and proton precipitation\nfluxes, predicts the chemical elements of the surface regolith below. It serves\nas an estimator for the surface-exosphere interaction and the processes leading\nto exosphere formation. Because the DNN requires a comprehensive exospheric\ndataset not available from previous missions, this study uses simulated\nexosphere components and simulated drivers. Extensive training and testing\ncampaigns demonstrate the MLP's ability to accurately predict and reconstruct\nsurface composition maps from these simulated measurements. Although this\ninitial version does not aim to reproduce Mercury's actual surface composition,\nit provides a proof of concept, showcasing the algorithm's robustness and\ncapacity for handling complex datasets to create estimators for exospheric\ngeneration models. Moreover, our tests reveal substantial potential for further\ndevelopment, suggesting that this method could significantly enhance the\nanalysis of complex surface-exosphere interactions and complement planetary\nexosphere models. This work anticipates applying the approach to data from the\nBepiColombo mission, specifically the SERENA package, whose nominal phase\nbegins in 2027.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4ece\u6c34\u661f\u4e2d\u6027\u5916\u9038\u5c42\u7684\u539f\u4f4d\u6d4b\u91cf\u6570\u636e\u4e2d\u63a8\u5bfc\u5176\u8868\u9762\u5143\u7d20\u7ec4\u6210\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5916\u9038\u5c42\u6d4b\u91cf\u6570\u636e\u8865\u5145\u8868\u9762\u6210\u50cf\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u8868\u9762\u91ca\u653e\u8fc7\u7a0b\u3001\u884c\u661f\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u53ca\u6f14\u5316\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u4ece\u6a21\u62df\u7684\u5916\u9038\u5c42\u5bc6\u5ea6\u548c\u8d28\u5b50\u964d\u6c34\u901a\u91cf\u6570\u636e\u4e2d\u9884\u6d4b\u8868\u9762\u5143\u7d20\u7ec4\u6210\u3002", "result": "MLP\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u548c\u91cd\u5efa\u6a21\u62df\u6570\u636e\u4e2d\u7684\u8868\u9762\u7ec4\u6210\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u5904\u7406\u590d\u6742\u6570\u636e\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u6790\u590d\u6742\u8868\u9762-\u5916\u9038\u5c42\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5e76\u6709\u671b\u5e94\u7528\u4e8e\u672a\u6765\u7684BepiColombo\u4efb\u52a1\u6570\u636e\u3002", "relevance": 20.0}}
{"id": "2505.11275", "pdf": "https://arxiv.org/pdf/2505.11275", "abs": "https://arxiv.org/abs/2505.11275", "authors": ["Pengju Xu", "Yan Wang", "Shuyuan Zhang", "Xuan Zhou", "Xin Li", "Yue Yuan", "Fengzhao Li", "Shunyuan Zhou", "Xingyu Wang", "Yi Zhang", "Haiying Zhao"], "title": "TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs", "categories": ["cs.MM", "cs.AI", "cs.CY"], "comment": "Preprint", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the ability of artificial intelligence systems to\nunderstand and generate multimodal content. However, these models often exhibit\nlimited effectiveness when applied to non-Western cultural contexts, which\nraises concerns about their wider applicability. To address this limitation, we\npropose the \\textbf{T}raditional \\textbf{C}hinese \\textbf{C}ulture\nunderstanding \\textbf{Bench}mark (\\textbf{TCC-Bench}), a bilingual\n(\\textit{i.e.}, Chinese and English) Visual Question Answering (VQA) benchmark\nspecifically designed for assessing the understanding of traditional Chinese\nculture by MLLMs. TCC-Bench comprises culturally rich and visually diverse\ndata, incorporating images from museum artifacts, everyday life scenes, comics,\nand other culturally significant contexts. We adopt a semi-automated pipeline\nthat utilizes GPT-4o in text-only mode to generate candidate questions,\nfollowed by human curation to ensure data quality and avoid potential data\nleakage. The benchmark also avoids language bias by preventing direct\ndisclosure of cultural concepts within question texts. Experimental evaluations\nacross a wide range of MLLMs demonstrate that current models still face\nsignificant challenges when reasoning about culturally grounded visual content.\nThe results highlight the need for further research in developing culturally\ninclusive and context-aware multimodal systems. The code and data can be found\nat: https://github.com/Morty-Xu/TCC-Bench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TCC-Bench\uff0c\u4e00\u4e2a\u53cc\u8bed\uff08\u4e2d\u82f1\uff09\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4f20\u7edf\u4e2d\u56fd\u6587\u5316\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u897f\u65b9\u6587\u5316\u80cc\u666f\u4e0b\u8868\u73b0\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5229\u7528GPT-4\u751f\u6210\u5019\u9009\u95ee\u9898\uff0c\u5e76\u7ecf\u8fc7\u4eba\u5de5\u7b5b\u9009\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u76f8\u5173\u7684\u89c6\u89c9\u5185\u5bb9\u63a8\u7406\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u6587\u5316\u5305\u5bb9\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u3002", "relevance": 75.0}}
{"id": "2505.11089", "pdf": "https://arxiv.org/pdf/2505.11089", "abs": "https://arxiv.org/abs/2505.11089", "authors": ["Yiran Yang", "Rui Chen"], "title": "Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "In this paper, we consider a score-based Integer Programming (IP) approach\nfor solving the Bayesian Network Structure Learning (BNSL) problem.\nState-of-the-art BNSL IP formulations suffer from the exponentially large\nnumber of variables and constraints. A standard approach in IP to address such\nchallenges is to employ row and column generation techniques, which dynamically\ngenerate rows and columns, while the complex pricing problem remains a\ncomputational bottleneck for BNSL. For the general class of $\\ell_0$-penalized\nlikelihood scores, we show how the pricing problem can be reformulated as a\ndifference of submodular optimization problem, and how the Difference of Convex\nAlgorithm (DCA) can be applied as an inexact method to efficiently solve the\npricing problems. Empirically, we show that, for continuous Gaussian data, our\nrow and column generation approach yields solutions with higher quality than\nstate-of-the-art score-based approaches, especially when the graph density\nincreases, and achieves comparable performance against benchmark\nconstraint-based and hybrid approaches, even when the graph size increases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u89c4\u5212\uff08IP\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\uff08BNSL\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u884c\u548c\u5217\u6765\u5e94\u5bf9\u53d8\u91cf\u548c\u7ea6\u675f\u7684\u6307\u6570\u7ea7\u589e\u957f\u95ee\u9898\uff0c\u5e76\u5229\u7528\u5b50\u6a21\u4f18\u5316\u548c\u51f8\u5dee\u7b97\u6cd5\uff08DCA\uff09\u9ad8\u6548\u89e3\u51b3\u5b9a\u4ef7\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684BNSL IP\u65b9\u6cd5\u56e0\u53d8\u91cf\u548c\u7ea6\u675f\u7684\u6307\u6570\u7ea7\u589e\u957f\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5b9a\u4ef7\u95ee\u9898\u63d0\u9ad8\u6c42\u89e3\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u884c\u548c\u5217\u751f\u6210\u6280\u672f\uff0c\u5c06\u5b9a\u4ef7\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5b50\u6a21\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5e94\u7528DCA\u4f5c\u4e3a\u8fd1\u4f3c\u6c42\u89e3\u65b9\u6cd5\u3002", "result": "\u5728\u8fde\u7eed\u9ad8\u65af\u6570\u636e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u5f62\u5bc6\u5ea6\u589e\u52a0\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u56fe\u5f62\u89c4\u6a21\u589e\u5927\u65f6\u4e0e\u57fa\u51c6\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728BNSL\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6c42\u89e3\u8d28\u91cf\u548c\u6548\u7387\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u3002", "relevance": 30.0}}
{"id": "2505.11311", "pdf": "https://arxiv.org/pdf/2505.11311", "abs": "https://arxiv.org/abs/2505.11311", "authors": ["Ardian Selmonaj", "Alessandro Antonucci", "Adrian Schneider", "Michael R\u00fcegsegger", "Matthias Sommer"], "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Published as a journal chapter in NATO Journal of Science and\n  Technology", "summary": "Artificial intelligence (AI) is reshaping strategic planning, with\nMulti-Agent Reinforcement Learning (MARL) enabling coordination among\nautonomous agents in complex scenarios. However, its practical deployment in\nsensitive military contexts is constrained by the lack of explainability, which\nis an essential factor for trust, safety, and alignment with human strategies.\nThis work reviews and assesses current advances in explainability methods for\nMARL with a focus on simulated air combat scenarios. We proceed by adapting\nvarious explainability techniques to different aerial combat scenarios to gain\nexplanatory insights about the model behavior. By linking AI-generated tactics\nwith human-understandable reasoning, we emphasize the need for transparency to\nensure reliable deployment and meaningful human-machine interaction. By\nilluminating the crucial importance of explainability in advancing MARL for\noperational defense, our work supports not only strategic planning but also the\ntraining of military personnel with insightful and comprehensible analyses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u519b\u4e8b\u6218\u7565\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u6a21\u62df\u7a7a\u6218\u573a\u666f\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u63d0\u9ad8\u4fe1\u4efb\u548c\u5b89\u5168\u6027\u3002", "motivation": "MARL\u5728\u654f\u611f\u519b\u4e8b\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u9650\u5236\uff0c\u800c\u53ef\u89e3\u91ca\u6027\u662f\u786e\u4fdd\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u4e0e\u4eba\u7c7b\u7b56\u7565\u4e00\u81f4\u7684\u5173\u952e\u3002", "method": "\u8bba\u6587\u56de\u987e\u5e76\u8bc4\u4f30\u4e86\u5f53\u524dMARL\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e0d\u540c\u7a7a\u6218\u573a\u666f\uff0c\u4ee5\u83b7\u53d6\u6a21\u578b\u884c\u4e3a\u7684\u89e3\u91ca\u6027\u89c1\u89e3\u3002", "result": "\u901a\u8fc7\u5c06AI\u751f\u6210\u7684\u6218\u672f\u4e0e\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u63a8\u7406\u8054\u7cfb\u8d77\u6765\uff0c\u5f3a\u8c03\u4e86\u900f\u660e\u6027\u5bf9\u53ef\u9760\u90e8\u7f72\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u53ef\u89e3\u91ca\u6027\u5728\u63a8\u52a8MARL\u7528\u4e8e\u9632\u5fa1\u64cd\u4f5c\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u652f\u6301\u6218\u7565\u89c4\u5212\u548c\u519b\u4e8b\u4eba\u5458\u57f9\u8bad\u3002", "relevance": 60.0}}
{"id": "2505.11340", "pdf": "https://arxiv.org/pdf/2505.11340", "abs": "https://arxiv.org/abs/2505.11340", "authors": ["Zeyu Gao", "Yuxin Cui", "Hao Wang", "Siliang Qin", "Yuanda Wang", "Bolun Zhang", "Chao Zhang"], "title": "DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Decompilers are fundamental tools for critical security tasks, from\nvulnerability discovery to malware analysis, yet their evaluation remains\nfragmented. Existing approaches primarily focus on syntactic correctness\nthrough synthetic micro-benchmarks or subjective human ratings, failing to\naddress real-world requirements for semantic fidelity and analyst usability. We\npresent DecompileBench, the first comprehensive framework that enables\neffective evaluation of decompilers in reverse engineering workflows through\nthree key components: \\textit{real-world function extraction} (comprising\n23,400 functions from 130 real-world programs), \\textit{runtime-aware\nvalidation}, and \\textit{automated human-centric assessment} using LLM-as-Judge\nto quantify the effectiveness of decompilers in reverse engineering workflows.\nThrough a systematic comparison between six industrial-strength decompilers and\nsix recent LLM-powered approaches, we demonstrate that LLM-based methods\nsurpass commercial tools in code understandability despite 52.2% lower\nfunctionality correctness. These findings highlight the potential of LLM-based\napproaches to transform human-centric reverse engineering. We open source\n\\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a\nframework to advance research on decompilers and assist security experts in\nmaking informed tool selections based on their specific requirements.", "AI": {"tldr": "DecompileBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u53cd\u7f16\u8bd1\u5668\u7684\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u51fd\u6570\u63d0\u53d6\u3001\u8fd0\u884c\u65f6\u611f\u77e5\u9a8c\u8bc1\u548c\u81ea\u52a8\u5316\u4eba\u672c\u8bc4\u4f30\uff08\u4f7f\u7528LLM-as-Judge\uff09\uff0c\u53d1\u73b0\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u4e0a\u4f18\u4e8e\u5546\u4e1a\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u53cd\u7f16\u8bd1\u5668\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u788e\u7247\u5316\uff0c\u4e3b\u8981\u5173\u6ce8\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5206\u6790\u5e08\u53ef\u7528\u6027\u7b49\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u63d0\u51faDecompileBench\u6846\u67b6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u51fd\u6570\u63d0\u53d6\u3001\u8fd0\u884c\u65f6\u611f\u77e5\u9a8c\u8bc1\u548cLLM-as-Judge\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u4e0a\u4f18\u4e8e\u5546\u4e1a\u5de5\u5177\uff0c\u5c3d\u7ba1\u529f\u80fd\u6b63\u786e\u6027\u4f4e52.2%\u3002", "conclusion": "LLM\u65b9\u6cd5\u6709\u671b\u6539\u53d8\u4eba\u672c\u9006\u5411\u5de5\u7a0b\uff0cDecompileBench\u5f00\u6e90\u4ee5\u63a8\u52a8\u53cd\u7f16\u8bd1\u5668\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.11143", "pdf": "https://arxiv.org/pdf/2505.11143", "abs": "https://arxiv.org/abs/2505.11143", "authors": ["William R. P. Denault"], "title": "Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Sparse linear regression is a fundamental tool in data analysis. However,\ntraditional approaches often fall short when covariates exhibit structure or\narise from heterogeneous sources. In biomedical applications, covariates may\nstem from distinct modalities or be structured according to an underlying\ngraph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that\nintegrates covariate-specific side information into sparse regression via\nneural networks. Nash adaptively modulates penalties on a per-covariate basis,\nlearning to tailor regularization without cross-validation. We develop a\nvariational inference algorithm for efficient training and establish\nconnections to empirical Bayes regression. Experiments on real data demonstrate\nthat Nash can improve accuracy and adaptability over existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNash\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5c06\u534f\u53d8\u91cf\u7279\u5b9a\u7684\u8f85\u52a9\u4fe1\u606f\u6574\u5408\u5230\u7a00\u758f\u56de\u5f52\u4e2d\uff0c\u81ea\u9002\u5e94\u5730\u8c03\u8282\u60e9\u7f5a\u9879\uff0c\u65e0\u9700\u4ea4\u53c9\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u7ed3\u6784\u6216\u6765\u81ea\u5f02\u8d28\u6765\u6e90\u7684\u534f\u53d8\u91cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u3002", "method": "\u5f15\u5165Neural Adaptive Shrinkage (Nash)\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u81ea\u9002\u5e94\u5730\u8c03\u8282\u6bcf\u4e2a\u534f\u53d8\u91cf\u7684\u60e9\u7f5a\u9879\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u53d8\u5206\u63a8\u65ad\u7b97\u6cd5\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNash\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "Nash\u4e3a\u7a00\u758f\u56de\u5f52\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u534f\u53d8\u91cf\u7684\u8f85\u52a9\u4fe1\u606f\u3002", "relevance": 40.0}}
{"id": "2505.11416", "pdf": "https://arxiv.org/pdf/2505.11416", "abs": "https://arxiv.org/abs/2505.11416", "authors": ["Pouya Shaeri", "Ariane Middel"], "title": "MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "Submitted in a Computer Science Conference, currently in Review", "summary": "Modern neural networks often activate all neurons for every input, leading to\nunnecessary computation and inefficiency. We introduce Matrix-Interpolated\nDropout Layer (MID-L), a novel module that dynamically selects and activates\nonly the most informative neurons by interpolating between two transformation\npaths via a learned, input-dependent gating vector. Unlike conventional dropout\nor static sparsity methods, MID-L employs a differentiable Top-k masking\nstrategy, enabling per-input adaptive computation while maintaining end-to-end\ndifferentiability. MID-L is model-agnostic and integrates seamlessly into\nexisting architectures. Extensive experiments on six benchmarks, including\nMNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves\nup to average 55\\% reduction in active neurons, 1.7$\\times$ FLOPs savings, and\nmaintains or exceeds baseline accuracy. We further validate the informativeness\nand selectivity of the learned neurons via Sliced Mutual Information (SMI) and\nobserve improved robustness under overfitting and noisy data conditions.\nAdditionally, MID-L demonstrates favorable inference latency and memory usage\nprofiles, making it suitable for both research exploration and deployment on\ncompute-constrained systems. These results position MID-L as a general-purpose,\nplug-and-play dynamic computation layer, bridging the gap between dropout\nregularization and efficient inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u795e\u7ecf\u5143\u7684\u6a21\u5757MID-L\uff0c\u901a\u8fc7\u8f93\u5165\u76f8\u5173\u7684\u95e8\u63a7\u5411\u91cf\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6fc0\u6d3b\u6240\u6709\u795e\u7ecf\u5143\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\u3002MID-L\u65e8\u5728\u52a8\u6001\u9009\u62e9\u6700\u6709\u4fe1\u606f\u7684\u795e\u7ecf\u5143\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "MID-L\u91c7\u7528\u53ef\u5fae\u5206\u7684Top-k\u63a9\u7801\u7b56\u7565\uff0c\u52a8\u6001\u9009\u62e9\u795e\u7ecf\u5143\uff0c\u5e76\u96c6\u6210\u5230\u73b0\u6709\u67b6\u6784\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMID-L\u5e73\u5747\u51cf\u5c1155%\u7684\u6d3b\u8dc3\u795e\u7ecf\u5143\uff0c\u8282\u77011.7\u500dFLOPs\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8fc7\u57fa\u7ebf\u51c6\u786e\u6027\u3002", "conclusion": "MID-L\u662f\u4e00\u79cd\u901a\u7528\u3001\u5373\u63d2\u5373\u7528\u7684\u52a8\u6001\u8ba1\u7b97\u5c42\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u53d7\u9650\u7684\u7cfb\u7edf\u3002", "relevance": 70.0}}
{"id": "2505.11417", "pdf": "https://arxiv.org/pdf/2505.11417", "abs": "https://arxiv.org/abs/2505.11417", "authors": ["Patryk Bartkowiak", "Michal Podstawski"], "title": "EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces a novel dataset and evaluation benchmark designed to\nassess and improve small language models deployable on edge devices, with a\nfocus on user profiling from multi-session natural language interactions in\nsmart home environments. At the core of the dataset are structured user\nprofiles, each defined by a set of routines - context-triggered, repeatable\npatterns of behavior that govern how users interact with their home systems.\nUsing these profiles as input, a large language model (LLM) generates\ncorresponding interaction sessions that simulate realistic, diverse, and\ncontext-aware dialogues between users and their devices.\n  The primary task supported by this dataset is profile reconstruction:\ninferring user routines and preferences solely from interactions history. To\nassess how well current models can perform this task under realistic\nconditions, we benchmarked several state-of-the-art compact language models and\ncompared their performance against large foundation models. Our results show\nthat while small models demonstrate some capability in reconstructing profiles,\nthey still fall significantly short of large models in accurately capturing\nuser behavior. This performance gap poses a major challenge - particularly\nbecause on-device processing offers critical advantages, such as preserving\nuser privacy, minimizing latency, and enabling personalized experiences without\nreliance on the cloud. By providing a realistic, structured testbed for\ndeveloping and evaluating behavioral modeling under these constraints, our\ndataset represents a key step toward enabling intelligent, privacy-respecting\nAI systems that learn and adapt directly on user-owned devices.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u4ece\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u7684\u591a\u4f1a\u8bdd\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e2d\u91cd\u5efa\u7528\u6237\u753b\u50cf\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6a21\u62df\u4ea4\u4e92\u4f1a\u8bdd\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7528\u6237\u753b\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u5c0f\u578b\u6a21\u578b\u5728\u753b\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728\u91cd\u5efa\u7528\u6237\u753b\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u663e\u8457\u843d\u540e\u4e8e\u5927\u578b\u6a21\u578b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7684\u8fb9\u7f18AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u6027\u80fd\u4ecd\u9700\u63d0\u5347\u3002", "relevance": 60.0}}
{"id": "2505.11228", "pdf": "https://arxiv.org/pdf/2505.11228", "abs": "https://arxiv.org/abs/2505.11228", "authors": ["Derrick Gilchrist Edward Manoharan", "Anubha Goel", "Alexandros Iosifidis", "Henri Hansen", "Juho Kanniainen"], "title": "Learning hidden cascades via classification", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "The spreading dynamics in social networks are often studied under the\nassumption that individuals' statuses, whether informed or infected, are fully\nobservable. However, in many real-world situations, such statuses remain\nunobservable, which is crucial for determining an individual's potential to\nfurther spread the infection. While this final status is hidden, intermediate\nindicators such as symptoms of infection are observable and provide important\ninsights into the spread process. We propose a partial observability-aware\nMachine Learning framework to learn the characteristics of the spreading model.\nWe term the method Distribution Classification, which utilizes the power of\nclassifiers to infer the underlying transmission dynamics. We evaluate our\nmethod on two types of synthetic networks and extend the study to a real-world\ninsider trading network. Results show that the method performs well, especially\non complex networks with high cyclic connectivity, supporting its utility in\nanalyzing real-world spreading phenomena where direct observation of individual\nstatuses is not possible.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff08Distribution Classification\uff09\uff0c\u7528\u4e8e\u63a8\u65ad\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u52a8\u6001\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u4e2a\u4f53\u7684\u611f\u67d3\u6216\u4fe1\u606f\u4f20\u64ad\u72b6\u6001\u5f80\u5f80\u4e0d\u53ef\u76f4\u63a5\u89c2\u6d4b\uff0c\u4f46\u4e2d\u95f4\u6307\u6807\uff08\u5982\u75c7\u72b6\uff09\u53ef\u89c2\u6d4b\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u6307\u6807\u63a8\u65ad\u4f20\u64ad\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u4e86Distribution Classification\u65b9\u6cd5\uff0c\u5229\u7528\u5206\u7c7b\u5668\u7684\u80fd\u529b\u63a8\u65ad\u6f5c\u5728\u7684\u4f20\u64ad\u52a8\u6001\u3002", "result": "\u65b9\u6cd5\u5728\u5408\u6210\u7f51\u7edc\u548c\u771f\u5b9e\u5185\u5e55\u4ea4\u6613\u7f51\u7edc\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u5728\u5177\u6709\u9ad8\u5faa\u73af\u8fde\u901a\u6027\u7684\u590d\u6742\u7f51\u7edc\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5206\u6790\u65e0\u6cd5\u76f4\u63a5\u89c2\u6d4b\u4e2a\u4f53\u72b6\u6001\u7684\u73b0\u5b9e\u4f20\u64ad\u73b0\u8c61\u3002", "relevance": 30.0}}
{"id": "2505.11237", "pdf": "https://arxiv.org/pdf/2505.11237", "abs": "https://arxiv.org/abs/2505.11237", "authors": ["Wenhao Qian", "Zhenzhen Hu", "Zijie Song", "Jia Li"], "title": "Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor Identification", "categories": ["cs.MM", "cs.LG"], "comment": "ICMR'25, June 30-July 3, 2025, Chicago, IL, USA", "summary": "Metaphorical imagination, the ability to connect seemingly unrelated\nconcepts, is fundamental to human cognition and communication. While\nunderstanding linguistic metaphors has advanced significantly, grasping\nmultimodal metaphors, such as those found in internet memes, presents unique\nchallenges due to their unconventional expressions and implied meanings.\nExisting methods for multimodal metaphor identification often struggle to\nbridge the gap between literal and figurative interpretations. Additionally,\ngenerative approaches that utilize large language models or text-to-image\nmodels, while promising, suffer from high computational costs. This paper\nintroduces \\textbf{C}oncept \\textbf{D}rift \\textbf{G}uided \\textbf{L}ayerNorm\n\\textbf{T}uning (\\textbf{CDGLT}), a novel and training-efficient framework for\nmultimodal metaphor identification. CDGLT incorporates two key innovations: (1)\nConcept Drift, a mechanism that leverages Spherical Linear Interpolation\n(SLERP) of cross-modal embeddings from a CLIP encoder to generate a new,\ndivergent concept embedding. This drifted concept helps to alleviate the gap\nbetween literal features and the figurative task. (2) A prompt construction\nstrategy, that adapts the method of feature extraction and fusion using\npre-trained language models for the multimodal metaphor identification task.\nCDGLT achieves state-of-the-art performance on the MET-Meme benchmark while\nsignificantly reducing training costs compared to existing generative methods.\nAblation studies demonstrate the effectiveness of both Concept Drift and our\nadapted LN Tuning approach. Our method represents a significant step towards\nefficient and accurate multimodal metaphor understanding. The code is\navailable:\n\\href{https://github.com/Qianvenh/CDGLT}{https://github.com/Qianvenh/CDGLT}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDGLT\u7684\u9ad8\u6548\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u9690\u55bb\u8bc6\u522b\uff0c\u901a\u8fc7\u6982\u5ff5\u6f02\u79fb\u548c\u63d0\u793a\u6784\u9020\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u591a\u6a21\u6001\u9690\u55bb\uff08\u5982\u7f51\u7edc\u8ff7\u56e0\uff09\u56e0\u5176\u975e\u4f20\u7edf\u8868\u8fbe\u548c\u9690\u542b\u610f\u4e49\u800c\u96be\u4ee5\u8bc6\u522b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5b57\u9762\u548c\u6bd4\u55bb\u89e3\u91ca\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u4e14\u751f\u6210\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "CDGLT\u7ed3\u5408\u6982\u5ff5\u6f02\u79fb\uff08\u5229\u7528CLIP\u7f16\u7801\u5668\u7684\u8de8\u6a21\u6001\u5d4c\u5165\u751f\u6210\u65b0\u6982\u5ff5\uff09\u548c\u63d0\u793a\u6784\u9020\u7b56\u7565\uff08\u8c03\u6574\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u65b9\u6cd5\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5728MET-Meme\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "CDGLT\u4e3a\u9ad8\u6548\u51c6\u786e\u7684\u591a\u6a21\u6001\u9690\u55bb\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.11449", "pdf": "https://arxiv.org/pdf/2505.11449", "abs": "https://arxiv.org/abs/2505.11449", "authors": ["Nicholas Carlini", "Milad Nasr", "Edoardo Debenedetti", "Barry Wang", "Christopher A. Choquette-Choo", "Daphne Ippolito", "Florian Tram\u00e8r", "Matthew Jagielski"], "title": "LLMs unlock new paths to monetizing exploits", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We argue that Large language models (LLMs) will soon alter the economics of\ncyberattacks. Instead of attacking the most commonly used software and\nmonetizing exploits by targeting the lowest common denominator among victims,\nLLMs enable adversaries to launch tailored attacks on a user-by-user basis. On\nthe exploitation front, instead of human attackers manually searching for one\ndifficult-to-identify bug in a product with millions of users, LLMs can find\nthousands of easy-to-identify bugs in products with thousands of users. And on\nthe monetization front, instead of generic ransomware that always performs the\nsame attack (encrypt all your data and request payment to decrypt), an\nLLM-driven ransomware attack could tailor the ransom demand based on the\nparticular content of each exploited device.\n  We show that these two attacks (and several others) are imminently practical\nusing state-of-the-art LLMs. For example, we show that without any human\nintervention, an LLM finds highly sensitive personal information in the Enron\nemail dataset (e.g., an executive having an affair with another employee) that\ncould be used for blackmail. While some of our attacks are still too expensive\nto scale widely today, the incentives to implement these attacks will only\nincrease as LLMs get cheaper. Thus, we argue that LLMs create a need for new\ndefense-in-depth approaches.", "AI": {"tldr": "LLMs\u5c06\u6539\u53d8\u7f51\u7edc\u653b\u51fb\u7684\u7ecf\u6d4e\u5b66\uff0c\u4f7f\u653b\u51fb\u8005\u80fd\u591f\u9488\u5bf9\u6bcf\u4e2a\u7528\u6237\u5b9a\u5236\u653b\u51fb\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5927\u89c4\u6a21\u901a\u7528\u653b\u51fb\u3002", "motivation": "\u63a2\u8ba8LLMs\u5982\u4f55\u4f7f\u7f51\u7edc\u653b\u51fb\u66f4\u4e2a\u6027\u5316\u3001\u9ad8\u6548\uff0c\u5e76\u5206\u6790\u5176\u6f5c\u5728\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5b9e\u4f8b\u5c55\u793aLLMs\u5982\u4f55\u81ea\u52a8\u53d1\u73b0\u654f\u611f\u4fe1\u606f\uff08\u5982Enron\u90ae\u4ef6\u4e2d\u7684\u9690\u79c1\u6570\u636e\uff09\u5e76\u7528\u4e8e\u52d2\u7d22\u3002", "result": "LLMs\u5df2\u80fd\u5b9e\u73b0\u4e2a\u6027\u5316\u653b\u51fb\uff0c\u4e14\u968f\u7740\u6210\u672c\u964d\u4f4e\uff0c\u653b\u51fb\u89c4\u6a21\u5c06\u6269\u5927\u3002", "conclusion": "LLMs\u7684\u666e\u53ca\u9700\u8981\u65b0\u7684\u9632\u5fa1\u7b56\u7565\u3002", "relevance": 75.0}}
{"id": "2505.11259", "pdf": "https://arxiv.org/pdf/2505.11259", "abs": "https://arxiv.org/abs/2505.11259", "authors": ["Gabriele Iommazzo", "David Mart\u00ednez-Rubio", "Francisco Criado", "Elias Wirth", "Sebastian Pokutta"], "title": "Linear Convergence of the Frank-Wolfe Algorithm over Product Polytopes", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We study the linear convergence of Frank-Wolfe algorithms over product\npolytopes. We analyze two condition numbers for the product polytope, namely\nthe \\emph{pyramidal width} and the \\emph{vertex-facet distance}, based on the\ncondition numbers of individual polytope components. As a result, for convex\nobjectives that are $\\mu$-Polyak-{\\L}ojasiewicz, we show linear convergence\nrates quantified in terms of the resulting condition numbers. We apply our\nresults to the problem of approximately finding a feasible point in a polytope\nintersection in high-dimensions, and demonstrate the practical efficiency of\nour algorithms through empirical results.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Frank-Wolfe\u7b97\u6cd5\u5728\u4e58\u79ef\u591a\u9762\u4f53\u4e0a\u7684\u7ebf\u6027\u6536\u655b\u6027\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u6761\u4ef6\u6570\uff08\u91d1\u5b57\u5854\u5bbd\u5ea6\u548c\u9876\u70b9-\u9762\u8ddd\u79bb\uff09\uff0c\u5e76\u9488\u5bf9\u03bc-Polyak-\u0141ojasiewicz\u51f8\u76ee\u6807\u51fd\u6570\u5c55\u793a\u4e86\u7ebf\u6027\u6536\u655b\u7387\u3002", "motivation": "\u7814\u7a76Frank-Wolfe\u7b97\u6cd5\u5728\u4e58\u79ef\u591a\u9762\u4f53\u4e0a\u7684\u6536\u655b\u6027\uff0c\u65e8\u5728\u4e3a\u9ad8\u7ef4\u591a\u9762\u4f53\u4ea4\u96c6\u4e2d\u7684\u53ef\u884c\u70b9\u8fd1\u4f3c\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790\u4e58\u79ef\u591a\u9762\u4f53\u7684\u4e24\u79cd\u6761\u4ef6\u6570\uff08\u91d1\u5b57\u5854\u5bbd\u5ea6\u548c\u9876\u70b9-\u9762\u8ddd\u79bb\uff09\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6761\u4ef6\u6570\u8bc1\u660e\u03bc-Polyak-\u0141ojasiewicz\u51f8\u76ee\u6807\u51fd\u6570\u7684\u7ebf\u6027\u6536\u655b\u7387\u3002", "result": "\u5c55\u793a\u4e86\u7ebf\u6027\u6536\u655b\u7387\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5b9e\u9645\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u7ef4\u591a\u9762\u4f53\u4ea4\u96c6\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "relevance": 10.0}}
{"id": "2505.11281", "pdf": "https://arxiv.org/pdf/2505.11281", "abs": "https://arxiv.org/abs/2505.11281", "authors": ["Yuejiang Wen", "Paul D. Franzon"], "title": "Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization", "categories": ["stat.ML", "cs.LG"], "comment": "working, to be submitted", "summary": "Bayesian Optimization (BO) in high-dimensional spaces remains fundamentally\nlimited by the curse of dimensionality and the rigidity of global\nlow-dimensional assumptions. While Random EMbedding Bayesian Optimization\n(REMBO) mitigates this via linear projections into low-dimensional subspaces,\nit typically assumes a single global embedding and a stationary objective. In\nthis work, we introduce Self-Adaptive embedding REMBO (SA-REMBO), a novel\nframework that generalizes REMBO to support multiple random Gaussian\nembeddings, each capturing a different local subspace structure of the\nhigh-dimensional objective. An index variable governs the embedding choice and\nis jointly modeled with the latent optimization variable via a product kernel\nin a Gaussian Process surrogate. This enables the optimizer to adaptively\nselect embeddings conditioned on location, effectively capturing locally\nvarying effective dimensionality, nonstationarity, and heteroscedasticity in\nthe objective landscape. We theoretically analyze the expressiveness and\nstability of the index-conditioned product kernel and empirically demonstrate\nthe advantage of our method across synthetic and real-world high-dimensional\nbenchmarks, where traditional REMBO and other low-rank BO methods fail. Our\nresults establish SA-REMBO as a powerful and flexible extension for scalable BO\nin complex, structured design spaces.", "AI": {"tldr": "SA-REMBO\u6269\u5c55\u4e86REMBO\uff0c\u901a\u8fc7\u591a\u968f\u673a\u9ad8\u65af\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u9009\u62e9\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684\u5c40\u90e8\u5b50\u7a7a\u95f4\u7ed3\u6784\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002", "motivation": "\u9ad8\u7ef4\u8d1d\u53f6\u65af\u4f18\u5316\u53d7\u9650\u4e8e\u7ef4\u5ea6\u707e\u96be\u548c\u5168\u5c40\u4f4e\u7ef4\u5047\u8bbe\u7684\u521a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982REMBO\u65e0\u6cd5\u5904\u7406\u5c40\u90e8\u53d8\u5316\u7684\u6709\u6548\u7ef4\u5ea6\u548c\u975e\u5e73\u7a33\u6027\u3002", "method": "\u63d0\u51faSA-REMBO\u6846\u67b6\uff0c\u652f\u6301\u591a\u968f\u673a\u9ad8\u65af\u5d4c\u5165\uff0c\u901a\u8fc7\u7d22\u5f15\u53d8\u91cf\u548c\u4e58\u79ef\u6838\u5728\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u4e2d\u8054\u5408\u5efa\u6a21\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u5d4c\u5165\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u9ad8\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfREMBO\u548c\u5176\u4ed6\u4f4e\u79e9BO\u65b9\u6cd5\u3002", "conclusion": "SA-REMBO\u4e3a\u590d\u6742\u7ed3\u6784\u5316\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684\u53ef\u6269\u5c55\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u6269\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.11315", "pdf": "https://arxiv.org/pdf/2505.11315", "abs": "https://arxiv.org/abs/2505.11315", "authors": ["Chin-Yun Yu", "Marco A. Mart\u00ednez-Ram\u00edrez", "Junghyun Koo", "Wei-Hsiang Liao", "Yuki Mitsufuji", "Gy\u00f6rgy Fazekas"], "title": "Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Submitted to WASPAA 2025", "summary": "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach\nfor transferring the applied effects of a reference audio to a raw audio track.\nIt optimises the effect parameters to minimise the distance between the style\nembeddings of the processed audio and the reference. However, this method\ntreats all possible configurations equally and relies solely on the embedding\nspace, which can lead to unrealistic or biased results. We address this pitfall\nby introducing a Gaussian prior derived from a vocal preset dataset, DiffVox,\nover the parameter space. The resulting optimisation is equivalent to\nmaximum-a-posteriori estimation. Evaluations on vocal effects transfer on the\nMedleyDB dataset show significant improvements across metrics compared to\nbaselines, including a blind audio effects estimator, nearest-neighbour\napproaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter\nmean squared error by up to 33% and matches the reference style better.\nSubjective evaluations with 16 participants confirm our method's superiority,\nespecially in limited data regimes. This work demonstrates how incorporating\nprior knowledge in inference time enhances audio effects transfer, paving the\nway for more effective and realistic audio processing systems.", "AI": {"tldr": "ST-ITO\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u9ad8\u65af\u5148\u9a8c\u4f18\u5316\u97f3\u9891\u6548\u679c\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u6548\u679c\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u89e3\u51b3ST-ITO\u65b9\u6cd5\u5728\u97f3\u9891\u6548\u679c\u8fc1\u79fb\u4e2d\u56e0\u5e73\u7b49\u5bf9\u5f85\u6240\u6709\u914d\u7f6e\u548c\u4ec5\u4f9d\u8d56\u5d4c\u5165\u7a7a\u95f4\u5bfc\u81f4\u7684\u4e0d\u73b0\u5b9e\u6216\u504f\u5dee\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8eDiffVox\u6570\u636e\u96c6\u7684\u9ad8\u65af\u5148\u9a8c\uff0c\u8fdb\u884c\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\u4f18\u5316\u3002", "result": "\u5728MedleyDB\u6570\u636e\u96c6\u4e0a\uff0c\u53c2\u6570\u5747\u65b9\u8bef\u5dee\u964d\u4f4e33%\uff0c\u4e3b\u89c2\u8bc4\u4f30\u663e\u793a\u65b9\u6cd5\u4f18\u8d8a\u6027\u3002", "conclusion": "\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u53ef\u63d0\u5347\u97f3\u9891\u6548\u679c\u8fc1\u79fb\u7684\u6709\u6548\u6027\u548c\u771f\u5b9e\u6027\u3002", "relevance": 30.0}}
{"id": "2505.11318", "pdf": "https://arxiv.org/pdf/2505.11318", "abs": "https://arxiv.org/abs/2505.11318", "authors": ["Donald Loveland", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Danai Koutra"], "title": "On the Role of Weight Decay in Collaborative Filtering: A Popularity Perspective", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted at SIGKDD 2025", "summary": "Collaborative filtering (CF) enables large-scale recommendation systems by\nencoding information from historical user-item interactions into dense\nID-embedding tables. However, as embedding tables grow, closed-form solutions\nbecome impractical, often necessitating the use of mini-batch gradient descent\nfor training. Despite extensive work on designing loss functions to train CF\nmodels, we argue that one core component of these pipelines is heavily\noverlooked: weight decay. Attaining high-performing models typically requires\ncareful tuning of weight decay, regardless of loss, yet its necessity is not\nwell understood. In this work, we question why weight decay is crucial in CF\npipelines and how it impacts training. Through theoretical and empirical\nanalysis, we surprisingly uncover that weight decay's primary function is to\nencode popularity information into the magnitudes of the embedding vectors.\nMoreover, we find that tuning weight decay acts as a coarse, non-linear knob to\ninfluence preference towards popular or unpopular items. Based on these\nfindings, we propose PRISM (Popularity-awaRe Initialization Strategy for\nembedding Magnitudes), a straightforward yet effective solution to simplify the\ntraining of high-performing CF models. PRISM pre-encodes the popularity\ninformation typically learned through weight decay, eliminating its necessity.\nOur experiments show that PRISM improves performance by up to 4.77% and reduces\ntraining times by 38.48%, compared to state-of-the-art training strategies.\nAdditionally, we parameterize PRISM to modulate the initialization strength,\noffering a cost-effective and meaningful strategy to mitigate popularity bias.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u534f\u540c\u8fc7\u6ee4\uff08CF\uff09\u4e2d\u6743\u91cd\u8870\u51cf\uff08weight decay\uff09\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u529f\u80fd\u662f\u5c06\u6d41\u884c\u5ea6\u4fe1\u606f\u7f16\u7801\u5230\u5d4c\u5165\u5411\u91cf\u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRISM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u7f16\u7801\u6d41\u884c\u5ea6\u4fe1\u606f\u7b80\u5316\u8bad\u7ec3\u3002", "motivation": "\u6743\u91cd\u8870\u51cf\u5728CF\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u4f46\u5176\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u6743\u91cd\u8870\u51cf\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51faPRISM\u65b9\u6cd5\u9884\u7f16\u7801\u6d41\u884c\u5ea6\u4fe1\u606f\u3002", "result": "PRISM\u5c06\u6027\u80fd\u63d0\u53474.77%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1138.48%\uff0c\u5e76\u80fd\u6709\u6548\u7f13\u89e3\u6d41\u884c\u5ea6\u504f\u5dee\u3002", "conclusion": "PRISM\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u66ff\u4ee3\u6743\u91cd\u8870\u51cf\uff0c\u63d0\u5347CF\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.11323", "pdf": "https://arxiv.org/pdf/2505.11323", "abs": "https://arxiv.org/abs/2505.11323", "authors": ["Haowei Wang", "Jingyi Wang", "Zhongxiang Dai", "Nai-Yuan Chiang", "Szu Hui Ng", "Cosmin G. Petra"], "title": "Convergence Rates of Constrained Expected Improvement", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Constrained Bayesian optimization (CBO) methods have seen significant success\nin black-box optimization with constraints, and one of the most commonly used\nCBO methods is the constrained expected improvement (CEI) algorithm. CEI is a\nnatural extension of the expected improvement (EI) when constraints are\nincorporated. However, the theoretical convergence rate of CEI has not been\nestablished. In this work, we study the convergence rate of CEI by analyzing\nits simple regret upper bound. First, we show that when the objective function\n$f$ and constraint function $c$ are assumed to each lie in a reproducing kernel\nHilbert space (RKHS), CEI achieves the convergence rates of $\\mathcal{O}\n\\left(t^{-\\frac{1}{2}}\\log^{\\frac{d+1}{2}}(t) \\right) \\ \\text{and }\\\n\\mathcal{O}\\left(t^{\\frac{-\\nu}{2\\nu+d}} \\log^{\\frac{\\nu}{2\\nu+d}}(t)\\right)$\nfor the commonly used squared exponential and Mat\\'{e}rn kernels, respectively.\nSecond, we show that when $f$ and $c$ are assumed to be sampled from Gaussian\nprocesses (GPs), CEI achieves the same convergence rates with a high\nprobability. Numerical experiments are performed to validate the theoretical\nanalysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ea6\u675f\u8d1d\u53f6\u65af\u4f18\u5316\uff08CBO\uff09\u65b9\u6cd5\u4e2d\u7684\u7ea6\u675f\u671f\u671b\u6539\u8fdb\uff08CEI\uff09\u7b97\u6cd5\u7684\u6536\u655b\u901f\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u7684\u7406\u8bba\u6536\u655b\u901f\u7387\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1CEI\u662f\u7ea6\u675f\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u4f46\u5176\u7406\u8bba\u6536\u655b\u901f\u7387\u5c1a\u672a\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790CEI\u7684\u7b80\u5355\u9057\u61be\u4e0a\u754c\uff0c\u5047\u8bbe\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\u51fd\u6570\u5206\u522b\u4f4d\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u6216\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u4e2d\uff0c\u63a8\u5bfc\u5176\u6536\u655b\u901f\u7387\u3002", "result": "\u8bc1\u660e\u4e86CEI\u5728\u5e73\u65b9\u6307\u6570\u6838\u548cMat\u00e9rn\u6838\u4e0b\u7684\u6536\u655b\u901f\u7387\u5206\u522b\u4e3aO(t^(-1/2) log^((d+1)/2)(t))\u548cO(t^(-\u03bd/(2\u03bd+d)) log^(\u03bd/(2\u03bd+d))(t))\uff0c\u5e76\u5728GP\u5047\u8bbe\u4e0b\u5f97\u5230\u76f8\u540c\u7ed3\u679c\u3002", "conclusion": "CEI\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\uff0c\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "relevance": 30.0}}
{"id": "2505.11329", "pdf": "https://arxiv.org/pdf/2505.11329", "abs": "https://arxiv.org/abs/2505.11329", "authors": ["Raja Gond", "Nipun Kwatra", "Ramachandran Ramjee"], "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference", "categories": ["cs.DC", "cs.LG"], "comment": "13 pages, 15 figures", "summary": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLINK. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurther, the communication itself uses many streaming multiprocessors (SMs),\nadding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The computation of one\nsubset is then overlapped with the communication of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce-RMSNorm kernel carefully leveraging Multimem instruction support\navailable on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to\nperform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel\nenables the memory bound RMSNorm to be overlapped with the other batch's\ncomputation, providing additional gains. Our evaluations demonstrate up to 29%\nlatency gains and up to 26% throughput gains across multiple models and\nworkloads. In several settings, TokenWeave results in better performance\ncompared to an equivalent model with all communication removed.", "AI": {"tldr": "TokenWeave\u63d0\u51fa\u4e86\u4e00\u79cdToken-Splitting\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u6279\u6b21\u4e2d\u7684\u4ee4\u724c\u5206\u6210\u4e24\u4e2a\u5b50\u96c6\u5e76\u91cd\u53e0\u8ba1\u7b97\u4e0e\u901a\u4fe1\uff0c\u4f18\u5316\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u6027\u80fd\u3002\u7ed3\u5408\u5c42\u5f52\u4e00\u5316\u8ba1\u7b97\u7684\u987a\u5e8f\u4f18\u5316\u548c\u65b0\u578b\u878d\u5408AllReduce-RMSNorm\u5185\u6838\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe29%\u7684\u5ef6\u8fdf\u548c26%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5206\u5e03\u5f0f\u63a8\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u5373\u4f7f\u4f7f\u7528\u9ad8\u901f\u4e92\u8fde\uff08\u5982NVLINK\uff09\uff0c\u901a\u4fe1\u5f00\u9500\u4ecd\u9ad8\u8fbe20%\u3002\u73b0\u6709\u6280\u672f\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u89e3\u548c\u901a\u4fe1\u91cd\u53e0\u6765\u7f13\u89e3\uff0c\u4f46\u4f1a\u5f15\u5165\u989d\u5916\u5f00\u9500\u3002TokenWeave\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "1. Token-Splitting\u6280\u672f\u5c06\u4ee4\u724c\u5206\u6210\u4e24\u4e2a\u5b50\u96c6\uff0c\u91cd\u53e0\u8ba1\u7b97\u4e0e\u901a\u4fe1\u30022. \u4f18\u5316\u5c42\u5f52\u4e00\u5316\u8ba1\u7b97\u987a\u5e8f\u30023. \u5b9e\u73b0\u878d\u5408AllReduce-RMSNorm\u5185\u6838\uff0c\u5229\u7528NVIDIA Hopper GPU\u7684Multimem\u6307\u4ee4\u652f\u6301\uff0c\u4ec5\u97002-8\u4e2aSMs\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cTokenWeave\u5728\u591a\u79cd\u6a21\u578b\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe29%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c26%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u4f18\u4e8e\u65e0\u901a\u4fe1\u7684\u7b49\u6548\u6a21\u578b\u3002", "conclusion": "TokenWeave\u901a\u8fc7\u521b\u65b0\u7684\u4ee4\u724c\u5206\u5272\u548c\u5185\u6838\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u6548\u7387\uff0c\u4e3a\u9ad8\u6027\u80fd\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.11343", "pdf": "https://arxiv.org/pdf/2505.11343", "abs": "https://arxiv.org/abs/2505.11343", "authors": ["Rajeeva Laxman Karandikar", "Bhamidi Visweswara Rao", "Mathukumalli Vidyasagar"], "title": "Revisiting Stochastic Approximation and Stochastic Gradient Descent", "categories": ["math.OC", "cs.LG", "stat.ML", "62L20, 60G17, 93D05"], "comment": "22 pages", "summary": "In this paper, we take a fresh look at stochastic approximation (SA) and\nStochastic Gradient Descent (SGD). We derive new sufficient conditions for the\nconvergence of SA. In particular, the \"noise\" or measurement error need not\nhave a finite second moment, and under suitable conditions, not even a finite\nmean. By adapting this method of proof, we also derive sufficient conditions\nfor the convergence of zero-order SGD, wherein the stochastic gradient is\ncomputed using only two function evaluations, and no gradient computations. The\nsufficient conditions derived here are the weakest to date, thus leading to a\nconsiderable expansion of the applicability of SA and SGD theory.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u968f\u673a\u903c\u8fd1\uff08SA\uff09\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6536\u655b\u6761\u4ef6\uff0c\u653e\u5bbd\u4e86\u5bf9\u566a\u58f0\u6216\u6d4b\u91cf\u8bef\u5dee\u7684\u9650\u5236\uff0c\u751a\u81f3\u65e0\u9700\u6709\u9650\u5747\u503c\u3002\u540c\u65f6\uff0c\u4e3a\u96f6\u9636SGD\u63d0\u4f9b\u4e86\u6536\u655b\u6761\u4ef6\uff0c\u6269\u5c55\u4e86SA\u548cSGD\u7406\u8bba\u7684\u9002\u7528\u8303\u56f4\u3002", "motivation": "\u7814\u7a76SA\u548cSGD\u7684\u6536\u655b\u6761\u4ef6\uff0c\u653e\u5bbd\u4f20\u7edf\u5047\u8bbe\uff0c\u62d3\u5c55\u5176\u7406\u8bba\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u65b0\u7684\u5145\u5206\u6761\u4ef6\uff0c\u653e\u5bbd\u5bf9\u566a\u58f0\u6216\u6d4b\u91cf\u8bef\u5dee\u7684\u9650\u5236\uff0c\u4e3a\u96f6\u9636SGD\u63d0\u4f9b\u6536\u655b\u8bc1\u660e\u3002", "result": "\u63d0\u51fa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5f31\u7684\u6536\u655b\u6761\u4ef6\uff0c\u663e\u8457\u6269\u5c55\u4e86SA\u548cSGD\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "\u672c\u6587\u4e3aSA\u548cSGD\u7684\u6536\u655b\u6027\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\u3002", "relevance": 30.0}}
{"id": "2505.11355", "pdf": "https://arxiv.org/pdf/2505.11355", "abs": "https://arxiv.org/abs/2505.11355", "authors": ["Simon Urbainczyk", "Aretha L. Teckentrup", "Jonas Latz"], "title": "STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "Gaussian processes (GPs) have gained popularity as flexible machine learning\nmodels for regression and function approximation with an in-built method for\nuncertainty quantification. However, GPs suffer when the amount of training\ndata is large or when the underlying function contains multi-scale features\nthat are difficult to represent by a stationary kernel. To address the former,\ntraining of GPs with large-scale data is often performed through inducing point\napproximations (also known as sparse GP regression (GPR)), where the size of\nthe covariance matrices in GPR is reduced considerably through a greedy search\non the data set. To aid the latter, deep GPs have gained traction as\nhierarchical models that resolve multi-scale features by combining multiple\nGPs. Posterior inference in deep GPs requires a sampling or, more usual, a\nvariational approximation. Variational approximations lead to large-scale\nstochastic, non-convex optimisation problems and the resulting approximation\ntends to represent uncertainty incorrectly. In this work, we combine\nvariational learning with MCMC to develop a particle-based\nexpectation-maximisation method to simultaneously find inducing points within\nthe large-scale data (variationally) and accurately train the GPs\n(sampling-based). The result is a highly efficient and accurate methodology for\ndeep GP training on large-scale data. We test our method on standard benchmark\nproblems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u5b66\u4e60\u548cMCMC\u7684\u7c92\u5b50\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u9ad8\u6548\u51c6\u786e\u5730\u8bad\u7ec3\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u5728\u5927\u89c4\u6a21\u6570\u636e\u6216\u591a\u5c3a\u5ea6\u7279\u5f81\u5efa\u6a21\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7a00\u758fGPR\u548c\u6df1\u5ea6GPs\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u53d8\u5206\u5b66\u4e60\u548cMCMC\uff0c\u5f00\u53d1\u7c92\u5b50\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u8bf1\u5bfc\u70b9\u548c\u8bad\u7ec3GPs\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6df1\u5ea6GP\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6GP\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "relevance": 40.0}}
{"id": "2505.11366", "pdf": "https://arxiv.org/pdf/2505.11366", "abs": "https://arxiv.org/abs/2505.11366", "authors": ["Ali Rabiee", "Sima Ghafoori", "MH Farhadi", "Robert Beyer", "Xiangyu Bai", "David J Lin", "Sarah Ostadabbas", "Reza Abiri"], "title": "Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space", "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Current invasive assistive technologies are designed to infer\nhigh-dimensional motor control signals from severely paralyzed patients.\nHowever, they face significant challenges, including public acceptance, limited\nlongevity, and barriers to commercialization. Meanwhile, noninvasive\nalternatives often rely on artifact-prone signals, require lengthy user\ntraining, and struggle to deliver robust high-dimensional control for dexterous\ntasks. To address these issues, this study introduces a novel human-centered\nmultimodal AI approach as intelligent compensatory mechanisms for lost motor\nfunctions that could potentially enable patients with severe paralysis to\ncontrol high-dimensional assistive devices, such as dexterous robotic arms,\nusing limited and noninvasive inputs. In contrast to the current\nstate-of-the-art (SoTA) noninvasive approaches, our context-aware, multimodal\nshared-autonomy framework integrates deep reinforcement learning algorithms to\nblend limited low-dimensional user input with real-time environmental\nperception, enabling adaptive, dynamic, and intelligent interpretation of human\nintent for complex dexterous manipulation tasks, such as pick-and-place. The\nresults from our ARAS (Adaptive Reinforcement learning for Amplification of\nlimited inputs in Shared autonomy) trained with synthetic users over 50,000\ncomputer simulation episodes demonstrated the first successful implementation\nof the proposed closed-loop human-in-the-loop paradigm, outperforming the SoTA\nshared autonomy algorithms. Following a zero-shot sim-to-real transfer, ARAS\nwas evaluated on 23 human subjects, demonstrating high accuracy in dynamic\nintent detection and smooth, stable 3D trajectory control for dexterous\npick-and-place tasks. ARAS user study achieved a high task success rate of\n92.88%, with short completion times comparable to those of SoTA invasive\nassistive technologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001AI\u65b9\u6cd5ARAS\uff0c\u7528\u4e8e\u5e2e\u52a9\u4e25\u91cd\u762b\u75ea\u60a3\u8005\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u8f93\u5165\u63a7\u5236\u9ad8\u7ef4\u8f85\u52a9\u8bbe\u5907\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u52a8\u6001\u610f\u56fe\u8bc6\u522b\u548c\u4efb\u52a1\u5b8c\u6210\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4fb5\u5165\u6027\u548c\u975e\u4fb5\u5165\u6027\u8f85\u52a9\u6280\u672f\u5728\u7528\u6237\u63a5\u53d7\u5ea6\u3001\u957f\u671f\u4f7f\u7528\u548c\u5546\u4e1a\u5316\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u975e\u4fb5\u5165\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u5171\u4eab\u81ea\u6cbb\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u7528\u6237\u6a21\u62df\u8bad\u7ec350,000\u6b21\uff0c\u5b9e\u73b0\u52a8\u6001\u610f\u56fe\u8bc6\u522b\u548c\u4efb\u52a1\u63a7\u5236\u3002", "result": "ARAS\u572823\u540d\u4eba\u7c7b\u53d7\u8bd5\u8005\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u610f\u56fe\u68c0\u6d4b\u548c\u7a33\u5b9a3D\u8f68\u8ff9\u63a7\u5236\uff0c\u4efb\u52a1\u6210\u529f\u7387\u8fbe92.88%\uff0c\u6027\u80fd\u5ab2\u7f8e\u4fb5\u5165\u6027\u6280\u672f\u3002", "conclusion": "ARAS\u4e3a\u4e25\u91cd\u762b\u75ea\u60a3\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u6027\u7684\u9ad8\u7ef4\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5546\u4e1a\u5316\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.11375", "pdf": "https://arxiv.org/pdf/2505.11375", "abs": "https://arxiv.org/abs/2505.11375", "authors": ["Alistair Carson", "Alec Wright", "Stefan Bilbao"], "title": "Anti-aliasing of neural distortion effects via model fine tuning", "categories": ["eess.AS", "cs.LG", "eess.SP"], "comment": "Accepted for DAFx25", "summary": "Neural networks have become ubiquitous with guitar distortion effects\nmodelling in recent years. Despite their ability to yield perceptually\nconvincing models, they are susceptible to frequency aliasing when driven by\nhigh frequency and high gain inputs. Nonlinear activation functions create both\nthe desired harmonic distortion and unwanted aliasing distortion as the\nbandwidth of the signal is expanded beyond the Nyquist frequency. Here, we\npresent a method for reducing aliasing in neural models via a teacher-student\nfine tuning approach, where the teacher is a pre-trained model with its weights\nfrozen, and the student is a copy of this with learnable parameters. The\nstudent is fine-tuned against an aliasing-free dataset generated by passing\nsinusoids through the original model and removing non-harmonic components from\nthe output spectra. Our results show that this method significantly suppresses\naliasing for both long-short-term-memory networks (LSTM) and temporal\nconvolutional networks (TCN). In the majority of our case studies, the\nreduction in aliasing was greater than that achieved by two times oversampling.\nOne side-effect of the proposed method is that harmonic distortion components\nare also affected. This adverse effect was found to be model-dependent, with\nthe LSTM models giving the best balance between anti-aliasing and preserving\nthe perceived similarity to an analog reference device.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5e08\u751f\u5fae\u8c03\u65b9\u6cd5\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u9891\u7387\u6df7\u53e0\u7684\u6280\u672f\uff0c\u663e\u8457\u6291\u5236\u4e86LSTM\u548cTCN\u4e2d\u7684\u6df7\u53e0\u73b0\u8c61\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u5409\u4ed6\u5931\u771f\u6548\u679c\u5efa\u6a21\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9ad8\u9891\u7387\u548c\u9ad8\u589e\u76ca\u8f93\u5165\u4f1a\u5bfc\u81f4\u9891\u7387\u6df7\u53e0\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5e08\u751f\u5fae\u8c03\u65b9\u6cd5\uff0c\u6559\u5e08\u6a21\u578b\u4e3a\u9884\u8bad\u7ec3\u4e14\u6743\u91cd\u56fa\u5b9a\u7684\u6a21\u578b\uff0c\u5b66\u751f\u6a21\u578b\u4e3a\u53ef\u5b66\u4e60\u7684\u526f\u672c\uff0c\u901a\u8fc7\u65e0\u6df7\u53e0\u6570\u636e\u96c6\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u6291\u5236\u4e86LSTM\u548cTCN\u4e2d\u7684\u6df7\u53e0\u73b0\u8c61\uff0c\u6548\u679c\u4f18\u4e8e\u4e24\u500d\u8fc7\u91c7\u6837\u3002", "conclusion": "\u5e08\u751f\u5fae\u8c03\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u6df7\u53e0\uff0c\u4f46\u5bf9\u8c10\u6ce2\u5931\u771f\u7684\u5f71\u54cd\u56e0\u6a21\u578b\u800c\u5f02\uff0cLSTM\u8868\u73b0\u6700\u4f73\u3002", "relevance": 30.0}}
{"id": "2505.11378", "pdf": "https://arxiv.org/pdf/2505.11378", "abs": "https://arxiv.org/abs/2505.11378", "authors": ["Alexander Kim", "Charlotte Botha"], "title": "Machine Learning Approaches to Vocal Register Classification in Contemporary Male Pop Music", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "8 pages, 8 figures", "summary": "For singers of all experience levels, one of the most daunting challenges in\nlearning technical repertoire is navigating placement and vocal register in and\naround the passagio (passage between chest voice and head voice registers).\nParticularly in pop music, where a single artist may use a variety of timbre's\nand textures to achieve a desired quality, it can be difficult to identify what\nvocal register within the vocal range a singer is using. This paper presents\ntwo methods for classifying vocal registers in an audio signal of male pop\nmusic through the analysis of textural features of mel-spectrogram images.\nAdditionally, we will discuss the practical integration of these models for\nvocal analysis tools, and introduce a concurrently developed software called\nAVRA which stands for Automatic Vocal Register Analysis. Our proposed methods\nachieved consistent classification of vocal register through both Support\nVector Machine (SVM) and Convolutional Neural Network (CNN) models, which\nsupports the promise of more robust classification possibilities across more\nvoice types and genres of singing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08SVM\u548cCNN\uff09\u7528\u4e8e\u7537\u6027\u6d41\u884c\u97f3\u4e50\u97f3\u9891\u4fe1\u53f7\u4e2d\u58f0\u533a\u7684\u5206\u7c7b\uff0c\u5e76\u5f00\u53d1\u4e86\u8f6f\u4ef6AVRA\u7528\u4e8e\u81ea\u52a8\u58f0\u533a\u5206\u6790\u3002", "motivation": "\u6d41\u884c\u97f3\u4e50\u4e2d\u6b4c\u624b\u4f7f\u7528\u591a\u79cd\u97f3\u8272\u548c\u7eb9\u7406\uff0c\u96be\u4ee5\u8bc6\u522b\u58f0\u533a\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6885\u5c14\u9891\u8c31\u56fe\u7684\u7eb9\u7406\u7279\u5f81\uff0c\u4f7f\u7528SVM\u548cCNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5747\u80fd\u7a33\u5b9a\u5206\u7c7b\u58f0\u533a\uff0c\u5c55\u793a\u4e86\u8de8\u66f4\u591a\u58f0\u97f3\u7c7b\u578b\u548c\u97f3\u4e50\u98ce\u683c\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u58f0\u533a\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u5e94\u7528\u573a\u666f\u3002", "relevance": 10.0}}
{"id": "2505.11388", "pdf": "https://arxiv.org/pdf/2505.11388", "abs": "https://arxiv.org/abs/2505.11388", "authors": ["Petr Kasalick\u00fd", "Martin Spi\u0161\u00e1k", "Vojt\u011bch Van\u010dura", "Daniel Bohun\u011bk", "Rodrigo Alves", "Pavel Kord\u00edk"], "title": "The Future is Sparse: Embedding Compression for Scalable Retrieval in Recommender Systems", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Industry-scale recommender systems face a core challenge: representing\nentities with high cardinality, such as users or items, using dense embeddings\nthat must be accessible during both training and inference. However, as\nembedding sizes grow, memory constraints make storage and access increasingly\ndifficult. We describe a lightweight, learnable embedding compression technique\nthat projects dense embeddings into a high-dimensional, sparsely activated\nspace. Designed for retrieval tasks, our method reduces memory requirements\nwhile preserving retrieval performance, enabling scalable deployment under\nstrict resource constraints. Our results demonstrate that leveraging sparsity\nis a promising approach for improving the efficiency of large-scale\nrecommenders. We release our code at https://github.com/recombee/CompresSAE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u5b66\u4e60\u7684\u5d4c\u5165\u538b\u7f29\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u57fa\u6570\u5b9e\u4f53\u7684\u8868\u793a\u95ee\u9898\uff0c\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u7ea7\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u9ad8\u57fa\u6570\u5b9e\u4f53\uff08\u5982\u7528\u6237\u6216\u7269\u54c1\uff09\u7684\u5bc6\u96c6\u5d4c\u5165\u8868\u793a\u95ee\u9898\uff0c\u968f\u7740\u5d4c\u5165\u5c3a\u5bf8\u589e\u957f\uff0c\u5185\u5b58\u9650\u5236\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u5c06\u5bc6\u96c6\u5d4c\u5165\u6295\u5f71\u5230\u9ad8\u7ef4\u7a00\u758f\u6fc0\u6d3b\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5d4c\u5165\u538b\u7f29\u6280\u672f\uff0c\u9002\u7528\u4e8e\u68c0\u7d22\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u3002", "conclusion": "\u5229\u7528\u7a00\u758f\u6027\u662f\u63d0\u9ad8\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u6548\u7387\u7684\u6709\u6548\u9014\u5f84\u3002", "relevance": 40.0}}
