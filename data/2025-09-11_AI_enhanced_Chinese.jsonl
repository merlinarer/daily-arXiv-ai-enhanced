{"id": "2509.07998", "pdf": "https://arxiv.org/pdf/2509.07998", "abs": "https://arxiv.org/abs/2509.07998", "authors": ["Mesay Gemeda Yigezu", "Girma Yohannis Bade", "Atnafu Lambebo Tonja", "Olga Kolesnikova", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Bilingual Word Level Language Identification for Omotic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language identification is the task of determining the languages for a given\ntext. In many real world scenarios, text may contain more than one language,\nparticularly in multilingual communities. Bilingual Language Identification\n(BLID) is the task of identifying and distinguishing between two languages in a\ngiven text. This paper presents BLID for languages spoken in the southern part\nof Ethiopia, namely Wolaita and Gofa. The presence of words similarities and\ndifferences between the two languages makes the language identification task\nchallenging. To overcome this challenge, we employed various experiments on\nvarious approaches. Then, the combination of the BERT based pretrained language\nmodel and LSTM approach performed better, with an F1 score of 0.72 on the test\nset. As a result, the work will be effective in tackling unwanted social media\nissues and providing a foundation for further research in this area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u57c3\u585e\u4fc4\u6bd4\u4e9a\u5357\u90e8Wolaita\u548cGofa\u8bed\u8a00\u7684\u53cc\u8bed\u8bc6\u522b(BLID)\u65b9\u6cd5\uff0c\u7ed3\u5408BERT\u9884\u8bad\u7ec3\u6a21\u578b\u548cLSTM\u53d6\u5f97\u4e860.72\u7684F1\u5206\u6570", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u793e\u533a\u4e2d\u6587\u672c\u5305\u542b\u591a\u79cd\u8bed\u8a00\u65f6\u7684\u8bed\u8a00\u8bc6\u522b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728Wolaita\u548cGofa\u8fd9\u4e24\u79cd\u76f8\u4f3c\u8bed\u8a00\u4e4b\u95f4\u7684\u533a\u5206\u6311\u6218", "method": "\u91c7\u7528\u591a\u79cd\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u6700\u7ec8\u786e\u5b9aBERT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0eLSTM\u7684\u7ec4\u5408\u65b9\u6cd5", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e860.72\u7684F1\u5206\u6570\uff0c\u8868\u73b0\u6700\u4f73", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u8bed\u8a00\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u57fa\u7840", "relevance": 15.0}}
{"id": "2509.08000", "pdf": "https://arxiv.org/pdf/2509.08000", "abs": "https://arxiv.org/abs/2509.08000", "authors": ["Debdeep Sanyal", "Manodeep Ray", "Murari Mandal"], "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs", "categories": ["cs.CL"], "comment": "19 pages", "summary": "The release of open-weight large language models (LLMs) creates a tension\nbetween advancing accessible research and preventing misuse, such as malicious\nfine-tuning to elicit harmful content. Current safety measures struggle to\npreserve the general capabilities of the LLM while resisting a determined\nadversary with full access to the model's weights and architecture, who can use\nfull-parameter fine-tuning to erase existing safeguards. To address this, we\nintroduce AntiDote, a bi-level optimization procedure for training LLMs to be\nresistant to such tampering. AntiDote involves an auxiliary adversary\nhypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)\nweights conditioned on the defender model's internal activations. The defender\nLLM is then trained with an objective to nullify the effect of these\nadversarial weight additions, forcing it to maintain its safety alignment. We\nvalidate this approach against a diverse suite of 52 red-teaming attacks,\nincluding jailbreak prompting, latent space manipulation, and direct\nweight-space attacks. AntiDote is upto 27.4\\% more robust against adversarial\nattacks compared to both tamper-resistance and unlearning baselines. Crucially,\nthis robustness is achieved with a minimal trade-off in utility, incurring a\nperformance degradation of upto less than 0.5\\% across capability benchmarks\nincluding MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute\nefficient methodology for building open-weight models where safety is a more\nintegral and resilient property.", "AI": {"tldr": "AntiDote\u662f\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\u8bad\u7ec3LLM\u62b5\u6297\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u89e3\u51b3\u5f00\u6e90\u6743\u91cdLLM\u9762\u4e34\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9632\u6b62\u6076\u610f\u653b\u51fb\u8005\u901a\u8fc7\u5168\u53c2\u6570\u5fae\u8c03\u7ed5\u8fc7\u73b0\u6709\u5b89\u5168\u63aa\u65bd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\uff1a1) \u8f85\u52a9\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\u5b66\u4e60\u751f\u6210\u6076\u610fLoRA\u6743\u91cd 2) \u9632\u5fa1\u6a21\u578b\u8bad\u7ec3\u76ee\u6807\u62b5\u6d88\u5bf9\u6297\u6743\u91cd\u5f71\u54cd\uff0c\u4fdd\u6301\u5b89\u5168\u5bf9\u9f50", "result": "\u572852\u79cd\u7ea2\u961f\u653b\u51fb\u6d4b\u8bd5\u4e2d\uff0cAntiDote\u6bd4\u57fa\u51c6\u65b9\u6cd5\u5f3a27.4%\uff0c\u5728MMLU\u7b49\u80fd\u529b\u57fa\u51c6\u4e0a\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e0.5%", "conclusion": "AntiDote\u4e3a\u6784\u5efa\u5b89\u5168\u4e14\u5b9e\u7528\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4f7f\u5b89\u5168\u6027\u6210\u4e3a\u66f4\u5185\u5728\u548c\u5f39\u6027\u7684\u5c5e\u6027", "relevance": 85.0}}
{"id": "2509.08022", "pdf": "https://arxiv.org/pdf/2509.08022", "abs": "https://arxiv.org/abs/2509.08022", "authors": ["Yao Liang", "Dongcheng Zhao", "Feifei Zhao", "Guobin Shen", "Yuwei Wang", "Dongqi Liang", "Yi Zeng"], "title": "MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe and effective deployment across diverse user populations.\nHowever, existing benchmarks often neglect cultural and demographic diversity,\nleading to limited understanding of how value alignment generalizes globally.\nIn this work, we introduce MVPBench, a novel benchmark that systematically\nevaluates LLMs' alignment with multi-dimensional human value preferences across\n75 countries. MVPBench contains 24,020 high-quality instances annotated with\nfine-grained value labels, personalized questions, and rich demographic\nmetadata, making it the most comprehensive resource of its kind to date. Using\nMVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,\nrevealing substantial disparities in alignment performance across geographic\nand demographic lines. We further demonstrate that lightweight fine-tuning\nmethods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization\n(DPO), can significantly enhance value alignment in both in-domain and\nout-of-domain settings. Our findings underscore the necessity for\npopulation-aware alignment evaluation and provide actionable insights for\nbuilding culturally adaptive and value-sensitive LLMs. MVPBench serves as a\npractical foundation for future research on global alignment, personalized\nvalue modeling, and equitable AI development.", "AI": {"tldr": "MVPBench\u662f\u4e00\u4e2a\u5305\u542b24,020\u4e2a\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u572875\u4e2a\u56fd\u5bb6\u4e2d\u7684\u591a\u7ef4\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u8de8\u5730\u57df\u548c\u4eba\u53e3\u7edf\u8ba1\u7ebf\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u8bc1\u660e\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u4ef7\u503c\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5ffd\u89c6\u6587\u5316\u548c\u4eba\u53e3\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5bf9\u4ef7\u503c\u5bf9\u9f50\u5728\u5168\u7403\u8303\u56f4\u5185\u6cdb\u5316\u80fd\u529b\u7684\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u8d44\u6e90\u6765\u652f\u6301\u5168\u7403\u5bf9\u9f50\u7814\u7a76\u3002", "method": "\u6784\u5efaMVPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7cbe\u7ec6\u4ef7\u503c\u6807\u7b7e\u3001\u4e2a\u6027\u5316\u95ee\u9898\u548c\u4e30\u5bcc\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\uff1b\u4f7f\u7528LoRA\u548cDPO\u7b49\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0SOTA LLM\u5728\u5730\u7406\u548c\u4eba\u53e3\u7edf\u8ba1\u7ebf\u4e0a\u5b58\u5728\u663e\u8457\u5bf9\u9f50\u6027\u80fd\u5dee\u5f02\uff1b\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u5728\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u663e\u8457\u63d0\u5347\u4ef7\u503c\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u9700\u8981\u8fdb\u884c\u4eba\u53e3\u611f\u77e5\u7684\u5bf9\u9f50\u8bc4\u4f30\uff0cMVPBench\u4e3a\u5168\u7403\u5bf9\u9f50\u3001\u4e2a\u6027\u5316\u4ef7\u503c\u5efa\u6a21\u548c\u516c\u5e73AI\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "relevance": 85.0}}
{"id": "2509.08025", "pdf": "https://arxiv.org/pdf/2509.08025", "abs": "https://arxiv.org/abs/2509.08025", "authors": ["Hoang-Trung Nguyen", "Tan-Minh Nguyen", "Xuan-Bach Le", "Tuan-Kiet Le", "Khanh-Huyen Nguyen", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong", "Le-Minh Nguyen"], "title": "NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents the methodologies and results of the NOWJ team's\nparticipation across all five tasks at the COLIEE 2025 competition, emphasizing\nadvancements in the Legal Case Entailment task (Task 2). Our comprehensive\napproach systematically integrates pre-ranking models (BM25, BERT, monoT5),\nembedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large\nLanguage Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance\nscoring, and contextual re-ranking. Specifically, in Task 2, our two-stage\nretrieval system combined lexical-semantic filtering with contextualized LLM\nanalysis, achieving first place with an F1 score of 0.3195. Additionally, in\nother tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal\nTextual Entailment, and Legal Judgment Prediction--we demonstrated robust\nperformance through carefully engineered ensembles and effective prompt-based\nreasoning strategies. Our findings highlight the potential of hybrid models\nintegrating traditional IR techniques with contemporary generative models,\nproviding a valuable reference for future advancements in legal information\nprocessing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86NOWJ\u56e2\u961f\u5728COLIEE 2025\u7ade\u8d5b\u4e2d\u4e94\u4e2a\u4efb\u52a1\u7684\u65b9\u6cd5\u4e0e\u7ed3\u679c\uff0c\u91cd\u70b9\u5728\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1(Task 2)\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0cF1\u5206\u65700.3195\u3002\u56e2\u961f\u7ed3\u5408\u4e86\u4f20\u7edf\u68c0\u7d22\u6280\u672f(BM25\u3001BERT\u7b49)\u4e0e\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b(Qwen-2\u3001DeepSeek-V3\u7b49)\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u4e0e\u73b0\u4ee3\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4e24\u9636\u6bb5\u68c0\u7d22\u7cfb\u7edf\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u8bcd\u6c47\u8bed\u4e49\u8fc7\u6ee4(BM25\u3001BERT\u3001monoT5)\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u4e0a\u4e0b\u6587LLM\u5206\u6790\u3002\u6574\u5408\u4e86\u9884\u6392\u5e8f\u6a21\u578b\u3001\u8bed\u4e49\u8868\u793a(BGE-m3\u3001LLM2Vec)\u548c\u5148\u8fdbLLM(Qwen-2\u3001QwQ-32B\u3001DeepSeek-V3)\u8fdb\u884c\u6458\u8981\u3001\u76f8\u5173\u6027\u8bc4\u5206\u548c\u4e0a\u4e0b\u6587\u91cd\u6392\u5e8f\u3002", "result": "\u5728\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1(Task 2)\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0cF1\u5206\u65700.3195\u3002\u5728\u5176\u4ed6\u56db\u4e2a\u4efb\u52a1(\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u3001\u6cd5\u89c4\u68c0\u7d22\u3001\u6cd5\u5f8b\u6587\u672c\u8574\u542b\u3001\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b)\u4e2d\u4e5f\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u7ed3\u5408\u4f20\u7edfIR\u6280\u672f\u548c\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "relevance": 85.0}}
{"id": "2509.07996", "pdf": "https://arxiv.org/pdf/2509.07996", "abs": "https://arxiv.org/abs/2509.07996", "authors": ["Lingdong Kong", "Wesley Yang", "Jianbiao Mei", "Youquan Liu", "Ao Liang", "Dekai Zhu", "Dongyue Lu", "Wei Yin", "Xiaotao Hu", "Mingkai Jia", "Junyuan Deng", "Kaiwen Zhang", "Yang Wu", "Tianyi Yan", "Shenyuan Gao", "Song Wang", "Linfeng Li", "Liang Pan", "Yong Liu", "Jianke Zhu", "Wei Tsang Ooi", "Steven C. H. Hoi", "Ziwei Liu"], "title": "3D and 4D World Modeling: A Survey", "categories": ["cs.CV", "cs.RO"], "comment": "Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at\n  https://github.com/worldbench/survey", "summary": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e3D\u548c4D\u4e16\u754c\u5efa\u6a21\u7684\u7efc\u8ff0\u8bba\u6587\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u6574\u7406\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u660e\u786e\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\u4e86\u89c6\u9891\u3001\u5360\u636e\u7f51\u683c\u548cLiDAR\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e16\u754c\u5efa\u6a21\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u57282D\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff0c\u5ffd\u89c6\u4e86\u5feb\u901f\u53d1\u5c55\u76843D/4D\u8868\u793a\u65b9\u6cd5\uff08\u5982RGB-D\u3001\u5360\u636e\u7f51\u683c\u3001LiDAR\u70b9\u4e91\uff09\u3002\u540c\u65f6\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\u4f53\u7cfb\uff0c\u5bfc\u81f4\u6587\u732e\u4e2d\u7684\u7814\u7a76\u7ed3\u679c\u5206\u6563\u4e14\u4e0d\u4e00\u81f4\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u7cbe\u786e\u7684\u5b9a\u4e49\u4f53\u7cfb\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff08VideoGen\u89c6\u9891\u751f\u6210\u3001OccGen\u5360\u636e\u7f51\u683c\u751f\u6210\u3001LiDARGen\u6fc0\u5149\u96f7\u8fbe\u751f\u6210\uff09\uff0c\u7cfb\u7edf\u603b\u7ed33D/4D\u573a\u666f\u4e0b\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf93D\u548c4D\u4e16\u754c\u5efa\u6a21\u4e0e\u751f\u6210\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u53c2\u8003\u6846\u67b6\uff0c\u5305\u62ec\u5206\u7c7b\u4f53\u7cfb\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a3D/4D\u4e16\u754c\u5efa\u6a21\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u53d1\u5c55\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u524d\u666f\u3002", "relevance": 40.0}}
{"id": "2509.07993", "pdf": "https://arxiv.org/pdf/2509.07993", "abs": "https://arxiv.org/abs/2509.07993", "authors": ["Federico Fontana", "Anxhelo Diko", "Romeo Lanzino", "Marco Raoul Marini", "Bachir Kaddar", "Gian Luca Foresti", "Luigi Cinque"], "title": "Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.GR"], "comment": null, "summary": "The rapid evolution of deepfake generation technologies poses critical\nchallenges for detection systems, as non-continual learning methods demand\nfrequent and expensive retraining. We reframe deepfake detection (DFD) as a\nContinual Learning (CL) problem, proposing an efficient framework that\nincrementally adapts to emerging visual manipulation techniques while retaining\nknowledge of past generators. Our framework, unlike prior approaches that rely\non unreal simulation sequences, simulates the real-world chronological\nevolution of deepfake technologies in extended periods across 7 years.\nSimultaneously, our framework builds upon lightweight visual backbones to allow\nfor the real-time performance of DFD systems. Additionally, we contribute two\nnovel metrics: Continual AUC (C-AUC) for historical performance and Forward\nTransfer AUC (FWT-AUC) for future generalization. Through extensive\nexperimentation (over 600 simulations), we empirically demonstrate that while\nefficient adaptation (+155 times faster than full retraining) and robust\nretention of historical knowledge is possible, the generalization of current\napproaches to future generators without additional training remains near-random\n(FWT-AUC $\\approx$ 0.5) due to the unique imprint characterizing each existing\ngenerator. Such observations are the foundation of our newly proposed\nNon-Universal Deepfake Distribution Hypothesis.\n  \\textbf{Code will be released upon acceptance.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\u6765\u589e\u91cf\u9002\u5e94\u65b0\u5174\u89c6\u89c9\u64cd\u7eb5\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u8fc7\u53bb\u751f\u6210\u5668\u7684\u77e5\u8bc6\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u7ed9\u68c0\u6d4b\u7cfb\u7edf\u5e26\u6765\u4e25\u5cfb\u6311\u6218\uff0c\u4f20\u7edf\u975e\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u9891\u7e41\u4e14\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u7684\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u6a21\u62df7\u5e74\u95f4\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u771f\u5b9e\u65f6\u95f4\u6f14\u5316\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u65b0\u8bc4\u4f30\u6307\u6807C-AUC\u548cFWT-AUC\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u9002\u5e94\uff08\u6bd4\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5feb155\u500d\uff09\u548c\u7a33\u5065\u7684\u5386\u53f2\u77e5\u8bc6\u4fdd\u7559\uff0c\u4f46\u5bf9\u672a\u6765\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff08FWT-AUC\u22480.5\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u975e\u901a\u7528\u6df1\u5ea6\u4f2a\u9020\u5206\u5e03\u5047\u8bbe\uff0c\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5bf9\u672a\u6765\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "relevance": 45.0}}
{"id": "2509.07997", "pdf": "https://arxiv.org/pdf/2509.07997", "abs": "https://arxiv.org/abs/2509.07997", "authors": ["Abigail Breitfeld", "Alberto Candela", "Juan Delfa", "Akseli Kangaslahti", "Itai Zilberstein", "Steve Chien", "David Wettergreen"], "title": "Learning-Based Planning for Improving Science Return of Earth Observation Satellites", "categories": ["cs.AI", "cs.RO"], "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "Earth observing satellites are powerful tools for collecting scientific\ninformation about our planet, however they have limitations: they cannot easily\ndeviate from their orbital trajectories, their sensors have a limited field of\nview, and pointing and operating these sensors can take a large amount of the\nspacecraft's resources. It is important for these satellites to optimize the\ndata they collect and include only the most important or informative\nmeasurements. Dynamic targeting is an emerging concept in which satellite\nresources and data from a lookahead instrument are used to intelligently\nreconfigure and point a primary instrument. Simulation studies have shown that\ndynamic targeting increases the amount of scientific information gathered\nversus conventional sampling strategies. In this work, we present two different\nlearning-based approaches to dynamic targeting, using reinforcement and\nimitation learning, respectively. These learning methods build on a dynamic\nprogramming solution to plan a sequence of sampling locations. We evaluate our\napproaches against existing heuristic methods for dynamic targeting, showing\nthe benefits of using learning for this application. Imitation learning\nperforms on average 10.0\\% better than the best heuristic method, while\nreinforcement learning performs on average 13.7\\% better. We also show that\nboth learning methods can be trained effectively with relatively small amounts\nof data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\uff09\u6765\u89e3\u51b3\u536b\u661f\u52a8\u6001\u76ee\u6807\u9009\u62e9\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u63d0\u534710.0%\uff0c\u5f3a\u5316\u5b66\u4e60\u63d0\u534713.7%\uff0c\u4e14\u90fd\u80fd\u7528\u5c11\u91cf\u6570\u636e\u6709\u6548\u8bad\u7ec3\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u536b\u661f\u5728\u6570\u636e\u6536\u96c6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8f68\u9053\u56fa\u5b9a\u3001\u4f20\u611f\u5668\u89c6\u91ce\u6709\u9650\u548c\u64cd\u4f5c\u8d44\u6e90\u6d88\u8017\u5927\u3002\u9700\u8981\u4f18\u5316\u6570\u636e\u6536\u96c6\u7b56\u7565\uff0c\u53ea\u91c7\u96c6\u6700\u91cd\u8981\u7684\u4fe1\u606f\u3002\u52a8\u6001\u76ee\u6807\u9009\u62e9\u662f\u4e00\u79cd\u65b0\u5174\u6982\u5ff5\uff0c\u5229\u7528\u536b\u661f\u8d44\u6e90\u548c\u524d\u77bb\u4eea\u5668\u6570\u636e\u667a\u80fd\u8c03\u6574\u4e3b\u4eea\u5668\u914d\u7f6e\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5b66\u4e60\u65b9\u6cd5\uff1a1\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1b2\uff09\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u6765\u89c4\u5212\u91c7\u6837\u4f4d\u7f6e\u5e8f\u5217\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u52a8\u6001\u76ee\u6807\u9009\u62e9\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6a21\u4eff\u5b66\u4e60\u6bd4\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u534710.0%\uff0c\u5f3a\u5316\u5b66\u4e60\u5e73\u5747\u63d0\u534713.7%\u3002\u4e24\u79cd\u5b66\u4e60\u65b9\u6cd5\u90fd\u80fd\u7528\u76f8\u5bf9\u8f83\u5c11\u7684\u6570\u636e\u6709\u6548\u8bad\u7ec3\u3002", "conclusion": "\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u536b\u661f\u52a8\u6001\u76ee\u6807\u9009\u62e9\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6548\u679c\u6700\u4f73\uff0c\u4e14\u6570\u636e\u6548\u7387\u9ad8\uff0c\u9002\u5408\u5b9e\u9645\u536b\u661f\u5e94\u7528\u573a\u666f\u3002", "relevance": 35.0}}
{"id": "2509.08032", "pdf": "https://arxiv.org/pdf/2509.08032", "abs": "https://arxiv.org/abs/2509.08032", "authors": ["Fengyu She", "Nan Wang", "Hongfei Wu", "Ziyi Wan", "Jingmian Wang", "Chang Wang"], "title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery", "categories": ["cs.CL"], "comment": null, "summary": "Scientific literature is growing exponentially, creating a critical\nbottleneck for researchers to efficiently synthesize knowledge. While\ngeneral-purpose Large Language Models (LLMs) show potential in text processing,\nthey often fail to capture scientific domain-specific nuances (e.g., technical\njargon, methodological rigor) and struggle with complex scientific tasks,\nlimiting their utility for interdisciplinary research. To address these gaps,\nthis paper presents SciGPT, a domain-adapted foundation model for scientific\nliterature understanding and ScienceBench, an open source benchmark tailored to\nevaluate scientific LLMs.\n  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:\n(1) low-cost domain distillation via a two-stage pipeline to balance\nperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention\nmechanism that cuts memory consumption by 55\\% for 32,000-token long-document\nreasoning; and (3) knowledge-aware adaptation integrating domain ontologies to\nbridge interdisciplinary knowledge gaps.\n  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in\ncore scientific tasks including sequence labeling, generation, and inference.\nIt also exhibits strong robustness in unseen scientific tasks, validating its\npotential to facilitate AI-augmented scientific discovery.", "AI": {"tldr": "SciGPT\u662f\u57fa\u4e8eQwen3\u67b6\u6784\u7684\u79d1\u5b66\u9886\u57df\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u84b8\u998f\u3001\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u77e5\u8bc6\u611f\u77e5\u9002\u5e94\u6765\u63d0\u5347\u79d1\u5b66\u6587\u732e\u7406\u89e3\u80fd\u529b\uff0c\u5728ScienceBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-4o\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u5904\u7406\u6280\u672f\u672f\u8bed\u3001\u65b9\u6cd5\u4e25\u8c28\u6027\u548c\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u66f4\u6709\u6548\u7684AI\u5de5\u5177\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4f4e\u6210\u672c\u9886\u57df\u84b8\u998f\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\uff1b2\uff09\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u51cf\u5c11\u957f\u6587\u6863\u63a8\u7406\u5185\u5b58\u6d88\u801755%\uff1b3\uff09\u77e5\u8bc6\u611f\u77e5\u9002\u5e94\u6574\u5408\u9886\u57df\u672c\u4f53\u4ee5\u5f25\u5408\u8de8\u5b66\u79d1\u77e5\u8bc6\u9e3f\u6c9f\u3002", "result": "\u5728ScienceBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSciGPT\u5728\u5e8f\u5217\u6807\u6ce8\u3001\u751f\u6210\u548c\u63a8\u7406\u7b49\u6838\u5fc3\u79d1\u5b66\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u5e76\u5728\u672a\u89c1\u79d1\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\u3002", "conclusion": "SciGPT\u8bc1\u660e\u4e86\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u79d1\u5b66LLM\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u4fc3\u8fdbAI\u8f85\u52a9\u79d1\u5b66\u53d1\u73b0\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2509.08003", "pdf": "https://arxiv.org/pdf/2509.08003", "abs": "https://arxiv.org/abs/2509.08003", "authors": ["Shahid Shafi Dar", "Bharat Kaurav", "Arnav Jain", "Chandravardhan Singh Raghaw", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities", "categories": ["cs.CV"], "comment": null, "summary": "In an era of escalating climate change, urban flooding has emerged as a\ncritical challenge for sustainable cities, threatening lives, infrastructure,\nand ecosystems. Traditional flood detection methods are constrained by their\nreliance on unimodal data and static rule-based systems, which fail to capture\nthe dynamic, non-linear relationships inherent in flood events. Furthermore,\nexisting attention mechanisms and ensemble learning approaches exhibit\nlimitations in hierarchical refinement, cross-modal feature integration, and\nadaptability to noisy or unstructured environments, resulting in suboptimal\nflood classification performance. To address these challenges, we present\nXFloodNet, a novel framework that redefines urban flood classification through\nadvanced deep-learning techniques. XFloodNet integrates three novel components:\n(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically\naligns visual and textual features, enabling precise multi-granularity\ninteractions and resolving contextual ambiguities; (2) a Heterogeneous\nConvolutional Adaptive Multi-Scale Attention module, which leverages\nfrequency-enhanced channel attention and frequency-modulated spatial attention\nto extract and prioritize discriminative flood-related features across spectral\nand spatial domains; and (3) a Cascading Convolutional Transformer Feature\nRefinement technique that harmonizes hierarchical features through adaptive\nscaling and cascading operations, ensuring robust and noise-resistant flood\ndetection. We evaluate our proposed method on three benchmark datasets, such as\nChennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves\nstate-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,\nsurpassing existing methods by significant margins.", "AI": {"tldr": "XFloodNet\u662f\u4e00\u4e2a\u7528\u4e8e\u57ce\u5e02\u6d2a\u6c34\u5206\u7c7b\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u8de8\u6a21\u6001\u95e8\u63a7\u6ce8\u610f\u529b\u3001\u5f02\u6784\u5377\u79ef\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u548c\u7ea7\u8054\u5377\u79efTransformer\u7279\u5f81\u7cbe\u70bc\u6280\u672f\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6d2a\u6c34\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\u548c\u9759\u6001\u89c4\u5219\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6355\u6349\u6d2a\u6c34\u4e8b\u4ef6\u7684\u52a8\u6001\u975e\u7ebf\u6027\u5173\u7cfb\u3002\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u5c42\u6b21\u7cbe\u70bc\u3001\u8de8\u6a21\u6001\u7279\u5f81\u96c6\u6210\u548c\u5bf9\u566a\u58f0\u73af\u5883\u7684\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u6d2a\u6c34\u5206\u7c7b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86XFloodNet\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5c42\u6b21\u5316\u8de8\u6a21\u6001\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u52a8\u6001\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff1b2\uff09\u5f02\u6784\u5377\u79ef\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528\u9891\u7387\u589e\u5f3a\u7684\u901a\u9053\u6ce8\u610f\u529b\u548c\u9891\u7387\u8c03\u5236\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\uff1b3\uff09\u7ea7\u8054\u5377\u79efTransformer\u7279\u5f81\u7cbe\u70bc\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u548c\u7ea7\u8054\u64cd\u4f5c\u534f\u8c03\u5c42\u6b21\u7279\u5f81\u3002", "result": "\u5728Chennai Floods\u3001Rhine18 Floods\u548cHarz17 Floods\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230\u4e8693.33%\u300182.24%\u548c88.60%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "XFloodNet\u901a\u8fc7\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u91cd\u65b0\u5b9a\u4e49\u4e86\u57ce\u5e02\u6d2a\u6c34\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6d2a\u6c34\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "relevance": 25.0}}
{"id": "2509.08058", "pdf": "https://arxiv.org/pdf/2509.08058", "abs": "https://arxiv.org/abs/2509.08058", "authors": ["Kai Ye", "Liangcai Su", "Chenxiong Qian"], "title": "How Far Are We from True Unlearnability?", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by ICLR 2025", "summary": "High-quality data plays an indispensable role in the era of large models, but\nthe use of unauthorized data for model training greatly damages the interests\nof data owners. To overcome this threat, several unlearnable methods have been\nproposed, which generate unlearnable examples (UEs) by compromising the\ntraining availability of data. Clearly, due to unknown training purposes and\nthe powerful representation learning capabilities of existing models, these\ndata are expected to be unlearnable for models across multiple tasks, i.e.,\nthey will not help improve the model's performance. However, unexpectedly, we\nfind that on the multi-task dataset Taskonomy, UEs still perform well in tasks\nsuch as semantic segmentation, failing to exhibit cross-task unlearnability.\nThis phenomenon leads us to question: How far are we from attaining truly\nunlearnable examples? We attempt to answer this question from the perspective\nof model optimization. To this end, we observe the difference in the\nconvergence process between clean and poisoned models using a simple model\narchitecture. Subsequently, from the loss landscape we find that only a part of\nthe critical parameter optimization paths show significant differences,\nimplying a close relationship between the loss landscape and unlearnability.\nConsequently, we employ the loss landscape to explain the underlying reasons\nfor UEs and propose Sharpness-Aware Learnability (SAL) to quantify the\nunlearnability of parameters based on this explanation. Furthermore, we propose\nan Unlearnable Distance (UD) to measure the unlearnability of data based on the\nSAL distribution of parameters in clean and poisoned models. Finally, we\nconduct benchmark tests on mainstream unlearnable methods using the proposed\nUD, aiming to promote community awareness of the capability boundaries of\nexisting unlearnable methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u5931\u6548\uff0c\u4ece\u6a21\u578b\u4f18\u5316\u89d2\u5ea6\u5206\u6790\u4e86\u635f\u5931\u666f\u89c2\u4e0e\u4e0d\u53ef\u5b66\u4e60\u6027\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u53c2\u6570\u9510\u5ea6\u611f\u77e5\u53ef\u5b66\u4e60\u6027(SAL)\u7684\u4e0d\u53ef\u5b66\u4e60\u8ddd\u79bb(UD)\u6765\u8861\u91cf\u6570\u636e\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u5bf9\u4e3b\u6d41\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u9ad8\u8d28\u91cf\u6570\u636e\u5728\u5927\u6a21\u578b\u65f6\u4ee3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u4f7f\u7528\u635f\u5bb3\u6570\u636e\u6240\u6709\u8005\u5229\u76ca\u3002\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7684\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u5728\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u4ecd\u7136\u6709\u6548\uff0c\u672a\u80fd\u5c55\u73b0\u8de8\u4efb\u52a1\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u771f\u6b63\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u7684\u5b9e\u73b0\u8ddd\u79bb\u3002", "method": "1) \u89c2\u5bdf\u5e72\u51c0\u6a21\u578b\u548c\u4e2d\u6bd2\u6a21\u578b\u5728\u6536\u655b\u8fc7\u7a0b\u4e2d\u7684\u5dee\u5f02\uff1b2) \u4ece\u635f\u5931\u666f\u89c2\u89d2\u5ea6\u5206\u6790\u5173\u952e\u53c2\u6570\u4f18\u5316\u8def\u5f84\u7684\u5dee\u5f02\uff1b3) \u63d0\u51fa\u9510\u5ea6\u611f\u77e5\u53ef\u5b66\u4e60\u6027(SAL)\u6765\u91cf\u5316\u53c2\u6570\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\uff1b4) \u57fa\u4e8eSAL\u5206\u5e03\u63d0\u51fa\u4e0d\u53ef\u5b66\u4e60\u8ddd\u79bb(UD)\u6765\u8861\u91cf\u6570\u636e\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\uff1b5) \u5bf9\u4e3b\u6d41\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u635f\u5931\u666f\u89c2\u4e0e\u4e0d\u53ef\u5b66\u4e60\u6027\u5bc6\u5207\u76f8\u5173\uff0c\u63d0\u51fa\u7684UD\u6307\u6807\u80fd\u591f\u6709\u6548\u8861\u91cf\u6570\u636e\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "\u5f53\u524d\u8ddd\u79bb\u5b9e\u73b0\u771f\u6b63\u4e0d\u53ef\u5b66\u4e60\u7684\u6837\u672c\u8fd8\u6709\u5dee\u8ddd\uff0c\u635f\u5931\u666f\u89c2\u4e3a\u7406\u89e3\u4e0d\u53ef\u5b66\u4e60\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0cSAL\u548cUD\u6307\u6807\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u9700\u8981\u793e\u533a\u8fdb\u4e00\u6b65\u5173\u6ce8\u73b0\u6709\u65b9\u6cd5\u7684\u80fd\u529b\u8fb9\u754c\u3002", "relevance": 75.0}}
{"id": "2509.08088", "pdf": "https://arxiv.org/pdf/2509.08088", "abs": "https://arxiv.org/abs/2509.08088", "authors": ["Linyao Chen", "Zimian Peng", "Yingxuan Yang", "Yikun Wang", "Wenzheng Tom Tang", "Hiroki H. Kobayashi", "Weinan Zhang"], "title": "EnvX: Agentize Everything with Agentic AI", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The widespread availability of open-source repositories has led to a vast\ncollection of reusable software components, yet their utilization remains\nmanual, error-prone, and disconnected. Developers must navigate documentation,\nunderstand APIs, and write integration code, creating significant barriers to\nefficient software reuse. To address this, we present EnvX, a framework that\nleverages Agentic AI to agentize GitHub repositories, transforming them into\nintelligent, autonomous agents capable of natural language interaction and\ninter-agent collaboration. Unlike existing approaches that treat repositories\nas static code resources, EnvX reimagines them as active agents through a\nthree-phase process: (1) TODO-guided environment initialization, which sets up\nthe necessary dependencies, data, and validation datasets; (2) human-aligned\nagentic automation, allowing repository-specific agents to autonomously perform\nreal-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple\nagents to collaborate. By combining large language model capabilities with\nstructured tool integration, EnvX automates not just code generation, but the\nentire process of understanding, initializing, and operationalizing repository\nfunctionality. We evaluate EnvX on the GitTaskBench benchmark, using 18\nrepositories across domains such as image processing, speech recognition,\ndocument analysis, and video manipulation. Our results show that EnvX achieves\na 74.07% execution completion rate and 51.85% task pass rate, outperforming\nexisting frameworks. Case studies further demonstrate EnvX's ability to enable\nmulti-repository collaboration via the A2A protocol. This work marks a shift\nfrom treating repositories as passive code resources to intelligent,\ninteractive agents, fostering greater accessibility and collaboration within\nthe open-source ecosystem.", "AI": {"tldr": "EnvX\u662f\u4e00\u4e2a\u5229\u7528Agentic AI\u5c06GitHub\u4ed3\u5e93\u8f6c\u5316\u4e3a\u667a\u80fd\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u4ee3\u7406\u95f4\u534f\u4f5c\u5b9e\u73b0\u8f6f\u4ef6\u7ec4\u4ef6\u7684\u81ea\u52a8\u5316\u91cd\u7528\u3002", "motivation": "\u89e3\u51b3\u5f00\u6e90\u8f6f\u4ef6\u7ec4\u4ef6\u91cd\u7528\u8fc7\u7a0b\u4e2d\u624b\u52a8\u64cd\u4f5c\u3001\u6613\u51fa\u9519\u548c\u5272\u88c2\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u5f00\u53d1\u8005\u7406\u89e3API\u548c\u7f16\u5199\u96c6\u6210\u4ee3\u7801\u7684\u95e8\u69db\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) TODO\u5f15\u5bfc\u7684\u73af\u5883\u521d\u59cb\u5316\uff1b2) \u4eba\u7c7b\u5bf9\u9f50\u7684\u4ee3\u7406\u81ea\u52a8\u5316\uff1b3) \u4ee3\u7406\u95f4(A2A)\u534f\u4f5c\u534f\u8bae\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u548c\u7ed3\u6784\u5316\u5de5\u5177\u96c6\u6210\u3002", "result": "\u5728GitTaskBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEnvX\u8fbe\u523074.07%\u7684\u6267\u884c\u5b8c\u6210\u7387\u548c51.85%\u7684\u4efb\u52a1\u901a\u8fc7\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff0c\u5e76\u80fd\u5b9e\u73b0\u591a\u4ed3\u5e93\u534f\u4f5c\u3002", "conclusion": "EnvX\u5c06\u4ed3\u5e93\u4ece\u88ab\u52a8\u4ee3\u7801\u8d44\u6e90\u8f6c\u53d8\u4e3a\u667a\u80fd\u4ea4\u4e92\u4ee3\u7406\uff0c\u63d0\u5347\u4e86\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u534f\u4f5c\u6027\u3002", "relevance": 65.0}}
{"id": "2509.08075", "pdf": "https://arxiv.org/pdf/2509.08075", "abs": "https://arxiv.org/abs/2509.08075", "authors": ["Flor Miriam Plaza-del-Arco", "Paul R\u00f6ttger", "Nino Scherrer", "Emanuele Borgonovo", "Elmar Plischke", "Dirk Hovy"], "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u4e2a\u6027\u5316\u4e2d\u7684\u4eba\u7269\u63d0\u793a\u5bf9\u9519\u8bef\u62d2\u7edd\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u548c\u4efb\u52a1\u7c7b\u578b\u6bd4\u4eba\u7269\u7279\u5f81\u5bf9\u9519\u8bef\u62d2\u7edd\u7684\u5f71\u54cd\u66f4\u5927", "motivation": "\u91cf\u5316LLM\u4e2a\u6027\u5316\u4e2d\u4eba\u7269\u63d0\u793a\u5bfc\u81f4\u7684\u9519\u8bef\u62d2\u7edd\u95ee\u9898\uff0c\u63a2\u7a76\u4e0d\u540c\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4eba\u7269\u5bf9\u6a21\u578b\u62d2\u7edd\u884c\u4e3a\u7684\u5f71\u54cd", "method": "\u4f7f\u752815\u79cd\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u4eba\u7269\u7279\u5f81\u300116\u4e2a\u4e0d\u540c\u6a21\u578b\u30013\u4e2a\u4efb\u52a1\u7c7b\u578b\u548c9\u79cd\u63d0\u793a\u6539\u5199\uff0c\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u91cf\u5316\u5206\u6790", "result": "\u6a21\u578b\u80fd\u529b\u8d8a\u5f3a\uff0c\u4eba\u7269\u7279\u5f81\u5bf9\u62d2\u7edd\u7387\u7684\u5f71\u54cd\u8d8a\u5c0f\uff1b\u67d0\u4e9b\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u4f1a\u589e\u52a0\u9519\u8bef\u62d2\u7edd\u7387\uff1b\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u7c7b\u578b\u5bf9\u9519\u8bef\u62d2\u7edd\u6709\u663e\u8457\u5f71\u54cd", "conclusion": "\u4eba\u7269\u6548\u5e94\u53ef\u80fd\u88ab\u9ad8\u4f30\uff0c\u9519\u8bef\u62d2\u7edd\u66f4\u591a\u53d7\u6a21\u578b\u80fd\u529b\u548c\u4efb\u52a1\u654f\u611f\u6027\u5f71\u54cd\uff0c\u800c\u975e\u4eba\u7269\u7279\u5f81\u672c\u8eab", "relevance": 85.0}}
{"id": "2509.08016", "pdf": "https://arxiv.org/pdf/2509.08016", "abs": "https://arxiv.org/abs/2509.08016", "authors": ["Hyungjin Chung", "Hyelin Nam", "Jiyeon Kim", "Hyojun Go", "Byeongjun Park", "Junho Kim", "Joonseok Lee", "Seongsu Ha", "Byung-Hoon Kim"], "title": "Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/hyungjin-chung/VPS", "summary": "Video Large Language Models (VideoLLMs) face a critical bottleneck:\nincreasing the number of input frames to capture fine-grained temporal detail\nleads to prohibitive computational costs and performance degradation from long\ncontext lengths. We introduce Video Parallel Scaling (VPS), an inference-time\nmethod that expands a model's perceptual bandwidth without increasing its\ncontext window. VPS operates by running multiple parallel inference streams,\neach processing a unique, disjoint subset of the video's frames. By aggregating\nthe output probabilities from these complementary streams, VPS integrates a\nricher set of visual information than is possible with a single pass. We\ntheoretically show that this approach effectively contracts the Chinchilla\nscaling law by leveraging uncorrelated visual evidence, thereby improving\nperformance without additional training. Extensive experiments across various\nmodel architectures and scales (2B-32B) on benchmarks such as Video-MME and\nEventHallusion demonstrate that VPS consistently and significantly improves\nperformance. It scales more favorably than other parallel alternatives (e.g.\nSelf-consistency) and is complementary to other decoding strategies, offering a\nmemory-efficient and robust framework for enhancing the temporal reasoning\ncapabilities of VideoLLMs.", "AI": {"tldr": "Video Parallel Scaling (VPS) \u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u89c6\u9891\u5e27\u5b50\u96c6\u5e76\u805a\u5408\u8f93\u51fa\u6982\u7387\uff0c\u5728\u4e0d\u589e\u52a0\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347VideoLLMs\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b", "motivation": "VideoLLMs\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u548c\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u65f6\u5e8f\u7ec6\u8282\u6355\u6349\u80fd\u529b", "method": "VPS\u8fd0\u884c\u591a\u4e2a\u5e76\u884c\u63a8\u7406\u6d41\uff0c\u6bcf\u4e2a\u6d41\u5904\u7406\u89c6\u9891\u5e27\u7684\u4e0d\u540c\u5b50\u96c6\uff0c\u7136\u540e\u805a\u5408\u8fd9\u4e9b\u4e92\u8865\u6d41\u7684\u8f93\u51fa\u6982\u7387\u6765\u6574\u5408\u66f4\u4e30\u5bcc\u7684\u89c6\u89c9\u4fe1\u606f", "result": "\u5728Video-MME\u548cEventHallusion\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVPS\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21(2B-32B)\u4e0a\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6bd4Self-consistency\u7b49\u5176\u4ed6\u5e76\u884c\u65b9\u6cd5\u6269\u5c55\u6027\u66f4\u597d", "conclusion": "VPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e0d\u76f8\u5173\u7684\u89c6\u89c9\u8bc1\u636e\u6765\u589e\u5f3aVideoLLMs\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u4e0e\u5176\u4ed6\u89e3\u7801\u7b56\u7565\u4e92\u8865", "relevance": 75.0}}
{"id": "2509.08086", "pdf": "https://arxiv.org/pdf/2509.08086", "abs": "https://arxiv.org/abs/2509.08086", "authors": ["Michael Kishelev", "Pranab Bhadani", "Wanying Ding", "Vinay Chaudhri"], "title": "JEL: A Novel Model Linking Knowledge Graph entities to News Mentions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present JEL, a novel computationally efficient end-to-end multi-neural\nnetwork based entity linking model, which beats current state-of-art model.\nKnowledge Graphs have emerged as a compelling abstraction for capturing\ncritical relationships among the entities of interest and integrating data from\nmultiple heterogeneous sources. A core problem in leveraging a knowledge graph\nis linking its entities to the mentions (e.g., people, company names) that are\nencountered in textual sources (e.g., news, blogs., etc) correctly, since there\nare thousands of entities to consider for each mention. This task of linking\nmentions and entities is referred as Entity Linking (EL). It is a fundamental\ntask in natural language processing and is beneficial in various uses cases,\nsuch as building a New Analytics platform. News Analytics, in JPMorgan, is an\nessential task that benefits multiple groups across the firm. According to a\nsurvey conducted by the Innovation Digital team 1 , around 25 teams across the\nfirm are actively looking for news analytics solutions, and more than \\$2\nmillion is being spent annually on external vendor costs. Entity linking is\ncritical for bridging unstructured news text with knowledge graphs, enabling\nusers access to vast amounts of curated data in a knowledge graph and\ndramatically facilitating their daily work.", "AI": {"tldr": "JEL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8ba1\u7b97\u9ad8\u6548\u7aef\u5230\u7aef\u591a\u795e\u7ecf\u7f51\u7edc\u5b9e\u4f53\u94fe\u63a5\u6a21\u578b\uff0c\u5728\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u9700\u8981\u5c06\u6587\u672c\u4e2d\u7684\u63d0\u53ca\u6b63\u786e\u94fe\u63a5\u5230\u5b9e\u4f53\uff0c\u8fd9\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5bf9\u65b0\u95fb\u5206\u6790\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u6469\u6839\u5927\u901a\u6bcf\u5e74\u82b1\u8d39\u8d85\u8fc7200\u4e07\u7f8e\u5143\u7528\u4e8e\u5916\u90e8\u4f9b\u5e94\u5546\u6210\u672c\uff0c\u5b9e\u4f53\u94fe\u63a5\u80fd\u591f\u6865\u63a5\u975e\u7ed3\u6784\u5316\u65b0\u95fb\u6587\u672c\u4e0e\u77e5\u8bc6\u56fe\u8c31", "method": "\u57fa\u4e8e\u591a\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aef\u5b9e\u4f53\u94fe\u63a5\u6a21\u578b\uff0c\u8ba1\u7b97\u9ad8\u6548", "result": "\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u6a21\u578b", "conclusion": "JEL\u6a21\u578b\u5728\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u65b0\u95fb\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528", "relevance": 20.0}}
{"id": "2509.08151", "pdf": "https://arxiv.org/pdf/2509.08151", "abs": "https://arxiv.org/abs/2509.08151", "authors": ["Botao Zhu", "Jeslyn Wang", "Dusit Niyato", "Xianbin Wang"], "title": "Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI", "categories": ["cs.AI"], "comment": null, "summary": "Accurate trustworthiness evaluation of potential collaborating devices is\nessential for the effective execution of complex computing tasks. This\nevaluation process involves collecting diverse trust-related data from\npotential collaborators, including historical performance and available\nresources, for collaborator selection. However, when each task owner\nindependently assesses all collaborators' trustworthiness, frequent data\nexchange, complex reasoning, and dynamic situation changes can result in\nsignificant overhead and deteriorated trust evaluation. To overcome these\nchallenges, we propose a task-specific trust semantics distillation (2TSD)\nmodel based on a large AI model (LAM)-driven teacher-student agent\narchitecture. The teacher agent is deployed on a server with powerful\ncomputational capabilities and an augmented memory module dedicated to\nmultidimensional trust-related data collection, task-specific trust semantics\nextraction, and task-collaborator matching analysis. Upon receiving\ntask-specific requests from device-side student agents, the teacher agent\ntransfers the trust semantics of potential collaborators to the student agents,\nenabling rapid and accurate collaborator selection. Experimental results\ndemonstrate that the proposed 2TSD model can reduce collaborator evaluation\ntime, decrease device resource consumption, and improve the accuracy of\ncollaborator selection.", "AI": {"tldr": "\u57fa\u4e8e\u5927AI\u6a21\u578b\u7684\u5e08\u751f\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u8bed\u4e49\u84b8\u998f(2TSD)\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u534f\u4f5c\u8bbe\u5907\u4fe1\u4efb\u8bc4\u4f30\u548c\u9009\u62e9", "motivation": "\u89e3\u51b3\u4f20\u7edf\u534f\u4f5c\u8bbe\u5907\u4fe1\u4efb\u8bc4\u4f30\u4e2d\u6570\u636e\u4ea4\u6362\u9891\u7e41\u3001\u63a8\u7406\u590d\u6742\u3001\u52a8\u6001\u53d8\u5316\u5e26\u6765\u7684\u5f00\u9500\u5927\u548c\u8bc4\u4f30\u8d28\u91cf\u4e0b\u964d\u95ee\u9898", "method": "\u91c7\u7528LAM\u9a71\u52a8\u7684\u5e08\u751f\u4ee3\u7406\u67b6\u6784\uff0c\u6559\u5e08\u4ee3\u7406\u8d1f\u8d23\u591a\u7ef4\u4fe1\u4efb\u6570\u636e\u6536\u96c6\u548c\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u8bed\u4e49\u63d0\u53d6\uff0c\u5b66\u751f\u4ee3\u7406\u63a5\u6536\u8bed\u4e49\u8fdb\u884c\u5feb\u901f\u534f\u4f5c\u9009\u62e9", "result": "\u5b9e\u9a8c\u8868\u660e2TSD\u6a21\u578b\u80fd\u51cf\u5c11\u8bc4\u4f30\u65f6\u95f4\u3001\u964d\u4f4e\u8bbe\u5907\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u9ad8\u534f\u4f5c\u9009\u62e9\u51c6\u786e\u6027", "conclusion": "\u63d0\u51fa\u76842TSD\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u4fe1\u4efb\u8bc4\u4f30\u6311\u6218", "relevance": 45.0}}
