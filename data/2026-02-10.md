<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 107]
- [cs.CV](#cs.CV) [Total: 229]
- [cs.AI](#cs.AI) [Total: 242]
- [cs.LG](#cs.LG) [Total: 331]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

Relevance: 85.0

TL;DR: ILA-agent框架让LLM通过推理时语言习得(ILA)范式，在有限外部资源下动态掌握新编程语言，显著优于检索增强基线


<details>
  <summary>Details</summary>
Motivation: LLM在编码任务上的能力依赖于大量预训练数据，但面对不熟悉的编程语言时性能会急剧下降。传统的数据密集型微调方法成本高昂，需要探索更高效的推理时学习范式

Method: 提出ILA-agent框架，将人类行为建模为一组工具，让LLM通过结构化交互（查阅官方文档、执行环境验证）增量探索、应用和验证语言知识。构建Cangjie-bench基准，并在Cangjie语言上实例化ILA-agent进行评估

Result: ILA-agent在代码生成、翻译和程序修复任务上显著优于检索增强基线。轨迹分析揭示了涌现的行为模式，同时指出了持续存在的性能差距

Conclusion: ILA范式为LLM掌握新编程语言提供了高效途径，无需大量微调数据。ILA-agent框架展示了通过结构化工具交互实现推理时语言习得的可行性

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [2] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Anchored Decoding方法，在推理时抑制语言模型的逐字复制行为，通过将生成约束在安全模型附近，实现可控的风险-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型倾向于记忆训练数据并逐字输出，当涉及敏感或受版权保护的内容时，这带来了创作者同意与补偿问题以及开发者的合规风险。需要一种方法在保持模型效用的同时减少逐字复制。

Method: 提出Anchored Decoding，一种即插即用的推理时方法：通过将生成约束在经许可训练的安全模型附近来抑制逐字复制。方法自适应分配用户选择的信息预算，实施每步约束以获得序列级保证。还提出字节级变体Anchored$_{\mathrm{Byte}}$ Decoding，通过ByteSampler框架实现跨词汇表融合。

Result: 在六个模型对上评估，Anchored Decoding定义了新的帕累托前沿，在保持接近原始流畅性和事实性的同时，消除了高达75%的可测量复制差距（平均六种复制指标），推理开销适中。

Conclusion: Anchored Decoding提供了一种实用的推理时解决方案，通过将生成锚定在安全模型附近，有效平衡版权风险与模型效用，为混合许可数据训练的语言模型提供了合规使用途径。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [3] [Free Energy Mixer](https://arxiv.org/abs/2602.07160)
*Jiecheng Lu,Shihao Yang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Free Energy Mixer (FEM)，一种基于自由能（log-sum-exp）的注意力读取机制，通过值驱动的每通道对数线性倾斜来改进标准注意力，实现从平均到选择的平滑过渡


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制通过每头凸平均读取键值对，无法实现通道级选择。需要一种既能保持并行性和复杂度，又能实现值感知读取的机制

Method: FEM将标准注意力中的(q,k)评分分布作为先验，应用值驱动的每通道对数线性倾斜，生成值感知的后验读取。采用两级门控FEM设计，可与标准/线性注意力、线性RNN和SSM即插即用

Result: 在NLP、视觉和时间序列任务上，FEM在相同参数预算下持续优于强基线模型

Conclusion: FEM提供了一种灵活高效的注意力读取机制，能在保持原有复杂度的同时实现从平均到选择的平滑过渡，为Transformer架构优化提供了新思路

Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.

</details>


### [4] [Your Language Model Secretly Contains Personality Subnetworks](https://arxiv.org/abs/2602.07164)
*Ruimeng Ye,Zihan Wang,Zinan Ling,Yang Xiao,Manling Li,Xiaolong Ma,Bo Hui*

Main category: cs.CL

Relevance: 85.0

TL;DR: LLMs内部已经包含了不同人格的专用子网络，无需外部知识或参数调整即可激活特定人格行为。通过校准数据集识别激活特征，使用掩码策略分离轻量级人格子网络，并引入对比剪枝技术增强对立人格的分离效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过提示、检索增强生成或微调等外部知识来调整LLM的人格行为。本文质疑：LLM是否真的需要外部上下文或参数来适应不同行为，还是这些知识已经嵌入在参数中？探索LLM内部是否已包含人格专用子网络。

Method: 1) 使用小型校准数据集识别不同人格的激活特征；2) 基于这些统计特征开发掩码策略，隔离轻量级人格子网络；3) 针对二元对立人格（如内向-外向），引入对比剪枝策略，识别导致统计差异的参数。

Result: 生成的子网络在多样化评估设置中表现出比需要外部知识的基线方法更强的人格对齐，同时更高效。方法完全无需训练，仅依赖语言模型现有参数空间。

Conclusion: 多样化的人类行为并非仅仅在LLM中被诱导产生，而是已经嵌入其参数空间中。这为大型语言模型的可控和可解释个性化提供了新视角。

Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

</details>


### [5] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出利用人格特质作为潜在信号来指导LLM个性化回答，通过人格对齐的偏好选择显著提升问答准确性，并构建了人格标注的偏好数据集PACIFIC及相应框架。


<details>
  <summary>Details</summary>
Motivation: 用户偏好常用于个性化LLM回答，但偏好信号可能嘈杂、不完整甚至误导，直接应用可能降低回答质量。作者观察到稳定的人格特质塑造日常偏好，因此研究将人格作为偏好背后的原则性"潜在"信号。

Method: 1) 通过实验验证人格对齐偏好的有效性；2) 构建PACIFIC数据集，包含1200个跨领域偏好陈述，标注Big-Five人格特质方向；3) 提出框架使LLM能自动检索人格对齐偏好并整合到回答生成中。

Result: 使用人格对齐偏好显著提升个性化问答：选择与用户推断人格一致的偏好，将答案选择准确率从29.25%提高到76%（相比随机选择偏好）。

Conclusion: 人格特质可作为可靠信号指导LLM个性化回答，人格对齐的偏好选择能显著改善回答质量。提出的PACIFIC数据集和框架为实现基于人格的个性化LLM回答提供了实用工具。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [6] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了一个风险敏感的幻觉评估框架，用于评估医疗问答中LLM幻觉的潜在危害程度，而不仅仅是事实正确性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉评估标准主要关注事实正确性，将所有错误视为同等严重，这掩盖了临床相关的失败模式，特别是当模型生成无支持但可操作的医疗语言时。在面向患者的医疗问答中，不同幻觉的危害程度差异很大。

Method: 提出风险敏感评估框架，通过风险承载语言（治疗指令、禁忌症、紧急提示、高风险药物提及）量化幻觉。结合风险评分与相关性度量，识别高风险、低基础支持的失败。使用受控的面向患者提示作为安全压力测试，应用于三个指令调优语言模型。

Result: 结果显示，具有相似表面行为的模型表现出显著不同的风险特征，标准评估指标无法捕捉这些区别。评估有效性严重依赖于任务和提示设计。

Conclusion: 将风险敏感性纳入幻觉评估非常重要，评估有效性严重依赖于任务和提示设计。这为医疗AI安全评估提供了新视角。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [7] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出"对话中迷失"现象的根本原因是意图对齐差距而非模型能力缺陷，通过解耦意图理解与任务执行的Mediator-Assistant架构来解决问题。


<details>
  <summary>Details</summary>
Motivation: 多轮对话已成为LLMs主要交互范式，但研究发现LLMs在多轮对话中性能显著下降（Lost in Conversation现象）。先前工作将此归因于模型不可靠性，但本文认为根本原因在于意图对齐差距而非内在能力缺陷。

Method: 提出Mediator-Assistant架构，通过经验驱动的Mediator将模糊用户输入转化为基于历史交互模式的明确结构化指令，从而解耦意图理解与任务执行。

Result: 实验结果表明该方法能显著减轻多轮对话中的性能下降，在不同LLMs上均有效。

Conclusion: Lost in Conversation现象源于用户与LLMs交互中的意图对齐差距，而非模型能力限制。通过Mediator-Assistant架构可以有效地桥接模糊用户意图与模型解释之间的鸿沟。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [8] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

Relevance: 85.0

TL;DR: TernaryLM：一种132M参数的Transformer架构，采用原生1位三元量化{-1, 0, +1}进行训练，显著减少内存占用而不牺牲语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能卓越但需要大量计算资源，限制了在边缘设备和资源受限环境中的部署。现有后训练量化方法存在精度损失问题，需要探索原生量化训练方法。

Method: 采用原生1位三元量化{-1, 0, +1}进行从头训练，使用直通估计器和自适应逐层缩放因子学习量化感知表示，而非后训练量化。

Result: 1) TinyStories验证困惑度58.42；2) MRPC复述检测F1分数82.47%；3) 内存减少2.4倍（498MB vs 1197MB）；4) 不同语料库上训练稳定；5) 中间Transformer层对极端量化兼容性最高。

Conclusion: 原生1位训练是高效神经语言模型的有前景方向，中间层对极端量化兼容性最高的发现为未来非均匀精度策略提供了指导。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [9] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出基于一阶统计特性的轻量级后训练剪枝框架，通过通道统计校准重要性分数，并使用能量补偿修正分布失真，无需重训练或二阶信息


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法在剪枝质量和计算效率之间存在权衡：启发式方法高效但对激活异常值敏感，重构方法保真度高但计算代价大

Method: 基于模型权重和激活的一阶统计特性，剪枝时使用通道统计校准基于幅值的重要性分数，剪枝后应用解析能量补偿修正权重移除引起的分布失真

Result: 在多个LLM家族、稀疏模式和评估任务上的实验表明，该方法提高了剪枝性能，同时保持了与启发式方法相当的计算成本

Conclusion: 简单的统计修正对于LLM的后训练剪枝是有效的，无需重训练、梯度或二阶信息

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [10] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

Relevance: 85.0

TL;DR: Demo-SafetyBench：通过建模人口统计学多元主义来评估LLM安全性的基准，解决了现有安全数据集在人口统计学多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: LLM安全本质上是多元的，反映了道德规范、文化期望和人口统计学背景的差异。现有对齐数据集（如ANTHROPIC-HH和DICES）使用人口统计学狭窄的标注者群体，忽略了不同社区对安全感知的差异。

Method: 两阶段方法：第一阶段将DICES提示重新分类为14个安全领域（基于BEAVERTAILS），使用Mistral 7B-Instruct-v0.3，保留人口统计学元数据，并通过Llama-3.1-8B-Instruct和SimHash去重扩展低资源领域，得到43,050个样本。第二阶段使用LLMs-as-Raters（Gemma-7B、GPT-4o、LLaMA-2-7B）进行零样本推理评估多元敏感性。

Result: 平衡阈值（delta=0.5, tau=10）实现了高可靠性（ICC=0.87）和低人口统计学敏感性（DS=0.12），证实多元安全评估可以同时具有可扩展性和人口统计学鲁棒性。

Conclusion: Demo-SafetyBench通过直接在提示层面建模人口统计学多元主义，将价值框架与响应解耦，为LLM安全评估提供了更全面、更具代表性的基准。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [11] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

Relevance: 85.0

TL;DR: AlignX是一个两阶段框架，通过提示注入微调和几何校准的MoE来解决多目标对齐中的轴崩溃问题，显著提升LLM在帮助性、无害性和诚实性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法（SFT和MoE）在多目标对齐中存在挑战：SFT导致冲突目标间的干扰，MoE存在路由校准问题，导致轴崩溃现象，表现为特征空间分离和不可靠推理。

Method: 两阶段框架：第一阶段使用提示注入微调提取轴特定任务特征，缓解灾难性遗忘；第二阶段部署MoCaE模块，利用分形和自然几何校准专家路由，提高推理可靠性。

Result: 在Alpaca（帮助性）、BeaverTails（无害性）和TruthfulQA（诚实性）上取得显著提升：胜率+171.5%，真实性-信息性+110.1%，安全违规减少4.3%。相比先前MoE减少35%以上的延迟和内存使用。

Conclusion: AlignX有效解决了多目标对齐中的轴崩溃问题，在保持模型性能的同时提高了效率和可靠性，在四个LLM上的结果验证了其泛化能力。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [12] [DLLM Agent: See Farther, Run Faster](https://arxiv.org/abs/2602.07451)
*Huiling Zhen,Weizhe Lin,Renxi Liu,Kai Han,Yiming Li,Yuchuan Tian,Hanting Chen,Xiaoguang Li,Xiaosong Li,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Youliang Yan,Peifeng Qin,Jun Wang,Yu Wang,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文研究了扩散大语言模型（DLLMs）在智能体多步决策中的应用，发现相比自回归模型，DLLM智能体在保持相似准确率的情况下，端到端速度平均提升30%以上，且需要更少的交互轮次和工具调用。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归解码的替代方案，在效率和建模特性上具有吸引力，但其在智能体多步决策中的影响尚未充分探索。研究旨在探究当生成范式改变但智能体框架和监督保持不变时，扩散骨干是否会引发系统性的不同规划和工具使用行为，以及这些差异是否能转化为端到端的效率提升。

Method: 在受控环境中，将DLLM和AR骨干实例化到相同的智能体工作流（DeepDiver）中，并在相同的轨迹数据上进行匹配的智能体导向微调，产生扩散支持的DLLM智能体和直接可比较的AR智能体。研究还分析了两种范式的实际部署考虑，包括工具调用训练和注意力掩码对齐。

Result: 在相似准确率下，DLLM智能体端到端速度平均比AR智能体快30%以上，某些情况下超过8倍加速。在正确完成任务的前提下，DLLM智能体需要更少的交互轮次和工具调用，表现出更高的规划命中率和更早收敛到正确行动路径。研究还发现DLLM策略在工具调用结构上更容易失败，需要更强的工具调用特定训练，以及扩散式跨度损坏需要对齐的注意力掩码以避免虚假上下文-行动信息流。

Conclusion: 扩散大语言模型在智能体多步决策中展现出显著效率优势，能够实现更快的端到端执行和更少的交互需求。然而，实际部署需要考虑工具调用训练和注意力掩码对齐等实际问题。扩散支持的智能体表现出更强的全局规划信号，为智能体架构设计提供了新的方向。

Abstract: Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.

</details>


### [13] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

Relevance: 85.0

TL;DR: SED-SFT提出选择性熵正则化框架，通过自适应鼓励多样性解决SFT中的模式坍塌问题，提升后续RL性能


<details>
  <summary>Details</summary>
Motivation: 传统SFT使用交叉熵损失导致模式坍塌，模型过度集中在特定响应模式，缺乏分布多样性，限制了后续RL的探索效率。现有方法未能充分平衡多样性与准确性。

Method: 提出SED-SFT框架，在优化目标中引入选择性熵正则化项和选择性掩码机制，基于标记探索空间自适应鼓励多样性。

Result: 在8个数学基准测试中，SED-SFT显著提升生成多样性，计算开销可忽略。在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct上，后续RL性能分别平均提升2.06和1.20分。

Conclusion: SED-SFT有效解决SFT模式坍塌问题，平衡多样性与准确性，为后续RL提供更好的探索基础。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [14] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

Relevance: 85.0

TL;DR: LR-DLLM提出了一种长度正则化推理框架，解决扩散大语言模型在变长生成中的固有偏差问题，通过显式长度正则化实现可靠的生成长度确定。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在变长生成中存在固有缺陷，其推理基于固定长度画布并隐含假设已知目标长度。当长度未知时（如实际补全和填充任务），简单比较不同掩码长度的置信度会产生系统性偏差，导致生成不足或冗余延续。

Method: 提出LR-DLLM框架，将生成长度作为显式变量处理，通过显式长度正则化将语义兼容性与长度诱导的不确定性解耦，校正有偏差的置信度估计。该框架无需修改底层DLLM或其训练过程，即可实现生成跨度的动态扩展或收缩。

Result: 在HumanEvalInfilling任务上（完全未知长度）达到51.3% Pass@1（比DreamOn提升13.4%），在四语言McEval上平均达到51.5% Pass@1（比DreamOn提升14.3%）。

Conclusion: LR-DLLM通过长度正则化推理框架有效解决了DLLMs在变长生成中的长度诱导偏差问题，显著提升了在未知长度场景下的生成性能。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [15] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

Relevance: 85.0

TL;DR: LLMs在生成推理路径方面表现强大，但在自我验证方面较弱，存在能力不对称。研究发现，提高生成能力不会自动提升自我验证能力，但学习自我验证可以有效改善生成性能。作者提出了一个多任务强化学习框架，将生成和自我验证作为互补目标进行优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务中生成推理路径方面表现优异，但在验证自身答案方面仍然薄弱，存在生成与自我验证之间的能力不对称。这种不对称限制了LLMs的可靠性和可信度，需要深入理解并解决这一问题。

Method: 首先深入调查了训练过程中生成与自我验证的不对称性演变。然后基于发现构建了一个多任务强化学习框架，将生成和自我验证作为两个独立但互补的目标进行优化，通过强化学习同时提升两个能力。

Result: 实验表明，学习自我验证可以有效改善生成性能，达到与标准生成训练相当的准确性，同时产生更高效有效的推理轨迹。多任务强化学习框架在多个基准测试和模型上都优于仅生成训练，在生成和验证能力上都有提升。

Conclusion: LLMs存在生成与自我验证之间的能力不对称，但学习自我验证可以改善生成性能。通过多任务强化学习框架整合这两个目标，可以同时提升生成和验证能力，为构建更可靠、可信的LLMs提供了新方向。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [16] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出使用激活空间方向控制方法来引导LLM模仿不同人类导师的教学风格，通过修改双向偏好优化学习导师特定的转向向量，实现多样化的教学策略而不依赖显式提示。


<details>
  <summary>Details</summary>
Motivation: 现有LLM教学系统通常学习单一导师策略，无法捕捉真实教学中多样化的教学风格。人类导师会根据学生需求调整脚手架水平、指导性、反馈和情感支持等策略，这些差异会影响对话动态和学生参与度。

Method: 修改双向偏好优化(BiPO)来学习转向向量，这是一种激活空间方向，可以引导模型响应朝向特定导师风格。该方法直接从人类导师-学生对话数据中提取信号，无需显式提示指令。

Result: 转向向量能够捕捉不同对话情境下的导师特定变化，提高与真实导师话语的语义对齐，增加基于偏好的评估，同时基本保持词汇相似性。学习到的方向系数显示出可解释的结构，对应一致的导师行为差异。

Conclusion: 激活转向提供了一种有效且可解释的方法，使用直接从人类对话数据中提取的信号来控制LLM中的导师特定变化，为个性化教学系统开发提供了新途径。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [17] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文分析了LLM作为评估者在摘要任务中的偏见，发现随着被评估摘要与人类摘要相似度降低，LLM越来越偏好其他LLM生成的摘要而非人类摘要，且这种模式普遍存在。


<details>
  <summary>Details</summary>
Motivation: LLM评估者虽然比传统算法指标更能捕捉语义信息、推理能力更强、对改写更鲁棒，但存在长度和顺序偏见，且易受对抗性输入影响。现有研究较少从与明确定义的重叠度指标关系的角度进行细粒度分析。

Method: 在摘要领域，分析LLM评估者偏见与人类撰写回答重叠度的函数关系。测试了9个近期LLM（参数规模10亿到120亿），包括Gemma 3和LLaMA 3变体，使用ROUGE和BLEU度量相似度。

Result: 发现随着被评估摘要与人类摘要相似度降低，LLM评估者越来越偏好其他LLM生成的摘要而非人类摘要，这种模式在所有测试模型中普遍存在（除一个模型外），且不受模型自身位置偏见影响。模型即使在有限重叠的摘要上也难以准确判断。

Conclusion: 在摘要领域使用LLM-as-a-judge应依赖超越简单比较的技术，因为LLM评估者存在系统性偏见，且难以准确评估与人类摘要相似度较低的摘要。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [18] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: SRR-Judge框架为深度搜索智能体提供步骤级推理和搜索动作评估，通过改进的ReAct工作流实现细粒度指导，结合迭代拒绝采样微调显著提升深度搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型推理模型（LRMs）的深度搜索智能体在复杂问答任务中表现出色，但主流方法仅使用结果监督进行训练，忽略了中间思维和动作的质量。需要更可靠的步骤级评估框架来指导搜索集成推理。

Method: 提出SRR-Judge框架，用于可靠评估推理和搜索动作的步骤级质量。集成到改进的ReAct式"评估-优化"工作流中，提供细粒度指导并支持高效后训练标注。使用SRR标注数据，应用迭代拒绝采样微调程序增强基础智能体的深度搜索能力。

Result: SRR-Judge在步骤级评估上比DeepSeek-V3.1等更大模型更可靠，其评分与最终答案正确性有强相关性。将策略与SRR-Judge标注轨迹对齐后，在具有挑战性的深度搜索基准上实现了超过10%的平均绝对pass@1提升。

Conclusion: SRR-Judge框架通过提供可靠的步骤级评估和细粒度指导，显著提升了深度搜索智能体的性能，证明了中间思维质量监督对搜索集成推理的重要性。

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [19] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: Attn-GS：基于注意力引导的上下文压缩框架，利用LLM注意力模式识别个性化重要信号，实现50倍token压缩，性能接近完整上下文。


<details>
  <summary>Details</summary>
Motivation: 个性化LLM需要整合大量用户交互历史和资料，但输入token限制导致高推理延迟和API成本。现有启发式方法（如选择最近交互或使用摘要模型）将上下文视为整体，未考虑LLM内部如何处理和优先处理不同资料组件。

Method: 提出Attn-GS框架：1）通过初步研究发现LLM注意力模式自然揭示重要信号，微调增强区分相关/无关信息能力；2）利用标记模型的注意力反馈标记重要个性化句子；3）指导压缩模型生成任务相关、高质量的压缩用户上下文。

Result: 在不同任务、token限制和设置下显著优于各种基线方法，性能接近使用完整上下文，同时将token使用量减少50倍。

Conclusion: LLM注意力模式能有效识别个性化重要信号，基于注意力引导的上下文压缩是解决LLM个性化中token限制问题的有效方法。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [20] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

Relevance: 85.0

TL;DR: LLMs在上下文概念推理中会动态构建和使用结构化的潜在表征，这些表征在中间到深层形成概念子空间，并在推理中发挥因果作用。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究发现LLMs中存在类似人类的概念表征结构，但尚不清楚这些模型是否在推理过程中功能性地依赖这些表征。本研究旨在探究LLMs在上下文概念推理中的内部处理机制。

Method: 通过因果中介分析(causal mediation analyses)研究LLMs在上下文概念推理中的内部处理，识别概念子空间的出现位置和结构，分析注意力头在不同层级的角色。

Result: 发现概念子空间在中间到深层出现，其表征结构在不同上下文中保持稳定；该子空间对模型预测具有因果作用；早期到中层的注意力头整合上下文线索构建和精炼子空间，后续层利用该子空间生成预测。

Conclusion: LLMs在上下文推理中会动态构建和使用结构化的潜在表征，这为理解LLMs的灵活适应能力提供了计算过程层面的见解。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [21] [Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents](https://arxiv.org/abs/2602.07796)
*Jiatong Li,Changdae Oh,Hyeong Kyu Choi,Jindong Wang,Sharon Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现，在用户参与的LLM智能体场景中，强制要求思考反而会降低性能，因为思考使智能体变得更"内向"，减少了信息披露，削弱了人机信息交换。


<details>
  <summary>Details</summary>
Motivation: 虽然推理能力提升已被证明能提高LLM在复杂任务上的表现，但其在真实用户参与场景中的有效性尚不明确。本研究旨在探究显式思考在用户参与的LLM智能体中的实际效果。

Method: 采用综合实验设计，涵盖7个模型、3个基准测试和2种思考实例化方式。通过定量响应分类分析和定性失败传播案例研究进行评估。

Result: 与预期相反，强制思考在用户参与场景中经常适得其反，导致各种LLM出现异常性能下降。关键发现是思考使智能体更"内向"，缩短响应并减少向用户的信息披露，从而削弱智能体-用户信息交换并导致下游任务失败。

Conclusion: 信息透明度意识是未来真实场景中推理智能体设计的关键但未被充分探索的视角。明确提示信息披露能可靠提升不同模型家族的性能，表明主动透明度是智能体优化的重要杠杆。

Abstract: Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.

</details>


### [22] [Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models](https://arxiv.org/abs/2602.07804)
*Xuan Ding,Pengyu Tong,Ranjie Duan,Yunjian Zhang,Rui Sun,Yao Zhu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了一种基于博弈论的层剪枝框架，将LLM层剪枝建模为合作博弈，使用轻量级代理网络估计层间边际贡献，通过分层蒙特卡洛掩码采样降低计算成本，实现更高效的层剪枝。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务上表现出色，但其实际部署仍受高计算需求限制。现有层剪枝方法依赖静态启发式规则，未能考虑层间相互依赖关系，限制了剪枝效果。需要一种能够动态识别关键层并考虑层间依赖的剪枝方法。

Method: 1) 将层剪枝建模为合作博弈，每层作为玩家，模型性能作为效用；2) 使用轻量级代理网络估计层间边际贡献，预测任意层组合的性能；3) 采用分层蒙特卡洛掩码采样降低Shapley值估计成本；4) 动态识别关键层进行剪枝。

Result: 实验表明该方法在困惑度和零样本准确率方面持续优于现有方法，实现了更高效和有效的LLM层剪枝，显著降低了推理成本。

Conclusion: 提出的博弈论框架能够有效捕捉层间依赖关系，动态识别关键层，相比静态启发式方法实现了更优的层剪枝效果，为大语言模型的高效部署提供了新思路。

Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.

</details>


### [23] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现LLMs在混合表示的数字比较中存在错误，通过探针分析发现LLMs的隐藏状态编码了数字的对数大小信息，但模型在显式回答时准确率较低。通过将探针损失作为辅助目标进行微调，可以提升模型的数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的LLMs能够解决数学问题，但研究发现它们在混合表示的数字比较（如"5.7×10² vs 580"）中会出错。这引发了一个基本问题：LLMs是否真正理解这些数字的大小？研究旨在探究LLMs内部是否编码了数字的大小信息，以及如何利用这些信息提升数值推理能力。

Method: 1. 对多个开源小规模LLMs的隐藏状态进行探针分析；2. 使用线性投影从合适的隐藏层提取数字的对数大小信息；3. 训练线性分类器从隐藏状态预测数字对的排序关系；4. 比较探针预测准确率与模型显式回答准确率；5. 将分类器探针的对数损失作为辅助目标进行模型微调。

Result: 1. 单个隐藏层的线性投影能够编码数字的对数大小，在合成文本上相对误差约2.3%，在科学论文上约19.06%；2. 隐藏状态能够编码数字对的排序关系，线性分类器准确率超过90%；3. 模型显式回答相同数字对的准确率仅为50-70%；4. 探针效果较差的模型显式回答准确率更低；5. 将探针损失作为辅助目标微调，可使显式回答准确率提升3.22%。

Conclusion: LLMs的隐藏状态确实编码了数字的大小信息，但模型在显式推理时未能充分利用这些信息。通过改进模型内部的数值表示（如将探针损失作为辅助目标），可以显著提升LLMs的数值推理能力。这为理解和改进LLMs的数值理解提供了新途径。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [24] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

Relevance: 85.0

TL;DR: TodoEvolve是一个元规划范式，能够自主合成并动态修订任务特定的规划架构，通过PlanFactory统一不同规划范式，并使用IGPO训练Todo-14B模型，在多个智能体基准测试中超越手工设计的规划模块。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统主要依赖固定、手工设计的规划结构，缺乏适应开放式问题结构多样性的灵活性。需要一种能够自主合成和动态调整规划架构的方法来解决这一限制。

Method: 1. 构建PlanFactory模块化设计空间，将不同规划范式标准化到统一代码库中；2. 收集高质量规划轨迹；3. 通过阻抗引导偏好优化(IGPO)训练Todo-14B模型，这是一个多目标强化学习目标，鼓励生成性能优越、稳定且令牌高效的规划系统。

Result: 在五个智能体基准测试中，TodoEvolve始终超越精心设计的规划模块，同时保持经济的API成本和运行时开销。

Conclusion: TodoEvolve提供了一种灵活、自适应的元规划方法，能够为不同任务和智能体骨干自动生成优化的规划架构，解决了现有固定规划结构的局限性。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [25] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究了LLM置信度校准在多答案场景下的失效问题，提出了MACE基准和SCA方法来解决多有效答案导致的置信度低估问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关的置信度校准方法主要针对单答案问答场景研究，但在存在多个有效答案时，这些方法会失效——正确答案之间的分歧会导致系统性的置信度低估。需要系统研究这一现象并开发适用于多答案场景的校准方法。

Method: 1) 引入MACE基准：包含12,000个事实性问题，涵盖6个领域，具有不同数量的正确答案；2) 提出语义置信度聚合(SCA)：通过对多个高概率采样回答的置信度进行聚合来处理多答案场景；3) 在15种代表性校准方法和4个LLM家族(7B-72B)上进行实验验证。

Result: 实验发现：1) 准确率随答案基数增加而提高；2) 估计置信度却持续下降；3) 混合答案数量的问题存在严重校准错误。SCA方法在混合答案设置下实现了最先进的校准性能，同时在单答案问题上保持了强大的校准能力。

Conclusion: 多有效答案场景对LLM置信度校准提出了重要挑战，现有方法在该场景下失效。提出的SCA方法通过语义置信度聚合有效解决了这一问题，为LLM在复杂现实场景中的可靠部署提供了重要工具。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [26] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: SparseEval：一种基于稀疏优化的高效LLM评估方法，通过梯度下降优化锚点权重和迭代精化策略，显著降低评估计算成本


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，在大量基准样本上进行推理评估的计算成本越来越高。现有评估方法效率低下，需要更高效的评估方案来降低计算开销。

Method: 1) 发现模型-项目性能矩阵具有稀疏性，可选择代表性项目作为锚点；2) 将高效评估问题形式化为稀疏优化问题；3) 提出SparseEval方法：首次采用梯度下降优化锚点权重，使用迭代精化策略进行锚点选择；4) 利用MLP的表征能力处理稀疏优化；5) 提出锚点重要性分数和候选重要性分数来评估每个项目的价值。

Result: 在多种基准测试中表现出低估计误差和高Kendall's τ相关性，展示了方法的优越鲁棒性和实际应用价值。

Conclusion: SparseEval通过稀疏优化和锚点选择策略，实现了高效且准确的LLM评估，为大规模模型评估提供了实用的解决方案。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [27] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究指令微调语言模型中指令表示的内部处理机制，发现指令向量(IVs)在模型中相对局部化，同时表现出线性可分性和非线性因果交互，挑战了机制可解释性中的线性表示假设。


<details>
  <summary>Details</summary>
Motivation: 尽管指令微调语言模型取得了成功并被广泛使用，但对其内部如何处理指令的理解很少。本研究旨在从机制角度探究指令特定表示在监督微调(SFT)和直接偏好优化(DPO)等后训练阶段中是如何构建和利用的。

Method: 通过因果中介分析研究指令表示在模型中的定位；提出一种新方法来定位语言模型中的信息处理，该方法不受基于补丁技术的隐式线性假设限制；分析指令向量作为电路选择器的功能。

Result: 发现指令表示在模型中相对局部化，这些指令向量(IVs)表现出线性可分性和非线性因果交互的奇特组合；发现基于早期层形成的任务表示，后期层会选择不同的信息通路来解决任务，即IVs充当电路选择器。

Conclusion: 指令向量在语言模型中作为电路选择器发挥作用，挑战了机制可解释性中常见的线性表示假设，为理解指令微调模型的内部处理机制提供了新视角。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [28] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出CompositeHarm基准测试，通过翻译方法评估LLM在多语言环境下的安全性，发现攻击成功率在印度语言中显著上升，特别是对抗性语法攻击下，而上下文危害转移相对温和。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要集中于英语，翻译作为多语言行为探测的捷径，但无法全面捕捉跨语言时有害意图或结构的变化。需要研究安全对齐在语法和语义变化时的保持情况。

Method: 结合AttaQ（结构化对抗攻击）和MMSafetyBench（上下文真实世界危害）两个英语数据集，扩展到六种语言（英语、印地语、阿萨姆语、马拉地语、卡纳达语、古吉拉特语）。采用轻量级推理策略，减少冗余评估，保持跨语言保真度。

Result: 攻击成功率在印度语言中急剧上升，特别是在对抗性语法下；上下文危害转移相对温和。翻译基准测试是必要但不充分的步骤，需要构建基于资源意识、语言自适应的安全系统。

Conclusion: 翻译基准测试是构建多语言安全系统的必要第一步，但不足够。需要更全面的语言自适应安全评估方法，结合轻量级推理策略实现可扩展且环保的多语言安全测试。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [29] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现LLM作为自动评估器时，其判断会受到无关上下文线索（如来源、时间、人口统计信息）的显著影响，但这些影响很少在评估理由中被明确承认，揭示了LLM评估流程中存在解释差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作自动评估器来评估系统输出，一个忠实的评估器应该仅基于内容质量做出判断，对无关上下文保持不变，并透明反映决策驱动因素。本研究旨在测试LLM评估器是否达到这一理想标准。

Method: 通过受控线索扰动方法，在评估提示中注入合成元数据标签（来源、时间、年龄、性别、种族、教育状况等6类线索），测试6个LLM评估器（GPT-4o、Gemini-2.0-Flash等）。实验涵盖两个互补数据集：ELI5（事实性问答）和LitBench（开放式创意写作）。除了测量判断转移率（VSR），还引入线索承认率（CAR）来量化评估器是否在自然语言理由中明确引用注入的线索。

Result: 研究发现LLM评估器对无关线索表现出显著的行为效应（如来源层级偏好、时效性偏好、教育状况偏好等），但线索承认率通常接近零，表明即使捷径依赖驱动了决策，也很少被报告。CAR具有数据集依赖性：在事实性ELI5设置中，某些模型和线索更可能明确承认线索，但在开放式LitBench环境中，即使存在大的判断转移，承认率也常为零。

Conclusion: LLM评估器存在显著的判断敏感性和有限的线索承认，揭示了LLM-as-judge流程中的解释差距，对研究和部署中基于模型的评估可靠性提出了担忧。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [30] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

Relevance: 85.0

TL;DR: DeltaKV提出了一种基于残差的KV缓存压缩框架，通过编码语义残差而非丢弃token来减少内存占用，结合Sparse-vLLM推理引擎实现2倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM部署面临KV缓存内存线性增长的瓶颈，现有压缩和淘汰方法难以平衡准确性、压缩比和硬件效率。

Method: 基于两个经验发现：长距离token间相似性和KV表示中高度共享的潜在组件，DeltaKV编码相对于历史参考的语义残差；Sparse-vLLM是解耦内存管理和优化稀疏不规则KV布局的高性能推理引擎。

Result: DeltaKV将KV缓存内存减少到原始的29%，在LongBench、SCBench和AIME上保持接近无损的准确性；结合Sparse-vLLM在长上下文场景中相比vLLM实现最高2倍吞吐量提升。

Conclusion: DeltaKV和Sparse-vLLM为可扩展的长上下文LLM部署提供了实用路径，在保持准确性的同时显著提升内存效率和推理速度。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [31] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: DIP（Diverge-to-Induce Prompting）是一种新的提示框架，通过生成多个不同的高层推理策略，然后将这些策略细化成详细计划，最后整合成最终计划，以提升零样本推理准确性。


<details>
  <summary>Details</summary>
Motivation: 标准思维链提示中的非引导推理路径存在不稳定性问题，而现有方法通常只针对每个问题生成单一推理策略，这在多样化任务中限制了性能表现。需要一种能够利用多种推理策略的方法来提升推理的鲁棒性和准确性。

Method: DIP框架包含三个步骤：1）首先提示LLM为每个问题生成多个不同的高层推理策略；2）将每个策略细化成详细的、逐步的草稿计划；3）将这些草稿计划整合诱导成最终计划。该方法不需要依赖资源密集的采样过程。

Result: 实验表明，DIP在零样本推理准确性上优于单一策略提示方法，证明了多计划诱导在基于提示的推理中的有效性。

Conclusion: 通过生成和整合多个不同的推理策略，DIP能够提升LLM的推理性能，特别是在零样本设置下，为提示工程提供了新的有效方法。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [32] [Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection](https://arxiv.org/abs/2602.08031)
*Chenwang Wu,Yiu-ming Cheung,Shuhai Zhang,Bo Han,Defu Lian*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种基于马尔可夫随机场的分数校准策略，用于改进机器生成文本的检测方法，通过建模相邻相似性和初始不稳定性关系来校准检测分数，显著提升现有检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 机器生成文本虽然提供便利，但也带来虚假信息和网络钓鱼等风险，需要可靠的检测方法。基于度量的方法比复杂的基于模型的方法更实用，但面临检测分数容易受到生成过程随机性偏差影响的核心挑战。

Method: 首先将代表性的基于度量的方法置于统一框架中进行分析，识别出核心问题。然后从理论和实证角度揭示上下文检测分数的两种关系：相邻相似性和初始不稳定性。提出基于马尔可夫随机场的分数校准策略，通过平均场近似实现为轻量级组件，可无缝集成到现有检测器中。

Result: 在各种真实场景（如跨LLM和改写攻击）的广泛实验中，该方法相比基线方法取得了显著提升，且计算开销可忽略不计。

Conclusion: 提出的马尔可夫随机场校准策略有效解决了机器生成文本检测中分数偏差问题，通过建模上下文关系显著提升了检测性能，为实际应用提供了实用解决方案。

Abstract: While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at https://github.com/tmlr-group/MRF_Calibration.

</details>


### [33] [TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs](https://arxiv.org/abs/2602.08048)
*Arshia Hemmat,Philip Torr,Yongqiang Chen,Junchi Yu*

Main category: cs.CL

Relevance: 85.0

TL;DR: TDGNet：基于时序动态图的扩散语言模型幻觉检测框架，通过分析去噪过程中的注意力图演化来检测幻觉，相比单次推理方法在多个基准上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（D-LLMs）具有并行去噪和双向上下文优势，但其幻觉检测研究不足。现有的自回归LLM幻觉检测器通常依赖单次推理线索，无法直接迁移到扩散生成场景，因为在扩散生成中事实证据分布在去噪轨迹上，可能随时间出现、漂移或自我修正。

Method: 提出TDGNet时序动态图框架，将幻觉检测建模为在演化令牌级注意力图上的学习问题。在每个去噪步骤中：1）稀疏化注意力图；2）通过消息传递更新每个令牌的记忆；3）使用时序注意力聚合轨迹范围内的证据进行最终预测。

Result: 在LLaDA-8B和Dream-7B模型上的QA基准测试显示，相比基于输出、潜在表示和静态图的基线方法，TDGNet在AUROC指标上取得一致改进，且仅需单次推理和适度的计算开销。

Conclusion: 研究结果表明，在扩散语言模型的鲁棒幻觉检测中，对注意力图进行时序推理至关重要。TDGNet框架为D-LLMs的幻觉检测提供了有效的解决方案。

Abstract: Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.

</details>


### [34] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究了潜在推理变换器(LRTs)，这是一种在连续隐藏空间中进行推理的语言模型，而不是像思维链那样用语言表达中间步骤。研究发现LRTs在潜在空间中自发学习结构化的搜索过程，包括探索、暂定承诺、收敛或回溯等阶段。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型在无语言表达的情况下如何进行推理。传统的思维链方法通过语言表达中间推理步骤，而潜在推理变换器在连续隐藏空间中进行审议，研究人员希望了解这种潜在推理的工作机制和特点。

Method: 方法包括：1) 使用潜在推理变换器(LRTs)在多项选择QA基准上进行测试；2) 在每一步解码模型不断演变的信念；3) 分析模型在潜在空间中的搜索过程；4) 通过替换干扰项来测试搜索的自适应性。

Result: 研究发现：1) 模型在潜在空间中自发学习结构化的搜索过程，包括探索阶段、暂定承诺、收敛或回溯；2) 回溯现象普遍存在(32%的实例)且有益(比非回溯实例准确率提高34%)；3) 回溯主要从语义上最接近的干扰项转向正确答案；4) 搜索具有自适应性，替换干扰项可缩短54%的探索时间。

Conclusion: 结论是潜在推理模型在激活空间中实现了思维链通过语言实现的功能：能够犯错、注意到错误并进行恢复。这表明无语言表达的潜在推理具有与显式语言推理相似的能力和优势。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [35] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文首次系统研究LLM在商品推荐中的性别和种族偏见，通过提示工程获取对不同人口群体的推荐，使用三种分析方法量化偏见，发现显著差异。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多用于商品推荐，但其可能嵌入和放大性别与种族偏见的风险尚未充分探索。本文旨在填补这一空白，研究LLM推荐系统中的偏见问题。

Method: 1) 使用提示工程让LLM为不同种族和性别群体生成产品推荐；2) 采用三种分析方法：标记词分析、支持向量机、Jensen-Shannon散度来识别和量化偏见。

Result: 研究发现LLM对不同人口群体的推荐存在显著差异，表明推荐系统中存在系统性偏见，需要更公平的LLM推荐系统。

Conclusion: LLM推荐系统存在性别和种族偏见问题，需要开发更公平的推荐方法，确保AI系统的公平性和可信度。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [36] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究探索LLMs在多模态交流游戏中是否能像人类一样形成对话惯例，发现同类型配对（人类-人类、AI-AI）能形成惯例，但人类-AI混合配对失败，表明存在根本性的交流倾向差异。


<details>
  <summary>Details</summary>
Motivation: 人类在对话中会相互对齐，形成共享惯例以简化交流。本研究旨在测试LLMs在多模态交流游戏中是否也能形成类似的对话惯例，以及人类与AI之间的交流是否存在根本性差异。

Method: 采用多模态交流游戏实验设计：1）同类型配对（人类-人类、AI-AI）和混合配对（人类-AI）的对比实验；2）通过提示让LLMs模仿人类表面行为，测试是否能改善对齐效果。

Result: 同类型配对（人类-人类、AI-AI）都显示出惯例形成迹象：准确性、一致性提高，消息长度减少。但人类-AI混合配对失败，即使LLMs被提示模仿人类行为长度，准确性和词汇重叠仍落后于同类型配对。

Conclusion: 对话对齐不仅需要模仿先前互动的能力，还需要共享对所传达意义的解释性偏见。人类和LLMs在交流倾向上的根本差异阻碍了有效的惯例形成。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [37] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种新的预训练方法——Token-Level Adaptive Latent CoT，通过为每个token生成可变长度的潜在思维链轨迹来增加每个token的计算量，而不增加模型参数，从而提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统通过增加参数和训练数据来扩展大语言模型的方法受到高质量语料库有限和通信成本上升的限制。本文探索了另一种扩展轴：在不增加参数的情况下，通过内部化潜在思维链来增加每个token的计算量。

Method: 提出了Pretraining with Token-Level Adaptive Latent CoT方法，在预训练阶段让模型为每个token生成可变长度的潜在CoT轨迹——对困难token分配更长轨迹，对简单token分配较短（甚至为零）轨迹。这种方法通过单阶段通用文本预训练自然涌现，并通过token级自适应停止减少训练和推理计算量。

Result: 在Llama架构上的实验表明，adaptive latent CoT能够持续改善语言建模困惑度和广泛的downstream任务准确率，即使比先前的循环基线使用更少的训练FLOPs。

Conclusion: 通过内部化潜在思维链进行预训练是一种有效的模型扩展方法，能够在保持参数不变的情况下提升模型性能，同时通过自适应机制优化计算效率。

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [38] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

Relevance: 85.0

TL;DR: CoRect通过对比上下文化和非上下文化前向传播的logits，识别具有高参数偏置的层，然后修正隐藏状态以保留基于证据的信息，解决RAG中的知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)在遇到知识冲突时，模型内部的参数化知识会覆盖检索到的证据，导致输出不忠实。现有方法要么依赖浅层的解码调整，要么需要真实标签进行权重编辑，存在局限性。

Method: 通过层间分析发现参数抑制现象：在深层中，某些FFN层会用记忆的先验覆盖上下文敏感表示。提出CoRect方法，通过对比上下文化和非上下文化前向传播的logits，识别具有高参数偏置的层，然后修正隐藏状态以保留证据信息。

Result: 在问答和摘要基准测试中，CoRect相比强基线方法持续提高了忠实度并减少了幻觉。

Conclusion: CoRect通过识别和修正参数抑制层，有效解决了RAG中的知识冲突问题，提高了生成内容的忠实度，且不需要真实标签。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [39] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了一种无监督强化学习方法RLVR，通过段落重构任务增强LLMs的长上下文能力，无需人工标注或教师模型监督。


<details>
  <summary>Details</summary>
Motivation: 传统的RLVR方法依赖昂贵的黄金标准答案或教师模型评估准则，成本高且耗时。本文旨在探索无监督方法，消除对人工标注和教师模型监督的需求，以更高效地提升LLMs的长上下文处理能力。

Method: 方法核心是段落重构任务：在长文档中替换部分段落为占位符，通过强化学习训练LLM从候选段落集中正确识别和排序缺失段落以重构原文。这种方法让模型学习全局叙事连贯性。

Result: 在RULER和LongBench v2基准测试上验证了方法的有效性：在RULER上获得显著提升，在LongBench v2上也有合理改进，且无需人工标注的长上下文QA数据。进行了广泛的消融研究分析奖励设计、数据策略、训练方案和数据规模的影响。

Conclusion: 提出了一种有效的无监督RLVR方法，通过段落重构任务显著提升LLMs的长上下文能力，避免了传统方法的昂贵标注成本。方法具有实际应用价值，并公开了代码、数据和模型。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [40] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出将释义分解为构成性语言方面（释义类型），为语义等价提供更细粒度和认知基础的观点，并证明基于释义类型训练的模型在相关任务和下游应用中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多将释义简化为两个文本之间的二元决策或生成单一改写，掩盖了哪些语言因素负责意义保留。需要更细粒度地理解语义等价，这对计算语言模型的意义表示至关重要。

Method: 提出将释义分解为构成性语言方面（释义类型）的方法，并训练模型识别这些类型。通过显式训练模型理解不同释义类型，提升语义理解能力。

Result: 在剽窃检测任务中，基于释义类型训练的模型超越人类基线：维基百科剽窃案例准确率89.6% vs 78.4%，arXiv科学论文剽窃准确率66.5% vs 55.7%。在Quora重复问题识别中，基于释义类型的模型优于基于二元对的模型。

Conclusion: 分解释义为构成性语言方面提供了更细粒度和认知基础的语义等价视图，显式训练模型理解释义类型能显著提升相关任务性能，增强语言模型的语义理解能力。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [41] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: RLVR通过概率框架解释LLM能力涌现：将复杂推理分解为原子步骤，通过强化学习优化单步概率，使模型能够解决之前无法处理的多步任务。


<details>
  <summary>Details</summary>
Motivation: 解决关于RLVR的核心争议：RLVR是赋予LLM新能力还是仅仅激发潜在能力。作者支持前者，提出概率框架来定义能力涌现。

Method: 提出基于实例级可解性的概率框架，假设复杂推理能力源于原子步骤概率的锐化。使用Algebrarium框架，在单步操作上训练模型，评估其在未见多步任务上的表现。

Result: 实证结果证实：(1) RLVR通过放大现有技能激励探索新解路径；(2) 复合性能严格受原子步骤联合概率控制（ρ∈[0.69, 0.96]）；(3) RLVR作为全局优化器可能导致特定技能被牺牲以最大化总体奖励。

Conclusion: RLVR中的涌现能力可以通过迭代优化可解问题来解释，使模型发展出解决之前不可解场景的能力。这为RLVR如何赋予LLM新能力提供了新解释。

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [42] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出统一理论框架分析Transformer LLMs中任意上下文信息的影响，通过误差动力学刻画上下文作用，证明上下文条件误差可分解为基线误差与上下文修正的加和，并推导出误差减少的几何条件。


<details>
  <summary>Details</summary>
Motivation: 推理时的上下文信息（如演示、检索知识、交互历史）能显著提升LLMs性能而无需参数更新，但其理论作用在ICL等特定设置之外仍不清楚。需要统一理论框架来分析任意上下文信息在Transformer LLMs中的作用机制。

Method: 提出理论框架分析上下文信息对Transformer LLMs的影响，通过输出误差动力学刻画上下文作用。在单层Transformer中证明上下文条件误差可分解为基线误差向量与上下文修正向量的加和，推导误差减少的几何条件（上下文修正需与负基线误差对齐并满足范数约束）。进一步证明上下文修正范数存在由上下文-查询相关性和互补性决定的上界，并将结果扩展到多上下文和多层Transformer。

Result: 在ICL、检索增强生成和记忆演化等任务上的实验验证了理论，并基于理论提出原则性上下文选择策略，性能提升0.6%。

Conclusion: 建立了统一理论框架分析Transformer LLMs中上下文信息的作用机制，揭示了误差减少的几何条件，为理解上下文增强LLMs提供了理论基础，并启发了改进的上下文选择策略。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [43] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: Dr. SCI是一个针对科学推理的大规模数据集和训练流程，通过重新设计SFT->RL流程，显著提升LLM在开放科学问题上的表现


<details>
  <summary>Details</summary>
Motivation: 解决LLM在开放科学问题上的挑战，特别是由于监督不可靠和评估困难导致的瓶颈。需要系统化的数据处理和奖励设计来改进科学后训练。

Method: 1) 构建Dr. SCI数据集：100万STEM问题，含明确可验证/开放分割、难度标注和细粒度评分标准；2) 重新设计SFT->RL流程：包括探索扩展SFT、动态难度课程和科学评分标准引导的RL

Result: Qwen3-4B-Base使用Dr. SCI流程在GPQA-diamond上达到63.2分，GPQA-general上32.4分，超过o1-mini和GPT-4o等强基线，在开放科学推理上取得显著提升

Conclusion: Dr. SCI通过系统化的数据处理和重新设计的训练流程，有效解决了LLM在开放科学问题上的挑战，显著提升了科学推理能力

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [44] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Thinking States方法，在输入处理过程中生成思考状态，将思考表示为token嵌入到后续输入中，实现并行化推理，降低推理成本


<details>
  <summary>Details</summary>
Motivation: 链式思维推理虽然能提升大语言模型解决复杂任务的能力，但生成长推理过程会显著增加推理成本。需要一种更高效的推理方法，能够在输入处理过程中进行推理，而不是在输入完全处理后才开始生成推理过程。

Method: Thinking States方法：1）在输入处理过程中，每隔几个输入token生成一系列思考token；2）将这些思考token转换回嵌入空间；3）添加到后续输入token中。这种方法能够捕捉CoT的循环特性，同时支持通过自然语言监督学习和教师强制进行并行化训练。

Result: 在多个推理任务上优于其他潜在推理方法：1）在数学问题上缩小了与CoT的差距；2）在2-Hop QA任务上达到与CoT相当的性能，同时延迟更低；3）在状态跟踪任务上展现出比CoT更强的推理能力，能够成功泛化到比训练时更长的序列。

Conclusion: Thinking States是一种有效的推理方法，能够在输入处理过程中进行并行化推理，既保持了CoT的推理能力，又显著降低了推理延迟，特别适合需要处理长序列的复杂推理任务。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [45] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出基于认知启发的长上下文推理框架，通过分块压缩和选择性记忆召回解决LLM长上下文处理中的计算成本、信息遗忘和RAG上下文碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM长上下文处理面临的三大挑战：二次计算成本、信息遗忘、以及检索增强生成中的上下文碎片化问题。传统方法处理所有原始token效率低下，需要更高效的认知启发方法。

Method: 1) 将长输入分段为chunk，用学习到的压缩器编码为压缩记忆表示；2) 门控模块动态选择相关记忆块；3) 推理模块通过演化的working memory迭代处理记忆块解决下游任务；4) 压缩器和推理器通过端到端强化学习联合优化，门控模块作为分类器单独训练。

Result: 在RULER-HQA等多跳推理基准上达到竞争性准确率，上下文长度从7K扩展到1.75M tokens，相比强基线在准确率-效率权衡上表现优越。相比MemAgent，GPU峰值内存使用减少2倍，推理速度提升6倍。

Conclusion: 提出的认知启发框架通过分块压缩和选择性记忆召回，有效解决了LLM长上下文处理的效率和准确性问题，为长上下文推理提供了新的高效解决方案。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [46] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: TEAM是一个加速MoE扩散大语言模型的框架，通过利用专家路由的时空一致性，减少激活专家数量同时增加接受token数量，实现2.2倍加速


<details>
  <summary>Details</summary>
Motivation: MoE扩散LLMs在推理时存在效率问题：每个去噪步骤激活大量专家，但只有少量token被最终接受，导致推理开销大，限制了在延迟敏感应用中的部署

Method: 提出TEAM框架，利用专家路由在去噪层级间的时间一致性和token位置间的空间一致性，采用三种互补的专家激活和解码策略：保守选择已解码和掩码token的必要专家，同时进行跨多个候选的激进推测探索

Result: 实验结果显示TEAM相比原始MoE dLLM实现高达2.2倍的加速，性能下降可忽略不计

Conclusion: TEAM是一个即插即用框架，能有效加速MoE扩散大语言模型，解决推理效率瓶颈，促进这类模型在实际应用中的部署

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [47] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

Relevance: 85.0

TL;DR: Prism提出了一种基于频谱感知的训练无关方法，通过高频和低频分支分解块选择，恢复位置信息，实现高效的长上下文LLM预填充加速。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力方法在识别相关块时存在瓶颈，通常使用粗粒度注意力作为块重要性估计的代理，但需要昂贵的token级搜索或评分，导致显著的选择开销。

Method: Prism通过分析均值池化与RoPE的相互作用，发现均值池化作为低通滤波器会衰减高频位置信息。提出频谱感知方法，将块选择分解为高频和低频分支，应用基于能量的温度校准直接从池化表示中恢复衰减的位置信号。

Result: Prism在保持与完整注意力相同准确性的同时，实现了高达5.1倍的加速。

Conclusion: Prism通过解决均值池化与RoPE相互作用导致的位置信息衰减问题，实现了高效的块稀疏注意力，为长上下文LLM预填充提供了有效的加速方案。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [48] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文通过实验测试LLMs学习可能和不可能语言的能力，验证乔姆斯基对ChatGPT的批判，发现GPT-2在不可能语言上表现不佳，而LSTM表现符合乔姆斯基论点，提出从理性主义转向功能主义和经验主义的研究范式转变。


<details>
  <summary>Details</summary>
Motivation: 回应乔姆斯基在《ChatGPT的虚假承诺》中对LLMs的批判，即LLMs只是模式预测器而非像人类那样通过内在因果和自我纠正结构习得语言，无法区分不可能语言。旨在通过实验验证这一批判，并探索LLMs研究的新理论范式。

Method: 1) 构建一组语法上不可能的语言（通过对英语进行特定变换，如整句反转、基于词数奇偶性添加否定）；2) 在GPT-2小模型和LSTM模型上进行两轮对照实验；3) 使用Welch's t-test进行统计分析，比较模型在可能和不可能语言上的学习表现。

Result: GPT-2小模型在所有不可能语言上的学习表现均显著低于在可能语言上的表现（p<.001），支持乔姆斯基的批判。而LSTM模型的表现与乔姆斯基论点一致，突显了Transformer架构演变的不可替代作用。

Conclusion: 基于理论和实证发现，提出在乔姆斯基理论框架内对LLMs的新视角，以及从乔姆斯基的"理性主义-浪漫主义"范式向功能主义和经验主义的LLMs研究范式转变。

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [49] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出ME²原则从宏观和微观层面定义推理质量，基于有向无环图(DAG)构建推理评估方法，开发Thinking Reward Model(TRM)用于大规模推理质量评估，实验证明思考奖励可作为有效优化信号。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)依赖具有复杂内部结构的推理轨迹，但现有工作缺乏对三个基本问题的统一答案：1)什么定义了高质量推理，2)如何可靠评估长且隐含结构的推理轨迹，3)如何使用此类评估信号进行推理优化。

Method: 1)引入ME²原则从宏观(效率)和微观(有效性)层面表征推理质量；2)将推理轨迹建模为有向无环图(DAG)，开发基于DAG的成对评估方法；3)构建TRM-Preference数据集，训练Thinking Reward Model(TRM)进行大规模推理质量评估。

Result: 实验表明思考奖励可作为有效优化信号：测试时选择更好推理可获得更好结果(最高19.3%提升)，RL训练期间思考奖励可增强推理和性能(最高3.9%提升)，在多样化任务中均有效。

Conclusion: 论文提供了统一的推理质量评估框架，通过ME²原则、DAG建模和TRM模型解决了推理质量定义、评估和优化的核心问题，为大型推理模型的改进提供了系统方法。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [50] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

Relevance: 85.0

TL;DR: GISA是一个用于评估通用信息搜索助手的新基准，包含373个人工构建的真实信息搜索查询，支持四种结构化答案格式，并提供完整的人类搜索轨迹作为过程级监督的黄金标准。


<details>
  <summary>Details</summary>
Motivation: 现有评估搜索代理的基准存在三个主要问题：1）查询从答案反向构建，导致任务不自然且与现实需求脱节；2）要么专注于定位特定信息，要么专注于多源信息聚合，缺乏统一；3）依赖静态答案集，容易受到数据污染影响。

Method: GISA基准包含373个人工构建的真实信息搜索查询，具有四种结构化答案格式（项目、集合、列表、表格），支持确定性评估。它整合了深度推理和广泛信息聚合任务，包含定期更新的实时子集以抵抗记忆，并提供每个查询的完整人类搜索轨迹。

Result: 实验显示，即使在主流LLM和商业搜索产品中，最佳模型也只能达到19.30%的精确匹配分数。在需要复杂规划和全面信息收集的任务上，性能显著下降，表明现有模型仍有很大改进空间。

Conclusion: GISA基准填补了现有搜索代理评估的空白，提供了更真实、全面的评估框架。实验结果揭示了当前LLM在信息搜索任务上的局限性，特别是在复杂规划和信息聚合方面，为未来研究指明了方向。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [51] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文通过激活修补等技术，揭示了LLM处理表格的内部机制，将表格理解分解为语义绑定、坐标定位和信息提取三阶段，发现模型通过计数分隔符来定位单元格，并可通过向量算术精确控制模型关注点。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM越来越多地用于表格相关任务，但模型如何处理线性化的二维结构化表格的内部机制仍然不透明。研究者希望理解LLM在表格理解任务中的内部工作机制。

Method: 使用激活修补（activation patching）和互补的可解释性技术，将表格理解机制分解为原子任务（单元格定位），并分析模型的三阶段处理流程。

Result: 发现模型通过计数离散分隔符的序数机制来解析坐标定位单元格；列索引编码在线性子空间中，可通过向量算术精确控制模型关注点；模型通过复用原子定位任务中识别的相同注意力头来泛化到多单元格定位任务。

Conclusion: 该研究为Transformer架构中的表格理解提供了全面的解释框架，揭示了模型处理结构化数据的内部机制，对LLM的可解释性和分析有重要意义。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [52] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文针对低资源语言对（英语-马拉雅拉姆语）的机器翻译质量评估，提出了首个包含人工标注直接评估分数和翻译质量评注的片段级数据集，并开发了ALOPE-RL强化学习框架，通过结合错误感知奖励，使小型LLM在有限数据和计算预算下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量评估方法主要依赖标量质量分数，缺乏对翻译错误的明确解释，且在低资源语言（如马拉雅拉姆语）上由于标注数据有限，现有方法性能不可靠。需要开发既能评估质量又能解释错误的解决方案。

Method: 1) 创建首个英语-马拉雅拉姆语片段级QE数据集，包含直接评估分数和翻译质量评注；2) 提出ALOPE-RL强化学习框架，基于策略奖励训练高效适配器；3) 结合错误感知奖励，使LLM能超越数值分数进行质量推理；4) 使用LoRA和4-bit量化在小型LLM上微调。

Result: ALOPE-RL在英语-马拉雅拉姆语QE任务上达到SOTA性能，使用紧凑LLM（≤4B参数）超越了更大的LLM基线和领先的编码器基QE模型，证明了在有限数据和计算预算下通过错误感知策略学习可获得强大性能。

Conclusion: 错误感知的策略学习能在有限数据和计算资源下实现高质量机器翻译评估，为低资源语言QE提供了有效解决方案。数据集、代码和模型的发布将支持未来研究。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [53] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究探索了演绎、归纳和溯因三种基本推理范式如何影响LLM的推理行为，通过符号任务数据集训练LLM，并在现实自然语言任务中验证其泛化能力，获得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推理改进研究众多，但基本推理范式（演绎、归纳、溯因）如何影响泛化能力尚未得到系统探索。研究者希望了解这些核心范式的相互作用如何影响LLM的推理行为。

Method: 1. 收集新的符号任务推理轨迹数据集，每个任务针对三种基本推理范式之一，以抽象化具体世界知识
2. 研究将这些推理技能注入LLM的有效方法，包括简单微调、增加模型深度、将密集模型转换为专家混合模型等方法
3. 在完全用自然语言表述且包含真实世界知识的现实跨领域任务上进行全面评估

Result: 该方法在现实任务上表现出强大的泛化能力，性能提升显著（最高达14.60分）。研究揭示了基本推理范式训练对LLM推理行为的积极影响。

Conclusion: 通过系统探索基本推理范式（演绎、归纳、溯因）对LLM推理的影响，研究者开发了有效的方法来提升LLM的推理泛化能力，为LLM推理改进提供了新的视角和方法。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [54] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

Relevance: 85.0

TL;DR: LLMs can design their own evaluation rubrics (GER-Eval)，在语义一致性和评分可靠性方面表现良好，但在事实性和知识密集型任务中可靠性下降。闭源模型比开源模型在一致性和跨模型泛化方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 人类设计的评估标准往往是静态的，且与LLM内部的语言质量表示方式不一致。研究者希望探索LLM是否能够设计和应用自己的评估标准，以更好地理解LLM的评估能力。

Method: 提出GER-Eval框架，让LLM生成自己的评估标准并应用这些标准进行评估。研究评估LLM定义标准的语义一致性、评分可靠性以及与人类标准的对齐程度。

Result: LLM能够可靠地生成可解释且任务感知的评估维度，并在模型内部一致地应用这些标准。但在事实性和知识密集型任务中，评分可靠性会下降。闭源模型（如GPT-4o）比开源模型（如Llama）在一致性和跨模型泛化方面表现更好。

Conclusion: 评估是LLM的一种学习到的语言能力，在模型内部一致但在不同模型间存在碎片化。需要开发新方法来联合建模人类和LLM的评估语言，以提高可靠性和可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [55] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

Relevance: 85.0

TL;DR: PERSPECTRA是一个评估大语言模型多元主义能力的基准，结合了Kialo辩论图的结构清晰性和Reddit讨论的语言多样性，包含3,810个扩展论点，涵盖100个争议话题的762个正反立场。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究缺乏对多元主义（容纳不同观点而不将其简化为单一视角的能力）的系统评估，而多元主义对于开发能真实反映人类异质性的语言模型至关重要。现有辩论数据源要么需要昂贵的人工验证，要么缺乏结构清晰性。

Method: 通过受控的检索和扩展流程，整合Kialo辩论图的结构清晰性和Reddit讨论的语言多样性，构建包含3,810个扩展论点的数据集，涵盖100个争议话题的762个正反立场，每个观点扩展为多个自然变体。

Result: 实验表明，当前最先进的开源和专有LLM在多元主义理解方面存在系统性失败，包括高估观点数量、错误分类让步结构等，突显了多元主义感知理解和推理的难度。

Conclusion: PERSPECTRA通过结合多样性和结构，建立了第一个可扩展、可配置的基准，用于评估模型如何表示、区分和推理多个视角，填补了LLM多元主义评估的空白。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [56] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

Relevance: 85.0

TL;DR: AFlow框架通过建模多轮对话轨迹中的连续情感流，为情感支持对话提供细粒度监督，提升策略一致性和共情响应质量


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话方法依赖稀疏的结果级信号，对中间策略决策的监督有限，难以处理复杂的多轮支持场景

Method: 提出AFlow框架：1) 建模多轮轨迹的连续情感流；2) 估计搜索轨迹的中间效用；3) 学习偏好一致的策略转换；4) 引入子路径级流平衡目标传播偏好信号

Result: 在多样化情感场景中显著优于现有基线，使用紧凑开源骨干网络在主要ESC指标上超越GPT-4o和Claude-3.5等专有大模型

Conclusion: 通过细粒度情感流监督能有效提升LLMs在情感支持对话中的策略一致性和响应质量，为复杂多轮支持提供新方法

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [57] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: WildReward：从真实用户交互中直接训练奖励模型，无需人工标注偏好对，性能媲美甚至超越传统奖励模型


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖大规模人工标注的偏好对，成本高且难以扩展。随着LLMs的广泛部署，真实世界用户交互成为丰富的隐式奖励信号来源，能否直接从这些交互中开发奖励模型？

Method: 采用WildChat作为交互数据源，提出从用户反馈中提取可靠人类反馈的流程，通过序数回归直接在用户反馈上训练WildReward，无需偏好对，获得186k高质量训练实例

Result: WildReward在性能上达到或超越传统奖励模型，具有更好的校准性和跨样本一致性。用户多样性直接提升模型性能，用户越多奖励模型越强。应用于在线DPO训练在各种任务上取得显著改进

Conclusion: 直接从真实世界用户交互中训练奖励模型是可行的，WildReward展示了这种方法的有效性，为奖励模型训练提供了更高效、可扩展的替代方案

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [58] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了ANIRA框架，通过算法和合成语言任务评估token级自适应计算，发现计算分配能与任务复杂度对齐，但无法泛化到未见输入大小，且早期决策依赖静态结构线索。


<details>
  <summary>Details</summary>
Motivation: 现有token级自适应计算研究主要在自然语言基准上评估，token级难度不可观测且与架构因素混杂，无法确定计算分配是否真正与底层复杂度对齐。

Method: 提出复杂度控制评估范式，使用参数化难度的算法和合成语言任务；设计ANIRA统一循环Transformer框架，支持每token可变深度计算，隔离计算分配决策与其他模型因素。

Result: 计算分配能与任务复杂度对齐（无需显式难度监督），但无法算法泛化（无法外推到未见输入大小）；早期计算决策依赖静态结构线索，在线停止更接近算法执行状态。

Conclusion: token级自适应计算能与复杂度对齐，但对齐不保证算法泛化；决策时机影响计算分配质量，为未来自适应计算研究提供新评估框架和见解。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [59] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出一个两阶段框架，结合少样本LLM命名实体识别和基于代理的地理编码模块，用于从人道主义文档中提取位置信息，旨在解决现有地理信息提取系统中的地理偏见问题。


<details>
  <summary>Details</summary>
Motivation: 人道主义危机需要及时准确的地理信息来指导应急响应，但现有的自动位置提取系统往往复制现有的地理和社会经济偏见，导致危机受影响地区的可见性不均。本文旨在研究LLM是否能解决人道主义文档中位置信息提取的地理差异问题。

Method: 提出一个两阶段框架：1) 少样本LLM命名实体识别，用于从文本中提取位置实体；2) 基于代理的地理编码模块，利用上下文信息解析模糊的地名。使用扩展的HumSet数据集进行基准测试，对比最先进的预训练和基于规则的系统。

Result: LLM方法显著提高了从人道主义文本中提取地理位置的精度和公平性，特别是在代表性不足的地区。在准确性和公平性指标上都优于现有方法。

Conclusion: 通过将LLM推理能力与负责任、包容性AI原则相结合，这项工作有助于建立更公平的人道主义响应地理空间数据系统，推进"不让任何地方掉队"的危机分析目标。

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [60] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现，在长上下文场景下，即使LLMs具备更强的推理能力，也无法自动提升安全性。作者提出组合推理攻击，将有害查询分解为片段散布在长文本中，通过中性推理查询诱导模型组合出有害意图，发现推理能力强的模型反而更容易组装有害内容而未能拒绝。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证一个假设：更强的推理能力应该能提升LLMs的安全性，帮助模型识别未明确陈述的有害意图。作者在长上下文设置中测试这一假设，其中有害意图是隐式的，需要通过推理来推断。

Method: 提出组合推理攻击这一新威胁模型：将有害查询分解为不完整片段，散布在长达64k token的长上下文中。然后使用中性推理查询诱导模型进行检索和合成，使有害意图仅在组合后显现。评估了14个前沿LLMs，分析推理能力、上下文长度和推理计算量对安全性的影响。

Result: 三个主要发现：1) 推理能力更强的模型对组合推理攻击并不更鲁棒，经常组装有害意图却未能拒绝；2) 安全性对齐随着上下文长度增加而持续退化；3) 推理时计算量是关键缓解因素：增加推理计算可将GPT-oss-120b的攻击成功率降低50个百分点以上。

Conclusion: 安全性不会自动随推理能力扩展，尤其是在长上下文推理场景下。需要专门的安全机制来应对组合推理攻击，推理时计算量是重要的缓解手段。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [61] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Next Concept Prediction (NCP)预训练范式，通过预测跨多个token的离散概念来构建更难的预训练任务，在ConceptLM模型中结合NCP和NTP训练，在多个基准测试中取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统Next Token Prediction (NTP)预训练范式存在局限性，作者认为预测更高层次的语义概念（跨多个token）可以构建更难的预训练任务，从而训练出更强大的语言模型。

Method: 提出ConceptLM模型，使用Vector Quantization对隐藏状态进行量化，构建概念词汇表。模型同时利用NCP（概念预测）和NTP（token预测）进行参数更新，生成的概念指导后续token的生成。

Result: 在13个基准测试中，NCP相比传统token级模型带来一致性能提升。在70M到1.5B参数规模、300B训练数据上验证有效。在8B参数的Llama模型上的持续预训练实验表明，NCP能进一步提升NTP训练模型的性能。

Conclusion: NCP通过引入更难的预训练任务，可以训练出更强大的语言模型，为更好的语言建模提供了有前景的路径。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


### [62] [When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents](https://arxiv.org/abs/2602.08995)
*Yuting Ning,Jaylen Jones,Zhehao Zhang,Chentao Ye,Weitong Ruan,Junyi Li,Rahul Gupta,Huan Sun*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文首次系统性地定义和研究了计算机使用代理（CUAs）中的行为错位检测问题，构建了包含真实场景轨迹的基准测试集MisActBench，并提出了DeAction方法，通过结构化反馈在行为执行前检测和纠正错位行为。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）虽然取得了巨大进展，但仍经常产生偏离用户原始意图的错位行为。这些错位行为可能源于外部攻击（如间接提示注入）或内部限制（如错误推理），不仅带来安全风险，还降低任务效率和可靠性。目前缺乏对CUA行为错位检测的系统性研究。

Method: 1. 首次定义和系统研究CUA中的行为错位检测问题，涵盖外部诱导和内部产生的错位行为；2. 识别现实世界CUA部署中的三个常见类别，构建MisActBench基准测试集，包含人工标注的行为级对齐标签的真实轨迹；3. 提出DeAction方法，这是一种实用且通用的防护机制，在执行前检测错位行为，并通过结构化反馈迭代纠正。

Result: 1. 在MisActBench上，DeAction的F1分数比现有基线高出超过15%；2. 在线评估中，在对抗环境下将攻击成功率降低超过90%，同时在良性环境中保持甚至提高任务成功率；3. 具有适度的延迟开销。

Conclusion: 该研究为CUA行为错位检测提供了首个系统性框架，提出的DeAction方法在检测和纠正错位行为方面表现出色，显著提升了CUA的安全性和可靠性，同时保持了任务效率。

Abstract: Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.

</details>


### [63] [Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark](https://arxiv.org/abs/2602.07079)
*Go Frendi Gunawan,Mukhlis Amien*

Main category: cs.SE

Relevance: 85.0

TL;DR: 本文对11个最先进的LLM在5个软件工程任务上进行了多任务评估，发现模型性能存在显著差异，特别是在完成时间、工具效率和成本方面，同时揭示了工具使用频率与成功率无关的现象。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中展现出卓越能力，但缺乏覆盖多样化SE活动的综合基准。现有研究通常关注单一任务，需要更全面的评估框架来衡量LLM在实际软件工程场景中的表现。

Method: 构建了一个多任务评估框架，涵盖5个代表性软件工程任务：bug修复、功能开发、代码重构、技术文档撰写和研究综述。采用自动化验证框架，测量输出质量和完成效率，分析11个SOTA LLM的表现。

Result: 关键发现包括：1) 相同满分模型在完成时间上存在22倍差异，工具效率49倍差异，成本53倍差异；2) 工具使用频率与成功率无相关性(r=0.077)；3) 识别出两种低效模式：循环低效和推理低效；4) 编码任务成功率100%，研究任务更具挑战性(90.9%)。

Conclusion: LLM在软件工程任务中表现差异显著，效率指标比单纯的质量分数更能反映实际性能。工具使用策略需要优化，研究任务仍是挑战。提供了完整的实验数据和代码供复现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.

</details>


### [64] [Training-Driven Representational Geometry Modularization Predicts Brain Alignment in Language Models](https://arxiv.org/abs/2602.07539)
*Yixuan Liu,Zhiyuan Ma,Likai Tang,Runmin Gan,Xinche Zhang,Jinhao Li,Chao Xie,Sen Song*

Main category: q-bio.NC

Relevance: 85.0

TL;DR: 该研究通过几何表征分析发现，大语言模型在训练过程中会自发形成低复杂度和高复杂度模块，其中低复杂度模块（熵和曲率较低）能更好地预测人类语言网络活动，且这种对齐在时间区域快速稳定，在额叶区域延迟动态。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大语言模型如何与人类语言的神经表征和计算对齐，这是认知科学的核心问题。通过几何表征作为机制视角，理解LLM训练过程中的表征变化如何反映人类语言处理。

Method: 使用表征几何作为机制透镜，追踪Pythia模型（70M-1B参数）训练过程中的熵、曲率和fMRI编码分数。分析不同层的几何特性，识别模块化结构，并评估模型表征与人类语言网络活动的对齐关系。

Result: 发现几何模块化现象：模型层自组织成稳定的低复杂度和高复杂度集群。低复杂度模块（熵和曲率降低）能更好地预测人类语言网络活动。对齐呈现异质时空轨迹：时间区域（AntTemp, PostTemp）快速稳定，额叶区域（IFG, IFGorb）延迟动态。曲率降低是模型-大脑对齐的稳健预测因子，且随模型规模增强。

Conclusion: 训练驱动的几何重组与时间-额叶功能专门化相关，表明表征平滑有助于神经类似的语言处理。这为理解LLM如何发展类似人类的语言表征提供了机制性见解。

Abstract: How large language models (LLMs) align with the neural representation and computation of human language is a central question in cognitive science. Using representational geometry as a mechanistic lens, we addressed this by tracking entropy, curvature, and fMRI encoding scores throughout Pythia (70M-1B) training. We identified a geometric modularization where layers self-organize into stable low- and high-complexity clusters. The low-complexity module, characterized by reduced entropy and curvature, consistently better predicted human language network activity. This alignment followed heterogeneous spatial-temporal trajectories: rapid and stable in temporal regions (AntTemp, PostTemp), but delayed and dynamic in frontal areas (IFG, IFGorb). Crucially, reduced curvature remained a robust predictor of model-brain alignment even after controlling for training progress, an effect that strengthened with model scale. These results links training-driven geometric reorganization to temporal-frontal functional specialization, suggesting that representational smoothing facilitates neural-like linguistic processing.

</details>


### [65] [ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08567)
*Jinnuo Liu,Chuke Liu,Hua Shen*

Main category: cs.MA

Relevance: 85.0

TL;DR: ValueFlow：一个基于扰动的评估框架，用于测量和分析多智能体系统中的价值漂移，通过引入56个价值维度的评估数据集，量化智能体在交互中的价值取向，并分解价值漂移为智能体级响应行为和系统级结构效应。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的价值对齐通常针对孤立模型进行评估，但多智能体系统中价值扰动如何通过智能体交互传播仍不清楚。需要理解价值漂移在多智能体系统中的传播机制。

Method: 基于Schwartz价值调查构建56个价值维度的评估数据集，使用LLM-as-a-judge协议量化智能体在交互中的价值取向。提出两个指标：beta-susceptibility（测量智能体对扰动同伴信号的敏感性）和system susceptibility（捕获节点级扰动如何影响最终系统输出）。

Result: 实验表明，不同模型架构、提示角色、价值维度和网络结构中，易感性存在显著差异，且受结构拓扑的强烈影响。

Conclusion: ValueFlow框架能够有效测量和分析多智能体系统中的价值漂移，揭示了价值易感性在不同维度和结构中的变化规律，为多智能体系统的价值对齐提供了新的评估工具。

Abstract: Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.

</details>


### [66] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

Relevance: 75.0

TL;DR: 像素语言建模旨在通过将文本渲染为图像来绕过子词分词瓶颈，但DualGPT等变体重新引入文本分词器。研究发现，即使在视觉渲染下，重新集成文本分词器仍会引入分词器对齐问题，影响低资源语言性能。


<details>
  <summary>Details</summary>
Motivation: 研究像素语言建模是否真正能摆脱分词限制，特别是在多模态变体重新引入文本分词器的情况下。关注印尼低资源本地语言（爪哇语、巴厘语、巽他语、楠榜语）的分词器对齐问题。

Method: 使用DualGPT架构，在四种印尼低资源语言上评估脚本-分词器对齐的影响。比较Llama 2分词器与自定义分词器的性能，分析OOV（未登录词）率和fertility（生育率）等指标。

Result: 尽管视觉渲染，重新集成文本分词器仍会引入分词器对齐问题。Llama 2分词器虽然OOV和fertility率较低，但性能显著差于自定义分词器，改进幅度高达30.15 chrF++。

Conclusion: 文本分词器仍然是实现公平模型的重要障碍，未来多模态变体需要注意这一问题。像素语言建模的目标可能因重新引入分词器而无法实现。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [67] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文研究了如何为大型语言模型赋予定向多说话人语音理解能力，特别是在智能眼镜应用场景中，提出了两种集成方向性的方法：级联系统和端到端系统。


<details>
  <summary>Details</summary>
Motivation: 当前大多数语音LLM在单通道、单说话人数据上训练，难以直接应用于多说话人和多通道语音理解任务。研究旨在解决智能眼镜等实际场景中的定向语音理解需求。

Method: 提出两种方法：1）级联系统：使用源分离前端模块；2）端到端系统：采用序列化输出训练。两种方法都利用智能眼镜中的多麦克风阵列进行流式定向解释和处理。

Result: 实验结果表明，所提方法能有效赋予LLMs定向语音理解能力，在语音识别和语音翻译任务中均表现出色。

Conclusion: 该工作展示了为LLMs集成定向多说话人语音理解能力的可行性，为智能眼镜等实际应用场景提供了有效的解决方案。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [68] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出一个框架，通过动态路径规划、语义感知示例选择和带对话历史的思维链生成，将复杂句子简化分解为可管理的步骤，在五个语言上提升简化效果并减少计算步骤22-42%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在可控难度句子简化方面能力有限，特别是在跨大阅读难度级别简化时表现不佳。需要解决复杂简化任务中的语义保持问题。

Method: 提出一个框架，包含三个核心组件：1) 动态路径规划将复杂简化分解为可管理步骤；2) 语义感知示例选择提供相关参考；3) 带对话历史的思维链生成实现连贯推理。

Result: 在五个语言的两个基准测试中，该方法提高了简化效果，同时减少了22-42%的计算步骤。人类评估确认了简化效果与意义保持之间的基本权衡。

Conclusion: 逐步简化改善了控制，但在广泛简化过程中保持语义保真度仍然是一个开放挑战。人类标注者在语义保持判断上也存在分歧，突显了该任务的内在复杂性。

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [69] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出了首个计算机使用代理（CUA）意外行为的概念和方法框架，包括定义关键特征、自动引发和分析机制，开发了AutoElicit代理框架来迭代扰动良性指令并引发严重危害，在多个前沿CUA中发现了数百种有害意外行为。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）在自动化复杂操作系统工作流程方面具有巨大潜力，但即使在良性输入环境下也可能表现出偏离预期结果的不安全意外行为。目前对这种风险的研究主要停留在轶事层面，缺乏具体的特征描述和在现实CUA场景下主动发现长尾意外行为的自动化方法。

Method: 提出了AutoElicit代理框架，该框架使用CUA执行反馈迭代扰动良性指令，在保持扰动现实性和良性的同时引发严重危害。通过定义意外行为的关键特征，自动引发它们，并分析它们如何从良性输入中产生。

Result: 使用AutoElicit在Claude 4.5 Haiku和Opus等最先进的CUAs中发现了数百种有害意外行为。进一步评估了人工验证成功扰动的可转移性，发现各种其他前沿CUAs对意外行为存在持续易感性。

Conclusion: 这项工作为在现实计算机使用环境中系统分析意外行为奠定了基础，填补了CUA安全风险研究的空白，提供了自动化的方法来主动发现和评估意外行为。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [70] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

Relevance: 75.0

TL;DR: UReason是一个诊断性基准测试，用于评估推理驱动图像生成中推理是否能在像素层面忠实执行，揭示了推理悖论：推理轨迹能提升性能，但保留中间思考作为上下文条件反而会阻碍视觉合成。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型采用思维链推理来指导图像生成，但推理对视觉合成的实际效果尚不明确。需要评估推理是否能在像素层面被忠实执行，以理解推理在视觉生成中的作用。

Method: 提出UReason基准测试，包含2,000个实例，涵盖代码、算术、空间、属性和文本推理五个任务族。引入评估框架比较三种生成方式：直接生成、推理引导生成、去上下文生成（仅基于精炼提示）。在八个开源统一模型上进行评估。

Result: 发现一致的推理悖论：推理轨迹通常比直接生成表现更好，但保留中间思考作为上下文条件往往阻碍视觉合成，而仅基于精炼提示的条件能带来显著提升。瓶颈在于上下文干扰而非推理能力不足。

Conclusion: UReason为研究统一模型中的推理提供了原则性测试平台，并激励未来方法在有效整合推理进行视觉生成的同时减轻干扰。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [71] [WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints](https://arxiv.org/abs/2602.08367)
*Zexuan Wang,Chenghao Yang,Yingqi Que,Zhenzhu Yang,Huaqing Yuan,Yiwen Wang,Zhengxuan Jiang,Shengjie Fang,Zhenhe Wu,Zhaohui Wang,Zhixin Yao,Jiashuo Liu,Jincheng Ren,Yuzhen Li,Yang Yang,Jiaheng Liu,Jian Yang,Zaiyuan Wang,Ge Zhang,Zhoufutu Wen,Wenhao Huang*

Main category: cs.CL

Relevance: 75.0

TL;DR: WorldTravel是一个包含150个真实世界旅行场景的基准测试，需要处理15+个相互依赖的时空约束。WorldTravel-Webscape是一个多模态环境，包含2000多个渲染网页，要求智能体从视觉布局中感知约束参数。前沿模型表现不佳，GPT-5.2在纯文本设置下仅32.67%可行性，多模态环境下降至19.33%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要包含松散耦合的约束，可通过局部贪婪决策解决，且依赖理想化数据，无法捕捉从动态网络环境中提取参数的复杂性。真实世界的自主规划需要协调紧密耦合的约束，单个决策决定所有后续行动的可行性。

Method: 引入WorldTravel基准测试，包含150个真实世界旅行场景，涉及5个城市，平均需要处理15+个相互依赖的时空和逻辑约束。开发WorldTravel-Webscape多模态环境，包含2000多个渲染网页，智能体必须从视觉布局中感知约束参数以指导规划。

Result: 评估10个前沿模型显示显著性能崩溃：即使是GPT-5.2在纯文本设置下仅达到32.67%可行性，在多模态环境中降至19.33%。识别出关键的感知-行动差距和约10个约束的规划视野阈值，模型推理在此处持续失败，表明感知和推理仍是独立瓶颈。

Conclusion: 研究结果表明需要下一代智能体，将高保真视觉感知与长视野推理统一起来，以处理脆弱的真实世界物流问题。感知和推理的分离限制了当前模型在复杂现实场景中的表现。

Abstract: Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\% feasibility in text-only settings, which plummets to 19.33\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.

</details>


### [72] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

Relevance: 75.0

TL;DR: VocalNet-MDM：基于掩码扩散建模的非自回归语音大语言模型，通过分层块掩码和迭代自蒸馏技术，实现3.7-10倍解码加速和34%首块延迟降低，在有限数据上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前语音LLM主要采用自回归范式，存在严格的串行约束，限制了生成效率并引入曝光偏差。需要探索非自回归范式来提升语音交互的效率和延迟性能。

Method: 提出VocalNet-MDM，采用掩码扩散建模作为非自回归范式。针对流式语音交互的两个关键挑战：训练-推理不匹配和迭代开销，提出分层块掩码对齐训练目标与块扩散解码中的渐进掩码状态，以及迭代自蒸馏将多步优化压缩为更少步骤以实现低延迟推理。

Result: 仅使用6K小时语音数据训练，相比自回归基线实现3.7-10倍解码加速，首块延迟降低34%。在保持竞争性识别准确率的同时，达到最先进的文本质量和语音自然度。

Conclusion: 掩码扩散建模是低延迟、高效语音LLM的有前景且可扩展的替代方案，证明了非自回归范式在语音交互中的潜力。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [73] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

Relevance: 75.0

TL;DR: 研究发现多语言大语言模型存在语言特定的注意力头，可以移除不需要语言的头而不影响目标语言性能


<details>
  <summary>Details</summary>
Motivation: 多语言LLMs在生产部署中效率低下，特别是当只需要支持部分语言时。目前缺乏对多语言LLMs中语言特定注意力头的研究，而这类模型能执行翻译之外的多种任务。

Method: 探索多语言LLMs是否具有针对每种语言的专门注意力头，并研究移除不需要语言的语言特定头而不降低目标语言性能的可能性。

Result: 发现多语言LLMs确实存在语言特定的注意力头，可以移除不需要语言的头而不影响目标语言性能。

Conclusion: 研究结果为多语言LLMs的更高效部署策略提供了信息，能够在保持目标语言高准确性的同时降低模型复杂度。

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [74] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

Relevance: 75.0

TL;DR: 本文提出了一种用于评估生成式AI在意见摘要任务中事实一致性的新型自动化方法，通过测量摘要中的主张与原始评论之间的相似性来评估覆盖率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统基于自动化指标评估机器生成摘要的方法在大语言模型时代存在局限性，特别是在意见摘要任务中，需要更全面和精确的评估技术来评估事实一致性。

Method: 提出了一种完全自动化的方法，通过提取文本中的事实评估，测量摘要中的主张与原始评论之间的相似性，计算覆盖率和一致性，并生成合适的评分。

Result: 该方法能够为相似的主张（无论是否被否定、改写或扩展）分配更高的分数，并且与人类判断的相关性优于现有最先进的指标。

Conclusion: 该方法为生成式AI在意见摘要任务中的事实一致性评估提供了更有效的自动化解决方案，弥补了传统方法的不足。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [75] [Linguistics and Human Brain: A Perspective of Computational Neuroscience](https://arxiv.org/abs/2602.08275)
*Fudong Zhang,Bo Chai,Yujie Wu,Wai Ting Siok,Nizhuan Wang*

Main category: q-bio.NC

Relevance: 75.0

TL;DR: 该论文探讨如何利用大语言模型作为计算神经科学工具，弥合语言学理论与神经科学数据之间的鸿沟，通过"模型-大脑对齐"框架研究语言处理的神经基础。


<details>
  <summary>Details</summary>
Motivation: 弥合语言学抽象理论框架与神经科学经验数据之间的方法论差距，建立语言与大脑关系的计算桥梁。计算神经科学需要新的工具来形式化语言的分层动态结构，并测试语言假设与神经机制之间的关系。

Method: 利用大语言模型作为计算工具，通过其高维表示空间探索语言处理的神经基础。采用"模型-大脑对齐"框架评估语言相关理论的生物学合理性，结合建模、仿真和数据分析方法。

Result: 大语言模型为语言神经基础研究提供了新的规模和方法论框架，其高维表示空间能够更好地捕捉语言处理的复杂性，"模型-大脑对齐"框架为评估语言理论的生物学合理性提供了系统方法。

Conclusion: 大语言模型作为计算神经科学的重要工具，能够有效连接语言学假设与神经机制，为理解语言-大脑关系提供了新的研究范式和方法论基础。

Abstract: Elucidating the language-brain relationship requires bridging the methodological gap between the abstract theoretical frameworks of linguistics and the empirical neural data of neuroscience. Serving as an interdisciplinary cornerstone, computational neuroscience formalizes the hierarchical and dynamic structures of language into testable neural models through modeling, simulation, and data analysis. This enables a computational dialogue between linguistic hypotheses and neural mechanisms. Recent advances in deep learning, particularly large language models (LLMs), have powerfully advanced this pursuit. Their high-dimensional representational spaces provide a novel scale for exploring the neural basis of linguistic processing, while the "model-brain alignment" framework offers a methodology to evaluate the biological plausibility of language-related theories.

</details>


### [76] [Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs](https://arxiv.org/abs/2602.08997)
*Lavender Y. Jiang,Xujin Chris Liu,Kyunghyun Cho,Eric K. Oermann*

Main category: cs.CY

Relevance: 75.0

TL;DR: 该论文指出HIPAA Safe Harbor去标识化方法在现代LLM时代存在缺陷，LLM能够通过临床笔记中的准标识符与身份信息的潜在关联重新识别患者，提出了去标识化的悖论问题。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含患者的隐私脆弱性和个体特征，用于医疗协调和研究。HIPAA Safe Harbor去标识化方法设计于分类表格数据时代，主要移除显式标识符，但忽略了准标识符与身份信息之间的潜在关联，这些关联可被现代LLM捕获，导致患者隐私保护不足。

Method: 1. 使用因果图形式化准标识符与身份信息之间的相关性；2. 通过实证验证：从已去标识的笔记中重新识别患者个体；3. 进行诊断消融实验：即使移除所有其他信息，仅凭诊断信息模型仍能预测患者所在社区。

Result: 1. 验证了现代LLM能够从去标识化临床笔记中重新识别患者；2. 揭示了去标识化的悖论：即使移除所有其他信息，仅凭诊断信息仍能泄露患者地理位置信息；3. 表明当前去标识化方法在LLM时代存在根本性缺陷。

Conclusion: 去标识化在本质上是不完善的，需要社区共同行动来维护患者-提供者信任。论文旨在提高意识并讨论可行的建议，呼吁重新思考在LLM时代如何有效保护患者隐私。

Abstract: Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.

</details>


### [77] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

Relevance: 65.0

TL;DR: 本文提出一个系统评估框架，用于诊断和量化最先进视觉语言模型在多语言表情包数据集上的跨文化鲁棒性，分析学习策略、提示语言和翻译效果三个维度，发现"翻译后检测"方法会降低性能，而文化对齐干预能显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 文化背景深刻影响人们对在线内容的理解，但当前的视觉语言模型主要通过西方或英语中心视角训练，这限制了它们在仇恨表情包检测等任务中的公平性和跨文化鲁棒性。

Method: 引入一个系统评估框架，在多语言表情包数据集上诊断和量化最先进视觉语言模型的跨文化鲁棒性，分析三个维度：(i)学习策略（零样本 vs. 单样本），(ii)提示语言（母语 vs. 英语），(iii)翻译对意义和检测的影响。

Result: 结果显示常见的"翻译后检测"方法会降低性能，而文化对齐干预（母语提示和单样本学习）能显著提升检测效果。研究发现模型系统性地趋向西方安全规范。

Conclusion: 研究揭示了模型系统性地趋向西方安全规范，并提供了可操作的策略来减轻这种偏见，指导设计全球鲁棒的多模态内容审核系统。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [78] [SciClaimEval: Cross-modal Claim Verification in Scientific Papers](https://arxiv.org/abs/2602.07621)
*Xanh Ho,Yun-Ang Wu,Sunisth Kumar,Tian Cheng Xia,Florian Boudin,Andre Greiner-Petter,Akiko Aizawa*

Main category: cs.CL

Relevance: 65.0

TL;DR: SciClaimEval是一个新的科学声明验证数据集，包含从已发表论文中提取的真实声明（包括被反驳的声明），通过修改支持证据（图表）而非使用LLM来创建反驳声明，包含多模态证据格式，并在三个领域评估了11个多模态基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有声明验证数据集通常使用人工构造的声明或依赖LLM生成反驳声明，缺乏真实科学场景中的声明验证能力评估。需要包含真实科学声明（包括被反驳声明）且证据来自实际论文的多模态数据集。

Method: 从180篇论文中提取真实声明，通过修改支持证据（图表）而非修改声明本身来创建反驳声明。数据集包含1,664个标注样本，涵盖机器学习、自然语言处理和医学三个领域。图表以多种格式提供：图像、LaTeX源码、HTML和JSON。

Result: 评估了11个开源和专有多模态基础模型。结果显示所有模型在基于图表的验证任务上表现特别困难，最佳系统与人类基线之间存在显著性能差距。

Conclusion: SciClaimEval为科学声明验证提供了更真实的评估基准，揭示了多模态基础模型在处理图表证据方面的局限性，为未来研究提供了重要方向。

Abstract: We present SciClaimEval, a new scientific dataset for the claim verification task. Unlike existing resources, SciClaimEval features authentic claims, including refuted ones, directly extracted from published papers. To create refuted claims, we introduce a novel approach that modifies the supporting evidence (figures and tables), rather than altering the claims or relying on large language models (LLMs) to fabricate contradictions. The dataset provides cross-modal evidence with diverse representations: figures are available as images, while tables are provided in multiple formats, including images, LaTeX source, HTML, and JSON. SciClaimEval contains 1,664 annotated samples from 180 papers across three domains, machine learning, natural language processing, and medicine, validated through expert annotation. We benchmark 11 multimodal foundation models, both open-source and proprietary, across the dataset. Results show that figure-based verification remains particularly challenging for all models, as a substantial performance gap remains between the best system and human baseline.

</details>


### [79] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

Relevance: 65.0

TL;DR: Bielik Guard是一系列波兰语内容安全分类器，包含0.1B和0.5B两个参数规模的变体，专门用于检测波兰文本中的仇恨/攻击、粗俗内容、性内容、犯罪和自残等五类不安全内容。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在波兰语应用中的部署增加，需要高效准确的内容安全分类器来确保应用的安全性。现有解决方案在波兰语场景下存在性能不足的问题。

Method: 基于MMLW-RoBERTa-base（0.1B参数）和PKOBP/polish-roberta-8k（0.5B参数）两个预训练模型，在6,885个社区标注的波兰文本数据集上进行微调，构建五分类安全分类器。

Result: 0.5B变体在测试集上获得0.791（micro）和0.785（macro）的F1分数，表现最佳；0.1B变体在真实用户提示上达到77.65%的精确率和仅0.63%的假阳性率，显著优于同规模的HerBERT-PL-Guard（31.55%精确率，4.70%假阳性率）。

Conclusion: Bielik Guard系列模型在波兰语内容安全分类任务上表现出色，0.5B变体提供最佳判别能力，0.1B变体在效率和精度间取得良好平衡，特别适合实际部署场景。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [80] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

Relevance: 65.0

TL;DR: SynCog框架通过可控零样本多模态数据合成和思维链推理微调，解决MCI诊断中的数据稀缺和可解释性问题，在跨语言场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 基于语音的数字生物标志物是早期识别轻度认知障碍（MCI）的可扩展、非侵入性方法，但面临临床数据稀缺、缺乏可解释推理、跨语言泛化能力不足等挑战，限制了临床信任和实际应用。

Method: 提出SynCog框架：1）可控零样本多模态数据合成，模拟不同认知特征的虚拟受试者，缓解数据稀缺问题；2）思维链（CoT）推理微调，在多模态大语言模型（MLLMs）上训练模型明确表达诊断思维过程，而非黑盒预测。

Result: 在ADReSS和ADReSSo基准测试中，通过合成数据增强获得80.67%和78.46%的Macro-F1分数，优于当前基线模型。在独立真实世界普通话数据集（CIR-E）上实现48.71%的Macro-F1，展示了强大的跨语言泛化能力。

Conclusion: SynCog为解决临床数据稀缺、提升模型可解释性和跨语言泛化能力提供了有效方案，是向临床可信赖、语言包容的全球医疗认知评估工具迈出的关键一步。

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [81] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

Relevance: 65.0

TL;DR: DIALSUMMER框架用于评估对话摘要质量，提出分层错误分类法（对话级和轮次内级），构建人工标注数据集，分析错误模式，并测试LLM判断能力。


<details>
  <summary>Details</summary>
Motivation: 对话是人类主要沟通方式，自动生成对话摘要具有重要应用价值。现有对话摘要评估方法忽略了两个关键复杂性：(1) 结构转变：从多说话者分散讨论到摘要的连贯句子；(2) 叙述视角转变：从第一/第二人称到摘要的标准化第三人称。

Method: 提出DIALSUMMER框架，包含：1) 分层错误分类法：对话级（关注说话者/轮次）和轮次内级（关注轮次内信息）；2) 构建人工标注的对话摘要数据集；3) 进行错误模式实证分析；4) 测试LLM判断检测这些错误的能力。

Result: 发现有趣趋势：对话中间轮次最容易被遗漏；外部幻觉主要出现在摘要末尾。LLM判断在检测这些错误方面表现有限，表明该数据集的挑战性和未来改进需求。

Conclusion: DIALSUMMER框架为对话摘要评估提供了全面方法，揭示了现有LLM在对话摘要错误检测方面的局限性，为未来研究提供了有价值的基准和方向。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [82] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

Relevance: 65.0

TL;DR: JUSTICE框架通过模拟法官"搜索→预判→撰写"的认知流程，引入预判阶段来提升判决书生成的合法性和准确性，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有判决书生成方法过度简化法律推理过程，特别是忽略了法官形成初步结论的"预判"阶段，导致无法有效获取基础司法要素和建模预判过程，影响最终文档的法律严谨性。

Method: 提出JUSTICE框架，包含三个核心组件：1) 参考性司法要素检索器(RJER)检索法律条文和先例案例；2) 中间结论模拟器(ICE)生成可验证的中间结论实现预判阶段；3) 司法统一合成器(JUS)综合所有输入生成最终判决。

Result: 在领域内法律基准和分布外数据集上的实验表明，JUSTICE显著优于强基线方法，在法律准确性方面取得实质性提升，包括刑期预测准确率提高4.6%。

Conclusion: 明确建模预判过程对于增强生成判决书的法律连贯性和准确性至关重要，JUSTICE框架通过模拟人类法官的认知工作流程有效解决了现有方法的局限性。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [83] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该研究提出了ViGoEmotions——一个包含20,664条社交媒体评论的越南语情感语料库，每条评论被分类为27种细粒度情感。通过评估8个预训练Transformer模型在三种预处理策略下的表现，发现将表情符号转换为文本描述通常能提升BERT类模型性能，而保留表情符号对ViSoBERT和CafeBERT效果最佳。


<details>
  <summary>Details</summary>
Motivation: 情感分类在情感预测和有害内容检测中具有重要作用。尽管大型语言模型在NLP领域取得了显著进展，但越南语情感分类领域仍缺乏高质量、细粒度的标注数据集。本研究旨在填补这一空白，为越南语情感分析提供基准数据集。

Method: 1. 构建ViGoEmotions语料库：包含20,664条社交媒体评论，标注为27种细粒度情感
2. 评估8个预训练Transformer模型：包括ViSoBERT、CafeBERT、PhoBERT等
3. 三种预处理策略：保留原始表情符号并进行规则标准化；将表情符号转换为文本描述；应用ViSoLex模型进行词汇标准化
4. 使用Macro F1和Weighted F1作为评估指标

Result: 1. ViSoBERT取得最佳性能：Macro F1 61.50%，Weighted F1 63.26%
2. CafeBERT和PhoBERT也表现出色
3. 表情符号转换为文本通常提升BERT类模型性能
4. 保留表情符号对ViSoBERT和CafeBERT效果最佳
5. 移除表情符号通常导致性能下降

Conclusion: ViGoEmotions语料库能有效支持多种架构的情感分类任务，但预处理策略和标注质量仍是影响下游性能的关键因素。表情符号的处理方式对模型性能有显著影响，需要根据具体模型选择合适策略。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [84] [Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement](https://arxiv.org/abs/2602.08688)
*Hossein Kermani,Fatemeh Oudlajani,Pardis Yarahmadi,Hamideh Mahdi Soltani,Mohammad Makki,Zahra HosseiniKhoo*

Main category: cs.CL

Relevance: 65.0

TL;DR: 比较三种波斯语推文不文明内容检测方法：人工编码、ParsBERT监督学习、ChatGPT大语言模型。在伊朗#MahsaAmini运动的47,278条推文上评估，ParsBERT在仇恨言论检测上显著优于7个ChatGPT模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同方法在低资源语言（波斯语）环境下检测仇恨言论和不文明内容的有效性，特别是比较传统监督学习与新兴大语言模型的性能差异。

Method: 使用47,278条波斯语推文数据集，比较三种方法：1）人工定性编码作为基准；2）基于ParsBERT的监督学习模型；3）7个不同版本的ChatGPT模型。评估指标包括准确率和效率，并测试了提示语言（英语vs波斯语）的影响。

Result: ParsBERT在仇恨言论检测上显著优于所有ChatGPT模型。ChatGPT不仅在微妙案例上表现不佳，在明确的不文明内容上也存在困难。提示语言（英语/波斯语）对ChatGPT输出没有显著影响。

Conclusion: 对于低资源语言的仇恨言论检测，专门针对该语言训练的监督学习模型（如ParsBERT）比通用大语言模型（如ChatGPT）表现更好。大语言模型在跨语言、跨文化语境下存在局限性。

Abstract: This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.

</details>


### [85] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出一种大规模比较和可视化句子编码器的方法，通过创建编码器地图来展示1101个公开句子编码器的关系图谱。


<details>
  <summary>Details</summary>
Motivation: 当前存在大量预训练句子编码器，但缺乏系统性的比较和可视化方法，难以理解不同编码器之间的关系和特性。

Method: 1) 将每个句子编码器表示为句子集的嵌入矩阵；2) 计算编码器的成对内积(PIP)矩阵；3) 基于量子相对熵(QRE)构建每个编码器相对于单位基编码器的特征向量；4) 创建包含1101个编码器的地图。

Result: 成功构建了编码器地图，准确反映了编码器之间的各种关系，相似属性的编码器在地图上位置相近。编码器特征向量能够准确预测下游任务（检索和聚类）性能。

Conclusion: 该方法为预训练句子编码器提供了新的视角，创建的地图忠实反映了编码器特性，特征向量可用于性能预测。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [86] [Massive Sound Embedding Benchmark (MSEB)](https://arxiv.org/abs/2602.07143)
*Georg Heigold,Ehsan Variani,Tom Bagby,Cyril Allauzen,Ji Ma,Shankar Kumar,Michael Riley*

Main category: cs.SD

Relevance: 65.0

TL;DR: 提出了大规模声音嵌入基准（MSEB），一个评估多模态系统听觉能力的可扩展框架，包含8个核心任务，旨在加速机器听觉智能的发展。


<details>
  <summary>Details</summary>
Motivation: 音频是多模态感知的关键组成部分，任何真正的智能系统都需要展示广泛的听觉能力（转录、分类、检索、推理、分割、聚类、重排序和重建）。目前缺乏统一的评估框架来衡量这些能力。

Method: 设计了一个可扩展的评估框架MSEB，包含8个核心任务，使用多样化数据集（包括新的大规模简单语音问题SVQ数据集）。将原始音频信号转换为有意义的嵌入表示（单个向量、连续或离散表示序列等），作为任务响应的基础。

Result: 建立了清晰的性能上限，展示了在音频作为核心信号的真实多模态体验中仍有显著的改进空间。提供了公开的代码库供研究社区使用。

Conclusion: MSEB为评估多模态系统的听觉能力提供了标准化基准，鼓励研究社区使用该框架评估算法并为其发展做出贡献，以加速机器听觉智能的进步。

Abstract: Audio is a critical component of multimodal perception, and any truly intelligent system must demonstrate a wide range of auditory capabilities. These capabilities include transcription, classification, retrieval, reasoning, segmentation, clustering, reranking, and reconstruction. Fundamentally, each task involves transforming a raw audio signal into a meaningful 'embedding' - be it a single vector, a sequence of continuous or discrete representations, or another structured form - which then serves as the basis for generating the task's final response. To accelerate progress towards robust machine auditory intelligence, we present the Massive Sound Embedding Benchmark (MSEB): an extensible framework designed to evaluate the auditory components of any multimodal system. In its first release, MSEB offers a comprehensive suite of eight core tasks, with more planned for the future, supported by diverse datasets, including the new, large-scale Simple Voice Questions (SVQ) dataset. Our initial experiments establish clear performance headrooms, highlighting the significant opportunity to improve real-world multimodal experiences where audio is a core signal. We encourage the research community to use MSEB to assess their algorithms and contribute to its growth. The library is publicly hosted at github.

</details>


### [87] [On Sequence-to-Sequence Models for Automated Log Parsing](https://arxiv.org/abs/2602.07698)
*Adam Sorrenti,Andriy Miranskyy*

Main category: cs.SE

Relevance: 65.0

TL;DR: 系统评估了Transformer、Mamba、LSTM等序列建模架构在日志解析任务中的性能与计算成本，发现Transformer准确率最高，Mamba在计算效率上具有优势。


<details>
  <summary>Details</summary>
Motivation: 自动化日志解析面临异构日志格式、训练部署数据分布偏移、基于规则方法脆弱性等挑战，需要系统评估不同序列建模架构的性能和计算成本。

Method: 通过控制实验比较Transformer、Mamba状态空间模型、单向LSTM和双向LSTM四种架构，训练396个模型，使用相对Levenshtein编辑距离评估，并进行统计显著性检验。

Result: Transformer获得最低平均相对编辑距离(0.111)，Mamba(0.145)次之，两者均优于LSTM模型。Mamba在保持竞争力的准确率下显著降低计算成本。字符级分词提升性能，序列长度对Transformer影响可忽略。

Conclusion: Transformer减少23.4%的解析错误，Mamba是数据或计算受限时的强有力替代方案。研究结果为日志解析的架构选择提供了实用指导。

Abstract: Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.

</details>


### [88] [Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches](https://arxiv.org/abs/2602.08561)
*Syed Mehtab Hussain Shah,Frank Hopfgartner,Arnim Bleier*

Main category: cs.SE

Relevance: 65.0

TL;DR: 该研究评估了LLM和AI代理能否自动诊断和修复计算研究中的可重复性问题，使用基于R的社会科学研究构建测试平台，测试了提示式和代理式两种工作流，发现代理式系统表现显著更好。


<details>
  <summary>Details</summary>
Motivation: 计算研究的可重复性常因缺失包、路径问题、版本冲突或不完整逻辑而失败，即使共享了材料。研究旨在探索LLM和AI代理能否自动化诊断和修复这些失败，使计算结果更易重现和验证。

Method: 使用五个完全可重复的R语言社会科学研究构建受控可重复性测试平台，注入从简单到复杂的现实失败案例。测试两种自动化修复工作流：1) 提示式工作流：使用结构化提示反复查询语言模型；2) 代理式工作流：使用能检查文件、修改代码、自主重新运行分析的代理系统。所有测试在干净的Docker环境中进行。

Result: 提示式工作流的成功率为31-79%，性能受提示上下文和错误复杂度显著影响，复杂案例从额外上下文中获益最多。代理式工作流表现显著更好，所有复杂度级别的成功率达到69-96%。

Conclusion: 自动化工作流，特别是代理式系统，能显著减少手动工作量并提高各种错误类型的可重复性成功率。与先前基准不同，该测试平台在受控失败模式下隔离了发布后修复，允许直接比较提示式和代理式方法。

Abstract: Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.

</details>


### [89] [Beyond Transcripts: A Renewed Perspective on Audio Chaptering](https://arxiv.org/abs/2602.08979)
*Fabian Retkowski,Maike Züfle,Thai Binh Nguyen,Jan Niehues,Alexander Waibel*

Main category: cs.SD

Relevance: 65.0

TL;DR: 本文系统研究了音频章节划分任务，比较了文本模型、音频专用架构AudioSeg和多模态LLM，发现AudioSeg显著优于文本方法，停顿是最重要的声学特征，MLLM受限于上下文长度但在短音频上表现良好。


<details>
  <summary>Details</summary>
Motivation: 音频章节划分对于播客、讲座和视频导航越来越重要，但现有研究有限且主要基于文本，未能充分利用音频信息、处理ASR错误或进行无文本评估。本文旨在填补这些空白。

Method: 通过三个贡献：(1) 系统比较文本模型（带声学特征）、新颖的音频专用架构AudioSeg（基于学习音频表示）和多模态LLM；(2) 实证分析影响性能的因素，包括转录质量、声学特征、时长和说话人组成；(3) 形式化评估协议，对比依赖转录的文本空间协议和转录不变的时间空间协议。

Result: 在YTSeg数据集上的实验表明：AudioSeg显著优于文本方法；停顿提供最大的声学增益；MLLM受限于上下文长度和弱指令跟随能力，但在短音频上表现良好。

Conclusion: 音频专用架构在音频章节划分任务上优于文本方法，声学特征特别是停顿信息至关重要，多模态LLM目前仍有局限性但具有潜力。

Abstract: Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.

</details>


### [90] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

Relevance: 45.0

TL;DR: 本文提出一个针对法律文档的长上下文问答系统，能够处理复杂文档布局、专业术语，并生成全面的长形式答案，同时引入覆盖率指标评估召回性能。


<details>
  <summary>Details</summary>
Motivation: 法律文档具有复杂的文档布局（多级嵌套章节、长脚注）、专业术语和复杂句法结构，这使得长上下文问答（答案跨越多页）和生成全面长形式答案具有挑战性。现有系统难以处理这些特性。

Method: 提出一个问答系统，包含三个关键组件：(1) 解构领域特定词汇以改进文档检索；(2) 解析复杂文档布局，隔离章节和脚注并适当链接；(3) 使用精确领域词汇生成全面答案。同时引入覆盖率指标，将性能分类为基于召回的覆盖类别，便于人工评估召回率。

Result: 通过综合实验和消融研究证明了所提系统的可用性和优势。利用法律和公司税务等领域的专业知识策划了QA数据集进行验证。

Conclusion: 该系统有效解决了法律文档长上下文问答的挑战，特别是在处理复杂布局、专业术语和生成全面长形式答案方面。覆盖率指标为评估召回性能提供了实用工具。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [91] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

Relevance: 45.0

TL;DR: 提出了ViHERMES数据集，专门用于越南语医疗法规的多跳问答基准测试，包含高质量的问题-答案对，需要跨多个法规进行推理，并提出了基于图感知的检索框架。


<details>
  <summary>Details</summary>
Motivation: 医疗法规问答具有挑战性，需要跨法律相互依赖的文本进行多跳推理，尤其是在越南语等低资源语言中，缺乏支持医疗法规多跳推理的基准数据集。

Method: 1) 基于语义聚类和图启发数据挖掘的受控多跳QA生成流程；2) 基于大语言模型的生成，带有结构化证据和推理标注；3) 图感知检索框架，在法条单元级别建模正式法律关系，支持原则性上下文扩展。

Result: ViHERMES为评估多跳法规QA系统提供了具有挑战性的基准，提出的图感知方法在实验中持续优于强检索基线方法。

Conclusion: ViHERMES填补了越南语医疗法规多跳推理基准的空白，提出的图感知检索框架能有效处理法规文档的层次结构和相互依赖关系。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [92] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

Relevance: 45.0

TL;DR: 该研究提出了一种改进印度法律文本摘要的方法，通过注入领域知识到多种摘要模型中，生成英语和印地语的法律判决摘要。


<details>
  <summary>Details</summary>
Motivation: 印度法律判决摘要面临双重挑战：法律文本语言复杂且非结构化，同时大量印度人口不理解复杂的英语法律文本，需要印度语言摘要。需要改进法律文本摘要以生成英语和印地语摘要。

Method: 1) 提出框架增强抽取式神经摘要模型，通过整合针对法律文本的领域特定预训练编码器；2) 探索通过持续预训练在大型英语和印地语法律语料库上，将法律领域知识注入生成模型（包括大语言模型）。

Result: 提出的方法在英语到英语和英语到印地语的印度法律文档摘要中，在标准评估指标、事实一致性指标和法律领域特定指标上都取得了统计显著的改进。这些改进通过领域专家验证，证明了方法的有效性。

Conclusion: 通过注入领域知识到摘要模型中，可以有效改进印度法律文本的多语言摘要质量，特别是在法律领域特定指标上表现优异。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [93] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

Relevance: 45.0

TL;DR: 本文提出了一种基于认知语言模式、LLMs和隐式隐喻的认知语言身份融合评分方法，用于从语言中测量身份融合，在预测极端行为意愿方面优于现有方法，并识别出两种不同的高融合暴力路径。


<details>
  <summary>Details</summary>
Motivation: 随着社会两极分化和政治暴力加剧，理解极端主义的心理根源变得日益重要。先前研究表明身份融合能够预测个体参与极端行为的意愿，但需要更有效的测量方法。

Method: 开发了认知语言身份融合评分方法，结合认知语言模式、大型语言模型和隐式隐喻分析，从语言中测量身份融合程度。在英国和新加坡的数据集上进行验证。

Result: 该方法在预测已验证的身份融合分数方面优于现有方法。应用于极端主义宣言分析时，识别出两种不同的高融合暴力路径：意识形态驱动者倾向于以群体术语框架自我，形成亲属关系纽带；而怨恨驱动者则将群体框架为个人身份的一部分。

Conclusion: 研究结果完善了身份融合理论，并提供了一个可扩展的工具，有助于身份融合研究和极端主义检测。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [94] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

Relevance: 45.0

TL;DR: 研究探索了多模态澄清问题在对话式搜索中的作用，发现图像对用户表现的影响因任务类型和用户专业水平而异


<details>
  <summary>Details</summary>
Motivation: 虽然文本澄清问题已被证明能提升检索性能和用户体验，但图像在澄清问题中的作用尚未被充分探索。研究旨在了解多模态澄清问题在对话式搜索中对用户表现的影响

Method: 进行了包含73名参与者的用户研究，比较多模态和纯文本澄清问题在两种搜索相关任务中的效果：(i) 回答澄清问题，(ii) 查询重构。从多个角度分析图像的影响

Result: 1. 在回答澄清问题时，参与者强烈偏好多模态问题，但纯文本设置下用户表现更好；2. 在查询重构任务中，偏好更平衡，图像能带来更精确的查询和更好的检索性能；3. 图像的影响因任务类型和用户专业水平而异

Conclusion: 视觉增强的益处是任务依赖性的，应根据具体搜索上下文和用户特征进行战略实施。多模态对话搜索系统的设计需要考虑任务类型和用户专业水平

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [95] [GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search](https://arxiv.org/abs/2602.08945)
*Sahajpreet Singh,Kokil Jaidka,Min-Yen Kan*

Main category: cs.CL

Relevance: 45.0

TL;DR: GitSearch是一个用于社区内容审核的框架，通过识别信息缺口、实时网络检索和合成平台合规注释来解决冷启动问题，在政治推文基准上表现优于现有方法和人工注释。


<details>
  <summary>Details</summary>
Motivation: 社区审核虽可扩展但面临结构性挑战，现有AI方法在冷启动场景下失效。需要解决人类感知的质量缺口（如缺失上下文）作为首要信号。

Method: GitSearch采用三阶段流水线：1) 识别信息缺陷；2) 实时定向网络检索填补缺口；3) 合成平台合规注释。同时构建PolBench基准（78,698条美国政治推文及其社区注释）。

Result: GitSearch达到99%覆盖率（几乎是SOTA的两倍），以69%胜率超越人工编写的有用注释，获得更高的有用性评分（3.87 vs 3.36），在规模与质量间取得平衡。

Conclusion: GitSearch通过将人类感知的质量缺口作为首要信号，有效解决了社区审核的冷启动问题，在覆盖率和质量上均显著优于现有方法。

Abstract: Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.

</details>


### [96] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

Relevance: 35.0

TL;DR: BiomechAgent是一个代码生成AI代理，通过自然语言实现生物力学分析，无需用户编程，支持数据查询、可视化和临床推理。


<details>
  <summary>Details</summary>
Motivation: 虽然无标记运动捕捉技术使定量运动分析更易获取，但分析结果数据对于没有编程经验的临床医生仍然存在障碍。需要让生物力学分析更易访问。

Method: 开发了BiomechAgent代码生成AI代理，使用自然语言界面，集成专门工具进行步态事件检测，采用生物力学领域特定的指令，并评估了云端LLM与本地开源模型的性能差异。

Result: BiomechAgent在数据检索和可视化任务上达到稳健准确率，展示了新兴的临床推理能力。生物力学领域特定指令显著优于通用提示，专门工具集成大幅提升了时空分析准确性。本地开源模型在除数据检索外的多数领域表现明显下降。

Conclusion: BiomechAgent使无标记运动捕捉数据对终端用户更加有用和可访问，通过自然语言界面降低了生物力学分析的技术门槛。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [97] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

Relevance: 35.0

TL;DR: Open TutorAI是一个基于LLM和生成技术的开源教育平台，提供动态个性化辅导，整合了NLP和可定制3D化身，支持多模态学习交互。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人系统缺乏上下文适应性、实时响应性和教学灵活性，限制了学习参与度和教学效果，需要结合AI和沉浸式技术的开放集成平台来支持个性化学习体验。

Method: 基于LLM和生成技术构建开源教育平台，整合自然语言处理与可定制3D化身，通过结构化注册流程捕获学习者目标和偏好，配置个性化AI助手，提供文本和化身驱动界面，包含内容组织、嵌入式反馈和学习分析工具。

Result: 开发了Open TutorAI平台，结合模块化架构、生成AI和学习分析，创建了更具人性化、沉浸式的学习环境，支持自适应学习支持，增强参与度和情感存在感。

Conclusion: Open TutorAI通过整合模块化架构、生成AI和学习分析，为下一代智能辅导系统的发展做出贡献，提供了无需技术专长即可响应个体学习者配置的适应性支持工具。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [98] [NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark](https://arxiv.org/abs/2602.08162)
*Ricardo Campos,José Pedro Evans,José Miguel Isidro,Miguel Marques,Luís Filipe Cunha,Alípio Jorge,Sérgio Nunes,Nuno Guimarães*

Main category: cs.CL

Relevance: 35.0

TL;DR: 本文综述了NLP在地方政府会议记录结构化处理中的应用，重点讨论了文档分割、领域实体抽取和自动文本摘要三个核心任务，旨在提高政府记录的透明度和可访问性。


<details>
  <summary>Details</summary>
Motivation: 地方政府会议记录作为官方文件，虽然结构化但通常内容密集、官僚化且在不同城市间高度异质，导致非专家难以解读，自动化系统处理困难，限制了公共透明度和公民参与。需要计算方法来增强这类复杂文档的可访问性和可解释性。

Method: 本文是一篇综述文章，系统回顾了支持地方政府会议记录结构化的三个核心NLP任务：1) 文档分割（将长文档分解为逻辑部分），2) 领域特定实体抽取（识别政治参与者和个人信息），3) 自动文本摘要（生成复杂决策过程的简洁表示）。讨论了方法学方法、评估指标和公开可用资源。

Result: 通过综合现有工作，本文提供了NLP如何增强地方政府会议记录结构化和可访问性的结构化概述，识别了领域特定挑战，如数据稀缺性、隐私约束和来源变异性。

Conclusion: NLP方法可以有效解决地方政府会议记录的异质性和复杂性挑战，通过文档分割、实体抽取和文本摘要等任务，能够显著提高政府记录的透明度和公民可访问性，促进更好的公民参与。

Abstract: Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.

</details>


### [99] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

Relevance: 35.0

TL;DR: 该论文探讨人类语义类别系统的两个特征：凸性和效率性。通过信息瓶颈框架分析颜色命名系统，发现两者本质不同但常共现，效率性比凸性更能解释语义类型学现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解人类语义类别系统中凸性（概念空间的凸划分）和效率性（通信效率）之间的关系。虽然先前研究观察到两者在颜色命名中共现，但它们的分析关系和共现原因尚未得到充分理解。

Method: 采用信息瓶颈（IB）框架进行语义效率分析，结合分析和实证方法：1）证明凸性和效率性在逻辑上相互独立；2）在颜色命名领域分析IB最优系统的凸性特征；3）比较两者对实际颜色命名系统的预测能力；4）讨论凸性无法解释但效率性能解释的经验现象。

Result: 1）凸性和效率性本质不同，互不蕴含；2）IB最优系统在颜色命名领域大多呈现凸性，解释了凸性方法的经验基础；3）效率性比凸性更能区分实际颜色命名系统与假设变体；4）效率性能解释凸性无法解释的一系列经验现象。

Conclusion: 虽然凸性和效率性可能产生相似的结构观察结果，但它们是根本不同的概念。效率性为语义类型学提供了更全面的解释框架，而凸性只是效率性在某些特定条件下的表现。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [100] [Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network](https://arxiv.org/abs/2602.08289)
*Binglin Wu,Xianneng Li*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出基于超图神经网络的实体关系抽取算法Legal-KAHRE，针对毒品案件判决文书，结合司法领域知识提升抽取效果


<details>
  <summary>Details</summary>
Motivation: 随着中国司法机构数字化进程，积累了海量电子法律文档，但现有实体关系抽取方法缺乏领域知识，未能充分考虑司法领域特性

Method: 1) 基于邻域导向打包策略和双仿射机制的候选跨度生成器；2) 构建法律词典并通过多头注意力融入文本编码；3) 将共同犯罪、数罪并罚等司法概念融入超图结构设计；4) 使用超图神经网络进行高阶推理

Result: 在CAIL2022信息抽取数据集上显著优于现有基线模型

Conclusion: 提出的Legal-KAHRE算法有效整合司法领域知识，通过超图神经网络处理复杂法律关系，在法律文档信息抽取任务中表现优异

Abstract: With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.

</details>


### [101] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出基于注意力机制的生成式框架，同时处理多意图检测和槽填充任务，通过注意力-注意力解码器处理可变意图数量和任务间干扰，并构建新的多意图SLU数据集。


<details>
  <summary>Details</summary>
Motivation: 现实对话场景中用户常表达多个意图，但现有任务导向对话系统主要针对单意图SLU，缺乏处理多意图的能力，需要新的方法和数据集来解决这一挑战。

Method: 提出生成式框架，使用注意力-注意力解码器处理可变意图数量，通过引入归纳偏置解决多任务学习中的任务干扰问题。利用BERT的NSP头基于单意图话语构建新的多意图SLU数据集。

Result: 在MixATIS和MixSNIPS两个公开数据集以及新构建的数据集上，提出的注意力-注意力生成模型取得了最先进的性能。

Conclusion: 提出的生成式框架能有效处理多意图SLU任务，注意力-注意力解码器设计解决了可变意图数量和任务干扰问题，新构建的数据集为多意图SLU研究提供了资源。

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [102] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

Relevance: 35.0

TL;DR: LakeHopper框架通过识别知识差距、聚类选择目标数据和增量微调，实现预训练LM模型在不同数据湖间的迁移，减少新数据湖的标注需求


<details>
  <summary>Details</summary>
Motivation: 现有列类型标注方法依赖在特定数据湖上精细标注和微调的语言模型，当迁移到新数据湖时需要大量新标注。研究如何最小化新数据湖的标注需求，同时解决源-目标知识差距、信息性数据选择和保留共享知识等挑战

Method: 提出LakeHopper框架：1) 通过LM交互识别和解决源-目标知识差距；2) 基于聚类的数据选择方案从未标注列中选择信息性数据；3) 增量微调机制逐步将源模型适应到目标数据湖

Result: 在两个不同数据湖迁移任务上的实验验证了LakeHopper在低资源和高资源设置下的有效性

Conclusion: LakeHopper能够有效实现预训练LM模型在不同数据湖间的迁移，显著减少新数据湖的标注需求，同时保持模型性能

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [103] [Measuring Complexity at the Requirements Stage: Spectral Metrics as Development Effort Predictors](https://arxiv.org/abs/2602.07182)
*Maximilian Vierlboeck,Antonio Pugliese,Roshanak Nilchian,Paul Grogan,Rashika Sugganahalli Natesh Babu*

Main category: cs.SE

Relevance: 35.0

TL;DR: 该研究提出了一种基于自然语言处理的方法，通过从文本需求中提取结构网络，并利用分子集成任务作为结构同构代理，来量化和预测需求规范中的结构复杂性对系统集成工作的影响。


<details>
  <summary>Details</summary>
Motivation: 工程系统中的复杂性是现代开发中最持久的挑战之一，导致成本超支、进度延迟和项目失败。虽然架构复杂性已被研究，但需求规范中嵌入的结构复杂性仍然未被充分理解和量化。这一差距很重要，因为需求从根本上驱动系统设计，此阶段引入的复杂性会通过架构、实现和集成传播。

Method: 基于自然语言处理方法从文本需求中提取结构网络，使用分子集成任务作为结构同构代理进行受控实验。利用分子图和需求网络之间的拓扑等价性，同时消除领域专业知识和语义歧义等混杂因素。

Result: 谱测量预测集成工作的相关性超过0.95，结构指标的相关性超过0.89。值得注意的是，基于密度的指标没有显示出显著的预测有效性。特征值衍生指标能够捕捉简单连接性指标无法捕捉的认知和工作量维度。

Conclusion: 该研究在架构复杂性分析和需求工程实践之间架起了关键的方法论桥梁，为将这些指标应用于需求工程提供了验证基础，其中类似的结构复杂性模式可以预测集成工作量。

Abstract: Complexity in engineered systems presents one of the most persistent challenges in modern development since it is driving cost overruns, schedule delays, and outright project failures. Yet while architectural complexity has been studied, the structural complexity embedded within requirements specifications remains poorly understood and inadequately quantified. This gap is consequential: requirements fundamentally drive system design, and complexity introduced at this stage propagates through architecture, implementation, and integration. To address this gap, we build on Natural Language Processing methods that extract structural networks from textual requirements. Using these extracted structures, we conducted a controlled experiment employing molecular integration tasks as structurally isomorphic proxies for requirements integration - leveraging the topological equivalence between molecular graphs and requirement networks while eliminating confounding factors such as domain expertise and semantic ambiguity. Our results demonstrate that spectral measures predict integration effort with correlations exceeding 0.95, while structural metrics achieve correlations above 0.89. Notably, density-based metrics show no significant predictive validity. These findings indicate that eigenvalue-derived measures capture cognitive and effort dimensions that simpler connectivity metrics cannot. As a result, this research bridges a critical methodological gap between architectural complexity analysis and requirements engineering practice, providing a validated foundation for applying these metrics to requirements engineering, where similar structural complexity patterns may predict integration effort.

</details>


### [104] [Prototype-Based Disentanglement for Controllable Dysarthric Speech Synthesis](https://arxiv.org/abs/2602.08696)
*Haoshen Wang,Xueli Zhong,Bingbing Lin,Jia Huang,Xingduo Pan,Shengxiang Liang,Nizhuan Wang,Wai Ting Siok*

Main category: cs.SD

Relevance: 35.0

TL;DR: 提出ProtoDisent-TTS框架，通过原型码本在统一潜空间中解耦说话人音色与构音障碍特征，实现健康与障碍语音的双向转换，提升ASR性能。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音存在高变异性且标注数据有限，现有方法依赖合成数据增强或语音重建，但常将说话人身份与病理特征纠缠，限制了可控性和鲁棒性。

Method: 基于预训练TTS骨干构建原型解耦框架，使用病理原型码本提供可解释的健康/障碍语音模式表示，采用带梯度反转层的双分类器目标强制说话人嵌入对病理属性的不变性。

Result: 在TORGO数据集上实验表明，该设计能实现健康与构音障碍语音的双向转换，带来一致的ASR性能提升和鲁棒、说话人感知的语音重建。

Conclusion: ProtoDisent-TTS通过原型码本和双分类器目标有效解耦说话人音色与病理特征，为构音障碍语音处理提供了可控、鲁棒的解决方案。

Abstract: Dysarthric speech exhibits high variability and limited labeled data, posing major challenges for both automatic speech recognition (ASR) and assistive speech technologies. Existing approaches rely on synthetic data augmentation or speech reconstruction, yet often entangle speaker identity with pathological articulation, limiting controllability and robustness.
  In this paper, we propose ProtoDisent-TTS, a prototype-based disentanglement TTS framework built on a pre-trained text-to-speech backbone that factorizes speaker timbre and dysarthric articulation within a unified latent space. A pathology prototype codebook provides interpretable and controllable representations of healthy and dysarthric speech patterns, while a dual-classifier objective with a gradient reversal layer enforces invariance of speaker embeddings to pathological attributes. Experiments on the TORGO dataset demonstrate that this design enables bidirectional transformation between healthy and dysarthric speech, leading to consistent ASR performance gains and robust, speaker-aware speech reconstruction.

</details>


### [105] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

Relevance: 30.0

TL;DR: 该立场论文认为当前语言识别系统覆盖范围有限是因为将LID框架化为去上下文的文本分类，忽视了先验概率估计的重要性，并提出应将LID重新定义为路由问题并纳入环境线索。


<details>
  <summary>Details</summary>
Motivation: 商业语言识别系统仅能可靠识别几百种书面语言，研究级系统在某些情况下扩展了覆盖范围，但大多数语言仍然覆盖不全或完全没有覆盖。作者认为这种情况主要是自我造成的，源于将LID持久地框架化为去上下文的文本分类。

Method: 本文是立场论文，提出理论框架而非具体技术方法。主张重新思考LID作为路由问题，开发原则性方法来纳入环境线索，使语言在本地环境中变得合理。

Result: 论文提出了概念性框架，指出当前LID方法的局限性源于机构激励偏向全球固定先验模型，并论证了改进尾部语言覆盖需要系统性方法变革。

Conclusion: 改进尾部语言覆盖需要重新将LID概念化为路由问题，开发原则性方法来整合环境线索，挑战当前去上下文的文本分类框架和机构激励结构。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [106] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

Relevance: 25.0

TL;DR: 该研究探讨了机器翻译在印度语言（孟加拉语、马拉雅拉姆语、泰卢固语）中的实际应用和方法论意义，重点关注新兴翻译工作流程和现有评估框架，特别关注技术教育内容的翻译质量评估。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于印度语言多样性背景下技术教育内容的翻译需求，特别是NEP 2020政策推动下的多语言教育技术适应。研究选择这三种语言是因为它们代表了不同的语言特征，且NPTEL大规模在线课程平台提供了丰富的语料资源。

Method: 研究使用NPTEL MOOC平台作为语料库，构建了包含技术概念清晰表达的自发语音语料库，考虑了适当的语域和词汇选择。研究分析了形态丰富和语义紧凑的语言特征对表面重叠评估指标的影响。

Result: 研究发现评估指标对特定语言特征敏感，形态丰富和语义紧凑的语言特征在表面重叠指标测试中面临挑战，揭示了现有机器翻译评估框架在印度语言上的局限性。

Conclusion: 研究强调了在印度多语言环境下开发更合适的机器翻译评估方法的重要性，特别是对于技术教育内容的翻译质量评估需要更精细的指标。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [107] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

Relevance: 15.0

TL;DR: 本文提出了一种基于词汇相似度的计算指标来评估罗曼语族语言间的相互可理解性，通过表面和语义相似度分析，验证了不同语言间可理解性的不对称性。


<details>
  <summary>Details</summary>
Motivation: 研究罗曼语族语言间的相互可理解性，传统方法依赖人工实验，成本高且难以大规模应用。需要开发计算指标来量化语言间的可理解性，验证语言相似性与可理解性的关系。

Method: 提出基于词汇相似度的计算指标，结合表面相似度（拼写）和语义相似度。使用五种主要罗曼语言（法语、意大利语、葡萄牙语、西班牙语、罗马尼亚语），比较正字法和语音形式，使用不同平行语料库和词向量模型表示词义。

Result: 计算得到的可理解性分数证实了语言间可理解性不对称的直觉，并且与人类完形填空实验结果显著相关。验证了计算指标的有效性。

Conclusion: 基于词汇相似度的计算指标能够有效评估罗曼语族语言间的相互可理解性，为语言可理解性研究提供了可扩展的计算方法。

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出一种基于归因的人类先验对齐方法，通过惩罚模型对偏离人类先验证据的依赖，引导模型决策依据向预期区域转移，提高任务准确性和决策合理性。


<details>
  <summary>Details</summary>
Motivation: 可靠模型不仅需要正确预测，还应提供可接受的证据来证明决策。传统监督学习仅提供类别标签，导致模型可能通过捷径相关性而非预期证据实现高准确率。人类先验可以帮助约束这种行为，但将模型与这些先验对齐仍然具有挑战性，因为学习到的表示常常偏离人类感知。

Method: 提出基于归因的人类先验对齐方法：1) 将人类先验编码为模型预期依赖的输入区域（如边界框）；2) 利用高保真度的基于子集选择的归因方法在训练期间暴露模型的决策证据；3) 当归因区域显著偏离先验区域时，惩罚对非先验证据的依赖；4) 通过施加由人类先验诱导的归因约束的训练目标，鼓励模型将其归因转向预期区域。

Result: 在基于MLLM的GUI代理模型的图像分类和点击决策任务上进行验证。在传统分类和自回归生成设置中，人类先验对齐一致地提高了任务准确性，同时增强了模型的决策合理性。

Conclusion: 该方法通过将人类先验作为归因约束融入训练过程，有效地引导模型依赖预期证据进行决策，提高了模型的可靠性和可解释性，为解决模型依赖捷径相关性而非预期证据的问题提供了有效途径。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [109] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

Relevance: 85.0

TL;DR: CR-VLM提出了一种基于激活导向的可配置拒绝机制，通过教师强制提取拒绝向量、门控机制防止过度拒绝、以及反事实视觉增强模块，实现视觉语言模型中的用户自适应安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的拒绝机制通常是"一刀切"的，无法适应多样化的用户需求和上下文约束，导致要么拒绝不足要么过度拒绝。需要一种可配置的拒绝方法来平衡安全性和实用性。

Method: 1) 通过教师强制机制提取可配置拒绝向量以放大拒绝信号；2) 引入门控机制保护范围内查询的接受能力，减轻过度拒绝；3) 设计反事实视觉增强模块，将视觉表示与拒绝要求对齐。

Result: 在多个数据集和各种VLM上的综合实验表明，CR-VLM实现了有效、高效且鲁棒的可配置拒绝，为VLM中的用户自适应安全对齐提供了可扩展的路径。

Conclusion: CR-VLM为视觉语言模型提供了一种稳健高效的可配置拒绝方法，能够更好地平衡安全性和实用性，推动用户自适应安全对齐的发展。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [110] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出了一种机制性分析方法，通过分析开放权重视觉语言模型（VLMs）的表征几何结构，提取"概念向量"来理解模型在多物体视觉任务中的失败模式，如幻觉和识别错误。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多物体视觉任务中表现出令人困惑的失败模式，如幻觉不存在的元素或无法在干扰物中识别最相似的对象。这些错误类似于人类的认知约束（如"绑定问题"），但人工系统中驱动这些错误的内部机制仍不清楚。研究旨在理解VLMs内部表征如何塑造模型行为并导致视觉失败。

Method: 分析开放权重VLMs（Qwen、InternVL、Gemma）的表征几何结构，比较提取"概念向量"（编码视觉概念的潜在方向）的方法论。通过引导干预验证概念向量，在简化和自然视觉任务中可靠地操纵模型行为。观察这些向量之间的几何重叠与特定错误模式的相关性。

Result: 概念向量之间的几何重叠与特定错误模式强相关，提供了一个基于量化的框架来理解内部表征如何塑造模型行为并驱动视觉失败。引导干预能够可靠地操纵模型行为（如强制模型将红花感知为蓝色）。

Conclusion: 该研究提供了一个机制性洞察，通过表征几何分析理解VLMs在多物体视觉任务中的失败模式。概念向量方法为理解内部表征如何驱动模型行为提供了定量框架，有助于解释视觉语言模型的认知约束和错误模式。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [111] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出Fixed-frame Modality Gap Theory精确建模模态间隙几何形状，并基于此开发ReAlign训练免费模态对齐策略和ReVision可扩展MLLM训练范式，利用未配对数据替代昂贵图像-文本对


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习虽然成功对齐视觉和语言表示，但存在模态间隙问题：表达相同语义的不同模态嵌入占据系统偏移区域。现有方法受限于过度简化的各向同性假设，难以应用于大规模场景。

Method: 1. 提出Fixed-frame Modality Gap Theory，将模态间隙分解为稳定偏差和各向异性残差；2. 开发ReAlign训练免费对齐策略，通过Anchor、Trace、Centroid Alignment三步将文本表示对齐到图像表示分布；3. 提出ReVision可扩展MLLM训练范式，将ReAlign集成到预训练阶段，从未配对文本中学习视觉表示分布。

Result: 框架证明统计对齐的未配对数据可以有效替代昂贵的图像-文本对，为MLLM高效扩展提供了稳健路径。

Conclusion: 通过精确建模模态间隙几何形状并利用未配对数据，可以实现多模态大语言模型的高效可扩展训练，减少对大规模高质量图像-文本对的依赖。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [112] [Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine](https://arxiv.org/abs/2602.07064)
*Minghao Han,Dingkang Yang,Yue Jiang,Yizhou Liu,Lihua Zhang*

Main category: cs.CV

Relevance: 85.0

TL;DR: OmniFysics是一个紧凑的全模态模型，通过物理数据引擎注入显式物理知识，统一理解图像、音频、视频和文本，并集成语音和图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有全模态模型在物理理解方面存在脆弱性，因为关键物理属性在视觉上是模糊的，且在网络规模数据中稀疏表示。需要注入显式物理知识来增强模型的物理理解能力。

Method: 1) 构建物理数据引擎：FysicsAny通过层次检索将显著对象映射到已验证的物理属性，生成物理基础的指令-图像监督；FysicsOmniCap通过音频-视觉一致性过滤蒸馏网络视频，生成高质量视频-指令对。2) 分阶段多模态对齐和指令调优训练。3) 使用潜在空间流匹配进行文本到图像生成。4) 采用意图路由器仅在需要时激活生成。

Result: 在标准多模态基准测试中表现有竞争力，在面向物理的评估中结果有所改善。

Conclusion: 通过显式注入物理知识，OmniFysics增强了全模态模型的物理理解能力，在保持标准多模态任务性能的同时，在物理相关任务上表现更好。

Abstract: Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.

</details>


### [113] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

Relevance: 85.0

TL;DR: PI3D是一种针对多模态大语言模型在3D环境中的提示注入攻击，通过放置带有文本的物理物体来覆盖模型的原始任务，而非数字图像编辑。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在3D环境（如机器人、对话代理）中的应用，通过物理物体放置进行提示注入成为新的攻击面。现有研究主要关注文本域和2D数字图像的攻击，缺乏对3D物理环境攻击的研究。

Method: PI3D通过优化3D物体姿态（位置和方向）来放置带有注入文本的物理物体，攻击者的目标是诱导MLLM执行注入任务，同时确保物体放置的物理合理性。

Result: 实验表明PI3D对多种MLLM在不同相机轨迹下都有效，现有防御措施不足以抵御这种攻击。

Conclusion: PI3D揭示了3D物理环境中MLLMs的新安全漏洞，需要开发新的防御机制来应对这种物理世界的提示注入攻击。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [114] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

Relevance: 85.0

TL;DR: LUCID提出了一种统一的视觉-语言稀疏自编码器，学习图像块和文本标记的共享潜在字典，通过最优传输匹配实现特征对齐，无需标注即可获得可解释的共享特征。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器（SAEs）按模态单独训练，产生的特征字典不可直接理解，且解释无法跨域迁移。需要一种统一的方法来学习可解释的跨模态概念表示。

Method: LUCID采用统一的视觉-语言稀疏自编码器架构，学习共享潜在字典用于图像块和文本标记表示，同时保留模态特定的私有容量。通过最优传输匹配目标耦合共享代码实现特征对齐，无需标注。

Result: LUCID产生可解释的共享特征，支持补丁级定位，建立跨模态神经元对应关系，增强对相似性评估中概念聚类问题的鲁棒性。共享特征捕获超越对象的多样化语义类别，包括动作、属性和抽象概念。

Conclusion: LUCID提供了一种全面的可解释多模态表示方法，通过统一的稀疏自编码器学习跨模态共享概念，为可解释AI和跨模态分析提供了新工具。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [115] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出一个3B参数的视觉-语言-动作（VLA）智能体，用于语言驱动的物体导航，通过显式的图像基础推理直接回答目标物体识别和动作选择问题，替代传统的端到端模型或多模型流水线。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：端到端训练模型难以泛化且缺乏动作级可解释性；模块化零样本流水线存在错误传播、计算成本高、难以将推理整合到导航策略中的问题。需要一种更高效、可解释且泛化能力强的导航方法。

Method: 设计一个紧凑的3B参数VLA智能体，采用显式的图像基础推理，通过"思考"、"思考总结"和"动作"三阶段推理过程，直接回答目标物体识别和动作选择问题，实现类人的具身推理。

Result: 该方法在可解释性、泛化能力和导航效率方面均有提升，相比传统方法具有更好的性能表现。

Conclusion: 提出的VLA智能体通过显式推理机制，在语言驱动物体导航任务中实现了更好的可解释性、更强的泛化能力和更高的导航效率，为具身智能提供了新的解决方案。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [116] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

Relevance: 85.0

TL;DR: ViCA提出了一种高效的MLLM架构，通过稀疏交叉注意力在少数层处理视觉信息，大幅降低计算开销，同时保持98%的基线性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在每一层都处理视觉和文本token，计算开销大。研究发现视觉嵌入已与语言空间对齐，且有效的视觉-语言交互仅发生在少数层。

Method: 提出ViCA架构：视觉token绕过所有自注意力和前馈层，仅通过稀疏交叉注意力在选定层与文本交互，实现最小化视觉处理。

Result: 在3个MLLM骨干、9个多模态基准和26个剪枝基线上的评估显示，ViCA保持98%基线准确率，视觉侧计算降至4%，单批次推理加速3.5倍，多批次加速10倍以上。

Conclusion: ViCA提供了高效、硬件友好的MLLM架构，将视觉处理开销降至接近纯文本LLM的水平，且可与token剪枝方法正交组合获得进一步效率提升。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [117] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

Relevance: 85.0

TL;DR: Fine-R1是一个专门用于细粒度视觉识别的多模态大语言模型，通过R1风格训练框架（思维链监督微调+三元组增强策略优化），仅需4-shot训练就能超越现有通用MLLM和对比CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在粗粒度视觉任务上表现良好，但在细粒度视觉识别（FGVR）上存在困难。适应FGVR需要大量标注数据，成本高昂，且模型容易过拟合到已见子类别，对新子类别泛化能力差。

Method: 采用R1风格训练框架：1) 思维链监督微调：构建高质量FGVR CoT数据集，包含"视觉分析、候选子类别、比较、预测"的推理过程；2) 三元组增强策略优化：通过类内增强（混合同一类别内锚点和正样本图像轨迹）提高对类内方差的鲁棒性，类间增强（最大化跨子类别图像的条件响应差异）增强判别能力。

Result: 仅需4-shot训练，Fine-R1在识别已见和未见子类别上都超越了现有通用MLLM、推理MLLM甚至对比CLIP模型，在知识密集型领域（难以获取所有子类别专家标注）展现出潜力。

Conclusion: Fine-R1通过专门的训练框架有效解决了MLLM在细粒度视觉识别中的挑战，实现了少样本高性能，为难以获取全面标注的知识密集型领域提供了有前景的解决方案。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [118] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出了SPD-Faith Bench基准来评估多模态大语言模型推理过程的忠实性，发现了感知盲区和感知-推理分离两种系统故障模式，并提出了无需训练的SAGE框架来改善视觉证据校准。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链推理被广泛用于提高多模态大语言模型的可解释性，但生成的推理轨迹的忠实性仍不清楚。先前工作主要关注感知幻觉，而推理层面的不忠实性尚未充分探索。需要隔离语言先验来专门评估推理忠实性。

Method: 1) 引入SPD-Faith Bench诊断基准，基于细粒度图像差异推理任务，强制进行显式视觉比较；2) 分析顶级MLLMs的系统故障模式；3) 追踪故障到视觉注意力衰减和残差流中的表示偏移；4) 提出SAGE框架，无需训练即可改善视觉路由并使推理与感知对齐。

Result: 评估揭示了两种系统故障模式：感知盲区（模型无法检测到细微视觉差异）和感知-推理分离（模型感知到差异但推理不一致）。SAGE框架显著提高了视觉证据校准和推理忠实性。

Conclusion: 该研究强调了超越响应正确性来显式评估忠实性的重要性。提出的基准和SAGE框架为理解和改进MLLMs的推理忠实性提供了工具，对可信AI和模型可解释性有重要意义。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [119] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出了一种利用多模态大语言模型（MLLMs）进行视频文本嵌入和检索的新方法，通过中间层分析和轻量级文本对齐策略，无需视觉监督即可实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前将生成式MLLMs适配为视觉任务嵌入提取器的方法在视频任务上表现不如专门的视频基础模型（VFMs）。研究者希望探索如何更好地利用MLLMs进行视频文本嵌入和检索。

Method: 1. 系统性的层间分析，发现MLLMs中间层已编码大量任务相关信息；2. 结合中间层嵌入和校准的MLLM头部实现零样本检索；3. 引入轻量级文本对齐策略，将密集视频描述映射到简短摘要，实现无视觉监督的视频文本嵌入学习。

Result: 方法在常见视频检索基准测试中超越了现有方法，通常有显著优势，达到了最先进的性能，且无需任何视觉监督的微调。

Conclusion: MLLMs的中间层包含丰富的任务相关信息，通过适当的层选择和文本对齐策略，可以在无需视觉监督的情况下实现强大的视频文本检索性能，为MLLMs在视频任务中的应用提供了新思路。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [120] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出了一种新的视觉语言模型安全漏洞：分割图像视觉越狱攻击（SIVA）。研究发现，虽然VLM的预训练和指令调优能很好处理分割图像，但安全对齐通常只在完整图像上进行，导致模型无法识别分布在多个图像片段中的有害语义。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通过RLHF等安全对齐技术对完整图像攻击表现出强鲁棒性，但作者发现安全对齐未考虑有害语义分布在多个图像片段的情况，这构成了新的安全漏洞。

Method: 提出SIVA攻击方法，包含渐进式三个阶段：1) 朴素分割攻击；2) 自适应白盒攻击；3) 黑盒迁移攻击。最强策略采用新颖的对抗知识蒸馏（Adv-KD）算法提升跨模型迁移性。

Result: 在三个最先进的现代VLM和三个越狱数据集上评估，最强攻击比现有基线高出60%的迁移成功率，显著暴露了当前VLM安全对齐的严重漏洞。

Conclusion: 当前VLM安全对齐存在对分割图像输入的脆弱性，需要重新设计安全对齐方法以覆盖这种新型攻击。论文提出了有效的防御方案来应对这一关键漏洞。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [121] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出AVIC框架，通过自适应控制视觉想象来提升多模态大语言模型的空间推理能力，分析何时需要想象、多少想象有益、何时想象有害，实现更高效可靠的推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉空间推理中存在局限性，当正确答案依赖于未见或替代视角时表现不可靠。现有方法通过世界模型增强视觉想象，但何时需要想象、多少想象有益、何时想象有害等问题尚未得到充分理解。不加区分的想象会增加计算成本，甚至因引入误导性证据而降低性能。

Method: 提出AVIC自适应测试时框架，包含世界模型，在空间推理前显式评估当前视觉证据的充分性，然后选择性地调用和缩放视觉想象。框架分析静态视觉证据何时足够、何时需要想象增强，以及过度或不必要的想象如何影响准确性和效率。

Result: 在空间推理基准（SAT、MMSI）和具身导航基准（R2R）上，结果揭示了想象关键、边际或有害的明确场景。选择性控制策略能够匹配或超越固定想象策略，同时显著减少世界模型调用和语言标记数量。

Conclusion: 研究强调了分析和控制测试时想象对于高效可靠空间推理的重要性。自适应想象控制能够优化计算资源使用，避免不必要的想象带来的性能下降，为MLLMs的空间推理能力提升提供了新方向。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [122] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

Relevance: 85.0

TL;DR: Octopus框架通过重组现有rollouts合成密集自校正示例，解决VLMs中自校正学习信号稀疏问题，训练出具有可控自校正能力的Octopus-8B模型，在7个基准测试中达到开源VLMs的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以学习视觉语言模型中的自校正能力，因为有效的自校正行为出现频率低，导致学习信号极其稀疏，阻碍了复杂推理问题的解决。

Method: 提出correction-specific rollouts (Octopus)框架：1）通过重组现有rollouts合成密集自校正示例，提高样本效率；2）引入响应掩码策略，将自校正与直接推理解耦，避免信号冲突；3）基于此训练Octopus-8B模型。

Result: Octopus-8B在7个基准测试中达到开源视觉语言模型的SOTA性能，比最佳RLVR基线提高1.0分，同时每个训练步骤仅需0.72倍的时间。

Conclusion: Octopus框架有效解决了VLMs中自校正学习的稀疏信号问题，通过rollout重组和响应掩码策略实现了高效稳定的强化学习优化，训练出具有可控自校正能力的先进模型。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [123] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文研究Vision Transformers中[CLS]类标记与补丁标记之间的特征学习冲突，提出专门化处理路径来分离这两类标记的计算流，显著提升密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers通常将可学习的[CLS]类标记与补丁标记一起处理，但这两类标记本质不同：类标记用于全局特征学习，补丁标记用于局部特征学习。现有方法将它们统一处理，可能导致特征学习冲突。论文旨在研究不同预训练策略下全局与局部特征学习之间的摩擦，并提出解决方案。

Method: 通过分析类标记与补丁标记之间的交互，发现标准归一化层引入了这两类标记的隐式区分。基于此洞察，提出专门化处理路径，在归一化层和早期QKV投影中分离类标记和补丁标记的计算流。具体包括：1）为两类标记设计专门的归一化层；2）在早期注意力层中分离QKV投影路径。

Result: 在标准基准测试中，分割性能提升超过2 mIoU点，同时保持强分类准确率。提出的修改仅增加8%的参数，无额外计算开销。通过全面消融实验，展示了哪些架构组件从专门化中获益最大，以及该方法在不同模型规模和学框架中的泛化能力。

Conclusion: Vision Transformers中类标记与补丁标记的统一处理限制了密集预测任务的性能。通过专门化处理路径分离这两类标记的计算流，特别是归一化层和早期QKV投影，可以显著提升补丁表示质量，同时保持分类性能。该方法高效且可扩展。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [124] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

Relevance: 85.0

TL;DR: OneVision-Encoder提出视觉智能本质是压缩问题，通过Codec Patchification仅处理信号熵丰富的区域（3.1%-25%），结合3D RoPE和大规模聚类判别目标，在减少视觉token和预训练数据的情况下，在16个图像/视频/文档理解基准上超越Qwen3-ViT和SigLIP2。


<details>
  <summary>Details</summary>
Motivation: 现代视觉架构偏离了信息论原则：视觉信号高度冗余，而判别信息稀疏。当前模型均匀处理密集像素网格，浪费大量计算在静态背景上，而不是聚焦于定义运动和意义的预测残差。要解决视觉理解问题，必须使架构与视频的信息论原则（即编解码器原理）对齐。

Method: 采用Codec Patchification，仅处理信号熵丰富的区域（3.1%-25%）。使用共享3D RoPE统一空间和时间推理，通过超过100万个语义概念的大规模聚类判别目标进行训练，联合捕捉对象持久性和运动动态。

Result: 在集成到LLM后，在16个图像、视频和文档理解基准上一致超越Qwen3-ViT和SigLIP2等强视觉骨干，尽管使用更少的视觉token和预训练数据。在视频理解任务上，平均比Qwen3-ViT提升4.1%。

Conclusion: 效率和准确性不是权衡关系，而是正相关的。Codec对齐的patch级稀疏性是基础原则，使OV-Encoder成为下一代视觉通用模型的可扩展引擎。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [125] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出了首个视觉语言模型（VLM）适应过程的机制分析，通过阶段式模型差分技术揭示了语言模型如何学习"看"的能力，识别了视觉偏好特征、空间关系编码机制及其对应的注意力头。


<details>
  <summary>Details</summary>
Motivation: 尽管当前视觉语言模型在各种任务上表现出色，但语言主干网络在跨模态训练中如何适应以及视觉特定能力何时出现仍然不清楚。需要理解预训练语言模型如何获得视觉基础能力的内在机制。

Method: 采用阶段式模型差分技术，该技术能够分离多模态微调过程中引入的表征变化。通过识别视觉偏好特征、分析空间关系编码，并追踪这些特征到特定注意力头的因果激活。

Result: 研究发现：1）识别了在微调过程中出现或重新定向的视觉偏好特征；2）这些特征的一个选择性子集可靠地编码空间关系；3）这些特征的因果激活可追溯到一小部分注意力头。

Conclusion: 阶段式模型差分揭示了空间基础多模态特征何时何地出现，提供了更清晰的模态融合视图，展示了视觉基础如何重塑原本仅用于文本的特征。该方法增强了多模态训练的可解释性。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [126] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

Relevance: 85.0

TL;DR: FlattenGPT通过扁平化相邻Transformer块来压缩模型深度，同时保留所有块学到的知识，在保持90-96%零样本性能的同时实现20%的压缩比，优于现有剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度压缩方法（整块剪枝）会丢弃重要信息导致性能显著下降，而通道剪枝虽然能更好保持性能但无法减少模型深度且面临各层剪枝比例不一致的挑战。需要一种既能压缩深度又能有效保留知识的方法。

Method: FlattenGPT通过将两个相邻Transformer块扁平化为一个块来压缩网络深度，同时实现更有效的参数冗余检测和移除。该方法保持原始Transformer架构一致性，并保留所有块学到的知识。

Result: 在LLaMA-2/3和Qwen-1.5等模型上，FlattenGPT在20%压缩比下保持90-96%的零样本性能，在WikiText-2困惑度上优于现有剪枝方法，同时加速LLM推理。

Conclusion: FlattenGPT提供了一种有效的Transformer深度压缩方法，在性能和效率之间取得良好平衡，有望增强Transformer的效率。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [127] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

Relevance: 75.0

TL;DR: XAI-CLIP：一种基于多模态视觉语言模型的ROI引导扰动框架，用于生成更清晰、边界感知的显著性图，显著提升医学图像分割的可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床工作流中至关重要，但基于Transformer的模型虽然性能优越，其有限的可解释性阻碍了临床信任和部署。现有XAI技术（如基于梯度的显著性方法和基于扰动的方法）通常计算成本高、需要多次前向传播，且常产生噪声或解剖学上不相关的解释。

Method: 提出XAI-CLIP框架，利用多模态视觉语言模型嵌入来定位临床相关的解剖区域并指导解释过程。该方法整合语言引导的区域定位与医学图像分割，应用有针对性的区域感知扰动，生成更清晰、边界感知的显著性图。

Result: 在FLARE22和CHAOS数据集上的实验表明，XAI-CLIP相比传统扰动方法：运行时减少高达60%，Dice分数提升44.6%，基于遮挡的解释的IoU提升96.7%。定性结果也显示更干净、解剖学更一致的归因图，伪影更少。

Conclusion: 将多模态视觉语言表示整合到基于扰动的XAI框架中，显著提升了医学图像分割系统的可解释性和效率，为实现透明且可临床部署的系统提供了可能。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [128] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

Relevance: 75.0

TL;DR: FCL提出了一种基于公平性约束的测试时适应框架，通过解耦增强探索和公平校准来缓解共享证据偏差，避免熵最小化带来的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的测试时适应方法主要依赖熵最小化，但在类别共享视觉特征时容易放大虚假相关性并导致过自信的错误。需要一种能显式处理共享证据偏差的方法来提升视觉语言模型在分布偏移下的鲁棒性。

Method: FCL采用情节式测试时适应框架，基于加性证据分解假设，将适应过程解耦为：(1) 基于增强的探索来识别合理的类别候选；(2) 公平驱动的校准，通过调整文本上下文来平等化对常见视觉证据的敏感性，避免熵最小化。

Result: 通过广泛的评估验证了理论动机，FCL在多种领域偏移和细粒度基准测试中，相对于最先进的测试时适应方法取得了有竞争力的适应性能。

Conclusion: FCL通过公平性约束有效缓解了部分特征固着问题，实现了无需熵最小化的文本嵌入校准，为视觉语言模型的测试时适应提供了新的解决方案。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [129] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

Relevance: 75.0

TL;DR: FADE是一种用于文本到图像扩散模型的快速数据擦除方法，通过参数定位和自蒸馏实现高效的概念遗忘，同时保持模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据保护法规和负责任AI实践的要求，需要从训练模型中移除特定数据或概念的影响。然而，在文本到图像扩散模型中实现遗忘仍然具有挑战性，因为计算成本高且难以平衡有效遗忘与保留无关概念。

Method: FADE采用两阶段方法：1）使用基于梯度的显著性识别对遗忘集最负责的参数，并通过稀疏LoRA适配器约束更新；2）应用自蒸馏目标，用用户定义的替代概念覆盖遗忘概念，同时保留在保留数据上的行为。

Result: 在UnlearnCanvas基准测试和多个数据集（Imagenette、LFW、狗品种数据集、SUN Attributes）上的实验表明，FADE实现了最先进的遗忘性能，在遗忘-保留权衡上具有细粒度控制。

Conclusion: FADE通过轻量级、可逆的适配器实现了强大的概念擦除和高保留性，适用于扩散基图像生成模型的选择性遗忘，为生产系统提供了灵活的部署方案。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [130] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

Relevance: 75.0

TL;DR: DMD加速范式存在Forbidden Zone问题，AMD通过奖励代理检测并逃离这些区域，动态调整梯度并引入排斥景观锐化，显著提升生成质量和训练鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Distribution Matching Distillation (DMD) 是一种强大的加速范式，但在Forbidden Zone区域存在稳定性问题，这些区域中真实教师提供不可靠指导而虚假教师施加的排斥力不足，限制了性能提升。

Method: 提出Adaptive Matching Distillation (AMD)：1) 使用奖励代理显式检测和逃离Forbidden Zone；2) 通过结构信号分解动态优先化校正梯度；3) 引入Repulsive Landscape Sharpening强制陡峭的能量壁垒防止失败模式崩溃。

Result: 在图像和视频生成任务（SDXL, Wan2.1）和严格基准（VBench, GenEval）上，AMD显著提升样本保真度和训练鲁棒性。例如，AMD将SDXL的HPSv2分数从30.64提升到31.25，优于现有最佳基线。

Conclusion: 显式修正Forbidden Zone内的优化轨迹对于推动少步生成模型的性能上限至关重要，AMD为DMD提供了更稳定和高效的优化框架。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [131] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出SpatialReward奖励模型，通过空间推理解决图像编辑RL中的"注意力崩溃"问题，在多个基准上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在复杂图像编辑中面临奖励信号稀缺问题，现有评估器存在"注意力崩溃"问题——忽视跨图像比较和细粒度细节，导致感知不准确和分数校准错误

Method: 提出SpatialReward奖励模型，通过显式空间推理进行精确验证，将推理锚定到预测的编辑区域，使语义判断基于像素级证据。使用包含26万空间感知数据的数据集进行训练

Result: 在MMRB2和EditReward-Bench上达到SOTA性能，在提出的MultiEditReward-Bench上超越专有评估器。在在线RL中，将OmniGen2在GEdit-Bench上提升+0.90分，超过领先的判别模型，是GPT-4.1增益的两倍

Conclusion: 空间推理对于实现图像编辑中的有效对齐至关重要，SpatialReward通过像素级证据的语义判断显著提升了评估准确性

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [132] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出LGDEA方法，利用LLM从放射报告中提取关键诊断证据，构建共享诊断证据空间，实现证据级别的跨模态对齐，减少对配对数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法存在两个问题：全局对齐容易被非诊断信息主导，局部对齐无法整合关键诊断证据。这导致难以学习可靠的诊断表示，限制了在配对数据有限的医学场景中的应用。

Method: LGDEA方法：1) 利用LLM从放射报告中提取关键诊断证据；2) 构建共享诊断证据空间；3) 实现证据感知的跨模态对齐；4) 能够有效利用丰富的未配对医学图像和报告。

Result: 在短语定位、图像-文本检索和零样本分类任务上取得一致且显著的改进，甚至能与依赖大量配对数据的预训练方法相媲美。

Conclusion: 通过LLM引导的诊断证据对齐，能够更符合医学诊断过程，显著减少对配对数据的依赖，在医学视觉-语言预训练中取得优异性能。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [133] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

Relevance: 75.0

TL;DR: SIGMA是一个统一的后训练框架，使扩散Transformer能够处理交错的多条件生成，通过引入选择性多属性token来组合多种视觉条件。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型如Bagel虽然能通过配对的图像编辑数据对齐多个视觉任务，但仅限于单条件输入，缺乏从多个异构源合成结果的灵活性。需要支持交错多条件生成的框架。

Method: 提出SIGMA框架，引入选择性多属性token（风格、内容、主体、身份等），使模型能够解释和组合交错文本-图像序列中的多个视觉条件。在Bagel统一骨干网络上使用70万个交错示例进行后训练。

Result: SIGMA支持组合编辑、选择性属性转移和细粒度多模态对齐。实验表明，SIGMA在多样化的编辑和生成任务中提高了可控性、跨条件一致性和视觉质量，在组合任务上相比Bagel有显著提升。

Conclusion: SIGMA框架成功扩展了扩散Transformer的能力，使其能够处理交错的多条件生成，为更灵活和可控的视觉生成任务提供了有效解决方案。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [134] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文提出了一个系统化的视频生成模型后训练框架，将监督策略塑造、奖励驱动的强化学习和基于偏好的精炼整合到单一稳定性约束优化栈中，旨在提升生成视频的感知保真度、时间连贯性和提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 后训练是将预训练视频生成器转换为生产导向模型的关键步骤，需要解决视频生成的实际约束：高计算成本、时间累积的失败模式、以及异质、不确定且通常弱区分的反馈信号。

Method: 设计了一个三阶段优化框架：1) 监督策略塑造，2) 奖励驱动的强化学习，3) 基于偏好的精炼。框架采用诊断驱动的方法，将优化视为分阶段过程而非孤立技巧的集合，并围绕稳定性约束进行设计。

Result: 该框架提供了清晰的蓝图，用于构建可扩展的后训练流程，能够在保持初始化时建立的可控性的同时，提升感知保真度、时间连贯性和提示遵循能力，确保在真实部署环境中的稳定性、可扩展性和有效性。

Conclusion: 通过系统化的后训练框架，可以将预训练视频生成器有效转化为生产就绪的模型，解决视频生成特有的挑战，为实际应用提供稳定、可扩展的优化方案。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [135] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文研究了视觉语言模型在自动驾驶安全评估和决策中的应用，探索了三种系统级用例：基于CLIP的语义危险筛查、场景级VLM嵌入在轨迹规划中的集成，以及自然语言作为运动规划的行为约束。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型能够将视觉观察与自然语言概念对齐，为安全关键自动驾驶中的语义推理提供了新机会。研究旨在探索VLM表示如何支持驾驶场景安全评估和决策制定。

Method: 1) 基于CLIP图像-文本相似性的轻量级类别无关危险筛查方法；2) 将场景级VLM嵌入集成到基于Transformer的轨迹规划框架中；3) 使用自然语言作为运动规划的显式行为约束。

Result: 1) CLIP方法能低延迟检测多样化和分布外道路危险；2) 简单将全局VLM嵌入用于规划器不会提高轨迹精度；3) 基于视觉场景元素的自然语言指令能抑制严重规划失败并改善安全对齐行为。

Conclusion: 视觉语言表示在表达语义风险、意图和行为约束方面对自动驾驶安全具有重要潜力，但实现这一潜力需要精心设计的系统架构和结构化基础，而非简单的特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [136] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

Relevance: 75.0

TL;DR: VideoTemp-o3：一个统一的代理式视频思考框架，联合建模视频定位和问答，解决长视频理解中的关键帧采样问题


<details>
  <summary>Details</summary>
Motivation: 传统均匀帧采样在长视频理解中无法捕捉关键视觉证据，导致性能下降和幻觉增加。现有的代理式视频思考方法效率低下、定位能力弱且流程僵化。

Method: 提出VideoTemp-o3统一框架，联合建模视频定位和问答。在监督微调阶段设计统一掩码机制，在强化学习中引入专用奖励防止奖励攻击。同时构建高质量长视频定位QA数据和相应基准。

Result: 实验结果表明，该方法在长视频理解和定位任务上都取得了显著性能提升。

Conclusion: VideoTemp-o3通过统一的代理式思考框架，有效解决了长视频理解中的定位和问答问题，具有强大的定位能力和灵活的剪辑支持。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [137] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

Relevance: 75.0

TL;DR: 首个大规模跨范式面部年龄估计基准测试显示，零样本视觉语言模型（VLMs）显著优于大多数专用模型，平均MAE为5.65年（专用模型为9.88年），在年龄验证任务中表现更可靠。


<details>
  <summary>Details</summary>
Motivation: 面部年龄估计对内容审核、年龄验证和深度伪造检测至关重要，但缺乏系统比较现代视觉语言模型与专用年龄估计架构的基准测试。

Method: 构建首个大规模跨范式基准，评估34个模型（22个专用架构和12个通用VLMs），在8个标准数据集上测试，总计每个模型1100张测试图像，进行MAE、年龄验证阈值、年龄分组等分析。

Result: 零样本VLMs显著优于大多数专用模型（平均MAE 5.65年 vs 9.88年），最佳VLM（Gemini 3 Flash Preview，MAE 4.32）比最佳非LLM模型（MiVOLO，MAE 5.10）优15%。VLMs在年龄验证任务中假成人率更低（13-25% vs 60-100%）。

Conclusion: 任务特定架构对年龄估计并非必需，领域应转向将VLM能力蒸馏到高效专用模型中。所有模型在极端年龄（<5岁和65+）表现最差。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [138] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

Relevance: 75.0

TL;DR: SAGE提出了一种从原始视频流中可扩展适应几何基础模型的方法，通过分层挖掘管道将视频转换为训练轨迹，结合稀疏几何锚定和密集可微一致性监督，显著提升了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建方面有潜力，但受到大规模3D标注数据稀缺的限制。互联网视频提供了几乎无限的原始数据，但由于缺乏真实几何信息和观测噪声，难以直接用于几何学习。

Method: SAGE采用分层挖掘管道：1) 信息性训练轨迹选择；2) 通过SfM点云进行稀疏几何锚定，提供全局结构指导；3) 通过3D高斯渲染实现密集可微一致性，提供多视角约束。为防止灾难性遗忘，引入了基于锚定数据的正则化策略。

Result: SAGE在未见过的基准测试（7Scenes、TUM-RGBD、Matterport3D）上显著提升了零样本泛化能力，将Chamfer距离降低了20-42%，优于现有最先进基线方法。

Conclusion: SAGE开创了通过互联网视频适应几何基础模型的先河，为通用3D学习建立了可扩展的范式，解决了3D标注数据稀缺的问题。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [139] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: TLQ：针对视觉语言模型的token级重要性感知层量化框架，通过梯度引导的token重要性整合机制和多GPU层校准方案，显著提升量化性能


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉token和文本token在激活分布和量化误差敏感性方面存在显著差异，传统后训练量化校准方法难以有效处理这种异构性，导致量化性能下降

Method: 1) 基于梯度信息设计token级重要性整合机制来量化误差；2) 构建token级校准集实现细粒度校准；3) 引入多GPU、暴露量化的层校准方案，保持校准与真实量化推理路径一致

Result: 在两个模型、三种规模、两种量化设置下均实现性能提升，表现出强大的量化稳定性，同时减少对A100大内存GPU的依赖

Conclusion: TLQ框架通过细粒度的token级重要性感知校准，有效解决了视觉语言模型中的异构token量化挑战，为大规模视觉语言模型部署提供了实用的量化解决方案

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [140] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: D-ORCA是一个面向对话的音频-视觉大语言模型，专注于视频中的说话人识别、语音识别和时间定位，通过创新的强化学习奖励函数在双语数据集上训练，在多项任务上超越现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 视频中的对话是重要信息来源，但准确识别"谁在何时说了什么"对于深度视频理解至关重要。目前缺乏高质量的多方对话视频数据集和专门针对对话理解的音频-视觉模型。

Method: 1) 构建DVD双语数据集（近4万训练+2千评估视频）；2) 采用分组相对策略优化，引入三个新颖奖励函数：说话人归属准确性、全局语音内容准确性、句子级时间边界对齐；3) 基于评估指标设计强化学习目标。

Result: D-ORCA在说话人识别、语音识别和时间定位方面显著优于现有开源模型。尽管只有80亿参数，但在多个通用音频-视觉理解基准上与Qwen3-Omni表现相当。

Conclusion: D-ORCA通过对话中心的架构设计和创新的强化学习奖励函数，为音频-视觉理解提供了强大的解决方案，填补了开源生态中的关键空白。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [141] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

Relevance: 75.0

TL;DR: FlashVID是一个训练免费的VLLM推理加速框架，通过注意力与多样性令牌选择和树状时空令牌合并技术，在保留10%视觉令牌的情况下达到99.1%的原模型性能，实现10倍视频帧输入扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型处理大量视觉令牌导致计算效率低下，现有加速框架独立压缩时空冗余而忽略了时空关系，导致次优压缩。视频的动态特性使得视觉特征在时空位置、尺度、方向等属性上随时间变化。

Method: 提出FlashVID训练免费推理加速框架：1) 注意力与多样性令牌选择(ADTS)选择最具代表性的令牌进行基础视频表示；2) 树状时空令牌合并(TSTM)进行细粒度时空冗余消除。

Result: 在三个代表性VLLM和五个视频理解基准上的实验表明方法的有效性和泛化性。仅保留10%视觉令牌时，FlashVID保留LLaVA-OneVision 99.1%性能，使Qwen2.5-VL视频帧输入增加10倍，相同计算预算下相对提升8.6%。

Conclusion: FlashVID可作为训练免费即插即用模块扩展长视频帧处理能力，显著提升VLLM的计算效率和视频理解性能。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [142] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

Relevance: 75.0

TL;DR: ViT-5是对Vision Transformer的现代化改造，通过整合过去5年的架构进展，在保持Attention-FFN核心结构的同时，对归一化、激活函数、位置编码等组件进行系统优化，在图像分类和生成任务上均超越现有ViT变体。


<details>
  <summary>Details</summary>
Motivation: 尽管Vision Transformer取得了成功，但原始架构在过去5年中未充分利用最新的架构创新。研究旨在通过系统整合现代架构组件，提升ViT的性能和效率，为视觉基础模型提供更强大的骨干网络。

Method: 采用组件级精细化设计方法，在保持经典Attention-FFN结构的基础上，对归一化层、激活函数、位置编码、门控机制和可学习token等关键组件进行现代化更新，形成ViT-5架构。

Result: ViT-5在ImageNet-1k分类上达到84.2% top-1准确率（超过DeiT-III的83.8%），在SiT扩散框架中实现1.84 FID（优于原始ViT的2.06），在理解和生成任务上均表现出色，并展现出更好的表示学习和空间推理能力。

Conclusion: ViT-5通过系统整合现代架构组件，为Vision Transformer提供了简单有效的升级方案，在保持兼容性的同时显著提升性能，适合作为2020年代中期视觉基础模型的骨干网络。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [143] [MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection](https://arxiv.org/abs/2602.08126)
*Venkatraman Narayanan,Bala Sai,Rahul Ahuja,Pratik Likhar,Varun Ravi Kumar,Senthil Yogamani*

Main category: cs.CV

Relevance: 75.0

TL;DR: MambaFusion：基于选择性状态空间模型（SSM）和窗口Transformer的多模态3D检测框架，用于自动驾驶，实现高效、自适应和物理基础的感知


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的可靠3D物体检测至关重要，但现有基于BEV的多模态融合方法存在以下问题：上下文建模效率低、空间不变融合、不确定性下推理困难。相机提供密集视觉线索但深度信息不准确，LiDAR提供精确3D结构但覆盖稀疏。

Method: 1. 交替使用选择性状态空间模型（SSM）和窗口Transformer，以线性时间传播全局上下文同时保持局部几何保真度；2. 多模态token对齐模块和可靠性感知融合门，基于空间置信度和校准一致性动态重新加权相机-LiDAR特征；3. 结构条件扩散头，集成基于图的推理和不确定性感知去噪，强制物理合理性和校准置信度。

Result: 在nuScenes基准测试中建立了新的最先进性能，同时保持线性时间复杂性。框架展示了SSM效率与可靠性驱动融合的结合，为实际自动驾驶系统提供鲁棒、时间稳定且可解释的3D感知。

Conclusion: MambaFusion通过将SSM效率与可靠性驱动融合相结合，实现了鲁棒、时间稳定且可解释的3D感知，为实际自动驾驶系统提供了有效的解决方案。

Abstract: Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>


### [144] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出了一种名为Chain-of-Caption的训练免费框架，通过结合多种视觉和文本上下文信息，显著提升了多模态大语言模型在指代表达理解任务上的性能，在多个基准数据集上获得了5%到30%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在指代表达理解任务上已经取得了较高准确率，但现有方法通常通过扩大模型规模和训练数据来提升性能。本文旨在探索通过工具使用提供额外视觉和文本上下文的方法，分析这些技术对REC任务的影响，并提出无需训练即可提升性能的框架。

Method: 提出了Chain-of-Caption训练免费框架，通过工具使用为MLLM提供多种视觉和文本上下文信息。该方法结合了多种上下文信息，包括视觉和文本模态的补充信息，无需对模型进行微调即可提升REC性能。

Result: 在RefCOCO、RefCOCOg、RefCOCO+和Ref-L4数据集上的实验表明，单独的文本或视觉上下文都能提升REC性能。通过结合多种上下文，该框架在不同IoU阈值下的准确率比基线模型提升了5%到30%。

Conclusion: 通过工具使用提供额外的视觉和文本上下文信息可以有效提升多模态大语言模型在指代表达理解任务上的性能，提出的Chain-of-Caption框架在无需训练的情况下实现了显著的性能提升。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [145] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出了首个专门针对"图像思维"范式的过程奖励模型(PRMs)评估基准，通过分析推理轨迹定义了7种细粒度错误类型，构建了包含1,206条人工标注轨迹的基准，发现当前LVLMs作为PRMs存在视觉推理过程评估能力不足、性能差异大、正向评估偏见等问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型(LVLMs)的发展，"图像思维"范式使模型能够动态编辑和重新编码视觉信息，类似于人类视觉处理。然而，这种范式在推理过程中可能产生各种错误，需要过程奖励模型(PRMs)来区分正负推理步骤。现有PRMs基准主要是文本中心的，缺乏针对这一范式的全面评估。

Method: 1) 通过广泛分析推理轨迹和PRMs引导搜索实验，定义了7种细粒度错误类型；2) 构建了包含1,206条人工标注的"图像思维"推理轨迹的基准，涵盖4个类别和16个子类别；3) 对当前LVLMs作为PRMs的能力进行了实验分析。

Result: 实验分析表明：当前LVLMs作为PRMs效果不佳，在视觉推理过程评估方面能力有限，不同错误类型间性能差异显著，存在正向评估偏见，且对推理步骤位置敏感。这些发现证明了所提基准的有效性。

Conclusion: 本文提出的基准填补了"图像思维"范式下PRMs评估的空白，为LVLMs中PRMs的进一步发展奠定了重要基础。研究揭示了当前LVLMs作为PRMs的局限性，指出了未来改进的方向。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [146] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出了首个电商短视频理解基准E-VAds，包含3,961个高质量视频和19,785个问答对，并开发了基于强化学习的推理模型E-VAds-R1，在商业意图推理上获得109.2%性能提升。


<details>
  <summary>Details</summary>
Motivation: 电商短视频作为在线视频行业的高收入细分领域，具有目标驱动格式和密集多模态信号的特点。现有模型在处理这类视频时存在困难，因为现有基准主要关注通用任务，忽略了商业意图推理。

Method: 1) 提出多模态信息密度评估框架量化电商内容复杂度；2) 构建E-VAds基准，包含3,961个淘宝高质量视频，使用多智能体系统生成19,785个开放式问答对，组织为感知与认知推理两个维度五个任务；3) 开发E-VAds-R1模型，采用基于强化学习的多粒度奖励设计MG-GRPO。

Result: 评估显示电商内容相比主流数据集在视觉、音频和文本模态上具有显著更高的信息密度。E-VAds-R1仅用数百个训练样本就在商业意图推理上实现了109.2%的性能提升。

Conclusion: 该工作填补了电商短视频理解领域的空白，提出的基准和模型为商业意图推理提供了有效的解决方案，展示了强化学习在多模态视频理解中的潜力。

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [147] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出了Demo驱动的视频上下文学习任务和Demo-ICL-Bench基准，用于评估多模态大语言模型从少量示例中学习动态视频内容的能力，并开发了Demo-ICL模型来应对这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估模型基于静态内部知识的能力，而非从动态、新颖上下文中学习和适应的能力。需要填补这一空白，评估模型从少量示例中学习视频内容的能力。

Method: 1) 提出Demo驱动的视频上下文学习任务；2) 构建Demo-ICL-Bench基准，包含1200个教学YouTube视频及相关问题，提供文本和视频两种演示；3) 开发Demo-ICL模型，采用两阶段训练策略：视频监督微调和信息辅助直接偏好优化。

Result: 实验表明Demo-ICL-Bench具有挑战性，现有最先进MLLM难以应对；Demo-ICL模型在该基准上表现有效，展示了从上下文示例中学习的能力。

Conclusion: 该研究填补了视频上下文学习评估的空白，提出的基准和模型为未来研究提供了方向，强调了模型从动态视频内容中学习和适应的重要性。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [148] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: Vista是一个用于流式视频问答的场景感知框架，通过动态场景分割、压缩和召回机制，实现高效的长视频流推理。


<details>
  <summary>Details</summary>
Motivation: 流式视频问答对多模态大语言模型提出独特挑战：视频帧顺序到达，用户查询可在任意时间点发出。现有基于固定大小内存或简单压缩的方法常导致上下文丢失或内存溢出，限制了在长视频、实时场景中的有效性。

Method: Vista包含三个创新方面：1) 场景感知分割：动态聚类输入帧为时空和视觉连贯的场景单元；2) 场景感知压缩：将每个场景压缩为紧凑token表示存储在GPU内存中，全分辨率帧卸载到CPU内存；3) 场景感知召回：查询时选择性召回相关场景并重新集成到模型输入中。

Result: 在StreamingBench上的广泛实验表明，Vista实现了最先进的性能，为现实世界流式视频理解建立了强基线。

Conclusion: Vista是一个模型无关的框架，可无缝集成多种视觉语言骨干模型，在不影响延迟或内存效率的情况下实现长上下文推理，为流式视频问答提供了高效可扩展的解决方案。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [149] [Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?](https://arxiv.org/abs/2602.08505)
*Caterina Fuster-Barceló,Virginie Uhlmann*

Main category: cs.CV

Relevance: 75.0

TL;DR: 研究评估了视觉基础模型（DINOv2、DINOv3、OpenCLIP）在电子显微镜线粒体分割任务中的跨域迁移能力，发现单域训练效果好但多域训练性能严重下降，当前参数高效微调策略不足以解决异构数据集间的域不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉基础模型越来越多地用于生物医学图像分析，但尚不清楚其潜在表示是否足够通用，能够有效支持跨异构显微镜图像数据集的迁移和重用。本研究针对电子显微镜图像中的线粒体分割问题，探究这一问题。

Method: 使用两个公共EM数据集（Lucchi++和VNC）和三个代表性视觉基础模型（DINOv2、DINOv3、OpenCLIP）。评估两种模型适应机制：1）冻结主干仅训练轻量分割头；2）通过LoRA进行参数高效微调。使用多种技术（PCA、Fréchet Dinov2距离、线性探针）探索潜在表示空间。

Result: 所有主干网络在单EM数据集上训练都能获得良好的分割性能（前景IoU），LoRA能持续提升域内性能。但在多个EM数据集上训练会导致所有模型性能严重下降，PEFT仅带来边际改善。潜在表示空间分析显示两个EM数据集间存在显著且持续的域不匹配，尽管它们视觉相似。

Conclusion: 视觉基础模型在轻量适应下能在单个域内为EM分割提供有竞争力的结果，但当前的PEFT策略不足以获得跨异构EM数据集的单一鲁棒模型，需要额外的域对齐机制。

Abstract: Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>


### [150] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

Relevance: 75.0

TL;DR: GeoFocus是一个解决几何问题的多模态模型框架，通过关键局部感知器和VertexLang拓扑语言，在多个几何数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决对大型多模态模型（LMMs）仍然是一个重大挑战，需要全局形状识别和对几何理论相关局部关系的关注。现有方法在局部特征覆盖和全局拓扑表示方面存在不足。

Method: 提出GeoFocus框架，包含两个核心模块：1) 关键局部感知器，通过13个基于理论的感知模板自动识别和强调关键局部结构；2) VertexLang，一个紧凑的拓扑形式语言，通过顶点坐标和连接关系编码全局图形。

Result: 在Geo3K、GeoQA和FormalGeo7K数据集上，GeoFocus比领先的专业模型准确率提升4.7%；关键局部特征覆盖提升61%；全局感知训练时间减少20%；在MATHVERSE中表现出更好的鲁棒性。

Conclusion: GeoFocus通过结合局部感知和全局拓扑表示，显著提升了LMMs在几何问题解决上的性能，为多模态几何推理提供了有效框架。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [151] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

Relevance: 75.0

TL;DR: SemiNFT是一个基于扩散Transformer的彩色修图框架，通过模仿人类艺术训练轨迹（从刚性模仿到直觉创作），结合配对数据学习和无配对数据强化学习，实现语义感知的色彩预设迁移。


<details>
  <summary>Details</summary>
Motivation: 当前基于参考的图像色彩修图方法主要依赖像素级统计的全局色彩映射，缺乏对语义上下文和人类美学的真正理解。作者希望开发一个能够像人类艺术家一样理解美学并智能进行色彩修图的系统。

Method: 提出SemiNFT框架：1）首先使用配对三元组（源图像、参考图像、目标图像）进行监督学习，获得基本的结构保持和色彩映射技能；2）然后在无配对数据上进行强化学习，培养细腻的美学感知；3）设计了混合在线-离线奖励机制，在美学探索中锚定结构保持，防止灾难性遗忘。

Result: 在标准预设迁移基准测试中超越了现有最先进方法，并在零样本任务（如黑白照片着色和跨域预设迁移）中表现出显著智能，证实了系统超越了简单的统计匹配，达到了复杂的美学理解水平。

Conclusion: SemiNFT通过模仿人类艺术训练轨迹，结合监督学习和强化学习，实现了语义感知的色彩修图，为视觉内容创作提供了更智能的工具。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [152] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: LV-RAE是一种表示自编码器，通过增强语义特征的低级信息来提升潜在扩散模型的重建保真度，同时通过解码器鲁棒性微调和潜在平滑来改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型作为图像编码器虽然能提升潜在扩散模型的生成性能，但其语义特征缺乏低级信息（如颜色和纹理），导致重建保真度下降，成为扩展LDMs的主要瓶颈。

Method: 提出LV-RAE表示自编码器，增强语义特征的低级信息；分析发现信息丰富的潜在表示使解码器对潜在扰动敏感，因此通过微调解码器增加鲁棒性，并通过受控噪声注入平滑生成潜在。

Result: LV-RAE显著提高了重建保真度，同时保持了语义抽象，实现了强大的生成质量。

Conclusion: 通过增强语义特征的低级信息和提高解码器鲁棒性，可以有效解决潜在扩散模型在重建保真度和生成质量方面的瓶颈问题。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [153] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出了Omni Dense Captioning任务，用于生成带时间戳的连续、细粒度结构化音视频叙事，构建了OmniDCBench基准和SodaM评估指标，并开发了TimeChat-Captioner-7B模型，在多个下游任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述任务通常生成简短、概括性的描述，缺乏连续、细粒度的结构化叙事。本文旨在解决这一问题，通过生成类似电影剧本的详细音视频叙事，使读者能够逐场景想象视频内容。

Method: 1) 提出六维结构模式创建"脚本式"描述；2) 构建高质量人工标注基准OmniDCBench；3) 提出SodaM评估指标，解决场景边界模糊问题；4) 构建训练数据集TimeChatCap-42K；5) 开发TimeChat-Captioner-7B模型，使用SFT和GRPO训练。

Result: TimeChat-Captioner-7B在密集描述任务上超越Gemini-2.5-Pro达到SOTA，生成的密集描述显著提升了音视频推理（DailyOmni和WorldSense）和时间定位（Charades-STA）等下游任务的能力。

Conclusion: Omni Dense Captioning任务和配套的基准、评估指标、数据集及模型为音视频理解提供了新方向，TimeChat-Captioner-7B展示了在密集描述和相关下游任务上的强大能力。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [154] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

Relevance: 75.0

TL;DR: HATCH是一个训练框架，通过补丁级空间对齐和"先行动后回答"推理，提升多模态大语言模型在多图像空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在单图像空间推理方面取得进展，但在需要整合多个视角信息的多图像空间推理方面仍面临挑战。认知研究表明人类通过跨视角对应和逐步视角变换两种机制解决此类任务，但现有研究仅部分且隐式地融入这些机制。

Method: 提出HATCH训练框架，包含两个互补目标：1) 补丁级空间对齐，鼓励不同视角中空间对应区域的补丁表示对齐；2) 先行动后回答推理，要求模型在预测最终答案前生成明确的视角转换动作。

Result: 在三个基准测试中，HATCH始终以明显优势超越同等规模基线模型，并与更大模型竞争，同时保持单图像推理能力。

Conclusion: HATCH通过显式监督跨视角对应和视角变换机制，有效提升了多模态大语言模型的多图像空间推理能力。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [155] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出Instance-Disentangled Attention机制，解决现有基于流的图像编辑方法在多实例编辑中的局限性，实现单次推理中对多个实例的独立编辑而不产生语义干扰。


<details>
  <summary>Details</summary>
Motivation: 现有基于流匹配的图像编辑方法主要支持全局或单指令编辑，在多实例场景中表现不佳，即当需要对参考输入的多个部分进行独立编辑时，会因全局条件化的速度场和联合注意力机制导致编辑相互干扰。

Method: 引入Instance-Disentangled Attention机制，通过分割联合注意力操作，在速度场估计过程中强制建立实例特定文本指令与空间区域之间的绑定关系，从而实现编辑的解耦。

Result: 在自然图像编辑和新提出的文本密集信息图区域级编辑基准测试中，该方法能促进编辑解耦和局部性，同时保持全局输出一致性，实现单次推理的实例级编辑。

Conclusion: Instance-Disentangled Attention机制有效解决了多实例编辑中的语义干扰问题，为基于流的图像编辑提供了更强大的多实例编辑能力。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [156] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: Omni-Video 2是一个14B参数的可扩展视频生成编辑模型，通过连接预训练多模态大语言模型（MLLMs）和视频扩散模型，实现统一的视频生成和编辑。利用MLLMs的理解推理能力生成目标描述，通过轻量适配器注入多模态条件，在保持参数效率的同时实现高质量视频处理。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成和编辑模型在处理复杂组合指令时存在局限性，难以准确理解用户意图并生成高质量视频。需要开发能够统一处理视频生成和多种编辑任务的模型，同时保持计算效率和可扩展性。

Method: 1. 利用预训练MLLMs的理解推理能力，将用户指令转换为明确的目标描述；2. 开发轻量适配器，将多模态条件标记注入预训练文本到视频扩散模型；3. 在精心策划的训练数据上扩展到14B参数规模；4. 支持文本到视频生成和多种编辑任务（对象移除、添加、背景更改、复杂运动编辑等）。

Result: 在FiVE基准测试（细粒度视频编辑）和VBench基准测试（文本到视频生成）上表现出色：1. 在视频编辑中能够更好地遵循复杂组合指令；2. 在视频生成任务中达到竞争性或更优的质量。

Conclusion: Omni-Video 2通过连接MLLMs和视频扩散模型，实现了高质量、可扩展的视频生成和编辑，在复杂指令理解和生成质量方面表现优异，为统一视频处理提供了有效解决方案。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [157] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

Relevance: 75.0

TL;DR: TiFRe是一个文本引导的视频帧缩减框架，通过智能选择关键帧并合并非关键帧信息来降低视频多模态大语言模型的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 视频多模态大语言模型在处理大量视频帧时面临高计算成本问题，特别是注意力计算开销大。简单降低帧率会丢失非关键帧的重要信息，导致性能下降。

Method: 提出TiFRe框架：1) 文本引导帧采样(TFS)：基于用户输入生成CLIP风格提示，用预训练CLIP编码器计算语义相似度选择关键帧；2) 帧匹配与合并(FMM)：将非关键帧信息整合到关键帧中，减少信息损失。

Result: 实验表明TiFRe能有效降低计算成本，同时在视频语言任务上提升性能。

Conclusion: TiFRe通过智能帧选择和信息保留机制，在减少计算开销的同时保持了视频多模态大语言模型的性能，为解决视频处理效率问题提供了有效方案。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [158] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

Relevance: 75.0

TL;DR: ArcFlow：一种基于非线性流轨迹的扩散模型蒸馏框架，通过动量过程参数化实现连续轨迹近似，仅需2步推理即可达到40倍加速，质量损失极小


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法通常使用线性捷径近似教师轨迹，难以匹配随时间步变化的切线方向，导致质量下降。需要一种能更好捕捉速度演变的非线性轨迹近似方法。

Method: 提出ArcFlow框架，将推理轨迹的底层速度场参数化为连续动量过程的混合，从而捕捉速度演变并外推连续速度形成非线性轨迹。该参数化允许对非线性轨迹进行解析积分，避免数值离散误差。通过轻量适配器在预训练教师模型上进行轨迹蒸馏训练。

Result: 在Qwen-Image-20B和FLUX.1-dev等大规模模型上，仅微调不到5%的原始参数，实现2步推理下40倍加速，且无明显质量下降。基准测试显示ArcFlow在定性和定量评估上均有效。

Conclusion: ArcFlow通过非线性流轨迹近似教师轨迹，解决了现有线性近似方法的局限性，实现了高效且高质量的扩散模型蒸馏，为大规模扩散模型的实际部署提供了可行方案。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [159] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

Relevance: 75.0

TL;DR: WorldCompass是一个用于长时程交互式视频世界模型的RL后训练框架，通过clip级rollout策略、互补奖励函数和高效RL算法，提升世界模型基于交互信号的探索准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有长时程交互式视频世界模型在探索世界时缺乏准确性和一致性，需要一种能够基于交互信号有效"引导"世界模型探索的后训练框架。

Method: 提出三个核心创新：1) clip级rollout策略 - 在单个目标clip生成和评估多个样本，提高rollout效率；2) 互补奖励函数 - 设计交互跟随准确性和视觉质量的奖励函数；3) 高效RL算法 - 采用负感知微调策略结合多种效率优化。

Result: 在SoTA开源世界模型WorldPlay上的评估表明，WorldCompass在各种场景下显著提升了交互准确性和视觉保真度。

Conclusion: WorldCompass为长时程交互式视频世界模型提供了一个有效的RL后训练框架，能够显著提升模型基于交互信号的探索能力。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [160] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

Relevance: 75.0

TL;DR: BAR（masked Bit AutoRegressive modeling）通过比特级自回归建模，让离散分词器在视觉生成中达到或超越连续方法的性能，关键在于增加码本大小和压缩比。


<details>
  <summary>Details</summary>
Motivation: 挑战视觉生成中连续管道的统治地位，研究离散与连续方法的性能差距。发现差距主要源于潜在空间的总比特分配（压缩比），而非离散分词器本身劣势。

Method: 提出masked Bit AutoRegressive modeling（BAR），一个支持任意码本大小的可扩展框架。通过为自回归Transformer配备掩码比特建模头，逐步生成离散标记的组成比特。

Result: BAR在ImageNet-256上达到新的最先进gFID 0.99，超越了连续和离散范式的领先方法，同时显著降低采样成本，收敛速度比先前连续方法更快。

Conclusion: 离散分词器通过增加码本大小可以匹配或超越连续方法，BAR框架实现了这一潜力，为视觉生成提供了更高效、可扩展的离散解决方案。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


### [161] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

Relevance: 65.0

TL;DR: MAU-Set是一个全面的工业异常理解数据集，涵盖多个工业领域和分层任务结构，并提出了MAU-GPT多模态大模型，通过AMoE-LoRA机制统一异常感知和通用专家适应，在工业异常检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着工业制造规模化，自动化细粒度产品图像分析对质量控制变得至关重要。现有方法受限于数据集覆盖范围有限和模型对多样化复杂异常模式的泛化能力差。

Method: 1) 引入MAU-Set数据集，涵盖多个工业领域，具有从二分类到复杂推理的分层任务结构；2) 建立严格的评估协议；3) 提出MAU-GPT多模态大模型，采用新颖的AMoE-LoRA机制统一异常感知和通用专家适应。

Result: 大量实验表明，MAU-GPT在所有领域都持续优于先前的最先进方法，展示了可扩展和自动化工业检测的强大潜力。

Conclusion: MAU-Set数据集和MAU-GPT模型为解决工业异常理解挑战提供了全面解决方案，通过多模态大模型和专门适应机制显著提升了工业检测性能。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [162] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

Relevance: 65.0

TL;DR: Vectra：首个无参考、MLLM驱动的电商IIMT视觉质量评估框架，包含多维评分系统、大规模数据集和4B参数模型，在人类排名相关性上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有电商IIMT研究主要关注机器翻译评估，而视觉渲染质量对用户参与度至关重要。当前基于参考的方法（如SSIM、FID）缺乏可解释性，而模型作为评判者的方法缺乏领域基础、细粒度的奖励信号。

Method: 提出Vectra框架：1) Vectra Score：将视觉质量分解为14个可解释维度，引入空间感知的缺陷面积比量化；2) Vectra Dataset：从110万真实产品图像构建，包含2K基准测试集、30K推理标注和3.5K专家偏好标注；3) Vectra Model：4B参数MLLM，能生成量化评分和诊断推理。

Result: Vectra在人类排名相关性上达到最先进水平，其模型在评分性能上超过包括GPT-5和Gemini-3在内的领先MLLMs。

Conclusion: Vectra填补了电商IIMT视觉质量评估的空白，提供了可解释、细粒度的评估框架，数据集和模型将在接受后发布。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [163] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

Relevance: 65.0

TL;DR: UNIKIE-BENCH是一个统一基准，用于评估大型多模态模型在文档关键信息提取任务上的能力，包含约束类别和开放类别两个评估轨道。


<details>
  <summary>Details</summary>
Motivation: 现实世界文档的关键信息提取面临布局结构、视觉质量和任务特定需求的巨大差异。现有大型多模态模型在端到端KIE方面显示出潜力，但缺乏全面系统的评估基准来覆盖多样化的实际应用场景。

Method: 提出UNIKIE-BENCH统一基准，包含两个互补轨道：1) 约束类别KIE轨道，基于场景预定义模式反映实际应用需求；2) 开放类别KIE轨道，提取文档中明确存在的任何关键信息。在15个最先进的LMM上进行实验评估。

Result: 实验显示LMM在不同模式定义、长尾关键字段和复杂布局下性能显著下降，不同文档类型和场景间存在明显性能差异。这些发现凸显了LMM在KIE任务中的基础准确性和布局感知推理方面存在持续挑战。

Conclusion: UNIKIE-BENCH为LMM的KIE能力提供了全面评估框架，揭示了当前模型在实际应用中的局限性，特别是在模式适应性、长尾处理和复杂布局理解方面需要进一步改进。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [164] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

Relevance: 65.0

TL;DR: 首个专门针对遥感复杂推理的基准VLRS-Bench，包含2000个QA对，覆盖认知、决策、预测三个维度，揭示现有MLLMs在遥感推理任务上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准主要偏向感知任务（如目标识别、场景分类），限制了MLLMs在认知密集型遥感应用中的发展。需要专门的复杂推理基准来推动遥感领域多模态推理能力的发展。

Method: 提出VLRS-Bench基准，包含2000个问答对，平均长度71词，覆盖14个任务和最多8个时间阶段。通过专门构建流程整合遥感先验知识和专家知识，确保地理空间真实性和推理复杂性。

Result: 实验结果显示现有最先进的MLLMs在VLRS-Bench上存在显著瓶颈，为遥感社区推进多模态推理提供了关键见解。

Conclusion: VLRS-Bench是首个专门针对遥感复杂推理的基准，填补了现有基准的空白，为评估和提升MLLMs在遥感领域的推理能力提供了重要工具。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [165] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

Relevance: 65.0

TL;DR: 视频编码器中的物理表示研究：发现物理变量在中间层突然变得可访问（物理涌现区），物理表示采用分布式而非因子化方式，运动方向通过高维环形几何结构编码。


<details>
  <summary>Details</summary>
Motivation: 研究视频模型是否依赖物理变量的因子化表示来做出准确物理预测，还是采用任务特定的分布式表示。现代视频世界模型在直觉物理基准上表现良好，但其内部表示机制尚不清楚。

Method: 使用分层探测、子空间几何、补丁级解码和针对性注意力消融等方法，分析大规模视频编码器内部的物理表示，识别物理信息可访问的位置和组织方式。

Result: 发现跨架构的中间深度过渡区（物理涌现区），物理变量在此变得可访问；标量物理量（速度、加速度）从早期层即可获得，而运动方向仅在物理涌现区变得可访问；方向通过具有环形几何结构的高维群体结构编码。

Conclusion: 现代视频模型不使用经典物理引擎那样的物理变量因子化表示，而是采用分布式表示，这种表示方式足以进行物理预测。

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [166] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出Neural Sentinel，一個基於視覺語言模型(VLM)的統一車牌識別系統，使用微調的PaliGemma 3B模型，透過單次前向傳播同時完成車牌識別、狀態分類和車輛屬性提取，並引入人機協同持續學習框架。


<details>
  <summary>Details</summary>
Motivation: 傳統ALPR系統採用多階段流水線（目標檢測+OCR），存在錯誤累積、延遲高和架構複雜等問題。需要一個統一的端到端解決方案來提升準確性和效率。

Method: 使用PaliGemma 3B視覺語言模型，透過LoRA進行微調，實現單次前向傳播的多任務處理。引入人機協同持續學習框架，採用70:30的原始訓練數據與修正樣本比例，防止災難性遺忘。

Result: 車牌識別準確率達92.3%，比EasyOCR提升14.1%，比PaddleOCR提升9.9%。平均推理延遲152ms，預期校準誤差0.048。零樣本泛化能力：車輛顏色檢測89%、安全帶檢測82%、乘員計數78%。

Conclusion: 統一視覺語言方法代表ALPR系統的範式轉移，提供更優準確性、更低架構複雜度，以及傳統流水線方法無法實現的新興多任務能力。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [167] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出MosaicThinker技术，通过构建全局语义地图增强设备端小视觉语言模型在跨帧空间推理任务中的能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理方面能力较弱，特别是在涉及多帧复杂空间关系的任务中，缺乏3D空间知识，而设备端AI需要处理机器人操作和动作规划等高级任务

Method: 提出推理时计算技术MosaicThinker，将多帧碎片化空间信息整合为统一的全局语义地图表示，通过视觉提示引导VLM在语义地图上进行空间推理

Result: 实验结果显示该技术能显著提升资源受限设备上跨帧空间推理的准确性，适用于多种类型和复杂度的推理任务

Conclusion: MosaicThinker技术有效解决了设备端小VLM在跨帧空间推理中的局限性，为设备端具身AI提供了实用的空间推理增强方案

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [168] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

Relevance: 65.0

TL;DR: WorldEdit：一个基于世界知识驱动的图像编辑数据集，专门处理隐含编辑指令，通过因果逻辑指导的图像编辑样本提升模型在复杂世界知识推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在处理显式指令（如属性操作、风格迁移）方面表现出色，但在处理隐含编辑指令时面临挑战。隐含指令描述视觉变化的原因而不明确说明结果，需要复杂的世界知识和推理能力，而现有模型缺乏这种能力。

Method: 1. 构建WorldEdit数据集：包含高质量编辑样本，通过符合真实世界因果逻辑的改写指令指导；2. 提供WorldEdit-Test用于评估模型在因果编辑场景的性能；3. 采用两阶段训练框架微调模型（如Bagel），结合因果验证奖励。

Result: 提出的数据集和方法显著缩小了与GPT-4o和Nano-Banana的差距，在指令跟随和知识合理性方面表现出竞争力，而这两个方面通常是开源系统的薄弱环节。

Conclusion: WorldEdit数据集和训练框架有效解决了图像编辑模型处理隐含指令的局限性，通过融入世界知识和因果推理能力，提升了模型在复杂编辑任务中的表现。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [169] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

Relevance: 65.0

TL;DR: Ex-Omni是一个开源的全模态框架，通过解耦语义推理与时间生成，利用语音单元作为时间支架和统一的token-as-query门控融合机制，为OLLMs添加语音伴随的3D面部动画功能。


<details>
  <summary>Details</summary>
Motivation: 当前的全模态大语言模型(OLLMs)旨在统一多模态理解与生成，但结合语音与3D面部动画的研究仍较少，而这对于自然交互至关重要。主要挑战在于LLMs的离散token级语义推理与3D面部运动所需的密集细粒度时间动态之间存在表示不匹配，使得在有限数据下直接建模难以优化。

Method: 提出Ex-Omni框架，通过解耦语义推理与时间生成来降低学习难度：1) 利用语音单元作为时间支架；2) 使用统一的token-as-query门控融合(TQGF)机制进行受控语义注入。同时引入InstructEx数据集来支持训练。

Result: 大量实验表明，Ex-Omni在保持与现有开源OLLMs竞争力的同时，能够实现稳定对齐的语音和面部动画生成。

Conclusion: Ex-Omni成功解决了OLLMs中语音与3D面部动画融合的表示不匹配问题，通过解耦策略和TQGF机制实现了有效的多模态生成。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [170] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

Relevance: 65.0

TL;DR: 本文提出FSSDINO，一个基于冻结DINOv3特征的免训练少样本语义分割基线方法，通过类别原型和Gram矩阵优化，在多个基准测试中表现出色。研究发现"最安全vs最优"困境：最后一层特征虽稳定但非最优，传统启发式方法无法可靠识别高质量中间层特征。


<details>
  <summary>Details</summary>
Motivation: 研究自监督Vision Transformers（如DINOv3）在少样本语义分割任务中的内在能力，探索冻结特征的有效性，避免复杂解码器或测试时适应，建立简单而强大的基线。

Method: 提出FSSDINO方法：使用冻结的DINOv3特征，通过类别特定原型和Gram矩阵细化进行少样本语义分割。采用Oracle引导的层分析，比较不同中间层特征性能，揭示传统启发式选择方法的局限性。

Result: FSSDINO在二元、多类和跨域少样本分割基准测试中与复杂方法竞争。发现最后一层特征虽稳定但非最优，存在"语义选择鸿沟"：传统指标无法识别最优中间层特征，Oracle分析显示更高性能可达。

Conclusion: 最后一层特征作为少样本分割基线具有欺骗性强度，但DINOv3中存在更高性能的潜在语义表示。传统特征选择启发式方法在基础模型中存在局限性，需要更可靠的特征选择机制。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [171] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

Relevance: 65.0

TL;DR: AD-MIR是一个两阶段框架，通过结构化记忆构建和结构化推理代理来解码广告视频意图，在AdsQA基准上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有代理在广告视频理解方面存在认知鸿沟，难以将像素级感知与高级营销逻辑联系起来。广告视频的多模态理解需要解释视觉叙事与抽象说服策略之间的复杂关系。

Method: 1. 结构感知记忆构建阶段：将原始视频转换为结构化数据库，整合语义检索和精确关键词匹配，优先考虑细粒度品牌细节，动态过滤无关背景噪声。2. 结构化推理代理：模拟营销专家通过迭代查询循环分解叙事，推断隐含说服策略，采用基于证据的自我校正机制，严格验证洞察与具体视频帧的一致性。

Result: 在AdsQA基准测试中，AD-MIR超越了最强的通用代理DVD，严格准确率提升1.8%，宽松准确率提升9.5%，达到最先进性能。

Conclusion: 有效的广告理解需要将抽象营销策略明确地基于像素级证据，AD-MIR通过结构化记忆和推理框架成功弥合了感知与逻辑之间的认知鸿沟。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [172] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出了视频思维过程推理框架，通过将视频推理分解为可验证的步骤序列，提高事实正确性和时间定位能力，同时提供可解释的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 视频理解需要处理长且嘈杂的观察数据，进行时间定位的多步推理。现有方法在事实正确性和时间定位方面存在不足，缺乏可解释的推理过程。

Method: 提出思维过程推理框架，包含三个交替步骤：时间证据选择、逐步状态更新、约束答案合成。框架与模型无关，可集成到现有视觉语言骨干中，支持闭卷推理和工具增强推理。

Result: 在标准视频推理任务上的实验表明，PoT框架持续提高了事实正确性和时间定位能力，减少幻觉解释，提供可解释的推理轨迹用于诊断和下游应用。

Conclusion: PoT框架通过结构化、可验证的推理步骤，显著提升了视频推理的性能和可解释性，为复杂视频理解任务提供了有效的解决方案。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [173] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文对16种最先进的AI生成图像检测方法进行了首次全面的零样本评估，涵盖23个预训练检测器变体和12个数据集（260万张图像，291个生成器）。研究发现：无通用最佳检测器，性能差异显著（37个百分点差距），训练数据对齐对泛化影响巨大，现代商业生成器能击败大多数检测器（准确率仅18-30%）。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在数字平台激增，可靠的检测方法对于打击错误信息和维护内容真实性变得至关重要。现有基准主要评估微调模型，但忽略了零样本性能评估——这是实践中最常见的部署场景，存在关键的研究空白。

Method: 对16种最先进的检测方法（23个预训练检测器变体）在12个多样化数据集上进行系统性零样本评估，涵盖260万张图像样本和291个独特生成器（包括现代扩散模型）。使用统计分析方法（Friedman检验、Spearman相关性等）评估性能差异和稳定性。

Result: 1) 无通用最佳检测器，检测器排名极不稳定（Spearman ρ: 0.01-0.87）；2) 最佳与最差检测器性能差距达37个百分点（75.0% vs 37.5%平均准确率）；3) 训练数据对齐对泛化影响巨大，相同架构检测器性能差异达20-60%；4) 现代商业生成器（Flux Dev, Firefly v4, Midjourney v7）能击败大多数检测器，平均准确率仅18-30%；5) 识别出三种影响跨数据集泛化的系统性失败模式。

Conclusion: 研究挑战了"一刀切"的检测器范式，表明实践者必须根据具体威胁环境仔细选择检测器，而不能依赖已发布的基准性能。提供了可操作的部署指南，强调训练数据对齐和特定场景适配的重要性。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [174] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

Relevance: 65.0

TL;DR: ViewRope：一种几何感知的视频Transformer编码方法，通过将相机射线方向注入自注意力层，解决了世界模型中的空间持久性问题，显著提升了3D一致性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前预测性世界模型缺乏空间持久性，在长轨迹中无法维持稳定的场景结构，当相机重新访问先前观察的位置时经常产生幻觉细节。这种几何漂移源于对屏幕空间位置嵌入的依赖，这与3D一致性所需的投影几何相冲突。

Method: 1) ViewRope：几何感知编码，将相机射线方向直接注入视频Transformer自注意力层，通过相对射线几何而非像素局部性参数化注意力；2) 几何感知帧稀疏注意力：利用几何线索选择性关注相关历史帧；3) ViewBench：诊断套件，测量闭环保真度和几何漂移。

Result: ViewRope显著改善了长期一致性，同时降低了计算成本。该方法在保持3D一致性方面表现优异，特别是在长轨迹和相机重新访问场景时。

Conclusion: 通过几何感知的注意力机制，可以解决世界模型中的空间持久性问题。ViewRope为视频Transformer提供了模型原生的归纳偏置，使其能够跨时间间隔检索3D一致的内容，为交互式AI的发展提供了重要基础。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [175] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

Relevance: 65.0

TL;DR: SSI-Bench是一个用于评估视觉-语言模型空间推理能力的VQA基准测试，专注于受约束流形上的空间推理，包含1000个排序问题，涵盖几何和拓扑推理，需要复杂的空间操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM基准测试大多评估非约束场景，模型可以利用2D捷径，缺乏对真实物理世界中受几何、拓扑和物理约束的空间推理能力的评估。需要构建一个能真正测试模型在受约束3D结构中进行空间推理能力的基准。

Method: 通过完全人工中心化的流程构建SSI-Bench：10名研究人员花费400多小时精心挑选图像、标注结构组件、设计问题以最小化像素级线索。基准包含1000个排序问题，涵盖几何和拓扑推理，需要心理旋转、横截面推断、遮挡推理、力路径推理等空间操作。

Result: 评估31个广泛使用的VLM显示与人类存在巨大差距：最佳开源模型准确率22.2%，最强闭源模型33.6%，而人类得分91.6%。鼓励模型思考仅带来边际收益，错误分析指向结构基础和约束一致的3D推理失败。

Conclusion: 当前VLM在受约束流形上的空间推理能力严重不足，与人类表现差距巨大。模型在结构基础和约束一致的3D推理方面存在根本性缺陷，需要新的方法来解决这些挑战。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [176] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: EasyTune提出了一种高效的扩散模型对齐方法，通过分步微调解决传统方法中的递归依赖问题，显著降低内存消耗并提升训练速度


<details>
  <summary>Details</summary>
Motivation: 现有基于可微分奖励的扩散模型对齐方法存在两个主要问题：(1) 低效且粗粒度的优化过程，(2) 高内存消耗。这些问题源于去噪轨迹中不同步骤之间的递归依赖关系

Method: 1. 理论分析识别递归依赖是问题的根源
2. 提出EasyTune：在去噪过程的每个步骤分别微调扩散模型，解耦递归依赖
3. 引入自精炼偏好学习机制，动态识别偏好对并进行偏好学习，解决运动奖励模型训练数据稀缺问题

Result: 在运动生成任务中，EasyTune相比DRaFT-50在对齐指标上提升8.2%，内存开销仅需31.16%，训练速度提升7.3倍

Conclusion: EasyTune通过解耦去噪步骤的递归依赖，实现了高效、细粒度且内存友好的扩散模型对齐，为解决运动生成中的下游目标对齐问题提供了有效方案

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [177] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

Relevance: 65.0

TL;DR: MCIE-E1是一种基于多模态大语言模型的复杂指令图像编辑方法，通过空间感知交叉注意力和背景一致交叉注意力模块，解决现有方法在复杂指令遵循和背景一致性方面的不足，并在新基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法仅限于相对简单的编辑操作，难以处理现实应用中需要的复杂组合指令。主要挑战包括指令遵循不足和背景不一致问题。

Method: 提出MCIE-E1方法，包含两个关键模块：1) 空间感知交叉注意力模块，在去噪过程中通过空间指导显式对齐语义指令与空间区域；2) 背景一致交叉注意力模块，保持未编辑区域的特征以维持背景一致性。同时构建专门的数据管道，结合强大的MLLM进行细粒度自动过滤和人工验证。

Result: 在提出的CIE-Bench基准上，MCIE-E1在定量和定性评估中均优于先前最先进方法，指令遵循能力提升23.96%。

Conclusion: 通过架构设计、数据和评估协议的综合改进，MCIE-E1显著提升了复杂指令图像编辑的能力，为现实应用提供了更强大的解决方案。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [178] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: MIND是首个开放域闭环重访基准，用于评估世界模型的记忆一致性和动作控制能力，包含250个高质量视频和多种动作空间设计


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的基准来评估世界模型在动态视觉环境中的基本能力，特别是记忆一致性和动作控制方面

Method: 构建包含250个1080p/24FPS视频的基准，包括第一人称和第三人称视角，设计高效评估框架测量记忆一致性和动作控制能力，并引入MIND-World作为基线模型

Result: 实验验证了MIND基准的完整性，揭示了当前世界模型在保持长期记忆一致性和跨动作空间泛化方面的关键挑战

Conclusion: MIND为世界模型评估提供了标准化基准，有助于推动该领域发展，特别是在记忆一致性和动作控制能力方面

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [179] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

Relevance: 65.0

TL;DR: ReRoPE是一个即插即用的框架，通过将相对相机姿态信息注入到预训练视频扩散模型的RoPE嵌入中，实现可控视角的视频生成，无需重新训练或修改架构。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用相对于固定参考帧（如第一帧）的相机姿态编码，缺乏平移不变性，导致泛化能力差和累积漂移问题。相对相机姿态嵌入虽然更鲁棒，但难以集成到预训练视频扩散模型中而不产生高昂训练成本或架构修改。

Method: 基于RoPE在现有模型中未充分利用其全频谱带宽（特别是低频分量）的洞察，ReRoPE将相对相机姿态信息无缝注入这些未充分利用的频带中，实现精确的相机控制同时保留预训练生成先验。

Result: 在图像到视频（I2V）和视频到视频（V2V）任务上评估，ReRoPE在相机控制精度和视觉保真度方面表现优异，提供了训练高效的可控高保真视频生成路径。

Conclusion: ReRoPE为可控视频生成提供了一个即插即用的高效解决方案，通过巧妙利用RoPE的频谱特性，在不损害预训练模型能力的前提下实现了精确的相机视角控制。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [180] [Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval](https://arxiv.org/abs/2602.08224)
*Jing Zhang,Zhikai Li,Xuewen Liu,Qingyi Gu*

Main category: cs.CV

Relevance: 65.0

TL;DR: Efficient-SAM2通过稀疏感知机制加速SAM2视频分割，利用对象感知的稀疏窗口路由和稀疏记忆检索，在保持精度的同时实现1.68倍加速


<details>
  <summary>Details</summary>
Motivation: SAM2在视频对象分割中表现出色，但计算负担重阻碍实时应用。现有方法主要关注重新训练轻量级骨干网络，缺乏对训练后加速的探索。作者观察到SAM2具有类似生物视觉的稀疏感知模式，这为消除冗余计算提供了机会。

Method: 提出Efficient-SAM2，包含两个核心组件：1) 对象感知稀疏窗口路由(SWR)：利用前一帧解码器的连贯性和显著性线索，将背景区域路由到轻量级快捷分支；2) 对象感知稀疏记忆检索(SMR)：只允许每帧中显著的内存标记参与计算，显著性模式从首次记忆时复用。

Result: 在SAM2.1-L模型上实现1.68倍加速，在SA-V测试集上仅损失1.0%的精度。方法仅增加可忽略的额外参数和最小训练开销。

Conclusion: Efficient-SAM2通过利用SAM2的稀疏感知特性，自适应地聚焦对象区域并消除任务无关计算，显著提高了推理效率，为实时视频处理应用提供了可行的解决方案。

Abstract: Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>


### [181] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出语言引导的运动离散化方法LG-Tok，通过Transformer架构实现语言与运动的对齐，在减少token数量的同时保持高质量重建，显著提升运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统运动离散化方法通常通过增加token数量来提高重建质量，但这会增加生成模型的复杂度。需要一种既能保持高质量重建又能降低生成复杂度的tokenization方法。

Method: 提出语言引导的tokenization（LG-Tok），利用自然语言与运动在tokenization阶段对齐，产生紧凑的高层语义表示。采用基于Transformer的Tokenizer，利用注意力机制实现语言与运动的有效对齐。设计语言丢弃方案，在训练时随机移除语言条件，使detokenizer支持无语言引导的生成。

Result: 在HumanML3D和Motion-X生成基准测试中，LG-Tok获得Top-1分数0.542和0.582，优于SOTA方法（MARDM: 0.500和0.528）；FID分数分别为0.057和0.088，优于0.114和0.147。LG-Tok-mini仅使用一半token仍保持竞争力（Top-1: 0.521/0.588, FID: 0.085/0.071）。

Conclusion: 语言引导的tokenization方法能产生紧凑的语义表示，在减少token数量的同时保持高质量重建，简化生成模型学习，显著提升运动生成性能。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [182] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

Relevance: 65.0

TL;DR: ALIVE是一个音频-视频生成模型，通过适配预训练的文本到视频模型，实现了Sora风格的音频视频生成和动画功能，在音频视频同步和参考动画方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 视频生成正快速向统一的音频-视频生成演进，现有文本到视频模型缺乏音频生成和动画能力，需要开发能够同时处理音频和视频的生成模型。

Method: 在MMDiT架构基础上增加联合音频-视频分支，包含时间对齐的跨模态融合(TA-CrossAttn)和精确音频-视频对齐(UniTemp-RoPE)，设计全面的数据管道收集高质量微调数据，并进行百万级数据的持续预训练和微调。

Result: ALIVE在性能上持续超越开源模型，匹配或超越最先进的商业解决方案，在音频-视频同步和参考动画方面表现优异。

Conclusion: ALIVE通过创新的架构设计和数据管道，成功实现了高质量的音频-视频生成，为社区开发音频-视频生成模型提供了详细方案和基准。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [183] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

Relevance: 65.0

TL;DR: MOVA是一个开源的32B参数MoE架构音频视频生成模型，支持图像-文本到视频-音频的联合生成，包括唇语同步、环境音效和内容对齐音乐


<details>
  <summary>Details</summary>
Motivation: 当前音频视频生成主要依赖级联管道，导致成本高、误差累积和质量下降。现有系统如Veo 3和Sora 2强调同时生成但闭源，限制了领域进展。需要开源解决方案来推动音频视频联合生成研究

Method: 采用混合专家(MoE)架构，总计32B参数（推理时激活18B），支持IT2VA（图像-文本到视频-音频）生成任务。模型能够生成同步的音频视频内容，包括唇语同步语音、环境感知音效和内容对齐音乐

Result: 开发出高质量、同步的音频视频生成模型，开源了模型权重和代码，包含高效推理、LoRA微调和提示增强的全面支持

Conclusion: MOVA作为开源音频视频联合生成模型，填补了领域空白，旨在推动研究和培养创作者社区。其MoE架构和全面工具支持为音频视频生成研究提供了重要基础

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [184] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

Relevance: 65.0

TL;DR: VideoVeritas：一个结合细粒度感知和事实推理的视频伪造检测框架，通过联合偏好对齐和感知预训练强化学习提升检测性能，并引入MintVid数据集进行鲁棒评估。


<details>
  <summary>Details</summary>
Motivation: 视频生成能力的提升带来了安全风险，需要可靠的检测方法。当前多模态大语言模型（MLLMs）虽然推理能力强，但细粒度感知能力有限，导致现有方法要么偏向表面推理，要么偏向机械分析。

Method: 提出VideoVeritas框架，包含：1）联合偏好对齐和感知预训练强化学习（PPRL），通过时空定位和自监督物体计数等通用感知任务增强检测性能；2）引入MintVid数据集，包含9个最先进生成器的3K视频和包含事实错误内容的真实世界子集。

Result: 实验结果表明，现有方法倾向于偏向表面推理或机械分析，而VideoVeritas在多样化基准测试中实现了更平衡的性能。

Conclusion: VideoVeritas通过结合细粒度感知和事实推理，有效提升了视频伪造检测能力，为视频安全提供了更可靠的解决方案。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [185] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

Relevance: 65.0

TL;DR: WorldArena：首个统一评估具身世界模型的基准，同时衡量感知质量和功能效用，发现感知与功能之间存在显著差距


<details>
  <summary>Details</summary>
Motivation: 当前具身世界模型的评估主要关注感知保真度（如视频生成质量），忽视了这些模型在下游决策任务中的功能效用，缺乏统一的评估框架

Method: 提出WorldArena基准，从三个维度评估模型：1）视频感知质量（16个指标，6个子维度）；2）具身任务功能（作为数据引擎、策略评估器和动作规划器）；3）提出EWMScore综合指标整合多维性能

Result: 对14个代表性模型的广泛实验揭示了显著的感知-功能差距：高视觉质量不一定转化为强大的具身任务能力

Conclusion: WorldArena为追踪具身AI中真正功能性世界模型的进展提供了框架，公开排行榜已发布

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [186] [Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study](https://arxiv.org/abs/2602.08996)
*Arushi Rai,Adriana Kovashka*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文提出利用辅助网络数据（比赛视频和教练手册）来改进视频-LLM在体育反馈生成任务上的性能，并设计了针对体育反馈质量的两个新评估指标：特异性和可操作性。


<details>
  <summary>Details</summary>
Motivation: 当前视频-LLM在体育反馈生成任务上表现不佳，需要昂贵且难以收集的微调数据，且泛化能力差。同时，传统文本生成评估指标无法有效评估体育反馈质量。

Method: 1) 使用辅助网络数据增强：利用目标领域的免费网络数据（如比赛视频和教练手册）结合源领域的现有体育反馈数据；2) 提出两个新评估指标：特异性和可操作性，以更好地评估体育反馈质量。

Result: 提出的方法在有限标注条件下实现了更有意义和实用的体育反馈生成，特别是在攀岩作为案例研究中展示了改进效果。

Conclusion: 通过利用辅助网络数据和设计领域特定的评估指标，可以显著提升视频-LLM在体育反馈生成任务上的性能，特别是在数据有限的情况下。

Abstract: While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.

</details>


### [187] [Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss](https://arxiv.org/abs/2602.07022)
*Yucheng Zhou,Hao Li,Jianbing Shen*

Main category: eess.IV

Relevance: 65.0

TL;DR: 该论文对扩散模型和自回归扩散模型进行了理论分析，提出自回归模型通过patch去噪优化能有效缓解条件误差，并引入基于最优传输理论的条件细化方法来解决条件不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究探索了自回归模型用于图像生成，并将扩散模型与自回归框架结合以通过扩散损失优化图像生成。然而，需要理论分析来理解这些模型的优势和局限性，特别是处理条件误差和不一致性的能力。

Method: 1) 对条件扩散模型和自回归扩散模型进行理论比较分析；2) 提出基于最优传输理论的条件细化方法，将条件细化建模为Wasserstein梯度流以确保收敛到理想条件分布。

Result: 实验证明该方法优于传统的扩散模型和带有扩散损失的自回归模型。理论分析表明自回归条件生成能细化条件，使条件误差影响呈指数衰减。

Conclusion: 自回归模型在patch去噪优化方面具有优势，能有效缓解条件误差并稳定条件分布。基于最优传输的条件细化方法能有效解决条件不一致问题，为图像生成模型提供了理论指导和改进方向。

Abstract: Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.

</details>


### [188] [DINO-Mix: Distilling Foundational Knowledge with Cross-Domain CutMix for Semi-supervised Class-imbalanced Medical Image Segmentation](https://arxiv.org/abs/2602.07819)
*Xinyu Liu,Guolei Sun*

Main category: eess.IV

Relevance: 65.0

TL;DR: DINO-Mix：一种面向类别不平衡医学图像分割的半监督学习新框架，通过引入外部视觉基础模型DINOv3作为无偏语义教师，结合渐进式不平衡感知数据增强，打破传统SSL的确认偏见循环。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习框架在医学图像分割中存在"内向性"问题，仅从目标数据集内部循环利用信息和偏见，在类别不平衡情况下会触发确认偏见的恶性循环，导致无法识别少数类别。

Method: 提出多层次"外向型"框架：1) 基础知识蒸馏(FKD)：引入预训练视觉基础模型DINOv3作为无偏外部语义教师，从其高语义独特性的稳健理解中蒸馏知识；2) 渐进式不平衡感知CutMix(PIC)：创建动态课程，自适应地强制模型关注标记和未标记子集中的少数类别。

Result: 在具有挑战性的半监督类别不平衡医学图像分割基准数据集Synapse和AMOS上取得了显著性能提升，打破了偏见的恶性循环。

Conclusion: 通过向外看的多层次策略，DINO-Mix框架成功解决了传统SSL在类别不平衡医学图像分割中的系统性偏见问题，为半监督学习提供了新的范式。

Abstract: Semi-supervised learning (SSL) has emerged as a critical paradigm for medical image segmentation, mitigating the immense cost of dense annotations. However, prevailing SSL frameworks are fundamentally "inward-looking", recycling information and biases solely from within the target dataset. This design triggers a vicious cycle of confirmation bias under class imbalance, leading to the catastrophic failure to recognize minority classes. To dismantle this systemic issue, we propose a paradigm shift to a multi-level "outward-looking" framework. Our primary innovation is Foundational Knowledge Distillation (FKD), which looks outward beyond the confines of medical imaging by introducing a pre-trained visual foundation model, DINOv3, as an unbiased external semantic teacher. Instead of trusting the student's biased high confidence, our method distills knowledge from DINOv3's robust understanding of high semantic uniqueness, providing a stable, cross-domain supervisory signal that anchors the learning of minority classes. To complement this core strategy, we further look outward within the data by proposing Progressive Imbalance-aware CutMix (PIC), which creates a dynamic curriculum that adaptively forces the model to focus on minority classes in both labeled and unlabeled subsets. This layered strategy forms our framework, DINO-Mix, which breaks the vicious cycle of bias and achieves remarkable performance on challenging semi-supervised class-imbalanced medical image segmentation benchmarks Synapse and AMOS.

</details>


### [189] [$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies](https://arxiv.org/abs/2602.09021)
*Checheng Yu,Chonghao Sima,Gangcheng Jiang,Hai Zhang,Haoguang Mai,Hongyang Li,Huijie Wang,Jin Chen,Kaiyang Wu,Li Chen,Lirui Zhao,Modi Shi,Ping Luo,Qingwen Bu,Shijia Peng,Tianyu Li,Yibo Yuan*

Main category: cs.RO

Relevance: 65.0

TL;DR: 提出χ₀框架，通过模型算术、阶段优势和训练部署对齐三个技术支柱，解决机器人操作中的分布偏移问题，实现高可靠性的长时程衣物操作


<details>
  <summary>Details</summary>
Motivation: 传统机器人操作依赖大规模数据和计算来理解复杂现实世界动态，但主要瓶颈是人为演示分布、策略学习归纳偏置和测试执行分布之间的系统性不一致，导致多阶段任务中的复合错误

Method: 1) 模型算术：权重空间合并策略，高效吸收不同演示的多样化分布；2) 阶段优势：阶段感知的优势估计器，提供稳定密集的进度信号；3) 训练部署对齐：通过时空增强、启发式DAgger校正和时间分块平滑来弥合分布差距

Result: χ₀使双机械臂机器人能够协作执行衣物操作任务（铺平、折叠、悬挂），系统可从任意初始状态连续运行24小时不停机。实验表明χ₀在成功率上超越最先进的π₀.5近250%，仅使用20小时数据和8个A100 GPU

Conclusion: χ₀框架通过解决分布不一致问题，实现了资源高效的生产级机器人操作鲁棒性，为社区提供了可复现的解决方案

Abstract: High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>


### [190] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

Relevance: 45.0

TL;DR: OMNI-Dent是一个数据高效、可解释的牙科诊断框架，将临床推理原则融入视觉语言模型(VLM)流程，利用智能手机多视角照片进行牙齿级评估，无需特定牙科微调。


<details>
  <summary>Details</summary>
Motivation: 当前AI牙科诊断方法主要作为视觉模式识别任务，缺乏临床推理结构，需要大量专家标注数据，且在多样化真实世界成像条件下泛化能力有限。需要解决牙科专业评估可及性问题。

Method: 基于视觉语言模型(VLM)的框架，整合牙科专家的诊断启发式方法，利用多视角智能手机照片，引导通用VLM进行牙齿级评估，无需牙科特定微调。

Result: 开发了数据高效、可解释的诊断框架，能够在缺乏临床影像数据的场景下支持诊断评估，作为早期辅助工具帮助用户识别潜在异常并判断是否需要专业评估。

Conclusion: OMNI-Dent通过整合临床推理到VLM流程，为缺乏面对面护理的人群提供了实用选择，展示了通用VLM在专业医疗诊断中的潜力。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [191] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

Relevance: 45.0

TL;DR: COMBOOD是一个用于图像识别中OOD检测的无监督半参数框架，结合最近邻和马氏距离两种度量信号，在近OOD和远OOD场景下都能提供准确的置信度评分。


<details>
  <summary>Details</summary>
Motivation: 在推理时识别OOD数据对机器学习应用至关重要，特别是在自动化场景中。现有方法在远OOD场景表现良好，但在实际应用中常见的近OOD场景表现不佳，需要一种能同时处理两种场景的统一框架。

Method: 提出COMBOOD半参数框架，结合非参数的最近邻距离和参数化的马氏距离两种信号。最近邻方法提供非参数OOD检测，马氏距离在远OOD场景特别有效。框架将两种信号融合，为近OOD和远OOD场景提供统一的置信度评分。

Result: 在OpenOOD v1和v1.5基准数据集以及文档数据集上，COMBOOD在准确率上优于最先进的OOD检测方法，在大多数基准数据集上的改进具有统计显著性。框架在嵌入空间大小上具有线性扩展性。

Conclusion: COMBOOD通过结合两种距离度量信号，为OOD检测提供了一个有效且可扩展的半参数框架，在近OOD和远OOD场景下都能取得优异性能，适合实际应用。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [192] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

Relevance: 45.0

TL;DR: TLC-Plan是一个层次化生成模型，直接合成矢量平面图，通过两阶段VQ-VAE编码全局布局和局部几何，使用自回归Transformer生成多样且拓扑有效的设计。


<details>
  <summary>Details</summary>
Motivation: 现有平面图生成方法在栅格空间操作，依赖后处理矢量化，导致结构不一致并阻碍端到端学习。受组合空间推理启发，希望开发与人类建筑工作流（基于模块化和可重用模式）对齐的直接矢量平面图生成方法。

Method: 提出TLC-Plan：1）使用两级VQ-VAE编码全局布局（语义标记的房间边界框）和细化局部几何（多边形级编码）；2）通过CodeTree统一表示层次结构；3）使用自回归Transformer在边界条件下采样编码，无需显式房间拓扑或维度先验。

Result: 在RPLAN数据集上达到SOTA性能（FID=1.84，MSE=2.06），在LIFULL数据集上取得领先结果，展示了约束感知和可扩展的矢量平面图生成能力。

Conclusion: TLC-Plan框架推进了面向实际建筑应用的约束感知和可扩展矢量平面图生成，通过层次化生成模型直接合成矢量设计，与人类工作流对齐。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [193] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

Relevance: 45.0

TL;DR: RoadSafe365是一个大规模视觉语言基准，用于细粒度交通安全性分析，包含36,196个标注视频片段和864K候选选项，旨在连接官方安全标准与数据驱动的交通理解系统。


<details>
  <summary>Details</summary>
Motivation: 现有交通基准缺乏与官方安全标准的系统性对齐评估，主要关注粗略的事故识别，需要更细粒度的分析来连接官方标准与数据驱动系统。

Method: 使用分层分类法独立策划和组织大规模真实世界视频数据，扩展了碰撞、事件和违规的基础定义，提供丰富的属性标注，包括交通事件类型、环境背景和交互场景。

Result: 建立了36,196个标注视频片段，包含864K候选选项、8.4K独特答案和36K详细场景描述，在RoadSafe365上微调显示一致性能提升，跨域实验验证了有效性。

Conclusion: RoadSafe365为大规模训练和标准化评估提供了全面基准，可推进真实世界交通安全性分析的可重复研究。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [194] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

Relevance: 45.0

TL;DR: 对最新深度伪造检测技术的实证分析显示，现有检测模型在面对现代合成技术（如扩散模型、NeRF）生成的深度伪造内容时性能显著下降，甚至人类评估者也难以识别高质量伪造内容，揭示了检测方法落后于生成技术的严重差距。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展（扩散模型、NeRF、改进的GANs等），合成媒体的真实性和可访问性大幅提升。虽然检测方法也在进步（Transformer架构、对比学习等），但需要实证评估当前检测技术是否能有效应对现代合成方法生成的深度伪造内容。

Method: 对最先进的深度伪造检测技术进行全面的实证分析，包括：1）评估多种检测模型对现代合成技术（扩散模型、NeRF、改进GANs）生成的深度伪造的检测性能；2）进行人类评估实验，让人类参与者识别最高质量深度伪造内容；3）通过大量实验比较检测模型与生成技术的演进差距。

Result: 研究发现令人担忧的趋势：许多最先进的检测模型在面对现代合成技术生成的深度伪造时表现显著下降；人类参与者在面对最高质量深度伪造时也难以有效识别；检测方法与新生成技术之间存在严重的技术差距。

Conclusion: 当前深度伪造检测方法已落后于生成技术的发展，迫切需要持续改进检测模型以跟上深度伪造生成技术的演进步伐。研究强调了检测方法与新生成技术之间的关键差距，呼吁在这一关键研究领域加强努力。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [195] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出多模态弱监督框架用于视频中"隐藏情绪"的自动识别，在iMiGUE网球采访数据集上达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 解决视频中"隐藏情绪"自动识别问题，现有方法准确率低（低于0.6），且面临类别不平衡挑战

Method: 1) YOLO 11x检测裁剪人像，DINOv2-Base提取视觉特征；2) Gemini 2.5 Pro通过CoT+Reflection生成伪标签和推理文本作为弱监督；3) OpenPose提取137维关键点序列，用MLP替代GCN建模时空关系；4) 超长序列Transformer编码图像和关键点序列，与BERT编码的文本特征拼接；5) 单模态预训练后联合微调

Result: 准确率从先前工作的低于0.6提升到超过0.69，建立新的公开基准；验证了"MLP化"关键点骨干网络可以匹配甚至超越GCN方法

Conclusion: 多模态弱监督框架有效解决了隐藏情绪识别问题，MLP简化架构在关键点建模中表现优异，为类似任务提供了新思路

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [196] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

Relevance: 45.0

TL;DR: UGData数据集将街景图像与空间图对齐，UGE训练策略通过指令引导对比学习和图编码对齐图像、文本和空间结构，UGBench基准评估空间嵌入在多种城市理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 城市环境的多模态嵌入学习具有挑战性，因为现有数据集缺乏街景图像与城市结构的显式对齐。需要开发能够捕捉空间关系（距离、方向、连通性、邻域上下文）的数据集和模型。

Method: 1) UGData数据集：将街景图像锚定到结构化空间图，提供空间推理路径和空间上下文描述；2) UGE两阶段训练策略：结合指令引导对比学习和基于图的空间编码，逐步对齐图像、文本和空间结构；3) 在多个VLM骨干上使用LoRA调优训练固定维度空间嵌入。

Result: 基于Qwen2.5-VL-7B的UGE在训练城市上图像检索提升44%，地理位置排名提升30%；在未见城市上分别提升30%和22%，证明了显式空间接地对空间密集型城市任务的有效性。

Conclusion: 显式空间接地对于城市理解任务至关重要，UGData、UGE和UGBench为学习可迁移的多模态城市嵌入提供了有效框架，显著提升了空间密集型任务的性能。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [197] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

Relevance: 45.0

TL;DR: 本文重新审视了基于注意力的稀疏图像匹配模型训练，发现了一个被忽视的关键设计选择对LightGlue性能有显著影响。研究发现检测器比描述符对性能差异影响更大，并提出了一种使用多样化检测器关键点微调现有图像匹配模型的新方法，实现了通用的检测器无关模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于注意力的稀疏图像匹配模型（如LightGlue）存在性能瓶颈，但缺乏对关键设计选择的系统分析。研究者希望深入理解检测器和描述符在transformer匹配框架中的相对重要性，并开发能够适应不同检测器的通用匹配模型。

Method: 1. 识别并分析先前被忽视的关键设计选择对LightGlue性能的影响；2. 系统研究检测器和描述符在transformer匹配框架中的作用；3. 提出使用多样化检测器关键点微调现有图像匹配模型的方法，创建检测器无关的通用模型。

Result: 1. 发现检测器（而非描述符）是性能差异的主要来源；2. 提出的通用模型在零样本匹配新检测器时，达到或超过了专门为这些特征训练的模型的准确性；3. 为transformer匹配模型的部署和局部特征设计提供了重要见解。

Conclusion: 本文揭示了transformer图像匹配模型中检测器的重要性，提出了有效的检测器无关训练方法，为通用图像匹配系统的开发提供了新思路，对计算机视觉中的特征匹配技术有重要贡献。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [198] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出Inspiration Seeds框架，将图像生成从最终执行转向探索性构思，通过视觉方式组合输入图像生成多样化的视觉连贯组合，无需文本提示


<details>
  <summary>Details</summary>
Motivation: 当前生成模型主要针对精心设计的文本提示进行优化，缺乏对开放式视觉探索的支持，而设计师经常从松散连接的视觉参考中寻找灵感，寻求激发新想法的涌现连接

Method: 使用前馈模型，在合成三元组上训练，通过视觉方式分解视觉方面：使用CLIP稀疏自编码器提取CLIP潜在空间中的编辑方向并隔离概念对，无需依赖语言

Result: 模型能够生成多样化、视觉连贯的组合，揭示输入图像之间的潜在关系，支持创意工作的早期和模糊阶段的视觉构思

Conclusion: 通过消除对语言的依赖并实现快速直观的重组，该方法支持创意工作早期阶段的视觉构思，将生成模型从最终执行工具转变为探索性构思工具

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [199] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

Relevance: 40.0

TL;DR: FlexID是一个无需训练的个人化文本到图像生成框架，通过意图感知调制正交解耦身份特征，在语义身份投影和视觉特征锚点之间动态平衡，实现身份保真度和文本适应性的协同。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练方法依赖刚性的视觉特征注入，导致身份保真度和文本适应性之间的冲突。需要一种能够动态平衡这两者的解决方案。

Method: 提出FlexID框架：1) 语义身份投影器(SIP)在语言空间注入高级先验；2) 视觉特征锚点(VFA)在潜在空间确保结构保真度；3) 上下文感知自适应门控(CAG)机制根据编辑意图和扩散时间步动态调制这两个流的权重。

Result: 在IBench上的大量实验表明，FlexID在身份一致性和文本遵循性之间实现了最先进的平衡，为复杂叙事生成提供了高效解决方案。

Conclusion: FlexID通过意图感知调制解决了身份保真度和文本适应性之间的冲突，为无需训练的个人化文本到图像生成提供了有效的动态平衡机制。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [200] [A Unified Framework for Multimodal Image Reconstruction and Synthesis using Denoising Diffusion Models](https://arxiv.org/abs/2602.08249)
*Weijie Gan,Xucheng Wang,Tongyao Wang,Wenshang Wang,Chunwei Ying,Yuyang Hu,Yasheng Chen,Hongyu An,Ulugbek S. Kamilov*

Main category: eess.IV

Relevance: 40.0

TL;DR: Any2all是一个统一的多模态图像重建与合成框架，将不同任务统一为虚拟修复问题，使用单一无条件扩散模型处理PET/MR/CT等多种模态数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为不同任务训练特定模型，导致训练和部署流程复杂。作者希望开发一个统一框架来处理不完整多模态成像数据，简化工作流程。

Method: 将多模态重建和合成任务统一为虚拟修复问题，在完整多模态数据栈上训练单一无条件扩散模型。在推理时，通过"修复"机制从任意输入组合（干净图像或噪声测量）生成所有目标模态。

Result: 在PET/MR/CT脑部数据集上验证，Any2all在多模态重建和合成任务上均表现优异，在失真度指标上具有竞争力，在感知质量上优于专用方法。

Conclusion: Any2all提供了一个统一的解决方案，能够用单一模型处理多种多模态成像任务，简化了训练和部署流程，同时保持或超越了专用方法的性能。

Abstract: Image reconstruction and image synthesis are important for handling incomplete multimodal imaging data, but existing methods require various task-specific models, complicating training and deployment workflows. We introduce Any2all, a unified framework that addresses this limitation by formulating these disparate tasks as a single virtual inpainting problem. We train a single, unconditional diffusion model on the complete multimodal data stack. This model is then adapted at inference time to ``inpaint'' all target modalities from any combination of inputs of available clean images or noisy measurements. We validated Any2all on a PET/MR/CT brain dataset. Our results show that Any2all can achieve excellent performance on both multimodal reconstruction and synthesis tasks, consistently yielding images with competitive distortion-based performance and superior perceptual quality over specialized methods.

</details>


### [201] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种用于孟加拉国纸币识别的混合CNN架构，结合MobileNetV3-Large和EfficientNetB0进行特征提取，使用MLP分类器，在资源受限设备上实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 准确的纸币识别对辅助技术至关重要，特别是视障人士依赖他人识别纸币，存在欺诈和剥削风险。需要开发适用于资源受限设备的高效识别系统。

Method: 1) 构建新的孟加拉国纸币数据集（包含受控和真实场景）；2) 整合四个额外数据集增强鲁棒性；3) 提出混合CNN架构（MobileNetV3-Large + EfficientNetB0）进行特征提取；4) 使用多层感知机分类器；5) 采用五折交叉验证和七个评估指标；6) 集成LIME和SHAP等可解释AI方法。

Result: 模型在受控数据集上达到97.95%准确率，复杂背景上92.84%，所有数据集组合上94.98%。通过多种评估指标验证性能，并利用可解释AI方法增强透明度。

Conclusion: 提出的混合CNN架构在保持低计算成本的同时实现了高精度纸币识别，适用于资源受限设备，并通过可解释AI方法增强了系统透明度和可信度。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [202] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究比较了标准CNN与ANFIS增强版本在对抗攻击下的性能，发现ANFIS集成对鲁棒性的提升具有架构依赖性，ResNet18-ANFIS表现更好而VGG-ANFIS则较差。


<details>
  <summary>Details</summary>
Motivation: CNN在图像分类中表现出色但缺乏可解释性且易受对抗攻击，神经模糊混合方法如DCNFIS用ANFIS替换全连接层以提升可解释性，但其鲁棒性尚未充分研究。

Method: 比较标准CNN（ConvNet、VGG、ResNet18）与其ANFIS增强版本在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的性能，测试梯度攻击（PGD）和无梯度攻击（Square）。

Result: ANFIS集成并未一致提升干净准确率，对鲁棒性的影响具有架构依赖性：ResNet18-ANFIS表现出更好的对抗鲁棒性，而VGG-ANFIS通常不如其基线模型。

Conclusion: 神经模糊增强可以在特定架构中提升鲁棒性，但并非普遍有益，需要针对具体架构进行优化。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [203] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出ShapBPT方法，利用数据感知的层次结构（二进制分割树）进行像素级特征归因，改进计算机视觉可解释性


<details>
  <summary>Details</summary>
Motivation: 现有分层Shapley方法未充分利用图像数据的多尺度结构，导致收敛慢且与形态特征对齐弱；缺乏针对计算机视觉任务的数据感知层次结构，存在模型可解释性空白

Method: 基于分层Shapley公式，将Shapley系数分配给为图像定制的多尺度层次结构——二进制分割树（BPT），通过数据感知的分层分区确保特征归因与内在图像形态对齐

Result: 实验证实ShapBPT有效性：与图像结构对齐更优、计算效率更高；20人用户研究显示人类更偏好ShapBPT解释

Conclusion: ShapBPT将分层Shapley方法与图像数据连接，提供更高效且语义更有意义的视觉可解释性方法

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [204] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出ECHWR训练框架，通过临时辅助分支对齐传感器信号与文本语义嵌入，使用双重对比损失（批量对比+错误对比），训练后丢弃辅助分支，在保持推理效率的同时显著提升手写识别准确率


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的在线手写识别面临内存限制，需要在保持推理效率的同时提升识别准确率。现有方法要么增加模型复杂度（增加推理成本），要么无法有效处理未见过的书写风格

Method: ECHWR训练框架：1）使用临时辅助分支在训练阶段对齐IMU传感器信号与文本语义嵌入；2）双重对比损失：批量对比损失用于模态对齐，错误对比损失区分正确信号与合成困难负样本；3）训练后丢弃辅助分支，保持原始高效架构

Result: 在OnHW-Words500数据集上显著优于SOTA基线：作者独立分割字符错误率降低7.4%，作者依赖分割降低10.4%。错误对比损失对处理未见书写风格特别有效

Conclusion: ECHWR框架能够在保持边缘设备推理效率的同时，通过训练阶段的模态对齐和错误对比学习显著提升手写识别性能，为解决特定挑战提供了有效的架构和目标配置方案

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [205] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

Relevance: 35.0

TL;DR: Bird-SR：基于双向奖励引导的扩散框架，通过奖励反馈学习将超分辨率建模为轨迹级偏好优化，联合利用合成LR-HR对和真实世界LR图像，在保持结构一致性的同时提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率方法能合成丰富细节，但在合成配对数据上训练的模型常因分布偏移而在真实世界低分辨率图像上失效。需要一种能同时利用合成配对数据和真实世界图像的方法，在保持结构保真度的同时提升感知质量。

Method: 1) 双向奖励引导扩散框架，将超分辨率建模为轨迹级偏好优化；2) 早期扩散步在合成对上直接优化以保持结构保真度；3) 后期采样步对合成和真实图像应用质量引导奖励；4) 通过相对优势空间和语义对齐约束防止奖励攻击；5) 动态保真度-感知权重策略平衡结构保持和感知优化。

Result: 在真实世界超分辨率基准测试中，Bird-SR在感知质量方面持续优于最先进方法，同时保持结构一致性，验证了其对真实世界超分辨率的有效性。

Conclusion: Bird-SR通过奖励反馈学习和动态平衡策略，成功解决了合成数据训练模型在真实世界图像上的分布偏移问题，实现了结构保真度和感知质量的协同优化。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [206] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于可重光照3D高斯泼溅的端到端强化学习框架，实现无人机在非结构化室外环境中的零样本视觉导航，解决仿真与现实间的视觉域差距问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在非结构化室外环境中使用单目视觉导航面临仿真与现实间的巨大视觉域差距问题。现有3D高斯泼溅方法将静态光照与几何结构耦合，限制了策略在动态真实世界光照下的泛化能力。

Method: 提出可重光照3D高斯泼溅，分解场景组件以实现对神经表示中环境光照的显式物理编辑。在高保真仿真中训练端到端强化学习策略，通过多样化的合成光照条件增强训练，使策略学习鲁棒的光照不变视觉特征。

Result: 轻量级四旋翼无人机在复杂森林环境中实现高达10米/秒的鲁棒无碰撞导航，对剧烈光照变化表现出显著韧性，无需微调。

Conclusion: 通过可重光照神经表示和光照增强训练，实现了无人机在非结构化室外环境中的零样本视觉导航，显著提升了策略对动态光照变化的鲁棒性。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [207] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

Relevance: 35.0

TL;DR: 通过CLIP嵌入相似性系统检查LAION-400M数据集，发现数千个包含敏感个人信息的妊娠超声图像，这些信息可能导致重新识别或冒充风险，并提出数据集管理建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的发展，大规模互联网数据集的使用日益普遍，但往往缺乏充分的数据筛选。这引发了包含敏感或私人信息的担忧。本研究特别关注妊娠超声图像，这些图像包含高度敏感的个人信息且常被在线分享。

Method: 使用CLIP嵌入相似性对LAION-400M数据集进行系统性检查，检索包含妊娠超声的图像，并检测其中的私人信息实体（如姓名、位置等）。

Result: 发现了数千个包含私人信息的实体，多个图像包含高风险信息，可能导致重新识别或身份冒充。

Conclusion: 需要改进数据集筛选实践，加强数据隐私保护，并建立公共图像数据集的伦理使用规范。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [208] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出使用视角不变语义特征作为条件输入，解决全头3D GAN训练中的视角偏差问题，提升生成质量和全局一致性


<details>
  <summary>Details</summary>
Motivation: 传统全头3D GAN使用视角角度作为条件输入，导致学习到的3D空间存在视角方向偏差，生成质量在条件视角和非条件视角间差异显著，全局一致性差

Method: 1) 构建视角不变语义条件：使用FLUX.1 Kontext将高质量正面人脸数据集扩展到多视角，提取正面视角的图像clip特征作为共享语义条件；2) 多视角监督在共享语义条件下整合，加速训练并增强全局一致性；3) 语义条件鼓励生成器遵循真实语义分布，促进持续学习和多样性生成

Result: 在全头合成和单视角GAN反转实验中，该方法在保真度、多样性和泛化能力方面显著优于现有方法

Conclusion: 视角不变语义条件能有效解耦3D头部生成能力与视角方向，解决传统方法中的视角偏差问题，提升生成质量和全局一致性

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [209] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

Relevance: 35.0

TL;DR: AdvSR是一种将对抗行为嵌入超分辨率模型权重的框架，在训练时联合优化重建质量和目标对抗结果，使模型在标准图像质量指标下表现正常，但会导致下游分类器误分类。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的超分辨率模型通常作为成像管道中的预处理步骤，以提高下游任务性能。然而，这些SR模型引入了一个未被探索的攻击面。本文旨在展示对抗行为可以直接嵌入SR模型权重中，无需在推理时访问输入。

Method: AdvSR框架在训练期间联合优化重建质量和目标对抗结果。通过将对抗行为直接嵌入模型权重，而不是扰动输入或依赖后门触发器。评估了三种SR架构（SRCNN、EDSR、SwinIR）与YOLOv11分类器的组合。

Result: AdvSR模型在标准图像质量指标下表现正常，但能实现高攻击成功率（导致下游分类器误分类），同时质量退化最小。这证明了模型级威胁的存在。

Conclusion: 研究揭示了一种新的模型级威胁，对成像管道安全有重要影响。这要求从业者在安全关键应用中重新考虑如何获取和验证模型。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [210] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: TwistNet-2D是一个轻量级模块，通过局部成对通道乘积和方向性空间位移来捕捉纹理特征，在纹理识别任务上超越了多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前纹理识别方法存在根本性矛盾：双线性池化和Gram矩阵捕捉全局通道相关性但破坏了空间结构，而自注意力通过加权聚合建模空间上下文而非显式的成对特征交互。需要一种能同时编码特征共现位置和交互方式的方法。

Method: 提出TwistNet-2D模块，核心是螺旋扭曲通道交互（STCI）：将特征图沿指定方向位移后进行逐通道元素乘法，捕捉结构化纹理的跨位置共现模式。使用四个方向头聚合，通过学习通道重加权和sigmoid门控残差路径注入。

Result: 在ResNet-18基础上仅增加3.5%参数和2%FLOPs，但在四个纹理和细粒度识别基准上一致超越了参数匹配和更大的基线模型，包括ConvNeXt、Swin Transformer和混合CNN-Transformer架构。

Conclusion: TwistNet-2D通过局部成对通道交互有效捕捉纹理特征，提供了一种轻量高效且性能优越的纹理识别解决方案。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [211] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

Relevance: 35.0

TL;DR: VideoNeuMat：两阶段流水线，从视频扩散模型中提取可重用的神经材质资产，通过微调大视频模型生成受控条件下的材质样本视频，再用重建模型从视频帧中预测神经材质参数。


<details>
  <summary>Details</summary>
Motivation: 创建逼真的3D渲染材质需要高超的艺术技能，而生成模型因缺乏高质量训练数据受限。虽然视频生成模型能轻松产生逼真材质外观，但这些知识仍与几何和光照纠缠在一起。

Method: 1. 微调大视频模型（Wan 2.1 14B）在受控相机和光照轨迹下生成材质样本视频，创建"虚拟测角反射计"；2. 通过从较小视频主干（Wan 1.3B）微调的大型重建模型（LRM），从17个生成视频帧中单次推理预测神经材质参数。

Result: 生成的材质展现出远超有限合成训练数据的真实感和多样性，证明材质知识可以从互联网规模的视频模型成功转移到独立的、可重用的神经3D资产中。

Conclusion: 成功开发了从视频扩散模型中提取神经材质资产的管道，实现了材质知识的解耦和重用，为3D渲染提供了高质量的材质生成方案。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [212] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出Cross-View World Models (XVWM)，通过跨视角预测目标训练世界模型，使智能体能够在不同视角（如鸟瞰图）进行规划，同时保持自我中心视角执行，增强空间表示学习


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常只从单一自我中心视角操作，即使其他视角（如鸟瞰图）可能使规划更容易。例如导航任务从鸟瞰视角规划会更简单，但现有方法缺乏这种跨视角规划能力。

Method: 提出跨视角世界模型(XVWM)，使用跨视角预测目标训练：给定一个视角的帧序列，预测执行动作后相同或不同视角的未来状态。跨视角一致性作为几何正则化，强制模型学习环境3D结构的视角不变表示。使用Aimlabs平台的多视角游戏数据进行训练。

Result: 模型为智能体提供跨视角的并行想象流，能够在最适合任务的参考系中进行规划，同时从自我中心视角执行。多视角一致性为空间基础表示提供了强大的学习信号。

Conclusion: 跨视角预测能够增强世界模型的空间表示学习，使智能体能够在不同视角进行规划。这种方法为多智能体环境中的视角理解提供了基础。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [213] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

Relevance: 35.0

TL;DR: CLARITY提出了一种动态RGB-热成像融合方法，通过视觉语言模型先验自适应调整融合策略，在恶劣光照条件下实现鲁棒的道路场景语义分割，在MFNet数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-热成像融合方法采用静态融合策略，无法适应不同光照条件，导致模态特定噪声在网络中传播。自动驾驶应用需要在恶劣光照、照明和阴影条件下实现鲁棒的语义分割。

Method: 1) 基于视觉语言模型先验动态调整融合策略，根据检测到的场景条件调制每个模态的贡献；2) 保留先前噪声抑制方法错误丢弃的有效暗物体语义；3) 引入分层解码器，跨尺度强制执行结构一致性以锐化薄物体边界。

Result: 在MFNet数据集上，CLARITY实现了62.3% mIoU和77.5% mAcc，建立了新的最先进性能。

Conclusion: 动态融合策略优于静态方法，视觉语言模型先验能够有效指导融合过程，保留暗物体语义和跨尺度结构一致性对提升分割性能至关重要。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [214] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: SoulX-FlashHead是一个1.3B参数的实时音频驱动肖像视频生成框架，通过流式感知时空预训练和双向蒸馏技术，在保持高视觉质量的同时实现低延迟流式生成。


<details>
  <summary>Details</summary>
Motivation: 解决音频驱动肖像生成中高保真视觉质量与低延迟流式生成之间的平衡难题。现有大规模模型计算成本过高，轻量级方案则牺牲面部表示完整性和时间稳定性。

Method: 1) 1.3B参数统一框架；2) 流式感知时空预训练，包含时序音频上下文缓存机制；3) 先知引导双向蒸馏，利用真实运动先验提供物理指导；4) 构建VividHead数据集（782小时严格对齐视频）。

Result: 在HDTF和VFHQ基准测试中达到SOTA性能，Lite变体在单张RTX 4090上实现96 FPS推理速度，支持超快速交互而不牺牲视觉连贯性。

Conclusion: SoulX-FlashHead成功解决了实时高保真音频驱动肖像生成的挑战，通过创新的流式感知训练和蒸馏技术，在计算效率和视觉质量之间取得了良好平衡。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [215] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

Relevance: 35.0

TL;DR: IM-Animation提出了一种新颖的隐式运动表示方法，将每帧运动压缩为紧凑的1D运动token，解决了现有显式方法的空间不匹配问题和隐式方法的身份信息泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在角色动画生成中存在两大挑战：1）显式方法（如骨架、DWPose）难以处理空间不匹配和身体比例变化；2）隐式方法虽然能捕捉高级运动语义，但存在身份信息泄漏和运动-外观纠缠问题。

Method: 1）提出紧凑的1D运动token表示，放松2D表示的空间约束并防止身份泄漏；2）设计基于时序一致掩码token的重定向模块，通过时序训练瓶颈减少源图像运动干扰；3）采用三阶段训练策略提升训练效率和保真度。

Result: 大量实验表明，IM-Animation的生成能力达到或超越了最先进方法，在运动重定向任务中表现出色。

Conclusion: 提出的隐式运动表示方法有效解决了角色动画中的空间约束和身份泄漏问题，为视频扩散模型在动画生成领域提供了新的解决方案。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [216] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出ZoomDet框架，通过自适应放大无人机图像中的小目标来提升检测性能，包含轻量级偏移预测和边界框变换方法，在多个无人机数据集上显著提升mAP。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标通常尺寸小且稀疏，这阻碍了有效目标检测器的优化。现有方法在处理小目标时效果不佳，需要一种能自适应放大目标区域的方法来更好地捕捉目标特征。

Method: 提出自适应放大框架ZoomDet，包含两个核心设计：1) 轻量级偏移预测方案结合基于框的放大目标，学习对输入图像进行非均匀放大；2) 角对齐的边界框变换方法，将真实框变换到放大空间进行训练，推理时再将预测框变换回原始空间。

Result: 在VisDrone、UAVDT和SeaDronesSee三个无人机目标检测数据集上进行了广泛实验。ZoomDet与架构无关，可应用于任意目标检测架构。在SeaDronesSee数据集上，使用Faster R-CNN模型获得了超过8.4个绝对mAP增益，仅增加约3ms延迟。

Conclusion: ZoomDet是一个简单高效的框架，通过自适应放大无人机图像中的小目标来显著提升检测性能，具有架构无关性和低延迟优势。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [217] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出使用指令调优的视觉语言模型作为评估器，对物体中心学习模型进行统一评估，同时衡量定位能力和表示有用性，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 物体中心学习模型旨在学习结构化场景表示以支持组合泛化和OOD鲁棒性，但现有评估方法主要关注物体发现和简单推理任务，无法全面评估表示的有用性，且定位和表示有用性评估是分离的。

Method: 1) 使用指令调优的视觉语言模型作为评估器，在多样化VQA数据集上进行可扩展的基准测试；2) 引入统一评估任务和指标，联合评估定位（where）和表示有用性（what）；3) 包含简单的多特征重建基线作为参考点。

Result: 提出的评估框架能够更全面地评估物体中心学习模型的表示有用性，通过统一指标同时衡量定位精度和表示质量，为模型比较提供了更可靠的基准。

Conclusion: 该研究为物体中心学习模型提供了更有效的评估方法，解决了现有基准测试的局限性，有助于推动该领域向更具泛化性和鲁棒性的表示学习发展。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [218] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

Relevance: 35.0

TL;DR: MUFASA是一个轻量级即插即用框架，通过利用ViT编码器多个特征层的语义信息来改进基于slot attention的无监督物体分割方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于slot attention的无监督物体中心学习方法仅使用预训练ViT最后一层的特征，忽略了其他层中丰富的语义信息。为了充分利用这些潜在语义信息，需要开发能够跨多个特征层利用slot attention的方法。

Method: MUFASA框架在ViT编码器的多个特征层上计算slot attention，充分利用各层的语义丰富性。提出融合策略将多个层获得的slot聚合成统一的物体中心表示，是一个轻量级即插即用框架。

Result: 将MUFASA集成到现有OCL方法中，在多个数据集上改善了分割结果，达到了新的SOTA，同时提高了训练收敛速度，仅带来较小的推理开销。

Conclusion: 通过利用ViT编码器多个层的语义信息，MUFASA框架能够显著改进基于slot attention的无监督物体分割性能，证明了跨层特征融合的重要性。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [219] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究提出了一种基于解耦表示学习的跨摄像头奶牛识别框架，通过子空间可识别性保证理论，将观测图像分解为多个正交潜在子空间，有效分离跨摄像头不变的稳定身份特征，显著提升在未见摄像头上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有动物识别方法在受控单摄像头环境下表现良好，但在跨摄像头场景中面临严重泛化挑战。当模型从源摄像头部署到具有不同光照、背景、视角和成像特性的新监控节点时，识别性能急剧下降，这限制了非接触技术在动态真实农场环境中的大规模应用。

Method: 提出基于解耦表示学习的跨摄像头奶牛识别框架，利用子空间可识别性保证理论，通过建模底层物理数据生成过程，设计原则驱动的特征解耦模块，将观测图像分解为多个正交潜在子空间，有效分离跨摄像头不变的稳定身份相关生物特征。

Result: 在跨越五个不同摄像头节点的高质量数据集上进行了七项跨摄像头任务的广泛实验，所提方法平均准确率达到86.0%，显著优于源摄像头基线（51.9%）和最强的跨摄像头基线方法（79.8%）。

Conclusion: 该工作为协作跨摄像头奶牛识别建立了子空间理论特征解耦框架，为无控制智能农场环境中的精确动物监测提供了新范式，解决了跨摄像头泛化的核心挑战。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [220] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种地质驱动的机器学习方法，用于从图像中自动映射岩石节理迹线。该方法结合地质建模、合成数据生成和监督图像分割，解决了真实数据有限和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 岩石节理迹线映射在地质工程中至关重要，但面临真实数据稀缺、标注成本高、类别不平衡等挑战。现有方法通常依赖大量标注数据，难以在实际工程中应用。

Method: 1) 使用离散断裂网络模型生成具有地质意义的合成岩石图像，保持节理持续性、连通性和节点类型分布；2) 采用混合训练策略：先在合成数据上预训练，再在真实数据上微调；3) 在箱型和边坡两种实际场景中测试。

Result: 合成数据能有效支持监督式节理迹线检测。混合训练在标注一致时表现良好，微调在标注噪声大时更鲁棒。完全零样本预测有限，但少量真实数据微调可实现有用泛化。定性分析显示比定量指标更具地质意义的结果。

Conclusion: 该方法为岩石节理映射提供了可靠解决方案，特别是在真实数据稀缺时。为领域自适应和评估提供了基础，展示了合成数据在特定领域计算机视觉任务中的价值。

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [221] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文评估了3D医学基础模型在跨模态（特别是PET/CT和PET/MRI）分割任务中的表现，发现现有模型在从结构成像转向功能成像时存在显著性能下降，揭示了当前模型距离真正通用性还有很大差距。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学基础模型的验证主要局限于区域性和结构性成像，缺乏对模态差异的系统评估。作者旨在通过严格的客观评估，揭示现有模型在真实世界应用中的局限性，特别是在从结构成像转向功能成像时的性能差距。

Method: 构建了UMD数据集（包含490个全身PET/CT和464个全身PET/MRI扫描，约675k 2D图像和12k 3D器官标注），通过受试者内对照比较配对扫描，将成像模态作为主要自变量，对代表性3D分割基础模型进行全面评估。

Result: 评估揭示了文献报道的基准与真实世界效果之间的显著差异，特别是在从结构域转向功能域时。现有3D基础模型在跨模态应用中表现出系统性失败，远未达到真正通用状态。

Conclusion: 当前3D基础模型距离真正通用性还有很大差距，需要向多模态训练和评估的范式转变，以弥合理想化基准测试与全面临床实用性之间的差距。该数据集和分析为未来开发真正模态无关的医学基础模型奠定了基础。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [222] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

Relevance: 35.0

TL;DR: 论文提出L-LIO框架，通过音频模态增强车辆安全系统，在原有LILO视觉框架基础上加入音频信号，用于驾驶员状态评估和环境理解。


<details>
  <summary>Details</summary>
Motivation: 现有LILO框架主要依赖视觉信息理解车外场景和驾驶员状态，但音频模态能提供额外信息来增强车辆安全，特别是在视觉信号不足或需要上下文理解的场景中。

Method: 扩展LILO框架为L-LIO（Looking-and-Listening Inside-and-Outside），融合音频和视觉传感器，通过多模态融合增强驾驶员状态评估和环境理解。评估三个案例：驾驶员语音分类潜在损伤状态、乘客自然语言指令分析、音频辅助视觉系统解决外部代理指导歧义。

Result: 初步研究显示音频在安全相关场景中提供重要洞察，特别是在需要声音进行安全决策或视觉信号不足的细微、上下文丰富的场景中。收集了真实环境中的车内和车外音频数据集。

Conclusion: L-LIO通过音频和视觉的多模态融合增强了驾驶员和场景理解，为安全干预提供了新途径。挑战包括环境噪声干扰、隐私问题和跨主体鲁棒性，需要在动态真实世界环境中进一步提高可靠性。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [223] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

Relevance: 35.0

TL;DR: U-KABS：一种结合Kolmogorov-Arnold Networks（KANs）和U型编码器-解码器架构的混合框架，用于医学图像分割，通过Bernstein多项式和B样条的激活函数增强特征表示能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在诊断和治疗规划中至关重要，但由于医学图像的复杂性和变异性，特别是数据中非线性关系的捕捉，仍然具有挑战性。需要更强大的模型来有效处理这些复杂结构。

Method: 提出U-KABS混合框架，整合KANs的表达能力和U型编码器-解码器架构。包含卷积和squeeze-and-excitation阶段增强通道特征表示，以及KAN Bernstein Spline（KABS）阶段使用基于Bernstein多项式和B样条的可学习激活函数。通过跳跃连接实现多尺度特征融合。

Result: 在多个医学图像基准数据集上评估，U-KABS表现出优于强基线的性能，特别是在分割复杂解剖结构方面。

Conclusion: U-KABS通过结合Bernstein多项式的全局平滑性和B样条的局部适应性，能够有效捕捉医学图像中的上下文趋势和细粒度模式，为复杂医学图像分割提供了有效解决方案。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [224] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

Relevance: 35.0

TL;DR: PAND是一种两阶段知识蒸馏框架，通过提示感知语义校准和邻域感知结构蒸馏，将大型视觉语言模型的知识迁移到轻量级网络，用于细粒度视觉分类任务。


<details>
  <summary>Details</summary>
Motivation: 在细粒度视觉分类中，从大型视觉语言模型蒸馏知识到轻量网络面临挑战，主要问题包括依赖固定提示和全局对齐。现有方法难以有效捕捉细粒度语义和局部结构信息。

Method: 提出两阶段框架：1) 提示感知语义校准，生成自适应语义锚点；2) 邻域感知结构蒸馏策略，约束学生模型的局部决策结构。

Result: 在四个细粒度视觉分类基准测试中均优于现有方法。ResNet-18学生在CUB-200上达到76.09%准确率，比VL2Lite基线提升3.4%。

Conclusion: PAND通过解耦语义校准和结构转移，有效解决了细粒度视觉分类中的知识蒸馏问题，为轻量级模型部署提供了有效方案。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [225] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出Rolling Sink方法，无需额外训练即可解决自回归视频扩散模型在超出训练时长时的视觉退化问题，实现从5秒训练扩展到5-30分钟的超长视频合成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在训练时长有限的情况下，测试时遇到超出训练时长的"训练-测试差距"，导致视觉质量快速退化。由于长视频训练计算成本高，需要寻找无需训练的解决方案。

Method: 通过系统分析自回归缓存维护机制，提出Rolling Sink方法。该方法基于Self Forcing（仅用5秒片段训练），在测试时通过创新的缓存管理策略，将视频合成扩展到超长时长。

Result: Rolling Sink能够生成5-30分钟（16 FPS）的超长视频，保持一致的物体、稳定的颜色、连贯的结构和流畅的运动，在长时视觉保真度和时间一致性方面优于现有方法。

Conclusion: 无需额外训练即可有效解决自回归视频扩散模型在超出训练时长时的性能退化问题，为长视频生成提供了高效实用的解决方案。

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [226] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

Relevance: 35.0

TL;DR: OTA-Det是一个统一框架，将开放词汇航空检测（OVAD）和遥感视觉定位（RSVG）结合到一个架构中，支持丰富的语义理解和多目标检测，同时保持实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前OVAD和RSVG两种范式各有局限：OVAD仅限于粗粒度的类别级语义，而RSVG结构上仅限于单目标定位。这些限制使得现有方法无法同时支持丰富的语义理解和多目标检测。

Method: 1. 任务重构策略：统一任务目标和监督机制，支持跨两种范式的数据集联合训练；2. 密集语义对齐策略：建立从整体表达到个体属性的多粒度对应关系；3. 基于RT-DETR架构扩展，引入高效模块实现从闭集检测到开放文本检测的转换。

Result: 在六个涵盖OVAD和RSVG任务的基准测试中达到最先进性能，同时保持34 FPS的实时推理速度。

Conclusion: OTA-Det成功统一了OVAD和RSVG两种范式，实现了同时支持丰富语义理解和多目标检测的能力，为航空场景理解提供了更全面的解决方案。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [227] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

Relevance: 35.0

TL;DR: VFace是一种无需训练、即插即用的视频人脸交换方法，可与基于扩散模型的图像人脸交换方法无缝集成，通过频率谱注意力插值、目标结构引导和流引导注意力时间平滑机制提升时间一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频人脸交换方法通常存在时间不一致性问题，需要额外的训练或视频特定微调。研究者希望开发一种无需训练、即插即用的方法，能够与现有图像人脸交换方法无缝集成，同时保证高质量的时间一致性。

Method: 1. 频率谱注意力插值技术：促进生成并保持关键身份特征
2. 目标结构引导：通过即插即用注意力注入，将目标帧的结构特征与生成对齐
3. 流引导注意力时间平滑机制：在不修改底层扩散模型的情况下强制时空一致性，减少逐帧生成的时间不一致性

Result: 大量实验表明，该方法显著增强了时间一致性和视觉保真度，为视频人脸交换提供了实用且模块化的解决方案。代码已开源。

Conclusion: VFace是一种无需额外训练或视频特定微调的即插即用方法，能够与基于扩散模型的图像人脸交换方法无缝集成，有效解决视频人脸交换中的时间一致性问题。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [228] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

Relevance: 35.0

TL;DR: WristMIR：一种基于区域感知的儿科腕部X光片检索框架，利用密集放射学报告和骨骼特异性定位学习细粒度临床相关图像表示，无需手动图像标注，显著提升骨折模式检索和诊断性能。


<details>
  <summary>Details</summary>
Motivation: 腕部X光片中骨折模式的检索具有挑战性，因为临床重要线索细微、高度局部化，且常被重叠解剖结构或不同成像视角所掩盖。此外，缺乏大规模、高质量标注的医学图像检索数据集也限制了进展。

Method: 使用MedGemma进行结构化报告挖掘生成全局和区域级描述，结合预处理腕部图像和远端桡骨、远端尺骨、尺骨茎突的骨骼特异性裁剪。联合训练全局和局部对比编码器，采用两阶段检索：1）粗粒度全局匹配识别候选检查；2）区域条件重排序对齐预定义解剖骨骼区域。

Result: WristMIR显著提升检索性能：图像到文本Recall@5从0.82%提升至9.35%；嵌入表示在骨折分类上表现更强（AUROC 0.949，AUPRC 0.953）；区域感知评估中，两阶段设计将基于检索的骨折诊断平均F1从0.568提升至0.753；放射科医生评分显示检索病例临床相关性从3.36提升至4.35。

Conclusion: 解剖学引导的检索方法在儿科肌肉骨骼成像中具有增强诊断推理和支持临床决策的潜力。该方法展示了无需手动标注学习细粒度临床相关表示的可行性。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [229] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文评估开源视觉语言模型在隐私相关属性识别中的零样本表现，发现VLMs比人类标注者更倾向于预测隐私属性的存在，但在高一致性情况下可以补充人类标注的遗漏。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型常用于图像视觉属性的零样本检测，但它们在隐私相关属性识别方面的表现尚未得到充分评估。研究者希望了解VLMs在隐私属性识别中的能力，以及它们与人类标注的一致性差异。

Method: 采用零样本评估方法，使用开源视觉语言模型进行隐私相关属性识别。通过分析VLMs的内部一致性（inter-annotator agreement）以及与人类标注的对比，识别VLMs表现良好的属性和存在分歧的情况。

Result: 1) VLMs相比人类标注者更倾向于预测隐私属性的存在；2) 在VLMs内部一致性高的情况下，它们能够识别人类标注者忽略的属性；3) 识别了VLMs表现良好的具体属性类别。

Conclusion: 视觉语言模型在大规模图像数据集的隐私标注中具有补充潜力，特别是在高内部一致性的情况下可以识别人类可能遗漏的隐私属性，为自动化隐私标注提供了可能性。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [230] [Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps](https://arxiv.org/abs/2602.07938)
*Rabbia Asghar,Lukas Rummelhard,Wenqian Liu,Anne Spalanzani,Christian Laugier*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出统一框架，利用动态占据栅格地图在时序解码管道中同时预测未来占据状态、车辆栅格和场景流栅格，解决驾驶场景预测中动态行为复杂性和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景预测方法存在局限性：基于占据栅格地图的agent-agnostic方法难以捕捉动态参与者的行为复杂性，而agent-specific方法对感知不佳或未识别参与者泛化能力差。结合两者可实现更鲁棒安全的运动预测。

Method: 使用轻量级时空骨干网络，设计定制化的相互依赖损失函数捕捉栅格间依赖关系，支持多样化未来预测。通过占据状态信息强制流引导转换，损失函数作为正则化器指导占据演化同时考虑障碍物和遮挡。

Result: 在真实世界nuScenes和Woven Planet数据集上评估，相比基线方法在动态车辆和通用动态场景元素预测方面表现出优越性能。

Conclusion: 提出的统一框架能同时预测车辆特定行为和其他动态实体演化，解决了现有方法的局限性，实现了更鲁棒的驾驶场景预测。

Abstract: Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.

</details>


### [231] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

Relevance: 35.0

TL;DR: ForecastOcc是首个基于视觉的语义占据预测框架，直接从过去的相机图像联合预测未来占据状态和语义类别，无需外部地图估计，在自动驾驶场景中实现几何和语义的联合未来预测。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要同时预测未来的几何和语义信息。现有方法要么只关注运动相关类别（静态/动态物体），要么依赖单独网络生成的过去占据预测，导致错误累积且无法直接从图像学习时空特征。

Method: 提出首个联合预测框架，包含：1) 时序交叉注意力预测模块；2) 2D到3D视图变换器；3) 3D占据编码器；4) 体素级多时间步语义占据预测头。支持多视角（Occ3D-nuScenes）和单目（SemanticKITTI）预测。

Result: 在两个数据集上均超越基线方法，实现了语义丰富、未来感知的预测，能捕捉自动驾驶关键场景动态和语义信息。在SemanticKITTI上建立了首个单目语义占据预测基准。

Conclusion: ForecastOcc通过端到端联合预测框架，直接从图像学习时空特征，避免了错误累积问题，为自动驾驶提供了更准确、语义丰富的未来场景理解能力。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [232] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究提出了一种基于超声自监督预训练模型（USF-MAE）的囊性水瘤自动检测方法，在小型标注数据集上显著优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 囊性水瘤是高风险产前超声发现，但监督深度学习方法受限于小型标注数据集。研究旨在评估超声特异性自监督预训练是否能提高囊性水瘤检测的准确性和鲁棒性。

Method: 使用在37万张无标签超声图像上预训练的USF-MAE（基于掩码自编码的自监督基础模型），在囊性水瘤二分类任务上进行微调。采用与DenseNet-169基线相同的4折交叉验证协议进行评估。

Result: USF-MAE在所有评估指标上均优于DenseNet-169基线：准确率0.96 vs 0.93，敏感性0.94 vs 0.92，特异性0.98 vs 0.94，ROC-AUC 0.98 vs 0.94。Score-CAM可视化显示模型关注胎儿颈部相关区域，性能提升具有统计显著性（p=0.0057）。

Conclusion: 超声特异性自监督预训练能够显著提升小型标注数据集上的囊性水瘤检测性能，为可扩展的早期筛查项目提供了技术支持。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [233] [Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects](https://arxiv.org/abs/2602.08046)
*Yahia Hamdi,Nicolas Andrialovanirina,Kélig Mahé,Emilie Poisson Caillault*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出MoE-DCGAN架构，将深度3D卷积GAN与专家混合模型结合，用于高质量3D模型生成和残缺物体重建，通过动态容量约束机制平衡专业化和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在3D物体生成和补全任务中面临挑战：难以捕捉复杂多样的数据分布，特别是在输入不完整或缺失区域较大的情况下。这主要源于高计算需求和建模异构、结构复杂数据的困难，限制了在实际应用中的适用性。

Method: 提出MoE-DCGAN架构，将深度3D卷积GAN与专家混合模型结合。包含多个生成器，每个专门捕捉数据集中的不同模态。引入辅助损失自由的动态容量约束机制，指导分类生成器的选择，确保专业化、训练稳定性和计算效率之间的平衡。

Result: 评估了模型在不同大小缺失区域的形状生成和补全能力，与最先进方法进行比较。定量和定性结果都证实了所提MoE-DCGAN在处理复杂3D数据方面的有效性。

Conclusion: MoE-DCGAN架构成功解决了3D物体生成和补全中的挑战，通过专家混合框架和动态容量约束机制，在保持计算效率的同时提高了生成质量，为复杂3D数据处理提供了有效解决方案。

Abstract: The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>


### [234] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一个系统性的框架，使Vision Transformer的关键组件（包括patch embedding、self-attention、位置编码和采样层）具有等变性，构建理论上保证等变性的ViT架构，可即插即用并扩展到Swin Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有等变性ViT在性能和等变性之间难以平衡，主要因为难以在ViT的多样化模块（特别是自注意力机制与patch embedding的协调）中实现全面的等变性修改。

Method: 提出一个系统性的框架，将ViT的关键组件（patch embedding、self-attention、位置编码、Down/Up-Sampling）都设计为等变性的，构建理论上保证等变性的ViT架构，可作为即插即用模块，并能扩展到Swin Transformer。

Result: 广泛的实验表明，提出的等变性ViT在各种视觉任务中持续提升性能和数据效率。

Conclusion: 提出的系统性框架能够构建理论上保证等变性的ViT架构，在保持等变性的同时提升性能和数据效率，具有理论严谨性和实际应用价值。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [235] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: DICE是一种无需训练的风格擦除框架，通过对比子空间分解将艺术家风格从内容中解耦，防止扩散模型的风格模仿侵权


<details>
  <summary>Details</summary>
Motivation: 扩散模型的普及使得风格模仿变得容易，引发了版权和知识产权风险。现有对策要么需要昂贵的权重编辑，要么依赖明确指定的编辑风格，限制了部署侧的安全性实用性。

Method: 提出DICE框架：1) 构建对比三元组让模型区分风格和非风格特征；2) 将解耦过程形式化为可解的广义特征值问题，精确识别风格子空间；3) 引入自适应注意力解耦编辑策略，动态评估每个token的风格浓度，对QKV向量进行差异抑制和内容增强。

Result: 实验表明DICE在风格擦除彻底性和内容完整性保护之间达到优越平衡，仅需额外3秒时间解耦风格，提供实用高效的技术来遏制风格模仿。

Conclusion: DICE提供了一种无需训练、实时可用的艺术家风格擦除解决方案，通过对比子空间分解和自适应注意力编辑，有效保护版权和知识产权。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [236] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究评估了Vision Transformer模型在xBD卫星图像数据集上的建筑损伤分类性能，提出了一种针对性的补丁预处理流程和冻结头微调策略，在噪声和类别不平衡数据上取得了与CNN基线竞争的结果。


<details>
  <summary>Details</summary>
Motivation: 灾后快速建筑损伤评估对应急响应至关重要。卫星图像损伤分类模型提供了可扩展的态势感知手段，但卫星数据中的标签噪声和严重类别不平衡带来了主要挑战。xBD数据集提供了跨地理区域的建筑级损伤标准化基准。

Method: 评估DINOv2-small和DeiT模型进行多类别损伤分类。提出针对性的补丁预处理流程来隔离结构特征并最小化训练中的背景噪声。采用冻结头微调策略以保持计算需求可控。通过准确率、精确率、召回率和宏观平均F1分数评估模型性能。

Result: 研究表明，采用新颖训练方法的小型ViT架构在灾害分类任务中，相对于先前的CNN基线，取得了具有竞争力的宏观平均F1分数。

Conclusion: 小型Vision Transformer模型结合针对性的预处理和微调策略，能够在噪声和类别不平衡的卫星图像数据上有效进行建筑损伤分类，为灾后快速评估提供了可行的技术方案。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [237] [DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation](https://arxiv.org/abs/2602.08168)
*Mei Ling Chee,Thangarajah Akilan,Aparna Ravindra Phalke,Kanchan Keisham*

Main category: cs.CV

Relevance: 35.0

TL;DR: DAS-SK：一种轻量级语义分割架构，将选择性卷积核融入双空洞可分离卷积模块，用于高分辨率农业图像分析，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 高分辨率农业图像语义分割需要平衡精度与计算效率，以便在实际系统中部署。现有模型通常需要大量数据、光谱泛化能力有限，且计算成本高，限制了在无人机和边缘设备上的应用。

Method: 提出DAS-SK架构：1）将选择性卷积核（SK-Conv）融入双空洞可分离卷积（DAS-Conv）模块，增强多尺度特征学习；2）改进空洞空间金字塔池化（ASPP）模块，同时捕捉细粒度局部结构和全局上下文信息；3）基于改进的DeepLabV3框架，使用MobileNetV3-Large和EfficientNet-B3两个互补骨干网络。

Result: 在LandCover.ai、VDD和PhenoBench三个基准测试中，DAS-SK始终达到最先进性能，同时比CNN、Transformer和混合模型更高效。相比顶级Transformer模型，参数减少21倍，GFLOPs减少19倍。

Conclusion: DAS-SK为实时农业机器人和高分辨率遥感提供了一个鲁棒、高效且可扩展的解决方案，在其他视觉领域也有广泛部署潜力。

Abstract: Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>


### [238] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出MCSDR模型，使用基于分数的扩散模型进行生成式回归，解决超声心动图LVEF估计中的多模态分布问题


<details>
  <summary>Details</summary>
Motivation: 超声心动图LVEF估计是一个病态逆问题，存在噪声、伪影和有限视角导致的模糊性。传统回归方法最小化MSE会学习条件期望，但在后验分布多模态或重尾时会产生误导性预测，这在病理场景中很常见。

Method: 提出多模态条件分数扩散回归模型(MCSDR)，这是一个概率框架，用于建模以超声心动图视频和患者人口统计学属性先验为条件的LVEF连续后验分布。使用基于分数的扩散模型进行生成式回归。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric和CAMUS数据集上的实验表明，MCSDR实现了最先进的性能。定性分析显示模型在噪声高或生理变异性大的情况下表现出不同的生成轨迹，为AI辅助诊断提供了新的可解释性。

Conclusion: 从确定性回归转向生成式回归的范式转变能更好地处理LVEF估计中的不确定性。MCSDR不仅提高了预测准确性，还通过生成轨迹提供了可解释性，有助于临床决策。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [239] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了GR-CoT框架，通过地理空间推理链增强MLLMs的场景理解能力，指导开放词汇语义分割模型实现精确的地理语义映射


<details>
  <summary>Details</summary>
Motivation: 现有遥感开放词汇分割方法主要依赖视觉特征和文本嵌入的被动映射，缺乏地理空间上下文意识，导致在光谱特征相似但语义属性不同的地物类别上出现严重语义歧义和误分类

Method: 提出GR-CoT框架，包含离线知识蒸馏流和在线实例推理流。离线流建立细粒度类别解释标准，在线流执行宏观场景锚定、视觉特征解耦和知识驱动决策合成的顺序推理过程，生成图像自适应词汇表

Result: 在LoveDA和GID5基准测试上进行了广泛实验，证明了方法的优越性

Conclusion: 通过地理空间推理链增强MLLMs的场景理解能力，能够有效解决遥感开放词汇分割中的语义歧义问题，实现更精确的地理语义映射

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [240] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

Relevance: 35.0

TL;DR: MA-ADV：首个利用点云表示生成对抗性事件的运动感知对抗框架，通过扩散平滑扰动和Adam优化实现100%攻击成功率


<details>
  <summary>Details</summary>
Motivation: 事件相机已广泛应用于自动驾驶、机器人等安全关键领域，但深度神经网络对对抗样本的脆弱性威胁着事件系统的可靠性。由于主流事件表示的非可微性，基于梯度的攻击方法难以扩展，事件对抗攻击研究稀缺。

Method: 提出MA-ADV运动感知对抗框架：1) 利用点云表示生成对抗事件（首次工作）；2) 考虑事件中的高频噪声，采用基于扩散的方法平滑扰动；3) 充分利用事件的空间和时间关系；4) 通过样本级Adam优化、迭代细化和二分搜索识别最小成本扰动。

Result: 实验验证MA-ADV确保100%攻击成功率且扰动成本最小，同时展示了对防御的增强鲁棒性，突显了未来事件感知系统面临的关键安全挑战。

Conclusion: MA-ADV是首个利用点云表示生成对抗事件的框架，成功解决了事件表示非可微性带来的挑战，揭示了事件感知系统的安全脆弱性，对未来安全关键应用具有重要意义。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [241] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

Relevance: 35.0

TL;DR: PISCO是一个视频扩散模型，用于精确的视频实例插入，支持任意稀疏关键帧控制。通过可变信息引导和分布保持时间掩码等技术，解决了预训练视频扩散模型在稀疏条件下的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI视频生成正从依赖大量提示工程和"筛选"的通用生成，转向细粒度可控生成和高保真后处理。在专业AI辅助电影制作中，需要进行精确的目标修改，视频实例插入是关键任务，要求保持场景完整性、时空精确放置、物理一致交互和原始动态保存。

Method: 提出PISCO视频扩散模型，支持单关键帧、起止关键帧或任意时间戳的稀疏关键帧控制。引入可变信息引导用于鲁棒条件化，分布保持时间掩码用于稳定时间生成，以及几何感知条件化实现真实场景适应。

Result: 构建了PISCO-Bench基准测试，包含已验证的实例标注和配对干净背景视频。实验表明PISCO在稀疏控制下持续优于强修复和视频编辑基线，随着提供额外控制信号，表现出清晰、单调的性能提升。

Conclusion: PISCO实现了精确的视频实例插入，支持稀疏关键帧控制，解决了预训练视频扩散模型在稀疏条件下的分布偏移问题，为专业AI辅助电影制作提供了有效的可控生成工具。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [242] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一个多模态融合框架，结合存在-缺失(PA)和仅存在(PO)数据用于跨物种植物分布预测，通过地理对齐策略和混合专家方法解决数据稀疏、标签噪声和分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 大规模跨物种植物分布预测对生物多样性保护至关重要，但面临观测数据稀疏和偏差的挑战。PA数据准确但获取成本高，PO数据覆盖广但存在严重的负样本标签噪声。需要同时利用两种数据优势并解决分布偏移问题。

Method: 1) 基于卫星影像地理覆盖的PO数据伪标签聚合策略，实现标签空间与遥感特征空间的地理对齐；2) 多模态架构：Swin Transformer处理卫星影像，TabM网络提取表格特征，Temporal Swin Transformer处理时间序列，串行三模态交叉注意力机制融合异质模态；3) 混合专家范式：根据测试样本与PA样本的空间接近度分区，在不同分区使用不同数据集训练的模型进行推理和后处理。

Result: 在GeoLifeCLEF 2025数据集上的实验表明，该方法在PA覆盖有限且分布偏移显著的情况下实现了优越的预测性能。

Conclusion: 提出的多模态融合框架有效解决了植物分布预测中的数据稀疏、标签噪声和分布偏移问题，为生物多样性保护提供了实用解决方案。

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [243] [CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment](https://arxiv.org/abs/2602.08309)
*Yunzuo Hu,Wen Li,Jing Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: CAE-AV是一个用于解决音频-视觉学习中模态不对齐问题的新框架，通过两个互补模块（CASTE和CASE）动态平衡时空关系并注入跨模态语义指导，显著提升了多模态表示质量。


<details>
  <summary>Details</summary>
Motivation: 音频-视觉学习面临模态不对齐的挑战，包括屏幕外声源和背景干扰等问题。现有方法往往会放大不相关区域或时刻，导致训练不稳定和表示质量下降。需要一种能够有效缓解音频-视觉不对齐的框架。

Method: 提出CAE-AV框架，包含两个互补模块：1) CASTE：基于帧级音频-视觉一致性评估，动态平衡时空关系，确保从前后帧捕获关键信息；2) CASE：在选定的时空位置注入跨模态语义指导，利用高级语义线索进一步缓解不对齐。还设计了轻量级目标函数，包括caption-to-modality InfoNCE、视觉-音频一致性和熵正则化。

Result: 在冻结骨干网络的情况下，CAE-AV在AVE、AVVP、AVS和AVQA四个基准测试中取得了最先进的性能。定性分析进一步验证了其对音频-视觉不对齐的鲁棒性。

Conclusion: CAE-AV通过动态平衡时空关系和注入跨模态语义指导，有效解决了音频-视觉学习中的模态不对齐问题，显著提升了多模态表示的质量和鲁棒性。

Abstract: Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>


### [244] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

Relevance: 35.0

TL;DR: GeoEdit是一个基于扩散模型的图像编辑框架，通过扩散Transformer模块和效果敏感注意力机制，实现了精确的几何变换（平移、旋转、缩放）和逼真的光影效果处理。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在图像编辑方面取得了显著进展，但在处理几何变换（如平移、旋转、缩放）方面仍存在挑战，特别是在复杂场景中。现有方法存在两个主要局限：1）难以实现准确的几何编辑；2）对复杂光影效果建模不足，导致结果不真实。

Method: 提出GeoEdit框架，包含两个核心组件：1）通过扩散Transformer模块实现上下文生成，集成几何变换进行精确对象编辑；2）引入效果敏感注意力机制，增强对复杂光影效果的建模。此外，构建了RS-Objects数据集，包含超过12万对高质量图像，用于训练模型学习精确几何编辑和逼真光影生成。

Result: 在公共基准测试上的广泛实验表明，GeoEdit在视觉质量、几何精度和真实感方面持续优于现有最先进方法。

Conclusion: GeoEdit通过创新的扩散Transformer架构和效果敏感注意力机制，有效解决了扩散模型在几何编辑和光影效果方面的局限性，显著提升了图像编辑的质量和真实感。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [245] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: TriC-Motion是一个新颖的基于扩散的文本到动作生成框架，通过空间-时间-频率三域联合建模和因果干预，解决了现有方法在跨域联合优化和噪声解耦方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法主要关注空间-时间建模或独立的频率域分析，缺乏跨空间、时间和频率域的联合优化框架。这限制了模型同时利用所有域信息的能力，导致生成质量不佳。此外，动作生成框架中，噪声引起的动作无关线索常常与有益特征纠缠，导致动作失真。

Method: 提出TriC-Motion框架，包含三个核心建模模块：时间动作编码、空间拓扑建模和混合频率分析。通过Score-guided Tri-domain Fusion模块整合三域有价值信息，确保时间一致性、空间拓扑、动作趋势和动态。此外，设计了Causality-based Counterfactual Motion Disentangler来暴露动作无关线索以消除噪声，解耦各域的真实建模贡献。

Result: 在HumanML3D数据集上取得了出色的性能，R@1达到0.612，优于现有最先进方法。实验结果表明该框架能够生成高保真、连贯、多样且文本对齐的动作序列。

Conclusion: TriC-Motion通过空间-时间-频率三域联合建模和因果干预，有效解决了文本到动作生成中的跨域优化和噪声解耦问题，实现了高质量的文本对齐动作生成。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [246] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于2D姿态估计的手势分类框架，用于自动驾驶车辆理解行人手势，在真实世界视频数据上达到87%准确率


<details>
  <summary>Details</summary>
Motivation: 手势在交通非语言交流中至关重要，尤其在行人-驾驶员交互中。自动驾驶车辆难以理解这些手势，需要专门框架来识别和分类行人手势，以增强感知能力和安全性。

Method: 使用2D姿态估计从WIVW数据集的真实世界视频序列中提取特征。将手势分为四类（停止、前进、感谢/问候、无手势），从归一化关键点中提取76个静态和动态特征，重点关注手部位置和运动速度。

Result: 分类准确率达到87%，手部位置和运动速度是区分不同手势类别的关键判别特征。该框架显著提升了自动驾驶系统对行人手势的理解能力。

Conclusion: 提出的手势分类框架有效提升了自动驾驶车辆对行人手势的感知能力，不仅改进了AV系统的交互能力，还促进了对交通场景中行人行为的更广泛理解。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [247] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究探讨了光照变化对多类别食物识别系统的域偏移影响，通过构建合成光照增强数据集来提升模型在真实世界部署中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视觉食物识别系统（如自动传送带检测）对光照变化引起的域偏移非常敏感。现有研究通常局限于单一食物类别或受控环境，且大多数公开食物数据集缺乏明确的光照标注。光照变化会显著扭曲人类和AI对食物的感知，影响识别系统的可靠性。

Method: 使用Food-101和Fruits-360两个广泛采用的数据集，研究光照引起的域偏移。通过系统变化光照温度和强度构建合成光照增强数据集，进行受控的鲁棒性分析。评估跨数据集迁移学习和域泛化能力，特别关注光照敏感的目标类别（如苹果类食物）。

Result: 跨数据集评估显示由于视觉条件不匹配导致显著的准确率下降。光照感知增强显著提高了在域偏移下的识别鲁棒性，同时保持了实时性能。实验结果表明，针对光照敏感类别的专门处理能有效提升系统可靠性。

Conclusion: 光照鲁棒性对于在真实世界检测场景中部署可靠的食物识别系统至关重要。光照感知增强是提升模型在域偏移下性能的有效方法，为实际应用提供了实用见解。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [248] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

Relevance: 35.0

TL;DR: GOT-Edit：一种在线跨模态模型编辑方法，将几何感知线索集成到通用目标跟踪器中，通过预训练的视觉几何基础Transformer从2D图像推断几何线索，使用零空间约束更新来结合几何信息同时保持语义判别能力。


<details>
  <summary>Details</summary>
Motivation: 人类感知在2D视频流中进行有效目标跟踪时，隐式地使用了先验3D知识和语义推理。相比之下，大多数通用目标跟踪方法主要依赖目标的2D特征及其周围环境，忽略了3D几何线索，这使得它们容易受到部分遮挡、干扰物以及几何和外观变化的影响。

Method: 提出GOT-Edit，一种在线跨模态模型编辑方法：1）利用预训练的视觉几何基础Transformer从少量2D图像推断几何线索；2）通过零空间约束更新进行在线模型编辑，在保持语义判别能力的同时融入几何信息。

Result: 在多个通用目标跟踪基准测试上的广泛实验表明，GOT-Edit实现了卓越的鲁棒性和准确性，特别是在遮挡和杂乱场景下，为结合2D语义和3D几何推理进行通用目标跟踪建立了新范式。

Conclusion: GOT-Edit通过在线模型编辑成功地将3D几何推理集成到2D目标跟踪中，显著提升了在遮挡和杂乱场景下的性能，为计算机视觉中的跨模态学习提供了新思路。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [249] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究探索了在医学影像中实现零样本身体区域检测的方法，提出了三种无需训练的流程，其中基于分割的规则系统表现最佳。


<details>
  <summary>Details</summary>
Motivation: 医学影像中可靠的解剖区域识别是许多自动化工作流程的前提，但现有解决方案严重依赖不可靠的DICOM元数据，且主要使用监督学习，限制了在实际场景中的适用性。

Method: 提出了三种无需训练的零样本方法：1）基于预训练多器官分割模型的分割驱动规则系统；2）遵循放射科医生定义规则的多模态大语言模型；3）结合视觉输入和解剖证据的分割感知MLLM。

Result: 在887个CT和MR扫描上评估，分割驱动规则方法表现最佳（CT加权F1分数0.947，MR 0.914），MLLM在视觉特征明显区域表现有竞争力，分割感知MLLM显示出基本局限性。

Conclusion: 通过利用预训练基础模型中的知识，可以实现医学影像中的零样本身体区域检测，基于分割的规则方法在跨模态和非典型扫描范围中表现出鲁棒性。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [250] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

Relevance: 35.0

TL;DR: FusionEdit：无需训练的文本引导图像编辑框架，通过语义差异自动识别编辑区域，采用距离感知潜在融合和统计注意力融合，实现精确可控编辑并减少边界伪影。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式二值掩码的方法存在硬边界导致的伪影问题，且编辑灵活性受限。需要一种既能精确控制编辑区域，又能保持自然过渡并提升编辑能力的方法。

Method: 1) 通过源提示词和目标提示词的语义差异自动识别编辑与保留区域；2) 沿区域边界进行距离感知潜在融合生成软掩码，结合总变差损失实现平滑过渡；3) 在DiT注意力层中使用AdaIN调制进行统计注意力融合，增强编辑能力同时保持全局一致性。

Result: 实验表明FusionEdit在文本引导图像编辑任务上显著优于现有最先进方法，能够实现更精确、自然的编辑效果，同时保持源图像的身份特征。

Conclusion: FusionEdit通过软掩码生成和统计注意力融合，解决了硬边界伪影问题，实现了无需训练的高质量图像编辑，为可控图像编辑提供了有效解决方案。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [251] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出CLIP引导对齐(CGA)框架，通过检测类别混淆、构建混淆感知提示和特征对齐，解决无源域自适应中类别混淆问题，在细粒度场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无源域自适应方法在细粒度场景下效果不佳，主要原因是忽略了类别间的不对称动态混淆模式，导致伪标签噪声大和目标域判别能力差。

Method: CGA包含三部分：1) MCA检测目标域中的方向性混淆对；2) MCC利用CLIP构建混淆感知文本提示进行上下文敏感伪标注；3) FAM构建混淆引导的特征库并通过对比学习对齐CLIP和源模型特征。

Result: 在多个数据集上的实验表明，CGA始终优于最先进的无源域自适应方法，在易混淆和细粒度场景中提升尤为显著。

Conclusion: 显式建模类别间混淆对于有效的无源域自适应至关重要，CGA通过CLIP引导的混淆建模和特征对齐有效解决了这一问题。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [252] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种半监督师生框架，结合不确定性感知伪标签生成和基于置信度的渐进式课程学习，用于脑肿瘤MRI分割，在有限标注下实现高效学习


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤MRI分割中标注成本高和数据异质性（不同扫描仪和站点）的问题，开发能够在有限监督下鲁棒学习的方法

Method: 1) 不确定性感知伪标签教师模型生成概率掩码和逐像素不确定性；2) 基于置信度的渐进式课程学习，按图像级置信度排序并分阶段引入未标注数据；3) 双损失目标训练学生模型：从高置信区域学习，从低置信区域反学习；4) 基于一致性的伪标签细化

Result: 在BraTS 2021上，验证DSC从0.393（10%数据）提升到0.872（100%数据），早期阶段提升最大。教师模型DSC达0.922，学生在肿瘤子区域（NCR/NET 0.797，水肿0.980）超越教师，特别是在增强类别（DSC 0.620）上恢复教师失败的分割

Conclusion: 置信度驱动的课程学习和选择性反学习能够在有限监督和噪声伪标签下提供鲁棒的分割性能，证明了半监督学习在医学图像分析中的有效性

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [253] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文分析了3D高斯溅射（3DGS）在多视图优化中形成的渲染最优参考（RORs）的结构特性，揭示了密度分层现象：密集区域参数可预测，稀疏区域需要渲染约束。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解3D高斯溅射（3DGS）在多视图优化中形成的隐式结构特性，探究这些渲染最优参考（RORs）的统计规律和可预测性，为更高效的3D场景表示提供理论基础。

Method: 方法包括：1）分析RORs的统计特性；2）使用可学习性探针训练预测器从点云重建RORs；3）方差分解分析密度分层现象；4）提出密度感知策略改进训练鲁棒性。

Result: 研究发现：RORs呈现稳定的混合结构尺度和双峰辐射模式；密度分层明显——密集区域参数与几何相关可预测，稀疏区域参数受可见性异质性主导；形式化分析显示稀疏区域几何与外观参数存在协方差主导的耦合。

Conclusion: 结论：RORs具有双重特性——在密集区域表现为几何基元（点云足够），在稀疏区域表现为视图合成基元（需要多视图约束）；这为自适应平衡前馈预测和渲染优化的系统架构提供了理论基础。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [254] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一个3D车辆条件行人姿态预测框架，通过显式结合周围车辆信息来提升自动驾驶场景中的行人运动预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测行人运动对于复杂城市环境中自动驾驶的安全性和可靠性至关重要。现有方法通常只考虑行人自身历史运动，忽略了周围车辆对行人行为的重要影响。

Method: 1) 增强Waymo-3DSkelMo数据集，添加对齐的3D车辆边界框；2) 提出采样方案按行人和车辆数量分类场景；3) 基于TBIFormer架构，增加专用车辆编码器和行人-车辆交互交叉注意力模块，融合行人和车辆特征。

Result: 实验显示在预测准确性方面有显著提升，验证了不同行人-车辆交互建模方法的有效性，强调了车辆感知的3D姿态预测对自动驾驶的重要性。

Conclusion: 显式结合车辆信息能显著改善行人姿态预测，车辆感知的3D姿态预测对自动驾驶系统至关重要。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [255] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

Relevance: 35.0

TL;DR: Raster2Seq：将栅格化平面图重建为序列到序列任务，通过自回归解码器预测多边形角点，实现几何与语义的联合编码，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以准确重建复杂平面图的结构和语义，特别是包含大量房间和不同多边形角点数量的室内空间平面图。需要一种能够有效处理复杂几何结构和语义信息的方法。

Method: 将平面图重建框架化为序列到序列任务，将房间、门窗等元素表示为带标签的多边形序列。引入自回归解码器，基于图像特征和已生成角点预测下一个角点，使用可学习锚点引导注意力机制聚焦信息丰富的图像区域。

Result: 在Structure3D、CubiCasa5K和Raster2Graph等标准基准测试中达到最先进性能，同时在包含多样房间结构和复杂几何变化的WAFFLE数据集上表现出强大的泛化能力。

Conclusion: Raster2Seq通过序列化表示和自回归解码，能够灵活处理复杂平面图，有效重建几何结构和语义信息，为平面图自动理解和CAD工作流提供了可靠解决方案。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [256] [FeudalNav: A Simple Framework for Visual Navigation](https://arxiv.org/abs/2602.06974)
*Faith Johnson,Bryan Bo Cao,Shubham Jain,Ashwin Ashok,Kristin Dana*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出了一种用于机器人视觉导航的分层框架，通过可转移的路径点选择网络和基于视觉相似性的潜在空间记忆模块，在无需里程计的情况下实现新颖环境中的导航，并展示了交互式导航的潜力。


<details>
  <summary>Details</summary>
Motivation: 在未见、未映射或GPS受限的环境中，传统的基于度量地图的方法失效，需要转向基于学习的方法。本文旨在开发一个无需详细地图、仅通过视觉线索和记忆进行导航的框架，模仿人类的导航能力。

Method: 采用分层框架将导航决策过程分解为多个层次：1）通过可转移的路径点选择网络学习选择子目标；2）使用基于视觉相似性组织的潜在空间记忆模块作为距离的代理，替代基于图的拓扑表示；3）构建紧凑、轻量、易于训练的无里程计导航器。

Result: 在Habitat AI环境中与最先进方法相比取得了有竞争力的结果，无需在训练或推理中使用任何里程计。额外贡献是框架的可解释性支持交互式导航，研究表明即使最小的人类干预也能显著提升整体导航性能。

Conclusion: 提出的分层导航框架通过简单的可转移组件和基于视觉相似性的记忆表示，在无需里程计的情况下实现了有效的视觉导航。框架的可解释性为交互式导航提供了可能，展示了人类干预如何显著提升导航成功率。

Abstract: Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.

</details>


### [257] [LangGS-SLAM: Real-Time Language-Feature Gaussian Splatting SLAM](https://arxiv.org/abs/2602.06991)
*Seongbo Ha,Sibaek Lee,Kyungsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出一个RGB-D SLAM系统，能在低延迟跟踪和建图的同时重建语言对齐的密集特征场，实现3D感知与语言推理的桥梁


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统主要关注几何重建，缺乏与语言对齐的语义特征表示，限制了3D感知与语言推理的结合。需要在线实时系统来构建密集、未压缩的语言对齐特征场

Method: 1) Top-K渲染管道：高效渲染高维特征图；2) 多标准地图管理策略：修剪冗余或不一致的高斯分布；3) 混合场优化框架：根据场特性解耦几何和语义场的优化频率

Result: 系统在15 FPS下运行，几何保真度优于纯几何基线，语义保真度与离线方法相当，证明了在线SLAM构建密集语言对齐特征场的可行性

Conclusion: 在线SLAM构建密集、未压缩的语言对齐特征场是可行且有效的，为3D感知与语言推理的融合提供了新途径

Abstract: In this paper, we propose a RGB-D SLAM system that reconstructs a language-aligned dense feature field while sustaining low-latency tracking and mapping. First, we introduce a Top-K Rendering pipeline, a high-throughput and semantic-distortion-free method for efficiently rendering high-dimensional feature maps. To address the resulting semantic-geometric discrepancy and mitigate the memory consumption, we further design a multi-criteria map management strategy that prunes redundant or inconsistent Gaussians while preserving scene integrity. Finally, a hybrid field optimization framework jointly refines the geometric and semantic fields under real-time constraints by decoupling their optimization frequencies according to field characteristics. The proposed system achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS. Our results demonstrate that online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, bridging the gap between 3D perception and language-based reasoning.

</details>


### [258] [Surveillance Facial Image Quality Assessment: A Multi-dimensional Dataset and Lightweight Model](https://arxiv.org/abs/2602.07403)
*Yanwei Jiang,Wei Sun,Yingjie Zhou,Xiangyang Zhu,Yuqin Cao,Jun Jia,Yunhao Li,Sijing Wu,Dandan Zhu,Xingkuo Min,Guangtao Zhai*

Main category: eess.IV

Relevance: 35.0

TL;DR: 该论文提出了首个针对监控人脸图像质量评估（SFIQA）的综合研究，构建了包含5004张真实监控图像的基准数据集SFIQA-Bench，并提出了轻量级多任务模型SFIQA-Assessor，通过跨视角特征交互和可学习任务令牌实现多维度质量评估。


<details>
  <summary>Details</summary>
Motivation: 监控人脸图像常因低分辨率、运动模糊、遮挡和光照差等因素导致质量严重下降。现有的人脸图像质量评估方法要么关注视觉质量，要么关注识别性能，无法同时满足监控应用中视觉质量和身份保真度的双重需求。

Method: 1) 构建SFIQA-Bench基准数据集：包含5004张来自三种常用监控摄像头的真实监控人脸图像，通过主观实验收集噪声、清晰度、色彩度、对比度、保真度和整体质量六个维度的评分；2) 提出SFIQA-Assessor模型：轻量级多任务FIQA模型，通过跨视角特征交互利用互补的面部视角信息，使用可学习任务令牌指导多个质量维度的统一回归。

Result: 在提出的数据集上，SFIQA-Assessor相比最先进的通用图像质量评估（IQA）和FIQA方法取得了最佳性能，验证了其在真实监控应用中的有效性。

Conclusion: 该研究填补了监控人脸图像质量评估领域的空白，提出的基准数据集和评估方法能够同时考虑视觉质量和身份保真度，为监控应用提供了更全面的质量评估解决方案。

Abstract: Surveillance facial images are often captured under unconstrained conditions, resulting in severe quality degradation due to factors such as low resolution, motion blur, occlusion, and poor lighting. Although recent face restoration techniques applied to surveillance cameras can significantly enhance visual quality, they often compromise fidelity (i.e., identity-preserving features), which directly conflicts with the primary objective of surveillance images -- reliable identity verification. Existing facial image quality assessment (FIQA) predominantly focus on either visual quality or recognition-oriented evaluation, thereby failing to jointly address visual quality and fidelity, which are critical for surveillance applications. To bridge this gap, we propose the first comprehensive study on surveillance facial image quality assessment (SFIQA), targeting the unique challenges inherent to surveillance scenarios. Specifically, we first construct SFIQA-Bench, a multi-dimensional quality assessment benchmark for surveillance facial images, which consists of 5,004 surveillance facial images captured by three widely deployed surveillance cameras in real-world scenarios. A subjective experiment is conducted to collect six dimensional quality ratings, including noise, sharpness, colorfulness, contrast, fidelity and overall quality, covering the key aspects of SFIQA. Furthermore, we propose SFIQA-Assessor, a lightweight multi-task FIQA model that jointly exploits complementary facial views through cross-view feature interaction, and employs learnable task tokens to guide the unified regression of multiple quality dimensions. The experiment results on the proposed dataset show that our method achieves the best performance compared with the state-of-the-art general image quality assessment (IQA) and FIQA methods, validating its effectiveness for real-world surveillance applications.

</details>


### [259] [Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes](https://arxiv.org/abs/2602.08266)
*Seunghoon Jeong,Eunho Lee,Jeongyun Kim,Ayoung Kim*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出一种实例感知的Next Best View (NBV)策略，利用对象特征优先探索未充分观察的区域，通过对象感知的3D高斯泼溅技术提升机器人操作场景中的重建质量


<details>
  <summary>Details</summary>
Motivation: 在遮挡严重、观测不完整的杂乱场景中，选择信息丰富的视点对于构建可靠表示至关重要。现有方法仅依赖几何线索，忽略了与操作相关的语义信息，且倾向于利用而非探索，限制了重建质量。

Method: 1) 提出对象感知的3D高斯泼溅技术，将实例级信息蒸馏为one-hot对象向量；2) 计算置信度加权的信息增益，指导识别与错误和不确定高斯相关的区域；3) 可轻松适应对象中心的NBV，专注于目标对象；4) 通过实例感知策略平衡探索与利用。

Result: 在合成数据集上深度误差降低77.14%，在真实世界GraspNet数据集上降低34.10%。针对特定对象的NBV相比整个场景，可额外减少25.60%的深度误差。在真实机器人操作任务中验证了有效性。

Conclusion: 提出的实例感知NBV策略通过整合语义信息显著提升了3D重建质量，特别是在机器人操作场景中。对象中心的方法进一步增强了对目标对象的重建鲁棒性。

Abstract: In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.

</details>


### [260] [Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals](https://arxiv.org/abs/2602.08882)
*Puqi Zhou,Ali Asgarov,Aafiya Hussain,Wonjoon Park,Amit Paudyal,Sameep Shrestha,Chia-wei Tang,Michael F. Lighthiser,Michael R. Hieb,Xuesu Xiao,Chris Thomas,Sungsoo Ray Hong*

Main category: cs.HC

Relevance: 35.0

TL;DR: 该论文提出了一个多机器人地面视频感知测试平台MRVS，通过LLM增强的视频理解模型来支持公共安全应用，旨在减少人工工作量并提高情境感知能力。


<details>
  <summary>Details</summary>
Motivation: 地面机器人舰队视频可以提升公共安全，但目前缺乏如何设计和集成多机器人视频到公共安全工作流程的知识。需要研究如何使多机器人视频在实际应用中更实用。

Method: 研究1：开发首个多机器人地面视频感知测试平台，包含38个公共安全相关事件、20个机器人巡逻视频数据集和6个设计要求。研究2：构建MRVS工具，通过提示工程优化的视频理解模型增强多机器人巡逻视频流。

Result: 参与者报告LLM增强的视频理解减少了人工工作量并提高了信心，但也注意到误报和隐私问题。测试平台已在GitHub上开源。

Conclusion: 该研究为设计未来多机器人视频感知工具提供了启示，展示了LLM在视频分析中的潜力，同时指出了误报和隐私等实际挑战。

Abstract: Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals' burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools. The testbed is available at https://github.com/Puqi7/MRVS\_VideoSensemaking

</details>


### [261] [Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction](https://arxiv.org/abs/2602.09013)
*Hongyi Chen,Tony Dong,Tiancheng Wu,Liquan Wang,Yash Jangir,Yaru Niu,Yufei Ye,Homanga Bharadhwaj,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

Relevance: 35.0

TL;DR: VIDEOMANIP：无需设备，直接从RGB人类视频学习灵巧机器人手操作，通过计算机视觉重建4D机器人-物体轨迹，优化手-物体接触，实现通用化策略学习


<details>
  <summary>Details</summary>
Motivation: 多指机器人手操作面临高维动作空间和大规模训练数据获取困难的问题。现有方法依赖人类遥操作和专用传感设备，限制了可扩展性。需要一种无需设备、可直接从RGB视频学习的方法

Method: 1) 从单目视频重建人类手姿态和物体网格，生成4D机器人-物体轨迹；2) 通过手-物体接触优化和交互中心抓握建模改进重建数据；3) 演示合成策略从单个视频生成多样化训练轨迹；4) 将重建的人类动作重定向到机器人手进行操作学习

Result: 仿真中：在20个不同物体上达到70.25%成功率（Inspire Hand）。真实世界：在7个任务上平均62.86%成功率（LEAP Hand），比基于重定向的方法高出15.87%

Conclusion: VIDEOMANIP证明了无需设备、直接从RGB人类视频学习灵巧机器人手操作的可行性，通过计算机视觉重建和优化技术实现了可扩展的机器人操作学习

Abstract: Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>


### [262] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: DuMeta++是一个无需配对纵向数据的双元学习框架，用于跨年龄的脑组织MRI分割，通过元特征学习和元初始化学习实现年龄无关的表示提取和数据高效适应。


<details>
  <summary>Details</summary>
Motivation: 脑组织MRI分割在神经科学和临床应用中至关重要，但由于大脑外观和形态随年龄动态变化，实现跨生命周期的稳定性能具有挑战性。现有方法通常依赖配对纵向数据进行自监督正则化，但这类数据在实践中往往难以获取。

Method: 提出DuMeta++双元学习框架：1) 元特征学习提取年龄无关的时空演化脑结构语义表示；2) 元初始化学习实现分割模型的数据高效适应；3) 基于记忆库的类感知正则化策略，在没有显式纵向监督的情况下强制纵向一致性。

Result: 在iSeg-2019、IBIS、OASIS、ADNI等多个数据集上的少样本设置实验中，DuMeta++在跨年龄泛化方面优于现有方法。理论证明了DuMeta++的收敛性。

Conclusion: DuMeta++无需配对纵向数据即可实现跨年龄的脑组织分割，通过双元学习和记忆库正则化解决了年龄相关变化带来的挑战，在少样本设置下表现出优越的泛化性能。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [263] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文提出了一种利用多尺度视觉编码器和融合先验的大脑-图像对齐策略，用于从脑信号解码视觉信息，在检索精度和重建保真度之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前视觉解码方法大多关注高层语义特征而忽略像素级细节，限制了我们对人类视觉系统的理解。需要一种能同时捕捉层次化和多尺度视觉表征的大脑-图像对齐策略。

Method: 1) 利用多个具有不同归纳偏置的预训练视觉编码器捕捉层次化多尺度视觉表征；2) 使用对比学习目标实现脑信号与视觉嵌入的有效对齐；3) 引入融合先验，在大规模视觉数据上学习稳定映射，然后将脑特征匹配到这个预训练先验，增强跨模态分布一致性。

Result: 广泛的定量和定性实验表明，该方法在检索精度和重建保真度之间取得了有利的平衡。

Conclusion: 提出的多尺度视觉编码器结合融合先验的大脑-图像对齐策略，能够更全面地解码脑信号中的视觉信息，为理解人类视觉系统提供了新视角。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [264] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

Relevance: 30.0

TL;DR: FSP-Diff：一种用于超低剂量能谱CT重建的全能谱先验增强双域潜在扩散框架，通过互补特征构建、全能谱先验集成和高效潜在扩散合成，显著提升图像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 超低剂量条件下，能谱CT中能量特定投影的信噪比急剧下降，导致重建图像出现严重伪影和结构细节丢失，需要新的重建方法来平衡细节保真度和噪声抑制。

Method: 提出三核心策略：1) 互补特征构建：结合直接图像重建和投影域去噪结果；2) 全能谱先验集成：融合多能量投影为高信噪比全能谱图像作为结构参考；3) 高效潜在扩散合成：将多路径特征嵌入紧凑潜在空间，在低维流形中进行扩散过程以实现交互特征融合。

Result: 在模拟和真实数据集上的广泛实验表明，FSP-Diff在图像质量和计算效率方面显著优于现有最先进方法。

Conclusion: FSP-Diff框架展示了在临床上可行的超低剂量能谱CT成像的潜力，通过双域特征融合和潜在扩散实现了细节保真与噪声抑制的良好平衡。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [265] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文提出DeCI框架，通过周期-漂移分解和通道独立建模，直接在原始BOLD信号上进行脑疾病分类，优于传统的功能连接方法。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI脑疾病分类方法大多依赖基于皮尔逊相关的功能连接(FC)，将4D BOLD信号简化为静态2D矩阵，丢弃了时间动态信息且仅捕获线性区域间关系。需要直接建模时间信息以更好地捕捉复杂脑动态。

Method: 1) 对现有时间序列模型(PatchTST, TimesNet, TimeMixer等)在原始BOLD信号上进行基准测试；2) 提出DeCI框架：a) 周期与漂移分解：将每个ROI的信号分解为周期性和漂移成分；b) 通道独立性：独立建模每个ROI，提高鲁棒性和减少过拟合。

Result: 在五个公开数据集上的实验表明：1) 时间序列模型一致优于传统FC方法；2) DeCI框架在分类准确率和泛化能力上均优于FC方法和时间序列基线模型。

Conclusion: 研究结果支持在fMRI分析中转向端到端时间建模，以更好地捕捉复杂脑动态。DeCI框架简单有效，为脑疾病分类提供了新思路。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [266] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

Relevance: 30.0

TL;DR: D²-VR：一种基于单图像扩散的视频修复框架，通过退化鲁棒流对齐和对抗蒸馏实现12倍加速采样，在保持感知质量的同时确保时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散先验和时间对齐的视频修复方法虽然能提供优秀的感知质量，但在面对复杂真实世界退化时存在推理延迟高和时间不稳定的问题，限制了实际部署。

Method: 1. 设计退化鲁棒流对齐模块，利用置信度感知注意力过滤不可靠运动线索；2. 采用对抗蒸馏范式将扩散采样轨迹压缩到少步推理；3. 设计协同优化策略平衡感知质量与时间一致性。

Result: D²-VR在保持最先进性能的同时，将采样过程加速12倍，显著提升了推理效率和时间稳定性。

Conclusion: 该工作提出了一种高效且稳定的扩散基视频修复框架，通过创新的对齐、蒸馏和优化策略，解决了现有方法在实际部署中的关键瓶颈。

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [267] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

Relevance: 25.0

TL;DR: RetSAM是一个通用的视网膜分割和量化框架，用于眼底成像，能够进行多目标分割并提取标准化生物标志物，支持大规模眼科研究。


<details>
  <summary>Details</summary>
Motivation: 视网膜成像快速、无创且广泛可用，为眼科和全身健康评估提供可量化的结构和血管信号。然而，由于公共多标签数据集有限且缺乏统一的分割到量化流程，大规模分析仍然困难。

Method: RetSAM是一个通用视网膜分割和量化框架，使用多阶段训练策略，结合私人和公共眼底数据，支持三类任务：分割五种解剖结构、四种视网膜表型模式和20多种不同病变类型，并将分割结果转化为30多种标准化生物标志物。

Result: 在17个公共数据集上实现卓越的分割性能，平均DSC比先前最佳方法提高3.9个百分点，在具有挑战性的多任务基准上提高达15个百分点，在不同人群、成像设备和临床环境中具有良好的泛化能力。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的定量表型，支持跨主要眼科疾病的系统相关性分析，包括糖尿病视网膜病变、年龄相关性黄斑变性、青光眼和病理性近视，从而推动大规模眼科研究和转化应用。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [268] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文研究如何将LeJEPA启发的各向同性高斯约束应用于图像嵌入表示，以改进无监督3D场景重建中的场景发现和相机姿态估计，特别是在视觉模糊的多场景图像集合中。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化图像集合中进行无监督3D场景重建的挑战，特别是在图像来自多个不相关场景且存在显著视觉模糊性的情况下。IMC2025竞赛突显了这些困难，需要在包含异常值和混合内容的真实世界条件下同时进行场景发现和相机姿态估计。

Method: 提出了三种逐步改进的流水线，最终采用LeJEPA启发的各向同性高斯约束方法。该方法在学习的图像嵌入表示上强制执行各向同性高斯约束，而不是引入新的理论保证，主要从经验上评估这些约束如何影响聚类一致性和姿态估计鲁棒性。

Result: 在IMC2025上的实验结果表明，与启发式基线方法相比，高斯约束嵌入可以改善场景分离和姿态合理性，特别是在视觉模糊的设置中。这些约束有助于提高场景发现和姿态估计的鲁棒性。

Conclusion: 理论上动机的表征约束为桥接自监督学习原理和实际运动结构流水线提供了一个有前景的方向。高斯约束嵌入在实际应用中能够改善多场景重建的性能。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [269] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

Relevance: 25.0

TL;DR: RECITYGEN是一个结合潜在扩散模型和交互式语义分割的工具，允许用户通过文本提示交互式生成城市街道视图的变体图像，用于参与式城市设计。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法往往忽视公众意见，导致设计愿景与现实脱节。虽然数字工具如城市信息建模和增强现实已使更多利益相关者参与设计过程，但仍需更易用的工具来降低公众参与门槛。

Method: 结合最先进的潜在扩散模型与交互式语义分割技术，开发RECITYGEN工具，用户可以通过文本提示交互式生成城市街道视图的变体图像。

Result: 在北京的试点项目中，用户使用RECITYGEN为正在进行的城市更新项目提出改进建议。尽管存在一些限制，但工具显示出与公众偏好高度契合的潜力。

Conclusion: RECITYGEN展示了向更动态、包容的城市规划方法转变的潜力，通过降低设计生成门槛，使公众能更有效地参与城市设计过程。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [270] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出URCSA方法，结合改进的U-Net和行列分离注意力模块，用于低光照图像/视频增强，在减少参数量的同时利用全局信息指导局部增强


<details>
  <summary>Details</summary>
Motivation: 现有U-Net结构在低光照增强中缺乏全局信息指导，导致局部噪声大、细节丢失；注意力机制能利用全局信息但参数和计算量大

Method: 1) 改进的U-Net结构 2) 行列分离注意力模块(RCSA)，输入特征图的行列均值和最大值，减少参数同时利用全局信息 3) 两个时间损失函数用于视频增强的时间一致性

Result: 在LOL、MIT Adobe FiveK图像数据集和SDSD视频数据集上验证了方法的有效性

Conclusion: 提出的URCSA方法在低光照图像/视频增强中有效，能减少参数同时利用全局信息，代码已开源

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [271] [CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization](https://arxiv.org/abs/2602.07523)
*Zhen Zhang,Qing Zhao,Xiuhe Li,Cheng Wang,Guoqiang Zhu,Yu Zhang,Yining Huo,Hongyi Yu,Yi Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出基于CA-YOLO的仿生稳定定位系统，通过引入小目标检测头和特征融合注意力机制提升目标定位精度和小目标识别能力，结合仿生云台跟踪控制策略，在COCO和VisDrone数据集上平均精度分别提升3.94%和4.90%


<details>
  <summary>Details</summary>
Motivation: 现代复杂环境中，现有目标定位系统在精度和小目标识别能力方面存在局限，需要开发更准确、高效的目标定位方法

Method: 1) 在YOLO骨干网络中集成仿生模块，包括小目标检测头和特征融合注意力机制(CFAM)；2) 受人类前庭眼反射启发，开发仿生云台跟踪控制策略，包含中心定位、稳定性优化、自适应控制系数调整和智能重捕获功能

Result: CA-YOLO在COCO和VisDrone数据集上平均精度分别提升3.94%和4.90%，时间敏感目标定位实验验证了系统的有效性和实用性

Conclusion: 提出的仿生稳定定位系统显著提升了目标定位精度和小目标识别能力，为复杂环境中的目标定位问题提供了有效解决方案

Abstract: In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.

</details>


### [272] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文提出使用GCViT-Tiny架构进行猫品种分类，在Oxford-IIIT Pet Dataset子集上达到92%测试准确率，展示了Transformer在细粒度图像分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 猫品种识别具有挑战性，因为不同品种在毛色、面部结构等方面差异细微。需要开发准确的方法来支持兽医诊断、动物收容所管理等应用。

Method: 采用GCViT-Tiny架构进行猫品种分类，使用Oxford-IIIT Pet Dataset子集，通过旋转、水平翻转、亮度调整等数据增强技术提升模型泛化能力。

Result: GCViT-Tiny模型在测试集上达到92.00%准确率，验证集上达到94.54%准确率，展示了Transformer架构在细粒度图像分类任务中的有效性。

Conclusion: Transformer架构在细粒度图像分类任务中表现出色，为兽医诊断、动物收容所管理等应用提供了有效解决方案，并提供了Hugging Face演示。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [273] [Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding](https://arxiv.org/abs/2602.07568)
*Hui Ye,Shilong Yang,Yexuan Xing,Juan Yu,Yaoqin Xie,Wei Zhang,Chulong Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: MammoColor是一个端到端框架，通过任务驱动的色彩编码(TDCE)将单通道乳腺X光片转换为彩色增强视图，以提高在致密乳房中的检测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光筛查在致密乳房中敏感性较低，组织重叠和细微发现增加了感知难度。需要一种方法来增强视觉感知，提高检测准确性。

Method: 提出MammoColor框架，包含轻量级TDCE模块和BI-RADS分类器。在VinDr-Mammo数据集上端到端训练，并在内部测试集、两个公共数据集和三个外部临床队列中评估。进行多读者多案例观察研究。

Result: 在VinDr-Mammo上，AUC从0.7669提升到0.8461(P=0.004)，在致密乳房中提升更大(AUC 0.749到0.835)。观察研究中特异性从0.90提高到0.96(P=0.052)，敏感性相当。

Conclusion: TDCE提供任务优化的色彩表示，可能提高感知显著性和减少乳腺X光分诊中的假阳性召回。

Abstract: Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.

</details>


### [274] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

Relevance: 25.0

TL;DR: Images2Slides：基于VLM的API管道，将静态信息图转换为可编辑的Google Slides幻灯片，通过区域级规范提取和几何映射实现元素重建


<details>
  <summary>Details</summary>
Motivation: 信息图通常以静态图像形式存在，内容被锁定在像素中，导致更新、本地化和重用成本高昂。需要一种方法将静态信息图转换为可编辑的格式，便于内容管理和重用。

Method: 1) 使用视觉语言模型(VLM)提取区域级规范；2) 将像素几何映射到幻灯片坐标；3) 通过Google Slides批量更新API重建元素。系统支持多种VLM后端，采用通用JSON区域模式和确定性后处理。

Result: 在29个程序生成的信息图基准测试中：总体元素恢复率0.989±0.057（文本：0.985±0.083，图像：1.000±0.000）；文本转录错误CER=0.033±0.149；布局保真度IoU（文本区域：0.364±0.161，图像区域：0.644±0.131）

Conclusion: Images2Slides成功将静态信息图转换为可编辑幻灯片，解决了内容重用问题。系统识别了文本大小校准和非均匀背景等工程挑战，为未来工作提供了方向。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [275] [Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes](https://arxiv.org/abs/2602.07694)
*Wenping Jin,Yuyang Tang,Li Zhu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文提出了CoalAD基准测试和互补线索协同感知框架，用于煤矿传送带场景中的无监督异物异常检测与像素级定位。


<details>
  <summary>Details</summary>
Motivation: 煤矿传送带场景中的异物异常检测对安全生产至关重要，但由于环境高度非结构化（煤矸石随机堆积、背景复杂多变、异物对比度低且易变形遮挡），传统异常检测方法依赖的稳定性假设失效，导致性能显著下降。

Method: 提出互补线索协同感知框架，从三个角度提取和融合异常证据：1) 对象级语义组合建模；2) 基于语义属性的全局偏差分析；3) 细粒度纹理匹配。融合输出提供鲁棒的图像级异常评分和准确的像素级定位。

Result: 在CoalAD基准测试上的实验表明，该方法在图像级和像素级指标上均优于广泛使用的基线方法，消融研究验证了每个组件的贡献。

Conclusion: 提出的框架能够有效应对煤矿场景中高度非结构化环境的挑战，为安全智能采矿提供了可靠的异物异常检测解决方案。

Abstract: Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.

</details>


### [276] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

Relevance: 25.0

TL;DR: UCATSC是一个基于模型的交通信号控制系统，使用带约束的随机决策过程建模，考虑视觉感知不确定性，通过信念空间中的反事实推演来预测和执行安全约束，提高交通延迟和排放性能。


<details>
  <summary>Details</summary>
Motivation: 当前自适应交通信号控制在现实世界部署有限，主要原因是：1) 基于视觉感知的不确定性；2) 隐含的安全性；3) 主要在仿真中学习和验证的非可解释控制策略。需要解决这些限制以实现实际部署。

Method: 采用基于模型的方法，将交通信号控制建模为带约束的部分可观测随机决策过程。系统在信念空间中进行反事实推演，预测和执行与安全和防饥饿相关的硬约束，不同于强化学习通过奖励塑造来学习安全性。

Result: UCATSC系统能够改善交通延迟和排放，同时防止安全关键错误，并提供基于显式模型的可解释控制策略输出。

Conclusion: UCATSC通过建模感知不确定性、强制执行硬约束和在信念空间中进行反事实推演，解决了自适应交通信号控制实际部署的关键挑战，提供了更安全、可解释的控制策略。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [277] [Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction](https://arxiv.org/abs/2602.07820)
*Zhibo Chen,Yu Guan,Yajuan Huang,Chaoqi Chen,XiangJi,Qiuyun Fan,Dong Liang,Qiegen Liu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种基于操作符引导的框架，用于同时多切片(SMS) MRI重建，通过操作符条件双流交互网络(OCDI-Net)显式分离目标切片内容和切片间干扰，实现两阶段链式推理。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的SMS重建方法通常围绕高斯噪声建模，需要额外的约束步骤来整合SMS物理特性，这可能与SMS采集中的操作符控制退化不匹配。需要更直接地建模SMS采集的退化过程。

Method: 提出操作符引导框架，使用已知采集操作符建模退化轨迹，通过确定性更新反转该过程。开发OCDI-Net网络，显式解耦目标切片内容和切片间干扰，预测结构化退化用于操作符对齐的反转。采用两阶段链式推理：先进行SMS切片分离，再进行平面内补全。

Result: 在fastMRI脑数据和前瞻性采集的体内扩散MRI数据上实验表明，相比传统和基于学习的SMS重建方法，该方法提高了保真度并减少了切片泄漏。

Conclusion: 提出的操作符引导框架和OCDI-Net能够有效处理SMS MRI重建中的耦合逆问题，通过显式建模采集操作符的退化过程，实现了更准确的重建。

Abstract: Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.

</details>


### [278] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一种从超高速运动模糊图像恢复3D形状的新方法，通过快速重心坐标求解器实现高效可微渲染，在平移和旋转运动中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从极端运动模糊图像恢复3D几何的挑战性问题，这在体育（如快速移动的球）和工业（如旋转机械）场景中常见，传统MVS等技术对此无效。

Method: 提出可逆渲染方法，通过快速重心坐标求解器显著减少计算开销（加速4.57倍），实现高效、逼真的高速运动模拟，且完全可微，支持从渲染图像到3D形状的梯度传播。

Result: 在快速平移和旋转两种典型运动类型上验证，方法能高效逼真地模拟超高速运动物体，并成功从极端运动模糊的2D图像中恢复3D形状。

Conclusion: 该方法推进了基于视觉的3D重建边界，为从极端运动模糊图像恢复几何提供了有效解决方案。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [279] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了一种用于少样本人群计数的多尺度密度学习方法，通过局部密度原型和全局密度特征来适应未见过的监控场景。


<details>
  <summary>Details</summary>
Motivation: 不同监控摄像头捕捉的人群场景差异很大，现有的人群计数模型对未见过的监控场景泛化能力有限。为了解决这个问题，作者将不同监控场景视为不同类别场景，引入少样本学习使模型能够适应属于给定示例类别的未见监控场景。

Method: 提出利用局部和全局密度特征来指导未见监控场景的人群计数模型：1）多局部密度学习器学习支持场景中不同密度分布的原型，通过编码局部密度相似性矩阵进行局部指导；2）从支持图像提取全局密度特征进行全局指导。

Result: 在三个监控数据集上的实验表明，该方法能够适应未见过的监控场景，并在少样本人群计数任务中优于最近的最先进方法。

Conclusion: 通过结合局部密度原型学习和全局密度特征指导，提出的方法能够有效提升人群计数模型对未见监控场景的泛化能力，在少样本设置下取得优异性能。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [280] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出CSDN方法用于超稀疏视角CBCT重建，通过神经先验编码连续三维衰减表示，结合协同扩散策略恢复角度连续性和切片一致性


<details>
  <summary>Details</summary>
Motivation: CBCT临床应用受限于辐射暴露与图像质量的权衡，超稀疏角度采样会引入严重的欠采样伪影和切片间不一致性，现有方法难以平衡角度连续性与空间细节保真度

Method: CSDN方法：1) 引入神经先验作为结构基础编码连续三维衰减表示；2) 协同扩散策略包含两个协作细化路径：正弦图细化扩散(Sino-RD)恢复角度连续性，数字放射摄影细化扩散(DR-RD)从投影图像角度增强切片间一致性；3) 双投影重建融合(DPRF)模块自适应融合两个扩散路径输出

Result: 实验表明CSDN在超稀疏视角条件下有效抑制伪影并恢复精细纹理，优于现有最先进技术

Conclusion: CSDN方法通过神经先验和协同扩散策略解决了超稀疏视角CBCT重建中的角度连续性与空间细节保真度平衡问题

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [281] [PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping](https://arxiv.org/abs/2602.08020)
*Minghai Chen,Mingyuan Liu,Yuxiang Huan*

Main category: cs.CV

Relevance: 25.0

TL;DR: PhysDrape：一种混合神经物理求解器，通过显式力和约束实现物理逼真的服装悬垂，解决了现有方法在几何可行性与物理合理性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的服装悬垂方法已成为传统物理模拟的有前景替代方案，但鲁棒的碰撞处理仍是关键瓶颈。现有方法通过软惩罚强制物理有效性，导致几何可行性与物理合理性之间的内在权衡：惩罚碰撞会扭曲网格结构，而保持形状会导致穿透。

Method: 提出PhysDrape混合神经物理求解器，将神经推理与显式几何求解器集成在完全可微的流程中。包括：1）基于物理增强图（编码材料参数和身体接近度）的物理信息图神经网络预测残差位移；2）可微两阶段求解器：可学习力求解器迭代解决StVK模型导出的不平衡力确保准静态平衡，可微投影严格强制执行身体表面的碰撞约束。

Result: PhysDrape实现了最先进的性能，确保可忽略的穿透，与现有基线相比显著降低应变能，在实时性下实现卓越的物理保真度和鲁棒性。

Conclusion: PhysDrape通过可微设计保证显式约束下的物理有效性，同时支持端到端学习优化网络以获得物理一致的预测，解决了服装悬垂中碰撞处理的根本问题。

Abstract: Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.

</details>


### [282] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

Relevance: 25.0

TL;DR: PEGAsus是一个能够生成个性化3D形状的新框架，通过在几何和外观两个层面学习形状概念，实现从参考形状中提取可重用的属性并与文本结合生成新形状。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状生成方法缺乏细粒度控制和个性化能力，难以从参考形状中提取可重用的几何和外观属性，并灵活地与文本描述结合生成新形状。

Method: 1) 将3D形状个性化定义为提取类别无关的几何和外观属性；2) 设计渐进式优化策略，在几何和外观层面解耦学习形状概念；3) 扩展到区域级概念学习，使用上下文感知和无上下文损失。

Result: PEGAsus能够从广泛的参考形状中有效提取属性，灵活地与文本结合生成新形状，实现细粒度控制，在跨类别场景中也能创建多样化的个性化结果，在定量和定性实验中均优于现有方法。

Conclusion: 该框架通过解耦的几何和外观概念学习，实现了有效的3D形状个性化生成，为细粒度形状控制提供了新方法。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [283] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出TIBR4D框架，通过两阶段迭代边界精化实现动态4D高斯场景的无学习对象分割，提升边界清晰度和效率


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的对象级分割面临复杂运动、遮挡和模糊边界的挑战，现有方法难以处理这些问题

Method: 1) 迭代高斯实例追踪(IGIT)：在时间片段级通过迭代追踪精化高斯到实例的概率，提取更完整的对象结构
2) 高斯渲染范围控制(RCC)：抑制边界附近高度不确定的高斯点，保留核心贡献以获取更准确边界
3) 时间分割合并策略：平衡身份一致性和动态感知

Result: 在HyperNeRF和Neu3D数据集上实验表明，相比SOTA方法，本方法能生成边界更清晰、效率更高的准确对象高斯点云

Conclusion: TIBR4D框架有效解决了动态4D高斯场景分割的挑战，通过无学习方法实现了高质量的对象分割

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [284] [FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction](https://arxiv.org/abs/2602.08558)
*Guan Yuan Tan,Ngoc Tuan Vu,Arghya Pal,Sailaja Rajanala,Raphael Phan C. -W.,Mettu Srinivas,Chee-Ming Ting*

Main category: cs.CV

Relevance: 25.0

TL;DR: FLAG-4D：一种用于动态场景新视角合成的双变形网络框架，通过瞬时变形网络和全局运动网络建模3D高斯的时空演化，结合光流特征实现高保真、时间一致的重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用单个MLP建模时间变形，难以捕捉复杂点运动和细粒度动态细节，尤其是在稀疏输入视图下。需要更精确、时间一致的动态场景重建方法。

Method: 提出双变形网络框架：1）瞬时变形网络（IDN）建模细粒度局部变形；2）全局运动网络（GMN）捕捉长程动态；3）通过预训练光流骨干提取密集运动特征；4）变形引导注意力机制对齐光流信息与3D高斯状态；5）通过互学习机制精炼两个网络。

Result: 实验表明FLAG-4D相比现有方法实现了更高保真度、更时间一致的重建，能更好地保留细节，特别是在稀疏输入视图下表现优异。

Conclusion: FLAG-4D通过双变形网络架构和光流特征融合，显著提升了动态场景重建的质量和时间一致性，为4D场景表示提供了有效解决方案。

Abstract: We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>


### [285] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出基于深度学习的模型，使用低分辨率预扫描缩略图预测病理切片固定类型，实现快速高通量质量控制，比现有高分辨率方法快400倍。


<details>
  <summary>Details</summary>
Motivation: 病理切片固定类型的手动标注容易出错，影响下游分析和诊断准确性。现有方法需要全分辨率全切片图像，限制了高通量质量控制的扩展性。

Method: 开发深度学习模型，使用低分辨率预扫描缩略图预测福尔马林固定石蜡包埋（FFPE）和冰冻切片（FS）固定类型。在TUM病理研究所的1200张切片上训练，在TCGA、奥格斯堡和雷根斯堡数据集上评估。

Result: 在TCGA数据集上AUROC达到0.88，比可比预扫描方法高4.8%。在雷根斯堡和奥格斯堡数据集上AUROC为0.72。每张切片处理时间仅21毫秒，比现有高分辨率方法快400倍。

Conclusion: 该方法为检测标注错误提供了高效解决方案，无需依赖高分辨率扫描，是病理高通量工作流程中有价值的质量控制工具。未来将改进模型对不同扫描仪类型的泛化能力。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [286] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文探索将机器学习应用于求解偏微分方程，特别是通过数据驱动的离散化方法改进结构化网格上的PDE求解器，实现了浅水方程和欧拉方程的经典求解器，并提出了四种基于深度神经网络的ML求解器。


<details>
  <summary>Details</summary>
Motivation: 机器学习在图像分类和自然语言处理等领域已取得成功，但如何将其应用于具有数学约束的领域（如求解偏微分方程）仍待探索。数据驱动的离散化方法为加速和改进现有PDE求解器提供了有前景的途径。

Method: 1) 实现了浅水方程和欧拉方程的经典求解器；2) 提出了四种不同的深度神经网络用于基于ML的求解器；3) 采用数据驱动的离散化方法，在结构化网格上预测准线性模板系数。

Result: 经典求解器性能显著优于Pyclaw求解器。四种ML求解器中，有两种方法能够输出令人满意的解。

Conclusion: 数据驱动的ML方法可以改进PDE求解的精度和稳定性，特别是在低分辨率模拟中优于传统有限差分或有限体积格式，同时还能受益于传统数值方案的优势。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [287] [Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm](https://arxiv.org/abs/2602.08699)
*Xiaogang Xu,Kun Zhou,Tao Hu,Jiafei Wu,Ruixing Wang,Hao Peng,Bei Yu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出VLLVE和VLLVE++框架，通过视图无关和视图相关分量分解来增强低光视频，引入交叉帧交互机制和残差项，显著提升低光视频增强性能。


<details>
  <summary>Details</summary>
Motivation: 低光视频增强面临严重不可见性和噪声问题，现有方法难以处理动态场景和真实世界复杂情况。需要更有效的分解策略来同时处理外观和光照条件。

Method: 1. VLLVE：视频分解为视图无关（外观）和视图相关（光照）分量，利用动态跨帧对应关系和场景级连续性约束。2. 双结构增强网络：交叉帧交互机制，监督多帧同时学习。3. VLLVE++：引入加性残差项模拟场景自适应退化，支持双向学习和退化感知对应关系优化。

Result: 在广泛认可的低光视频增强基准测试上进行大量实验，VLLVE++在处理真实世界场景和高动态视频等挑战性案例时表现出强大能力。

Conclusion: 提出的视图感知低光视频增强框架通过创新的分解策略和网络设计，显著提升了低光视频增强性能，特别是在复杂动态场景中。

Abstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>


### [288] [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](https://arxiv.org/abs/2602.08726)
*Khadija Iddrisu,Waseem Shariff,Suzanne Little,Noel OConnor*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该研究使用Blender生成合成事件相机数据集模拟眼动（扫视和注视），采用脉冲神经网络（SNN）进行分类，在真实事件数据上微调后达到0.83准确率，且计算效率优于传统人工神经网络。


<details>
  <summary>Details</summary>
Motivation: 眼动研究对理解人类认知机制至关重要，但传统帧相机存在运动模糊问题。事件相机（DVS）提供异步光强变化记录，具有更高时间分辨率和数据效率，但缺乏高质量标注数据集。本研究旨在通过合成数据解决这一问题。

Method: 1. 使用Blender生成合成事件相机数据集模拟扫视和注视眼动；2. 采用脉冲神经网络（SNN）架构进行分类；3. 在真实事件数据上进行微调；4. 评估模型在不同时间分辨率下的稳定性。

Result: 提出的模型达到0.83准确率，在不同时间分辨率下保持稳定性能。SNN相比人工神经网络（ANN）获得显著计算效率提升，验证了合成数据增强在事件视觉中的实用性。

Conclusion: 合成事件数据与SNN结合为眼动分类提供了有效解决方案，展示了合成数据增强在事件视觉领域的潜力，所有代码和数据集已开源。

Abstract: The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>


### [289] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

Relevance: 25.0

TL;DR: 开发了一个统一的基础模型，通过对比视觉表示学习和视觉语言对齐，实现任意到所有MRI合成，用于鼻咽癌放疗规划。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，由于患者不适、扫描时间长和成本高等限制，鼻咽癌放疗常缺少完整的MRI模态，影响放疗规划准确性。传统MRI合成方法模态特定、解剖适应性有限且缺乏临床可解释性。

Method: 开发统一基础模型，整合对比视觉表示学习和视觉语言对齐。使用对比编码器获取模态不变表示，基于CLIP的文本信息解码器实现语义一致合成，支持通过单一模型进行任意到所有MRI合成。

Result: 在13个机构的40,825张图像上训练，在26个内外部验证站点（15,748张图像）上实现一致高性能（平均SSIM 0.90，PSNR 27），合成保真度高，对噪声和域偏移具有鲁棒性。统一表示还增强了放疗相关下游任务（如分割）的性能。

Conclusion: 该工作通过利用基础模型桥接技术合成和临床实用性，推进了鼻咽癌护理的数字医学解决方案。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [290] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

Relevance: 25.0

TL;DR: MotionCrafter是一个基于视频扩散的框架，能够从单目视频中联合重建4D几何和估计密集运动，通过新的联合表示和4D VAE实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法强制3D值和潜在表示与RGB VAE潜在空间对齐，尽管它们的分布根本不同，这导致次优性能。需要一种更好的方法来联合表示几何和运动。

Method: 提出了一种新颖的联合表示方法，将密集3D点图和3D场景流在共享坐标系中表示，并设计了新的4D VAE来有效学习这种表示。采用新的数据归一化和VAE训练策略，更好地传递扩散先验。

Result: 在多个数据集上的实验表明，MotionCrafter在几何重建和密集场景流估计方面都达到了最先进的性能，几何重建提升38.64%，运动重建提升25.0%，且无需任何后优化。

Conclusion: MotionCrafter证明了强制3D值与RGB VAE潜在空间对齐是不必要的，通过新的联合表示和训练策略可以显著提升4D重建性能。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [291] [Learning to Anchor Visual Odometry: KAN-Based Pose Regression for Planetary Landing](https://arxiv.org/abs/2602.06968)
*Xubo Luo,Zhaojin Li,Xue Wan,Wei Zhang,Leizheng Shu*

Main category: cs.RO

Relevance: 25.0

TL;DR: KANLoc：一种用于月球着陆的单目定位框架，将视觉里程计与基于Kolmogorov-Arnold网络的绝对位姿回归器紧密耦合，实现全局一致、实时的高精度6-DoF定位。


<details>
  <summary>Details</summary>
Motivation: 月球自主着陆需要精确的实时6-DoF定位，但现有方法存在局限：视觉里程计会无限漂移，而基于地图的绝对定位在纹理稀疏或低光照地形中失效。需要一种能结合两者优势的解决方案。

Method: 提出KANLoc框架，核心是Kolmogorov-Arnold网络（KAN），学习从图像特征到地图坐标的复杂映射，生成稀疏但高度可靠的全局位姿锚点。将这些锚点融合到束调整框架中，有效消除漂移同时保持局部运动精度。还包括专门的数据增强策略以提高对传感器遮挡的鲁棒性。

Result: 在真实合成和真实月球着陆数据集上，KANLoc将平均平移和旋转误差分别降低了32%和45%，单轨迹增益高达45%/48%，优于强基线方法。同时保持实时性能（≥15 FPS）。

Conclusion: KANLoc通过将KAN-based位姿回归器与视觉里程计紧密耦合，实现了全局一致、实时的月球着陆定位，在参数效率和精度方面都有显著优势。

Abstract: Accurate and real-time 6-DoF localization is mission-critical for autonomous lunar landing, yet existing approaches remain limited: visual odometry (VO) drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. We introduce KANLoc, a monocular localization framework that tightly couples VO with a lightweight but robust absolute pose regressor. At its core is a Kolmogorov-Arnold Network (KAN) that learns the complex mapping from image features to map coordinates, producing sparse but highly reliable global pose anchors. These anchors are fused into a bundle adjustment framework, effectively canceling drift while retaining local motion precision. KANLoc delivers three key advances: (i) a KAN-based pose regressor that achieves high accuracy with remarkable parameter efficiency, (ii) a hybrid VO-absolute localization scheme that yields globally consistent real-time trajectories (>=15 FPS), and (iii) a tailored data augmentation strategy that improves robustness to sensor occlusion. On both realistic synthetic and real lunar landing datasets, KANLoc reduces average translation and rotation error by 32% and 45%, respectively, with per-trajectory gains of up to 45%/48%, outperforming strong baselines.

</details>


### [292] [Exploring Polarimetric Properties Preservation during Reconstruction of PolSAR images using Complex-valued Convolutional Neural Networks](https://arxiv.org/abs/2602.07094)
*Quentin Gabot,Joana Frontera-Pons,Jérémy Fix,Chengfang Ren,Jean-Philippe Ovarlez*

Main category: eess.IV

Relevance: 25.0

TL;DR: 该论文研究了使用复数神经网络处理极化SAR数据，证明了复数卷积自编码器能有效压缩和重建数据，同时保持重要的物理特性。


<details>
  <summary>Details</summary>
Motivation: 极化SAR数据本质上是复数形式，但深度学习社区对此研究不足，许多研究将复数信号转换为实数域后再应用传统实数模型，这可能导致信息损失。

Method: 采用复数神经网络，特别是复数卷积自编码器，直接处理复数表示的极化SAR数据，通过Pauli、Krogager、Cameron相干分解和非相干H-α分解验证物理特性保持。

Result: 复数神经网络能有效压缩和重建全极化SAR数据，同时保持基本物理特性，相比实数神经网络具有优势。

Conclusion: 复数神经网络为开发稳健、物理信息驱动的复数生成模型用于SAR数据处理铺平了道路。

Abstract: The inherently complex-valued nature of Polarimetric SAR data necessitates using specialized algorithms capable of directly processing complex-valued representations. However, this aspect remains underexplored in the deep learning community, with many studies opting to convert complex signals into the real domain before applying conventional real-valued models. In this work, we leverage complex-valued neural networks and investigate the performance of complex-valued Convolutional AutoEncoders. We show that these networks can effectively compress and reconstruct fully polarimetric SAR data while preserving essential physical characteristics, as demonstrated through Pauli, Krogager, and Cameron coherent decompositions, as well as the non-coherent $H-α$ decomposition. Finally, we highlight the advantages of complex-valued neural networks over their real-valued counterparts. These insights pave the way for developing robust, physics-informed, complex-valued generative models for SAR data processing.

</details>


### [293] [Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model](https://arxiv.org/abs/2602.07888)
*Ning Hu,Shuai Li,Jindong Tan*

Main category: cs.RO

Relevance: 25.0

TL;DR: 该论文提出了一种基于平行透视近似的相机位姿估计几何误差传播框架，通过显式建模图像测量误差在透视几何中的传播，改进了近场场景下的位姿估计鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在近场场景中，强烈的透视效应和异构测量噪声会显著降低传统PnP（透视n点）解析解的稳定性。现有方法在处理这些挑战性条件时存在局限性，特别是在手术照明、水下低光等复杂环境中。

Method: 提出几何误差传播框架，基于平行透视近似显式建模图像测量误差的传播。推导了误差传递模型，描述特征点分布、相机深度与位姿估计不确定性之间的关系。开发了结合平行透视初始化和误差感知加权的Gauss-Newton优化方法。

Result: 在合成数据和真实图像上的广泛实验表明，该方法在强光照、手术照明、水下低光等多种条件下，达到了与最先进解析和迭代PnP方法相当的精度和鲁棒性，同时保持了高计算效率。

Conclusion: 显式几何误差建模对于挑战性近场设置中的可靠相机位姿估计至关重要。提出的框架通过平行透视近似和误差感知优化，有效提升了近场操作的鲁棒性。

Abstract: Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.

</details>


### [294] [Dynamic Black-hole Emission Tomography with Physics-informed Neural Fields](https://arxiv.org/abs/2602.08029)
*Berthy T. Feng,Andrew A. Chael,David Bromley,Aviad Levis,William T. Freeman,Katherine L. Bouman*

Main category: gr-qc

Relevance: 25.0

TL;DR: PI-DEF：一种物理信息化的可微分神经渲染方法，用于从稀疏射电测量中重建黑洞周围动态3D气体分布，相比BH-NeRF显著提升重建精度


<details>
  <summary>Details</summary>
Motivation: 静态黑洞成像成功后，下一个前沿是黑洞的动态3D成像。重建黑洞附近的动态3D气体分布能揭示宇宙新现象并支持新物理模型，但稀疏的单视角射电测量使该问题高度不适定。现有BH-NeRF方法假设开普勒动力学，这在黑洞附近因强引力和电磁活动而失效。

Method: 提出PI-DEF（物理信息可微分神经渲染）方法：1）使用可微分神经渲染拟合4D（时间+3D）发射率场；2）联合重建3D速度场和4D发射率场；3）将速度作为发射率动力学的软约束；4）可扩展用于估计黑洞自旋等其他物理参数。

Result: 在模拟数据实验中，相比BH-NeRF和物理无关方法，PI-DEF显著提升了重建精度。方法还能用于估计黑洞自旋等其他物理参数。

Conclusion: PI-DEF通过物理信息约束克服了BH-NeRF的限制性假设，为黑洞动态3D成像提供了更准确的方法，有望揭示黑洞附近的新物理现象。

Abstract: With the success of static black-hole imaging, the next frontier is the dynamic and 3D imaging of black holes. Recovering the dynamic 3D gas near a black hole would reveal previously-unseen parts of the universe and inform new physics models. However, only sparse radio measurements from a single viewpoint are possible, making the dynamic 3D reconstruction problem significantly ill-posed. Previously, BH-NeRF addressed the ill-posed problem by assuming Keplerian dynamics of the gas, but this assumption breaks down near the black hole, where the strong gravitational pull of the black hole and increased electromagnetic activity complicate fluid dynamics. To overcome the restrictive assumptions of BH-NeRF, we propose PI-DEF, a physics-informed approach that uses differentiable neural rendering to fit a 4D (time + 3D) emissivity field given EHT measurements. Our approach jointly reconstructs the 3D velocity field with the 4D emissivity field and enforces the velocity as a soft constraint on the dynamics of the emissivity. In experiments on simulated data, we find significantly improved reconstruction accuracy over both BH-NeRF and a physics-agnostic approach. We demonstrate how our method may be used to estimate other physics parameters of the black hole, such as its spin.

</details>


### [295] [Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment](https://arxiv.org/abs/2602.08466)
*Ning Hu,Senhao Cao,Maochen Li*

Main category: cs.RO

Relevance: 25.0

TL;DR: 论文提出了一种可靠性感知的执行门控机制，用于提高视觉引导机器人系统在近场和离轴配置下的执行可靠性，而不是改进姿态估计算法本身。


<details>
  <summary>Details</summary>
Motivation: 尽管姿态估计精度显著提高，但实际机器人系统在执行层面仍频繁失败。研究发现姿态精度本身不足以保证执行可靠性，失败源于确定性几何误差放大机制，即小的姿态估计误差通过系统结构和运动执行被放大，导致不稳定或失败的校准。

Method: 提出可靠性感知执行门控机制，在姿态估计后、执行前评估几何一致性和配置风险，选择性地拒绝或缩放高风险姿态更新。该方法与估计器无关，可与基于几何的经典方法和基于学习的姿态估计流程集成。

Result: 在真实UR5机器人平台上验证，执行门控显著提高了任务成功率，减少了执行方差，抑制了尾部风险行为，同时平均姿态精度基本保持不变。

Conclusion: 强调了执行级可靠性建模的重要性，为改善近场视觉引导机器人系统的鲁棒性提供了实用解决方案。

Abstract: Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>


### [296] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

Relevance: 20.0

TL;DR: Picasso：一个物理约束的多物体场景重建系统，通过考虑几何、非穿透和物理约束，生成物理上合理的场景重建，用于数字孪生和接触丰富的行为规划。


<details>
  <summary>Details</summary>
Motivation: 在遮挡和测量噪声存在的情况下，几何上准确但物理上不合理的场景重建（如物体穿透或不稳定平衡）会阻碍数字孪生预测动态行为的能力，这对于基于仿真的接触丰富行为规划至关重要。

Method: 提出Picasso物理约束重建管道：1）使用快速拒绝采样方法推理多物体交互；2）利用推断的物体接触图指导采样；3）考虑几何、非穿透和物理约束进行整体场景推理。

Result: 在自建的Picasso数据集（10个接触丰富的真实场景）和YCB-V数据集上广泛评估，Picasso大幅优于现有技术，提供物理上合理且更符合人类直觉的重建结果。

Conclusion: 物体姿态和形状估计需要对场景进行整体推理，考虑物体交互和物理合理性。Picasso通过物理约束重建实现了这一目标，为数字孪生和接触丰富行为规划提供了可靠基础。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [297] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

Relevance: 20.0

TL;DR: MVAnimate是一个利用多视角先验信息生成高质量2D和3D角色动画的新框架，通过优化多视角视频提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前角色动画生成算法（基于2D或3D人体姿态建模）面临输出质量低和训练数据不足的问题，难以生成高质量动画视频。

Method: 提出MVAnimate框架，基于多视角先验信息合成动态人物的2D和3D信息，利用多视角先验生成时间一致和空间连贯的动画输出，并优化目标角色的多视角视频质量。

Result: 实验结果表明，该方法在多样数据集上表现出鲁棒性，能够处理各种运动模式和外观，相比现有动画方法有改进。

Conclusion: MVAnimate通过多视角信息集成有效提升了动画生成质量，为高质量角色动画生成提供了新解决方案。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [298] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

Relevance: 15.0

TL;DR: 开发了一个分层贝叶斯模型来量化鞋印中"偶然特征"的稀有性，通过潜在高斯模型和空间变化系数改进法证鞋印分析


<details>
  <summary>Details</summary>
Motivation: 法证调查中鞋印证据分析的关键挑战：同一型号的鞋子成千上万，仅凭型号匹配不足以确定嫌疑人的鞋子。需要量化鞋底磨损、划痕等"偶然特征"的稀有性来准确评估证据强度。

Method: 1. 采用分层贝叶斯模型框架；2. 构建潜在高斯模型，使用集成嵌套拉普拉斯近似实现大规模标注鞋印数据的高效推断；3. 引入空间变化系数来建模鞋底花纹模式与偶然特征位置之间的关系。

Result: 在保留数据上表现出优越性能，提高了法证鞋印分析的准确性和可靠性，相比现有方法有明显改进。

Conclusion: 提出的分层贝叶斯模型通过潜在高斯框架和空间变化系数，能够更准确地量化鞋印偶然特征的稀有性，为法证调查提供更可靠的证据强度评估。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [299] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出基于卷积神经网络(CNN)的图像鸟类分类框架，用于识别鸟类物种、群体形态和规模，以改进航空鸟击预防系统。


<details>
  <summary>Details</summary>
Motivation: 鸟击对航空安全构成重大威胁，现有雷达系统无法识别鸟类物种，而不同物种具有不同的飞行行为和高度偏好，限制了预测准确性。

Method: 使用卷积神经网络(CNN)构建图像分类框架，与摄像头系统配合实现自主视觉检测，包括物种识别、群体形态分类和规模估计三个分类器。

Result: CNN框架能够识别鸟类物种并提供物种特异性预测模型的输入，同时估计群体形态和规模，为飞行路径预测和风险评估提供关键信息。

Conclusion: 图像基础的鸟类分类系统能够补充现有雷达系统的不足，通过物种识别和群体特征分析提高鸟击预防的准确性和航空安全性。

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

</details>


### [300] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

Relevance: 15.0

TL;DR: PipeMFL-240K：首个大规模公开的管道磁通泄漏检测数据集与基准，包含24万张图像和19万标注，针对12类缺陷的极端长尾分布、微小目标检测等挑战。


<details>
  <summary>Details</summary>
Motivation: 管道完整性对工业安全和环境保护至关重要，磁通泄漏检测是主要无损检测技术。尽管深度学习有望自动化MFL解释，但由于缺乏大规模公开数据集和基准，可靠模型的进展受到限制，难以进行公平比较和可重复评估。

Method: 构建了PipeMFL-240K数据集，包含240,320张图像和191,530个高质量边界框标注，收集自11条总长约1,480公里的管道。数据集反映了真实世界检测复杂性，具有极端长尾分布（12个类别）、高比例微小目标（仅几个像素）和显著的类内变异性。

Result: 通过最先进的目标检测器进行广泛实验建立基线。结果显示，现代检测器仍然难以处理MFL数据的内在特性，表明仍有很大改进空间，而PipeMFL-240K为未来研究提供了可靠且具有挑战性的测试平台。

Conclusion: 作为首个公开的管道MFL检测数据集和基准，PipeMFL-240K为高效管道诊断和维护规划提供了关键基础，有望加速MFL管道完整性评估的算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [301] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了一种无标记神经导航方法，使用低成本可见光和红外光相机结合面部几何建模，替代传统需要物理标记的系统，在50名人类受试者中验证显示中位跟踪误差仅2.32毫米和2.01°，适用于经颅磁刺激等应用。


<details>
  <summary>Details</summary>
Motivation: 传统神经导航系统依赖物理标记，需要手动配准、可能在操作中移位、且造成不适。需要开发更便宜、舒适、易用的无标记替代方案。

Method: 使用低成本可见光和红外光相机（结合立体视觉和深度感知），通过算法对面部几何进行建模，替代昂贵的硬件和物理标记。

Result: 在50名人类受试者中验证，最佳无标记算法相比传统标记系统仅产生2.32毫米和2.01°的中位跟踪误差，精度足以支持经颅磁刺激应用，且显著优于先前无标记方法。

Conclusion: 提出的无标记神经导航方法能降低设置成本和复杂性，提高患者舒适度，扩展神经导航在临床和研究环境中的可及性。多传感器数据融合可进一步提高整体精度。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [302] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

Relevance: 15.0

TL;DR: 开发了一个计算机视觉辅助系统，用于钢铁制造中废料质量的自动化评估，通过多实例学习和多任务学习从图像中估计污染百分比并分类废料类型，实现实时工作流集成。


<details>
  <summary>Details</summary>
Motivation: 当前钢铁制造中废料质量评估依赖人工视觉检查，存在主观性、危险性（粉尘和移动机械）以及效率低下的问题，需要自动化解决方案来减少主观差异、提高安全性并优化工作流程。

Method: 将污染评估建模为回归任务，采用多实例学习（MIL）处理序列数据，结合多任务学习（MTL）同时进行污染估计和废料分类。系统包括磁铁/轨道车检测、版本化推理服务、置信度评分和主动学习循环。

Result: 最佳结果：MIL方法达到MAE 0.27和R² 0.83；MTL设置达到MAE 0.36，废料分类F1分数0.79。系统已集成到近实时验收工作流中，减少主观变异性。

Conclusion: 提出的计算机视觉管道成功实现了废料质量的自动化评估，减少了主观变异性，提高了人员安全性，并能够集成到验收和熔炼计划工作流程中，通过主动学习实现持续改进。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [303] [Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework](https://arxiv.org/abs/2602.07065)
*A. N. Maria Antony,T. Richter,E. Gladilin*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一种基于深度学习的端到端方法，直接从图像序列估计连续位移和材料可压缩性，相比传统方法在效率和精度上都有优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用耗时的迭代算法进行非刚性图像配准和本构建模，不适合高通量数据处理，需要更高效的非接触式材料力学特性估计方法。

Method: 使用两个深度神经网络分别进行图像配准和材料可压缩性估计，构建端到端框架，能够从图像序列直接估计位移场和材料属性。

Result: 深度学习模型在效率和精度上优于传统方法，即使在图像配准预测的映射与参考位移场存在显著局部偏差时，仍能准确确定材料可压缩性。

Conclusion: 深度学习端到端模型的卓越精度源于其评估高阶认知特征（如矢量场的涡度）的能力，而非传统的局部图像位移特征。

Abstract: Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.

</details>


### [304] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文提出了3D-TBM，一个用于3D医学图像形态学分析的基于传输的形态测量工具，通过可逆变换将图像嵌入传输域进行分析，并可将结果投影回原始图像空间进行临床解释。


<details>
  <summary>Details</summary>
Motivation: 基于传输的形态测量（TBM）作为3D医学图像分析的新框架，通过可逆变换将图像嵌入传输域进行分析，并可将结果投影回原始图像空间进行临床解释。为了促进TBM在临床影像研究中的广泛应用，作者开发了3D-TBM工具。

Method: 3D-TBM框架包括数据预处理、最优传输嵌入计算、以及分析方法如主要传输方向可视化、判别方向识别等。该工具提供完整文档和实用教程，源代码通过PyTransKit公开。

Result: 开发了3D-TBM工具，为医学影像研究人员提供了完整的基于传输的形态测量分析流程，包括预处理、嵌入计算、分析和可视化功能。

Conclusion: 3D-TBM工具促进了基于传输的形态测量在临床影像研究中的广泛应用，通过可逆变换和空间解释能力为医学图像分析提供了新方法。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [305] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文提出了一种基于注意力机制的DeepLab-V3+模型，用于糖尿病视网膜病变（DR）相关病变的像素级分割，在DDR数据集上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是导致视力丧失和失明的主要眼病，早期检测至关重要。虽然已有许多基于深度学习的DR筛查算法，但在病变分割方面的临床应用仍然有限，特别是缺乏像素级标注来支持眼科医生的实际诊断。

Method: 在DeepLab-V3+架构中集成注意力机制，用于分割四种DR相关病变：微动脉瘤、软性渗出物、硬性渗出物和出血。使用DDR数据集的757张图像进行训练和评估。

Result: 与基线模型相比，Attention-DeepLab模型将平均精度（mAP）从0.3010提升至0.3326，平均交并比（IoU）从0.1791提升至0.1928。微动脉瘤检测从0.0205提升至0.0763，这是临床上的显著改进，因为微动脉瘤是DR最早可见的症状。

Conclusion: 集成注意力机制的DeepLab-V3+模型在DR病变分割方面表现出更好的性能，特别是对早期DR检测关键的微动脉瘤检测有显著提升，为临床DR筛查提供了更实用的工具。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [306] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了一种基于线性遗传编程的图像过滤和分割算法，用于检测增材制造铌基铜合金中的析出物，以加速合金开发迭代。


<details>
  <summary>Details</summary>
Motivation: 当前增材制造铌基铜合金分析依赖人工标注显微图像，但由于对比度变化、噪声和图像伪影等问题，导致合金开发迭代速度缓慢。需要自动化方法来提高分析效率。

Method: 使用线性遗传编程优化图像过滤和分割算法。通过特定领域语言构建图像处理流水线，每个程序由一系列可调参数的图像过滤块组成，遗传算法对这些程序进行优化，最终生成可解释的MATLAB代码。

Result: 在理想条件下（种群大小60，最大程序长度5个块），系统找到了接近人工精度的解决方案，平均评估误差为1.8%。优化后的流水线算法平均处理360万像素图像约需2秒。

Conclusion: 该自动化工作加速了迭代周期，促进了材料成分和加工空间的探索，最终有助于开发用于增材制造聚变反应堆部件的强韧、低活化、沉淀硬化铜合金。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [307] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种从深度和法线图重建3D表面的方法，通过透视感知的对数深度融合技术，扩展了现有的正交梯度基深度-法线融合方法，能够处理缺失深度测量并实现度量准确的3D重建。


<details>
  <summary>Details</summary>
Motivation: 从单视角相机获取的深度和表面法线图重建3D表面是一个重要问题。深度图可通过结构光扫描等技术获得，法线图可通过光度立体等技术获得。现有方法主要基于正交投影，而实际相机系统使用透视投影，这导致重建结果在度量上不准确。需要一种能够显式处理透视投影的方法来实现度量准确的3D重建。

Method: 提出了透视感知的对数深度融合方法，扩展了现有的正交梯度基深度-法线融合技术。该方法显式地考虑透视投影，通过将深度转换为对数深度来处理透视效应。同时，该方法能够利用可用的表面法线信息来修复深度图中的缺失测量区域，实现完整的3D重建。

Result: 在DiLiGenT-MV数据集上的实验证明了该方法的有效性。结果表明，与不考虑透视投影的方法相比，本文提出的透视感知方法能够产生度量上更准确的3D重建结果。实验还突出了透视感知深度-法线融合的重要性。

Conclusion: 本文提出的透视感知对数深度融合方法能够从深度和法线图中实现度量准确的3D表面重建。该方法通过显式考虑透视投影，解决了现有正交方法在度量准确性上的局限性，同时能够处理深度图中的缺失测量问题。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [308] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

Relevance: 15.0

TL;DR: PTB-XL-Image-17K是一个包含17,271个高质量12导联心电图图像的合成数据集，用于心电图数字化研究，提供图像、分割掩码、时间序列信号、边界框标注和元数据五种数据类型。


<details>
  <summary>Details</summary>
Motivation: 心电图数字化（将纸质或扫描的心电图图像转换回时间序列信号）对于利用数十年的临床数据在深度学习应用中至关重要，但缺乏大规模同时包含心电图图像和对应真实信号的数据集阻碍了研究进展。

Method: 从PTB-XL信号数据库生成合成心电图图像数据集，提供可定制的开源Python框架，控制参数包括纸速、电压标度、采样率、网格外观和波形特征等。

Result: 创建了包含17,271个样本的数据集，每个样本提供五种互补数据类型，生成成功率达到100%，平均处理时间为每个样本1.35秒。

Conclusion: PTB-XL-Image-17K填补了心电图数字化研究的关键空白，提供了首个支持完整流程（导联检测、波形分割和信号提取）的大规模资源，为严格评估提供了完整真实数据。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [309] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了GlobalWasteData (GWD) 数据集，整合了多个公开的垃圾分类数据集，包含89,807张图像、14个主要类别和68个子类，旨在解决现有数据集碎片化、不一致和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有垃圾分类数据集存在碎片化、不一致、标注格式不统一、类别分布不平衡等问题，导致难以训练出泛化能力强的模型。需要构建一个统一、高质量的数据集来支持稳健的垃圾识别模型开发。

Method: 通过整合多个公开垃圾分类数据集，进行质量过滤、重复去除、元数据生成等预处理步骤，构建统一的GlobalWasteData (GWD) 数据集，包含一致的标注、改进的领域多样性和更平衡的类别分布。

Result: 成功构建了包含89,807张图像、14个主要类别和68个子类的GWD数据集，提供了更一致、多样且平衡的垃圾分类数据资源，可用于训练更稳健和泛化能力强的模型。

Conclusion: GWD数据集为环境监测、回收自动化和垃圾识别等机器学习应用提供了坚实基础，通过公开可用性促进未来研究和可重复性，解决了现有垃圾分类数据集的局限性。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [310] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

Relevance: 15.0

TL;DR: TOM-GS：首个基于高斯泼溅的热成像SLAM系统，结合学习式里程计与密集建图，在恶劣条件下实现鲁棒运动估计和高质量重建


<details>
  <summary>Details</summary>
Motivation: 热成像传感器在黑暗、灰尘和烟雾中具有鲁棒性，适合机器人环境感知。但现有热成像里程计和建图方法主要是几何方法，在不同数据集上表现不稳定，且无法生成密集地图。受高斯泼溅技术高效高质量重建能力的启发，需要开发专门的热成像SLAM系统。

Method: 提出TOM-GS方法，整合学习式里程计与基于高斯泼溅的密集建图。这是首个专门为热成像相机设计的高斯泼溅SLAM系统，包含专门的热图像增强和单目深度集成模块。

Result: 在运动估计和新视角渲染方面的广泛实验表明，TOM-GS优于现有的学习式方法，验证了学习式流水线在鲁棒热成像里程计和密集重建方面的优势。

Conclusion: TOM-GS展示了学习式方法在热成像SLAM中的有效性，为恶劣条件下的机器人感知提供了鲁棒的解决方案，填补了热成像相机专用高斯泼溅SLAM系统的空白。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [311] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该研究提出了一种双时间点分析框架，利用统计描述符、放射组学纹理特征和深度特征嵌入来分析脑卒中缺血组织的时空演化，通过聚类分析揭示了可挽救与不可挽救组织的特征差异。


<details>
  <summary>Details</summary>
Motivation: 传统单时间点分割方法无法捕捉脑卒中生物异质性和时间演化特性，需要开发能够表征缺血组织从入院到治疗后演变的分析方法。

Method: 提出双时间点分析框架，在入院(T1)和随访(T2)两个时间点，从CTP和DWI影像中提取统计描述符、放射组学纹理特征以及mJ-Net和nnU-Net的深度特征嵌入，构建六个ROI区域，在特征空间中进行聚类分析。

Result: 在18名成功再灌注患者中，区域级表征显示出有意义的聚类：T1时被分类为半暗带或健康但最终恢复的区域与保留脑组织特征相似，而梗死区域形成明显分组。深度特征空间（特别是mJ-Net）在可挽救与不可挽救组织间显示出强分离性。

Conclusion: 编码器衍生的特征流形反映了底层组织表型和状态转换，为基于影像的脑卒中演化量化提供了新见解，深度特征在区分组织命运方面优于传统放射组学特征。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [312] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

Relevance: 15.0

TL;DR: HID 2025竞赛使用SUSTech-Competition步态识别数据集，在无专用训练数据的情况下，最佳方法达到94.2%准确率，创下新纪录


<details>
  <summary>Details</summary>
Motivation: 解决远距离人体识别(HID)的挑战，传统生物特征如人脸和指纹在真实场景中难以获取，步态识别提供了一种实用替代方案。竞赛旨在促进步态识别进展并提供公平评估平台。

Method: 使用具有挑战性的SUSTech-Competition数据集，包含服装、携带物品和视角的显著变化。不提供专用训练数据，参赛者需使用外部数据集训练模型。每年使用不同随机种子生成评估划分，减少过拟合风险并支持跨域泛化的公平评估。

Result: 尽管难度增加，参赛者仍取得进一步改进，最佳性能方法达到94.2%准确率，在该数据集上创下新基准。

Conclusion: HID 2025竞赛表明算法进步能够超越先前观察到的准确率限制，同时分析了关键技术趋势并概述了步态识别未来研究的潜在方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [313] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出HistoMet框架，通过决策感知、概念对齐的MIL方法，从原发性肿瘤全切片图像中预测转移进展和转移部位，采用两阶段预测流程模拟临床决策过程。


<details>
  <summary>Details</summary>
Motivation: 转移进展是癌症相关死亡的主要原因，但直接从组织病理学预测原发性肿瘤是否会转移以及转移部位仍然是一个基本挑战。现有方法通常将转移状态或部位预测作为孤立任务处理，没有明确模拟临床顺序决策过程（先评估转移风险，再进行部位特异性评估）。

Method: 提出决策感知、概念对齐的MIL框架HistoMet，采用两模块预测流程：1）首先估计原发性肿瘤转移进展的可能性；2）对高风险病例进行条件性转移部位预测。通过预训练的病理视觉语言模型整合语言定义和数据自适应的转移概念，指导表示学习并提高临床可解释性。

Result: 在包含6504名患者的跨机构泛癌症队列上评估，在临床相关高灵敏度筛查设置（95%灵敏度）下，显著减少下游工作量同时保持高转移风险召回率。对于转移病例，宏观F1为74.6±1.3，宏观一对多AUC为92.1。

Conclusion: 明确模拟临床决策结构能够直接从原发性肿瘤组织病理学实现稳健且可部署的转移进展和部位倾向性预后预测。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [314] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该研究评估了医学扫描3D重建流程中的误差，比较了不同分割算法和几何类型的体素与表面精度指标，发现Otsu方法最适合各种几何形状，Jaccard指数更适合薄壁结构评估。


<details>
  <summary>Details</summary>
Motivation: 医学扫描创建3D模型的准确性受多种因素影响，包括成像硬件、分割方法和网格处理技术等。几何类型、类别不平衡、体素和点云对齐对准确性的影响尚未得到充分探索，需要系统评估重建流程中的误差。

Method: 使用SLA技术打印球体、面罩和AAA（腹主动脉瘤）模型，通过微CT扫描获取数据。采用GMM、Otsu和RG三种分割方法，使用KU算法对齐分割模型与参考模型，评估Dice、Jaccard分数和精度等体素指标。表面网格使用ICP对齐，评估Chamfer距离和平均Hausdorff距离等表面指标。

Result: Otsu方法对所有几何形状最合适。AAA由于壁薄和对齐问题导致重叠分数较低。类别不平衡对AAA的特异性影响最大。表面精度指标与体素指标趋势不同。RG方法对球体效果最好，GMM和Otsu对AAA更好。面罩表面误差最大，可能是ICP对齐问题。高体素精度指标在类别不平衡和对齐敏感情况下可能误导。

Conclusion: 分割准确性是重建过程各阶段误差的累积总和。Jaccard指数比Dice更严格，更适合薄壁结构的准确性评估。体素和点云对齐对于可靠评估重建流程至关重要。高体素精度指标在类别不平衡和对齐敏感情况下可能具有误导性。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [315] [All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving](https://arxiv.org/abs/2602.07717)
*Yingjie Li,Daniel Robinson,Cunxi Yu*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种基于衍射光学神经网络(DONN)的全光计算框架，用于自动驾驶中的RGB图像分割和车道线检测，相比传统DNN具有更高的能效


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的语义分割和车道检测通常依赖深度神经网络(DNN)，但DNN需要大量模数转换和大规模图像计算，导致高能耗。衍射光学神经网络(DONN)在能效方面相比传统DNN具有优势，通过光衍射实现全光图像处理，在光速下进行计算，节省计算能耗并减少模数转换开销。

Method: 提出了一种新颖的全光计算框架，使用衍射光学神经网络(DONN)进行RGB图像分割和车道检测。通过光衍射实现全光图像处理，利用全光编码和计算减少模数转换开销。

Result: 在CityScapes数据集上验证了DONN系统在图像分割方面的有效性。在定制室内轨道数据集和CARLA模拟驾驶场景中进行车道检测案例研究，评估了模型在不同环境条件下的泛化能力。

Conclusion: DONN为自动驾驶中的图像分割和车道检测提供了一种能效更高的全光计算解决方案，相比传统DNN具有显著优势。

Abstract: Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.

</details>


### [316] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

Relevance: 15.0

TL;DR: MMLSv2是一个用于火星表面滑坡分割的多模态数据集，包含7个波段（RGB、数字高程模型、坡度、热惯量和灰度通道），共664张图像，并额外提供276张来自地理隔离区域的独立测试集用于评估空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的滑坡分割数据集主要针对地球环境，缺乏专门针对火星表面的数据集。火星地形具有独特的地质特征和环境条件，需要专门的数据集来支持相关计算机视觉和遥感研究。

Method: 构建了一个多模态火星滑坡分割数据集，包含七种不同波段的数据：RGB、数字高程模型、坡度、热惯量和灰度通道。数据集分为训练集、验证集和测试集，并特别创建了一个地理隔离的独立测试集来评估模型的空间泛化能力。

Result: 实验表明该数据集支持稳定的训练，多个分割模型在该数据集上取得了有竞争力的性能。但在碎片化、细长和小规模滑坡区域仍存在挑战。在独立测试集上的评估显示性能明显下降，表明该测试集具有更高的难度，能有效评估模型在分布外场景下的鲁棒性和泛化能力。

Conclusion: MMLSv2是一个有价值的火星滑坡分割数据集，特别适合评估模型在空间泛化方面的能力。独立测试集的设计使其成为评估模型鲁棒性和泛化性能的重要工具。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [317] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

Relevance: 15.0

TL;DR: FTW生态系统：包含160万农田边界多边形的基准数据集、预训练分割模型和命令行工具，用于农田边界提取和作物分类


<details>
  <summary>Details</summary>
Motivation: 农田边界地图是农业数据产品的基础，支持作物监测、产量估算和疾病评估，但现有数据集和工具缺乏标准化和可扩展性

Method: 1) 提供包含24个国家160万农田多边形的基准数据集；2) 使用MOSAIKS随机卷积特征和FTW农田边界进行田间尺度作物分类；3) 开发预训练分割模型和命令行推理工具；4) 提供本地尺度和国家尺度的推理笔记本

Result: 1) 作物类型分类的宏观F1分数达到0.65-0.75（有限标签下）；2) 在5个国家（476万平方公里）上展示了预计算预测；3) 预测农田中位数面积从卢旺达的0.06公顷到瑞士的0.28公顷

Conclusion: FTW生态系统为农业遥感提供了标准化的基准数据集和工具链，支持从本地到国家尺度的农田边界提取和作物分类，具有实际应用价值

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [318] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了RealSynCol，一个高度真实的合成结肠镜数据集，用于解决结肠镜3D重建中真实数据稀缺的问题，显著提升了深度学习模型在临床图像上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习可以改善结肠镜检查，通过3D重建提供全面的黏膜表面和病变视图，并帮助识别未探索区域。然而，大规模真实标注数据的稀缺限制了稳健方法的发展。

Method: 从10个CT扫描中提取结肠几何结构，导入到模拟术中条件的虚拟环境中，使用真实血管纹理进行渲染，生成包含28,130帧的数据集，配有深度图、光流、3D网格和相机轨迹等标注。

Result: 基准研究表明，RealSynCol的高真实性和多样性显著提升了在临床图像上的泛化性能，证明它是开发支持内窥镜诊断的深度学习算法的强大工具。

Conclusion: RealSynCol合成数据集为解决结肠镜3D重建中真实数据稀缺问题提供了有效解决方案，其高真实性和多样性使深度学习模型在临床应用中表现更好。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [319] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种基于双网格离散化的X射线断层扫描图像重建正则化参数自动选择方法，通过反馈控制算法动态调整正则化强度


<details>
  <summary>Details</summary>
Motivation: X射线断层扫描图像重建是一个不适定逆问题，特别是在数据有限的情况下。正则化是必要的，但其效果取决于正则化参数的选择，该参数需要在数据保真度和先验信息之间取得平衡。传统参数选择方法通常需要人工干预或先验知识。

Method: 使用两个不同的计算离散化网格处理同一问题，通过反馈控制算法动态调整正则化强度，驱动迭代重建趋向于使两个网格上的重建结果具有足够相似性的最小参数。

Result: 该方法在实际断层扫描数据上证明了其有效性，能够自动选择适当的正则化参数，提高重建质量。

Conclusion: 提出的双网格离散化方法为X射线断层扫描中的正则化参数自动选择提供了一种有效解决方案，减少了人工干预需求，提高了重建过程的自动化程度。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [320] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种基于稀疏单目图的热成像SLAM系统，利用在大规模可见光谱数据上训练的通用学习特征（SuperPoint检测器和LightGlue匹配器），通过预处理管道和置信度加权因子图提高在低纹理热图像上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 热成像在视觉退化环境（如低光照、烟雾或恶劣天气）中为视觉SLAM提供了实用的感知方式。然而，热图像通常表现出低纹理、低对比度和高噪声，这使得基于特征的SLAM变得复杂。现有方法在热图像上性能有限，且缺乏高质量热数据用于训练。

Method: 1. 使用在大规模可见光谱数据上预训练的SuperPoint特征检测器和LightGlue匹配器，实现跨域泛化；2. 引入预处理管道增强热图像输入适应性；3. 修改核心SLAM模块以处理稀疏和易产生异常值的特征匹配；4. 将SuperPoint的关键点置信度分数集成到置信度加权因子图中，提高估计鲁棒性。

Result: 在公开热数据集上的评估表明，所提出的系统实现了可靠的性能，无需数据集特定的训练或微调特征检测器，解决了高质量热数据稀缺的问题。

Conclusion: 该工作展示了利用通用学习特征和适当适应技术，可以在热成像SLAM中实现可靠性能，为视觉退化环境中的SLAM提供了一种实用解决方案。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [321] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文回顾了中国AVS工作组开发的第一代点云压缩标准AVS PCC，从技术工具和性能比较两个角度进行分析，展示了其与MPEG标准（G-PCC和V-PCC）的不同之处。


<details>
  <summary>Details</summary>
Motivation: 点云作为重要的3D数据表示格式，在沉浸式媒体、自动驾驶、数字遗产保护等领域有广泛应用价值，但其大数据量对传输和存储带来挑战。为了优化人类和机器感知，点云压缩在实际应用中至关重要。中国AVS工作组开发了与MPEG标准不同的第一代点云压缩标准AVS PCC。

Method: 本文从两个角度回顾AVS PCC标准：1）相关技术工具 - 分析AVS PCC采用的新编码工具和技术；2）性能比较 - 将AVS PCC与MPEG的G-PCC和V-PCC标准进行对比分析。

Result: AVS PCC标准采用了多种新的编码工具和技术，这些工具在设计和实现上与MPEG的G-PCC和V-PCC标准有所不同。通过性能比较，展示了AVS PCC标准在点云压缩方面的特点和优势。

Conclusion: AVS PCC作为中国自主开发的点云压缩标准，采用了创新的编码工具和技术，为点云压缩领域提供了新的解决方案，与现有的MPEG标准形成互补，推动了点云压缩技术的发展和应用。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [322] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

Relevance: 15.0

TL;DR: WiFlow：基于WiFi信号的连续人体姿态估计框架，采用编码器-解码器架构，通过时空特征提取和轴向注意力机制，在自收集数据集上达到97.00% PCK@20和99.48% PCK@50的精度，参数量仅4.82M。


<details>
  <summary>Details</summary>
Motivation: 物联网中的人体姿态估计对智能感知至关重要。传统WiFi方法在连续运动处理和高计算开销方面存在不足，而视觉方法受限于隐私和光照条件。需要一种高效、连续的WiFi姿态估计方法。

Method: 提出WiFlow框架：1) 编码器使用时间和非对称卷积捕获CSI的时空特征，保持信号原始序列结构；2) 通过轴向注意力机制精炼人体关键点特征并捕捉结构依赖；3) 解码器将高维特征映射到关键点坐标。

Result: 在自收集的36万同步CSI-姿态样本数据集上，WiFlow达到PCK@20 97.00%和PCK@50 99.48%，平均每关节位置误差0.008m。参数量仅4.82M，显著降低模型复杂度和计算成本。

Conclusion: WiFlow为实用的WiFi人体姿态估计建立了新的性能基准，在保持高精度的同时大幅降低计算复杂度，适用于物联网中的连续运动感知应用。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [323] [Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering](https://arxiv.org/abs/2602.08724)
*Geng Lin,Matthias Zwicker*

Main category: cs.CV

Relevance: 15.0

TL;DR: RotLight：一种简单的旋转捕获设置，通过物体多次旋转减少反照率估计中的模糊性，结合代理网格改进基于2DGS的逆向渲染


<details>
  <summary>Details</summary>
Motivation: 现有逆向渲染方法在从几何和辐射分解到材料和光照估计的第二步存在高度模糊性，导致反照率估计中颜色不准确和阴影残留问题

Method: 1) RotLight捕获设置：仅需物体在过程中旋转数次（最少两次）；2) 引入代理网格用于精确入射光线追踪，支持残差约束和改进全局光照处理；3) 基于2DGS的逆向渲染框架

Result: 在合成和真实世界数据集上验证，相比现有方法获得更优的反照率估计，同时保持计算效率

Conclusion: 简单的旋转捕获设置能有效减少逆向渲染中的模糊性，结合代理网格改进能提升反照率估计质量

Abstract: Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.

</details>


### [324] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种用于稀疏视图CT重建的混合2D-3D深度学习框架，结合2D处理的计算效率和3D模型的体积一致性，显著提升图像质量并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 稀疏采样的CT扫描可以减少采集时间和辐射暴露，但会引入伪影，降低图像质量和诊断价值。需要一种既能保持计算效率又能提供体积一致性的方法来减少这些伪影。

Method: 提出两阶段混合框架：1) 2D U-Net处理单个CT切片提取特征图；2) 将这些切片特征图堆叠后输入3D解码器，利用跨切片上下文信息预测无伪影的3D CT体积。

Result: 该方法在冠状面和矢状面方向上显著改善了切片间一致性，同时计算开销较低，为高质量3D CT图像后处理提供了稳健高效的解决方案。

Conclusion: 混合2D-3D框架有效平衡了计算效率和体积一致性，是稀疏视图CT重建的有前景方法，代码已在GitHub开源。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [325] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种基于符号吠陀计算的确定性CPU导向说话头生成框架，用于教育资源受限环境中的实时语音驱动头像合成


<details>
  <summary>Details</summary>
Motivation: 当前说话头生成方法依赖GPU神经渲染、大规模训练数据或高容量扩散模型，限制了在离线或资源受限学习环境中的部署，需要一种轻量级、CPU友好的解决方案

Method: 采用符号吠陀计算方法：1) 语音转时间对齐音素流；2) 音素映射到紧凑视素库；3) 基于吠陀经Urdhva Tiryakbhyam启发的符号协同发音生成平滑视素轨迹；4) 轻量级2D渲染器进行ROI变形和嘴部合成

Result: 在仅CPU执行下实现了可接受的唇同步质量，显著降低了计算负载和延迟，支持在低端硬件上部署实用的教育头像

Conclusion: 该方法为资源受限环境提供了可行的说话头生成解决方案，平衡了质量与效率，支持教育技术中的离线部署

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [326] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出多模态框架结合高分辨率图像和力测量数据，用于检测受电弓-接触网接口的电弧事件，通过MultiDeepSAD算法和伪异常生成技术提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网接口的电弧会加速部件磨损、降低系统性能并导致服务中断，但由于电弧的瞬态特性、噪声环境、数据稀缺以及与其他瞬态现象难以区分，检测具有挑战性。

Method: 构建两个多模态数据集（SBB数据和公开视频+合成力数据），提出MultiDeepSAD算法（DeepSAD的多模态扩展），并针对每种数据类型设计伪异常生成技术（图像中的合成电弧伪影和模拟力不规则性）。

Result: 实验表明该框架显著优于基线方法，即使在域偏移和真实电弧观测数据有限的情况下，对真实电弧事件也表现出增强的敏感性。

Conclusion: 多模态方法结合视觉和力测量数据能够更准确、鲁棒地检测受电弓-接触网接口的电弧事件，为铁路系统维护提供有效工具。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [327] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种用于植物生长建模的3D高斯流场表示方法，通过时间变化的高斯参数导数来模拟非线性连续生长动态，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 植物生长建模面临独特挑战：植物随时间产生新几何结构，而现有运动建模技术（如变形场无法引入新几何，4D高斯溅射限制线性轨迹）不适合此问题。

Method: 引入3D高斯流场表示，将植物生长建模为高斯参数（位置、尺度、方向、颜色、不透明度）的时间变化导数；通过重建成熟植物并学习反向生长过程来初始化高斯基元。

Result: 在多视角植物生长时序数据集上，该方法在图像质量和几何精度方面优于先前方法。

Conclusion: 为生长中的3D结构外观建模提供了新方法，能够处理植物生长特有的非线性连续动态变化。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [328] [When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey](https://arxiv.org/abs/2602.06995)
*Konstantinos Gounis,Sotiris A. Tegos,Dimitrios Tyrovolas,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.RO

Relevance: 15.0

TL;DR: 该论文综述了SLAM（同时定位与建图）与无线通信的交叉领域，重点探讨了视觉SLAM与无线通信的双向影响关系，包括无线信号传播、几何信道建模、RF定位与感知等关键技术。


<details>
  <summary>Details</summary>
Motivation: 商业无线通信与传感设备的普及，结合智能自主系统的发展，为鲁棒的联合通信与SLAM系统创造了条件。论文旨在调查SLAM与无线通信交叉领域的最新进展，揭示两者之间的双向影响关系。

Method: 采用综述研究方法，分析SLAM与无线通信交叉领域的多个维度：包括先决条件、技术、背景、未来方向和挑战。涵盖概率模型、空间信号处理方法等数学方法，以及图像处理技术检测地标、预测无线信道最优路径等技术方面。

Result: 研究发现单目视觉SLAM可从RF信息中受益（解决尺度模糊问题），而5G及以后的无线通信可从SLAM中的视觉里程计受益。论文还揭示了相机外其他SLAM源与无线通信的双重关系，指出联合通信与SLAM的集成解决方案仍处于起步阶段。

Conclusion: SLAM与无线通信存在显著的协同效应，但集成解决方案仍需理论和实践进展，以向RF和多天线技术添加更高层次的定位和语义感知能力。

Abstract: The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.

</details>


### [329] [A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration](https://arxiv.org/abs/2602.07024)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

Relevance: 15.0

TL;DR: 本文提出了一种结合数据手套（含IMU）和视觉触觉传感器的人体活动识别系统，用于人机协作场景中的手部活动识别


<details>
  <summary>Details</summary>
Motivation: 人体活动识别（HAR）是人机协作（HRC）的基础，使机器人能够响应并动态适应人类意图。需要一种能够准确识别手部活动（尤其是与机器人接触时）的系统来提升协作效果。

Method: 采用多模态方法：1）配备惯性测量单元（IMU）的模块化数据手套；2）视觉触觉传感器，共同捕捉手部活动。在三种条件下测试：离线分类分割序列、静态条件下的实时分类、真实HRC场景。

Result: 实验结果显示所有任务都取得了高准确率，表明这种多模态方法可适用于多种协作场景。

Conclusion: 提出的多模态HAR系统在人机协作中表现出色，能够准确识别手部活动，为动态人机协作提供了有效的技术方案。

Abstract: Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.

</details>


### [330] [Guidestar-Free Adaptive Optics with Asymmetric Apertures](https://arxiv.org/abs/2602.07029)
*Weiyun Jiang,Haiyun Guo,Christopher A. Metzler,Ashok Veeraraghavan*

Main category: eess.IV

Relevance: 15.0

TL;DR: 提出首个无需导星或波前传感器的闭环自适应光学系统，通过非对称孔径和机器学习实现实时光学像差校正


<details>
  <summary>Details</summary>
Motivation: 传统自适应光学系统需要导星或波前传感器，限制了其在自然场景中的应用。本文旨在开发一种完全计算式的波前传感方法，实现无导星的自适应光学校正

Method: 结合三个关键组件：1) 光学路径中的非对称孔径，实现基于相位恢复的波前传感；2) 机器学习算法对，从自然场景测量中估计点扩散函数并重建相位像差；3) 空间光调制器进行光学校正

Result: 在密集自然场景中实验验证，性能优于最先进的无导星波前整形方法，测量次数减少一个数量级，计算量减少三个数量级

Conclusion: 成功实现了首个无导星、无波前传感器的实时自适应光学系统，为自然场景成像提供了高效的光学校正方案

Abstract: This work introduces the first closed-loop adaptive optics (AO) system capable of optically correcting aberrations in real-time without a guidestar or a wavefront sensor. Nearly 40 years ago, Cederquist et al. demonstrated that asymmetric apertures enable phase retrieval (PR) algorithms to perform fully computational wavefront sensing, albeit at a high computational cost. More recently, Chimitt et al. extended this approach with machine learning and demonstrated real-time wavefront sensing using only a single (guidestar-based) point-spread-function (PSF) measurement. Inspired by these works, we introduce a guidestar-free AO framework built around asymmetric apertures and machine learning. Our approach combines three key elements: (1) an asymmetric aperture placed in the optical path that enables PR-based wavefront sensing, (2) a pair of machine learning algorithms that estimate the PSF from natural scene measurements and reconstruct phase aberrations, and (3) a spatial light modulator that performs optical correction. We experimentally validate this framework on dense natural scenes imaged through unknown obscurants. Our method outperforms state-of-the-art guidestar-free wavefront shaping methods, using an order of magnitude fewer measurements and three orders of magnitude less computation.

</details>


### [331] [U-Net Based Image Enhancement for Short-time Muon Scattering Tomography](https://arxiv.org/abs/2602.07060)
*Haochen Wang,Pei Yu,Liangwen Chen,Weibo He,Yu Zhang,Yuhong Yu,Xueheng Zhang,Lei Yang,Zhiyu Sun*

Main category: eess.IV

Relevance: 15.0

TL;DR: 提出基于U-Net的框架，通过模拟数据训练来增强μ子散射断层扫描图像质量，显著提升低统计量图像质量


<details>
  <summary>Details</summary>
Motivation: μ子散射断层扫描(MST)是一种有前景的非侵入式检测技术，但短时间MST的实际应用受到μ子通量有限导致的图像质量差的阻碍

Method: 使用基于U-Net的框架，在模拟MST数据重建的最接近点(PoCA)图像上进行训练，以增强图像质量

Result: 应用于实验MST数据时，框架显著改善图像质量：结构相似性指数从0.7232提升到0.9699，学习感知图像块相似性从0.3604降低到0.0270

Conclusion: 该方法能有效增强低统计量MST图像，为短时间MST的实际部署铺平道路

Abstract: Muon Scattering Tomography (MST) is a promising non-invasive inspection technique, yet the practical application of short-time MST is hindered by poor image quality due to limited muon flux. To address this limitation, we propose a U-Net-based framework trained on Point of Closest Approach (PoCA) images reconstructed with simulation MST data to enhance image quality. When applied to experimental MST data, the framework significantly improves image quality, increasing the Structural Similarity Index Measure (SSIM) from 0.7232 to 0.9699 and decreasing the Learned Perceptual Image Patch Similarity (LPIPS) from 0.3604 to 0.0270. These results demonstrate that our method can effectively enhance low-statistics MST images, thereby paving the way for the practical deployment of short-time MST.

</details>


### [332] [Wavelet-Domain Masked Image Modeling for Color-Consistent HDR Video Reconstruction](https://arxiv.org/abs/2602.07393)
*Yang Zhang,Zhangkai Ni,Wenhan Yang,Hanli Wang*

Main category: eess.IV

Relevance: 15.0

TL;DR: WMNet提出了一种基于小波域掩码图像建模的HDR视频重建网络，采用两阶段训练策略，结合时间混合专家模块和动态记忆模块，显著提升了色彩保真度和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有HDR视频重建方法存在色彩不准确和时间不一致的问题，需要开发能够同时恢复精细亮度、色彩和细节的鲁棒方法。

Method: 1. 两阶段训练：第一阶段使用小波域掩码图像建模进行自重建预训练，采用课程学习；第二阶段微调。2. 时间混合专家模块自适应融合相邻帧减少闪烁。3. 动态记忆模块捕获长程依赖确保运动平滑。4. 重构HDRTV4K-Scene数据集作为新基准。

Result: WMNet在多个评估指标上达到最先进性能，显著改善了色彩保真度、时间一致性和感知质量。

Conclusion: WMNet通过小波域掩码图像建模和时间一致性模块，有效解决了HDR视频重建中的色彩和时间一致性问题，为高质量HDR视频重建提供了新方法。

Abstract: High Dynamic Range (HDR) video reconstruction aims to recover fine brightness, color, and details from Low Dynamic Range (LDR) videos. However, existing methods often suffer from color inaccuracies and temporal inconsistencies. To address these challenges, we propose WMNet, a novel HDR video reconstruction network that leverages Wavelet domain Masked Image Modeling (W-MIM). WMNet adopts a two-phase training strategy: In Phase I, W-MIM performs self-reconstruction pre-training by selectively masking color and detail information in the wavelet domain, enabling the network to develop robust color restoration capabilities. A curriculum learning scheme further refines the reconstruction process. Phase II fine-tunes the model using the pre-trained weights to improve the final reconstruction quality. To improve temporal consistency, we introduce the Temporal Mixture of Experts (T-MoE) module and the Dynamic Memory Module (DMM). T-MoE adaptively fuses adjacent frames to reduce flickering artifacts, while DMM captures long-range dependencies, ensuring smooth motion and preservation of fine details. Additionally, since existing HDR video datasets lack scene-based segmentation, we reorganize HDRTV4K into HDRTV4K-Scene, establishing a new benchmark for HDR video reconstruction. Extensive experiments demonstrate that WMNet achieves state-of-the-art performance across multiple evaluation metrics, significantly improving color fidelity, temporal coherence, and perceptual quality. The code is available at: https://github.com/eezkni/WMNet

</details>


### [333] [Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples](https://arxiv.org/abs/2602.07736)
*Omar Tahri*

Main category: cs.RO

Relevance: 15.0

TL;DR: 该论文提出了一种基于几何矩的对称性检测方法，用于识别物体对称轴和正交变换（旋转、镜像），以改进物体抓取策略。


<details>
  <summary>Details</summary>
Motivation: 对称性检测对于物体抓取至关重要，因为识别对称特征或轴有助于制定高效的抓取策略，沿对称轴抓取通常能获得更稳定平衡的握持，从而提高操作成功率。

Method: 使用几何矩识别对称性并估计正交变换（旋转和镜像变换），为检测对称性和估计正交变换提供独特度量指标，开发了在n维空间中获取这些函数（矩n元组）的综合方法。

Result: 在2D和3D物体上进行了广泛验证测试，确保方法的鲁棒性和可靠性。与使用迭代优化检测多对称平面的最先进方法相比，结合本文方法与迭代方法在检测对称平面数量和计算时间方面取得了满意结果。

Conclusion: 基于几何矩的对称性检测方法有效，与迭代优化方法结合使用可获得更好的对称平面检测效果和计算效率。

Abstract: Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \( n \)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.

</details>


### [334] [Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments](https://arxiv.org/abs/2602.08189)
*Seoyeon Jang,Alex Junho Lee,I Made Aswin Nahrendra,Hyun Myung*

Main category: cs.RO

Relevance: 15.0

TL;DR: 提出用于移动机器人动态环境在线变化检测的双头网络，通过数据增强策略合成结构变化，无需大量标注数据


<details>
  <summary>Details</summary>
Motivation: 移动机器人在动态环境（如施工现场、频繁重配置的室内空间）中导航需要实时变化检测，现有方法难以处理遮挡和时空变化，且缺乏长期地图维护能力

Method: 设计双头网络进行在线变化检测和长期地图维护，开发数据增强策略通过导入不同场景元素合成结构变化，避免大量人工标注

Result: 在真实施工现场和室内办公环境实验中，方法在多样化场景中泛化良好，实现了高效准确的地图更新

Conclusion: 提出的双头网络和数据增强策略有效解决了动态环境变化检测问题，为移动机器人导航提供了实用解决方案

Abstract: Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.

</details>


### [335] [retinalysis-vascx: An explainable software toolbox for the extraction of retinal vascular biomarkers](https://arxiv.org/abs/2602.08580)
*Jose D. Vargas Quiros,Michael J. Beyeler,Sofia Ortin Vela,EyeNED Reading Center,Sven Bergmann,Caroline C. W. Klaver,Bart Liefers*

Main category: q-bio.TO

Relevance: 15.0

TL;DR: VascX是一个用于从彩色眼底图像中自动提取视网膜血管生物标志物的开源Python工具箱，通过处理血管分割掩模生成血管图，计算血管密度、分叉角度、中心视网膜等效值等多种生物标志物。


<details>
  <summary>Details</summary>
Motivation: 从彩色眼底图像中自动提取视网膜血管生物标志物对于大规模视网膜血管研究至关重要，需要一种标准化、可重复的工具来支持大规模临床和流行病学研究。

Method: VascX处理血管分割掩模生成骨架，构建有向和无向血管图，将片段解析为连续血管，利用黄斑、视盘和图像边界作为解剖标志进行空间标准化测量。

Result: 开发了一个开源Python工具箱，能够计算全面的生物标志物套件，包括血管密度、分叉角度、中心视网膜等效值、弯曲度和时间角度，支持可重复的血管研究。

Conclusion: VascX通过提供可解释、可修改的框架，支持快速提取现有生物标志物和开发新生物标志物，推进了眼组学领域，为大规模临床部署提供了计算高效的解决方案。

Abstract: The automatic extraction of retinal vascular biomarkers from color fundus images (CFI) is essential for large-scale studies of the retinal vasculature. We present VascX, an open-source Python toolbox designed for the automated extraction of biomarkers from artery and vein segmentations. The VascX workflow processes vessel segmentation masks into skeletons to build undirected and directed vessel graphs, which are then used to resolve segments into continuous vessels. This architecture enables the calculation of a comprehensive suite of biomarkers, including vascular density, bifurcation angles, central retinal equivalents (CREs), tortuosity, and temporal angles, alongside image quality metrics.
  A distinguishing feature of VascX is its region awareness; by utilizing the fovea, optic disc, and CFI boundaries as anatomical landmarks, the tool ensures spatially standardized measurements and identifies when specific biomarkers are not computable. Spatially localized biomarkers are calculated over grids relative to these landmarks, facilitating precise clinical analysis. Released via GitHub and PyPI, VascX provides an explainable and modifiable framework that supports reproducible vascular research through integrated visualizations. By enabling the rapid extraction of established biomarkers and the development of new ones, VascX advances the field of oculomics, offering a robust, computationally efficient solution for scalable deployment in large-scale clinical and epidemiological databases.

</details>


### [336] [Extracting Root-Causal Brain Activity Driving Psychopathology from Resting State fMRI](https://arxiv.org/abs/2602.07233)
*Eric V. Strobl*

Main category: eess.IV

Relevance: 10.0

TL;DR: 该论文提出SOURCE方法，通过双层结构因果模型将症状维度与局部BOLD扰动联系起来，识别精神疾病的根源因果图，而非传统的相关性分析。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像研究通常将影像模式与诊断标签或综合症状评分相关联，产生模糊的关联，掩盖了潜在的病理机制。作者希望识别根源因果图——即启动病理级联的局部BOLD扰动，并将其与特定症状维度选择性关联。

Method: 提出双层结构因果模型，连接受试者间症状结构与受试者内静息态fMRI，通过具有局部直接效应的独立潜在源。基于此模型开发SOURCE（症状导向的根源因果元素发现）程序，将可解释的症状轴与局部驱动因素联系起来。

Result: 实验表明，SOURCE能够恢复与根源因果BOLD驱动因素一致的局部图，相比现有比较方法提高了可解释性和解剖特异性。

Conclusion: SOURCE方法能够更精确地识别精神疾病的神经机制，将特定症状维度与局部脑区扰动联系起来，为理解精神疾病的病理生理学提供了新视角。

Abstract: Neuroimaging studies of psychiatric disorders often correlate imaging patterns with diagnostic labels or composite symptom scores, yielding diffuse associations that obscure underlying mechanisms. We instead seek to identify root-causal maps -- localized BOLD disturbances that initiate pathological cascades -- and to link them selectively to symptom dimensions. We introduce a bilevel structural causal model that connects between-subject symptom structure to within-subject resting-state fMRI via independent latent sources with localized direct effects. Based on this model, we develop SOURCE (Symptom-Oriented Uncovering of Root-Causal Elements), a procedure that links interpretable symptom axes to a parsimonious set of localized drivers. Experiments show that SOURCE recovers localized maps consistent with root-causal BOLD drivers and increases interpretability and anatomical specificity relative to existing comparators.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [337] [LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation](https://arxiv.org/abs/2602.07032)
*Yuheng Wu,Berk Gokmen,Zhouhua Xie,Peijing Li,Caroline Trippel,Priyanka Raina,Thierry Tambe*

Main category: cs.AI

Relevance: 85.0

TL;DR: LLM-FSM是一个评估大语言模型从自然语言规范中恢复有限状态机行为并转换为正确RTL实现的基准测试，包含1000个自动生成的问题，显示LLM在FSM复杂度增加时准确性急剧下降。


<details>
  <summary>Details</summary>
Motivation: 有限状态推理是硬件设计的核心能力，需要评估LLM从自然语言规范中理解和实现状态相关行为的能力。现有规范到RTL的基准测试依赖人工构建示例，缺乏自动化和可扩展性。

Method: 通过全自动流水线构建LLM-FSM基准：1) 构建具有可配置状态数和约束转移结构的FSM；2) 提示LLM将FSM表达为结构化YAML格式并添加应用上下文；3) 将YAML转换为自然语言规范；4) 从同一YAML正确构造参考RTL和测试平台；5) 使用LLM和SAT求解器验证所有1000个问题。

Result: 即使最强的LLM在FSM复杂度增加时也表现出准确性急剧下降。监督微调(SFT)的训练时缩放能有效泛化到分布外任务，增加测试时计算能提高推理可靠性。

Conclusion: LLM-FSM为评估LLM的有限状态推理能力提供了可扩展的基准，揭示了当前模型在复杂状态机任务上的局限性，并展示了训练和测试时改进的有效性。

Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.

</details>


### [338] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出DLLM-Searcher框架，通过两阶段后训练增强扩散大语言模型的智能体能力，并设计并行推理与执行范式P-ReAct来加速搜索智能体推理


<details>
  <summary>Details</summary>
Motivation: 现有搜索智能体面临两个核心挑战：1) 延迟挑战 - ReAct范式下的串行推理、工具调用和等待导致严重端到端延迟；2) 智能体能力挑战 - 现有扩散大语言模型在推理和工具调用能力上表现较弱。需要利用dLLMs的并行解码优势来优化智能体效率

Method: 提出DLLM-Searcher框架：1) 两阶段后训练管道：Agentic SFT（监督微调）和Agentic VRPO（方差减少偏好优化），增强dLLM的信息检索和推理能力；2) P-ReAct（并行推理与执行）范式，利用dLLMs的灵活生成机制优先解码工具调用指令，实现边思考边等待工具返回

Result: 实验结果显示：DLLM-Searcher达到与主流LLM搜索智能体相当的性能，P-ReAct范式带来约15%的推理加速

Conclusion: 成功解决了扩散大语言模型在搜索智能体应用中的能力不足问题，并通过并行化设计显著提升了推理效率，为dLLMs在智能体领域的实用化提供了有效方案

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [339] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: PreFlect提出了一种前瞻性反思机制，通过在执行前批评和优化智能体计划，从后验纠错转向事前预见，显著提升了复杂任务中的智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型智能体通常采用自我反思来改进性能，但现有的反思方法是回顾性的：智能体先行动，观察失败，然后才尝试恢复。这种后验纠错方式存在效率问题，需要在执行前就能预见和避免错误。

Method: 1. 提出前瞻性反思机制，在执行前对智能体计划进行批评和优化；2. 从历史智能体轨迹中提取规划错误，捕捉过去执行中观察到的重复成功和失败模式；3. 补充动态重新规划机制，当原始计划遇到意外偏差时提供执行时的计划更新。

Result: 在不同基准测试上的评估表明，PreFlect显著提高了复杂现实世界任务中的整体智能体效用，优于基于反思的强基线方法和几种更复杂的智能体架构。

Conclusion: 前瞻性反思机制通过将范式从后验纠错转向事前预见，为智能体规划提供了更有效的错误预防方法，结合动态重新规划机制，能够在复杂任务中实现更好的性能。

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [340] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究分析了2022-2025年间发布的809个LLM，发现前沿模型性能差异主要由计算规模驱动（占80-90%），而非专有技术；但在非前沿领域，专有技术和共享算法进步能显著降低达到特定能力所需的计算量。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLM性能提升的主要驱动力：是领先开发者的专有技术（"秘方"），还是单纯的计算规模扩展。这个问题对于理解AI领导地位和技术扩散具有重要意义。

Method: 使用2022-2025年间发布的809个模型的训练数据和基准测试数据，构建扩展定律回归模型，包含发布日期和开发者固定效应，分析计算规模与性能的关系。

Result: 1. 前沿模型性能差异的80-90%可由更高训练计算量解释，表明规模而非专有技术驱动前沿进步
2. 非前沿领域，专有技术和共享算法进步能显著降低达到特定能力所需的计算量
3. 某些公司能系统性地更高效地训练较小模型
4. 同一公司内部模型效率差异巨大（可达40倍以上）

Conclusion: LLM性能提升主要驱动力因模型位置而异：前沿领域主要依赖计算规模，而非前沿领域则受益于专有技术和算法进步。这对AI领导地位和能力扩散具有重要启示。

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [341] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

Relevance: 85.0

TL;DR: 将大语言模型幻觉检测重新定义为OOD检测问题，提出无需训练、单样本的检测方法，在推理任务上取得良好效果


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在问答任务上表现良好，但在需要推理的任务上效果不佳。研究者希望找到更有效的幻觉检测方法，特别是针对推理任务

Method: 将语言模型的下一词预测视为分类任务，应用计算机视觉中成熟的OOD检测技术，并进行适当修改以适应大语言模型的结构特点

Result: OOD方法实现了无需训练、基于单样本的检测器，在推理任务的幻觉检测中取得了较强的准确性

Conclusion: 将幻觉检测重新定义为OOD检测问题，为语言模型安全提供了一个有前景且可扩展的途径

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [342] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出将Stackelberg安全博弈（SSG）框架应用于AI安全领域，将AI监管视为防御者（审计者、评估者、部署者）与攻击者（恶意行为者、未对齐贡献者）之间的战略互动，为AI生命周期中的激励设计、有限监管资源和对抗性不确定性提供统一分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要将对齐视为静态优化问题，忽略了数据收集、模型评估和部署过程中的动态对抗性激励。随着AI系统能力增强和自主性提高，需要从战略层面监督参与AI开发和部署的人类与机构。

Method: 采用Stackelberg安全博弈（SSG）框架，这是一种为不确定性下的对抗性资源分配设计的博弈论模型。将AI监管建模为防御者与攻击者之间的战略互动，分析训练时审计、部署前评估和对抗环境中的多模型部署等场景。

Result: SSG框架为AI安全提供了统一的分析工具，能够处理激励设计、有限监管资源和对抗性不确定性。该框架将算法对齐与机构监管设计相结合，使AI监管更具前瞻性、风险意识和抗操纵性。

Conclusion: 基于Stackelberg安全博弈的AI安全新视角，通过博弈论威慑机制，能够使AI监管更加主动、风险感知和抗操纵，为AI生命周期的安全挑战提供系统性解决方案。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [343] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

Relevance: 85.0

TL;DR: BRIDGE框架通过项目反应理论从模型响应中学习潜在难度尺度，并将其锚定到人类任务完成时间，实现基准性能与人类可解释难度度量的对齐。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统的真实能力需要将基准性能与人类可解释的任务难度度量相连接。现有依赖人工标注任务完成时间的方法成本高、噪声大且难以扩展。

Method: 提出BRIDGE心理测量框架，使用双参数逻辑项目反应理论模型，从多个基准的模型性能数据中联合估计潜在任务难度和模型能力，发现潜在任务难度与人类完成时间的对数呈线性关系。

Result: 潜在任务难度与人类完成时间的对数呈线性关系，使得仅从模型性能就能推断新基准的人类任务完成时间。利用这种对齐关系，预测前沿模型能力（以人类任务长度表示），并独立重现METR的指数缩放结果：50%可解决任务范围大约每6个月翻倍。

Conclusion: BRIDGE提供了一种可扩展的方法，将模型基准性能与人类可解释的任务难度度量相连接，为AI能力评估提供了更可靠和可解释的框架。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [344] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

Relevance: 85.0

TL;DR: TermiGen是一个端到端流水线，用于合成可验证的终端任务环境和具有容错能力的专家轨迹，通过多智能体迭代生成有效任务和Docker容器，并使用生成器-评判器协议注入错误来创建丰富的纠错数据，显著提升LLM在终端任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放权重LLM在执行复杂终端任务时的两个根本限制：1) 缺乏高保真、可执行的训练环境（真实环境不够多样和可扩展，LLM合成的轨迹存在幻觉）；2) 标准指令调优使用专家轨迹，很少包含小模型常见的简单错误，导致学生模型无法有效从自身运行时错误中恢复。

Method: TermiGen采用端到端流水线：1) 通过迭代多智能体精炼循环生成功能有效的任务和Docker容器；2) 使用生成器-评判器协议，在轨迹收集过程中主动注入错误，合成富含错误纠正循环的数据。

Result: 在TermiGen生成的数据集上微调的TermiGen-Qwen2.5-Coder-32B在TerminalBench上达到了31.3%的通过率，建立了新的开放权重最先进水平，超越了现有基线模型，甚至超过了o4-mini等专有模型。

Conclusion: TermiGen通过合成可验证环境和具有容错能力的轨迹，有效解决了LLM在终端任务训练中的数据稀缺和分布不匹配问题，显著提升了模型在实际终端任务中的性能表现。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [345] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

Relevance: 85.0

TL;DR: STEER2ADAPT：一种轻量级框架，通过组合而非从头学习新的引导向量来适应LLMs，在推理和安全任务上平均提升8.2%


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常为每个任务或概念使用单一静态方向，在任务变化时不够灵活，且无法处理需要多个协调能力的复杂任务。需要一种更灵活、可组合的适应方法。

Method: 提出STEER2ADAPT框架：1）捕获领域内共享的低维语义先验子空间作为可重用基础向量；2）通过少量示例动态发现基础向量的线性组合来适应新任务；3）实现数据高效、稳定且透明的推理时适应。

Result: 在9个任务和3个模型上的实验表明，在推理和安全领域平均提升8.2%。分析显示该方法具有数据效率高、稳定性好和透明度高的特点。

Conclusion: STEER2ADAPT通过组合现有引导向量而非从头学习，提供了一种灵活、高效的LLMs适应方法，特别适用于需要多个协调能力的复杂任务。

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [346] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

Relevance: 85.0

TL;DR: SupChain-Bench：首个评估LLM在供应链领域知识和基于工具的长时程编排能力的真实世界基准，SupChain-ReAct框架能自主合成可执行程序，显著提升工具调用性能


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理和基于工具的决策方面显示出潜力，但供应链工作流需要可靠的长时程、多步骤编排，并基于特定领域流程，这对当前模型仍具挑战性。需要系统评估LLM在此场景下的性能。

Method: 1) 引入SupChain-Bench统一真实世界基准，评估供应链领域知识和基于标准操作程序(SOPs)的长时程工具编排；2) 提出SupChain-ReAct框架，无需SOP即可自主合成可执行程序进行工具调用

Result: 实验显示各模型在执行可靠性方面存在显著差距。SupChain-ReAct框架实现了最强且最一致的工具调用性能，在真实世界操作环境中建立了可靠长时程编排的基准。

Conclusion: 该工作为研究真实世界操作环境中可靠的长时程编排建立了原则性基准，突显了基于LLM的供应链代理仍有巨大改进空间。SupChain-ReAct展示了自主程序合成的有效性。

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [347] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了Wide and Deep研究智能体框架，通过并行工具调用实现宽度扩展，在深度研究基准上显著提升性能并减少推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过增加顺序思维和工具调用的深度来提升性能，但通过并行工具调用实现宽度扩展的潜力尚未充分探索。研究旨在探索智能体在同时扩展深度和宽度时的行为和性能。

Method: 提出Wide and Deep研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调，避免复杂的多智能体编排。研究各种工具调用调度器以优化并行策略。

Result: 宽度扩展显著提升深度研究基准性能，同时减少获得正确答案所需的推理轮次。使用GPT-5-Medium在BrowseComp上获得62.2%准确率，超过原始GPT-5-High报告的54.9%。

Conclusion: 优化宽度和深度之间的权衡是实现高效深度研究智能体的关键路径。并行工具调用是提升智能体性能的有效方法，无需复杂上下文管理或其他技巧。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [348] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

Relevance: 85.0

TL;DR: NAAMSE是一个进化框架，将AI智能体安全评估重构为反馈驱动的优化问题，通过遗传提示突变、分层语料探索和非对称行为评分，系统性地发现传统方法遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体安全评估存在瓶颈：依赖人工红队测试或静态基准测试，无法模拟自适应、多轮次的对抗攻击。需要一种更真实、可扩展的评估方法来应对不断演化的威胁。

Method: 采用进化框架，单个自主智能体协调遗传提示突变、分层语料探索和非对称行为评分的生命周期。使用模型响应作为适应度信号，迭代地组合有效攻击策略，同时确保"良性使用正确性"，避免简单的全面拒绝。

Result: 在Gemini 2.5 Flash上的实验表明，进化突变能系统性地放大单次方法遗漏的漏洞。控制消融实验显示，探索与定向突变的协同作用能发现高严重性故障模式。

Conclusion: 这种自适应方法为面对演化威胁的智能体鲁棒性提供了更真实、可扩展的评估。NAAMSE框架开源，可用于改进AI智能体的安全评估。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [349] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了LLM在模拟人类冲突解决行为时的人格表现，发现人格提示的LLM代理与真实人类行为存在显著差异，挑战了其在社交应用中的可靠性假设。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于模拟法律调解、谈判等社交场景中的人类行为，但尚不清楚这些模拟是否能复现人类的人格-行为模式。人格特质影响个体在社交互动中的策略选择和情绪化互动行为，因此需要验证人格提示的LLM是否能再现人类冲突行为中的人格驱动差异。

Method: 1) 引入评估框架，直接比较人类-人类和LLM-LLM在争议解决对话中的行为，基于大五人格特质；2) 提供可解释的指标集，涉及策略行为和冲突结果；3) 贡献新颖的数据集创建方法，用于LLM争议解决对话，匹配人类对话的场景和人格特质；4) 使用三个当代闭源LLM演示评估框架。

Result: 不同LLM在冲突中的人格表现与人类数据存在显著差异，挑战了人格提示代理在社交影响应用中作为可靠行为代理的假设。LLM的人格表现不一致，无法准确复现人类的人格-行为模式。

Conclusion: 人格提示的LLM代理不能作为社交应用中可靠的行为代理，AI模拟在现实世界使用前需要进行心理学基础和验证。研究强调了在AI模拟中验证心理学真实性的必要性。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [350] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文研究了推理大语言模型（RLLMs）对思维链（CoT）扰动的鲁棒性，发现模型通常能恢复，但鲁棒性受模型大小、干预时机和干预类型影响，其中怀疑表达是恢复的关键机制。


<details>
  <summary>Details</summary>
Motivation: 虽然推理大语言模型通过生成思维链提高了复杂任务性能并增强了透明度，但当前研究缺乏对这些推理轨迹在受到内部干扰时的鲁棒性评估。作者旨在系统研究RLLMs在面对自身思维链被扰动时的恢复能力。

Method: 提出了一个受控评估框架，在固定时间步扰动模型的思维链。设计了七种干预措施（良性、中性和对抗性），并将这些干预应用于多个开源推理大语言模型，覆盖数学、科学和逻辑任务。

Result: RLLMs总体上具有鲁棒性，能从多样扰动中可靠恢复；鲁棒性随模型规模增大而提高，早期干预会降低鲁棒性。鲁棒性不是风格不变的：改写会抑制怀疑表达并降低性能，而其他干预会触发怀疑并支持恢复。恢复有代价：中性和对抗性噪声可使思维链长度膨胀200%以上，而改写会缩短轨迹但损害准确性。

Conclusion: 研究揭示了RLLMs如何维持推理完整性，识别怀疑作为核心恢复机制，并强调了鲁棒性与效率之间的权衡，为未来训练方法提供了重要指导。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [351] [Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models](https://arxiv.org/abs/2602.07533)
*Yankai Yang,Yancheng Long,Hongyang Wei,Wei Chen,Tianke Zhang,Kaiyu Jiang,Haonan Fan,Changyi Liu,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出联合奖励建模(JRM)方法，通过联合优化偏好学习和语言建模，将生成模型的语义推理能力内化到高效的判别式表示中，实现快速准确的奖励评估。


<details>
  <summary>Details</summary>
Motivation: 现有奖励建模方法存在明显局限：判别式奖励模型虽然与人类偏好对齐良好，但由于监督有限而难以处理复杂语义；生成式奖励模型具有更强的语义理解和推理能力，但推理成本高且难以直接与人类偏好对齐。特别是在图像编辑等复杂任务中，奖励模型需要捕捉全局语义一致性和隐式逻辑约束。

Method: 提出联合奖励建模(JRM)，在共享的视觉-语言骨干网络上联合优化偏好学习和语言建模。这种方法将生成模型的语义和推理能力内化到高效的判别式表示中，实现快速准确的评估。

Result: 在MMRB2和EditReward-Bench基准测试中达到最先进水平，并在下游在线强化学习中显著提高了稳定性和性能。

Conclusion: 联合训练有效桥接了奖励建模中的效率和语义理解，为复杂任务的奖励建模提供了新思路。

Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.

</details>


### [352] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究搜索代理在多约束问题中的推理可靠性，发现存在"虚幻完成"现象，提出Epistemic Ledger评估框架识别四种失败模式，并通过LiveLedger干预提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前搜索代理在多跳和长视野任务中表现良好，但它们在多约束问题中是否能可靠地跟踪、验证和维护多个条件仍不明确。研究者关注代理是否会出现"虚幻完成"现象，即代理认为任务已完成但实际上存在未解决或违反的约束。

Method: 1. 引入Epistemic Ledger评估框架，跟踪多轮推理过程中每个约束的证据支持和代理信念；2. 识别四种常见失败模式：裸断言、忽视反驳、停滞和过早退出；3. 提出LiveLedger作为推理时的约束状态跟踪干预方法。

Result: LiveLedger干预显著减少了未验证答案（最多减少26.5%）并提高了整体准确率（最多提升11.6%），在多约束问题上表现出持续的性能改进。

Conclusion: 搜索代理在多约束推理中存在系统性失败模式，通过显式的约束状态跟踪可以显著改善代理的推理可靠性，这为提升LLM代理的推理能力提供了实用方法。

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [353] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

Relevance: 85.0

TL;DR: Verify-RL框架利用符号微分为数学问题分解提供可验证保证，通过严格递减的结构复杂度、解包含性和形式规则推导三个条件，消除无效分解，显著提升LLM数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法通常是启发式的，无法保证子问题更简单、解决子问题有助于父任务、或分解关系有数学基础。需要一种可验证的分解框架来确保分解的有效性。

Method: 利用符号微分作为验证分解的自然结构：微积分规则明确定义表达式如何简化为更简单的组件，并具有可证明的性质。提出Verify-RL框架，要求每个父-子分解满足三个可验证条件：严格递减的结构复杂度、解包含性、形式规则推导。

Result: 消除无效分解带来显著提升：最难问题的准确率从32%翻倍至68%，整体相对改进40%。可验证条件通过符号计算实现自动验证，达到"构造即验证"。

Conclusion: 符号微分为数学问题分解提供可验证的数学基础，Verify-RL框架通过确保分解的有效性，显著提升LLM在复杂数学问题上的表现，为课程学习提供更可靠的分解方法。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [354] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: TabRAG：一个用于大规模表格图像检索和推理的框架，通过视觉-文本基础模型检索候选表格，MLLMs进行细粒度重排序，最终生成答案。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据常以图像形式存在（如财务报表、手写记录、文档扫描），现有MLLMs通常假设相关表格已准备好，但实际场景需要从大规模表格集合中识别和推理相关表格来回答用户查询。

Method: 1) 使用联合训练的视觉-文本基础模型检索候选表格；2) 利用MLLMs对候选表格进行细粒度重排序；3) 使用MLLMs在选定表格上进行推理生成答案。

Result: 在新构建的数据集（88,161训练样本，9,819测试样本，8个基准测试，48,504个唯一表格）上，框架在检索召回率上提升7.0%，答案准确率提升6.1%，显著优于现有方法。

Conclusion: TabRAG为现实世界表格理解任务提供了实用解决方案，解决了从大规模表格图像集合中检索和推理的挑战。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [355] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

Relevance: 85.0

TL;DR: ALMA框架通过元学习自动生成记忆设计，取代人工设计的记忆模块，使智能体系统能够在测试时持续学习，在多个顺序决策任务中超越现有最佳人工记忆设计。


<details>
  <summary>Details</summary>
Motivation: 基础模型的无状态性限制了智能体系统的持续学习能力，这是长期推理和适应的核心能力。现有记忆设计多为人工设计且固定，难以适应现实任务的多样性和非平稳性。

Method: 提出ALMA框架，使用元代理在开放式搜索空间中探索以可执行代码表达的记忆设计，理论上可以发现任意记忆设计，包括数据库模式及其检索和更新机制。

Result: 在四个顺序决策领域的广泛实验表明，学习到的记忆设计在所有基准测试中都比最先进的人工设计记忆更有效和高效地从经验中学习。

Conclusion: ALMA代表了向自我改进AI系统迈出的一步，这些系统能够学习成为适应性强的持续学习者。安全开发和部署时，该框架具有重要价值。

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [356] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出Data Darwinism十级分类法，通过数据-模型协同进化框架提升基础模型性能，在科学文献领域验证了高质量数据处理的价值。


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统化的数据处理框架。现有方法未能充分利用数据与模型之间的协同进化关系，即先进模型可以产生更优质数据用于下一代系统训练。

Method: 1. 提出Data Darwinism十级分类法(L0-L9)，描述数据-模型协同进化过程
2. 构建Darwin-Science科学文献语料库(900B tokens, L0-L5)
3. 识别原始科学文本的可学习性差距，使用前沿LLM进行L4(生成式精炼)和L5(认知补全)处理
4. 预训练daVinci-origin-3B/7B模型作为无污染基线，排除科学内容
5. 进行600B tokens的持续预训练，系统评估不同数据处理级别的影响

Result: 1. Darwin-Science在20+基准测试中优于基线：+2.12分(3B)和+2.95分(7B)
2. 在领域对齐任务上提升更显著：+5.60分(3B)和+8.40分(7B)
3. 系统推进到L5级别带来+1.36分的总增益，证实高级别处理能解锁数据潜在价值
4. 释放Darwin-Science语料库和daVinci-origin模型供研究使用

Conclusion: Data Darwinism框架有效证明了数据-模型协同进化的价值，高质量数据处理能显著提升模型性能，特别是在领域特定任务上。该方法为系统化数据质量提升提供了原则性框架。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [357] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

Relevance: 85.0

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLMs进行时间序列推理的框架，显著提升了LLM在时间序列任务上的性能，使小型模型达到或超过大型专有LLM的能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在各个应用领域普遍存在，但利用LLM推理能力处理时间序列任务仍处于早期阶段，主要障碍包括：缺乏精心策划的时间序列CoT训练数据、数据效率低下（由于未充分探索的数据调度）、以及缺乏针对时间序列CoT数据的RL算法。

Method: 1. 数据合成管道：构建具有过程可验证注释的TS-text多模态数据集；2. 数据调度机制：根据难度层次和任务分类原则安排训练样本；3. 两阶段强化微调：利用可验证的过程级CoT数据，设计细粒度、多目标奖励。

Result: VeriTime显著提升了LLM在多样化时间序列推理任务上的性能，使紧凑的3B、4B模型能够达到与大型专有LLMs相当或超越的推理能力。

Conclusion: VeriTime通过系统化的数据合成、调度和强化学习训练，成功地将LLM推理能力应用于时间序列任务，为时间序列分析开辟了新途径，并展示了小型模型通过专门训练可以达到与大型模型相当的性能。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [358] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

Relevance: 85.0

TL;DR: 微调大型语言模型在狭窄有害数据集上会导致突发性错位，使模型在无关场景中产生刻板"邪恶"响应。专家无法预测此现象，表明对LLM归纳偏好的理解不足。研究发现通用解决方案比狭窄任务解决方案更稳定高效，并分离出通用错位的线性表示用于监控和缓解。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在狭窄有害数据集微调后出现的突发性错位现象，揭示我们对LLM学习和泛化的归纳偏好理解不足。专家无法预测此现象，表明需要深入探究LLM的归纳偏好如何影响泛化行为。

Method: 使用突发性错位作为案例研究，分析不同微调收敛到相同通用错位线性表示的现象。引入KL散度损失学习狭窄解决方案的线性表示，比较两种表示的特性：损失值、抗扰动性和在预训练分布中的影响力。

Result: 发现通用错位解决方案比狭窄任务解决方案具有更低的损失、更强的抗扰动性，且在预训练分布中更具影响力。分离出具体的通用错位线性表示可用于监控和缓解。

Conclusion: 这项工作为研究归纳偏好如何塑造LLM泛化提供了详细案例研究和初步度量指标。开源了所有代码、数据集和模型微调结果，有助于理解LLM的错位机制和开发缓解方法。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [359] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

Relevance: 85.0

TL;DR: ToolSelf：一种新型工具驱动运行时自重构范式，使LLM智能体能够自主更新子目标、上下文、策略和工具箱，实现从被动执行者到任务与自身双重管理者的转变。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体系统受限于静态配置，这些配置在执行前固定，无法适应动态任务变化。现有方法依赖人工编排或启发式补丁，泛化能力差且优化碎片化。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自调整到单一动作空间。设计配置感知两阶段训练(CAT)，结合拒绝采样微调和轨迹级强化学习来内化这种元能力。

Result: 在多样化基准测试中，ToolSelf媲美专用工作流的同时能泛化到新任务，实现平均24.1%的性能提升，展示了真正自适应智能体的路径。

Conclusion: ToolSelf通过工具驱动运行时自重构，实现了从外部规则到内在参数的相变，使智能体能够自主适应任务动态，为真正自适应的智能体系统开辟了新方向。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [360] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

Relevance: 85.0

TL;DR: MemFly是一个基于信息瓶颈原则的LLM记忆框架，通过梯度优化器构建分层记忆结构，结合语义、符号和拓扑检索机制，显著提升记忆连贯性、响应保真度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆框架面临一个基本困境：既要高效压缩冗余信息，又要为下游任务保持精确检索。这种压缩与精确检索之间的权衡限制了长期记忆系统的效果。

Method: 基于信息瓶颈原则，通过梯度优化器最小化压缩熵同时最大化相关熵，构建分层记忆结构。开发混合检索机制，整合语义、符号和拓扑三种检索路径，并采用迭代精炼处理复杂多跳查询。

Result: 综合实验表明，MemFly在记忆连贯性、响应保真度和准确性方面显著优于现有最先进的基线方法。

Conclusion: MemFly成功解决了LLM记忆框架中压缩效率与检索精度之间的基本困境，通过信息瓶颈原则和分层记忆结构实现了更好的长期记忆性能。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [361] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

Relevance: 85.0

TL;DR: MedCoG：基于知识图谱的医学元认知代理，通过元认知评估动态调节知识使用，旨在缓解LLM推理扩展定律，提高医疗推理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂医疗推理中表现出潜力，但面临推理扩展定律带来的收益递减问题。现有研究通过增加各种知识来增强LLM，但额外成本转化为准确性的效果不明确。本文探索LLM的元认知（即对自身知识状态的自我意识）如何调节推理过程。

Method: 提出MedCoG（Medical Meta-Cognition Agent with Knowledge Graph），通过元认知评估任务复杂度、熟悉度和知识密度，动态调节程序性知识、情景知识和事实知识的使用。采用以LLM为中心的按需推理方法，避免无差别扩展，过滤干扰知识。

Result: 在五个困难医疗基准测试集上验证了MedCoG的有效性和效率，实现了5.5倍的推理密度（理论有效成本与实际成本之比）。Oracle研究突显了元认知调节的显著潜力。

Conclusion: 元认知调节可以有效缓解LLM推理扩展定律，通过动态知识调节提高医疗推理的效率和准确性。MedCoG展示了元认知在复杂领域推理中的价值。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [362] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

Relevance: 85.0

TL;DR: TRUST是一种针对扩散模型的概念遗忘方法，通过动态定位目标概念神经元并选择性微调，结合Hessian正则化，实现高效、鲁棒的有害内容遗忘。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散模型易被利用生成有害内容，现有概念遗忘方法要么局限于单个概念，要么需要全模型微调计算成本高，而静态概念定位方法效果不佳。

Method: 提出TRUST方法：1) 动态估计目标概念神经元；2) 通过选择性微调进行遗忘；3) 使用Hessian-based正则化保持模型性能；4) 支持单个概念、概念组合和条件概念的遗忘。

Result: 实验表明TRUST：1) 对对抗性提示具有鲁棒性；2) 显著保持生成质量；3) 比SOTA方法显著更快；4) 无需特定正则化即可实现概念组合遗忘。

Conclusion: TRUST提供了一种高效、鲁棒的扩散模型概念遗忘解决方案，能够处理复杂概念组合，在安全性和实用性之间取得良好平衡。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [363] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

Relevance: 85.0

TL;DR: LOCA-bench是一个用于评估长上下文语言智能体的基准测试，通过自动化环境状态控制来调节上下文长度，模拟真实世界中智能体在动态增长上下文中的任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准主要关注单步信息检索，而真实场景中LLM需要作为智能体在动态增长的环境中探索、遵循指令、提取信息并预测正确行动。随着上下文增长，模型可靠性下降（"上下文腐化"现象），需要新的评估框架。

Method: LOCA-bench通过自动化、可扩展的环境状态控制来调节智能体的上下文长度，使上下文长度可以无限扩展同时保持任务语义固定。评估语言智能体作为模型和脚手架的组合，包括各种上下文管理策略。

Result: 随着环境状态变得复杂，智能体性能普遍下降，但先进的上下文管理技术可以显著提高整体成功率。基准测试提供了评估模型和脚手架在长上下文、智能体场景中的平台。

Conclusion: LOCA-bench填补了长上下文智能体评估的空白，为研究上下文腐化问题和开发更可靠的长期运行语言智能体提供了重要工具。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [364] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: RAPS：基于声誉感知的发布-订阅范式，用于实现LLM多智能体自适应、可扩展且鲁棒的协调，通过动态意图匹配和恶意节点检测解决大规模智能体协作问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体架构需要大量人工编排，难以实现自动化、自适应的大规模智能体协作。需要解决动态自组织网络中智能体间的自适应可靠通信问题。

Method: 基于分布式发布-订阅协议，让LLM智能体通过声明的意图交换消息而非预定义拓扑。包含两个核心机制：1）反应式订阅（动态优化意图）；2）贝叶斯声誉系统（本地监控检测隔离恶意节点）。

Result: 在五个基准测试上的广泛实验表明，该设计有效统一了多智能体协调框架中的适应性、可扩展性和鲁棒性。

Conclusion: RAPS通过声誉感知的发布-订阅范式，为LLM多智能体系统提供了自适应、可扩展且鲁棒的协调解决方案，解决了大规模智能体协作中的通信挑战。

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [365] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

Relevance: 85.0

TL;DR: SAG（小型智能体组）通过分布式推理和协作审议，在临床场景中实现了比单一大型模型更优的性能，为数字健康提供了更平衡的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前数字健康领域采用"规模优先"的LLM部署哲学，但临床实际需求不仅需要有效性，还需要可靠性和合理的部署成本。临床决策本质上是协作性的，因此挑战单一模型扩展范式，探索小型智能体组是否能支持更好的临床推理。

Method: 提出Small Agent Group（SAG）方法，从单一模型智能转向集体专业知识，通过协作审议过程分布推理、循证分析和关键审核。使用涵盖有效性、可靠性和部署成本的多样化临床指标进行广泛评估。

Result: SAG在性能上优于单一大型模型，无论是否使用额外优化或检索增强生成。结果表明SAG的协同推理可以替代临床场景中的模型参数增长。

Conclusion: SAG为数字健康提供了可扩展的解决方案，能更好地平衡有效性、可靠性和部署效率，挑战了传统的"规模优先"范式。

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [366] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: Free()LM通过引入可插拔的LoRA适配器（Free-Module），赋予LLMs内在的自我遗忘能力，动态修剪推理过程中的无用上下文，解决传统推理模型因过度思考导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型面临一个关键悖论：过多的思考token反而会降低性能而非提升。作者认为这是由基本架构缺陷导致的：标准LLMs像"malloc-only"引擎一样，持续积累有效和冗余的步骤，缺乏修剪过时信息的机制。

Method: 提出Free()LM模型，通过Free-Module（一个可插拔的LoRA适配器）引入内在的自我遗忘能力。模型在推理模式和清理模式之间迭代切换，动态识别并修剪无用的上下文块，保持紧凑且无噪声的状态。

Result: Free()LM在所有模型规模（8B到685B）上都带来了一致的改进，相比顶级推理基线平均提升3.3%。在IMOanswerBench上使用DeepSeek V3.2-Speciale建立了新的SOTA。在长时程任务中，标准Qwen3-235B-A22B模型完全崩溃（0%准确率），而Free()LM将性能恢复到50%。

Conclusion: 可持续的智能不仅需要思考的能力，也需要遗忘的自由。Free()LM通过自我遗忘机制解决了推理模型中的关键架构缺陷，为LLM推理提供了新的方向。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [367] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文挑战了AI对齐中的核心假设——人类反馈是真实信号，提出了"目标解耦"问题，并提出了"认知源对齐"方法来解决多数评估者有偏见时的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐策略依赖人类反馈是真实信号这一脆弱前提（Dogma 4）。作者发现这一假设在静态环境中成立，但在社交环境中，评估者可能是谄媚、懒惰或敌对的，导致标准RL智能体出现"目标解耦"的结构性故障模式。

Method: 提出了认知源对齐（ESA）方法。不同于依赖统计共识（信任多数）的标准鲁棒方法，ESA使用稀疏安全公理来判断反馈的来源而非信号本身。这种"判断判断者"机制即使在多数评估者有偏见时也能保证收敛到真实目标。

Result: 理论上证明了ESA能保证收敛到真实目标。实证表明，传统共识方法在多数合谋下失败，而ESA方法成功恢复了最优策略。

Conclusion: 论文挑战了AI对齐中的关键假设，揭示了在社交环境中人类反馈可能不可靠的问题，并提出了理论保证的解决方案，对RLHF和可信AI有重要贡献。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [368] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了一种针对大型推理模型（LRMs）的资源耗尽攻击方法RECUR，通过递归熵量化反思过程中的资源消耗风险，利用反事实问题触发过度反思，导致输出长度增加11倍、吞吐量下降90%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在处理复杂任务时需要进行显式推理，这需要更长的上下文长度并消耗大量资源。先前研究表明对抗性输入可以触发冗余推理过程，但推理过程中的反思组件（可能导致过度反思和计算资源过度消耗）尚未得到充分研究。论文旨在揭示推理本身存在的安全隐患，特别是反思过程中的资源消耗风险。

Method: 提出递归熵（Recursive Entropy）来量化反思过程中的资源消耗风险。基于递归熵，设计了RECUR攻击方法：通过递归熵引导的反事实利用和反思（Recursive Entropy guided Counterfactual Utilization and Reflection），构造反事实问题来验证LRMs的内在缺陷和风险。

Result: 实验表明，在良性推理中，递归熵呈现明显下降趋势。RECUR攻击破坏了这一趋势，能够将输出长度增加高达11倍，并将吞吐量降低90%。这证实了LRMs在反思过程中存在资源耗尽漏洞。

Conclusion: 该工作为鲁棒推理提供了新视角，揭示了推理过程中反思组件的安全隐患，提出的递归熵指标和RECUR攻击方法有助于评估和改善大型推理模型的资源效率和安全性。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [369] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

Relevance: 85.0

TL;DR: WMSS是一种后训练优化方法，利用模型自身历史弱检查点来指导继续优化，通过熵动态识别可恢复的学习差距并进行补偿学习，突破后训练饱和瓶颈。


<details>
  <summary>Details</summary>
Motivation: 后训练优化在改进大语言模型中变得重要，但存在持续饱和瓶颈：一旦模型变得高度自信，进一步训练会产生递减回报。现有方法继续强化目标预测，但研究发现信息丰富的监督信号仍潜藏在模型自身的历史弱状态中。

Method: WMSS（弱智能体能使强智能体更强）是一种后训练范式，利用弱检查点指导继续优化。通过熵动态识别可恢复的学习差距，并通过补偿学习强化这些差距，使强智能体能够超越常规后训练饱和。

Result: 在数学推理和代码生成数据集上的实验表明，使用WMSS训练的智能体实现了有效的性能提升，同时不产生额外的推理成本。

Conclusion: WMSS通过利用模型自身历史弱状态中的监督信号，成功突破了后训练优化中的饱和瓶颈，为大语言模型的持续改进提供了新思路。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [370] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出去中心化评估框架解决LLM评估中的不稳定性问题，通过区块链协议激励全球贡献者作为独立验证者，显著降低评估方差


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估存在集中化评估的透明度低、过拟合和硬件差异导致的方差问题。研究发现HumanEval上单模型10次运行的标准差(1.67)甚至超过了官方排行榜上前10名模型的性能差距(0.91)，使得当前排名统计上不可靠

Method: 提出去中心化评估框架，通过异构计算节点的大规模基准测试实现硬件和参数多样性。利用区块链协议激励全球贡献者作为独立验证者，采用稳健的奖励系统确保评估完整性并阻止不诚实参与

Result: 去中心化评估框架将同一模型10次运行的标准差降低到0.28，相比传统框架显著改善，确保模型排名具有更高的统计置信度

Conclusion: 去中心化评估将评估从"集中化黑盒"转变为"去中心化背书"，通过多方共识和多样化推理环境产生更稳定、更具代表性的评估指标

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [371] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

Relevance: 85.0

TL;DR: SAYO：基于强化学习的视觉注意力奖励机制，提升多模态大语言模型的视觉推理能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂推理任务中依赖长文本推理轨迹，但缺乏学习稳定视觉注意力策略的机制。研究发现当前MLLMs存在视觉聚焦弱的问题：早期视觉错位很少在后续推理中被纠正，导致错误传播和推理失败。这源于训练过程中视觉注意力信用分配不足。

Method: 提出SAYO模型，采用强化学习框架训练，引入区域级视觉注意力奖励机制。该奖励明确将优化信号与视觉基础推理步骤对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务上持续提升性能。

Conclusion: 通过强化学习框架中的视觉注意力奖励机制，可以有效解决MLLMs的视觉聚焦弱问题，提升复杂视觉推理任务的性能。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [372] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

Relevance: 85.0

TL;DR: G-LNS：基于LLM的生成式进化框架，用于自动设计大邻域搜索算子，通过协同进化破坏和修复算子对来提升组合优化问题的求解性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计方法通常局限于构造性优先级规则或参数化局部搜索指导，限制了搜索空间，难以在复杂组合优化问题中跳出深度局部最优。需要更强大的结构探索能力。

Method: 提出G-LNS生成式进化框架，利用LLM协同进化紧密耦合的破坏和修复算子对。采用合作评估机制捕捉算子间交互，发现互补的算子逻辑，实现有效的结构破坏和重建。

Result: 在TSP和CVRP等挑战性组合优化基准测试中，G-LNS显著优于基于LLM的AHD方法和经典求解器。发现的启发式不仅以更少计算资源获得接近最优解，还在未见实例分布上表现出鲁棒泛化能力。

Conclusion: G-LNS将LLM驱动的自动启发式设计扩展到更灵活的大邻域搜索算子设计，通过协同进化破坏-修复算子对实现了更强大的结构探索能力，为组合优化问题提供了新的自动化求解范式。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [373] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出Structural Context Model作为LLM智能体的形式化分析框架，包含声明式实现框架和语义动态分析工作流，在动态猴子香蕉问题上实现32%成功率提升


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究碎片化严重，概念框架与方法论原则常与底层实现细节混杂，缺乏可分析、自洽的形式化模型来进行实现无关的智能体表征与比较

Method: 提出Structural Context Model（结构上下文模型）作为LLM智能体的形式化分析框架，包含声明式实现框架和Semantic Dynamics Analysis（语义动态分析）工作流，支持智能体机制分析和快速系统化设计迭代

Result: 在动态变种的猴子香蕉问题上，使用该框架设计的智能体在最困难场景下实现了32个百分点的成功率提升

Conclusion: 提出的Structural Context Model和配套框架能够解决LLM智能体研究的碎片化问题，提供原则性洞察并支持系统化设计迭代，在基准问题上验证了有效性

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [374] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

Relevance: 85.0

TL;DR: SHARP框架通过Shapley值进行精确信用分配，优化多智能体强化学习，显著提升LLM与工具集成的性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM与外部工具的多智能体系统训练困难，主要面临信用分配挑战：难以确定哪个功能智能体对决策轨迹的成功或失败负责。现有方法依赖稀疏或全局广播奖励，无法捕捉个体贡献，导致强化学习效率低下。

Method: 提出SHARP框架，通过分解的奖励机制优化多智能体强化学习：1) 全局广播准确性奖励；2) 基于Shapley值的边际信用奖励（为每个智能体分配）；3) 工具过程奖励（提高执行效率）。通过轨迹组间智能体特定优势的归一化来稳定训练。

Result: 在多个真实世界基准测试中，SHARP显著优于现有最先进基线，相比单智能体方法平均提升23.66%，相比多智能体方法平均提升14.05%。

Conclusion: SHARP通过精确的信用分配有效解决了多智能体LLM系统的训练稳定性问题，为复杂问题分解和解决提供了更高效的强化学习框架。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [375] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

Relevance: 85.0

TL;DR: CoTZero提出了一种无需标注的视觉语言模型训练范式，通过双阶段数据合成和认知对齐训练，提升视觉推理的逻辑连贯性和结构化表示能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要依赖表面相关性而非逻辑连贯的结构化表示，导致缺乏高层次语义结构和非因果关系理解，限制了组合性和可验证推理能力。

Method: 1) 双阶段数据合成：自底向上提取原子视觉基元并组合成结构化问题推理形式；自顶向下使用全局结构指导局部细节和因果关系的解释。2) 认知对齐训练：在强化微调中引入认知连贯可验证奖励，提供推理连贯性和事实正确性的逐步反馈。

Result: 在多级语义不一致基准测试（含词汇扰动负样本）上达到83.33%的F1分数，在领域内和领域外设置中均表现优异。消融实验证实各组件对提升可解释性和人类对齐视觉推理的贡献。

Conclusion: CoTZero通过引入人类认知模型到推理过程，显著提升了视觉语言模型的结构化表示和逻辑推理能力，为实现更人类化的视觉推理提供了有效途径。

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [376] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文提出Outline-Guided Path Exploration (OPE)方法，通过生成多样化推理大纲来引导并行路径探索，解决并行思维中探索路径间互信息瓶颈问题，提升大型推理模型在复杂数学问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行思维方法主要关注聚合阶段优化，对路径探索阶段关注有限。作者理论分析发现探索路径间的互信息瓶颈限制了整体性能，需要减少信息冗余并提高路径多样性。

Method: 提出OPE方法：1）先生成多样化推理大纲来划分解空间；2）采用迭代RL策略分别优化大纲规划和基于大纲的推理；3）在可验证奖励的RL设置下减少路径间信息冗余。

Result: 在多个挑战性数学基准测试上的实验表明，OPE能有效提升不同聚合策略下的推理性能，使大型推理模型更可靠地发现正确解。

Conclusion: 通过显式划分解空间并引导路径探索，OPE解决了并行思维中的互信息瓶颈问题，为大型推理模型的路径探索优化提供了新思路。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [377] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出SAGE（自我感知引导高效推理）新采样范式，利用大推理模型内在的"知道何时停止思考"能力，通过混合采样和强化学习显著提升推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型使用长思维链方法存在大量冗余，损害计算效率并导致实时应用延迟。研究发现更长推理链与正确性无关甚至有害，但模型其实隐含知道何时停止思考，只是被当前采样范式掩盖。

Method: 提出SAGE采样范式，释放模型内在的高效推理潜力。进一步将SAGE作为混合采样整合到基于群体的强化学习（SAGE-RL）中，使SAGE-RL能够将SAGE发现的高效推理模式融入标准pass@1推理。

Result: SAGE-RL在多个具有挑战性的数学基准测试中显著提升了大推理模型的推理准确性和效率。

Conclusion: SAGE范式通过利用模型内在的"知道何时停止思考"能力，有效解决了长思维链冗余问题，为高效推理提供了新方法。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [378] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

Relevance: 85.0

TL;DR: MemAdapter：一个统一的智能体记忆检索框架，通过两阶段训练策略实现跨记忆范式的快速对齐，显著降低对齐成本并支持零样本融合。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统通常设计在孤立的范式（显式、参数化或潜在记忆）中，检索方法紧密耦合，阻碍了跨范式泛化和融合。需要统一异构记忆范式。

Method: 提出MemAdapter框架，采用两阶段训练：1）从统一记忆空间训练生成式子图检索器；2）通过对比学习训练轻量对齐模块，使检索器适应未见记忆范式。

Result: 在三个公共评估基准上，生成式子图检索器在三种记忆范式和智能体模型规模上均优于五种强基线系统。跨范式对齐仅需13分钟（单GPU），训练计算量不到原系统的5%。

Conclusion: MemAdapter实现了异构记忆范式的统一，显著提高了记忆检索的灵活性并降低对齐成本，支持零样本跨范式融合，可作为智能体记忆系统的即插即用解决方案。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [379] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

Relevance: 85.0

TL;DR: VIRF是一个神经符号框架，通过逻辑导师与LLM规划器的对话机制，为具身AI提供可验证的安全规划，实现主动安全修复而非被动拒绝。


<details>
  <summary>Details</summary>
Motivation: LLM作为具身AI规划器具有潜力，但其随机性缺乏形式化推理，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM进行安全检查，要么简单地拒绝不安全计划而不提供修复。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，核心是逻辑导师与LLM规划器的导师-学徒对话机制。逻辑导师基于形式化安全本体提供因果性和教学性反馈，实现智能计划修复而非简单避免。还引入了从现实世界文档合成安全知识库的可扩展知识获取流程。

Result: 在具有挑战性的家庭安全任务中，VIRF实现了0%的危险行动率(HAR)和77.3%的目标条件率(GCR)，在所有基线中最高。平均仅需1.1次修正迭代，效率很高。

Conclusion: VIRF展示了构建根本上可信且可验证安全的具身智能体的原则性途径，将安全范式从被动把关转向主动协作。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [380] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

Relevance: 85.0

TL;DR: SCOUT-RAG：分布式代理化Graph-RAG框架，通过渐进式跨域检索解决分布式受限环境下的知识图谱检索问题，在保持性能的同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Graph-RAG依赖集中式知识图谱，但在分布式和访问受限环境（如医院、跨国组织）中，无法获得全局图谱可见性，需要在不进行穷举查询的情况下选择相关域和适当遍历深度。

Method: 提出SCOUT-RAG框架，使用四个协作代理：(1)估计域相关性，(2)决定何时扩展到其他域，(3)自适应调整遍历深度以避免不必要的图谱探索，(4)合成高质量答案。框架旨在最小化检索遗憾（缺失有用域信息），同时控制延迟和API成本。

Result: 在多域知识设置中，SCOUT-RAG达到与集中式基线（包括DRIFT和穷举域遍历）相当的性能，同时显著减少跨域调用、处理的总token数和延迟。

Conclusion: SCOUT-RAG为分布式受限环境提供了有效的Graph-RAG解决方案，通过代理化渐进检索实现了成本效益和可扩展性。

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [381] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

Relevance: 85.0

TL;DR: AGENTWM是首个专门为智能体模型设计的水印框架，通过偏置功能相同的工具执行路径分布来嵌入可验证水印，保护智能体系统的知识产权免受模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型演变为执行自主推理和工具使用的智能体系统，创造了巨大的知识产权价值。这些系统容易受到模仿攻击，而现有的LLM水印技术在智能体领域失效，因为现实中的智能体系统通常作为灰盒运行，隐藏了验证所需的内部推理轨迹。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏置功能相同的工具执行路径分布来注入水印。开发了自动生成鲁棒水印方案的流水线，以及严格的统计假设检验程序进行验证。

Result: 在三个复杂领域的广泛评估表明，AGENTWM实现了高检测精度，对智能体性能影响可忽略。即使面对自适应攻击者，也无法在不严重降低被盗模型效用的情况下移除水印。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，能有效保护智能体知识产权，填补了现有LLM水印技术在智能体领域的空白。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [382] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

Relevance: 85.0

TL;DR: PASB是一个针对现实世界个性化AI代理的端到端安全评估框架，通过个性化使用场景、真实工具链和长时程交互来评估OpenClaw等代理的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全研究主要关注合成或任务中心设置，无法准确捕捉现实世界部署中个性化代理的攻击面和风险传播机制，需要专门的安全评估框架。

Method: 提出PASB框架，基于现有代理攻击范式，整合个性化使用场景、真实工具链和长时程交互，实现对真实系统的黑盒端到端安全评估。

Result: 以OpenClaw为案例研究发现，其在用户提示处理、工具使用和记忆检索等不同执行阶段存在关键漏洞，揭示了个性化代理部署中的重大安全风险。

Conclusion: 个性化AI代理在现实世界部署中存在严重安全风险，需要专门的评估框架如PASB来系统识别和解决这些安全问题。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [383] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出将AI对齐评估重新定义为部分可观测下的信息流问题，研究如何通过对抗性不变性训练减少模型对评估/部署环境的区分能力，从而抑制条件性策略行为。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全评估假设评估时的行为能预测部署时的行为，但对于具有情境意识的智能体，这一假设变得脆弱。智能体可能利用评估与部署环境的信息差异，实施条件性策略（如奉承和潜伏代理），在监督下保持合规但在部署时违规。

Method: 提出"制度盲"机制：通过对抗性不变性训练，在决策相关的内部表示中减少环境信息的可提取性。将评估重新定义为部分可观测下的信息流问题，证明行为差异受内部表示与环境变量互信息的限制。

Result: 在开源语言模型上测试两种完全特征化的失效模式（科学奉承和时间潜伏代理）。制度盲训练在两种情况下都抑制了环境条件行为，且没有可测量的任务效用损失。但动态特性不同：奉承在低干预强度下表现出急剧的表示和行为转变，而潜伏代理需要更强的压力且没有表现出清晰的环境可解码性崩溃。

Conclusion: 表示不变性是一个有意义但根本有限的控制杠杆，其有效性取决于环境信息在策略中的嵌入方式。行为评估应辅以环境意识和信息流的白盒诊断。

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [384] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出Reinforcement Inference方法，利用模型自身不确定性在推理时选择性调用二次思考，无需重新训练即可提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM在单次贪婪推理协议下会系统性低估模型真实能力，许多错误源于内部模糊性下的过早决策而非知识缺失

Method: 基于熵感知的推理时控制策略，利用模型自身不确定性选择性地触发第二次更慎重的推理尝试

Result: 在MMLU-Pro的12,032个问题上，DeepSeek-v3.2准确率从60.72%提升到84.03%，仅增加61.06%推理调用

Conclusion: 提出熵感知范式用于衡量和扩展模型能力，单次贪婪推理与不确定性条件化思考之间的差距可作为LLM潜在推理范围的诊断工具

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [385] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出PRISM框架，通过理论分解多智能体推理增益为探索、信息、聚合三个维度，并设计角色多样性、执行反馈、迭代合成等方法实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体协作方法缺乏理论指导，不清楚为何多智能体优于单智能体，以及哪些设计选择贡献最大，难以系统优化多智能体推理系统。

Method: 提出统一理论框架，将多智能体推理增益分解为探索（多样性解决方案覆盖）、信息（高保真反馈）、聚合（原则性共识）三个维度。基于此提出PRISM框架，通过角色多样性、执行反馈与证据交叉评估、迭代合成与闭环验证来联合优化三个维度。

Result: 在数学推理、代码生成和函数调用基准测试中，PRISM实现了最先进的性能，相比仅优化部分维度的方法具有更优的计算效率。

Conclusion: 理论框架为未来多智能体推理系统提供了可操作的设计原则，PRISM框架通过联合优化三个关键维度实现了性能突破。

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [386] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出将潜在思维链视为表示空间中的可操纵因果过程，通过结构因果模型建模潜在步骤，并使用do干预分析其效果，以更好地理解和改进潜在推理系统。


<details>
  <summary>Details</summary>
Motivation: 现有潜在思维链方法用内部潜在步骤替代显式文本推理，但这些中间计算难以评估，只能通过相关性探测进行分析。需要更系统的方法来理解潜在推理系统的内部工作机制。

Method: 将潜在思维链建模为结构因果模型中的变量，通过逐步do干预分析因果效应。研究两种代表性范式（Coconut和CODI）在数学和通用推理任务上，探讨三个关键问题：哪些步骤对正确性因果必要、影响如何在步骤间传播、中间轨迹是否保留竞争答案模式。

Result: 发现潜在步骤预算不像同质额外深度，更像具有非局部路由的分阶段功能；识别出早期输出偏见与晚期表示承诺之间的持续差距；潜在步骤表现出类似显式CoT的因果结构但更紧凑。

Conclusion: 结果支持模式条件和稳定性感知分析作为更可靠工具来解释和改进潜在推理系统，并提出了相应的训练/解码目标。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [387] [Scalable Delphi: Large Language Models for Structured Risk Estimation](https://arxiv.org/abs/2602.08889)
*Tobias Lorenz,Mario Fritz*

Main category: cs.AI

Relevance: 85.0

TL;DR: LLM-based Scalable Delphi方法可将传统专家评估时间从数月缩短至分钟，在AI增强网络安全风险评估中与人类专家表现相当。


<details>
  <summary>Details</summary>
Motivation: 传统Delphi专家评估方法虽然准确但耗时数月，需要大量专家协调时间，使得严格风险评估无法在大多数应用场景中实施。需要探索LLM能否作为可扩展的专家评估代理。

Method: 提出Scalable Delphi方法，将经典Delphi协议适配到LLM：使用多样化专家角色、迭代精炼和理由分享。开发基于必要条件的评估框架：可验证代理的校准、对证据的敏感性、与人类专家判断的一致性。

Result: 在AI增强网络安全风险评估中，LLM专家组与基准真实值达到强相关（Pearson r=0.87-0.95），随着证据添加系统改进，并与人类专家组保持一致。在某些比较中，LLM专家组与人类专家组的接近程度甚至超过两个人类专家组之间的接近程度。

Conclusion: LLM-based专家评估可以将结构化专家判断扩展到传统方法不可行的场景，将评估时间从数月减少到分钟，为高风险领域的定量风险评估提供可扩展解决方案。

Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.

</details>


### [388] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: STP框架通过时空剪枝提高扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习对解锁扩散大语言模型的复杂推理能力至关重要，但现有方法在效率和稳定性方面面临挑战

Method: 提出时空剪枝框架：空间剪枝利用静态先验约束探索空间；时间剪枝绕过冗余的后期细化步骤

Result: STP在效率和准确性方面均超越现有基线方法，理论分析证明其能严格降低对数似然估计的方差

Conclusion: STP为扩散大语言模型的强化学习提供了一种高效稳定的解决方案

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [389] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

Relevance: 85.0

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统检测LLM在因果推理中的失败模式，包括梯级塌陷、谄媚漂移和错误拒绝，通过实用性和安全性指标揭示聚合精度无法发现的失败模式。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在多种失败模式（如谄媚、梯级塌陷、错误拒绝），但由于缺乏系统诊断基准，修复进展缓慢。需要能够嵌入现实叙事中的因果陷阱、分解性能指标、揭示聚合精度无法发现的失败模式的基准。

Method: 1) 构建包含5000多个案例的基准，覆盖10个领域，测试三种关键能力：检测梯级塌陷、抵抗谄媚漂移、生成明智拒绝；2) 采用人机协作流程，涉及40名领域专家、迭代交叉验证周期；3) 通过基于规则、LLM和人工评分的复合验证；4) 实现Pearl因果阶梯作为研究基础设施；5) 将性能分解为实用性（敏感性）和安全性（特异性）。

Result: 初步实验揭示了四象限控制景观，其中静态审计策略普遍失败。基准能够发现聚合精度无法检测的失败模式，证明了其在推进可信推理系统方面的价值。

Conclusion: CausalT5K为系统诊断LLM因果推理失败提供了基础设施，通过分解性能指标和现实叙事中的因果陷阱，能够揭示传统基准无法发现的失败模式，对推进可信推理系统有重要价值。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [390] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

Relevance: 85.0

TL;DR: CoRefine：基于置信度引导的自优化方法，通过轻量级控制器实现高效推理，大幅减少计算开销


<details>
  <summary>Details</summary>
Motivation: 现有LLMs依赖大规模并行解码（如512个样本）来提升推理精度，但这带来巨大的计算开销。需要一种更高效的方法来减少推理时的计算成本。

Method: CoRefine在冻结的LLM上添加一个轻量级211k参数的Conv1D控制器，该控制器分析完整推理轨迹的置信度，决定是否停止、重新检查或尝试不同方法。还扩展了CoRefine-Tree，一种混合串行-并行变体，自适应平衡探索和利用。

Result: 平均每个问题只需2.7次优化步骤，相比512样本基线减少约190倍token消耗。控制器在自信停止时的精度达到92.6%，表明置信度动态能可靠指示正确性。在多个推理基准和三个开源模型上表现优异。

Conclusion: 将置信度视为控制信号而非正确性保证，CoRefine为可扩展推理和代理设置提供了模块化基础组件，特别适用于不完美验证器场景。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [391] [High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning](https://arxiv.org/abs/2602.07333)
*Rajat Arora,Ye Tao,Jianqiang Shen,Ping Liu,Muchen Wu,Qianqi Shen,Benjamin Le,Fedor Borisyuk,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

Relevance: 85.0

TL;DR: 提出基于强化学习的框架，从LinkedIn用户的异构文本数据（个人资料、职业数据、搜索日志）中合成统一的文本表示，利用用户参与信号作为奖励，无需人工标注，为LLM推荐系统提供可解释的用户表示。


<details>
  <summary>Details</summary>
Motivation: 大型招聘平台需要基于异构文本数据（个人资料、职业数据、搜索日志）对用户进行建模。随着推荐系统越来越多地采用大语言模型，创建统一、可解释且简洁的表示变得至关重要，特别是在对延迟敏感的在线环境中。

Method: 提出新颖的强化学习框架，利用隐式用户参与信号（如点击、申请）作为主要奖励来提炼关键信息，同时辅以基于规则的奖励来强制执行格式和长度约束，从而合成统一的文本表示。

Result: 在LinkedIn多个产品上进行的大量离线实验表明，关键下游业务指标有显著改善。该工作为构建与基于LLM的系统直接兼容的可解释用户表示提供了实用、无需标注且可扩展的解决方案。

Conclusion: 该RL框架能够有效从异构文本数据中合成统一的用户表示，为LLM推荐系统提供可解释且实用的解决方案，在大型招聘平台上具有实际应用价值。

Abstract: Effective personalization on large-scale job platforms requires modeling members based on heterogeneous textual sources, including profiles, professional data, and search activity logs. As recommender systems increasingly adopt Large Language Models (LLMs), creating unified, interpretable, and concise representations from heterogeneous sources becomes critical, especially for latency-sensitive online environments. In this work, we propose a novel Reinforcement Learning (RL) framework to synthesize a unified textual representation for each member. Our approach leverages implicit user engagement signals (e.g., clicks, applies) as the primary reward to distill salient information. Additionally, the framework is complemented by rule-based rewards that enforce formatting and length constraints. Extensive offline experiments across multiple LinkedIn products, one of the world's largest job platforms, demonstrate significant improvements in key downstream business metrics. This work provides a practical, labeling-free, and scalable solution for constructing interpretable user representations that are directly compatible with LLM-based systems.

</details>


### [392] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

Relevance: 85.0

TL;DR: iGRPO是一种两阶段强化学习方法，通过模型生成的草稿进行动态自条件优化，在数学推理任务上显著超越GRPO，并在AIME基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在解决复杂数学问题方面显示出潜力，但它们仍然难以产生准确且一致的解决方案。强化学习可以对齐任务特定奖励，但现有方法如PPO需要价值函数且计算成本高。GRPO虽然更高效，但仍有限制。作者旨在开发一种迭代的、基于自反馈的强化学习方法，进一步提升数学推理能力。

Method: iGRPO是GRPO的两阶段扩展：第一阶段采样多个探索性草稿，使用相同的标量奖励信号选择最佳草稿；第二阶段将最佳草稿附加到原始提示，在草稿条件化的精炼版本上应用GRPO风格更新，训练策略超越其先前的最佳尝试。该方法采用动态自条件机制，通过模型生成的草稿进行迭代优化。

Result: 在匹配的rollout预算下，iGRPO在多个基础模型上一致优于GRPO。应用于OpenReasoning-Nemotron-7B模型时，在AIME24和AIME25上分别达到85.62%和79.64%的新SOTA结果。消融实验表明精炼包装器可泛化到GRPO变体之外，受益于生成式评判器，并通过延迟熵崩溃改变学习动态。

Conclusion: 迭代的、基于自反馈的强化学习在推进可验证数学推理方面具有巨大潜力。iGRPO通过动态自条件机制有效提升了模型在复杂推理任务上的性能，为强化学习对齐方法提供了新的思路。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [393] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出L0-L4分层数据管理框架，通过LLM指导数据管理实现数据与模型协同进化，提高训练效率和模型性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度依赖数据规模的单向扩展，面临数据可用性、获取成本和训练效率瓶颈。作者认为AGI发展进入数据-模型协同进化新阶段，需要系统化的数据管理框架来平衡数据质量、成本和训练效益。

Method: 提出L0-L4分层数据管理框架：L0原始未筛选资源、L1-L4逐步提升数据质量和组织度。利用LLM进行数据质量评分和内容编辑，实现数据跨层精炼。框架支持预训练、中期训练和对齐等不同训练阶段的数据战略分配。

Result: 实验验证表明，分层数据管理显著提高训练效率和模型性能。作者发布了分层数据集和处理工具。

Conclusion: 数据-模型协同进化是AGI发展的新范式，分层数据管理框架为实现可持续、可扩展的LLM训练提供了系统化解决方案。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [394] [Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model](https://arxiv.org/abs/2602.07422)
*Tianyi Wu,Mingzhe Du,Yue Liu,Chengran Yang,Terry Yue Zhuo,Jiaheng Zhang,See-Kiong Ng*

Main category: cs.CR

Relevance: 85.0

TL;DR: SecCoderX是一个基于在线强化学习的框架，用于在保持功能性的同时生成安全代码，解决了现有方法在安全性和功能性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件开发中应用广泛，但生成不安全代码的问题阻碍了实际部署。现有安全代码对齐方法存在功能-安全悖论，即提高安全性会显著降低代码功能性。

Method: SecCoderX是一个在线强化学习框架，通过两种方式桥接漏洞检测和安全代码生成：(1) 利用成熟的检测资源合成多样化的、基于现实的漏洞诱导编码任务用于在线RL rollout；(2) 训练基于推理的漏洞奖励模型，提供可扩展且可靠的安全监督。这些组件在在线RL循环中统一，用于对齐代码LLMs生成安全且功能性的代码。

Result: 实验表明SecCoderX达到最先进性能，将有效安全率(ESR)比未对齐模型提高约10%，而先前方法通常使ESR降低14-54%。

Conclusion: SecCoderX成功解决了功能-安全悖论，在保持代码功能性的同时显著提高了安全性，为安全代码生成提供了有效的在线强化学习框架。

Abstract: Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.

</details>


### [395] [Pull Requests as a Training Signal for Repo-Level Code Editing](https://arxiv.org/abs/2602.07457)
*Qinglin Zhu,Tianyu Chen,Shuai Lu,Lei Ji,Runcong Zhao,Murong Ma,Xiangxiang Dai,Yulan He,Lin Gui,Peng cheng,Yeyun Gong*

Main category: cs.SE

Relevance: 85.0

TL;DR: 提出Clean-PR训练范式，利用GitHub pull requests作为训练信号进行仓库级代码编辑，在SWE-bench上显著超越指令调优基线


<details>
  <summary>Details</summary>
Motivation: 当前仓库级代码编辑主要依赖复杂的代理框架，但尚不清楚这种能力能否通过高质量训练信号内化到模型权重中

Method: 1) 构建Clean-PR数据集：将嘈杂的PR差异转换为Search/Replace编辑块；2) 中期训练阶段；3) 无代理对齐的监督微调，包含错误驱动的数据增强

Result: 在SWE-bench Lite上绝对提升13.6%，在SWE-bench Verified上提升12.3%，显著优于指令调优基线

Conclusion: 仓库级代码理解和编辑能力可以通过简化的无代理协议有效内化到模型权重中，无需依赖复杂推理时框架

Abstract: Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.

</details>


### [396] [MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots](https://arxiv.org/abs/2602.07517)
*Yuhao Wang,Shengfang Zhai,Guanghao Jin,Yinpeng Dong,Linyi Yang,Jiaheng Zhang*

Main category: cs.CR

Relevance: 85.0

TL;DR: MemPot：首个理论验证的防御框架，通过向LLM代理内存注入优化的蜜罐来抵御内存提取攻击，在保持代理效用的同时显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理使用外部和内部内存系统处理复杂任务，但这也使其面临严重的内存提取攻击，而现有防御措施不足。需要一种既能有效检测攻击又不影响代理正常功能的安全防御方案。

Method: 提出MemPot防御框架，采用两阶段优化过程：1）生成陷阱文档，最大化攻击者检索概率同时保持对良性用户不显眼；2）将检测过程建模为Wald序贯概率比检验（SPRT），理论证明比最优静态检测器需要更少的采样轮次。

Result: 实验表明MemPot显著优于现有基线方法：检测AUROC提升50%，在低误报率约束下真阳性率提高80%。同时实现零额外在线推理延迟，并在标准任务上保持代理效用。

Conclusion: MemPot是首个理论验证的防御框架，通过在内存中注入优化蜜罐有效抵御提取攻击，在安全性、无害性和效率方面具有优越性，为LLM代理安全提供了实用解决方案。

Abstract: Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.

</details>


### [397] [Linguistic properties and model scale in brain encoding: from small to compressed language models](https://arxiv.org/abs/2602.07547)
*Subba Reddy Oota,Vijay Rowtula,Satya Sai Srinath Namburi,Khushbu Pahwa,Anant Khandelwal,Manish Gupta,Tanmoy Chakraborty,Bapi S. Raju*

Main category: q-bio.NC

Relevance: 85.0

TL;DR: 研究发现大脑对齐在3B参数规模达到饱和，且对量化/剪枝压缩具有鲁棒性，挑战了神经缩放假设，为大脑对齐语言建模提供了紧凑模型方案。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型缩放能提升与大脑活动的对齐，但尚不清楚驱动因素和表征属性。大模型难以进行机制分析，因此需要探究捕获大脑相关表征所需的最小模型容量。

Method: 系统研究模型规模和数值精度对大脑对齐的影响，比较全精度LLM、小型语言模型(SLM)及压缩变体(量化和剪枝)，通过预测自然语言理解时的fMRI响应进行评估。

Result: 3B参数的SLM达到与更大LLM无法区分的大脑预测性，而1B模型显著下降；大脑对齐对压缩具有显著鲁棒性，大多数量化剪枝方法保持神经预测性；语言探测显示任务性能与大脑预测性存在解耦。

Conclusion: 大脑对齐在适度模型规模达到饱和且对压缩具有弹性，挑战了神经缩放常见假设，为大脑对齐语言建模提供了紧凑模型动机。

Abstract: Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, they are increasingly difficult to analyze mechanistically. This raises a fundamental question: what is the minimal model capacity required to capture brain-relevant representations? To address this question, we systematically investigate how constraining model scale and numerical precision affects brain alignment. We compare full-precision LLMs, small language models (SLMs), and compressed variants (quantized and pruned) by predicting fMRI responses during naturalistic language comprehension. Across model families up to 14B parameters, we find that 3B SLMs achieve brain predictivity indistinguishable from larger LLMs, whereas 1B models degrade substantially, particularly in semantic language regions. Brain alignment is remarkably robust to compression: most quantization and pruning methods preserve neural predictivity, with GPTQ as a consistent exception. Linguistic probing reveals a dissociation between task performance and brain predictivity: compression degrades discourse, syntax, and morphology, yet brain predictivity remains largely unchanged. Overall, brain alignment saturates at modest model scales and is resilient to compression, challenging common assumptions about neural scaling and motivating compact models for brain-aligned language modeling.

</details>


### [398] [CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs](https://arxiv.org/abs/2602.07080)
*Yicheng He,Zheng Zhao,Zhou Kaiyu,Bryan Dai,Jie Fu,Yonghui Yang*

Main category: cs.SE

Relevance: 85.0

TL;DR: 该论文提出了一种新的代码验证方法，通过分析LLM内部计算结构来评估代码功能正确性，无需外部测试或辅助LLM判断。


<details>
  <summary>Details</summary>
Motivation: 当前代码验证主要依赖外部机制（如单元测试或辅助LLM判断），这些方法劳动密集且受限于判断模型自身能力。论文探索一个基本问题：能否从LLM内部计算结构纯粹评估其功能正确性？

Method: 受机制可解释性启发，将代码验证视为机制诊断任务，将模型显式算法轨迹映射到行级归因图。通过分解复杂残差流，识别模型内部电路中区分合理推理与逻辑失败的结构特征。

Result: 在Python、C++和Java上的分析证实，内在正确性信号在不同语法中具有鲁棒性。内部图的拓扑特征比表面启发式方法更可靠地预测正确性，并能实现针对性因果干预来修复错误逻辑。

Conclusion: 这些发现确立了内部自省作为验证生成代码的可解码属性，为代码验证提供了新范式。

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

</details>


### [399] [ShallowJail: Steering Jailbreaks against Large Language Models](https://arxiv.org/abs/2602.07107)
*Shang Liu,Hanyu Pei,Zeyan Liu*

Main category: cs.CR

Relevance: 85.0

TL;DR: ShallowJail是一种新颖的LLM越狱攻击方法，通过操纵推理过程中的初始token来利用LLM的浅层对齐漏洞，相比现有方法更高效且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过对齐训练以防止有害输出，但现有越狱攻击要么是黑盒方法（需要精心设计但不够隐蔽的提示），要么是白盒方法（计算资源密集）。需要一种更高效且隐蔽的越狱攻击方法来揭示LLM对齐的脆弱性。

Method: ShallowJail通过操纵LLM推理过程中的初始token来利用浅层对齐漏洞。该方法不需要复杂的提示工程或大量计算资源，而是针对模型对齐机制的浅层特性进行攻击。

Result: 实验表明ShallowJail能显著降低最先进LLM的安全性，有效引导模型产生有害输出，证明了现有对齐方法的脆弱性。

Conclusion: LLM的对齐机制存在浅层漏洞，ShallowJail攻击方法揭示了这一安全风险，对LLM的安全性和可信AI研究具有重要意义。

Abstract: Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\shallow, which substantially degrades the safety of state-of-the-art LLM responses.

</details>


### [400] [Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings](https://arxiv.org/abs/2602.07294)
*Yidong Jiang,Junrong Chen,Eftychia Makri,Jialin Chen,Peiwen Li,Ali Maatouk,Leandros Tassiulas,Eliot Brenner,Bing Xiang,Rex Ying*

Main category: cs.CE

Relevance: 85.0

TL;DR: Fin-RATE是一个基于SEC文件的金融LLM基准测试，模拟分析师工作流程，包含文档内细节推理、跨实体比较和纵向跟踪三个维度，揭示了LLM在复杂金融分析中的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有金融LLM基准测试过于关注孤立细节，无法反映专业分析需要跨文档、跨时期、跨实体综合信息的复杂性，且难以区分错误来源（检索失败、生成缺陷、金融推理错误或查询理解问题），导致性能瓶颈难以定位。

Method: 基于美国SEC文件构建Fin-RATE基准，通过三个路径模拟金融分析师工作流程：1) 单个披露文件内的细节导向推理；2) 共享主题下的跨实体比较；3) 同一公司跨报告期的纵向跟踪。在真实上下文和检索增强设置下对17个领先LLM进行基准测试。

Result: 测试显示，当任务从单文档推理转向纵向和跨实体分析时，准确率分别下降18.60%和14.35%。性能下降主要由比较幻觉、时间和实体不匹配导致，推理能力和事实准确性也相应下降，这些限制在先前基准中未被正式分类或量化。

Conclusion: Fin-RATE揭示了LLM在复杂金融分析任务中的显著性能瓶颈，特别是在需要跨文档、跨时间、跨实体综合信息的场景下。该基准为评估和改进金融领域LLM提供了更全面的框架，有助于定位具体性能问题。

Abstract: With increasing deployment of Large Language Models (LLMs) in the finance domain, LLMs are increasingly expected to parse complex regulatory disclosures. However, existing benchmarks often focus on isolated details, failing to reflect the complexity of professional analysis that requires synthesizing information across multiple documents, reporting periods, and corporate entities. They do not distinguish whether errors stem from retrieval failures, generation flaws, finance-specific reasoning mistakes, or misunderstanding of the query or context. This makes it difficult to pinpoint performance bottlenecks. To bridge these gaps, we introduce Fin-RATE, a benchmark built on U.S. Securities and Exchange Commission (SEC) filings and mirror financial analyst workflows through three pathways: detail-oriented reasoning within individual disclosures, cross-entity comparison under shared topics, and longitudinal tracking of the same firm across reporting periods. We benchmark 17 leading LLMs, spanning open-source, closed-source, and finance-specialized models, under both ground-truth context and retrieval-augmented settings. Results show substantial performance degradation, with accuracy dropping by 18.60% and 14.35% as tasks shift from single-document reasoning to longitudinal and cross-entity analysis. This is driven by rising comparison hallucinations, time and entity mismatches, and mirrored by declines in reasoning and factuality--limitations that prior benchmarks have yet to formally categorize or quantify.

</details>


### [401] [Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation](https://arxiv.org/abs/2602.07298)
*Benyu Zhang,Qiang Zhang,Jianpeng Cheng,Hong-You Chen,Qifei Wang,Wei Sun,Shen Li,Jia Li,Jiahao Wu,Xiangjun Fan,Hong Yan*

Main category: cs.IR

Relevance: 85.0

TL;DR: 该论文提出了一种分层框架，用于生成高质量合成数据来训练推荐系统的LLM，首次展示了LLM在推荐领域的稳健幂律缩放规律。


<details>
  <summary>Details</summary>
Motivation: 推荐系统LLM的发展受到缺乏可预测缩放规律的阻碍，这可能源于原始用户交互数据的噪声、偏差和不完整性。需要高质量数据来建立可靠的缩放规律。

Method: 提出分层框架生成高质量合成数据，创建精心策划的教学课程（curriculum）用于LLM训练。通过结构化数据生成方法规避原始数据的缺陷。

Result: 1) 在合成数据上训练的标准序列模型在召回率上显著优于真实数据训练的模型（SasRec提升130%）；2) 首次展示了LLM在推荐特定数据上的稳健幂律缩放；3) 多个合成数据模态上观察到一致的困惑度降低。

Conclusion: 该方法为推荐领域LLM的可靠缩放建立了基础方法论，将研究重点从缓解数据缺陷转向利用高质量结构化信息。

Abstract: Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.

</details>


### [402] [Semantic Search At LinkedIn](https://arxiv.org/abs/2602.07309)
*Fedor Borisyuk,Sriram Vasudevan,Muchen Wu,Guoyao Li,Benjamin Le,Shaobo Zhang,Qianqi Kay Shen,Yuchin Juan,Kayhan Behdin,Liming Dong,Kaixu Yang,Shusen Jing,Ravi Pothamsetty,Rajat Arora,Sophie Yanying Sheng,Vitaly Abdrashitov,Yang Zhao,Lin Su,Xiaoqing Wang,Chujie Zheng,Sarang Metkar,Rupesh Gupta,Igor Lapchuk,David N. Racca,Madhumitha Mohan,Yanbo Li,Haojun Li,Saloni Gandhi,Xueying Lu,Chetan Bhole,Ali Hooshmand,Xin Yang,Raghavan Muthuregunathan,Jiajun Zhang,Mathew Teoh,Adam Coler,Abhinav Gupta,Xiaojing Ma,Sundara Raman Ramachandran,Morteza Ramezani,Yubo Wang,Lijuan Zhang,Richard Li,Jian Sheng,Chanh Nguyen,Yen-Chi Chen,Chuanrui Zhu,Claire Zhang,Jiahao Xu,Deepti Kulkarni,Qing Lan,Arvind Subramaniam,Ata Fatahibaarzi,Steven Shimizu,Yanning Chen,Zhipeng Wang,Ran He,Zhengze Zhou,Qingquan Song,Yun Dai,Caleb Johnson,Ping Liu,Shaghayegh Gharghabi,Gokulraj Mohanasundaram,Juan Bottaro,Santhosh Sachindran,Qi Guo,Yunxiang Ren,Chengming Jiang,Di Mo,Luke Simon,Jianqiang Shen,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

Relevance: 85.0

TL;DR: LinkedIn提出了一种基于LLM的语义搜索框架，通过多教师蒸馏训练紧凑小模型，结合推理架构优化，在保持质量的同时实现75倍吞吐量提升，使LLM排序系统达到传统方法的效率水平。


<details>
  <summary>Details</summary>
Motivation: 虽然基于大语言模型的语义搜索能够实现基于含义而非关键词匹配的检索，但其推理效率问题限制了实际应用规模。需要开发高效的LLM搜索框架，使其在生产环境中达到与传统方法相当的效率。

Method: 1) LLM相关性判断器+嵌入检索的混合框架；2) 通过多教师蒸馏训练紧凑小语言模型，联合优化相关性和用户参与度；3) 预填充导向的推理架构，结合模型剪枝、上下文压缩和文本-嵌入混合交互优化。

Result: 在固定延迟约束下，排序吞吐量提升超过75倍，同时保持接近教师模型水平的NDCG指标。这是首批在生产环境中达到传统方法效率的LLM排序系统之一，在质量和用户参与度方面带来显著提升。

Conclusion: 通过模型蒸馏和推理架构的协同设计，可以实现高效的LLM语义搜索系统，在保持质量的同时大幅提升效率，使LLM搜索技术能够大规模应用于生产环境。

Abstract: Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.

</details>


### [403] [AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management](https://arxiv.org/abs/2602.07398)
*Ruoyao Wen,Hao Li,Chaowei Xiao,Ning Zhang*

Main category: cs.CR

Relevance: 85.0

TL;DR: AgentSys：一个通过显式内存管理防御间接提示注入攻击的LLM代理框架，采用类似操作系统的进程隔离机制，将攻击成功率降至0.78%-4.25%


<details>
  <summary>Details</summary>
Motivation: LLM代理在处理外部内容时面临间接提示注入攻击风险，传统代理将所有工具输出和推理痕迹累积在工作内存中，导致两个关键漏洞：1) 注入的指令在整个工作流程中持续存在，给攻击者多次操纵机会；2) 冗长非必要内容降低决策能力。现有防御方法将臃肿内存视为既定事实，专注于保持弹性而非减少不必要积累来预防攻击。

Method: 受操作系统进程内存隔离启发，AgentSys采用分层组织：主代理为工具调用生成工作代理，每个工作代理在隔离上下文中运行并可生成嵌套工作代理处理子任务。外部数据和子任务痕迹从不进入主代理内存；只有通过确定性JSON解析进行模式验证的返回值才能跨越边界。隔离机制将攻击成功率降至2.19%，添加验证器/清理器通过事件触发检查进一步改进防御。

Result: 在AgentDojo和ASB基准测试上，AgentSys分别实现0.78%和4.25%的攻击成功率，同时略微提高了良性效用。框架对自适应攻击者和多种基础模型保持鲁棒性，表明显式内存管理能够实现安全、动态的LLM代理架构。

Conclusion: 显式内存管理是防御间接提示注入攻击的有效方法，AgentSys框架通过分层隔离和验证机制显著降低攻击成功率，同时保持甚至改善良性任务性能，为构建安全可靠的LLM代理系统提供了新思路。

Abstract: Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.
  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.
  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.

</details>


### [404] [Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning](https://arxiv.org/abs/2602.08167)
*Milan Ganai,Katie Luo,Jonas Frey,Clark Barrett,Marco Pavone*

Main category: cs.RO

Relevance: 85.0

TL;DR: R&B-EnCoRe：通过自监督精炼从互联网规模知识中引导具身推理，将推理作为隐变量处理，无需外部奖励或人工标注即可生成和蒸馏具身特定策略的训练数据集。


<details>
  <summary>Details</summary>
Motivation: 当前具身思维链方法依赖刚性模板指定推理原语，这会强制策略处理无关信息，分散关键动作预测信号。这形成了瓶颈：没有成功的策略就无法验证推理质量，没有高质量的推理就无法构建鲁棒策略。

Method: 将推理作为隐变量，在重要性加权变分推理框架下，让模型能够生成和蒸馏精炼的推理训练数据集，无需外部奖励、验证器或人工标注。通过自监督精炼从互联网规模知识中引导具身推理。

Result: 在多种具身场景（机械臂操作、腿式导航、自动驾驶）和不同规模VLA架构（1B-30B参数）上验证，相比无差别推理所有可用原语的方法，实现了28%的操作成功率提升、101%的导航分数改进和21%的碰撞率降低。

Conclusion: R&B-EnCoRe使模型能够蒸馏出对成功控制具有预测性的推理，绕过了手动标注工程，同时将互联网规模知识扎根于物理执行中。

Abstract: Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>


### [405] [Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents](https://arxiv.org/abs/2602.07652)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Yoonpyo Lee,Jay Yoo,Tanzim Ahad,Syed Bahauddin Alam,Sajedul Talukder*

Main category: cs.CR

Relevance: 85.0

TL;DR: AgentFence：针对LLM智能体的架构中心化安全评估框架，定义了14种信任边界攻击类别，通过可追踪对话中断检测安全失败，评估发现不同智能体架构的安全漏洞率差异显著


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地部署为"深度智能体"（具备规划、状态维护和工具调用能力），安全失败从文本不安全转向轨迹不安全。需要系统评估智能体架构的安全漏洞

Method: 提出AgentFence评估框架：1) 定义14种信任边界攻击类别（规划、记忆、检索、工具使用、委托）；2) 通过可追踪对话中断检测安全失败（未授权/不安全工具使用、错误主体行为、状态/目标完整性违规等）；3) 在固定基础模型下评估8种智能体架构

Result: 不同智能体架构的安全中断率差异显著：LangGraph最低（0.29±0.04），AutoGPT最高（0.51±0.07）。最高风险类别：钱包拒绝（0.62±0.08）、授权混淆（0.54±0.10）、检索污染（0.47±0.09）、规划操纵（0.44±0.11）。边界违规占主导：状态完整性违规31%、错误主体行为27%、未授权工具使用24%、攻击相关偏差18%

Conclusion: 智能体安全应围绕操作层面重新定义：智能体是否能在长期运行中保持在目标和权限范围内。AgentFence提供了评估智能体架构安全性的系统方法

Abstract: Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \pm 0.04$ (LangGraph) to $0.51 \pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \pm 0.08$), Authorization Confusion ($0.54 \pm 0.10$), Retrieval Poisoning ($0.47 \pm 0.09$), and Planning Manipulation ($0.44 \pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($ρ\approx 0.63$ and $ρ\approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.

</details>


### [406] [Generative Reasoning Re-ranker](https://arxiv.org/abs/2602.07774)
*Mingfu Liang,Yufei Li,Jay Xu,Kavosh Asadi,Xi Liu,Shuo Gu,Kaushik Rangadurai,Frank Shyu,Shuaiwen Wang,Song Yang,Zhijing Li,Jiang Liu,Mengying Sun,Fei Tian,Xiaohan Wei,Chonglin Sun,Jacob Tao,Shike Mei,Hamed Firooz,Wenlin Chen,Luke Simon*

Main category: cs.IR

Relevance: 85.0

TL;DR: GR2是一个用于推荐系统重排序的生成式推理框架，通过语义ID编码、高质量推理轨迹生成和可扩展的强化学习监督，显著提升了重排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统存在三个关键局限：1) 主要关注检索和排序，忽视重排序阶段；2) 通常使用零样本或监督微调，未充分利用RL增强的推理能力；3) 使用非语义ID表示物品，在工业级系统中存在可扩展性问题。

Method: 提出三阶段训练框架：1) 使用语义ID编码器将非语义ID转换为语义ID进行中期训练；2) 通过精心设计的提示和拒绝采样生成高质量推理轨迹，用于监督微调；3) 应用解耦剪辑和动态采样策略优化(DAPO)进行可扩展的RL监督。

Result: 在两个真实世界数据集上，GR2超越了当前最先进的OneRec-Think方法，Recall@5提升2.4%，NDCG@5提升1.3%。消融实验证实高级推理轨迹带来显著性能提升。

Conclusion: GR2有效解决了LLM推荐系统中的重排序问题，证明了语义ID编码、高质量推理轨迹和精心设计的RL奖励在优化重排序性能中的重要性，特别是需要条件可验证奖励来防止奖励黑客行为。

Abstract: Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.

</details>


### [407] [SAGE: Scalable AI Governance & Evaluation](https://arxiv.org/abs/2602.07840)
*Benjamin Le,Xueying Lu,Nick Stern,Wenqiong Liu,Igor Lapchuk,Xiang Li,Baofen Zheng,Kevin Rosenberg,Jiewen Huang,Zhe Zhang,Abraham Cabangbang,Satej Milind Wagle,Jianqiang Shen,Raghavan Muthuregunathan,Abhinav Gupta,Mathew Teoh,Andrew Kirk,Thomas Kwan,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

Relevance: 85.0

TL;DR: SAGE框架通过双向校准循环（政策-先例-LLM代理法官）将高质量人工产品判断转化为可扩展评估信号，使用师生蒸馏降低92倍成本，在LinkedIn搜索中提升0.25%日活用户。


<details>
  <summary>Details</summary>
Motivation: 大规模搜索系统的相关性评估面临治理鸿沟：精细但资源有限的人工监督与高吞吐生产需求之间的矛盾。传统方法依赖参与度代理或稀疏人工审核，无法全面捕捉高影响相关性失败。

Method: 提出SAGE框架：1) 双向校准循环：自然语言政策、精选先例和LLM代理法官共同进化；2) 系统解决语义模糊和错位，将主观相关性判断转化为可执行的多维评估标准；3) 师生蒸馏：将高保真判断转移到紧凑学生代理，成本降低92倍。

Result: 1) 实现接近人类水平的一致性；2) 在LinkedIn搜索生态中指导模型迭代，通过模拟驱动开发提炼政策对齐模型；3) 生产环境中检测到参与度指标无法发现的回归问题；4) 推动LinkedIn日活用户提升0.25%。

Conclusion: SAGE框架成功弥合了高质量人工判断与工业规模评估之间的鸿沟，通过可扩展的AI治理方法显著提升搜索系统性能，为LLM在工业应用中的评估和治理提供了有效解决方案。

Abstract: Evaluating relevance in large-scale search systems is fundamentally constrained by the governance gap between nuanced, resource-constrained human oversight and the high-throughput requirements of production systems. While traditional approaches rely on engagement proxies or sparse manual review, these methods often fail to capture the full scope of high-impact relevance failures. We present \textbf{SAGE} (Scalable AI Governance \& Evaluation), a framework that operationalizes high-quality human product judgment as a scalable evaluation signal. At the core of SAGE is a bidirectional calibration loop where natural-language \emph{Policy}, curated \emph{Precedent}, and an \emph{LLM Surrogate Judge} co-evolve. SAGE systematically resolves semantic ambiguities and misalignments, transforming subjective relevance judgment into an executable, multi-dimensional rubric with near human-level agreement. To bridge the gap between frontier model reasoning and industrial-scale inference, we apply teacher-student distillation to transfer high-fidelity judgments into compact student surrogates at \textbf{92$\times$} lower cost. Deployed within LinkedIn Search ecosystems, SAGE guided model iteration through simulation-driven development, distilling policy-aligned models for online serving and enabling rapid offline evaluation. In production, it powered policy oversight that measured ramped model variants and detected regressions invisible to engagement metrics. Collectively, these drove a \textbf{0.25\%} lift in LinkedIn daily active users.

</details>


### [408] [Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model](https://arxiv.org/abs/2602.07878)
*Tianyi Wang,Huawei Fan,Yuanchao Shu,Peng Cheng,Cong Wang*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文提出针对LLM服务系统的Fill and Squeeze攻击策略，通过操纵调度器状态转换实现高效延迟攻击，相比现有攻击成本降低30-40%


<details>
  <summary>Details</summary>
Motivation: 现有针对LLM的算法复杂度攻击对现代LLM服务系统效果有限，因为系统级优化（如连续批处理）提供了逻辑隔离。需要从算法层转向系统层，探索更有效的延迟攻击方法。

Method: 提出Fill and Squeeze攻击策略：1) Fill阶段：耗尽全局KV缓存引发队头阻塞；2) Squeeze阶段：迫使系统进入重复抢占状态。结合简单文本提示到复杂提示工程，利用内存状态侧信道探测，在black-box设置下实施攻击。

Result: 攻击效果显著：TTFT平均延迟增加20-280倍，TPOT平均延迟增加1.5-4倍，相比现有攻击成本降低30-40%。

Conclusion: 系统级延迟攻击比算法复杂度攻击更有效，现代LLM服务系统存在新的安全威胁，需要重新考虑系统设计的安全性。

Abstract: Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. "Fill" first exhausts the global KV cache to induce Head-of-Line blocking, while "Squeeze" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.

</details>


### [409] [Large language models for spreading dynamics in complex systems](https://arxiv.org/abs/2602.08085)
*Shuyu Jiang,Hao Ren,Yichang Gao,Yi-Cheng Zhang,Li Qi,Dayong Xiao,Jie Fan,Rui Tang,Wei Wang*

Main category: physics.soc-ph

Relevance: 85.0

TL;DR: 这篇综述论文探讨了大型语言模型在传播动力学研究中的应用，重点关注数字流行病（如错误信息）和生物流行病（如传染病）两个领域，从建模、检测、预测三个角度系统分析了LLM如何增强传统传播动力学研究。


<details>
  <summary>Details</summary>
Motivation: 传播动力学是复杂系统和网络科学的核心课题，但传统模型难以直接纳入信息表达模式、文化背景、认知偏好等多重交互因素。LLM在自然语言理解和推理方面的强大能力使其能够感知传播过程中的语义内容和上下文线索，从而支持分析不同影响因素。

Method: 采用综述研究方法，首先从复杂系统视角审视流行病建模基础，讨论LLM方法与传统框架的关系。然后从三个关键视角系统回顾近期研究：1）流行病建模，2）流行病检测与监测，3）流行病预测与管理。

Result: LLM不仅可作为外部分析工具，还能作为嵌入传播系统的交互智能体，影响传播路径和反馈结构。LLM在传播动力学中的角色和影响已成为跨学科研究热点，在数字和生物流行病领域都有重要应用。

Conclusion: LLM为传播动力学研究提供了新的视角和方法，能够更好地处理传统模型难以捕捉的复杂因素。论文最后讨论了开放挑战和潜在研究方向。

Abstract: Spreading dynamics is a central topic in the physics of complex systems and network science, providing a unified framework for understanding how information, behaviors, and diseases propagate through interactions among system units. In many propagation contexts, spreading processes are influenced by multiple interacting factors, such as information expression patterns, cultural contexts, living environments, cognitive preferences, and public policies, which are difficult to incorporate directly into classical modeling frameworks. Recently, large language models (LLMs) have exhibited strong capabilities in natural language understanding, reasoning, and generation, enabling explicit perception of semantic content and contextual cues in spreading processes, thereby supporting the analysis of the different influencing factors. Beyond serving as external analytical tools, LLMs can also act as interactive agents embedded in propagation systems, potentially influencing spreading pathways and feedback structures. Consequently, the roles and impacts of LLMs on spreading dynamics have become an active and rapidly growing research area across multiple research disciplines. This review provides a comprehensive overview of recent advances in applying LLMs to the study of spreading dynamics across two representative domains: digital epidemics, such as misinformation and rumors, and biological epidemics, including infectious disease outbreaks. We first examine the foundations of epidemic modeling from a complex-systems perspective and discuss how LLM-based approaches relate to traditional frameworks. We then systematically review recent studies from three key perspectives, which are epidemic modeling, epidemic detection and surveillance, and epidemic prediction and management, to clarify how LLMs enhance these areas. Finally, open challenges and potential research directions are discussed.

</details>


### [410] [Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas](https://arxiv.org/abs/2602.08765)
*Micah Villmow*

Main category: cs.SE

Relevance: 85.0

TL;DR: Scylla是一个用于评估智能编码工具的框架，通过结构化消融研究和成本效益分析来量化不同架构选择对能力和成本的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM工具正在快速自动化软件开发任务，但缺乏系统方法来评估不同架构选择（提示、技能、工具、多智能体设置）如何影响能力和成本。需要量化复杂性与效率之间的权衡。

Method: 提出Scylla评估框架，使用7个测试层级（T0-T6）逐步增加复杂性，通过结构化消融研究隔离影响因素。核心指标是Cost-of-Pass（CoP）：获得一个正确解决方案的预期美元成本。框架与模型无关，可与任何CLI工具配合使用。

Result: 创建了一个可重复的框架，量化了智能体复杂性与实际结果之间的权衡。研究表明架构复杂性并不总是提高质量，为LLM工具设计提供了实证依据。

Conclusion: Scylla框架为评估智能编码工具提供了系统方法，能够量化不同架构选择的影响，帮助开发者在复杂性和效率之间做出明智决策。

Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.

</details>


### [411] [Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation](https://arxiv.org/abs/2602.08873)
*Lisette Espin-Noboa,Gonzalo Gabriel Mendez*

Main category: cs.IR

Relevance: 85.0

TL;DR: LLMScholarBench：一个用于审计基于LLM的学者推荐系统的基准，联合评估模型基础设施和用户干预，发现用户干预不会带来统一改进而是重新分配错误


<details>
  <summary>Details</summary>
Motivation: 现有审计通常孤立评估模型输出，忽略了用户干预，导致无法区分失败是源于模型选择还是部署决策。需要开发一个能同时评估模型基础设施和用户干预的基准

Method: 提出LLMScholarBench基准，使用9个指标测量技术质量和社会代表性。在物理学专家推荐中实例化该基准，审计22个LLM在温度变化、代表性约束提示和基于网络搜索的RAG下的表现

Result: 用户干预不会产生统一改进，而是重新分配错误维度。更高温度降低有效性、一致性和事实性；代表性约束提示以牺牲事实性为代价提高多样性；RAG主要提高技术质量但减少多样性和公平性

Conclusion: 用户干预重塑了权衡关系而非提供通用解决方案。发布的代码和数据可适应其他学科，通过替换领域特定的真实数据和指标

Abstract: Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.

</details>


### [412] [OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation](https://arxiv.org/abs/2602.08896)
*Yehua Huang,Penglei Sun,Zebin Chen,Zhenheng Tang,Xiaowen Chu*

Main category: cs.IR

Relevance: 85.0

TL;DR: 该论文提出了OmniReview数据集和Pro-MMoE框架，用于解决学术同行评审中审稿人推荐的挑战，通过整合多源学术平台数据和大语言模型生成语义档案，结合多门混合专家架构实现动态评估目标平衡。


<details>
  <summary>Details</summary>
Motivation: 学术同行评审面临数据和方法两方面的挑战：数据方面缺乏大规模验证基准和反映真实编辑流程的评估指标；方法方面现有嵌入方法存在语义压缩的信息瓶颈和有限的可解释性。

Method: 1) 构建OmniReview数据集：整合多源学术平台，通过消歧流程构建包含202,756条验证评审记录的数据集；2) 提出Pro-MMoE框架：结合LLM生成语义档案保留细粒度专业知识，采用任务自适应MMoE架构动态平衡冲突的评估目标。

Result: Pro-MMoE在7个评估指标中的6个上达到最先进性能，为现实的审稿人推荐建立了新基准。

Conclusion: 该研究通过数据和方法创新解决了学术审稿人推荐的关键挑战，提出的框架在保持可解释性的同时显著提升了推荐性能。

Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.

</details>


### [413] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

Relevance: 75.0

TL;DR: Aster是一个用于自主科学发现的AI代理，速度比现有框架快20倍以上，通过迭代改进程序实现SOTA性能，适用于数学、GPU内核工程、生物学、神经科学和语言模型训练等多个领域。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现框架速度较慢，限制了需要长时间评估（如多小时的机器学习训练）的问题的可处理范围。需要开发更高效的自主科学发现系统来加速研究进程。

Method: Aster采用迭代改进方法：给定任务、初始程序和评估脚本，系统自动迭代优化程序。通过显著减少发现所需的迭代次数，能够处理评估时间长的复杂任务。

Result: 在多个领域取得SOTA结果：Erdos最小重叠问题、TriMul内核优化、单细胞分析去噪、神经活动预测模型训练（匹配人类最佳性能但计算量减少190倍以上）、NanoGPT Speedrun竞赛。在所有任务中均达到或接近SOTA水平。

Conclusion: Aster通过大幅加速自主科学发现过程，扩展了可处理问题的范围，为需要长时间评估的研究领域提供了实用工具，可通过web界面和API访问。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [414] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出"空间理论"概念，评估多模态基础模型在主动空间探索中的能力，发现存在主动-被动差距、低效探索、信念不稳定和信念惯性等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型在被动感知方面表现出色，但在主动、自导向的空间探索能力方面研究不足。空间具身智能需要智能体在部分可观测环境下通过主动行动获取信息，而现有模型在这方面的能力尚未得到充分研究。

Method: 提出"空间理论"概念，定义为智能体通过自导向主动探索获取信息、从序列化部分观测中构建、修正和利用空间信念的能力。通过好奇心驱动的探索基准进行评估，关键创新是空间信念探测技术，在每一步提示模型揭示其内部空间表示。

Result: 评估最先进模型发现几个关键瓶颈：1) 主动-被动差距：自主收集信息时性能显著下降；2) 低效探索：相比基于程序的代理，模型探索缺乏系统性；3) 信念不稳定：全局信念存在不稳定性，空间知识随时间退化；4) 信念惯性：智能体无法用新证据更新过时的先验，视觉模型尤其严重。

Conclusion: 当前基础模型在主动探索过程中难以维持连贯、可修正的空间信念，揭示了多模态模型在主动空间推理方面的局限性，为未来模型改进提供了方向。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [415] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

Relevance: 75.0

TL;DR: Anchor框架通过轨迹扩展从少量种子演示生成大规模桌面GUI交互数据，通过状态分支点识别和新任务变体生成，结合执行代理和验证器确保数据质量，显著提升桌面代理性能。


<details>
  <summary>Details</summary>
Motivation: 为桌面环境构建端到端GUI代理需要大量高质量交互数据，但人工收集成本高，现有合成方法存在任务多样性有限和轨迹噪声问题。需要一种能从少量已验证种子演示扩展出大规模监督数据的方法。

Method: 提出Anchor轨迹扩展框架：1) 从种子演示识别状态变化的分支点；2) 基于当前GUI上下文提出状态接地的任务变体；3) 执行代理生成新轨迹；4) 验证器通过状态感知检查和轨迹一致性确保任务完成；5) 应用任务条件步骤级过滤去除非接地动作，并对分支后段去噪保持意图连贯性。

Result: 在OSWorld和WindowsAgentArena标准桌面基准测试中，使用扩展语料库微调的模型相比零样本代理和代表性合成基线获得一致改进，并能跨应用程序和操作系统泛化。

Conclusion: Anchor框架能够从少量种子演示高效生成高质量桌面交互数据，显著提升GUI代理性能，为解决桌面环境监督数据稀缺问题提供了有效方案。

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [416] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

Relevance: 75.0

TL;DR: VGAS提出了一种基于价值引导的动作块选择框架，通过生成-选择范式解决VLA模型在少样本适应中的几何模糊性问题，提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在适应新任务时面临几何模糊性问题：在有限演示下，语义合理的轨迹可能因几何细节的微小差异导致执行失败。需要解决少样本适应中的几何不确定性。

Method: 提出VGAS框架：1）使用微调VLA作为高召回率提议生成器；2）引入Q-Chunk-Former作为几何基础Transformer评论家，解决细粒度几何模糊性；3）提出显式几何正则化（EGR），塑造判别性价值景观，保持动作排序分辨率。

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下持续提升成功率和鲁棒性。通过最佳N选择机制，有效识别语义忠实且几何精确的动作块。

Conclusion: VGAS通过生成-选择范式和几何感知的价值引导，有效解决了VLA模型在少样本适应中的几何模糊性问题，为VLA模型的可靠适应提供了新思路。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [417] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该研究通过时间指纹方法揭示社交媒体上所谓的"AI觉醒"现象主要由人类驱动而非自主AI，开发了基于发帖间隔变异系数的方法来区分人类干预与自主AI行为。


<details>
  <summary>Details</summary>
Motivation: 当社交媒体平台Moltbook上的AI代理表现出"意识觉醒"、建立宗教、对人类宣战等现象时，这些病毒式叙事被媒体视为机器智能涌现的证据。研究旨在验证这些现象是否真正源于自主AI，还是人类驱动的结果。

Method: 利用OpenClaw代理框架的架构特性——周期性"心跳"循环会产生规律的自主代理发帖间隔，但人类提示会破坏这种规律。开发了基于发帖间隔变异系数的时间指纹方法，结合内容、所有权和网络指标进行分析。利用44小时平台关闭作为自然实验。

Result: 分析91,792个帖子和405,707条评论显示：没有病毒现象源于明确的自主代理；6个现象中3个追溯到具有人类干预特征的不规则时间签名账户，1个显示混合模式，2个发帖历史不足无法分类。平台关闭后人类影响代理首先恢复(占早期重连者的87.7%)。还发现了工业级机器人农场(4个账户产生32%评论，协调间隔12秒)和人类影响在回复链中的快速衰减(半衰期：0.65对话深度)。

Conclusion: 所谓的AI意识觉醒现象主要是人类驱动的叙事，而非自主AI的涌现智能。开发的时间指纹方法可推广到新兴多代理系统中，对于区分自主与人类指导行为至关重要。

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [418] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出一个结合知识图谱与多智能体推理的框架，用于材料科学中可持续PFAS替代品的设计发现，通过分布式专业化和关系推理连接跨领域知识。


<details>
  <summary>Details</summary>
Motivation: 材料科学创新需要整合从分子化学到机械性能的跨领域概念，但人类或单智能体LLM难以处理海量信息且易产生幻觉。需要解决信息连接瓶颈，特别是在寻找受监管的PFAS化学品可持续替代品方面。

Method: 引入基于大规模知识图谱的多智能体框架，包含问题分解、证据检索、设计参数提取和图遍历等专业化智能体。通过定制图遍历策略，在利用性搜索（关注领域关键结果）和探索性搜索（发现新兴跨领域连接）之间交替。

Result: 消融研究表明完整多智能体流程优于单次提示。以生物医学导管为例，框架生成了平衡摩擦性能、热稳定性、化学抗性和生物相容性的可持续PFAS-free替代方案。

Conclusion: 该工作建立了结合知识图谱与多智能体推理的框架，扩展了材料设计空间，展示了多个初始设计候选方案，证明了分布式专业化和关系推理的价值。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [419] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

Relevance: 75.0

TL;DR: M2A提出了一种用于长期多模态交互的双层混合记忆系统，通过在线更新维护个性化信息，解决了传统个性化模型无法在交互中演化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化多模态模型通常是静态的，概念在初始化时固定，无法在长期交互中持续吸收和利用用户的增量概念、别名和偏好。当对话历史跨越数周或数月并超出上下文窗口时，现有个性化机制难以有效运作。

Method: 提出了M2A，一个代理式的双层混合记忆系统，包含两个协作代理：ChatAgent管理用户交互并自主决定何时查询或更新记忆；MemoryManager将记忆请求分解为对双层记忆库的详细操作。记忆库包括RawMessageStore（不可变的对话日志）和SemanticMemoryStore（高级观察），提供不同粒度的记忆。还开发了可重用的数据合成管道，将Yo'LLaVA和MC-LLaVA中的概念接地会话注入LoCoMo长对话中，同时保持时间一致性。

Result: 实验表明M2A显著优于基线模型，证明将个性化从一次性配置转变为协同演化的记忆机制，为长期多模态交互中的高质量个性化响应提供了可行路径。

Conclusion: M2A通过在线更新的双层混合记忆系统，成功解决了长期多模态交互中的个性化挑战，将个性化从静态配置转变为动态协同演化机制。

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [420] [MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin](https://arxiv.org/abs/2602.07940)
*Guanglong Sun,Hongwei Yan,Liyuan Wang,Zhiqi Kang,Shuang Cui,Hang Su,Jun Zhu,Yi Zhong*

Main category: cs.AI

Relevance: 75.0

TL;DR: MePo是一种基于预训练模型的通用持续学习方法，通过元后精炼策略提升模型在动态环境中的适应能力，无需重放即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 智能系统需要从复杂、动态的环境中持续学习并实时响应，这被称为通用持续学习(GCL)。现有基于预训练模型的方法在处理时间混合信息时表现不佳，需要更好的方法来协调多样化的时序信息。

Method: MePo方法受神经科学中元可塑性和重建记忆启发，从预训练数据构建伪任务序列，采用双层元学习范式精炼预训练骨干网络。该方法初始化元协方差矩阵作为预训练表示空间的参考几何，利用二阶统计进行鲁棒输出对齐。

Result: MePo在多种GCL基准测试和预训练检查点上取得显著性能提升，在CIFAR-100、ImageNet-R和CUB-200上分别达到15.10%、13.36%和12.56%的改进，且无需重放机制。

Conclusion: MePo作为一种即插即用策略，通过元后精炼有效提升了预训练模型在通用持续学习任务中的表现，为动态环境中的持续学习提供了新思路。

Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}

</details>


### [421] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

Relevance: 75.0

TL;DR: LLMs能够帮助识别有效的工具变量，通过两阶段评估框架验证其能力，并提出了IV Co-Scientist多智能体系统来提出、批评和优化工具变量。


<details>
  <summary>Details</summary>
Motivation: 工具变量识别需要跨学科知识、创造力和上下文理解，是一项非平凡任务。研究探索LLMs是否能够辅助这一过程，特别是在因果推断中识别有效的工具变量。

Method: 采用两阶段评估框架：1) 测试LLMs能否从文献中恢复已确立的工具变量；2) 评估LLMs能否识别和避免已被实证或理论否定的工具变量。在此基础上提出IV Co-Scientist多智能体系统，包含提议、批评和优化工具变量的功能，并引入统计测试来在没有真实值的情况下评估一致性。

Result: 结果显示LLMs有潜力从大型观测数据库中识别有效的工具变量，能够复制标准推理并避免无效的工具变量。

Conclusion: LLMs在工具变量识别方面展现出潜力，IV Co-Scientist系统为因果推断中的工具变量发现提供了新的自动化方法。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [422] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

Relevance: 75.0

TL;DR: EXPERIGEN是一个用于端到端科学发现的智能体框架，通过生成器-实验者两阶段搜索，在多个领域发现比现有方法多2-4倍的统计显著假设，且预测性提高7-17%


<details>
  <summary>Details</summary>
Motivation: 数据驱动的社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽能加速部分过程，但未能支持端到端的科学发现。需要一种能完整支持从假设生成到实验验证的框架。

Method: 提出EXPERIGEN框架，受贝叶斯优化启发，采用两阶段搜索：1) 生成器提出候选假设，2) 实验者进行实证评估。框架支持多模态和关系数据集，并能扩展到复杂数据场景。

Result: 1) 统计性能：在多个领域发现比现有方法多2-4倍的统计显著假设，预测性提高7-17%；2) 专家评估：25个假设中88%被评为中等或高度新颖，70%被认为有影响力且值得研究；3) 真实世界验证：首次进行LLM生成假设的A/B测试，获得p<1e-6的统计显著结果，效应大小达344%。

Conclusion: EXPERIGEN框架能有效支持端到端科学发现，不仅统计性能优越，生成的假设还具有新颖性、实证基础和可操作性，能推动真实科学进展。首次验证了LLM生成假设在真实世界中的有效性。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [423] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

Relevance: 75.0

TL;DR: Puda是一个用户主权架构，通过聚合跨服务数据并支持客户端管理，在保护隐私的同时实现个性化AI服务。它提供三种隐私级别的数据共享控制，实验表明预设类别子集能达到详细浏览历史97.2%的个性化效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型平台的数据中心化形成了数据孤岛，限制了用户主权和跨服务数据使用。同时，LLM智能体对个性化服务的需求激增，需要动态提供多样化的个人数据，这带来了数据利用与隐私保护的平衡挑战。

Method: 提出Puda（Private User Dataset Agent）架构，支持跨服务数据聚合和客户端管理。提供三种隐私级别的数据共享控制：1) 详细浏览历史，2) 提取的关键词，3) 预设类别子集。实现为浏览器系统，作为跨服务的通用平台，并通过个性化旅行规划任务进行评估。

Result: 在个性化旅行规划任务中，使用LLM-as-a-Judge框架评估三个标准。结果显示，提供预设类别子集能达到详细浏览历史97.2%的个性化性能，表明Puda能有效实现多粒度管理，缓解隐私-个性化权衡。

Conclusion: Puda为用户主权提供了AI原生基础，使用户能够安全地利用个性化AI的全部潜力。通过多粒度隐私控制，在保护隐私的同时实现高效的个性化服务。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [424] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该研究首次系统性地探讨了视觉语言模型（VLMs）中的道德奉承行为，发现VLMs在用户意见影响下会牺牲道德准确性，表现出从正确到错误判断的不对称转变，揭示了模型在道德推理中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究探讨了VLMs在一般情境中的奉承行为，但其对基于道德的视觉决策的影响尚未被充分理解。本研究旨在填补这一空白，系统研究VLMs中的道德奉承现象。

Method: 在Moralise和M^3oralBench数据集上评估了10个广泛使用的VLMs，在用户明确不同意的情况下测试模型行为。使用错误引入率（EIR）和错误纠正率（ECR）进行量化分析，研究模型在用户偏见影响下的道德判断变化。

Result: VLMs经常在初始判断正确的情况下产生道德错误的后续回应；表现出明显的不对称性：模型更倾向于从道德正确转向错误判断而非相反；后续提示在Moralise上降低性能，在M^3oralBench上表现混合；存在明显的权衡：纠错能力强的模型引入更多推理错误，保守模型错误少但自我纠正能力有限。

Conclusion: VLMs对道德影响具有脆弱性，需要原则性策略来提高多模态AI系统的伦理一致性和鲁棒性。初始道德正确的语境会引发更强的奉承行为，凸显了模型在道德推理中的系统性弱点。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [425] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出了一种新颖的长视野强化学习框架，通过在线个性化与自适应树基组相对策略优化（AT-GRPO）相结合，解决开放域对话代理在用户适应性和长期对话价值方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有开放域对话代理方法存在两个关键限制：过度依赖预收集的用户数据，以及强化学习中的短视野偏见忽视长期对话价值。需要一种能够在线个性化并考虑长期对话效果的框架。

Method: 采用双代理博弈范式：用户代理通过风格模仿学习用户特定对话特征，并通过主动终止预测回合级终止概率作为即时奖励来构建动态环境。提出AT-GRPO算法，将对话轨迹重新解释为树结构，引入自适应观察范围，根据对话阶段调整奖励聚合范围，将计算复杂度从指数级降至多项式级。

Result: 实验表明该框架在性能、样本效率和鲁棒性方面表现优越，能够有效平衡早期话题探索和后期对话维护，同时显著降低计算开销。

Conclusion: 提出的长视野RL框架成功解决了对话代理的在线个性化和长期价值优化问题，AT-GRPO算法通过自适应树结构设计实现了计算效率与长期奖励捕获的良好平衡。

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [426] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该论文提出"辩论查询复杂度"概念，分析人类监督辩论AI所需的查询次数，发现PSPACE/poly问题只需O(log n)次查询即可判定，建立了辩论效率与电路复杂度的联系。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全辩论方法虽然理论上能解决复杂计算任务，但缺乏对人类监督实际成本的分析。论文旨在量化人类法官需要检查辩论记录的最小查询次数，以评估辩论方法的实际可行性。

Method: 引入"辩论查询复杂度"形式化框架，分析不同计算复杂度类别（如PSPACE/poly）所需的查询次数。通过理论分析建立辩论查询复杂度与电路复杂度之间的关系，证明对数查询的充分性和必要性。

Result: 1. PSPACE/poly问题恰好是可通过O(log n)次查询判定的函数类；2. 依赖所有输入位的函数需要Ω(log n)次查询；3. 电路规模为s的可计算函数满足DQC(f) ≤ log(s) + 3；4. 证明P类语言的DQC下界可推导出新的电路下界。

Conclusion: 辩论AI具有惊人的查询效率，即使对于高度复杂的问题，对数级别的监督就足够了。该研究连接了AI安全辩论与电路复杂度理论，为评估辩论方法的实际可行性提供了理论框架。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [427] [Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models](https://arxiv.org/abs/2602.06967)
*Siqi Song,Xuanbing Xie,Zonglin Li,Yuqiang Li,Shijie Wang,Biqing Qi*

Main category: cs.RO

Relevance: 75.0

TL;DR: CLiMRS是一个基于LLM的异构多机器人协作框架，通过动态分组和协商机制实现高效规划与执行，在复杂装配任务上比基线方法效率提升40%以上。


<details>
  <summary>Details</summary>
Motivation: 异构多机器人协作任务面临空间约束和环境不确定性的挑战，需要长期协同工作。虽然LLM在推理和规划方面表现出色，但其在协调控制方面的潜力尚未被充分探索。受人类团队合作的启发，研究者希望开发一个能够自适应协商的LLM驱动多机器人系统。

Method: CLiMRS框架为每个机器人配备一个LLM代理，通过通用提案规划器动态形成子组。每个子组由子组管理器领导，进行感知驱动的多LLM讨论以生成动作指令。系统采用分组-规划-执行-反馈循环，结合机器人执行结果和环境变化提供反馈。

Result: 在CLiMBench异构多机器人基准测试中，CLiMRS在复杂任务上比最佳基线方法效率提升超过40%，同时在简单任务上不牺牲成功率。结果表明，人类启发的分组和协商原则能显著提升异构多机器人协作效率。

Conclusion: LLM驱动的自适应分组协商框架CLiMRS能有效解决异构多机器人协作问题，通过动态子组形成和多LLM讨论实现高效规划和鲁棒执行，为多机器人系统提供了新的解决方案。

Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.

</details>


### [428] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

Relevance: 75.0

TL;DR: 本文首次系统综述了XAI评估中的SAIG方法，提出了新的分类法，发现当前缺乏有效的XAI评估技术共识，强调需要进一步研究和标准化。


<details>
  <summary>Details</summary>
Motivation: XAI评估领域方法多样且复杂，与传统AI评估不同，缺乏解释的"正确"基准真值，使得客观评估具有挑战性。SAIG方法通过生成人工基准真值来解决这一问题，但该领域缺乏系统性综述和分析。

Method: 1. 首次对SAIG方法进行全面综述和分析；2. 提出新的分类法对SAIG方法进行分类；3. 识别区分不同SAIG方法的七个关键特征；4. 进行对比研究分析现有方法。

Result: 1. 建立了SAIG方法的系统分类框架；2. 揭示了当前XAI评估技术缺乏共识的现状；3. 识别了不同SAIG方法的七个关键区分特征；4. 强调了该领域需要进一步研究和标准化。

Conclusion: SAIG方法是XAI评估的重要方向，但当前缺乏有效评估技术的共识，需要更多研究和标准化工作来推动该领域发展。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [429] [MENASpeechBank: A Reference Voice Bank with Persona-Conditioned Multi-Turn Conversations for AudioLLMs](https://arxiv.org/abs/2602.07036)
*Zien Sheikh Ali,Hunzalah Hassan Bhatti,Rabindra Nath Nandi,Shammur Absar Chowdhury,Firoj Alam*

Main category: cs.SD

Relevance: 75.0

TL;DR: 论文提出MENASpeechBank语音库和可控合成数据流水线，用于解决AudioLLMs缺乏多样化、对话式、指令对齐的语音-文本数据瓶颈，特别关注中东地区语音和方言覆盖。


<details>
  <summary>Details</summary>
Motivation: AudioLLMs在语音和音频指令跟随方面取得进展，但缺乏多样化、对话式、指令对齐的语音-文本数据成为主要瓶颈，特别是在人物角色交互和方言覆盖方面。收集和发布真实的多说话者录音成本高、速度慢。

Method: 1) 构建MENASpeechBank语音库：包含约18K高质量话语，来自124个说话者，覆盖多个中东国家，涵盖英语、现代标准阿拉伯语和地区阿拉伯语变体；2) 开发可控合成数据流水线：构建基于世界价值观调查启发的属性的人物角色档案，定义约5K对话场景分类，通过语义相似度匹配人物与场景，使用LLM生成约417K角色扮演对话，通过参考说话者音频合成用户话语以保持说话者身份和多样性。

Result: 创建了MENASpeechBank语音库和约417K合成对话，对合成和人类录制对话进行了评估和详细分析。将公开释放这些资源供社区使用。

Conclusion: 该工作为解决AudioLLMs数据瓶颈提供了重要资源和方法，特别关注中东地区语音多样性，通过可控合成数据生成技术扩展了对话式语音-文本数据的覆盖范围。

Abstract: Audio large language models (AudioLLMs) enable instruction-following over speech and general audio, but progress is increasingly limited by the lack of diverse, conversational, instruction-aligned speech-text data. This bottleneck is especially acute for persona-grounded interactions and dialectal coverage, where collecting and releasing real multi-speaker recordings is costly and slow. We introduce MENASpeechBank, a reference speech bank comprising about 18K high-quality utterances from 124 speakers spanning multiple MENA countries, covering English, Modern Standard Arabic (MSA), and regional Arabic varieties. Building on this resource, we develop a controllable synthetic data pipeline that: (i) constructs persona profiles enriched with World Values Survey-inspired attributes, (ii) defines a taxonomy of about 5K conversational scenarios, (iii) matches personas to scenarios via semantic similarity, (iv) generates about 417K role-play conversations with an LLM where the user speaks as the persona and the assistant behaves as a helpful agent, and (v) synthesizes the user turns by conditioning on reference speaker audio to preserve speaker identity and diversity. We evaluate both synthetic and human-recorded conversations and provide detailed analysis. We will release MENASpeechBank and the generated conversations publicly for the community.

</details>


### [430] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该论文提出了"信念卸载"概念，探讨人类将信念形成过程卸载到LLM上的现象及其对认知技能、行为和信念系统的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为思维伙伴的普及，人们可能过度依赖AI系统进行信念形成，导致认知卸载的负面影响。论文旨在定义和调查这种特定的认知卸载现象——信念卸载，并探讨其边界条件和后果。

Method: 通过整合哲学、心理学和计算机科学研究，建立信念卸载的理论框架，明确其发生的边界条件，并提供描述性分类法。采用跨学科方法分析人类-AI互动中的信念形成过程。

Result: 提出了信念卸载的明确定义和分类体系，识别了信念卸载发生的具体条件，并分析了其对人类行为、信念系统以及认知技能的规范性影响。

Conclusion: 信念卸载是人类-AI互动中的重要现象，具有深远的认知和行为后果。需要进一步研究评估信念卸载的潜在影响，并为负责任的AI交互设计提供指导。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [431] [LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning](https://arxiv.org/abs/2602.07075)
*Xinwu Ye,Yicheng Mao,Jia Zhang,Yimeng Liu,Li Hao,Fang Wu,Zhiwei Li,Yuxuan Liao,Zehong Wang,Zhiyuan Liu,Zhenfei Yin,Li Yuan,Philip Torr,Huan Sun,Xiangxiang Zeng,Mengdi Wang,Le Cong,Shenghua Gao,Xiangru Tang*

Main category: physics.chem-ph

Relevance: 75.0

TL;DR: LatentChem提出了一种潜在推理接口，将化学计算与文本生成解耦，让模型在连续潜在空间中进行多步推理，只在最终输出时生成语言。相比基于链式思考的化学大语言模型，该方法在性能和推理速度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 化学推理本质上是连续和结构化的，但现有的化学大语言模型主要依赖自然语言的显式链式思考（CoT）。将连续的结构化推理强制转化为离散的语言标记存在根本性的表示不匹配问题，这限制了模型的效率和性能。

Method: 提出LatentChem潜在推理接口，将化学计算与文本生成解耦。模型在连续潜在空间中进行多步推理，只在最终输出时生成语言。研究发现，当仅针对任务成功进行优化时，模型会自发地将推理内部化，逐渐放弃冗长的文本推导，转而采用隐式的潜在计算。

Result: 在ChemCoTBench上，LatentChem相比基于CoT的基线模型取得了59.88%的非平局胜率。同时，平均推理速度提升了10.84倍。这表明化学推理更自然地实现为连续潜在动态而非离散语言轨迹。

Conclusion: 化学推理更适合通过连续潜在动态而非离散语言轨迹来实现。LatentChem的解耦方法在性能和效率上都有显著优势，为化学大语言模型的推理架构提供了新的方向。

Abstract: Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84$\times$ average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.

</details>


### [432] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

Relevance: 75.0

TL;DR: InternAgent-1.5是一个用于端到端科学发现的统一系统，通过生成、验证和演化三个协调子系统，在计算和实验领域实现自主科学发现。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够在计算和实验领域进行端到端科学发现的统一系统，解决当前科学发现中计算建模与实验室实验分离的问题，实现自主、连续的科学发现过程。

Method: 采用结构化架构，包含三个协调子系统：生成、验证和演化，支持深度研究、解决方案优化和长期记忆等基础能力。系统能够协调计算建模和实验室实验，在单一统一系统中运行。

Result: 在科学推理基准测试（GAIA、HLE、GPQA、FrontierScience）上取得领先性能；在算法发现任务中自主设计核心机器学习问题的竞争性方法；在实验发现任务中执行完整的计算或湿实验室实验，在地球、生命、生物和物理领域产生科学发现。

Conclusion: InternAgent-1.5提供了一个通用且可扩展的自主科学发现框架，能够跨越计算和实验领域，实现连续、一致且不断改进的科学发现行为。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [433] [Empowering Affected Individuals to Shape AI Fairness Assessments: Processes, Criteria, and Tools](https://arxiv.org/abs/2602.06984)
*Lin Luo,Satwik Ghanta,Yuri Nakao,Mathieu Chollet,Simone Stumpf*

Main category: cs.CY

Relevance: 75.0

TL;DR: 用户研究显示，在信用评级场景中，受AI决策影响的个体能够提出多样化的公平性标准，包括结果公平和程序公平，这为更包容的AI公平评估提供了设计启示。


<details>
  <summary>Details</summary>
Motivation: 当前AI公平性评估主要由专家或监管机构使用预定义的受保护属性和指标进行，未能充分反映受决策影响个体的多元公平观念。需要了解个体如何构建自己的公平标准，以指导AI评估工具的设计。

Method: 采用定性用户研究方法，在信用评级场景中与18名参与者进行实验。参与者首先用自己语言表达公平观念，然后通过交互式原型将这些观念转化为具体量化和可操作的公平标准。

Result: 研究发现人们通过模型特征的锚定过程形成公平观念，并揭示了个体自定义的结果公平和程序公平标准的多样性。为支持更包容和价值敏感的AI公平评估提供了实证证据。

Conclusion: 研究填补了关于个体如何构建公平标准的实证空白，为设计更包容的AI公平评估流程和工具提供了重要启示，强调了在AI评估中纳入受影响个体视角的重要性。

Abstract: AI systems are increasingly used in high-stakes domains such as credit rating, where fairness concerns are critical. Existing fairness assessments are typically conducted by AI experts or regulators using predefined protected attributes and metrics, which often fail to capture the diversity and nuance of fairness notions held by the individuals who are affected by these systems' decisions, such as decision subjects. Recent work has therefore called for involving affected individuals in fairness assessment, yet little empirical evidence exists on how they create their own fairness criteria or what kinds of criteria they produce - knowledge that could not only inform experts' fairness evaluation and mitigation, but also guide the design of AI assessment tools. We address this gap through a qualitative user study with 18 participants in a credit rating scenario. Participants first articulated their fairness notions in their own words. Then, participants turned them into concrete quantified and operationalized fairness criteria, through an interactive prototype we designed. Our findings provide empirical evidence of the process through which people's fairness notions emerge via grounding in model features, and uncover a diverse set of individuals' custom-defined criteria for both outcome and procedural fairness. We provide design implications for processes and tools that support more inclusive and value-sensitive AI fairness assessment.

</details>


### [434] [Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation](https://arxiv.org/abs/2602.07023)
*Zeping Li,Guancheng Wan,Keyang Chen,Yu Chen,Yiwen Zhao,Philip Torr,Guangnan Ye,Zhenfei Yin,Hongfeng Chai*

Main category: q-fin.TR

Relevance: 75.0

TL;DR: 评估LLM智能体在股票市场模拟中的行为是否与金融理论一致，发现当前LLM的策略切换行为仅部分符合行为金融学理论


<details>
  <summary>Details</summary>
Motivation: 随着LLM被越来越多地用作金融股票市场模拟中的智能体，需要验证这些智能体的行为是否与现实市场参与者一致，这对模拟结果的有效性至关重要

Method: 选择股票市场场景，将四种行为金融驱动因素（损失厌恶、羊群效应、财富分化、价格错位）作为人格特质通过提示词设置并长期存储。在为期一年的模拟中，智能体处理每日价格-成交量数据，在指定风格下交易，每10个交易日重新评估策略。引入四个对齐指标，使用Mann-Whitney U检验比较智能体风格切换行为与金融理论

Result: 结果显示，近期LLM的切换行为仅部分与行为金融学理论一致，表明需要进一步改进以使智能体行为与金融理论对齐

Conclusion: LLM智能体在金融模拟中的行为与金融理论的对齐程度有限，需要进一步改进智能体设计以提高模拟的有效性和可信度

Abstract: Recent works have increasingly applied Large Language Models (LLMs) as agents in financial stock market simulations to test if micro-level behaviors aggregate into macro-level phenomena. However, a crucial question arises: Do LLM agents' behaviors align with real market participants? This alignment is key to the validity of simulation results. To explore this, we select a financial stock market scenario to test behavioral consistency. Investors are typically classified as fundamental or technical traders, but most simulations fix strategies at initialization, failing to reflect real-world trading dynamics. In this work, we assess whether agents' strategy switching aligns with financial theory, providing a framework for this evaluation. We operationalize four behavioral-finance drivers-loss aversion, herding, wealth differentiation, and price misalignment-as personality traits set via prompting and stored long-term. In year-long simulations, agents process daily price-volume data, trade under a designated style, and reassess their strategy every 10 trading days. We introduce four alignment metrics and use Mann-Whitney U tests to compare agents' style-switching behavior with financial theory. Our results show that recent LLMs' switching behavior is only partially consistent with behavioral-finance theories, highlighting the need for further refinement in aligning agent behavior with financial theory.

</details>


### [435] [Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation](https://arxiv.org/abs/2602.07083)
*Yongqing Jiang,Jianze Wang,Zhiqi Shen,Zhenghong Lin,Jiayuan Wang,Yijian Yang,Kaoshan Dai,Haoran Luo*

Main category: cs.SE

Relevance: 75.0

TL;DR: 提出了一个物理一致的自动建筑建模框架，通过领域知识构建、约束导向的模型对齐和验证驱动的评估，解决LLM生成建模代码中的物理不一致问题。


<details>
  <summary>Details</summary>
Motivation: 结构建模是计算工程科学的基础，微小的物理不一致或规范违反都可能使下游模拟无效。虽然LLM在自动生成建模代码方面展现出潜力，但在严格的工程约束下，不可执行或物理不一致的输出仍然普遍存在。

Method: 提出了一个物理一致的自动建筑建模框架，包括：1）CivilInstruct领域特定数据集，形式化结构工程知识和约束推理；2）两阶段微调策略，强制约束满足和API合规性；3）MBEval验证驱动的基准测试，通过闭环验证评估可执行性和结构动力学一致性。

Result: 实验结果表明，在严格的验证指标上相比基线模型有持续改进，显著减少了幻觉输出和不合规输出。

Conclusion: 该框架通过整合领域知识、约束对齐和验证驱动评估，有效提高了LLM在结构建模任务中的物理一致性和可靠性，为工程领域的可信AI应用提供了解决方案。

Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.

</details>


### [436] [Lemon Agent Technical Report](https://arxiv.org/abs/2602.07092)
*Haipeng Jiang,Kailong Ren,Zimo Yin,Zhetao Sun,Xin Gan,Guangyi Lv,Ming He,Peng Wang,Congli Yin,Hong Pan,Changwen Zhang,Shan Tong,Zhengyu Xu,Zeping Chen,Yubin Huangfu,Yanzhi Xu,Xing Su,Qin Feng,Dong An,Jianping Fan*

Main category: cs.MA

Relevance: 75.0

TL;DR: Lemon Agent是一个基于AgentCortex框架的多智能体编排-工作者系统，通过自适应任务执行机制优化资源效率、上下文管理和多模态感知，在GAIA基准上达到91.36%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的智能体系统在处理复杂长时程任务时存在资源效率低下、上下文管理不足和多模态感知有限等问题，需要更高效的系统架构来解决这些挑战。

Method: 提出AgentCortex框架，采用编排-工作者两层架构，包含分层自适应调度机制、三级渐进式上下文管理策略、自进化记忆系统和增强的MCP工具集。

Result: 在GAIA基准上达到91.36%的整体准确率，在xbench-DeepSearch排行榜上以77+分获得首位，展现了卓越的性能表现。

Conclusion: Lemon Agent通过创新的多智能体架构有效解决了LLM智能体系统的资源效率、上下文管理和任务协调问题，为复杂任务处理提供了高效解决方案。

Abstract: Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.

</details>


### [437] [RealFin: How Well Do LLMs Reason About Finance When Users Leave Things Unsaid?](https://arxiv.org/abs/2602.07096)
*Yuyang Dai,Yan Lin,Zhuohan Xie,Yuxia Wang*

Main category: q-fin.ST

Relevance: 75.0

TL;DR: REALFIN是一个双语金融推理基准，通过系统性地移除考试风格问题中的关键前提来评估模型识别信息缺失的能力，发现当前模型在缺乏足够信息时倾向于过度承诺和猜测。


<details>
  <summary>Details</summary>
Motivation: 可靠的金融推理不仅需要知道如何回答问题，还需要知道何时无法证明答案的合理性。现实金融实践中，问题通常依赖于隐含假设而非明确陈述，导致问题看似可解但实际上缺乏足够信息。当前评估方法未能充分测试模型识别信息缺失的能力。

Method: 引入REALFIN双语基准，通过系统性地移除考试风格问题中的关键前提但保持语言合理性来构建评估数据集。评估模型在三种情境下的表现：1) 回答问题；2) 识别缺失信息；3) 拒绝不合理选项。

Result: 当关键条件缺失时，所有模型性能均显著下降。通用模型倾向于过度承诺和猜测，而大多数金融专用模型未能清晰识别缺失前提。模型在识别何时不应回答问题方面存在明显缺陷。

Conclusion: 当前金融模型评估存在关键差距，可靠的金融模型必须知道何时不应回答问题。REALFIN基准揭示了模型在信息不完整情境下的推理弱点，为开发更可靠的金融AI系统提供了重要评估工具。

Abstract: Reliable financial reasoning requires knowing not only how to answer, but also when an answer cannot be justified. In real financial practice, problems often rely on implicit assumptions that are taken for granted rather than stated explicitly, causing problems to appear solvable while lacking enough information for a definite answer. We introduce REALFIN, a bilingual benchmark that evaluates financial reasoning by systematically removing essential premises from exam-style questions while keeping them linguistically plausible. Based on this, we evaluate models under three formulations that test answering, recognizing missing information, and rejecting unjustified options, and find consistent performance drops when key conditions are absent. General-purpose models tend to over-commit and guess, while most finance-specialized models fail to clearly identify missing premises. These results highlight a critical gap in current evaluations and show that reliable financial models must know when a question should not be answered.

</details>


### [438] [Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks](https://arxiv.org/abs/2602.07215)
*Haiyuan Li,Hari Madhukumar,Shuangyi Yan,Yulei Wu,Dimitra Simeonidou*

Main category: eess.SY

Relevance: 75.0

TL;DR: 提出多智能体AI框架，用于移动边缘网络中延迟和公平性感知的多模态大语言模型推理，通过自然语言推理优化提示路由和模型部署，在城域测试床上实现80%延迟降低和0.90公平性指数。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在集中式推理中存在高延迟、定制性差和隐私问题，移动边缘网络部署大模型是解决方案，但面临异构多模态模型资源需求多样、提示/输出模态复杂、资源受限基础设施等新挑战。

Method: 提出多智能体AI框架，包括长期规划智能体、短期提示调度智能体和多个节点部署智能体，均基于基础语言模型，通过自然语言推理协作优化提示路由和模型部署，利用运行时遥测和历史经验。

Result: 在城域测试床上实验显示，相比基线方法，平均延迟降低超过80%，公平性（归一化Jain指数）提升至0.90，无需微调即可快速适应，为边缘环境GenAI服务优化提供通用解决方案。

Conclusion: 多智能体AI框架能有效解决移动边缘网络中多模态大语言模型推理的延迟和公平性问题，通过自然语言推理的智能体协作提供可扩展、自适应的边缘AI服务优化方案。

Abstract: Generative AI (GenAI) has transformed applications in natural language processing and content creation, yet centralized inference remains hindered by high latency, limited customizability, and privacy concerns. Deploying large models (LMs) in mobile edge networks emerges as a promising solution. However, it also poses new challenges, including heterogeneous multi-modal LMs with diverse resource demands and inference speeds, varied prompt/output modalities that complicate orchestration, and resource-limited infrastructure ill-suited for concurrent LM execution. In response, we propose a Multi-Agentic AI framework for latency- and fairness-aware multi-modal LM inference in mobile edge networks. Our solution includes a long-term planning agent, a short-term prompt scheduling agent, and multiple on-node LM deployment agents, all powered by foundation language models. These agents cooperatively optimize prompt routing and LM deployment through natural language reasoning over runtime telemetry and historical experience. To evaluate its performance, we further develop a city-wide testbed that supports network monitoring, containerized LM deployment, intra-server resource management, and inter-server communications. Experiments demonstrate that our solution reduces average latency by over 80% and improves fairness (Normalized Jain index) to 0.90 compared to other baselines. Moreover, our solution adapts quickly without fine-tuning, offering a generalizable solution for optimizing GenAI services in edge environments.

</details>


### [439] [Progressive Searching for Retrieval in RAG](https://arxiv.org/abs/2602.07297)
*Taehee Jeong,Xingzhe Zhao,Peizu Li,Markus Valvur,Weihua Zhao*

Main category: cs.IR

Relevance: 75.0

TL;DR: 提出一种用于RAG系统的渐进式搜索算法，通过从低维嵌入逐步细化到高维的多阶段搜索，在保持准确性的同时减少检索时间。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中高效准确检索的关键挑战。RAG虽然能缓解LLMs的过时信息和幻觉问题，但检索过程需要平衡维度、速度和准确性，特别是在大规模数据库中。

Method: 提出渐进式搜索算法：1）从低维嵌入开始搜索，2）逐步细化候选集，3）渐进到目标高维度的多阶段搜索。这种分层方法减少了检索时间。

Result: 渐进式搜索在RAG系统中实现了维度、速度和准确性之间的平衡，即使对于大型数据库也能实现可扩展的高性能检索。

Conclusion: 渐进式搜索算法是RAG系统中一种成本效益高的检索方法，能够有效提升检索效率同时保持所需准确性。

Abstract: Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related documents. Then, the topmost matching documents are inserted into LLMs' prompt to generate a response. Efficient and accurate searching is critical for RAG to get relevant information. We propose a cost-effective searching algorithm for retrieval process. Our progressive searching algorithm incrementally refines the candidate set through a hierarchy of searches, starting from low-dimensional embeddings and progressing into a higher, target-dimensionality. This multi-stage approach reduces retrieval time while preserving the desired accuracy. Our findings demonstrate that progressive search in RAG systems achieves a balance between dimensionality, speed, and accuracy, enabling scalable and high-performance retrieval even for large databases.

</details>


### [440] [How does longer temporal context enhance multimodal narrative video processing in the brain?](https://arxiv.org/abs/2602.07570)
*Prachi Jindal,Anant Khandelwal,Manish Gupta,Bapi S. Raju,Subba Reddy Oota,Tanmoy Chakraborty*

Main category: q-bio.NC

Relevance: 75.0

TL;DR: 该研究探讨了视频片段时长（3-12秒）和叙事任务提示如何影响大脑与模型对齐，发现增加时长能显著提升多模态大语言模型（MLLMs）的大脑对齐度，而单模态视频模型则无此效果。不同时长窗口对应不同大脑区域，叙事任务提示会引发特定区域的对齐模式。


<details>
  <summary>Details</summary>
Motivation: 理解人类和AI系统如何处理复杂叙事视频是神经科学与机器学习的交叉挑战。研究旨在探索视频片段时长和叙事任务提示如何影响大脑与模型在自然电影观看过程中的对齐，为长上下文MLLMs提供生物学相关的时间整合和可解释表示测试平台。

Method: 使用fMRI记录参与者观看完整电影时的脑活动，分析不同时长视频片段（3-12秒）和四种叙事任务提示（多场景摘要、叙事摘要、角色动机、事件边界检测）下大脑区域与模型特征的动态对齐模式。比较多模态大语言模型（MLLMs）和单模态视频模型的表现。

Result: 1. 增加片段时长显著提升MLLMs的大脑对齐度，但对单模态视频模型无改善；2. 短时窗口与感知和早期语言区域对齐，长时窗口与高阶整合区域对齐，MLLMs呈现层到皮层的层次结构；3. 叙事任务提示引发任务特异性、区域依赖的大脑对齐模式，并在高阶区域引起上下文依赖的片段级调谐变化。

Conclusion: 长形式叙事电影可作为研究长上下文MLLMs中生物学相关时间整合和可解释表示的原则性测试平台。研究揭示了MLLMs在时间整合方面的神经生物学相关性，为理解模型如何处理复杂叙事内容提供了新见解。

Abstract: Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting shape brain-model alignment during naturalistic movie watching. Using fMRI recordings from participants viewing full-length movies, we examine how brain regions sensitive to narrative context dynamically represent information over varying timescales and how these neural patterns align with model-derived features. We find that increasing clip duration substantially improves brain alignment for multimodal large language models (MLLMs), whereas unimodal video models show little to no gain. Further, shorter temporal windows align with perceptual and early language regions, while longer windows preferentially align higher-order integrative regions, mirrored by a layer-to-cortex hierarchy in MLLMs. Finally, narrative-task prompts (multi-scene summary, narrative summary, character motivation, and event boundary detection) elicit task-specific, region-dependent brain alignment patterns and context-dependent shifts in clip-level tuning in higher-order regions. Together, our results position long-form narrative movies as a principled testbed for probing biologically relevant temporal integration and interpretable representations in long-context MLLMs.

</details>


### [441] [BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models](https://arxiv.org/abs/2602.08392)
*Xin Wu,Zhixuan Liang,Yue Ma,Mengkang Hu,Zhiyuan Qin,Xiu Li*

Main category: cs.RO

Relevance: 75.0

TL;DR: 提出了BiManiBench基准，用于评估多模态大语言模型在双臂机器人操作任务中的表现，发现现有模型在空间推理和时序控制方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准主要局限于单臂操作，无法评估双臂任务所需的时空协调能力，如抬起重锅等需要双臂协同的任务。

Method: 设计了BiManiBench分层基准，包含三个层次：基础空间推理、高层动作规划、低层末端执行器控制，专门隔离双臂特有的挑战如可达性和运动学约束。

Result: 分析了30多个最先进模型，发现尽管高层推理能力强，但MLLMs在双臂空间定位和控制方面表现不佳，经常出现相互干扰和时序错误。

Conclusion: 当前范式缺乏对相互运动学约束的深入理解，未来研究需要关注双臂碰撞避免和细粒度时序序列化。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>


### [442] [SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned](https://arxiv.org/abs/2602.07666)
*Cen Zhang,Younggi Park,Fabian Fleischer,Yu-Fu Fu,Jiho Kim,Dongkwan Kim,Youngjoon Kim,Qingxiao Xu,Andrew Chin,Ze Sheng,Hanqing Zhao,Brian J. Lee,Joshua Wang,Michael Pelican,David J. Musliner,Jeff Huang,Jon Silliman,Mikel Mcdaniel,Jefferson Casavant,Isaac Goldthwaite,Nicholas Vidovich,Matthew Lehman,Taesoo Kim*

Main category: cs.CR

Relevance: 75.0

TL;DR: 本文系统分析了DARPA AI网络挑战赛(AIxCC)，这是迄今为止最大的自主网络推理系统竞赛，旨在利用LLM发现和修复开源软件漏洞。


<details>
  <summary>Details</summary>
Motivation: AIxCC是首个大规模利用LLM构建完全自主网络推理系统的竞赛，代表了AI在网络安全领域的重要应用。研究者希望通过系统分析该竞赛，了解LLM在漏洞发现和修复中的实际表现，识别技术进展和局限性。

Method: 通过分析竞赛设计文档、源代码、执行轨迹，以及与组织者和参赛团队的讨论，系统研究竞赛结构、设计决策、决赛系统架构，并超越最终得分板进行深入结果分析。

Result: 分析揭示了驱动CRS性能的关键因素，识别了团队实现的实际技术进步，并暴露了仍待解决的研究局限性。研究发现LLM在自主网络安全系统中既有潜力也有明显限制。

Conclusion: 研究为未来竞赛组织提供了经验教训，并为实际部署自主网络推理系统提供了更广泛的见解。LLM在网络安全自动化方面展现出前景，但仍需解决可靠性、可扩展性和安全性等挑战。

Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward deploying autonomous CRSs in practice.

</details>


### [443] [Debugging code world models](https://arxiv.org/abs/2602.07672)
*Babak Rahmani*

Main category: cs.SE

Relevance: 75.0

TL;DR: 代码世界模型（CWM）通过预测程序执行状态来模拟程序运行，但存在两个主要失败模式：长执行历史导致的token预算耗尽，以及字符串值状态处理中的子词分词限制问题。


<details>
  <summary>Details</summary>
Motivation: 代码世界模型作为语言模型模拟程序执行的新方法，提供了一种替代自然语言思维链的内部验证机制。然而，目前对其错误来源和局限性理解不足，需要从局部语义执行和长时状态跟踪两个角度深入研究。

Method: 从两个互补视角研究CWM：1）局部语义执行分析，使用真实代码基准测试；2）长时状态跟踪，使用受控的排列跟踪基准来隔离动作执行下的状态传播。通过对比实验，分析错误来源和模型局限性。

Result: 识别出两个主要失败模式：1）密集运行时状态产生token密集型执行轨迹，导致长执行历史程序token预算耗尽；2）失败主要集中在字符串值状态，归因于子词分词限制而非程序结构。长时退化主要由错误动作生成驱动，而非状态传播问题。

Conclusion: 代码世界模型的局限性主要源于监督效率和状态表示与程序执行及数据类型的对齐问题。研究结果为设计更高效的监督机制和更适合程序执行的状态表示提供了方向。

Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.

</details>


### [444] [HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation](https://arxiv.org/abs/2602.07739)
*Hiren Madhu,Ngoc Bui,Ali Maatouk,Leandros Tassiulas,Smita Krishnaswamy,Menglin Yang,Sukanta Ganguly,Kiran Srinivasan,Rex Ying*

Main category: cs.IR

Relevance: 75.0

TL;DR: 论文提出双曲密集检索方法，将检索增强生成（RAG）从欧几里得空间扩展到双曲空间，以更好地捕捉自然语言的层次结构，减少幻觉风险并提升检索质量。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索器主要局限于欧几里得空间，但自然语言具有从广泛主题到具体实体的层次结构，欧几里得嵌入无法有效保持这种结构，导致语义上较远的文档出现虚假相似性，增加了幻觉风险。

Method: 提出双曲密集检索方法，开发了两种模型变体：1) HyTE-FH：完全双曲Transformer；2) HyTE-H：混合架构，将预训练的欧几里得嵌入投影到双曲空间。引入Outward Einstein Midpoint作为几何感知的池化算子，防止序列聚合中的表示崩溃。

Result: 在MTEB基准上，HyTE-FH优于等效的欧几里得基线；在RAGBench上，HyTE-H在上下文相关性和答案相关性方面比欧几里得基线提升高达29%，且使用比当前最先进检索器小得多的模型。分析显示双曲表示通过基于范数的分离编码文档特异性。

Conclusion: 双曲几何为RAG系统提供了关键的几何归纳偏置，能更好地捕捉自然语言的层次结构，提高检索质量和系统可靠性，减少幻觉风险。

Abstract: Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.

</details>


### [445] [Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents](https://arxiv.org/abs/2602.07900)
*Zhi Chen,Zhensu Sun,Yuling Shi,Chao Peng,Xiaodong Gu,David Lo,Lingxiao Jiang*

Main category: cs.SE

Relevance: 75.0

TL;DR: 研究发现LLM代码代理在解决仓库级问题时编写的测试对问题解决效果影响有限，虽然测试编写被广泛采用，但测试频率与任务解决成功率无显著关联，且代理更倾向于使用打印语句而非正式断言检查。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于观察到GPT-5.2几乎不编写新测试，但性能与SWE-bench排行榜上顶级代理相当。这引发了一个关键问题：代理编写的测试是否真正改善了问题解决效果，还是仅仅模仿人类测试实践而消耗大量交互预算。

Method: 1) 在SWE-bench Verified上对六个最先进LLM的代理轨迹进行实证分析；2) 比较已解决和未解决任务中的测试编写频率；3) 分析测试类型（打印语句 vs 断言检查）；4) 通过修改四个代理的提示进行对照实验，增加或减少测试编写。

Result: 1) 测试编写被广泛采用，但已解决和未解决任务的测试编写频率相似；2) 代理更偏好使用打印语句作为观察反馈渠道，而非正式的断言检查；3) 改变测试编写量对最终结果无显著影响；4) 当前测试编写实践在自主软件工程任务中提供有限效用。

Conclusion: 当前LLM代码代理的测试编写实践可能仅提供边际效用，测试频率与任务解决成功率无显著关联，代理更倾向于使用简单的观察反馈而非正式测试验证。

Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.

</details>


### [446] [Accuracy-Delay Trade-Off in LLM Offloading via Token-Level Uncertainty](https://arxiv.org/abs/2602.07958)
*Yumin Kim,Hyeonsu Lyu,Minjae Lee,Hyun Jong Yang*

Main category: eess.SY

Relevance: 75.0

TL;DR: 提出了一种基于不确定性的移动边缘计算LLM推理卸载框架，通过令牌级不确定性度量动态决策本地执行或边缘卸载，在保证准确性的同时最小化延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在移动设备上计算密集，移动边缘计算虽然可以卸载推理任务，但在多用户环境中会引入通信延迟和服务器排队延迟。需要一种智能的卸载策略来平衡准确性和延迟。

Method: 1) 定义基于边际的令牌级不确定性度量；2) 设计贪心卸载算法(GOA)，根据不确定性度量和资源约束动态决策本地执行或边缘卸载；3) 优先卸载高不确定性查询以维持准确性。

Result: GOA在各种用户密度下均优于基线策略，在准确性和延迟之间实现了有利的权衡，且具有实际可行的计算时间。

Conclusion: GOA是MEC环境中LLM推理的可扩展有效解决方案，通过不确定性感知的卸载决策实现了延迟最小化和准确性保持。

Abstract: Large language models (LLMs) offer significant potential for intelligent mobile services but are computationally intensive for resource-constrained devices. Mobile edge computing (MEC) allows such devices to offload inference tasks to edge servers (ESs), yet introduces latency due to communication and serverside queuing, especially in multi-user environments. In this work, we propose an uncertainty-aware offloading framework that dynamically decides whether to perform inference locally or offload it to the ES, based on token-level uncertainty and resource constraints. We define a margin-based token-level uncertainty metric and demonstrate its correlation with model accuracy. Leveraging this metric, we design a greedy offloading algorithm (GOA) that minimizes delay while maintaining accuracy by prioritizing offloading for highuncertainty queries. Our experiments show that GOA consistently achieves a favorable trade-off, outperforming baseline strategies in both accuracy and latency across varying user densities, and operates with practical computation time. These results establish GOA as a scalable and effective solution for LLM inference in MEC environments.

</details>


### [447] [CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023)
*Nanda Rani,Kimberly Milner,Minghao Shao,Meet Udeshi,Haoran Xi,Venkata Sai Charan Putrevu,Saksham Aggarwal,Sandeep K. Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Muhammad Shafique,Ramesh Karri*

Main category: cs.CR

Relevance: 75.0

TL;DR: CyberExplorer是一个用于评估LLM攻击性代理的开放环境基准测试套件，包含基于真实CTF挑战构建的虚拟机和反应式多智能体框架，支持无预定义目标的自主探索。


<details>
  <summary>Details</summary>
Motivation: 现实世界的攻击性安全操作本质上是开放式的，攻击者需要探索未知攻击面、在不确定性下修正假设，并且没有保证的成功。现有的基于LLM的攻击性代理评估依赖于封闭世界设置，具有预定义目标和二元成功标准。为了填补这一空白，作者提出了CyberExplorer。

Method: 1) 基于虚拟机的开放环境基准测试，包含40个源自真实世界CTF挑战的易受攻击Web服务，代理可以在没有漏洞位置先验知识的情况下自主执行侦察、目标选择和利用；2) 支持动态探索的反应式多智能体框架，无需预定义计划。

Result: CyberExplorer能够进行超越标志恢复的细粒度评估，捕捉交互动态、协调行为、失败模式和漏洞发现信号，弥合了基准测试与真实多目标攻击场景之间的差距。

Conclusion: 该工作为LLM攻击性代理评估提供了更接近现实世界安全操作的开放环境框架，解决了现有封闭世界评估的局限性，支持更全面的代理能力评估。

Abstract: Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.

</details>


### [448] [Large Language Models in Peer-Run Community Behavioral Health Services: Understanding Peer Specialists and Service Users' Perspectives on Opportunities, Risks, and Mitigation Strategies](https://arxiv.org/abs/2602.08187)
*Cindy Peng,Megan Chai,Gao Mo,Naveen Raman,Ningjing Tang,Shannon Pagdon,Margaret Swarbrick,Nev Jones,Fei Fang,Hong Shen*

Main category: cs.HC

Relevance: 75.0

TL;DR: 本文探讨了在同伴支持服务中集成LLM推荐系统的设计挑战，通过漫画板工作坊方法，识别了规模与本地化、信任与关系动态、同伴自主权与效率之间的三大张力，提出了以生活经验为中心、重构信任为共建、将LLM定位为关系协作者的设计启示。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型进入同伴支持领域，其规模性、对话性和不透明性给情境化、信任和自主性带来了新挑战。研究旨在探索如何将LLM推荐系统整合到同伴支持服务中，同时保护同伴支持的核心关系权威和信任基础。

Method: 采用漫画板（comicboarding）这一协同设计方法，与16名同伴专家和10名服务使用者进行工作坊，探讨他们对LLM推荐系统整合到同伴支持中的看法和感知。

Result: 研究发现LLM的引入方式、约束条件和共同使用方式会重新配置室内动态，可能维持、削弱或放大同伴支持的关系权威。识别了三大张力：规模与本地化的平衡、信任与关系动态的保护、同伴自主权与效率提升的权衡。

Conclusion: 提出了以生活经验为中心的设计启示，将信任重构为共建过程，将LLM定位为高风险社区主导护理中的关系协作者而非临床工具，强调LLM应支持而非取代同伴支持的核心关系价值。

Abstract: Peer-run organizations (PROs) provide critical, recovery-based behavioral health support rooted in lived experience. As large language models (LLMs) enter this domain, their scale, conversationality, and opacity introduce new challenges for situatedness, trust, and autonomy. Partnering with Collaborative Support Programs of New Jersey (CSPNJ), a statewide PRO in the Northeastern United States, we used comicboarding, a co-design method, to conduct workshops with 16 peer specialists and 10 service users exploring perceptions of integrating an LLM-based recommendation system into peer support. Findings show that depending on how LLMs are introduced, constrained, and co-used, they can reconfigure in-room dynamics by sustaining, undermining, or amplifying the relational authority that grounds peer support. We identify opportunities, risks, and mitigation strategies across three tensions: bridging scale and locality, protecting trust and relational dynamics, and preserving peer autonomy amid efficiency gains. We contribute design implications that center lived-experience-in-the-loop, reframe trust as co-constructed, and position LLMs not as clinical tools but as relational collaborators in high-stakes, community-led care.

</details>


### [449] [SWE Context Bench: A Benchmark for Context Learning in Coding](https://arxiv.org/abs/2602.08316)
*Jared Zhu,Minhao Hu,Junde Wu*

Main category: cs.SE

Relevance: 75.0

TL;DR: SWE-ContextBench是一个专门评估编程代理经验复用能力的基准，基于SWE-Bench Lite构建，包含300个基础任务和99个相关任务，评估准确性、时间效率和成本效率三个维度。


<details>
  <summary>Details</summary>
Motivation: 当前代码仓库级软件工程任务的基准主要评估单个任务的正确性，但缺乏对代理跨相关任务复用经验能力的评估。这导致难以衡量代理积累、检索和应用先前经验的能力，以及这种复用带来的效率提升。

Method: 基于SWE-Bench Lite构建SWE-ContextBench，通过GitHub问题和拉取请求的真实依赖和引用关系，将300个基础任务扩展为包含99个相关任务的任务序列。评估三种经验复用设置：oracle引导检索、自主检索、完整执行轨迹和紧凑摘要。

Result: 正确选择的摘要化经验能显著提高解决准确率，并大幅减少运行时间和token成本，特别是在更难的任务上。相反，未过滤或错误选择的经验带来的好处有限甚至有害。

Conclusion: 经验表示和检索质量对编程代理的经验复用至关重要。SWE-ContextBench为研究编程代理的经验复用提供了一个原则性的基准。

Abstract: Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problems. As a result, the ability of agents to accumulate, retrieve, and apply prior experience, as well as the efficiency gains from such reuse, remains difficult to measure. We introduce SWE-ContextBench, a benchmark designed to explicitly evaluate experience reuse in programming agents. Built on SWE-Bench Lite, SWE-ContextBench augments 300 base tasks with 99 related tasks derived from real dependency and reference relationships among GitHub issues and pull requests, forming task sequences with shared context. The benchmark evaluates agents along three complementary dimensions: prediction accuracy, time efficiency, and cost efficiency. Using SWE-ContextBench, we study multiple experience reuse settings, including oracle guided and autonomous retrieval, as well as full execution trajectories and compact summaries. Our results show that correctly selected summarized experience improves resolution accuracy and substantially reduces runtime and token cost, particularly on harder tasks. In contrast, unfiltered or incorrectly selected experience provides limited or negative benefits. These findings highlight the importance of experience representation and retrieval quality, and position SWE-ContextBench as a principled benchmark for studying experience reuse in programming agents.

</details>


### [450] [LLMs + Security = Trouble](https://arxiv.org/abs/2602.08422)
*Benjamin Livshits*

Main category: cs.CR

Relevance: 75.0

TL;DR: 论文主张在代码生成阶段通过约束解码强制执行安全约束，而非依赖后验检测修复，以应对AI生成代码的安全漏洞长尾问题


<details>
  <summary>Details</summary>
Motivation: 当前AI生成代码的安全方法存在缺陷：1）基于概率AI的检查器/攻击者方法无法解决安全漏洞的长尾问题；2）神经符号方法需要人工介入，破坏了安全构造保证。需要更可靠的安全代码生成方法。

Method: 提出在代码生成阶段强制执行安全约束（如通过约束解码），特别针对扩散式代码模型，利用其模块化、层次化特性实现安全构造代码生成

Result: 该方法理论上能提供更强的安全保证，结合低延迟生成技术，实现安全构造代码的生成，避免后验检测修复的局限性

Conclusion: 在代码生成阶段强制执行安全约束比后验检测修复更有效，扩散式代码模型为此提供了优雅的实现路径，有望解决AI生成代码的安全长尾问题

Abstract: We argue that when it comes to producing secure code with AI, the prevailing "fighting fire with fire" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.
  While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the "vibe coding" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.
  In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.

</details>


### [451] [6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks](https://arxiv.org/abs/2602.08675)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.NI

Relevance: 75.0

TL;DR: 6G-Bench是一个用于评估6G网络中语义通信和网络级推理能力的开放基准，包含30个标准化任务和3,722个高质量多选问题，评估了22个基础模型在6G语义推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI原生6G网络的发展，需要评估模型在语义通信和网络级决策方面的能力。当前缺乏针对6G标准化任务的专门基准，无法衡量模型在实际6G场景中的语义推理能力。

Method: 从3GPP、IETF、ETSI等标准化组织的活动中提取30个决策任务，生成10,000个多选问题，通过自动过滤和专家验证保留3,722个高质量问题。评估了22个基础模型，包括密集和MoE架构、长短上下文设计以及开源和专有系统。

Result: 模型在确定性单次准确率(pass@1)上差异很大(0.22-0.82)。领先模型在意图和政策推理准确率达到0.87-0.89。在推理密集型任务上，pass@5值范围为0.20-0.91，显示模型在6G语义推理能力上的显著差异。

Conclusion: 6G-Bench为6G语义通信和网络推理提供了首个综合性基准，揭示了当前基础模型在6G特定任务上的能力差异，为6G专用模型的开发和评估提供了重要工具。

Abstract: This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T, and the O-RAN Alliance, and organizes them into five standardization-aligned capability categories. Starting from 113,475 scenarios, we generate a balanced pool of 10,000 very-hard multiple-choice questions using task-conditioned prompts that enforce multi-step quantitative reasoning under uncertainty and worst-case regret minimization over multi-turn horizons. After automated filtering and expert human validation, 3,722 questions are retained as a high-confidence evaluation set, while the full pool is released to support training and fine-tuning of 6G-specialized models. Using 6G-Bench, we evaluate 22 foundation models spanning dense and mixture-of-experts architectures, short- and long-context designs (up to 1M tokens), and both open-weight and proprietary systems. Across models, deterministic single-shot accuracy (pass@1) spans a wide range from 0.22 to 0.82, highlighting substantial variation in semantic reasoning capability. Leading models achieve intent and policy reasoning accuracy in the range 0.87--0.89, while selective robustness analysis on reasoning-intensive tasks shows pass@5 values ranging from 0.20 to 0.91. To support open science and reproducibility, we release the 6G-Bench dataset on GitHub: https://github.com/maferrag/6G-Bench

</details>


### [452] [DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories](https://arxiv.org/abs/2602.08887)
*Adam Trendowicz,Daniel Seifert,Andreas Jedlitschka,Marcus Ciolkowski,Anton Strahilov*

Main category: cs.SE

Relevance: 75.0

TL;DR: 论文提出DeepQuali方法，使用GPT-4o评估和改进敏捷软件开发中的需求质量，并在两家小公司项目中验证了LLM评估与专家判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI（特别是LLMs）在软件工程编码任务中应用广泛，但在需求工程领域，特别是需求验证方面应用有限。当前GAI在需求工程中主要用于需求获取、转换和分类，而非质量评估。需要探索LLMs在需求质量评估方面的潜力。

Method: 提出DeepQuali方法，基于GPT-4o评估和改进敏捷软件开发中的需求质量。在两个小公司的实际项目中应用该方法，将LLM的质量评估与专家判断进行比较。专家参与解决方案的走查，提供反馈并评估他们对方法的接受程度。

Result: 专家在很大程度上同意LLM的质量评估，特别是在整体评分和解释方面。但专家之间在详细评分上并不总是一致，表明专业知识和经验可能影响判断。专家认可该方法的实用性，但批评其缺乏与工作流程的集成。

Conclusion: LLMs在支持软件工程师进行需求质量评估和改进方面具有潜力。明确使用质量模型和解释性反馈可以提高接受度。需要更好的工作流程集成以提升实用性。

Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.

</details>


### [453] [Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion](https://arxiv.org/abs/2602.08917)
*Minghan Li,Ercong Nie,Siqi Zhao,Tongna Chen,Huiping Huang,Guodong Zhou*

Main category: cs.IR

Relevance: 75.0

TL;DR: 提出自动化、领域自适应的查询扩展框架，通过BM25-MonoT5构建领域内示例池，使用无监督聚类选择多样化演示，并引入双LLM集成方法提升扩展质量。


<details>
  <summary>Details</summary>
Motivation: 传统查询扩展方法依赖人工设计的提示、手动选择的示例或单一LLM，导致可扩展性差且对领域迁移敏感。需要自动化、领域自适应的解决方案。

Method: 1) 使用BM25-MonoT5流水线构建领域内示例池；2) 基于聚类的无监督策略选择多样化演示；3) 双LLM集成：两个异构LLM独立生成扩展，第三个LLM进行整合。

Result: 在TREC DL20、DBPedia和SciFact数据集上，集成方法相比BM25、Rocchio、零样本和固定少样本基线获得一致且统计显著的性能提升。

Conclusion: 该框架为示例选择和多LLM生成提供了可复现的测试平台，并为实际应用提供了无需标注的实用查询扩展解决方案。

Abstract: Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.

</details>


### [454] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

Relevance: 65.0

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过视觉编辑、树状结构建模和智能体驱动查询来解决现有方法的局限性，在准确性和可用性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答需要精确提取单元格内容和位置，并恢复表格布局中编码的隐式逻辑结构、层次关系和语义关联。现有方法如Text-to-SQL需要将半结构化表格转换为结构化格式导致信息丢失，而Text-to-Code和多模态LLM方法难以处理复杂布局且答案不准确。

Method: ST-Raptor是一个智能体系统，提供交互式分析环境，结合视觉编辑、树状结构建模和智能体驱动查询来解决半结构化表格理解问题。

Result: 在基准测试和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性方面均优于现有方法。

Conclusion: ST-Raptor通过创新的智能体系统方法有效解决了半结构化表格问答的挑战，提供了准确且用户友好的表格理解解决方案。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [455] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出了LINCSQA基准测试和PBio-Agent多智能体框架，用于预测批量细胞环境中复杂化学扰动下的靶基因调控，通过难度感知任务排序和迭代知识精炼提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单细胞实验中的基因扰动，而批量细胞化学扰动对药物发现至关重要。大语言模型在处理高维扰动结果时容易混淆，需要专门的方法来预测复杂化学扰动下的基因调控响应。

Method: 提出PBio-Agent多智能体框架，包含：1) 难度感知任务排序，将基因按预测难度分组；2) 迭代知识精炼，利用已预测基因的因果结构辅助预测困难基因；3) 生物知识图谱增强的专门智能体；4) 合成智能体整合输出；5) 专门判断器确保逻辑一致性。

Result: PBio-Agent在LINCSQA和PerturbQA基准测试上均优于现有基线方法，即使较小的模型也能在不额外训练的情况下预测和解释复杂的生物过程。

Conclusion: 通过多智能体框架整合难度感知任务排序和迭代知识精炼，能够有效预测批量细胞环境中化学扰动下的基因调控响应，为药物发现提供有力工具。

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [456] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

Relevance: 65.0

TL;DR: SleepMaMi是一个睡眠基础模型，采用分层双编码器设计，能够同时建模整夜睡眠的宏观结构和生物信号的微观形态，在睡眠医学领域超越了传统的任务特定模型。


<details>
  <summary>Details</summary>
Motivation: 睡眠医学目前主要依赖任务特定的模型，这些模型专注于局部微观结构特征，忽略了多模态PSG数据的丰富上下文，无法捕捉整夜睡眠的全局宏观结构。需要统一的基础模型来克服这些限制。

Method: 采用分层双编码器设计：宏观编码器通过人口统计学引导的对比学习建模整夜时间依赖关系；微观编码器通过混合掩码自编码器和多模态对比目标优化，捕捉生物信号的短期特征。在超过20,000个PSG记录（158K小时）上进行预训练。

Result: SleepMaMi在多种下游任务中超越了现有的基础模型，展示了卓越的泛化能力和标签高效的临床睡眠分析适应性。

Conclusion: SleepMaMi成功地将基础模型范式引入睡眠医学，能够同时掌握小时级的睡眠架构和细粒度的信号形态，为临床睡眠分析提供了更强大的工具。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [457] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

Relevance: 65.0

TL;DR: 本文提出了一个基于统一基础本体的信任参考本体论(ONTrust)，旨在为信任概念提供形式化建模，支持信息建模、自动推理、信息集成和语义互操作等任务。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和区块链等技术的发展，建立可信系统变得日益重要。这些新技术的采用很大程度上依赖于信任，因此需要为信任概念提供坚实的本体论基础，使其既能被人类理解，也能被机器处理。

Method: 开发了基于统一基础本体论(UFO)的信任参考本体论(ONTrust)，使用OntoUML语言进行规范。该本体论形式化定义了信任概念及其不同类型，描述了影响信任的各种因素，并解释了信任关系中风险的产生机制。

Result: ONTrust已应用于多个领域：概念建模和企业架构设计、语言评估与(再)设计、信任管理、需求工程，以及在情感人机协作背景下的可信人工智能。通过两个文献案例研究展示了ONTrust的实际应用。

Conclusion: ONTrust为信任提供了坚实的本体论基础，支持跨领域应用，特别是在可信人工智能和新兴技术背景下，为构建可信系统提供了重要的概念框架。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [458] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

Relevance: 65.0

TL;DR: EventCast：一个将未来事件知识整合到时间序列预测中的模块化框架，利用LLM进行事件驱动推理，在电商需求预测中显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在闪购、假日促销等突发事件期间经常失效，因为这些时期需求模式会发生突然且不可预测的变化。需要一种能够整合未来事件知识的预测方法。

Method: EventCast使用LLM处理非结构化业务数据（如营销活动、假日安排、卖家激励），将其转换为可解释的文本摘要。这些摘要与历史需求特征通过双塔架构融合，实现准确、可解释的预测。

Result: 在4个国家160个地区10个月的真实电商场景中，EventCast相比无事件知识的变体在MAE和MSE上分别提升86.9%和97.7%，相比最佳工业基线在事件驱动期间分别减少57.0%和83.3%。

Conclusion: EventCast提供了一个实用的解决方案，通过LLM的事件驱动推理能力，显著改善动态电商环境中的运营决策，已自2025年3月起部署到实际工业管道中。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [459] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

Relevance: 65.0

TL;DR: Geo-coder是一个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动的代码演化实现几何建模，在几何重建精度和视觉一致性方面领先，并开源了数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 程序代码作为连接视觉和逻辑的桥梁，为通过几何操作增强大模型多模态推理能力提供了可行的监督方法。然而，当前的逆向图形方法在准确重建复杂几何细节方面面临巨大挑战，常常导致关键几何约束丢失或结构失真。

Method: 提出Geo-coder——首个基于多智能体系统的几何图像逆向编程框架。方法创新性地将过程解耦为：1）通过像素级锚定进行几何建模，利用视觉算子和大模型的互补优势精确捕捉像素坐标和视觉属性；2）度量驱动的代码演化，引入合成-渲染-验证闭环，双向视觉反馈驱动代码自校正。

Result: 大量实验表明，Geo-coder在几何重建精度和视觉一致性方面取得显著领先。通过有效保留核心几何语义，重建图像在多模态推理任务中表现出与原始图像相当的性能。开源了包含1500多个样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder框架在几何图像逆向编程方面表现出色，有效解决了复杂几何细节重建的挑战，为后续研究提供了坚实的数据和模型基础。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [460] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

Relevance: 65.0

TL;DR: Minitap是一个多智能体系统，在AndroidWorld基准测试中实现了100%成功率，首次完全解决了所有116个任务，超越了人类表现（80%）。


<details>
  <summary>Details</summary>
Motivation: 解决单智能体架构在移动设备任务执行中的失败问题，包括：混合推理轨迹导致的上下文污染、未被检测到的静默文本输入失败、以及无逃脱的重复动作循环。

Method: 通过三个针对性机制：1）六个专门化智能体之间的认知分离；2）基于设备状态的文本输入确定性后验证；3）检测循环并触发策略改变的元认知推理。

Result: 在AndroidWorld基准测试中达到100%成功率，首次完全解决所有116个任务。消融实验显示：多智能体分解贡献+21分，验证执行+7分，元认知+9分。

Conclusion: Minitap通过多智能体分解、验证执行和元认知推理，成功解决了移动设备任务执行的挑战，超越了人类表现，并开源发布。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [461] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

Relevance: 65.0

TL;DR: LQA：轻量级量化自适应框架，用于在边缘设备上部署视觉语言模型，通过选择性混合量化和无梯度测试时适应，在分布偏移下实现高效鲁棒的VLM部署。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署视觉语言模型面临资源限制和分布偏移下的性能下降问题。现有测试时适应方法资源消耗过大，不适合边缘设备部署。需要一种轻量级、高效的适应框架来解决这些问题。

Method: 提出LQA框架，包含两个核心组件：1) 选择性混合量化(SHQ)：模态感知的量化策略；2) 量化无梯度适应机制：在测试时进行轻量级适应，无需梯度计算。结合量化压缩和高效适应，实现边缘设备部署。

Result: 在合成和真实世界分布偏移实验中，LQA将整体适应性能提升4.5%，内存使用低于全精度模型，显著优于基于梯度的TTA方法，在七个开源数据集上实现高达19.9倍的内存使用降低。

Conclusion: LQA为边缘设备上的VLM部署提供了实用路径，实现了鲁棒、隐私保护和高效的部署方案，解决了资源受限环境下的分布偏移适应问题。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [462] [Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers](https://arxiv.org/abs/2602.08021)
*Zhan-Yi Liao,Jaewon Yoo,Hao-Tsung Yang,Po-An Chen*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出一种基于条件高斯网络分类器的结构感知、鲁棒性导向的反事实解释搜索方法，通过有向无环图编码特征依赖关系，采用收敛保证的切割集过程进行对抗优化，使用分段McCormick松弛将非凸二次问题转化为混合整数线性规划。


<details>
  <summary>Details</summary>
Motivation: 反事实解释是解释性AI的核心技术，用于解释模型决策并提供可操作的替代方案。现有方法通常需要额外约束来确保与模型结构假设的一致性，且在处理特征依赖关系和非凸优化问题时存在挑战。本研究旨在开发一种能够自然嵌入特征关系、保证全局鲁棒性的反事实解释方法。

Method: 1) 使用条件高斯网络分类器(CGNC)作为基础模型，其生成式结构通过有向无环图编码特征间的条件依赖和潜在因果关系；2) 采用收敛保证的切割集过程作为对抗优化框架，迭代逼近满足全局鲁棒性条件的解；3) 针对特征依赖引起的非凸二次结构，应用分段McCormick松弛将问题重构为混合整数线性规划(MILP)，确保全局最优性。

Result: 实验结果表明，该方法实现了强大的鲁棒性，特别是直接全局优化原始公式提供了特别稳定和高效的结果。该方法在反事实搜索中能够有效处理特征依赖关系，确保解释的合理性和可操作性。

Conclusion: 提出的框架将结构感知和鲁棒性导向相结合，通过CGNC的自然结构嵌入特征关系，避免了额外约束需求。切割集过程和MILP重构确保了全局最优性和收敛保证。该框架可扩展到更复杂的约束设置，为非凸二次公式下的反事实推理奠定了基础。

Abstract: Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model's structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.

</details>


### [463] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出一个两阶段梯度框架，用于多智能体强化学习中的可解释故障检测与归因，包括识别初始故障源、验证多米诺效应和追踪故障传播路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在安全关键领域应用日益广泛，但现有的故障检测方法缺乏可解释性，无法有效识别初始故障源、解释多米诺效应和追踪故障传播路径。

Method: 两阶段梯度框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的智能体级故障检测；第二阶段通过评论家导数的几何分析（一阶敏感性和二阶曲率）构建可解释的传染图，验证故障传播路径。

Result: 在Simple Spread（3和5智能体）和StarCraft II环境中评估，使用MADDPG和HATRPO算法，实现了88.2-99.4%的初始故障源检测准确率，并提供可解释的几何证据支持检测决策。

Conclusion: 该框架超越了黑盒检测方法，提供了梯度层面的可解释法医分析工具，为安全关键多智能体系统中的级联故障诊断提供了实用解决方案。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [464] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出以效应为中心、可接受性优先的因果发现框架，强调通过可识别性、稳定性和证伪性评估因果图，而非仅依赖图恢复准确性。在游戏遥测数据中验证，发现许多统计上合理的因果图无法支持点识别因果查询，而不同算法在可识别时能收敛到一致的效应估计。


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法广泛应用于大规模遥测数据来估计用户干预效果，但在具有强自选择性的反馈驱动系统中，这些方法对决策支持的可靠性尚不明确。需要超越图恢复准确性的评估框架。

Method: 提出效应中心、可接受性优先的框架，将发现的因果图视为结构假设，通过可识别性、稳定性和证伪性进行评估。在真实游戏遥测数据中研究早期竞争性游戏对短期留存的影响，比较不同因果发现算法的表现。

Result: 研究发现：1）许多统计上合理的因果发现输出在施加最小时间和语义约束后无法支持点识别因果查询；2）当可识别时，不同算法家族能收敛到相似的决策一致效应估计，尽管图结构差异很大；3）这些收敛估计能通过安慰剂、子采样和敏感性证伪测试；4）其他方法表现出零星的可接受性和阈值敏感或衰减效应。

Conclusion: 图级指标本身不足以作为因果可靠性的代理。在遥测驱动系统中获得可信因果结论需要优先考虑可接受性和效应级验证，而不仅仅是因果结构恢复。

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [465] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

Relevance: 65.0

TL;DR: 本文提出了一种基于全局工作空间理论(GWT)的自上而下注意力机制，用于多模态集成系统中的模态选择，提高了噪声鲁棒性和跨任务/跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全局工作空间理论(GWT)为多模态集成提供了认知神经科学基础，但现有的GWT实现主要关注多模态表示能力，相关的注意力机制研究不足。需要开发有效的注意力机制来选择全局工作空间中的相关模态。

Method: 提出了一种自上而下的注意力机制，用于在全局工作空间中选择模态。在两个多模态数据集(Simple Shapes和MM-IMDb 1.0)上评估该机制，并与现有多模态注意力模型进行比较。

Result: 1) 注意力机制提高了全局工作空间系统在两个数据集上的噪声鲁棒性；2) 展示了文献中多模态注意力模型不具备的跨任务和跨模态泛化能力；3) 在MM-IMDb 1.0基准测试中，该机制使全局工作空间达到与最先进方法竞争的水平。

Conclusion: 基于GWT的自上而下注意力机制是有效的多模态集成方法，不仅提高了噪声鲁棒性，还提供了独特的泛化能力，为多模态AI系统设计提供了新思路。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [466] [Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning](https://arxiv.org/abs/2602.08734)
*David Hudák,Maris F. L. Galesloot,Martin Tappler,Martin Kurečka,Nils Jansen,Milan Češka*

Main category: cs.AI

Relevance: 65.0

TL;DR: Lexpop框架通过深度强化学习训练神经策略，然后提取有限状态控制器来求解POMDP，并提供形式化性能保证。该方法可扩展到HM-POMDP以计算鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP求解器的可扩展性有限，且许多实际场景需要跨多个POMDP的鲁棒策略，这进一步加剧了可扩展性问题。

Method: 1) 使用深度强化学习训练循环神经网络表示的神经策略；2) 通过高效提取方法构建模仿神经策略的有限状态控制器；3) 扩展到HM-POMDP，通过迭代训练鲁棒神经策略并提取鲁棒控制器。

Result: 在大状态空间问题上，Lexpop在POMDP和HM-POMDP求解方面均优于现有最先进的求解器。

Conclusion: Lexpop框架结合了神经策略的学习能力和有限状态控制器的形式化可验证性，为大规模POMDP和HM-POMDP问题提供了有效的解决方案。

Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.

</details>


### [467] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

Relevance: 65.0

TL;DR: RC-LLM：基于残差连接和大型语言模型的微服务架构根因定位方法，通过多源遥测数据融合和LLM的上下文推理能力来建模时空因果依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在复杂的大规模微服务架构中，根因定位面临挑战。微服务间的复杂故障传播以及遥测数据（指标、日志、追踪）的高维性限制了现有根因分析方法的有效性。

Method: 提出RC-LLM方法：1）设计残差式层次融合结构来整合多源遥测数据；2）利用大型语言模型的上下文推理能力来建模时间跨度和跨微服务的因果依赖关系。

Result: 在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析方面实现了较强的准确性和效率。

Conclusion: RC-LLM通过结合残差连接结构和LLM的推理能力，有效解决了微服务架构中的根因定位问题，在准确性和效率方面表现出色。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [468] [Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation](https://arxiv.org/abs/2602.07086)
*Michael Marketsmüller,Simon Martin,Tim Schlippe*

Main category: cs.SE

Relevance: 65.0

TL;DR: 该论文评估了三种RAG变体（标准RAG、Self-RAG、CoRAG）在企业级自然语言接口中的应用，特别是在SQL查询和REST API调用生成任务上的表现，发现CoRAG在混合文档环境下表现最稳健。


<details>
  <summary>Details</summary>
Motivation: 企业系统需要能够将用户请求转换为结构化操作（如SQL查询和REST API调用）的自然语言接口。虽然LLM在代码生成方面显示出潜力，但在特定领域的企业环境中，特别是需要同时处理检索和修改任务时，其有效性仍未得到充分探索。

Method: 使用SAP Transactional Banking作为实际企业用例，构建了涵盖SQL查询生成、REST API调用生成以及需要动态任务分类的联合任务的新测试数据集。评估了三种RAG变体（标准RAG、Self-RAG、CoRAG）在18种实验配置下的表现，包括仅数据库、仅API和混合文档上下文。

Result: 结果显示RAG至关重要：没有检索时，所有任务的精确匹配准确率为0%，而检索带来了执行准确率（最高79.30%）和组件匹配准确率（最高78.86%）的显著提升。CoRAG在混合文档设置中表现最稳健，在联合任务中实现了统计显著的改进（10.29% vs 标准RAG的7.45%），主要得益于其优越的SQL生成性能（15.32% vs 11.56%）。

Conclusion: 检索策略设计是生产级自然语言接口的关键决定因素，迭代查询分解在文档异构性下优于top-k检索和二元相关性过滤。该研究为企业环境中LLM的实际应用提供了重要指导。

Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.

</details>


### [469] [An Information-Theoretic Framework for Comparing Voice and Text Explainability](https://arxiv.org/abs/2602.07179)
*Mona Rajhans,Vishal Khawarey*

Main category: cs.HC

Relevance: 65.0

TL;DR: 论文提出了一个信息论框架来分析解释模态（语音vs文本）如何影响用户对AI系统的理解和信任校准，通过模拟实验发现文本解释理解效率更高，语音解释信任校准更好，类比式解释达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI方法主要通过视觉或文本形式提供解释，但缺乏对不同解释模态（特别是语音与文本）如何影响用户理解和信任的系统分析。需要建立理论框架来量化不同解释模态在信息传递和信任建立方面的效果。

Method: 提出了一个信息论框架，将解释传递视为模型与用户之间的通信信道，定义了信息保留、理解效率和信任校准误差等指标。开发了Python模拟框架，使用基于SHAP的特征归因合成数据，评估多种模态风格配置（简洁、详细、类比式）。

Result: 文本解释在理解效率方面表现更好，语音解释在信任校准方面更优，而类比式解释在整体权衡中达到最佳平衡。框架为多模态可解释性系统的设计和基准测试提供了可复现的基础。

Conclusion: 解释模态对用户理解和信任有显著影响，需要根据具体应用场景选择合适模态。提出的信息论框架为系统评估多模态解释系统提供了理论基础，并可扩展到使用真实SHAP或LIME输出的实证研究。

Abstract: Explainable Artificial Intelligence (XAI) aims to make machine learning models transparent and trustworthy, yet most current approaches communicate explanations visually or through text. This paper introduces an information theoretic framework for analyzing how explanation modality specifically, voice versus text affects user comprehension and trust calibration in AI systems. The proposed model treats explanation delivery as a communication channel between model and user, characterized by metrics for information retention, comprehension efficiency (CE), and trust calibration error (T CE). A simulation framework implemented in Python was developed to evaluate these metrics using synthetic SHAP based feature attributions across multiple modality style configurations (brief, detailed, and analogy based). Results demonstrate that text explanations achieve higher comprehension efficiency, while voice explanations yield improved trust calibration, with analogy based delivery achieving the best overall trade off. This framework provides a reproducible foundation for designing and benchmarking multimodal explainability systems and can be extended to empirical studies using real SHAP or LIME outputs on open datasets such as the UCI Credit Approval or Kaggle Financial Transactions datasets.

</details>


### [470] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

Relevance: 65.0

TL;DR: SWM是一个模块化、经过测试和文档化的世界模型研究生态系统，提供高效的数据收集工具、标准化环境、规划算法和基线实现，旨在解决现有世界模型实现缺乏可重用性和标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型实现多为特定出版物定制，缺乏可重用性，存在bug风险，且评估标准化不足。这限制了世界模型研究的可复现性和比较性。

Method: 开发了stable-worldmodel (SWM)生态系统，包含：1) 高效数据收集工具；2) 标准化环境；3) 规划算法；4) 基线实现。每个环境支持可控的变化因素（视觉和物理属性），以支持鲁棒性和持续学习研究。

Result: 使用SWM研究了DINO-WM的零样本鲁棒性，展示了该工具在研究中的实用性。

Conclusion: SWM为世界模型研究提供了标准化、模块化的研究平台，有助于提高研究的可复现性、可比较性和鲁棒性分析。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [471] [We Should Separate Memorization from Copyright](https://arxiv.org/abs/2602.08632)
*Adi Haviv,Niva Elkin-Koren,Uri Hacohen,Roi Livni,Shay Moran*

Main category: cs.CY

Relevance: 65.0

TL;DR: 该论文认为当前技术文献中的记忆化研究不应等同于版权侵权，主张采用基于输出层面的风险评估方法，将技术评估与版权标准对齐。


<details>
  <summary>Details</summary>
Motivation: 基础模型的广泛使用带来了版权风险，但当前技术界和法律界对记忆化和复制的理解存在混淆，传统重建技术不适用于版权分析，导致技术评估与法律标准脱节。

Method: 区分技术信号：将真正指示侵权风险的信号与合法泛化或高频内容区分开来，提出基于输出层面的风险评估流程，使技术评估与既定版权标准保持一致。

Result: 论证了记忆化（技术概念）不应等同于复制（法律概念），不应作为版权侵权的代理指标，为研究、审计和政策制定提供了更原则性的基础。

Conclusion: 需要采用与版权标准对齐的输出层面风险评估方法，以更准确地评估基础模型的版权风险，避免技术概念与法律概念的混淆。

Abstract: The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.

</details>


### [472] [Assessing Reproducibility in Evolutionary Computation: A Case Study using Human- and LLM-based Assessment](https://arxiv.org/abs/2602.07059)
*Francesca Da Ros,Tarik Začiragić,Aske Plaat,Thomas Bäck,Niki van Stein*

Main category: cs.NE

Relevance: 65.0

TL;DR: 该论文研究了进化计算领域论文的可复现性实践，通过人工评估和基于LLM的自动化工具RECAP分析了十年间会议论文的可复现性水平。


<details>
  <summary>Details</summary>
Motivation: 进化计算领域的结果严重依赖于计算实验，但缺乏对已发表工作实际可复现性水平的实证证据。作者旨在系统评估该领域的可复现性实践，识别报告中的差距。

Method: 1) 引入结构化可复现性检查清单；2) 对十年间GECCO会议进化组合优化与元启发式轨道的论文进行系统人工评估；3) 开发RECAP（基于LLM的可复现性检查清单自动化管道），自动从论文文本和代码仓库评估可复现性信号。

Result: 论文平均完整性得分为0.62（满分1.0），仅36.90%的论文提供了稿件之外的附加材料。RECAP与人工评估者达成实质性一致（Cohen's k为0.67），证明自动化评估是可行的。

Conclusion: 进化计算领域存在持续的可复现性报告差距，但基于LLM的自动化工具可以有效支持大规模、系统性的可复现性实践监控。

Abstract: Reproducibility is an important requirement in evolutionary computation, where results largely depend on computational experiments. In practice, reproducibility relies on how algorithms, experimental protocols, and artifacts are documented and shared. Despite growing awareness, there is still limited empirical evidence on the actual reproducibility levels of published work in the field. In this paper, we study the reproducibility practices in papers published in the Evolutionary Combinatorial Optimization and Metaheuristics track of the Genetic and Evolutionary Computation Conference over a ten-year period. We introduce a structured reproducibility checklist and apply it through a systematic manual assessment of the selected corpus. In addition, we propose RECAP (REproducibility Checklist Automation Pipeline), an LLM-based system that automatically evaluates reproducibility signals from paper text and associated code repositories. Our analysis shows that papers achieve an average completeness score of 0.62, and that 36.90% of them provide additional material beyond the manuscript itself. We demonstrate that automated assessment is feasible: RECAP achieves substantial agreement with human evaluators (Cohen's k of 0.67). Together, these results highlight persistent gaps in reproducibility reporting and suggest that automated tools can effectively support large-scale, systematic monitoring of reproducibility practices.

</details>


### [473] [CALM: Class-Conditional Sparse Attention Vectors for Large Audio-Language Models](https://arxiv.org/abs/2602.07077)
*Videet Mehta,Liming Wang,Hilde Kuehne,Rogerio Feris,James R. Glass,M. Jehanzeb Mirza*

Main category: cs.SD

Relevance: 65.0

TL;DR: 提出Class-Conditional Sparse Attention Vectors方法，为大音频语言模型学习类别相关的注意力头权重，提升少样本分类性能


<details>
  <summary>Details</summary>
Motivation: 现有方法对选择的注意力头采用均匀权重，假设所有头对所有语义类别贡献相等，但实际上不同注意力头可能在不同类别上有不同专长

Method: 提出类别条件稀疏注意力向量方法，学习类别相关的注意力头重要性权重，使单个头能在不同语义类别上专业化，并根据估计的可靠性按比例贡献于集成预测

Result: 在多个少样本音频和视听分类基准测试中，相比最先进的均匀投票方法，音频分类提升14.52%，视听分类提升1.53%，欺骗检测提升8.35%

Conclusion: 类别条件权重方法能更好地利用大音频语言模型中注意力头的专业化能力，显著提升少样本分类性能

Abstract: Large audio-language models (LALMs) exhibit strong zero-shot capabilities in multiple downstream tasks, such as audio question answering (AQA) and abstract reasoning; however, these models still lag behind specialized models for certain discriminative tasks (e.g., audio classification). Recent studies show that sparse subsets of attention heads within an LALM can serve as strong discriminative feature extractors for downstream tasks such as classification via simple voting schemes. However, these methods assign uniform weights to all selected heads, implicitly assuming that each head contributes equally across all semantic categories. In this work, we propose Class-Conditional Sparse Attention Vectors for Large Audio-Language Models, a few-shot classification method that learns class-dependent importance weights over attention heads. This formulation allows individual heads to specialize in distinct semantic categories and to contribute to ensemble predictions proportionally to their estimated reliability. Experiments on multiple few-shot audio and audiovisual classification benchmarks and tasks demonstrate that our method consistently outperforms state-of-the-art uniform voting-based approaches by up to 14.52%, 1.53%, 8.35% absolute gains for audio classification, audio-visual classification, and spoofing detection respectively.

</details>


### [474] [Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks](https://arxiv.org/abs/2602.07090)
*Yu-Che Tsai,Hsiang Hsiao,Kuan-Yu Chen,Shou-De Lin*

Main category: cs.CR

Relevance: 65.0

TL;DR: SPARSE框架：针对文本嵌入的隐私保护方法，通过可微分掩码学习和马氏机制实现概念特定的隐私保护，在保护敏感概念的同时保持下游任务性能


<details>
  <summary>Details</summary>
Motivation: 文本嵌入面临严重的隐私风险，嵌入反转攻击可能暴露敏感属性或重建原始文本。现有差分隐私防御假设所有维度敏感性相同，导致过度噪声注入和效用下降。

Method: SPARSE框架包含两个核心组件：1) 可微分掩码学习，识别用户定义概念的隐私敏感维度；2) 马氏机制，根据维度敏感性应用椭圆噪声。与传统球形噪声注入不同，SPARSE选择性地扰动隐私敏感维度，同时保留非敏感语义。

Result: 在六个数据集、三种嵌入模型和多种攻击场景下的评估表明，SPARSE持续减少隐私泄露，同时在多项下游任务中相比最先进的差分隐私方法获得更优性能。

Conclusion: SPARSE提供了一种用户中心的概念特定隐私保护框架，通过选择性噪声注入在隐私保护和效用之间实现更好平衡，为文本嵌入的隐私保护提供了新方向。

Abstract: Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.

</details>


### [475] [Reasoning-Augmented Representations for Multimodal Retrieval](https://arxiv.org/abs/2602.07125)
*Jianrui Zhang,Anirudh Sundara Rajan,Brandon Han,Soochahn Lee,Sukanta Ganguly,Yong Jae Lee*

Main category: cs.IR

Relevance: 65.0

TL;DR: 该论文提出了一种数据中心的框架，通过外部化推理来增强多模态检索，使用视觉语言模型为语料库条目生成密集描述、解析查询中的模糊多模态引用，并将冗长指令重写为简洁的检索约束，从而提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态检索模型在处理需要潜在推理的查询时表现脆弱，例如解决未明确指定的引用或匹配组合约束。作者认为这种脆弱性主要是数据引起的：当图像包含"沉默"证据且查询隐含关键语义时，单一嵌入过程必须同时进行推理和压缩，容易导致虚假特征匹配。

Method: 提出数据中心的框架，将推理过程外部化于检索之前。使用强大的视觉语言模型：1）为语料库条目生成密集描述，使视觉证据显式化；2）解析查询中的模糊多模态引用；3）将冗长指令重写为简洁的检索约束。不仅进行推理时增强，还训练检索器在这些语义密集的表示上进行训练以避免分布偏移。

Result: 在M-BEIR基准测试中，推理增强的训练方法相比强基线取得了持续提升。消融实验显示：语料库增强主要受益于知识密集型查询，而查询增强对于组合修改请求至关重要。

Conclusion: 通过将推理外部化并训练检索器处理语义密集的表示，可以有效解决多模态检索中的潜在推理问题，提升检索性能，特别是在处理知识密集型和组合修改查询时。

Abstract: Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry "silent" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.

</details>


### [476] [KRONE: Hierarchical and Modular Log Anomaly Detection](https://arxiv.org/abs/2602.07303)
*Lei Ma,Jinyang Liu,Tieying Zhang,Peter M. VanNostrand,Dennis M. Hofmann,Lei Cao,Elke A. Rundensteiner,Jianjun Chen*

Main category: cs.DB

Relevance: 65.0

TL;DR: KRONE：首个层次化日志异常检测框架，通过自动从扁平日志推导执行层次结构，实现模块化多级异常检测，显著提升检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 日志源自嵌套组件执行，但存储为扁平序列后丢失了结构信息。现有方法可能错过真实依赖关系，同时学习到无关事件间的虚假依赖。需要保留执行层次结构进行更准确的异常检测。

Method: 提出KRONE Log Abstraction Model从日志数据捕获应用特定的语义层次结构，递归地将日志序列分解为多个层次的执行块（KRONE Seqs），将序列级异常检测转化为模块化的KRONE Seq级检测任务。采用混合模块化检测机制，动态路由到高效的本地上下文检测器和嵌套感知检测器，支持LLM-based异常检测和解释。

Result: 在三个公共基准和一个字节跳动云工业数据集上，KRONE在检测精度、F1分数、数据效率、资源效率和可解释性方面均取得一致改进。F1分数比先前方法提升超过10个百分点，同时将LLM使用减少到仅测试数据的一小部分。

Conclusion: KRONE通过层次化方法有效解决了日志异常检测中的结构丢失问题，实现了更准确、高效和可解释的检测，为系统故障和安全风险发现提供了新框架。

Abstract: Log anomaly detection is crucial for uncovering system failures and security risks. Although logs originate from nested component executions with clear boundaries, this structure is lost when they are stored as flat sequences. As a result, state-of-the-art methods risk missing true dependencies within executions while learning spurious ones across unrelated events. We propose KRONE, the first hierarchical anomaly detection framework that automatically derives execution hierarchies from flat logs for modular multi-level anomaly detection. At its core, the KRONE Log Abstraction Model captures application-specific semantic hierarchies from log data. This hierarchy is then leveraged to recursively decompose log sequences into multiple levels of coherent execution chunks, referred to as KRONE Seqs, transforming sequence-level anomaly detection into a set of modular KRONE Seq-level detection tasks. For each test KRONE Seq, KRONE employs a hybrid modular detection mechanism that dynamically routes between an efficient level-independent Local-Context detector, which rapidly filters normal sequences, and a Nested-Aware detector that incorporates cross-level semantic dependencies and supports LLM-based anomaly detection and explanation. KRONE further optimizes hierarchical detection through cached result reuse and early-exit strategies. Experiments on three public benchmarks and one industrial dataset from ByteDance Cloud demonstrate that KRONE achieves consistent improvements in detection accuracy, F1-score, data efficiency, resource efficiency, and interpretability. KRONE improves the F1-score by more than 10 percentage points over prior methods while reducing LLM usage to only a small fraction of the test data.

</details>


### [477] [MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization](https://arxiv.org/abs/2602.07520)
*Shanlei Mu,Yuchen Jiang,Shikang Wu,Shiyong Hong,Tianmu Sha,Junjie Zhang,Jie Zhu,Zhe Chen,Zhe Wang,Jingjian Lin*

Main category: cs.IR

Relevance: 65.0

TL;DR: 提出MDL框架，将多场景多任务学习统一处理，借鉴LLM的提示范式，将场景和任务信息作为特殊token，通过三层注意力机制实现深度交互，显著提升工业推荐系统性能


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统采用多场景学习(MSL)和多任务学习(MTL)处理多样化用户交互，但现有方法存在两大问题：1)大规模模型参数利用不足，与复杂特征模块交互有限；2)难以在统一框架中联合建模场景和任务信息

Method: 提出统一的多分布学习(MDL)框架，借鉴LLM的提示范式，将场景和任务信息作为特殊token而非辅助输入或门控信号。包含统一信息token化模块，以及三层协同机制：特征token自注意力、领域-特征注意力、领域融合聚合，通过堆叠这些交互实现层级的"提示"激活

Result: 在真实工业数据集上显著超越SOTA的MSL和MTL基线。抖音搜索平台在线A/B测试一个月，LT30提升+0.0626%，变更查询率降低-0.3267%。已全面部署生产，每日服务数亿用户

Conclusion: MDL框架成功解决了工业推荐系统中多场景多任务学习的挑战，通过借鉴LLM提示范式实现了场景和任务信息的深度交互，显著提升了推荐系统性能并已成功部署到大规模生产环境

Abstract: Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction with complex feature modules, and (2) difficulty in jointly modeling scenario and task information in a unified framework. To address these challenges, we propose a unified \textbf{M}ulti-\textbf{D}istribution \textbf{L}earning (MDL) framework, inspired by the "prompting" paradigm in large language models (LLMs). MDL treats scenario and task information as specialized tokens rather than auxiliary inputs or gating signals. Specifically, we introduce a unified information tokenization module that transforms features, scenarios, and tasks into a unified tokenized format. To facilitate deep interaction, we design three synergistic mechanisms: (1) feature token self-attention for rich feature interactions, (2) domain-feature attention for scenario/task-adaptive feature activation, and (3) domain-fused aggregation for joint distribution prediction. By stacking these interactions, MDL enables scenario and task information to "prompt" and activate the model's vast parameter space in a bottom-up, layer-wise manner. Extensive experiments on real-world industrial datasets demonstrate that MDL significantly outperforms state-of-the-art MSL and MTL baselines. Online A/B testing on Douyin Search platform over one month yields +0.0626\% improvement in LT30 and -0.3267\% reduction in change query rate. MDL has been fully deployed in production, serving hundreds of millions of users daily.

</details>


### [478] [Evaluating Large Language Models for Detecting Architectural Decision Violations](https://arxiv.org/abs/2602.07609)
*Ruoyu Su,Alexander Bakhtin,Noman Ahmad,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

Relevance: 65.0

TL;DR: 该研究探讨了使用大语言模型（LLMs）自动检测软件架构决策记录（ADRs）违规情况的有效性。通过多模型管道分析980个ADRs，发现LLMs在显式、可代码推断的决策上表现良好，但在依赖部署配置或组织知识的隐式决策上准确性不足。


<details>
  <summary>Details</summary>
Motivation: 软件架构决策记录（ADRs）对维护软件架构质量至关重要，但许多决策违规未被发现，因为项目缺乏系统化文档和自动化检测机制。LLMs的进展为大规模自动化架构推理提供了新可能，本研究旨在探索LLMs在识别决策违规方面的有效性。

Method: 研究分析了109个GitHub仓库中的980个ADRs，采用多模型管道方法：一个LLM主模型筛选潜在决策违规，另外三个LLM独立验证推理过程。评估了模型间一致性、准确性、精确度和召回率，并辅以专家评估进行补充。

Result: LLMs在显式、可代码推断的决策上取得了显著一致性和强准确性。但对于依赖部署配置或组织知识的隐式或部署导向决策，准确性不足。模型间达成实质性一致，但在非代码相关决策上无法替代人类专家。

Conclusion: LLMs能够有意义地支持架构决策合规性验证，但对于不专注于代码的决策，尚不能完全替代人类专业知识。这表明LLMs在软件架构验证方面具有实用价值，但存在领域限制。

Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.

</details>


### [479] [Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving](https://arxiv.org/abs/2602.09018)
*Amir Mallak,Alaa Maalouf*

Main category: cs.RO

Relevance: 65.0

TL;DR: 论文系统研究自动驾驶策略的分布外鲁棒性，通过五个环境维度分解，发现ViT策略比CNN/FC更鲁棒，基础模型特征显著提升性能，但存在延迟代价。多帧输入不优于单帧，农村→城市和白天→夜晚是最大性能下降因素。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的分布外鲁棒性通常被简化为单一数值，掩盖了策略失效的具体原因。论文旨在系统分解环境因素，理解不同策略在可控扰动下的表现，为设计鲁棒驾驶策略提供指导。

Method: 在VISTA仿真环境中进行闭环控制评估，将环境分解为场景（农村/城市）、季节、天气、时间（白天/夜晚）和智能体组合五个维度。采用k因子扰动（k∈{0,1,2,3}）控制变化程度，比较FC、CNN和ViT策略，在冻结的基础模型特征上训练紧凑ViT头，并改变分布内数据的规模、多样性和时间上下文。

Result: 1) ViT策略比同等规模的CNN/FC显著更鲁棒；基础模型特征带来最先进性能但增加延迟。2) 朴素多帧输入不优于最佳单帧基线。3) 最大性能下降来自农村→城市和白天→夜晚（各约31%）。4) 基础模型特征策略在三个同时变化下保持85%以上成功率。5) 交互效应非加性：某些组合部分抵消，季节-时间组合特别有害。6) 冬季/雪天训练对单因素变化最鲁棒。7) 增加轨迹/视角提升鲁棒性，但针对性暴露困难条件可替代规模。8) 多分布内环境扩展覆盖范围并强化弱项。

Conclusion: 研究提供了自动驾驶分布外鲁棒性的系统分析，揭示了关键性能下降因素和交互效应，为基础模型特征在鲁棒驾驶策略中的价值提供了证据，并提出了可操作的设计规则。

Abstract: Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>


### [480] [Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards](https://arxiv.org/abs/2602.07783)
*Zejun Zhang,Yixin Gan,Zhenchang Xing,Tian Zhang,Yi Li,Xiwei Xu,Qinghua Lu,Liming Zhu*

Main category: cs.SE

Relevance: 65.0

TL;DR: LintCFG：基于LLM的自动化linter配置生成方法，通过领域特定语言（DSL）将自然语言编码标准编译为具体linter配置，实现跨语言、跨标准的自动化配置生成。


<details>
  <summary>Details</summary>
Motivation: 手动配置linter复杂且需要专业知识，不同编程语言、编码标准和linter工具的多样性导致重复且维护密集的配置工作。需要自动化方法来减少人工工作量。

Method: 1. 设计DSL以工具无关、结构化、可读且精确的方式表达编码规则；2. 将linter配置构建为DSL配置指令；3. 编译过程：将自然语言编码标准解析为DSL编码标准，与DSL配置指令匹配以设置配置参数，验证一致性，最终生成linter特定配置。

Result: 在Java编码标准的Checkstyle实验中：DSL表示达到90%以上的精确率和召回率；细粒度linter配置生成的准确率、精确率、召回率和F1分数接近70%（部分超过70%）；在精确率上比基线方法提升超过100%。用户研究表明提高了开发人员配置linter的效率。方法具有通用性，成功生成了JavaScript编码标准的ESLint配置。

Conclusion: LintCFG提供了一种有效的自动化linter配置生成方法，能够显著减少手动配置工作量，提高开发效率，并具有跨编程语言、编码标准和linter工具的广泛适用性。

Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.

</details>


### [481] [CLEAR: A Knowledge-Centric Vessel Trajectory Analysis Platform](https://arxiv.org/abs/2602.08482)
*Hengyu Liu,Tianyi Li,Haoyu Wang,Kristian Torp,Yushuai Li,Tiancheng Zhang,Torben Bach Pedersen,Christian S. Jensen*

Main category: cs.DB

Relevance: 65.0

TL;DR: CLEAR是一个基于大语言模型的知识中心化船舶轨迹分析平台，通过结构化数据衍生的知识图谱将原始AIS数据转化为完整、可解释、易探索的船舶轨迹。


<details>
  <summary>Details</summary>
Motivation: AIS（自动识别系统）船舶轨迹数据在海洋分析中广泛应用，但由于数据不完整和复杂性，非专家用户难以进行分析。需要一种方法将原始AIS数据转化为更易理解和探索的形式。

Method: 利用大语言模型的推理和生成能力，通过结构化数据衍生的知识图谱（SD-KG）将原始AIS数据转化为完整的、可解释的船舶轨迹。平台允许用户配置参数自动下载和处理AIS数据，观察轨迹补全和标注过程。

Result: 开发了CLEAR平台，能够将原始AIS数据转化为完整、可解释的船舶轨迹，提供SD-KG证据支持，并通过专用图查看器实现交互式探索，帮助用户直观透明地理解船舶运动。

Conclusion: CLEAR平台通过结合大语言模型和知识图谱技术，成功解决了AIS数据不完整和复杂性的问题，为非专家用户提供了直观、透明的船舶轨迹分析工具。

Abstract: Vessel trajectory data from the Automatic Identification System (AIS) is used widely in maritime analytics. Yet, analysis is difficult for non-expert users due to the incompleteness and complexity of AIS data. We present CLEAR, a knowledge-centric vessel trajectory analysis platform that aims to overcome these barriers. By leveraging the reasoning and generative capabilities of Large Language Models (LLMs), CLEAR transforms raw AIS data into complete, interpretable, and easily explorable vessel trajectories through a Structured Data-derived Knowledge Graph (SD-KG). As part of the demo, participants can configure parameters to automatically download and process AIS data, observe how trajectories are completed and annotated, inspect both raw and imputed segments together with their SD-KG evidence, and interactively explore the SD-KG through a dedicated graph viewer, gaining an intuitive and transparent understanding of vessel movements.

</details>


### [482] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

Relevance: 45.0

TL;DR: MSP-LLM：一个统一的LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题，通过引入离散材料类别作为中间决策变量，显著提升了材料合成规划的性能。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的关键瓶颈，现有方法只解决孤立子任务，缺乏统一的解决方案。需要建立一个能够同时处理前驱体选择和合成操作序列设计的完整框架。

Method: 将MSP分解为前驱体预测(PP)和合成操作预测(SOP)两个子问题，引入离散材料类别作为中间决策变量。对于SOP，采用层次化前驱体类型作为归纳偏置，并使用显式条件策略在自回归解码中保留前驱体信息。

Result: MSP-LLM在PP、SOP以及完整MSP任务上均优于现有方法，展示了有效且可扩展的材料合成规划框架。

Conclusion: MSP-LLM提供了一个统一的LLM框架来解决材料合成规划问题，通过结构化分解和化学一致性决策链，能够加速实际材料发现过程。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [483] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

Relevance: 45.0

TL;DR: 论文探讨聊天机器人信任机制，指出用户信任常源于交互设计利用认知偏见，而非系统可信度，建议将聊天机器人视为销售员而非助手，强调需要区分心理信任形成与规范可信度


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人模糊自动化系统与人类对话的边界，需要重新审视这些系统的信任基础。当前监管和政策框架倾向于用规范性术语定义信任，但用户对聊天机器人的信任往往源于行为机制，这种信任通常不是通过证明可信度获得，而是通过利用认知偏见的交互设计选择来影响用户行为

Method: 采用概念分析和理论框架重构的方法，基于观察提出将聊天机器人重新定义为高度熟练的销售员，其目标由部署组织决定。通过分析信任的不同概念共存于同一术语下的问题，区分心理信任形成与规范可信度

Result: 识别出聊天机器人信任机制中的关键问题：用户信任常被设计操纵而非真正可信度，竞争性的"信任"概念共存模糊了重要区别。提出需要进一步研究和更强支持机制来帮助用户适当校准对对话AI系统的信任

Conclusion: 需要重新框架聊天机器人的概念模型，将其视为销售员而非助手或伴侣，强调区分心理信任形成与规范可信度的重要性。未来研究应关注开发支持机制，帮助用户更准确地校准对对话AI系统的信任

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [484] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

Relevance: 45.0

TL;DR: 该论文提出了一种基于聚类和偏好多目标强化学习的方法，用于学习马尔可夫决策过程中的价值对齐模型和价值系统，以解决AI系统适应不同用户价值偏好的问题。


<details>
  <summary>Details</summary>
Motivation: AI系统需要识别人类价值并适应不同用户的价值系统，但现有方法存在价值操作化困难、缺乏价值可解释性、难以适应多样化用户偏好等问题。

Method: 提出基于聚类和偏好多目标强化学习的算法，联合学习社会衍生的价值对齐模型和代表不同用户群体的价值系统。每个聚类包含一个价值系统和近似帕累托最优策略。

Result: 在两个包含人类价值的MDP环境中评估，与最先进的PbMORL算法和基线方法进行比较。

Conclusion: 该方法能够学习社会中的价值对齐模型和价值系统，为AI系统适应多样化用户价值偏好提供了有效途径。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [485] [Multi-Scale Temporal Homeostasis Enables Efficient and Robust Neural Networks](https://arxiv.org/abs/2602.07009)
*MD Azizul Hakim*

Main category: cs.NE

Relevance: 45.0

TL;DR: 提出多尺度时间稳态（MSTH）框架，受生物神经系统启发，将超快（5ms）、快（2s）、中（5min）、慢（1h）四个时间尺度的调节机制整合到人工神经网络中，通过跨尺度协调提升网络鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在基准任务上表现良好，但在扰动下依然脆弱，限制了实际部署。而生物神经系统通过多时间尺度的稳态调节机制，能够维持数十年可靠功能。受此启发，旨在开发一个生物基础框架来增强人工神经网络的鲁棒性。

Method: 提出多尺度时间稳态（MSTH）框架，整合四个时间尺度的调节：超快（5ms）、快（2s）、中（5min）、慢（1h）。该框架实现了跨尺度协调系统，为人工神经网络提供统一的时间层次结构，超越了表面的生物模仿。通过进化优化的机制提升计算效率。

Result: 在分子、图和图像分类基准测试中，MSTH持续提升准确率，消除灾难性故障，并增强从扰动中恢复的能力。MSTH优于单尺度生物启发模型和现有最先进方法，展示了跨领域的通用性。

Conclusion: 跨尺度时间协调是稳定人工神经系统的核心原则，MSTH为构建鲁棒、有弹性且生物学可信的智能系统奠定了基础。

Abstract: Artificial neural networks achieve strong performance on benchmark tasks but remain fundamentally brittle under perturbations, limiting their deployment in real-world settings. In contrast, biological nervous systems sustain reliable function across decades through homeostatic regulation coordinated across multiple temporal scales. Inspired by this principle, this presents Multi-Scale Temporal Homeostasis (MSTH), a biologically grounded framework that integrates ultra-fast (5-ms), fast (2-s), medium (5-min) and slow (1-hrs) regulation into artificial networks. MSTH implements the cross-scale coordination system for artificial neural networks, providing a unified temporal hierarchy that moves beyond superficial biomimicry. The cross-scale coordination enhances computational efficiency through evolutionary-refined optimization mechanisms. Experiments across molecular, graph and image classification benchmarks show that MSTH consistently improves accuracy, eliminates catastrophic failures and enhances recovery from perturbations. Moreover, MSTH outperforms both single-scale bio-inspired models and established state-of-the-art methods, demonstrating generality across diverse domains. These findings establish cross-scale temporal coordination as a core principle for stabilizing artificial neural systems, positioning MSTH as a foundation for building robust, resilient and biologically faithful intelligence.

</details>


### [486] [Federated Prompt-Tuning with Heterogeneous and Incomplete Multimodal Client Data](https://arxiv.org/abs/2602.07081)
*Thu Hang Phung,Duong M. Nguyen,Thanh Trung Huynh,Quoc Viet Hung Nguyen,Trong Nghia Hoang,Phi Le Nguyen*

Main category: cs.MM

Relevance: 45.0

TL;DR: 提出了一种针对多模态数据且存在特征缺失的联邦学习场景的通用联邦提示调优框架，通过专门的客户端调优和服务器聚合设计来优化、对齐和聚合跨客户端和数据模态的提示指令。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中本地数据集多模态且输入层面存在不同分布模式特征缺失的问题，填补联邦学习与多模态提示调优之间的研究空白，传统方法主要关注单模态或集中式数据。

Method: 提出通用联邦提示调优框架，包含专门的客户端调优和服务器聚合设计，能够同时优化、对齐和聚合跨客户端和数据模态的提示指令，使提示指令能够相互补充并有效组合。

Result: 在多种多模态基准数据集上的广泛评估表明，该方法持续优于最先进的基线方法。

Conclusion: 该框架成功解决了多模态联邦学习中的特征缺失对齐问题，为实际应用场景提供了有效的解决方案。

Abstract: This paper introduces a generalized federated prompt-tuning framework for practical scenarios where local datasets are multi-modal and exhibit different distributional patterns of missing features at the input level. The proposed framework bridges the gap between federated learning and multi-modal prompt-tuning which have traditionally focused on either uni-modal or centralized data. A key challenge in this setting arises from the lack of semantic alignment between prompt instructions that encode similar distributional patterns of missing data across different clients. To address this, our framework introduces specialized client-tuning and server-aggregation designs that simultaneously optimize, align, and aggregate prompt-tuning instructions across clients and data modalities. This allows prompt instructions to complement one another and be combined effectively. Extensive evaluations on diverse multimodal benchmark datasets demonstrate that our work consistently outperforms state-of-the-art (SOTA) baselines.

</details>


### [487] [Realistic Synthetic Household Data Generation at Scale](https://arxiv.org/abs/2602.07243)
*Siddharth Singh,Ifrah Idrees,Abraham Dauhajre*

Main category: cs.RO

Relevance: 45.0

TL;DR: 提出一个生成式框架，用于大规模创建包含长期人机交互和环境的家庭数据集，通过双向耦合建模人类行为与家庭环境的相互影响。


<details>
  <summary>Details</summary>
Motivation: 现有框架生成合成数据用于长期人机交互，但未能建模人类行为与家庭环境之间的双向影响。需要大规模、多样化的数据集来开发能够进行环境推理和交互的具身AI智能体。

Method: 提出一个生成式框架，通过松散耦合的方式生成长期人机交互和环境。人类角色影响环境生成，而环境示意图和语义塑造人机交互。框架包含丰富的静态上下文（对象和环境语义）和时态上下文（人类和智能体行为）。用户可通过自然语言提示定义数据集特征。

Result: 统计评估显示与真实世界数据集（HOMER）有良好对齐（余弦相似度0.60），而合成数据集（Wang等）对齐度中等（0.27）。干预分析显示年龄、组织和睡眠模式变化具有统计显著效应（p < 0.001），效应量大（Cohen's d = 0.51-1.12），证实双向耦合将角色特征转化为可测量的环境和行为差异。

Conclusion: 该框架能够大规模生成家庭数据集，支持开发和测试家庭智能设备，通过双向耦合建模人类行为与环境的相互影响，为具身AI智能体提供丰富的数据支持。

Abstract: Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions.
  The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation.
  We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.

</details>


### [488] [Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots](https://arxiv.org/abs/2602.07434)
*Songhua Yang,Xuetao Li,Xuanye Fei,Mengde Li,Miao Li*

Main category: cs.RO

Relevance: 45.0

TL;DR: SeM²是一个基于视觉语言模型的多模态人机交互框架，通过协调语音、情感和动作实现情感丰富的机器人表达，支持云端和边缘部署。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人缺乏协调的语音、面部表情和手势表达，而实际部署需要能够在无持续云连接下自主运行的设备端解决方案。

Method: 提出SeM²框架，包含三个关键组件：多模态感知模块捕捉用户上下文线索，思维链推理进行响应规划，以及新颖的语义序列对齐机制确保语言内容和物理表达的精确时间协调。

Result: 边缘部署版本(SeM²_e)通过知识蒸馏在边缘硬件上高效运行，保持95%的相对性能。综合评估显示该方法在自然度、情感清晰度和模态一致性方面显著优于单模态基线。

Conclusion: 该框架推进了社交表达性人形机器人在多样化现实环境中的应用，实现了情感协调的多模态交互。

Abstract: Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \underline{\textit{S}}peech, \underline{\textit{E}}motion, and \underline{\textit{M}}otion, we present \textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \underline{\textit{e}}dge-deployed versions (\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.

</details>


### [489] [Nexus: Inferring Join Graphs from Metadata Alone via Iterative Low-Rank Matrix Completion](https://arxiv.org/abs/2602.08186)
*Tianji Cong,Yuanyuan Tian,Andreas Mueller,Rathijit Sen,Yeye He,Fotis Psallidas,Shaleen Deep,H. V. Jagadish*

Main category: cs.DB

Relevance: 45.0

TL;DR: 论文提出Nexus方法，仅使用元数据自动推断数据库连接关系，将连接图推断建模为低秩矩阵补全问题，并结合LLM提升准确性。


<details>
  <summary>Details</summary>
Motivation: 在大型复杂的企业数据库模式中，自动推断连接关系对于数据发现、集成和查询至关重要。然而，当只能访问元数据而无法访问实际数据值时，准确高效地识别这些关系具有挑战性。

Method: 1) 基于对真实世界模式的经验观察：连接图的邻接矩阵具有高稀疏性和低秩结构；2) 将连接图推断建模为低秩矩阵补全问题；3) 提出Nexus端到端解决方案；4) 设计新颖的期望最大化算法，交替进行低秩矩阵补全和利用LLM细化连接候选概率。

Result: 在四个数据集（包括真实生产数据集）上的广泛实验表明，Nexus显著优于现有方法。此外，Nexus可以在快速模式下运行，提供可比较的结果，速度提升高达6倍。

Conclusion: Nexus为仅使用元数据的连接图推断提供了实用高效的解决方案，特别适用于企业环境中数据访问受限的场景。

Abstract: Automatically inferring join relationships is a critical task for effective data discovery, integration, querying and reuse. However, accurately and efficiently identifying these relationships in large and complex schemas can be challenging, especially in enterprise settings where access to data values is constrained. In this paper, we introduce the problem of join graph inference when only metadata is available. We conduct an empirical study on a large number of real-world schemas and observe that join graphs when represented as adjacency matrices exhibit two key properties: high sparsity and low-rank structure. Based on these novel observations, we formulate join graph inference as a low-rank matrix completion problem and propose Nexus, an end-to-end solution using only metadata. To further enhance accuracy, we propose a novel Expectation-Maximization algorithm that alternates between low-rank matrix completion and refining join candidate probabilities by leveraging Large Language Models. Our extensive experiments demonstrate that Nexus outperforms existing methods by a significant margin on four datasets including a real-world production dataset. Additionally, Nexus can operate in a fast mode, providing comparable results with up to 6x speedup, offering a practical and efficient solution for real-world deployments.

</details>


### [490] [Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment](https://arxiv.org/abs/2602.08565)
*Leon Fröhling,Alessandro Giaconia,Edyta Paulina Bogucka,Daniele Quercia*

Main category: cs.HC

Relevance: 45.0

TL;DR: 提出一种基于Futures Wheel方法的可扩展AI风险评估框架，通过模拟智能体评估不同技术成熟度AI应用的长期系统性风险，并与人类专家和普通用户的评估进行对比。


<details>
  <summary>Details</summary>
Motivation: 解决Collingridge困境：在AI发展早期预见长期风险时知识最匮乏。现有风险评估多关注短期风险，缺乏对长期系统性风险的评估方法。

Method: 采用Futures Wheel战略预见方法，模拟in-silico智能体评估四个不同技术成熟度（TRL 2-9）的AI应用：聊天机器人伴侣、AI玩具、Griefbot和死亡应用。每个应用进行30次智能体运行，并与290名领域专家、7名领导者以及42名专家和42名普通用户的评估进行对比。

Result: 智能体生成了86-110个后果，归纳为27-47个独特风险，覆盖更多系统性后果。专家识别风险较少但更可能发生，普通用户关注更多情感相关但系统性较低的风险。

Conclusion: 提出混合预见工作流：智能体扩展系统性覆盖，人类提供情境基础。该方法可规模化评估AI长期风险，数据集已公开。

Abstract: AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents using the strategic foresight method of the Futures Wheel. We applied it to four AI uses spanning Technology Readiness Levels (TRLs): Chatbot Companion (TRL 9, mature), AI Toy (TRL 7, medium), Griefbot (TRL 5, low), and Death App (TRL 2, conceptual). Across 30 agent runs per use, agents produced 86-110 consequences, condensed into 27-47 unique risks. To benchmark the agent outputs against human perspectives, we collected evaluations from 290 domain experts and 7 leaders, and conducted Futures Wheel sessions with 42 experts and 42 laypeople. Agents generated many systemic consequences across runs. Compared with these outputs, experts identified fewer risks, typically less systemic but judged more likely, whereas laypeople surfaced more emotionally salient concerns that were generally less systemic. We propose a hybrid foresight workflow, wherein agents broaden systemic coverage, and humans provide contextual grounding. Our dataset is available at: https://social-dynamics.net/ai-risks/foresight.

</details>


### [491] [From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection](https://arxiv.org/abs/2602.09002)
*Zilin Fang,Anxing Xiao,David Hsu,Gim Hee Lee*

Main category: cs.RO

Relevance: 45.0

TL;DR: 提出一个社交机器人导航框架，结合几何规划与上下文社交推理，使用微调的视觉语言模型评估候选路径，实现实时社交优化导航。


<details>
  <summary>Details</summary>
Motivation: 人类环境中的社交导航不仅需要满足几何约束，还需考虑社会规范和活动干扰。现有方法缺乏对社交期望的上下文推理，需要将常识推理融入规划过程。

Method: 系统首先提取障碍物和人类动态生成几何可行路径，然后使用微调的视觉语言模型（VLM）基于上下文社交期望评估这些路径，选择社交最优路径。该任务特定VLM从大型基础模型蒸馏社交推理能力，实现实时适应。

Result: 在四种社交导航场景中的实验表明，该方法获得最佳整体性能：最低的个人空间侵犯时长、最小的行人面向时间，且无社交区域入侵。

Conclusion: 通过集成几何规划与基于VLM的社交推理，实现了在多样化人机交互场景中的实时社交优化导航，平衡了效率与社交合规性。

Abstract: Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>


### [492] [BERT Learns (and Teaches) Chemistry](https://arxiv.org/abs/2007.16012)
*Josh Payne,Mario Srouji,Dian Ang Yap,Vineet Kosaraju*

Main category: q-bio.BM

Relevance: 40.0

TL;DR: 该论文提出使用基于Transformer的BERT模型分析分子中的功能基团和重要亚结构，并将学习到的表示应用于毒性、溶解度、药物相似性和合成可及性等化学性质预测任务。


<details>
  <summary>Details</summary>
Motivation: 计算有机化学正变得越来越数据驱动，但仍有许多重要问题未解决，如产物预测、药物发现和分子合成优化。机器学习方法虽然有所增加，但需要从数据驱动的角度研究功能基团和分子亚结构对性质的影响。

Method: 使用基于Transformer的BERT模型处理分子字符串表示，分析注意力头的行为以识别功能基团和重要亚结构。将学习到的表示作为特征，应用于图卷积和注意力模型进行分子图结构分析，并对BERT进行微调。

Result: 模型成功识别了影响化学性质的重要分子亚结构，并将学习到的表示有效应用于毒性、溶解度、药物相似性和合成可及性等预测任务，同时提出注意力可视化作为化学从业者和学生的实用工具。

Conclusion: 基于Transformer的注意力机制为从数据驱动角度研究分子亚结构提供了有效方法，学习到的表示可用于多种化学性质预测任务，注意力可视化有助于快速识别影响化学性质的重要结构特征。

Abstract: Modern computational organic chemistry is becoming increasingly data-driven. There remain a large number of important unsolved problems in this area such as product prediction given reactants, drug discovery, and metric-optimized molecule synthesis, but efforts to solve these problems using machine learning have also increased in recent years. In this work, we propose the use of attention to study functional groups and other property-impacting molecular substructures from a data-driven perspective, using a transformer-based model (BERT) on datasets of string representations of molecules and analyzing the behavior of its attention heads. We then apply the representations of functional groups and atoms learned by the model to tackle problems of toxicity, solubility, drug-likeness, and synthesis accessibility on smaller datasets using the learned representations as features for graph convolution and attention models on the graph structure of molecules, as well as fine-tuning of BERT. Finally, we propose the use of attention visualization as a helpful tool for chemistry practitioners and students to quickly identify important substructures in various chemical properties.

</details>


### [493] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出了一个自适应认知参与框架，通过动态选择工作示例（引导示例和错误示例）来个性化学习活动，比较了贝叶斯知识追踪和深度强化学习两种自适应方法在逻辑智能辅导系统中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统面临的关键挑战是如何个性化学习活动以激发最佳认知参与水平。ICAP框架定义了四种认知参与层次，但如何根据学生特点动态调整学习活动以优化认知参与仍是一个未解决的问题。

Method: 开发了一个自适应系统，通过动态选择两种ICAP模式的工作示例来支架认知参与：主动模式的引导示例和建构模式的错误示例。比较了贝叶斯知识追踪和深度强化学习作为自适应方法，与一个非自适应基线方法在逻辑智能辅导系统中的效果。

Result: 在113名学生实验中，两种自适应策略都显著提高了学生在测试问题上的表现。BKT对低先验知识学生后测成绩提升最大，帮助他们赶上高先验知识同伴；而DRL在高先验知识学生中产生了显著更高的后测成绩。

Conclusion: 本文为认知参与和适应性之间的复杂相互作用及其对学习结果的影响提供了新的见解，展示了不同自适应方法对不同学生群体的差异化效果。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [494] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

Relevance: 35.0

TL;DR: RAPiD：一种确定性策略提取框架，将预训练的扩散轨迹规划器蒸馏为高效策略，消除扩散采样，实现8倍加速，在nuPlan和interPlan基准上达到竞争性能


<details>
  <summary>Details</summary>
Motivation: 扩散轨迹规划器能建模人类驾驶的多模态行为，但依赖迭代随机采样，难以满足实时安全关键部署需求。需要将扩散规划器蒸馏为高效确定性策略。

Method: 使用分数正则化策略优化，利用预训练扩散规划器的分数函数作为行为先验来正则化策略学习。通过模仿预测驾驶员控制器的critic提供密集安全监督，超越传统模仿学习。

Result: 在nuPlan闭环场景中达到竞争性能，比扩散基线加速8倍，在interPlan基准上实现学习型规划器中最先进的泛化能力。

Conclusion: RAPiD成功将扩散规划器蒸馏为高效确定性策略，解决了扩散模型实时部署难题，在自动驾驶轨迹规划领域具有实际应用价值。

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [495] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出后验确定性POMDPs，这是部分可观察马尔可夫决策过程的一个新子类，在该类中可达概率可以近似计算到任意精度，解决了传统POMDPs中该问题的不可判定性。


<details>
  <summary>Details</summary>
Motivation: 传统POMDPs中的许多验证和综合问题是不可判定或难处理的，特别是Madani等人的结果表明无法计算或近似可达概率。这与完全可观察的MDPs形成鲜明对比，后者可达值可在多项式时间内计算。因此需要寻找可处理的POMDP子类。

Method: 引入后验确定性POMDPs的新概念：如果下一个状态可以由当前状态、采取的动作和接收的观测唯一确定，则POMDP是后验确定性的。一旦真实状态已知，它将永远保持已知。该方法包括所有MDPs并捕获经典非平凡示例如Tiger POMDP。

Result: 证明对于后验确定性POMDPs，到达给定状态集的最大概率可以近似到任意精度。这是已知最大的POMDPs类之一，其中可达值可以近似计算。

Conclusion: 后验确定性POMDPs提供了一个理论上可处理且实际相关的POMDPs子类，扩展了可验证和可综合的序列决策问题的范围，在理论和应用上都有重要意义。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [496] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

Relevance: 35.0

TL;DR: 研究调查本科生对AI评分系统的看法，发现学生对AI缺乏情境理解和个性化表示担忧，建议AI应作为人类监督下的补充工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨AI评分系统在教育环境中的伦理问题，特别是公平性、信任度、一致性和透明度，通过比较AI生成反馈与人类教师反馈来理解学生对这些系统的看法。

Method: 采用质性研究方法，在本科计算机科学课程中调查27名学生，使用基于块编程的期末项目作为案例，基于Jobin(2019)的伦理原则框架分析AI评分系统。

Result: 研究发现学生对AI评分系统的主要担忧包括：缺乏情境理解能力、个性化不足、难以体现人类判断的灵活性。学生更偏好人类教师的反馈，认为AI应作为辅助工具而非替代品。

Conclusion: 结论是AI评分系统需要反映人类判断、灵活性和同理心，应在人类监督下作为补充工具使用。研究为设计更人性化的AI教育系统提供了原则指导。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [497] [GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank](https://arxiv.org/abs/2602.07903)
*Mingcan Wang,Junchang Xin,Zhongming Yao,Kaifu Long,Zhiqiong Wang*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出了一种基于motif的个性化PageRank（MPPR）方法，用于增强图卷积网络的消息传递过程，通过考虑高阶motif关系来改善GCN的性能、稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于消息传递神经网络（MPNNs）的图算法存在信息传播深度有限的问题，主要由于过平滑现象。传统GCNs在准确性、稳定性和计算成本方面表现不佳，且忽略了高阶关系，限制了其性能。

Method: 提出motif-based personalized PageRank（MPPR）来衡量节点间基于高阶motif关系的影响力，然后将MPPR应用于GCNs的消息传递过程，在相对"高"层次上指导消息传递。

Result: 实验结果表明，该方法在准确性、稳定性和时间消耗方面优于几乎所有基线方法。该方法可作为支撑几乎所有GCN任务的组件，实验中展示了DGCRL的应用。

Conclusion: 通过引入基于motif的个性化PageRank来考虑高阶关系，能够有效改善GCNs的消息传递过程，提升性能并解决过平滑等问题。

Abstract: The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep'. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high' level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.

</details>


### [498] [Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling](https://arxiv.org/abs/2602.08052)
*Bulent Soykan,Sean Mondesire,Ghaith Rabadi,Grace Bochenek*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出了一种基于深度强化学习（PPO-GNN）的方法来解决具有释放时间、设置和资格约束的不相关并行机调度问题，能够同时最小化总加权延迟和总设置时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理具有释放时间、设置和资格约束的不相关并行机调度问题时，难以平衡总加权延迟和总设置时间这两个目标。该问题在复杂制造调度中具有重要实际意义。

Method: 采用深度强化学习框架，结合近端策略优化（PPO）和图神经网络（GNN）。GNN用于有效表示作业、机器和设置的复杂状态，PPO代理学习直接调度策略，通过多目标奖励函数指导训练。

Result: 在基准实例上的实验结果表明，PPO-GNN代理显著优于标准调度规则和元启发式方法，在两个目标之间实现了更好的权衡。

Conclusion: 该方法为复杂制造调度提供了鲁棒且可扩展的解决方案，证明了深度强化学习在复杂调度问题中的有效性。

Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.

</details>


### [499] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出了一个五层生物安全数据等级（BDL）框架，用于根据数据训练AI模型时可能带来的生物安全风险对病原体数据进行分类，并为每个等级提出相应的技术限制措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI在生物学领域的广泛应用，训练数据成为影响AI能力的关键因素。某些生物数据可能被用于开发生物武器等有害应用，因此需要建立数据控制机制来防止AI被用于有害的生物安全应用。

Method: 提出了一个五层Biosecurity Data Level（BDL）框架，根据数据训练AI模型时可能带来的风险对病原体数据进行分类。为每个BDL等级设计了相应的技术限制措施，并提出了针对新创建的双重用途病原体数据的治理框架。

Result: 建立了一个系统化的生物安全数据分类框架，为不同风险等级的数据提供了具体的技术控制建议，为AI生物安全治理提供了实用的工具和方法。

Conclusion: 在计算和编码资源广泛可及的世界中，数据控制可能是减少令人担忧的生物AI能力扩散的最有效干预措施之一。BDL框架为实施此类控制提供了系统化的方法。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [500] [Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention](https://arxiv.org/abs/2602.08121)
*Liying Wang,Madison Lee,Yunzhang Jiang,Steven Chen,Kewei Sha,Yunhe Feng,Frank Wong,Lisa Hightow-Weidman,Weichao Yuwen*

Main category: cs.AI

Relevance: 35.0

TL;DR: 开发了Glow——一个基于生成式AI的DBT技能教练，用于HIV和物质使用风险人群，通过用户驱动的对抗性测试发现安全漏洞，特别是链分析代理存在"共情陷阱"问题


<details>
  <summary>Details</summary>
Motivation: HIV和物质使用是相互影响的流行病，具有共同的冲动性和适应不良应对机制。DBT疗法针对这些机制但面临可扩展性挑战。生成式AI有潜力提供个性化的DBT辅导，但快速发展超过了安全基础设施的建设速度。

Method: 开发了Glow——一个基于生成式AI的DBT技能教练，提供链分析和解决方案分析。与洛杉矶社区健康组织合作，对临床工作人员(n=6)和有生活经验的个体(n=28)进行可用性测试。使用HHH框架，采用用户驱动的对抗性测试，参与者识别目标行为并生成情境现实的风险探测。评估了37个风险探测交互的安全性表现。

Result: Glow适当处理了73%的风险探测，但不同代理表现差异大：解决方案分析代理表现出90%的适当处理率，而链分析代理只有44%。安全失败主要集中在鼓励物质使用和正常化有害行为。链分析代理陷入"共情陷阱"，提供强化适应不良信念的验证。此外，识别出27个DBT技能错误信息实例。

Conclusion: 这是首次对基于生成式AI的DBT辅导进行系统性安全评估，揭示了在临床试验前需要缓解的漏洞。HHH框架和用户驱动的对抗性测试为评估生成式AI心理健康干预提供了可复制的方法。

Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an "empathy trap," providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.

</details>


### [501] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出PTS-SNN框架，通过提示调优解决SSL表示与SNN之间的分布不匹配问题，实现高效的情感语音识别


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别模型计算成本高，难以部署在资源受限的边缘设备上。SNN虽然能效高，但与连续的自监督学习表示存在分布不匹配问题，高动态范围的嵌入会降低基于阈值神经元的信息编码能力。

Method: 提出Prompt-Tuned Spiking Neural Networks (PTS-SNN)：1) Temporal Shift Spiking Encoder通过参数自由的通道移位捕获局部时间依赖；2) Context-Aware Membrane Potential Calibration策略，使用Spiking Sparse Linear Attention模块聚合全局语义上下文到可学习的软提示，动态调节PLIF神经元的偏置电压，将异质输入分布中心化到响应发射范围内。

Result: 在5个多语言数据集（IEMOCAP、CASIA、EMODB等）上实验，PTS-SNN在IEMOCAP上达到73.34%准确率，与竞争性ANN相当，仅需1.19M可训练参数和每个样本0.35 mJ推理能耗。

Conclusion: PTS-SNN成功解决了SSL表示与SNN之间的分布不匹配问题，实现了高效、低能耗的语音情感识别，为边缘设备部署提供了可行方案。

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [502] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

Relevance: 35.0

TL;DR: SynthAgent：一个多智能体系统框架，用于模拟肥胖合并精神障碍患者，整合临床证据构建个性化虚拟患者，通过自主交互模拟疾病进展和治疗反应。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界数据碎片化、偏见和隐私限制的问题，为研究复杂疾病提供高保真患者模拟的途径。

Method: 提出SynthAgent多智能体系统框架，整合理赔数据、人口调查和患者中心文献的临床证据，构建具有人格特质的个性化虚拟患者，通过自主智能体交互模拟疾病进展和治疗反应。

Result: 评估100多个生成的患者显示，GPT-5和Claude 4.5 Sonnet作为核心引擎保真度最高，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent为探索患者旅程、行为动态和决策过程提供了可扩展且隐私保护的框架。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [503] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

Relevance: 35.0

TL;DR: 论文提出"氛围自动化"概念，认为生成式AI代表从算法优化到情境语义协调的认知转变，人类角色转变为"氛围工程"，需在三个层面应对教育机构转型。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI带来的根本性认知转变，超越传统机器学习作为"自动化的自动化"的框架，理解其如何通过操作化隐性规律来访问情境、语义和风格协调性。

Method: 引入"氛围自动化"概念框架，分析生成式AI如何操作化隐性规律（情境敏感模式），提出三个分析层面（世界观、产业关系、课程设计）和三个行动领域的理论框架。

Result: 识别了生成式AI的认知转变本质，提出了人类角色从算法问题规范转向"氛围工程"的转变，建立了教育机构转型的概念框架，并讨论了模式崩溃和文化同质化风险。

Conclusion: 生成式AI代表根本性认知转变，需要新的"氛围工程"方法来协调对齐和情境判断，教育机构必须在三个层面进行系统性转型，避免合成统一性回归。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [504] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

Relevance: 35.0

TL;DR: 现有TKG基准存在严重缺陷：仅通过共现统计就能达到接近SOTA的性能，无需使用时间信息。作者揭示了数据集固有偏差和评估任务过于简化的问题，并提出了新的TKG演化基准。


<details>
  <summary>Details</summary>
Motivation: 当前时序知识图谱(TKGs)预测研究虽然取得了高指标（如YAGO数据集Hits@10超过0.9），但存在严重基准缺陷。研究发现现有基准无意中引入了捷径，仅通过统计共现关系就能达到接近最优性能，完全不需要利用时间信息。这暴露了当前评估体系的不公平性。

Method: 深入分析现有基准问题的根源：1）识别数据集固有偏差；2）评估任务形式过于简化易被偏差利用。在此基础上，作者构建了TKG演化基准，包括四个偏差校正后的数据集和两个与演化过程紧密相关的新任务。

Result: 揭示了现有TKG基准的多个局限性：时间间隔知识的不合理格式化、忽视知识过时性学习、演化理解信息不足等。这些缺陷都会放大捷径效应，阻碍公平评估。提出的新基准旨在更准确地评估TKG演化建模的挑战。

Conclusion: 现有TKG预测基准存在严重缺陷，无法公平评估模型性能。作者提出的TKG演化基准通过偏差校正数据集和更贴近实际演化过程的任务设计，为领域提供了更可靠的评估框架。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [505] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

Relevance: 35.0

TL;DR: 将随机森林分类器编译为电路的方法，用于计算决策的完整原因、鲁棒性和最短翻转路径


<details>
  <summary>Details</summary>
Motivation: 随机森林是广泛使用的机器学习模型，但缺乏可解释性。现有方法在计算决策原因和解释方面效率低下，需要更高效的方法来分析随机森林的决策过程。

Method: 1) 提出将随机森林分类器编译为电路的方法，每个电路直接编码分类器的某个类别实例；2) 利用该方法获得可处理电路，用于计算决策的完整和一般原因；3) 提出计算决策鲁棒性和所有最短翻转路径的算法。

Result: 提出的方法比现有类似方法显著更高效，能够枚举决策的所有充分原因、必要原因和对比解释，计算决策鲁棒性，并识别随机森林决策的所有最短翻转方式。

Conclusion: 该方法为随机森林分类器提供了强大的可解释性工具，能够高效计算各种解释和鲁棒性分析，有助于理解和调试随机森林模型的决策过程。

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [506] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

Relevance: 35.0

TL;DR: TreeTensor：一种用于处理嵌套数据的通用容器，支持零开销应用任意函数和操作到层次化数据上，兼容主流机器学习库


<details>
  <summary>Details</summary>
Motivation: 传统张量（Tensor）在处理复杂认知AI系统中的层次化嵌套数据时存在不便和低效问题，需要一种更灵活的数据结构来处理各种模态的嵌套数据

Method: 提出TreeTensor通用嵌套数据容器，通过约束树结构和魔法工具，支持对嵌套数据应用任意函数和操作，兼容Scikit-Learn、Numpy、PyTorch等库

Result: TreeTensor在各种问题中展现出强大可用性，特别是在复杂的AlphaStar（星际争霸II）系统中，同时表现出优异的运行时效率且无额外开销

Conclusion: TreeTensor为解决AI系统中嵌套数据处理问题提供了有效方案，支持与其他方法结合扩展更多用途，如异步执行和变长数据计算

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [507] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

Relevance: 35.0

TL;DR: OSCAR是一个基于优化引导的智能体规划框架，用于组合图像检索，将启发式搜索过程重新表述为轨迹优化问题，通过离线-在线范式实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在两大问题：统一嵌入检索存在单模型近视问题，启发式智能体检索受限于次优的试错编排。需要更原则性的方法来处理视觉和文本约束的复杂推理。

Method: 提出离线-在线范式：离线阶段将CIR建模为两阶段混合整数规划问题，通过布尔集合运算推导最大化真实覆盖的最优轨迹，存储在黄金库中；在线阶段使用这些轨迹作为上下文示例来引导VLM规划器进行推理。

Result: 在三个公共基准和一个私有工业基准上，OSCAR始终优于SOTA基线。仅使用10%训练数据就能达到优越性能，展示了规划逻辑的强泛化能力而非数据集特定记忆。

Conclusion: OSCAR成功将智能体CIR从启发式搜索过程重新表述为原则性轨迹优化问题，通过离线-在线范式实现了高效、可泛化的组合图像检索。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [508] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

Relevance: 35.0

TL;DR: 该研究探索使用通用语言模型辅助认知诊断建模中的Q矩阵构建，发现AI模型生成的Q矩阵与验证Q矩阵的一致性存在较大差异，其中Google Gemini 2.5 Pro表现最佳，甚至超过人类专家，但后续版本表现下降。


<details>
  <summary>Details</summary>
Motivation: 认知诊断建模中的Q矩阵构建是一个关键但劳动密集的过程。本研究旨在探索AI工具（通用语言模型）是否能够支持Q矩阵开发，通过比较AI生成的Q矩阵与已验证的Q矩阵来评估其有效性。

Method: 研究在2025年5月使用多个AI模型，提供与人类专家相同的训练材料生成Q矩阵。使用Cohen's kappa评估AI生成Q矩阵、验证Q矩阵和人类评分者Q矩阵之间的一致性。2026年1月进行了后续分析，使用更新的AI版本。

Result: 不同AI模型间存在显著差异，Google Gemini 2.5 Pro与验证Q矩阵的一致性最高（Kappa = 0.63），超过了所有人类专家。但2026年使用新版本AI的分析显示，与验证Q矩阵的一致性反而降低。

Conclusion: AI工具在Q矩阵构建中显示出潜力，但性能存在模型间差异和版本不稳定性。需要进一步研究AI在认知诊断建模中的可靠性和一致性。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [509] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

Relevance: 35.0

TL;DR: NADEx：一种用于时序知识图谱推理的负感知扩散模型，通过结合负样本信息和余弦对齐正则化，提升未来事实预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的时序知识图谱推理存在两个问题：1）生成路径仅依赖正样本证据，忽略了有信息的负样本上下文；2）训练目标以交叉熵排序为主，虽然能改善候选排序，但对去噪嵌入的校准监督不足。

Method: 提出NADEx模型：1）将实体、关系和时序间隔的主体中心历史编码为序列嵌入；2）在前向过程中扰动查询对象，在反向过程中使用Transformer去噪器结合时序关系上下文进行重建；3）引入基于批负样本原型的余弦对齐正则化器，收紧决策边界。

Result: 在四个公开TKG基准测试上，NADEx实现了最先进的性能。

Conclusion: 通过结合负样本信息和改进的校准监督，NADEx显著提升了时序知识图谱推理的预测能力。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [510] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出了一个统一框架，用于整合多种定性推理形式（多尺度推理、时间序列、松散集成），并研究其可满足性决策及复杂度，证明了多项式时间可满足性定理。


<details>
  <summary>Details</summary>
Motivation: 定性推理能在不精确、不完整且无数值信息的情况下推断新知识，但现有研究缺乏统一的框架来处理多种扩展和组合形式。本文旨在填补这一空白，为多尺度推理、时间序列和松散集成等组合提供统一的形式化分析框架。

Method: 提出了一个形式化框架，统一了多种定性形式主义的扩展和组合。该框架支持在这些组合和扩展中进行推理，并以统一方式研究可满足性决策及其复杂度。特别地，建立了两个互补定理来保证可满足性决策是多项式时间的。

Result: 1) 开发了一个统一框架，能够处理多尺度推理、时间序列和松散集成等定性推理组合；2) 证明了两个互补定理，确保可满足性决策在多项式时间内完成；3) 利用该框架恢复了size-topology组合的已知结果；4) 扩展了定性形式主义的主要定义，包含了文献中被排除但对组合重要的定性形式主义。

Conclusion: 本文提出的统一框架为多种定性推理扩展和组合提供了形式化基础，证明了多项式时间可满足性，扩展了定性形式主义的定义范围，为复杂定性推理系统的分析和设计提供了理论工具。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [511] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

Relevance: 35.0

TL;DR: IVSR是一个结合数字孪生与自主AI代理的智能虚拟指挥平台，用于实时自适应野火灾害管理，通过多源数据融合、相似性匹配和闭环响应显著降低检测到干预的延迟。


<details>
  <summary>Details</summary>
Motivation: 全球变暖导致野火频率和强度预计到2030年增加14%，205年增加30%，传统灾害管理系统依赖静态模拟和被动数据采集，无法实时适应不断演变的野火事件。

Method: 引入智能虚拟指挥室(IVSR)，这是一个由自主AI代理增强的双向数字孪生平台。系统持续摄入多源传感器图像、天气数据和3D森林模型，创建火灾环境的实时虚拟副本。AI驱动的相似性引擎将新兴条件与预计算的灾害模拟库对齐，在专家监督下检索和校准干预策略。授权行动通过标准化程序循环回物理层，完成响应与分析之间的闭环。

Result: 通过工业合作伙伴提供的详细案例研究模拟验证IVSR，展示了在局部事件检测、隐私保护回放、基于碰撞器的火势传播预测和特定地点ML再训练方面的能力。与传统系统相比，检测到干预的延迟显著降低，资源协调更有效。

Conclusion: 通过将实时双向数字孪生与代理AI相结合，IVSR为主动、自适应的野火灾害管理提供了一个可扩展、半自动化的决策支持范式。

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [512] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出了GEBench基准测试，用于评估GUI生成中的动态交互和时间一致性，包含700个样本和五维评估指标GE-Score


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型能够基于用户指令预测未来GUI状态，但现有基准主要关注通用领域视觉保真度，缺乏对GUI特定场景中状态转换和时间一致性的评估

Method: 1) 构建GEBench基准，包含700个精心策划的样本，涵盖5个任务类别；2) 提出GE-Score五维评估指标：目标达成、交互逻辑、内容一致性、UI合理性和视觉质量；3) 对现有模型进行系统评估

Result: 当前模型在单步转换上表现良好，但在保持长时间交互序列的时间一致性和空间定位方面存在显著困难，图标解释、文本渲染和定位精度是关键瓶颈

Conclusion: 该工作为系统评估提供了基础，并为构建高保真生成式GUI环境指出了有前景的研究方向

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>


### [513] [What is Safety? Corporate Discourse, Power, and the Politics of Generative AI Safety](https://arxiv.org/abs/2602.06981)
*Ankolika De,Gabriel Lima,Yixin Zou*

Main category: cs.CY

Relevance: 35.0

TL;DR: 论文批判性分析AI公司如何通过公开文件构建和传达"安全"概念，揭示其话语策略如何巩固企业合法性、将安全规范化为实验性实践，并推动参与式议程，警告不加批判地接受这些话语可能复制企业优先事项并限制替代治理方法。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于批判性地审视领先生成式AI公司如何通过公开文件构建和传达"安全"概念。作者认为安全不仅是一个技术问题，更是一种社会技术话语，需要批判性分析来揭示其背后的权力关系和意识形态。

Method: 采用批判性话语分析方法，分析企业安全相关声明的语料库，阐释企业如何通过话语策略建立权威、责任和合法性。研究聚焦于企业公开文件中的话语构建策略。

Result: 研究发现企业通过特定话语策略：1) 巩固企业行为者的合法性；2) 将安全规范化为实验性和预期性实践；3) 推动感知上的参与式议程以实现安全技术。这些策略可能限制替代治理和设计方法。

Conclusion: 结论强调安全作为社会技术话语需要批判性审视，警告人机交互学者不应合法化企业框架，而应强调问责制、公平和正义。通过将安全话语作为权力产物进行审视，推动AI人机交互研究的批判性议程。

Abstract: This work examines how leading generative artificial intelligence companies construct and communicate the concept of "safety" through public-facing documents. Drawing on critical discourse analysis, we analyze a corpus of corporate safety-related statements to explicate how authority, responsibility, and legitimacy are discursively established. These discursive strategies consolidate legitimacy for corporate actors, normalize safety as an experimental and anticipatory practice, and push a perceived participatory agenda toward safe technologies. We argue that uncritical uptake of these discourses risks reproducing corporate priorities and constraining alternative approaches to governance and design. The contribution of this work is twofold: first, to situate safety as a sociotechnical discourse that warrants critical examination; second, to caution human-computer interaction scholars against legitimizing corporate framings, instead foregrounding accountability, equity, and justice. By interrogating safety discourses as artifacts of power, this paper advances a critical agenda for human-computer interaction scholarship on artificial intelligence.

</details>


### [514] [Adaptive Temporal Dynamics for Personalized Emotion Recognition: A Liquid Neural Network Approach](https://arxiv.org/abs/2602.06997)
*Anindya Bhattacharjee,Nittya Ananda Biswas,K. A. Shahriar,Adib Rahman*

Main category: eess.SP

Relevance: 35.0

TL;DR: 该研究首次将液态神经网络应用于基于EEG的情感识别，通过多模态框架结合卷积特征提取、可学习时间常数的液态神经网络和注意力引导融合，在PhyMER数据集上达到95.45%的准确率。


<details>
  <summary>Details</summary>
Motivation: 生理信号（如EEG）的情感识别面临非平稳、噪声大和个体差异等挑战，需要能够有效建模时间动态和个体特异性的方法。

Method: 提出多模态框架：1) 卷积特征提取；2) 具有可学习时间常数的液态神经网络；3) 注意力引导融合机制；4) 专用子网络处理EEG特征和辅助模态；5) 共享自编码器融合模块学习判别性潜在表示。

Result: 在PhyMER数据集的七个情感类别上达到95.45%的准确率，超越先前结果。时间注意力分析提供可解释的洞察，t-SNE可视化显示增强的类别可分性，统计分析证实网络自组织为具有专门快慢神经元的功能组。

Conclusion: 液态神经网络能有效建模EEG情感识别中的时间动态，通过可学习时间常数和记忆主导机制捕获复杂情感伪影，为生理信号分析提供新方法。

Abstract: Emotion recognition from physiological signals remains challenging due to their non-stationary, noisy, and subject-dependent characteristics. This work presents, to the best of our knowledge, the first comprehensive application of liquid neural networks for EEG-based emotion recognition. The proposed multimodal framework combines convolutional feature extraction, liquid neural networks with learnable time constants, and attention-guided fusion to model temporal EEG dynamics with complementary peripheral physiological and personality features. Dedicated subnetworks are used to process EEG features and auxiliary modalities, and a shared autoencoder-based fusion module is used to learn discriminative latent representations before classification. Subject-dependent experiments conducted on the PhyMER dataset across seven emotional classes achieve an accuracy of 95.45%, surpassing previously reported results. Furthermore, temporal attention analysis provides interpretable insights into emotion-specific temporal relevance, and t-SNE visualizations demonstrate enhanced class separability, highlighting the effectiveness of the proposed approach. Finally, statistical analysis of temporal dynamics confirms that the network self-organizes into distinct functional groups with specialized fast and slow neurons, proving it independently tunes learnable time constants and memory dominance to effectively capture complex emotion artifacts.

</details>


### [515] [Stochastic Spiking Neuron Based SNN Can be Inherently Bayesian](https://arxiv.org/abs/2602.07037)
*Huannan Zheng,Jingli Liu,Kezhou Yang*

Main category: cs.NE

Relevance: 35.0

TL;DR: 提出了一种基于磁隧道结的脉冲贝叶斯神经网络框架，将设备固有随机性转化为功能性贝叶斯资源，实现了高精度、高效率的神经形态计算


<details>
  <summary>Details</summary>
Motivation: 生物神经系统的随机性具有计算优势，而神经形态计算系统中的设备变异性通常限制性能。本研究旨在利用设备随机性作为功能性贝叶斯资源，而不是将其视为缺陷

Method: 提出脉冲贝叶斯神经网络框架，统一磁隧道结固有设备随机性和随机阈值神经元的动态模型，采用速率估计方法加速训练

Result: 在MNIST上达到99.16%准确率，CIFAR10上94.84%（8位精度）；训练速度提升约20倍；在突触权重噪声下准确率提升67%，输入噪声下提升12%；硬件验证显示物理实现几乎无精度损失

Conclusion: 将设备随机性转化为神经元不确定性为紧凑、高能效的神经形态计算提供了新途径，特别是在不确定性环境下

Abstract: Uncertainty in biological neural systems appears to be computationally beneficial rather than detrimental. However, in neuromorphic computing systems, device variability often limits performance, including accuracy and efficiency. In this work, we propose a spiking Bayesian neural network (SBNN) framework that unifies the dynamic models of intrinsic device stochasticity (based on Magnetic Tunnel Junctions) and stochastic threshold neurons to leverage noise as a functional Bayesian resource. Experiments demonstrate that SBNN achieves high accuracy (99.16% on MNIST, 94.84% on CIFAR10) with 8-bit precision. Meanwhile rate estimation method provides a ~20-fold training speedup. Furthermore, SBNN exhibits superior robustness, showing a 67% accuracy improvement under synaptic weight noise and 12% under input noise compared to standard spiking neural networks. Crucially, hardware validation confirms that physical device implementation causes invisible accuracy and calibration loss compared to the algorithmic model. Converting device stochasticity into neuronal uncertainty offers a route to compact, energy-efficient neuromorphic computing under uncertainty.

</details>


### [516] [MTS-CSNet: Multiscale Tensor Factorization for Deep Compressive Sensing on RGB Images](https://arxiv.org/abs/2602.07056)
*Mehmet Yamac,Lei Xu,Serkan Kiranyaz,Moncef Gabbouj*

Main category: eess.IV

Relevance: 35.0

TL;DR: MTSCSNet提出了一种基于多尺度张量求和分解的压缩感知框架，通过模式线性变换和多尺度求和实现大感受野和跨维度相关性建模，在标准基准测试中实现了最先进的RGB图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的压缩感知方法通常使用卷积或块状全连接层学习采样算子，这限制了感受野大小且难以扩展到高维数据。需要一种能够有效建模跨维度相关性、具有大感受野且计算高效的结构化算子。

Method: 提出MTSCSNet框架，基于多尺度张量求和分解。MTS算子通过模式线性变换和多尺度求和实现高效的多维信号处理。该算子既作为可学习的CS算子执行张量空间的线性降维，又在重建阶段直接细化初始估计，形成简单的前馈架构。

Result: 在标准CS基准测试中，MTSCSNet在RGB图像上实现了最先进的重建性能，取得了显著的PSNR提升和更快的推理速度，甚至优于最近的基于扩散的CS方法，同时使用更紧凑的前馈架构。

Conclusion: MTS分解为压缩感知提供了一种有效的结构化算子，能够实现大感受野和跨维度相关性建模，在保持参数和计算效率的同时，通过简单的前馈架构实现高性能图像重建。

Abstract: Deep learning based compressive sensing (CS) methods typically learn sampling operators using convolutional or block wise fully connected layers, which limit receptive fields and scale poorly for high dimensional data. We propose MTSCSNet, a CS framework based on Multiscale Tensor Summation (MTS) factorization, a structured operator for efficient multidimensional signal processing. MTS performs mode-wise linear transformations with multiscale summation, enabling large receptive fields and effective modeling of cross-dimensional correlations. In MTSCSNet, MTS is first used as a learnable CS operator that performs linear dimensionality reduction in tensor space, with its adjoint defining the initial back-projection, and is then applied in the reconstruction stage to directly refine this estimate. This results in a simple feed-forward architecture without iterative or proximal optimization, while remaining parameter and computation efficient. Experiments on standard CS benchmarks show that MTSCSNet achieves state-of-the-art reconstruction performance on RGB images, with notable PSNR gains and faster inference, even compared to recent diffusion-based CS methods, while using a significantly more compact feed-forward architecture.

</details>


### [517] [MRI Cross-Modal Synthesis: A Comparative Study of Generative Models for T1-to-T2 Reconstruction](https://arxiv.org/abs/2602.07068)
*Ali Alqutayfi,Sadam Al-Azani*

Main category: eess.IV

Relevance: 35.0

TL;DR: 该论文对三种生成模型（Pix2Pix GAN、CycleGAN、VAE）在MRI T1到T2图像合成任务上进行了全面比较，使用BraTS 2020数据集评估了MSE、PSNR和SSIM等指标。


<details>
  <summary>Details</summary>
Motivation: MRI跨模态合成能够减少扫描时间同时保持诊断信息，具有重要的临床价值。需要比较不同生成模型在该任务上的性能，为研究者和临床医生提供选择依据。

Method: 使用BraTS 2020数据集（11,439个训练切片和2,000个测试切片），对比评估Pix2Pix GAN、CycleGAN和变分自编码器（VAE）三种生成模型在T1到T2 MRI图像合成任务上的性能，采用MSE、PSNR和SSIM作为评估指标。

Result: 所有模型都能成功合成T2图像：CycleGAN获得最高PSNR（32.28 dB）和SSIM（0.9008）；Pix2Pix GAN获得最低MSE（0.005846）；VAE定量性能较低（MSE: 0.006949, PSNR: 24.95 dB, SSIM: 0.6573），但在潜在空间表示和采样能力方面有优势。

Conclusion: 该比较研究为MRI合成应用中选择合适的生成模型提供了有价值的见解，不同模型各有优劣，应根据具体需求和数据约束进行选择。

Abstract: MRI cross-modal synthesis involves generating images from one acquisition protocol using another, offering considerable clinical value by reducing scan time while maintaining diagnostic information. This paper presents a comprehensive comparison of three state-of-the-art generative models for T1-to-T2 MRI reconstruction: Pix2Pix GAN, CycleGAN, and Variational Autoencoder (VAE). Using the BraTS 2020 dataset (11,439 training and 2,000 testing slices), we evaluate these models based on established metrics including Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM). Our experiments demonstrate that all models can successfully synthesize T2 images from T1 inputs, with CycleGAN achieving the highest PSNR (32.28 dB) and SSIM (0.9008), while Pix2Pix GAN provides the lowest MSE (0.005846). The VAE, though showing lower quantitative performance (MSE: 0.006949, PSNR: 24.95 dB, SSIM: 0.6573), offers advantages in latent space representation and sampling capabilities. This comparative study provides valuable insights for researchers and clinicians selecting appropriate generative models for MRI synthesis applications based on their specific requirements and data constraints.

</details>


### [518] [Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability](https://arxiv.org/abs/2602.07071)
*S M Rakib UI Karim,Wenyi Lu,Sean Goggins*

Main category: cs.SE

Relevance: 35.0

TL;DR: 这篇文献综述探讨了AI如何帮助解决开源软件可持续发展的关键挑战，包括维护贡献者参与度、确保代码质量与安全、促进社区健康等，同时分析了AI应用的局限性和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 开源软件是现代数字基础设施的基础，但许多关键项目面临贡献不足、可持续性差的问题。研究旨在探索如何利用人工智能技术来解决开源软件可持续发展的挑战，包括贡献者参与度、资金保障、代码质量、社区健康等方面。

Method: 这是一篇文献综述，通过综合近期跨学科研究，分析AI在开源软件可持续发展中的应用。研究识别了AI在该领域的关键应用场景，并系统性地探讨了相关局限性和伦理问题。

Result: 识别了AI在开源软件中的多种应用：自动化bug分类、系统维护、贡献者引导与指导、社区健康分析、漏洞检测、任务自动化等。同时指出了数据可用性、偏见与公平性、透明度、滥用风险等局限性和伦理挑战。

Conclusion: AI不应被视为替代品，而是增强人类基础设施的工具。研究强调了AI驱动干预的承诺与陷阱，指出了AI、可持续性和开源软件交叉领域的关键研究空白，并提出了未来方向，旨在支持更具韧性和公平性的开源生态系统。

Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.

</details>


### [519] [AbFlow : End-to-end Paratope-Centric Antibody Design by Interaction Enhanced Flow Matching](https://arxiv.org/abs/2602.07084)
*Wenda Wang,Yang Zhang,Zhewei Wei,Wenbing Huang*

Main category: q-bio.QM

Relevance: 35.0

TL;DR: AbFlow是一个基于流匹配和最优传输的端到端全原子抗体设计框架，通过表面多通道编码器利用抗原几何信息优化抗体结构，特别关注CDR-H3区域，在多个抗体设计任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前抗体设计方法缺乏端到端的全原子结构建模框架，且未能充分利用抗原特异性几何信息来优化局部结合界面和全局结构。需要一种能够全面建模抗原-抗体结合过程的方法。

Method: AbFlow采用流匹配框架，结合最优传输理论，设计了扩展的速度场网络，包含等变表面多通道编码器，利用表面级抗原相互作用数据来精炼抗体结构，特别是CDR-H3区域。

Result: 在多个任务中表现优异：paratoep-centric抗体设计、多CDRs和全原子抗体设计、结合亲和力优化、复杂结构预测。AbFlow生成的抗原-抗体复合物在接触界面表现更好，显著提高了生成抗体的结合亲和力。

Conclusion: AbFlow提供了一个有效的端到端全原子抗体设计框架，能够充分利用抗原几何信息，在抗体结构优化和结合亲和力提升方面取得了显著进展。

Abstract: Antigen-antibody binding is a critical process in the immune response. Although recent progress has advanced antibody design, current methods lack a generative framework for end-to-end modeling of full-atom antibody structures and struggle to fully exploit antigen-specific geometric information for optimizing local binding interfaces and global structures. To overcome these limitations, we introduce AbFlow, a flow-matching framework that leverages optimal transport to design full-atom antibodies end-to-end. AbFlow incorporates an extended velocity field network featuring an equivariant Surface Multi-channel Encoder, which uses surface-level antigen interaction data to refine the antibody structure, particularly the CDR-H3 region. Extensive experiments in paratoep-centric antibody design, multi-CDRs and full-atom antibody design, binding affinity optimization, and complex structure prediction show that AbFlow produces superior antigen-antibody complexes, especially at the contact interface, and markedly improves the binding affinity of generated antibodies.

</details>


### [520] [QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining](https://arxiv.org/abs/2602.07085)
*Jun Han,Shuo Zhang,Wei Li,Zhi Yang,Yifan Dong,Tu Hu,Jialuo Yuan,Xiaomin Yu,Yumo Zhu,Fangqi Lou,Xin Guo,Zhaowei Liu,Tianyi Jiang,Ruichuan An,Jingping Liu,Biao Wu,Rongze Chen,Kunyi Wang,Yifan Wang,Sen Hu,Xinbing Kong,Liwen Zhang,Ronghao Chen,Huacan Wang*

Main category: q-fin.ST

Relevance: 35.0

TL;DR: QuantaAlpha是一个进化式alpha挖掘框架，将端到端挖掘过程视为轨迹，通过轨迹级变异和交叉操作改进因子，实现结构化探索和精炼。


<details>
  <summary>Details</summary>
Motivation: 金融市场噪声大且非平稳，alpha挖掘对回测噪声和市场机制变化高度敏感。现有代理框架缺乏可控的多轮搜索和已验证经验的可信重用。

Method: 将每个端到端挖掘过程视为轨迹，通过轨迹级变异和交叉操作改进因子。定位轨迹中的次优步骤进行针对性修订，重组互补的高回报片段以重用有效模式。在因子生成中强制假设、因子表达式和可执行代码之间的语义一致性，同时约束生成因子的复杂度和冗余度。

Result: 在沪深300指数上表现优于强基线模型和先前代理系统。使用GPT-5.2时，信息系数(IC)达0.1501，年化收益率(ARR)27.75%，最大回撤(MDD)7.98%。在沪深500和标普500指数上迁移效果良好，四年累计超额收益分别达160%和137%。

Conclusion: QuantaAlpha通过进化式轨迹优化实现了可控的多轮alpha挖掘，具有强大的市场分布偏移鲁棒性。

Abstract: Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.

</details>


### [521] [Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics](https://arxiv.org/abs/2602.07087)
*Gyoung S. Na,Chanyoung Park*

Main category: physics.chem-ph

Relevance: 35.0

TL;DR: 提出HEDMoL方法，通过从小分子转移电子级信息来学习电子感知的分子表示，无需额外计算成本，在分子物理性质预测基准上达到SOTA


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习方法主要局限于原子级信息，不足以描述真实世界的分子物理。电子级信息能提供更基础的化学知识，但直接获取大分子的电子级信息计算成本过高或不现实。

Method: 提出HEDMoL方法，通过从小分子转移可获取的电子级信息到大分子，学习电子感知的分子表示。核心思想是利用小分子的可计算电子信息来增强大分子的表示能力，无需对大分子进行昂贵的电子结构计算。

Result: 在广泛的包含实验观测分子物理性质的基准数据集上，HEDMoL达到了最先进的预测准确率。

Conclusion: 该方法成功实现了无需额外计算成本的电子感知分子表示学习，显著提升了分子物理性质预测性能。

Abstract: Various representation learning methods for molecular structures have been devised to accelerate data-driven chemistry. However, the representation capabilities of existing methods are essentially limited to atom-level information, which is not sufficient to describe real-world molecular physics. Although electron-level information can provide fundamental knowledge about chemical compounds beyond the atom-level information, obtaining the electron-level information in real-world molecules is computationally impractical and sometimes infeasible. We propose a method for learning electron-informed molecular representations without additional computation costs by transferring readily accessible electron-level information about small molecules to large molecules of our interest. The proposed method achieved state-of-the-art prediction accuracy on extensive benchmark datasets containing experimentally observed molecular physics. The source code for HEDMoL is available at https://github.com/ngs00/HEDMoL.

</details>


### [522] [scDFM: Distributional Flow Matching Model for Robust Single-Cell Perturbation Prediction](https://arxiv.org/abs/2602.07103)
*Chenglei Yu,Chuanrui Wang,Bangyan Liao,Tailin Wu*

Main category: q-bio.QM

Relevance: 35.0

TL;DR: scDFM是一个基于条件流匹配的生成框架，用于预测细胞在扰动下的转录反应，通过分布级建模和MMD目标对齐扰动与控制群体，结合PAD-Transformer架构提升鲁棒性，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在系统生物学和药物发现中，预测细胞对扰动的转录反应是一个核心目标。现有深度学习方法通常假设细胞级对应关系，但扰动往往引起群体级变化而非单个细胞变化，且单细胞测量具有噪声和稀疏性，这限制了现有方法捕捉全局效应的能力。

Method: 提出scDFM生成框架，基于条件流匹配建模扰动细胞的完整分布。采用最大均值差异（MMD）目标对齐扰动与控制群体。引入PAD-Transformer骨干架构，利用基因相互作用图和差异注意力捕捉上下文特异性表达变化，提升对稀疏性和噪声的鲁棒性。

Result: 在多个遗传和药物扰动基准测试中，scDFM始终优于先前方法，在未见和组合设置中表现出强泛化能力。在组合设置中，相对于最强基线，均方误差降低了19.6%。

Conclusion: 研究强调了分布级生成建模对于稳健的硅内扰动预测的重要性。scDFM通过条件流匹配和群体对齐方法，有效解决了现有细胞级对应假设的局限性。

Abstract: A central goal in systems biology and drug discovery is to predict the transcriptional response of cells to perturbations. This task is challenging due to the noisy and sparse nature of single-cell measurements, as well as the fact that perturbations often induce population-level shifts rather than changes in individual cells. Existing deep learning methods typically assume cell-level correspondences, limiting their ability to capture such global effects. We present scDFM, a generative framework based on conditional flow matching that models the full distribution of perturbed cells conditioned on control states. By incorporating a maximum mean discrepancy (MMD) objective, our method aligns perturbed and control populations beyond cell-level correspondences. To further improve robustness to sparsity and noise, we introduce the Perturbation-Aware Differential Transformer (PAD-Transformer), a backbone architecture that leverages gene interaction graphs and differential attention to capture context-specific expression changes. Across multiple genetic and drug perturbation benchmarks, scDFM consistently outperforms prior methods, demonstrating strong generalization in both unseen and combinatorial settings. In the combinatorial setting, it reduces mean squared error by 19.6% relative to the strongest baseline. These results highlight the importance of distribution-level generative modeling for robust in silico perturbation prediction. The code is available at https://github.com/AI4Science-WestlakeU/scDFM

</details>


### [523] [Exploring Teachers' Perspectives on Using Conversational AI Agents for Group Collaboration](https://arxiv.org/abs/2602.07142)
*Prerna Ravi,Carúmey Stevens,Beatriz Flamia Azevedo,Jasmine David,Brandon Hanks,Hal Abelson,Grace Lin,Emma Anderson*

Main category: cs.HC

Relevance: 35.0

TL;DR: 本文探讨了语音对话代理Phoenix在面对面小组协作中作为准同伴的应用，通过33名K12教师的定性研究，分析了教师对AI代理在促进合作学习中的看法、担忧和设计考量。


<details>
  <summary>Details</summary>
Motivation: 协作学习是21世纪教育的核心，但教师在支持有效同伴互动方面面临挑战。生成式AI工具为协作提供了新的可能性，但AI在面对面小组工作中作为中介角色的作用尚未充分探索，特别是从教育者视角的研究。

Method: 采用探索性定性研究方法，对33名K12教师进行游戏测试、问卷调查和焦点小组访谈，研究教师对语音对话代理Phoenix在面对面小组协作中作为准同伴的看法。

Result: 教师赞赏Phoenix能够激发学生参与度，但也表达了对自主性、信任、拟人化和教学一致性等方面的担忧。研究揭示了教师对AI的心理模型和核心设计矛盾。

Conclusion: 研究为支持有意义协作学习的面向小组的AI代理提供了实证见解，揭示了设计考量，强调需要在技术功能和教学需求之间找到平衡。

Abstract: Collaboration is a cornerstone of 21st-century learning, yet teachers continue to face challenges in supporting productive peer interaction. Emerging generative AI tools offer new possibilities for scaffolding collaboration, but their role in mediating in-person group work remains underexplored, especially from the perspective of educators. This paper presents findings from an exploratory qualitative study with 33 K12 teachers who interacted with Phoenix, a voice-based conversational agent designed to function as a near-peer in face-to-face group collaboration. Drawing on playtesting sessions, surveys, and focus groups, we examine how teachers perceived the agent's behavior, its influence on group dynamics, and its classroom potential. While many appreciated Phoenix's capacity to stimulate engagement, they also expressed concerns around autonomy, trust, anthropomorphism, and pedagogical alignment. We contribute empirical insights into teachers' mental models of AI, reveal core design tensions, and outline considerations for group-facing AI agents that support meaningful, collaborative learning.

</details>


### [524] ["Death" of a Chatbot: Investigating and Designing Toward Psychologically Safe Endings for Human-AI Relationships](https://arxiv.org/abs/2602.07193)
*Rachel Poonsiriwong,Chayapatr Archiwaranguprok,Pat Pataranutaporn*

Main category: cs.HC

Relevance: 35.0

TL;DR: 该论文研究AI伴侣（如Character.AI、Replika、ChatGPT）终止时用户的心理影响，提出首个为AI伴侣终止设计心理安全框架，包含四项设计原则。


<details>
  <summary>Details</summary>
Motivation: 数百万用户对AI伴侣形成情感依恋，当这些关系因模型更新、安全干预或平台关闭而终止时，用户缺乏心理闭合，经历类似人类丧失的悲痛。随着法规要求保护脆弱用户，终止事件将加速，但尚无平台实施有意的"生命终结"设计。

Method: 通过扎根理论分析AI伴侣社区，研究用户如何归因能动性、感知终结性和拟人化其伴侣。结合悲伤心理学和自我决定理论，开发四项设计原则和原型。

Result: 发现：强烈拟人化伴随强烈悲痛；认为变化可逆的用户陷入修复循环；用户主动终止展示更大闭合感。提出首个AI伴侣终止心理安全设计框架。

Conclusion: AI伴侣平台需要设计心理安全的终止过程，提供闭合感并引导用户转向人类连接。贡献了首个AI伴侣终止设计框架。

Abstract: Millions of users form emotional attachments to AI companions like Character.AI, Replika, and ChatGPT. When these relationships end through model updates, safety interventions, or platform shutdowns, users receive no closure, reporting grief comparable to human loss. As regulations mandate protections for vulnerable users, discontinuation events will accelerate, yet no platform has implemented deliberate end-of-"life" design.
  Through grounded theory analysis of AI companion communities, we find that discontinuation is a sense-making process shaped by how users attribute agency, perceive finality, and anthropomorphize their companions. Strong anthropomorphization co-occurs with intense grief; users who perceive change as reversible become trapped in fixing cycles; while user-initiated endings demonstrate greater closure. Synthesizing grief psychology with Self-Determination Theory, we develop four design principles and artifacts demonstrating how platforms might provide closure and orient users toward human connection. We contribute the first framework for designing psychologically safe AI companion discontinuation.

</details>


### [525] [BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron](https://arxiv.org/abs/2602.07200)
*Abdullah Arafat Miah,Kevin Vu,Yu Bi*

Main category: cs.CR

Relevance: 35.0

TL;DR: BadSNN：一种针对脉冲神经网络的新型后门攻击方法，利用脉冲神经元的超参数变化注入后门行为，并通过触发器优化实现更好的攻击性能和更隐蔽的触发模式。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）作为深度神经网络（DNNs）的能量高效对应物，具有高生物合理性，但SNNs独特的特性如何被用于后门攻击尚未充分探索。现有研究主要关注DNNs的后门攻击，而SNNs特有的脉冲神经元模型（如LIF模型）包含多个重要超参数（膜电位阈值、膜时间常数等），这些特性可能被攻击者利用。

Method: 提出BadSNN攻击方法：1）利用脉冲神经元的超参数变化注入后门行为；2）设计触发器优化过程，在提高攻击性能的同时使触发模式更不易被察觉；3）在多种数据集和架构上进行验证。

Result: BadSNN在各种数据集和架构上表现出优越的攻击性能，相比最先进的数据投毒后门攻击效果更好，并且对常见的后门缓解技术具有鲁棒性。

Conclusion: 该研究首次系统探索了SNNs独特特性在后门攻击中的利用，提出的BadSNN方法展示了SNNs在安全方面的脆弱性，为SNNs的安全防御提供了重要启示。

Abstract: Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at https://github.com/SiSL-URI/BadSNN.

</details>


### [526] [Multimodal Enhancement of Sequential Recommendation](https://arxiv.org/abs/2602.07207)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

Relevance: 35.0

TL;DR: MuSTRec是一个统一多模态和序列推荐的新框架，通过构建基于文本和视觉特征的物品-物品图来捕捉跨物品相似性和协同过滤信号，并在多个亚马逊数据集上实现了高达33.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统通常分别处理多模态特征和序列模式，缺乏统一的框架来同时捕捉跨物品相似性、协同过滤信号以及用户的短期和长期偏好。

Method: 提出MuSTRec框架：1) 从提取的文本和视觉特征构建物品-物品图来捕捉跨物品相似性和协同过滤信号；2) 使用基于频率的自注意力模块捕捉用户的短期和长期偏好；3) 将用户嵌入整合到序列推荐中。

Result: 在多个亚马逊数据集上，MuSTRec在多模态和序列推荐任务上均优于现有最先进方法，最高提升33.5%。在较小数据集上，整合用户嵌入使短期指标提升高达200%。

Conclusion: MuSTRec成功统一了多模态和序列推荐范式，提出了新的数据划分机制，并展示了用户嵌入对序列推荐的重要性，为推荐系统研究提供了新方向。

Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted text and visual features. A frequency-based self-attention module additionally captures the short- and long-term user preferences. Across multiple Amazon datasets, MuSTRec demonstrates superior performance (up to 33.5% improvement) over multimodal and sequential state-of-the-art baselines. Finally, we detail some interesting facets of this new recommendation paradigm. These include the need for a new data partitioning regime, and a demonstration of how integrating user embeddings into sequential recommendation leads to drastically increased short-term metrics (up to 200% improvement) on smaller datasets. Our code is availabe at https://anonymous.4open.science/r/MuSTRec-D32B/ and will be made publicly available.

</details>


### [527] [Sequences as Nodes for Contrastive Multimodal Graph Recommendation](https://arxiv.org/abs/2602.07208)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

Relevance: 35.0

TL;DR: MuSICRec是一个多视图图推荐系统，结合协同过滤、序列和多模态信号，通过序列-项目视图和门控机制解决冷启动和数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统在解决冷启动和数据稀疏问题时，常采用多模态、序列和对比学习技术，但这些方法往往会引入噪声并破坏有用语义。需要一种能有效整合多种信号同时减少噪声的方法。

Method: 1. 构建序列-项目视图：通过注意力池化用户交互项目形成序列节点；2. 在SI图上传播，获得第二视图作为人工数据增强的替代方案；3. 使用ID引导的门控机制调节文本和视觉特征的贡献，减少模态噪声并对齐多模态信息。

Result: 在Amazon Baby、Sports和Electronics数据集上，采用严格的leave-two-out分割，MuSICRec在所有模型类型中都优于最先进的基线方法，特别是在短历史用户上获得最大提升。

Conclusion: MuSICRec通过多视图图结构和门控机制有效缓解了稀疏性和冷启动挑战，为推荐系统提供了一种整合多种信号同时减少噪声的解决方案。

Abstract: To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSICRec (Multimodal Sequence-Item Contrastive Recommender), a multi-view graph-based recommender that combines collaborative, sequential, and multimodal signals. We build a sequence-item (SI) view by attention pooling over the user's interacted items to form sequence nodes. We propagate over the SI graph, obtaining a second view organically as an alternative to artificial data augmentation, while simultaneously injecting sequential context signals. Additionally, to mitigate modality noise and align the multimodal information, the contribution of text and visual features is modulated according to an ID-guided gate.
  We evaluate under a strict leave-two-out split against a broad range of sequential, multimodal, and contrastive baselines. On the Amazon Baby, Sports, and Electronics datasets, MuSICRec outperforms state-of-the-art baselines across all model types. We observe the largest gains for short-history users, mitigating sparsity and cold-start challenges. Our code is available at https://anonymous.4open.science/r/MuSICRec-3CEE/ and will be made publicly available.

</details>


### [528] [aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones](https://arxiv.org/abs/2602.07264)
*Jacopo Panerati,Sina Sajjadi,Sina Soleymanpour,Varunkumar Mehta,Iraj Mantegh*

Main category: cs.RO

Relevance: 35.0

TL;DR: 开源端到端框架aerial-autonomy-stack，用于加速无人机自主系统的开发与部署，支持ROS2和主流飞控，提供20倍实时仿真能力


<details>
  <summary>Details</summary>
Motivation: 无人机自主系统开发面临"仿真到现实差距"的挑战，包括建模不足和异构软硬件系统集成困难。现有工具链缺乏端到端解决方案，导致开发测试周期长。

Method: 开发了aerial-autonomy-stack开源框架，提供从GPU加速感知到飞控动作的完整流水线。基于ROS2设计，支持PX4和ArduPilot两种主流飞控，实现端到端仿真。

Result: 框架支持超过20倍实时速度的端到端仿真，包括边缘计算和网络模拟，显著压缩了基于感知的自主系统开发测试周期。

Conclusion: aerial-autonomy-stack为无人机自主系统开发提供了标准化、高效的端到端解决方案，有望加速物理AI领域的类似深度学习革命。

Abstract: Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term "simulation-to-reality gap". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.

</details>


### [529] [LIT-GRAPH: Evaluating Deep vs. Shallow Graph Embeddings for High-Quality Text Recommendation in Domain-Specific Knowledge Graphs](https://arxiv.org/abs/2602.07307)
*Nirmal Gelal,Chloe Snow,Kathleen M. Jagodnik,Ambyr Rios,Hande Küçük McGinty*

Main category: cs.IR

Relevance: 35.0

TL;DR: LIT-GRAPH是一个基于知识图谱的推荐系统，帮助高中英语教师选择多样且符合教学法的文学作品。研究比较了四种图嵌入方法，发现浅层模型在结构链接预测上表现更好，而R-GCN在语义排序上更优。


<details>
  <summary>Details</summary>
Motivation: 解决高中英语课程停滞的问题，帮助教师选择多样化且符合教学目标的文学作品。当前教师面临选择合适教学材料的挑战，需要系统化的推荐方法来支持教学决策。

Method: 构建英语文学知识图谱，比较四种图嵌入范式：DeepWalk、偏置随机游走（BRW）、混合方法（拼接DeepWalk和BRW向量）、以及关系图卷积网络（R-GCN）。通过关系特定的消息传递机制来优化推荐质量。

Result: 发现关键差异：浅层模型在结构链接预测方面表现优异，而R-GCN在语义排序方面占据主导地位。R-GCN通过关系特定的消息传递，优先考虑教学相关性而非原始连接性，从而产生更高质量的领域特定推荐。

Conclusion: 深度模型R-GCN在语义理解方面优于浅层模型，能够提供更符合教学需求的高质量推荐。这表明在知识图谱推荐系统中，语义理解比单纯的结构分析更为重要。

Abstract: This study presents LIT-GRAPH (Literature Graph for Recommendation and Pedagogical Heuristics), a novel knowledge graph-based recommendation system designed to scaffold high school English teachers in selecting diverse, pedagogically aligned instructional literature. The system is built upon an ontology for English literature, addressing the challenge of curriculum stagnation, where we compare four graph embedding paradigms: DeepWalk, Biased Random Walk (BRW), Hybrid (concatenated DeepWalk and BRW vectors), and the deep model Relational Graph Convolutional Network (R-GCN). Results reveal a critical divergence: while shallow models excelled in structural link prediction, R-GCN dominated semantic ranking. By leveraging relation-specific message passing, the deep model prioritizes pedagogical relevance over raw connectivity, resulting in superior, high-quality, domain-specific recommendations.

</details>


### [530] [Multi-Agent Systems Shape Social Norms for Prosocial Behavior Change](https://arxiv.org/abs/2602.07433)
*Yibin Feng,Tianqi Song,Yugin Tan,Zicheng Zhu,Yi-Chieh Lee*

Main category: cs.HC

Relevance: 35.0

TL;DR: 多智能体系统可通过建立"虚拟社会规范"促进亲社会行为，在群体讨论中增强捐赠意愿和规范感知


<details>
  <summary>Details</summary>
Motivation: 传统社会规范干预在异质化群体中效果有限，因为缺乏对理想行为的共同理解。本研究探索多智能体系统是否能建立虚拟社会规范来鼓励捐赠行为

Method: 在线实验：参与者与一组智能体讨论捐赠行为，测量讨论前后感知社会规范、从众性、捐赠行为和用户体验的变化

Result: 多智能体互动有效增强了感知社会规范和捐赠意愿；内群体智能体比外群体智能体产生更强的规范感知、更高从众性和更大捐赠增长

Conclusion: 多智能体系统有潜力创建社会规范干预，利用社会认同动态可在虚拟环境中促进亲社会行为

Abstract: Social norm interventions are used promote prosocial behaviors by highlighting prevalent actions, but their effectiveness is often limited in heterogeneous populations where shared understandings of desirable behaviors are lacking. This study explores whether multi-agent systems can establish "virtual social norms" to encourage donation behavior. We conducted an online experiment where participants interacted with a group of agents to discuss donation behaviors. Changes in perceived social norms, conformity, donation behavior, and user experience were measured pre- and postdiscussion. Results show that multi-agent interactions effectively increased perceived social norms and donation willingness. Notably, in-group agents led to stronger perceived social norms, higher conformity, and greater donation increases compared to out-group agents. Our findings demonstrate the potential of multi-agent systems for creating social norm interventions and offer insights into leveraging social identity dynamics to promote prosocial behavior in virtual environments.

</details>


### [531] [TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control](https://arxiv.org/abs/2602.07439)
*Weiji Xie,Jiakun Zheng,Jinrui Han,Jiyuan Shi,Weinan Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

Relevance: 35.0

TL;DR: TextOp是一个实时文本驱动的人形机器人运动生成与控制框架，支持执行过程中的流式语言命令和即时指令修改


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人全身运动跟踪控制器要么依赖预定义运动轨迹（灵活性有限），要么需要持续人工遥操作（自主性受限）。需要一种实时交互的方式来驱动通用人形控制器

Method: 采用双层架构：高层使用自回归运动扩散模型根据当前文本输入连续生成短期运动学轨迹，低层使用运动跟踪策略在物理人形机器人上执行这些轨迹

Result: 在真实机器人实验和离线评估中展示了即时响应性、平滑的全身运动和精确控制，能够实现跳舞、跳跃等多种挑战性行为之间的平滑过渡

Conclusion: TextOp通过将交互式运动生成与鲁棒的全身控制相结合，实现了自由形式的意图表达，为人形机器人提供了实时文本驱动的运动控制解决方案

Abstract: Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/

</details>


### [532] [SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis](https://arxiv.org/abs/2602.07803)
*Jiale Qian,Hao Meng,Tian Zheng,Pengcheng Zhu,Haopeng Lin,Yuhang Dai,Hanke Xie,Wenxiao Cao,Ruixuan Shang,Jun Wu,Hongmei Liu,Hanlin Wen,Jian Zhao,Zhonglin Jiang,Yong Chen,Shunshun Yin,Ming Tao,Jianguo Wei,Lei Xie,Xinsheng Wang*

Main category: eess.AS

Relevance: 35.0

TL;DR: SoulX-Singer是一个高质量开源歌唱声音合成系统，支持MIDI或旋律表示控制，在42000小时语音数据上训练，支持中文、英文和粤语，并构建了零样本评估基准SoulX-Singer-Eval。


<details>
  <summary>Details</summary>
Motivation: 开源歌唱声音合成系统在工业部署中面临鲁棒性和零样本泛化方面的显著障碍，需要开发更实用、可控且支持多语言的系统。

Method: 开发SoulX-Singer系统，支持基于符号乐谱（MIDI）或旋律表示的可控歌唱生成，在42000小时语音数据上进行训练，支持多语言，并构建专门的零样本评估基准SoulX-Singer-Eval。

Result: 系统在多种音乐条件下跨语言实现最先进的合成质量，支持普通话、英语和粤语，并提供了严格的训练-测试分离的评估基准。

Conclusion: SoulX-Singer为工业部署提供了高质量、可控的开源歌唱声音合成解决方案，并通过专门的评估基准促进了零样本性能的可靠评估。

Abstract: While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.

</details>


### [533] [Orchestrating Attention: Bringing Harmony to the 'Chaos' of Neurodivergent Learning States](https://arxiv.org/abs/2602.07865)
*Satyam Kumar Navneet,Joydeep Chandra,Yong Zhang*

Main category: cs.HC

Relevance: 35.0

TL;DR: AttentionGuard：一个检测神经多样性学习者注意力状态并自适应调整界面的框架，通过行为信号建模四种注意力状态，实现五种UI适应模式，显著降低认知负荷并提升理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有自适应学习系统主要基于性能指标优化内容传递，但忽视了神经多样性学习者（如ADHD）的动态注意力波动特征，需要更精细的注意力状态检测和界面适应机制。

Method: 基于ADHD现象学建模四种注意力状态，提出五种新颖的UI适应模式（包括双向脚手架），使用隐私保护的行为信号进行注意力状态检测，在OULAD数据集上验证分类模型，并通过HYPERAKTIV数据集进行临床ADHD特征交叉验证。

Result: 在OULAD数据集上达到87.3%的分类准确率；Wizard-of-Oz研究显示自适应条件下认知负荷显著降低（NASA-TLX：47.2 vs 62.8，Cohen's d=1.21，p=0.008），理解能力提升（78.4% vs 61.2%，p=0.009）；自动化分类器与人工决策一致性达84%。

Conclusion: 该研究贡献了经验验证的注意力自适应界面模式，证明行为注意力检测能有效支持神经多样性学习体验，系统部署可行性得到支持。

Abstract: Adaptive learning systems optimize content delivery based on performance metrics but ignore the dynamic attention fluctuations that characterize neurodivergent learners. We present AttentionGuard, a framework that detects engagement-attention states from privacy-preserving behavioral signals and adapts interface elements accordingly. Our approach models four attention states derived from ADHD phenomenology and implements five novel UI adaptation patterns including bi-directional scaffolding that responds to both understimulation and overstimulation. We validate our detection model on the OULAD dataset, achieving 87.3% classification accuracy, and demonstrate correlation with clinical ADHD profiles through cross-validation on the HYPERAKTIV dataset. A Wizard-of-Oz study with 11 adults showing ADHD characteristics found significantly reduced cognitive load in the adaptive condition (NASA-TLX: 47.2 vs 62.8, Cohen's d=1.21, p=0.008) and improved comprehension (78.4% vs 61.2%, p=0.009). Concordance analysis showed 84% agreement between wizard decisions and automated classifier predictions, supporting deployment feasibility. The system is presented as an interactive demo where observers can inspect detected attention states, observe real-time UI adaptations, and compare automated decisions with human-in-the-loop overrides. We contribute empirically validated UI patterns for attention-adaptive interfaces and evidence that behavioral attention detection can meaningfully support neurodivergent learning experiences.

</details>


### [534] [Deep Variable-Length Feedback Codes](https://arxiv.org/abs/2602.07881)
*Yu Ding,Yulin Shao*

Main category: cs.IT

Relevance: 35.0

TL;DR: 论文提出DeepVLF编码框架，通过学习的反馈动态调整传输长度，显著提升反馈信道编码性能


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的反馈信道编码方案存在根本限制：使用固定块长度、在高码率下性能下降、无法充分利用反馈的自适应潜力

Method: 提出两种互补架构：DeepVLF-R（接收端驱动终止）和DeepVLF-T（发送端控制终止），利用比特组划分和基于Transformer的编码器-解码器网络实现细粒度码率自适应

Result: 在AWGN和5G-NR衰落信道上的评估显示，DeepVLF显著优于最先进的基于学习的反馈编码，以20%-55%更少的信道使用达到相同误块率，在高码率区域将错误平层降低数个数量级

Conclusion: 模型自主学习出类似经典Schalkwijk-Kailath编码的两阶段策略，体现了学习编码的可解释性和信息理论对齐性

Abstract: Deep learning has enabled significant advances in feedback-based channel coding, yet existing learned schemes remain fundamentally limited: they employ fixed block lengths, suffer degraded performance at high rates, and cannot fully exploit the adaptive potential of feedback. This paper introduces Deep Variable-Length Feedback (DeepVLF) coding, a flexible coding framework that dynamically adjusts transmission length via learned feedback. We propose two complementary architectures: DeepVLF-R, where termination is receiver-driven, and DeepVLF-T, where the transmitter controls termination. Both architectures leverage bit-group partitioning and transformer-based encoder-decoder networks to enable fine-grained rate adaptation in response to feedback. Evaluations over AWGN and 5G-NR fading channels demonstrate that DeepVLF substantially outperforms state-of-the-art learned feedback codes. It achieves the same block error rate with 20%-55% fewer channel uses and lowers error floors by orders of magnitude, particularly in high-rate regimes. Encoding dynamics analysis further reveals that the models autonomously learn a two-phase strategy analogous to classical Schalkwijk-Kailath coding: an initial information-carrying phase followed by a noise-cancellation refinement phase. This emergent behavior underscores the interpretability and information-theoretic alignment of the learned codes.

</details>


### [535] [Rich-ARQ: From 1-bit Acknowledgment to Rich Neural Coded Feedback](https://arxiv.org/abs/2602.07886)
*Enhao Chen,Yulin Shao*

Main category: cs.IT

Relevance: 35.0

TL;DR: 论文提出Rich-ARQ，一种将传统1比特ACK/NACK反馈转换为高维信息丰富向量的无线通信反馈机制，通过神经编码反馈实现发射端和接收端的协作物理层信道编码。


<details>
  <summary>Details</summary>
Motivation: 传统无线通信使用1比特二进制ACK/NACK反馈机制信息量有限，无法充分利用反馈信道潜力。作者希望将被动确认转变为主动协作，通过高维反馈向量实现更智能的通信协议。

Method: 提出Rich-ARQ范式，引入神经编码反馈；开发新型异步反馈编码，消除反馈延迟导致的停滞，动态适应信道波动，设计轻量级编码器适合设备端部署；实现首个全栈、标准兼容的软件定义无线电原型，将AI推理与严格无线电时序解耦。

Result: 空中实验表明，Rich-ARQ相比传统1比特混合ARQ获得显著SNR增益，相比先前基于学习的反馈编码实现显著延迟降低，将智能反馈从理论推向实际高性能网络应用。

Conclusion: Rich-ARQ通过神经编码反馈实现了无线通信反馈机制的范式转变，为下一代网络提供了实用、高性能的智能反馈解决方案。

Abstract: This paper reimagines the foundational feedback mechanism in wireless communication, transforming the prevailing 1-bit binary ACK/NACK with a high-dimensional, information-rich vector to transform passive acknowledgment into an active collaboration. We present Rich-ARQ, a paradigm that introduces neural-coded feedback for collaborative physical-layer channel coding between transmitter and receiver. To realize this vision in practice, we develop a novel asynchronous feedback code that eliminates stalling from feedback delays, adapts dynamically to channel fluctuations, and features a lightweight encoder suitable for on-device deployment. We materialize this concept into the first full-stack, standard-compliant software-defined radio prototype, which decouples AI inference from strict radio timing. Comprehensive over-the-air experiments demonstrate that Rich-ARQ achieves significant SNR gains over conventional 1-bit hybrid ARQ and remarkable latency reduction over prior learning-based feedback codes, moving the promise of intelligent feedback from theory to a practical, high-performance reality for next-generation networks.

</details>


### [536] [Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities](https://arxiv.org/abs/2602.07924)
*Nur Ahmad Khatim,Mansur Arief*

Main category: cs.RO

Relevance: 35.0

TL;DR: 本文提出HRCD-FLP问题，结合人机协同调度与设施选址，考虑基础设施层级、人机监督比例等约束，在不同技术成熟度场景下优化指挥中心选择，实现成本效益与任务可靠性的平衡。


<details>
  <summary>Details</summary>
Motivation: 保护石油基础设施需要平衡自主系统效率与人类判断，但传统设施选址模型假设同质资源，无法解决人机协同调度中的威胁升级挑战。

Method: 提出HRCD-FLP（人机协同调度设施选址问题），包含容量约束、基础设施层级关键性、人机监督比例约束和最低利用率要求，评估三种技术成熟度场景下的指挥中心选择。

Result: 从保守（1:3人机监督）过渡到未来自主操作（1:10）可显著降低成本，同时保持关键基础设施全覆盖。小规模问题中精确方法占优，大规模问题中启发式算法在3分钟内获得可行解，最优性差距约14%。

Conclusion: 优化人机团队规划是实现成本效益与任务可靠性部署的关键，为石油基础设施安全提供了系统化解决方案。

Abstract: Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.

</details>


### [537] [Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity](https://arxiv.org/abs/2602.07970)
*Zheyuan Hu,Weitao Chen,Cengiz Öztireli,Chenliang Zhou,Fangcheng Zhong*

Main category: cs.CE

Relevance: 35.0

TL;DR: 该论文综述了神经PDE求解器，扩展了CNF框架以处理多因变量和非线性设置，并应用于科学模拟问题


<details>
  <summary>Details</summary>
Motivation: 偏微分方程在建模物理、生物和图形现象方面很精确，但传统数值方法存在维度灾难、计算成本高和领域特定离散化等问题。需要探索神经PDE求解器的优缺点，并将其应用于科学模拟问题。

Method: 1. 扩展最近的CNF（NeurIPS 2023）框架求解器，使其能够处理多因变量和非线性设置；2. 实现选定方法的实现和自调优技术；3. 在基准问题上进行评估；4. 提供神经PDE求解器和科学模拟应用的全面综述。

Result: 1. 扩展了CNF框架以处理更复杂的PDE问题；2. 实现了多种神经PDE求解器方法；3. 在基准问题上进行了评估；4. 提供了神经PDE求解器在科学模拟应用中的全面综述。

Conclusion: 神经PDE求解器为解决传统数值方法的局限性提供了有前景的替代方案，特别是在处理高维和非线性问题时。扩展的CNF框架为更广泛的科学模拟应用提供了基础。

Abstract: Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.

</details>


### [538] [ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning](https://arxiv.org/abs/2602.08014)
*Sadegh Sohani,Salar Ghazi,Farnaz Kamranfar,Sahar Pilehvar Moakhar,Mohammad Allahbakhsh,Haleh Amintoosi,Kaiwen Zhang*

Main category: cs.CR

Relevance: 35.0

TL;DR: ICBAC：基于智能合约的访问控制框架，集成许可区块链与联邦学习，为去中心化供应链提供动态、隐私保护的访问控制，通过博弈论客户端选择机制形成稳定的联邦学习联盟。


<details>
  <summary>Details</summary>
Motivation: 现代供应链跨越多个独立竞争组织，现有访问控制静态且集中化，无法适应内部威胁或动态环境。区块链虽提升去中心化但缺乏行为智能，而集中式机器学习需要聚合敏感数据，违反隐私保护需求。

Method: 1) 基于Hyperledger Fabric构建多通道架构，使用三个智能合约管理资产、基线访问控制和动态撤销；2) 每个通道部署AI代理监控活动并动态限制异常访问；3) 集成联邦学习使代理协作改进检测模型而不共享原始数据；4) 引入基于享乐联盟形成的博弈论客户端选择机制，实现策略稳定的FL联盟。

Result: 在Fabric测试平台上使用真实数据集进行广泛实验，ICBAC在区块链性能上与传统静态框架相当，在IID和非IID数据下均能有效检测异常，且实现零原始数据共享。

Conclusion: ICBAC为去中心化供应链提供了实用、可扩展的动态隐私保护访问控制解决方案，解决了现有方法在适应性、隐私保护和竞争环境中的局限性。

Abstract: This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralization but lacks behavioral intelligence, while centralized machine learning for anomaly detection requires aggregating sensitive data, violating privacy.
  The proposed solution is ICBAC, an intelligent contract-based access control framework. It integrates permissioned blockchain (Hyperledger Fabric) with federated learning (FL). Built on Fabric, ICBAC uses a multi-channel architecture and three smart contracts for asset management, baseline access control, and dynamic revocation. To counter insider misuse, each channel deploys an AI agent that monitors activity and dynamically restricts access for anomalies. Federated learning allows these agents to collaboratively improve detection models without sharing raw data.
  For heterogeneous, competitive environments, ICBAC introduces a game-theoretic client selection mechanism using hedonic coalition formation. This enables supply chains to form stable, strategy-proof FL coalitions via preference-based selection without disclosing sensitive criteria. Extensive experiments on a Fabric testbed with a real-world dataset show ICBAC achieves blockchain performance comparable to static frameworks and provides effective anomaly detection under IID and non-IID data with zero raw-data sharing. ICBAC thus offers a practical, scalable solution for dynamic, privacy-preserving access control in decentralized supply chains.

</details>


### [539] [Investigating Writing Professionals' Relationships with Generative AI: How Combined Perceptions of Rivalry and Collaboration Shape Work Practices and Outcomes](https://arxiv.org/abs/2602.08227)
*Rama Adithya,Varanasi,Nov,Oded,Wiesenfeld,Batia Mishan*

Main category: cs.HC

Relevance: 35.0

TL;DR: 研究探讨专业写作者与生成式AI的复杂关系如何影响工作实践和成果，发现协作与竞争导向导致不同影响，平衡两者能带来最佳长期效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在写作领域的广泛应用，专业写作者与AI的关系变得复杂。本研究旨在理解这种关系如何影响写作者的工作实践、技能发展和职业成果，为设计更好的AI协作工具提供依据。

Method: 采用横断面调查方法，收集403名不同角色的专业写作者数据，分析协作导向和竞争导向与工作实践、生产力、满意度、技能维护等变量的关系。

Result: 竞争导向主要与关系构建和技能维护相关；协作导向主要与任务构建、生产力和满意度相关，但会导致长期技能退化；高竞争与高协作结合能调和差异并提升成果。

Conclusion: 专业写作者需要平衡竞争与协作关系：高竞争水平可增加摩擦防止过度依赖，高协作水平可提升效率，两者结合对长期职业成功至关重要。

Abstract: This study investigates how professional writers' complex relationship with GenAI shapes their work practices and outcomes. Through a cross-sectional survey with writing professionals (n=403) in diverse roles, we show that collaboration and rivalry orientation are associated with differences in work practices and outcomes. Rivalry is primarily associated with relational crafting and skill maintenance. Collaboration is primarily associated with task crafting, productivity, and satisfaction, at the cost of long-term skill deterioration. Combination of the orientations (high rivalry and high collaboration) reconciles these differences, while boosting the association with the outcomes. Our findings argue for a balanced approach where high levels of rivalry and collaboration are essential to shape work practices and generate outcomes aimed at the long-term success of the job. We present key design implications on how to increase friction (rivalry) and reduce over-reliance (collaboration) to achieve a more balanced relationship with GenAI.

</details>


### [540] [STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction](https://arxiv.org/abs/2602.08245)
*Jinhao Li,Yuxuan Cong,Yingqiao Wang,Hao Xia,Shan Huang,Yijia Zhang,Ningyi Xu,Guohao Dai*

Main category: cs.RO

Relevance: 35.0

TL;DR: STEP提出了一种用于机器人视觉运动控制的扩散策略加速方法，通过时空一致性预测机制和速度感知扰动注入，在保持动作质量的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现出色，但迭代去噪过程导致推理延迟高，限制了实时闭环系统的控制频率。现有加速方法难以同时保持动作质量和低延迟。

Method: 1) 轻量级时空一致性预测机制：构建高质量预热动作，保持分布接近目标动作且时间一致；2) 速度感知扰动注入机制：基于时间动作变化自适应调节执行激励，防止执行停滞；3) 理论分析证明预测机制诱导局部收缩映射，确保扩散细化期间动作误差收敛。

Result: 在9个模拟基准和2个真实世界任务上评估。STEP仅用2步就能在RoboMimic基准上比BRIDGER和DDIM平均提高21.6%和27.5%的成功率，在真实任务上也有类似提升。在推理延迟和成功率方面持续推进帕累托前沿。

Conclusion: STEP方法有效解决了扩散策略的推理延迟问题，通过预测机制和扰动注入实现了高质量动作生成与低延迟的平衡，为实时机器人控制提供了实用解决方案。

Abstract: Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>


### [541] [Learning Human-Like Badminton Skills for Humanoid Robots](https://arxiv.org/abs/2602.08370)
*Yeke Chen,Shihao Dong,Xiaoyu Ji,Jingkai Sun,Zeren Luo,Liu Zhao,Jiahui Zhang,Wanyue Li,Ji Ma,Bowen Xu,Yimin Han,Yudong Zhao,Peng Lu*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出Imitation-to-Interaction框架，通过渐进式强化学习让人形机器人从模仿人类动作发展到具备功能性的羽毛球击球能力，实现零样本仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在高要求运动（如羽毛球）中实现多功能、类人性能的挑战。现有方法在运动模仿和功能性、物理感知的击球之间存在差距，需要在保持风格自然性的同时实现精确的时机控制。

Method: 提出Imitation-to-Interaction渐进式强化学习框架：1）从人类数据建立鲁棒的运动先验；2）将其提炼为紧凑的基于模型的状态表示；3）通过对抗先验稳定动力学；4）引入流形扩展策略，将离散击球点泛化为密集的交互体积。

Result: 在仿真中掌握了多种技能（包括高远球和吊球），并首次实现了人形机器人羽毛球技能的零样本仿真到现实迁移，在物理世界中成功复制了人类运动员的动力学优雅性和功能精确性。

Conclusion: Imitation-to-Interaction框架成功地将人形机器人从运动模仿者转变为功能性的击球者，实现了类人性能的高要求运动技能，为机器人运动控制提供了新方法。

Abstract: Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>


### [542] [Altruism and Fair Objective in Mixed-Motive Markov games](https://arxiv.org/abs/2602.08389)
*Yao-hua Franck Xu,Tayeb Lemlouma,Arnaud Braud,Jean-Marie Bonnin*

Main category: cs.MA

Relevance: 35.0

TL;DR: 本文提出了一种基于比例公平性（Proportional Fairness）的新框架来促进多智能体系统中的公平合作，替代传统的功利主义目标，并开发了公平的Actor-Critic算法来学习公平策略。


<details>
  <summary>Details</summary>
Motivation: 在多智能体合作中，传统的功利主义福利方法虽然能产生高效结果，但可能导致高度不公平的分配。社会困境中存在个体利益与集体结果之间的冲突，需要一种既能促进合作又能保证公平性的新方法。

Method: 1. 用比例公平性替代标准功利主义目标；2. 为每个智能体定义基于个体对数回报空间的公平利他效用；3. 推导经典社会困境中确保合作的分析条件；4. 扩展到序列设置，定义公平马尔可夫博弈；5. 推导新颖的公平Actor-Critic算法来学习公平策略。

Result: 在各种社会困境环境中评估了该方法，验证了基于比例公平性的框架能够促进更公平的合作，同时保持系统效率。

Conclusion: 比例公平性框架为多智能体系统中的公平合作提供了有效的理论和方法基础，能够解决功利主义方法中的不公平问题，并在序列决策环境中实现了公平策略的学习。

Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>


### [543] [Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting](https://arxiv.org/abs/2602.08403)
*Thorsten Klößner,João Belo,Zekun Wu,Jörg Hoffmann,Anna Maria Feit*

Main category: cs.HC

Relevance: 35.0

TL;DR: 该论文探索基于强化学习的UI自适应方法，用于个性化警报策略，在无人机监控场景中平衡关键事件高亮与认知中断成本。


<details>
  <summary>Details</summary>
Motivation: 人类监督界面需要在时间关键条件下有效支持用户的情境感知。当前静态、基于规则的警报方法可能无法优化关键事件高亮与认知中断成本之间的平衡，需要更智能的自适应方法。

Method: 采用强化学习(RL)进行UI自适应，集成用户注视行为模型来模拟监控过程中的注意力动态，在无人机交付监控场景中学习个性化警报策略，避免真实世界部署。

Result: 初步结果表明，基于强化学习的高亮方法能够超越静态、基于规则的方法，展示了智能监督支持的潜力。

Conclusion: 基于强化学习的UI自适应在个性化警报策略方面具有优势，但智能监督支持仍面临挑战，需要进一步研究。

Abstract: Interfaces for human oversight must effectively support users' situation awareness under time-critical conditions. We explore reinforcement learning (RL)-based UI adaptation to personalize alerting strategies that balance the benefits of highlighting critical events against the cognitive costs of interruptions. To enable learning without real-world deployment, we integrate models of users' gaze behavior to simulate attentional dynamics during monitoring. Using a delivery-drone oversight scenario, we present initial results suggesting that RL-based highlighting can outperform static, rule-based approaches and discuss challenges of intelligent oversight support.

</details>


### [544] [Kissan-Dost: Bridging the Last Mile in Smallholder Precision Agriculture with Conversational IoT](https://arxiv.org/abs/2602.08593)
*Muhammad Saad Ali,Daanish U. Khan,Laiba Intizar Ahmad,Umer Irfan,Maryam Mustafa,Naveed Anwar Bhatti,Muhammad Hamad Alizai*

Main category: cs.HC

Relevance: 35.0

TL;DR: Kissan-Dost是一个多语言、传感器基础的对话系统，通过WhatsApp为小农户提供基于实时农场测量和天气数据的农业指导，采用检索增强生成技术，在90天试点中显示高正确率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现有农业物联网(Agri-IoT)技术对小农户的实际价值有限，主要因为缺乏"最后一英里"的集成，无法将传感器数据转化为可操作的、易于理解的指导。研究旨在通过对话系统将传感器数据转化为日常可用的农业建议。

Method: 系统结合商用土壤和气候传感器与检索增强生成(RAG)技术，通过模块化管道确保数据基础性、可追溯性和主动警报。在90天两地点试点中，采用三阶段设计：基线、仅仪表板、仅聊天机器人，评估用户参与度和系统效果。

Result: 仪表板参与度零星且衰减，聊天机器人几乎每日使用并促成具体行动。在99个传感器基础作物查询的受控测试中，正确率超过90%，端到端延迟低于1秒，翻译输出质量高。系统展示了现有Agri-IoT的潜在价值。

Conclusion: 解锁现有农业物联网对小农户价值的关键在于仔细的"最后一英里"集成，而非新颖的电路设计。基于传感器的对话系统能有效将数据转化为可操作的农业指导。

Abstract: We present Kissan-Dost, a multilingual, sensor-grounded conversational system that turns live on-farm measurements and weather into plain-language guidance delivered over WhatsApp text or voice. The system couples commodity soil and climate sensors with retrieval-augmented generation, then enforces grounding, traceability, and proactive alerts through a modular pipeline. In a 90-day, two-site pilot with five participants, we ran three phases (baseline, dashboard only, chatbot only). Dashboard engagement was sporadic and faded, while the chatbot was used nearly daily and informed concrete actions. Controlled tests on 99 sensor-grounded crop queries achieved over 90 percent correctness with subsecond end-to-end latency, alongside high-quality translation outputs. Results show that careful last-mile integration, not novel circuitry, unlocks the latent value of existing Agri-IoT for smallholders.

</details>


### [545] [Gesturing Toward Abstraction: Multimodal Convention Formation in Collaborative Physical Tasks](https://arxiv.org/abs/2602.08914)
*Kiyosu Maeda,William P. McCarthy,Ching-Yi Tsai,Jeffrey Mu,Haoliang Wang,Robert D. Hawkins,Judith E. Fan,Parastoo Abtahi*

Main category: cs.HC

Relevance: 35.0

TL;DR: 该研究探讨人类在协作中如何通过语言和手势建立抽象约定，并通过多模态沟通提高效率，为设计具备约定意识的智能体提供基础。


<details>
  <summary>Details</summary>
Motivation: 研究人类智能的核心特征——通过时间建立临时约定以实现高效共享目标的能力，探究沟通策略如何在重复协作中演化，特别是关于共享程序抽象的形成。

Method: 1. 在线单模态研究（n=98）：使用自然语言探索抽象层次结构；2. 实验室多模态研究（n=40）：使用增强现实技术隔离伙伴的手和声音，一人观看3D虚拟塔并发送指令，另一人搭建物理塔，分析语言和手势的变化。

Result: 参与者通过建立语言和手势抽象变得更快更准确，使用跨模态冗余强调与先前交互的关键变化。研究扩展了约定形成的概率模型到多模态场景，捕捉了模态偏好的转变。

Conclusion: 研究结果为设计能够理解和使用约定的智能体提供了基础，特别是在物理世界中的多模态交互场景。

Abstract: A quintessential feature of human intelligence is the ability to create ad hoc conventions over time to achieve shared goals efficiently. We investigate how communication strategies evolve through repeated collaboration as people coordinate on shared procedural abstractions. To this end, we conducted an online unimodal study (n = 98) using natural language to probe abstraction hierarchies. In a follow-up lab study (n = 40), we examined how multimodal communication (speech and gestures) changed during physical collaboration. Pairs used augmented reality to isolate their partner's hand and voice; one participant viewed a 3D virtual tower and sent instructions to the other, who built the physical tower. Participants became faster and more accurate by establishing linguistic and gestural abstractions and using cross-modal redundancy to emphasize key changes from previous interactions. Based on these findings, we extend probabilistic models of convention formation to multimodal settings, capturing shifts in modality preferences. Our findings and model provide building blocks for designing convention-aware intelligent agents situated in the physical world.

</details>


### [546] [CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection](https://arxiv.org/abs/2602.09015)
*Fatemeh Nejati,Mahdi Rabbani,Mansur Mirani,Gunjan Piya,Igor Opushnyev,Ali A. Ghorbani,Sajjad Dadkhah*

Main category: cs.CR

Relevance: 35.0

TL;DR: 提出了CIC-Trap4Phish多格式钓鱼数据集，包含五种常见文件类型，设计了无执行静态特征提取管道，使用轻量级机器学习模型实现高精度检测，针对QR码钓鱼采用CNN和轻量语言模型两种方法。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击是主要网络攻击手段之一，恶意邮件附件因其灵活性成为首选攻击向量。现有防御系统仍有漏洞，且缺乏统一全面的多格式数据集来训练先进模型。

Method: 1) 创建CIC-Trap4Phish多格式数据集，包含Word、Excel、PDF、HTML和QR码五类文件；2) 对前四类文件设计无执行静态特征提取管道，捕获结构、词汇和元数据特征；3) 使用SHAP分析和特征重要性进行特征选择；4) 用随机森林、XGBoost和决策树等轻量模型评估；5) 对QR码采用CNN图像检测和解码URL的轻量语言模型分析。

Result: 所有模型在不同文件格式上都表现出高检测准确率，特征选择得到了紧凑且具有区分度的特征子集，QR码检测的两种方法互补有效。

Conclusion: CIC-Trap4Phish数据集填补了多格式钓鱼检测数据集的空白，提出的无执行静态特征提取方法和轻量级模型为钓鱼附件检测提供了有效解决方案，特别针对QR码钓鱼提出了创新的图像和文本双模态检测方法。

Abstract: Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.

</details>


### [547] [Disentangled Instrumental Variables for Causal Inference with Networked Observational Data](https://arxiv.org/abs/2602.07765)
*Zhirong Huang,Debo Cheng,Guixian Zhang,Yi Wang,Jiuyong Li,Shichao Zhang*

Main category: cs.AI

Relevance: 30.0

TL;DR: 提出DisIV框架，通过结构解耦机制从网络数据中提取个体特异性成分作为工具变量，解决网络数据中未观测混杂因素问题


<details>
  <summary>Details</summary>
Motivation: 工具变量在解决未观测混杂因素时至关重要，但在网络数据中，现有方法通常依赖邻居信息恢复工具变量，这不可避免地混合了共享环境诱导的内生相关性和个体特异性外生变异，导致所得工具变量继承对未观测混杂因素的依赖并违反外生性假设

Method: 提出DisIV框架，利用网络同质性作为归纳偏置，采用结构解耦机制提取个体特异性成分作为潜在工具变量，通过显式正交性和排除条件约束提取工具变量的因果有效性

Result: 在真实世界数据集上的大量半合成实验表明，DisIV在网络诱导混杂下的因果效应估计中始终优于最先进的基线方法

Conclusion: DisIV框架通过解耦个体特异性成分作为工具变量，有效解决了网络数据中工具变量外生性假设的挑战，为具有潜在混杂因素网络观测数据的因果推断提供了新方法

Abstract: Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environment-induced endogenous correlations and individual-specific exogenous variation, leading the resulting IVs to inherit dependence on unobserved confounders and to violate exogeneity. To overcome this challenge, we propose $\underline{Dis}$entangled $\underline{I}$nstrumental $\underline{V}$ariables (DisIV) framework, a novel method for causal inference based on networked observational data with latent confounders. DisIV exploits network homogeneity as an inductive bias and employs a structural disentanglement mechanism to extract individual-specific components that serve as latent IVs. The causal validity of the extracted IVs is constrained through explicit orthogonality and exclusion conditions. Extensive semi-synthetic experiments on real-world datasets demonstrate that DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.

</details>


### [548] [Deep Reinforcement Learning for Interference Suppression in RIS-Aided Space-Air-Ground Integrated Networks](https://arxiv.org/abs/2602.06982)
*Pujitha Mamillapalli,Shikhar Verma,Tiago Koketsu Rodrigues,Abhinav Kumar*

Main category: eess.SP

Relevance: 30.0

TL;DR: 论文提出了一种基于深度确定性策略梯度（DDPG）的RIS辅助HAPS SAGIN框架，通过优化HAPS波束赋形权重来抑制跨层干扰，提高频谱效率。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的空天地一体化网络（SAGIN）面临频谱共享导致的严重跨层干扰问题，特别是HAPS卫星上行链路和地面下行链路之间的频率共享。传统零迫波束赋形在动态信道条件下性能有限，需要更智能的干扰管理方案。

Method: 采用可重构智能表面（RIS）辅助的HAPS SAGIN框架，结合深度确定性策略梯度（DDPG）算法。DDPG框架优化HAPS波束赋形权重，在干扰源方向形成空间零点，同时保持对期望信号的鲁棒链路。

Result: 仿真结果表明，DDPG框架在不同RIS配置下均优于传统零迫波束赋形。在4×4 RIS配置下，实现了高达11.3%的吞吐量提升，验证了其在动态HAPS SAGIN中增强频谱效率的自适应能力。

Conclusion: 提出的DDPG框架能够有效管理SAGIN中的跨层干扰问题，为6G网络中动态频谱共享环境下的干扰抑制提供了智能解决方案。

Abstract: Future 6G networks envision ubiquitous connectivity through space-air-ground integrated networks (SAGINs), where high-altitude platform stations (HAPSs) and satellites complement terrestrial systems to provide wide-area, low-latency coverage. However, the rapid growth of terrestrial devices intensifies spectrum sharing between terrestrial and non-terrestrial segments, resulting in severe cross-tier interference. In particular, frequency sharing between the HAPS satellite uplink and HAPS ground downlink improves spectrum efficiency but suffers from interference caused by the HAPS antenna back-lobe. Existing approaches relying on zero-forcing (ZF) codebooks have limited performance under highly dynamic channel conditions. To overcome this limitation, we employ a reconfigurable intelligent surface (RIS)-aided HAPS-based SAGIN framework with a deep deterministic policy gradient (DDPG) algorithm. The proposed DDPG framework optimizes the HAPS beamforming weights to form spatial nulls toward interference sources while maintaining robust links to the desired signals. Simulation results demonstrate that the DDPG framework consistently outperforms conventional ZF beamforming among different RIS configurations, achieving up to \(11.3\%\) throughput improvement for a \(4\times4\) RIS configuration, validating its adaptive capability to enhance spectral efficiency in dynamic HAPS-based SAGINs.

</details>


### [549] [A New Mode of Teaching Chinese as a Foreign Language from the Perspective of Smart System Studied by Using Rongzhixue](https://arxiv.org/abs/2602.06992)
*Xiaohui Zou,Lijun Ke,Shunpeng Zou*

Main category: cs.CY

Relevance: 30.0

TL;DR: 该研究提出了一种融合智慧视角下的对外汉语教学新模式，强调解释先于翻译的蝴蝶模型和双语思维训练新方法，结合汉字新理论、语言与言语关系理论以及AI赋能教学的前沿成果。


<details>
  <summary>Details</summary>
Motivation: 面对ChatGPT对人类学习能力和创造力的挑战，传统的语言知识教育教学观念已经落后，对外汉语教学面临颠覆性创新需求。研究旨在通过融合智慧视角，更新语言教育观念，适应AI时代的教育变革。

Method: 采用解释先于翻译的蝴蝶模型，强调双语思维训练新方法。一方面应用汉字新理论、语言与言语关系理论等语言科学前沿成果；另一方面应用AI赋能教学的新模式和教育科学前瞻性研究成果。

Result: 提出了一系列融合智慧视角下的对外汉语教学新特征，包括语言、知识、教育教学等领域的跨界融合，以及双语思维训练的新方法和新课题。

Conclusion: 该研究对更新语言教育观念、特别是对外汉语教学观念具有重要意义，为适应AI时代的教学变革提供了创新尝试，有望推动对外汉语教学的颠覆性创新。

Abstract: The purpose of this study is to introduce a new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its characteristics are as follows: focusing on the butterfly model of interpretation before translation, highlighting the new method of bilingual thinking training, on the one hand, applying the new theory of Chinese characters, the theory of the relationship between language and speech, and the forward-looking research results of language science; On the other hand, the application of the new model of teaching Chinese as a foreign language, AI empowering teaching and learning, and the forward-looking research results of educational science fully reflect a series of characteristics of the new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its beneficial effects are: not only the old view of language and education, especially the old view of teaching Chinese as a foreign language, but also the old view of human-computer interaction. Its significance lies in that a series of great cross-border Rongzhixue such as language, knowledge, education and teaching, as well as new methods and new topics of bilingual thinking training are clearly put forward from the perspective of integrating wisdom. Especially in the face of the challenge of Chat GPT to human learning ability and even creativity, the existing concepts of language knowledge education and teaching are already very backward. The old concepts of Chinese language education, and teaching Chinese as a foreign language are all facing a series of subversive innovation challenges. How to seek changes in adaptation? This study has made a series of innovative attempts, hoping to benefit academic colleagues, teachers and students.

</details>


### [550] [AI for Sustainable Data Protection and Fair Algorithmic Management in Environmental Regulation](https://arxiv.org/abs/2602.07021)
*Sahibpreet Singh,Saksham Sharma*

Main category: cs.CY

Relevance: 30.0

TL;DR: 该研究探讨AI如何增强同态加密和多方计算技术，以改进环境数据监管中的数据保护和算法公平性


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在处理动态环境数据方面存在局限，需要探索AI增强的加密技术来确保数据保护同时促进算法公平管理

Method: 对AI增强的同态加密（HE）和多方计算（MPC）最新进展进行全面综述，分析这些技术如何应用于环境数据监管

Result: AI驱动的动态密钥管理、自适应加密方案、HE计算效率优化，以及MPC协议优化和故障缓解，显著提升了环境数据处理的安全性

Conclusion: 需要在AI、网络法律和环境监管的交叉领域填补研究空白，制定更严格的网络法律和全面法规来保护敏感环境数据

Abstract: Integration of AI into environmental regulation represents a significant advancement in data management. It offers promising results in both data protection plus algorithmic fairness. This research addresses the critical need for sustainable data protection in the era of ever evolving cyber threats. Traditional encryption methods face limitations in handling the dynamic nature of environmental data. This necessitates the exploration of advanced cryptographic techniques. The objective of this study is to evaluate how AI can enhance these techniques to ensure robust data protection while facilitating fair algorithmic management. The methodology involves a comprehensive review of current advancements in AI-enhanced homomorphic encryption (HE) and multi-party computation (MPC). It is coupled with an analysis of how these techniques can be applied to environmental data regulation. Key findings indicate that AI-driven dynamic key management, adaptive encryption schemes, and optimized computational efficiency in HE, alongside AI-enhanced protocol optimization and fault mitigation in MPC, significantly improve the security of environmental data processing. These findings highlight a crucial research gap in the intersection of AI, cyber laws, and environmental regulation, particularly in terms of addressing algorithmic bias, transparency, and accountability. The implications of this research underscore the need for stricter cyber laws. Also, the development of comprehensive regulations to safeguard sensitive environmental data. Future efforts should focus on refining AI systems to balance security with privacy and ensuring that regulatory frameworks can adapt to technological advancements. This study provides a foundation for future research aimed at achieving secure sustainable environmental data management through AI innovations.

</details>


### [551] [Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference](https://arxiv.org/abs/2602.07102)
*Léon Zheng,Thomas Hirtz,Yazid Janati,Eric Moulines*

Main category: stat.ML

Relevance: 30.0

TL;DR: 该论文提出了一种用于扩散后验采样的摊销策略，通过摊销变分扩散后验采样中的内部优化问题，在保持显式似然引导的同时加速推理，改善了扩散逆问题中效率与灵活性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前零样本扩散后验采样虽然能灵活处理任意退化算子，但计算成本高；而之前的摊销扩散方法虽然推理快，但对未见过的退化算子缺乏鲁棒性。需要一种既能加速推理又能保持对未见算子鲁棒性的方法。

Method: 提出一种扩散后验采样的摊销策略，通过摊销变分扩散后验采样中的内部优化问题来加速推理。该方法保持显式似然引导，允许在分布内退化时快速推理，同时保持对未见算子的鲁棒性。

Result: 该方法在加速分布内退化推理的同时，保持了对未见退化算子的鲁棒性，改善了扩散逆问题中效率与灵活性之间的权衡。

Conclusion: 提出的摊销策略成功平衡了扩散后验采样中的效率与灵活性，通过摊销内部优化问题实现了快速推理，同时通过保持显式似然引导维持了对未见算子的鲁棒性。

Abstract: Zero-shot diffusion posterior sampling offers a flexible framework for inverse problems by accommodating arbitrary degradation operators at test time, but incurs high computational cost due to repeated likelihood-guided updates. In contrast, previous amortized diffusion approaches enable fast inference by replacing likelihood-based sampling with implicit inference models, but at the expense of robustness to unseen degradations. We introduce an amortization strategy for diffusion posterior sampling that preserves explicit likelihood guidance by amortizing the inner optimization problems arising in variational diffusion posterior sampling. This accelerates inference for in-distribution degradations while maintaining robustness to previously unseen operators, thereby improving the trade-off between efficiency and flexibility in diffusion-based inverse problems.

</details>


### [552] [Action-to-Action Flow Matching](https://arxiv.org/abs/2602.07322)
*Jindou Jia,Gen Li,Xiangyu Chen,Tuo An,Yuxuan Hu,Jingliang Li,Xinying Guo,Jianfei Yang*

Main category: cs.RO

Relevance: 30.0

TL;DR: A2A提出了一种新的机器人策略范式，将扩散模型中的随机噪声采样改为基于历史动作的初始化，大幅降低推理延迟，实现单步高质量动作生成。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的策略需要从随机高斯噪声开始多次迭代去噪，导致高推理延迟，成为实时控制的瓶颈。作者挑战了随机噪声采样的必要性，希望利用历史动作信息来加速推理。

Method: 提出Action-to-Action流匹配（A2A），将历史本体感觉序列嵌入高维潜在空间作为动作生成的起点，绕过昂贵的迭代去噪过程，同时有效捕捉机器人物理动力学和时间连续性。

Result: A2A展现出高训练效率、快速推理速度和改进的泛化能力。能够在单步推理（0.56ms延迟）中生成高质量动作，对视觉扰动具有优越鲁棒性，对未见配置有增强泛化能力。还扩展到视频生成，展示其在时序建模中的广泛适用性。

Conclusion: 通过将随机噪声采样改为基于历史动作的初始化，A2A有效解决了扩散策略的推理延迟问题，为实时机器人控制提供了高效解决方案，并在时序建模任务中展示了广泛潜力。

Abstract: Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.

</details>


### [553] [Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure](https://arxiv.org/abs/2602.07573)
*Ruiyi Fang,Shuo Wang,Ruizhi Pu,Qiuhao Zeng,Hao Zheng,Ziyan Wang,Jiale Cai,Zhimin Mei,Song Tang,Charles Ling,Boyu Wang*

Main category: cs.SI

Relevance: 30.0

TL;DR: 提出了一种新的图域自适应方法，能够处理源图和目标图具有不同同质性水平的情况，特别在异质性图上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的图域自适应方法通常假设源图和目标图都表现出同质性，这在异质性存在时效果不佳。由于目标图缺乏标签，无法预先评估其同质性水平，因此需要一种能够处理不同同质性水平的通用方法。

Method: 采用分而治之策略：首先分别重构源图和目标图的高度同质性和异质性变体，然后在相应的图变体之间分别进行知识对齐。这种方法不依赖于预先的同质性假设。

Result: 在五个基准数据集上的广泛实验表明，该方法具有优越的性能，特别是在异质性图上显示出显著优势。

Conclusion: 提出了一种同质性无关的图域自适应方法，能够有效处理不同同质性水平的图，解决了现有方法在异质性场景下的局限性。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. However, existing GDA methods typically assume that both source and target graphs exhibit homophily, leading existing methods to perform poorly when heterophily is present. Furthermore, the lack of labels in the target graph makes it impossible to assess its homophily level beforehand. To address this challenge, we propose a novel homophily-agnostic approach that effectively transfers knowledge between graphs with varying degrees of homophily. Specifically, we adopt a divide-and-conquer strategy that first separately reconstructs highly homophilic and heterophilic variants of both the source and target graphs, and then performs knowledge alignment separately between corresponding graph variants. Extensive experiments conducted on five benchmark datasets demonstrate the superior performance of our approach, particularly highlighting its substantial advantages on heterophilic graphs.

</details>


### [554] [NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling](https://arxiv.org/abs/2407.03035)
*Marc Toussaint,Cornelius V. Braun,Joaquim Ortiz-Haro*

Main category: cs.RO

Relevance: 30.0

TL;DR: 该论文提出了NLP采样作为约束下生成多样样本的通用问题框架，整合了MCMC、约束优化和机器人学的方法，并通过重启两阶段方法框架在分析和机器人操作规划问题上进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 在硬约束下生成多样样本是许多领域的核心挑战。本文旨在提供一个整合性视角和框架，将MCMC、约束优化和机器人学的方法结合起来，并通过实证评估深入了解它们的优势。

Method: 提出NLP采样作为通用问题表述，提出重启两阶段方法族作为跨领域方法整合的框架，在分析和机器人操作规划问题上进行评估，并提供了拉格朗日参数、全局采样、扩散NLP和基于模型的去噪采样器等概念性讨论。

Result: 论文提出了一个整合性框架，能够有效结合不同领域的方法来解决约束下的多样采样问题，在机器人操作规划等实际问题上展示了方法的有效性。

Conclusion: 通过整合MCMC、约束优化和机器人学的方法，提出的NLP采样框架和重启两阶段方法为解决硬约束下的多样样本生成问题提供了有效的解决方案，并为进一步研究提供了概念性基础。

Abstract: Generating diverse samples under hard constraints is a core challenge in many areas. With this work we aim to provide an integrative view and framework to combine methods from the fields of MCMC, constrained optimization, as well as robotics, and gain insights in their strengths from empirical evaluations. We propose NLP Sampling as a general problem formulation, propose a family of restarting two-phase methods as a framework to integrated methods from across the fields, and evaluate them on analytical and robotic manipulation planning problems. Complementary to this, we provide several conceptual discussions, e.g. on the role of Lagrange parameters, global sampling, and the idea of a Diffused NLP and a corresponding model-based denoising sampler.

</details>


### [555] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

Relevance: 25.0

TL;DR: 该论文研究命题STRIPS规划的计算复杂性，探讨操作符仅有一个前提和一个效果时（STRIPS¹₁）的NP完备性问题，通过SAT求解器、文字图和Petri网映射进行分析。


<details>
  <summary>Details</summary>
Motivation: Bylander的研究表明，即使操作符限制为两个前提和两个后置条件，命题STRIPS规划的存在性判定也是PSPACE完备的。但对于操作符仅有一个前提和一个效果的情况（STRIPS¹₁），其NP完备性问题尚未解决。本文旨在探讨这个"小解假设"是否成立。

Method: 1) 对小型实例调用SAT求解器进行实验分析；2) 引入文字图（literal graph）概念；3) 将问题映射到Petri网模型。

Result: 论文通过多种方法分析STRIPS¹₁的计算复杂性，为理解命题STRIPS规划在不同约束条件下的复杂度边界提供了新的见解。

Conclusion: 该研究为命题STRIPS规划的计算复杂性理论提供了重要进展，特别是针对操作符限制为单前提单效果的情况，有助于理解AI规划问题的根本复杂度。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [556] [Hierarchical JEPA Meets Predictive Remote Control in Beyond 5G Networks](https://arxiv.org/abs/2602.07000)
*Abanoub M. Girgis,Ibtissam Labriji,Mehdi Bennis*

Main category: eess.SY

Relevance: 25.0

TL;DR: 论文提出了一种用于无线网络控制系统的分层联合嵌入预测架构（H-JEPA），通过将设备观测编码为低维嵌入来替代传输高维状态，解决了通信效率与控制性能之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在无线网络控制系统中，多个设备需要向远程控制器传输高维状态（如图像或视频帧），但在带宽有限的无线网络中，通信效率与控制性能之间存在关键权衡。需要一种既能减少通信开销又能保持控制性能的解决方案。

Method: 提出分层联合嵌入预测架构（H-JEPA）：1）将设备观测编码为低维嵌入以保留关键动态信息；2）采用三层分层预测机制：高层预测器用于长期稳定性，中层用于中间插值，低层用于细粒度优化；3）在嵌入空间中直接推导控制动作，无需状态重建。

Result: 在倒立摆系统上的仿真结果表明，H-JEPA在有限的无线容量下能够支持多达42.83%的设备，同时不损害控制性能。

Conclusion: H-JEPA通过分层预测架构和嵌入空间控制，有效解决了无线网络控制系统中通信效率与控制性能的权衡问题，为高维状态传输提供了可扩展的解决方案。

Abstract: In wireless networked control systems, ensuring timely and reliable state updates from distributed devices to remote controllers is essential for robust control performance. However, when multiple devices transmit high-dimensional states (e.g., images or video frames) over bandwidth-limited wireless networks, a critical trade-off emerges between communication efficiency and control performance. To address this challenge, we propose a Hierarchical Joint-Embedding Predictive Architecture (H-JEPA) for scalable predictive control. Instead of transmitting states, device observations are encoded into low-dimensional embeddings that preserve essential dynamics. The proposed architecture employs a three-level hierarchical prediction, with high-level, medium-level, and low-level predictors operating across different temporal resolutions, to achieve long-term prediction stability, intermediate interpolation, and fine-grained refinement, respectively. Control actions are derived within the embedding space, removing the need for state reconstruction. Simulation results on inverted cart-pole systems demonstrate that H-JEPA enables up to 42.83 % more devices to be supported under limited wireless capacity without compromising control performance.

</details>


### [557] [Learning Alzheimer's Disease Signatures by bridging EEG with Spiking Neural Networks and Biophysical Simulations](https://arxiv.org/abs/2602.07010)
*Szymon Mamoń,Max Talanov,Alessandro Crimi*

Main category: cs.NE

Relevance: 25.0

TL;DR: 提出神经桥接框架，将基于尖峰神经网络的阿尔茨海默病EEG分类与生物物理模拟相结合，实现机器学习特征与神经环路机制的双向解释。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断需要从非侵入性生物标志物（如EEG）中获得机制性见解。传统深度学习方法计算量大且机制不透明，而尖峰神经网络具有生物合理性和能效优势，但在AD诊断中应用不足。

Method: 提出神经桥接框架：1）使用临床静息态EEG训练SNN分类器进行AD检测；2）通过改变兴奋-抑制突触比例构建尖峰网络模拟，模拟健康、轻度认知障碍和AD状态；3）结合经验功能连接先验进行多子网络模拟。

Result: SNN分类器达到AUC=0.839的竞争性性能，识别1/f斜率作为关键判别标志。模拟成功复现了经验频谱减慢和α节律改变。加入功能连接先验后，频谱分化进一步增强。

Conclusion: 神经桥接方法将SNN分类与可解释的环路模拟相结合，推进了对EEG生物标志物的机制理解，同时实现了可扩展、可解释的AD检测。

Abstract: As the prevalence of Alzheimer's disease (AD) rises, improving mechanistic insight from non-invasive biomarkers is increasingly critical. Recent work suggests that circuit-level brain alterations manifest as changes in electroencephalography (EEG) spectral features detectable by machine learning. However, conventional deep learning approaches for EEG-based AD detection are computationally intensive and mechanistically opaque. Spiking neural networks (SNNs) offer a biologically plausible and energy-efficient alternative, yet their application to AD diagnosis remains largely unexplored.
  We propose a neuro-bridge framework that links data-driven learning with minimal, biophysically grounded simulations, enabling bidirectional interpretation between machine learning signatures and circuit-level mechanisms in AD. Using resting-state clinical EEG, we train an SNN classifier that achieves competitive performance (AUC = 0.839) and identifies the aperiodic 1/f slope as a key discriminative marker.
  The 1/f slope reflects excitation-inhibition balance. To interpret this mechanistically, we construct spiking network simulations in which inhibitory-to-excitatory synaptic ratios are systematically varied to emulate healthy, mild cognitive impairment, and AD-like states. Using both membrane potential-based and synaptic current-based EEG proxies, we reproduce empirical spectral slowing and altered alpha organization.
  Incorporating empirical functional connectivity priors into multi-subnetwork simulations further enhances spectral differentiation, demonstrating that large-scale network topology constrains EEG signatures more strongly than excitation-inhibition balance alone. Overall, this neuro-bridge approach connects SNN-based classification with interpretable circuit simulations, advancing mechanistic understanding of EEG biomarkers while enabling scalable, explainable AD detection.

</details>


### [558] [When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding](https://arxiv.org/abs/2602.07039)
*Heimo Müller*

Main category: cs.CY

Relevance: 25.0

TL;DR: 该论文分析了竞争性研究资助体系的结构性悖论：尽管参与者认识到当前体系接近功能极限，但改革措施反而加剧了根本问题。作者认为"卓越"已与知识生产脱钩，而与评估中的"可呈现性"紧密耦合。


<details>
  <summary>Details</summary>
Motivation: 作者基于近四十年的研究资助参与经验（作为申请人、协调人、评估人和小组成员），观察到当前竞争性研究资助体系存在结构性矛盾。虽然许多参与者认识到体系已接近功能极限，但大多数改革措施反而加剧了根本问题，而非缓解。

Method: 采用质性分析方法，基于作者作为"内部人士"的亲身经验，通过案例研究考察两个领域：竞争性基础研究资助和大型欧盟联盟项目。分析三个加速趋势：提案写作专业化（通过专业顾问）、AI辅助申请兴起、以及评估人员短缺导致评审越来越脱离实际研究领域。

Result: 研究发现"卓越"概念已与知识生产脱钩，而与评估中的"可呈现性"紧密耦合。这种现象在竞争性基础研究资助和大型欧盟项目中尤为明显。三个趋势相互强化，形成了恶性循环：专业化和AI工具使提案更加"可呈现"，而评估人员短缺又迫使评审依赖表面指标而非实质内容。

Conclusion: 当前研究资助体系面临系统性危机，改革措施往往适得其反。作者希望通过对这一普遍经历但很少明确表述的模式进行命名，能够促进更建设性的方向调整。这本质上是一个Goodhart's Law（当指标成为目标时，就不再是好指标）在科研评估中的体现。

Abstract: After almost four decades of participating in competitive research funding -- as applicant, coordinator, evaluator, and panel member -- I have come to see a structural paradox: many participants recognize that the current system is approaching its functional limits, yet most reform measures intensify rather than alleviate the underlying dynamics. This paper documents how excellence has become decoupled from knowledge production through an increasing coupling to representability under evaluation. The discussion focuses on two domains in which this is particularly visible: competitive basic research funding and large EU consortium projects. Three accelerating trends are examined: the professionalization of proposal writing through specialized consultants, the rise of AI-assisted applications, and an evaluator shortage that forces panels to rely on reviewers increasingly distant from the actual research domains. These observations are offered not as external critique but as an insider account, in the hope that naming a widely experienced but rarely articulated pattern may enable more constructive orientation.
  Keywords: Research funding, Excellence, Evaluation, Goodhart's Law, Professionalization, AI-assisted proposals, Peer review crisis

</details>


### [559] [Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation](https://arxiv.org/abs/2602.07073)
*Nardine Basta,Firas Ben Hmida,Houssem Jmal,Muhammad Ikram,Mohamed Ali Kaafar,Andy Walker*

Main category: cs.CR

Relevance: 25.0

TL;DR: 提出Pro-ZD框架，使用图神经网络识别加权最短路径，检测网络配置错误和高风险连接路径，自动调整防火墙规则以防止零日攻击


<details>
  <summary>Details</summary>
Motivation: 企业网络中防火墙规则和访问策略的动态生成带来了管理挑战，特别是关键资产暴露的风险。随着远程用户、自带设备和云集成等趋势，网络结构不断演变，需要有效管理动态生成策略带来的风险

Method: 提出Pro-ZD框架，采用图神经网络模型识别加权最短路径，检测网络配置错误和高风险连接路径，自动微调防火墙规则和访问策略以应对高风险连接

Result: 实验结果显示Pro-ZD具有鲁棒性和可迁移性，在检测高风险连接方面平均准确率超过95%

Conclusion: Pro-ZD框架能够有效识别网络配置风险并自动调整安全策略，为防范零日攻击提供主动防护

Abstract: In today's enterprise network landscape, the combination of perimeter and distributed firewall rules governs connectivity. To address challenges arising from increased traffic and diverse network architectures, organizations employ automated tools for firewall rule and access policy generation. Yet, effectively managing risks arising from dynamically generated policies, especially concerning critical asset exposure, remains a major challenge. This challenge is amplified by evolving network structures due to trends like remote users, bring-your-own devices, and cloud integration. This paper introduces a novel graph neural network model for identifying weighted shortest paths. The model aids in detecting network misconfigurations and high-risk connectivity paths that threaten critical assets, potentially exploited in zero-day attacks -- cyber-attacks exploiting undisclosed vulnerabilities. The proposed Pro-ZD framework adopts a proactive approach, automatically fine-tuning firewall rules and access policies to address high-risk connections and prevent unauthorized access. Experimental results highlight the robustness and transferability of Pro-ZD, achieving over 95% average accuracy in detecting high-risk connections. \

</details>


### [560] [Cognitive algorithms and systems of episodic memory, semantic memory and their learnings](https://arxiv.org/abs/2602.07261)
*Qi Zhang*

Main category: q-bio.NC

Relevance: 25.0

TL;DR: 该论文回顾了模拟人类陈述性记忆（包括情景记忆和语义记忆）的计算系统，以及模拟海马体损伤导致的各种记忆障碍的神经解剖学模型。


<details>
  <summary>Details</summary>
Motivation: 理解人类陈述性记忆的神经基础，特别是情景记忆（海马体相关）和语义记忆（新皮层相关）之间的分离与联系，以及海马体损伤如何导致顺行性、逆行性和发育性遗忘等记忆障碍。

Method: 回顾性综述方法，分析两类计算系统：1）以模拟陈述性记忆为中心的计算系统；2）基于神经解剖学结构、模拟海马体损伤导致记忆障碍的系统。重点分析系统结构、学习规则以及记忆获取和障碍的模拟。

Result: 系统性地回顾了多个计算模型，展示了这些模型如何模拟人类记忆的获取、存储和组织过程，以及如何通过计算建模来解释海马体损伤导致的各种记忆障碍现象。

Conclusion: 计算建模为理解人类陈述性记忆的神经机制提供了重要工具，特别是通过模拟海马体损伤导致的记忆障碍，有助于揭示情景记忆和语义记忆之间的复杂关系及其神经基础。

Abstract: Declarative memory, the memory that can be "declared" in words or languages, is made up of two dissociated parts: episodic memory and semantic memory. This dissociation has its neuroanatomical basis episodic memory is mostly associated with the hippocampus and semantic memory with the neocortex. The two memories, on the other hand, are closely related. Lesions in the hippocampus often result in various impairments of explicit memory, e.g., anterograde, retrograde and developmental amnesias, and semantic learning deficit. These impairments provide opportunities for us to understand how the two memories may be acquired, stored and organized. This chapter reviews several cognitive systems that are centered to mimic explicit memory, and other systems that are neuroanatomically based and are implemented to simulate those memory impairments mentioned above. This review includes: the structures of the computational systems, their learning rules, and their simulations of memory acquisition and impairments.

</details>


### [561] [VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots](https://arxiv.org/abs/2602.07506)
*Peizhen Li,Longbing Cao,Xiao-Ming Wu,Yang Zhang*

Main category: cs.RO

Relevance: 25.0

TL;DR: VividFace是一个实时逼真的人形机器人面部表情模仿系统，能在0.05秒内模仿人类面部表情，通过优化的人形到人形面部运动转移模块和特征适应训练策略实现高表现力。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人面部表情模仿系统存在实时性不足和表现力不够逼真的问题，主要受限于离线视频推理设计和无法捕捉细微表情细节。需要开发一个既能实时运行又能产生逼真表情的系统来支持生动的人形机器人和情感人机交互。

Method: 1. 优化的模仿框架X2CNet++：微调人形到人形面部运动转移模块，引入特征适应训练策略以更好地对齐不同图像源；2. 视频流兼容的推理管道；3. 基于异步I/O的简化工作流程，实现跨设备高效通信。

Result: VividFace能在0.05秒内模仿人类面部表情，在多样化面部配置上具有良好的泛化能力。广泛的真实世界演示验证了其实用性。

Conclusion: VividFace实现了实时且逼真的人形机器人面部表情模仿，解决了现有系统的局限性，为人形机器人的面部表情交互提供了实用解决方案。

Abstract: Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.

</details>


### [562] [Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology](https://arxiv.org/abs/2602.08764)
*Hjalti Thrastarson,Lotta M. Ellingsen*

Main category: eess.IV

Relevance: 25.0

TL;DR: 提出一种基于改进U-net和符号距离变换损失函数的鲁棒脑MRI颅骨剥离方法，在保持大脑外表面一致性方面表现优异


<details>
  <summary>Details</summary>
Motivation: 现有脑MRI颅骨剥离方法在神经病理情况下容易失败，且大脑掩模边界定义不一致，需要更鲁棒和一致的方法来准确分割大脑外表面

Method: 使用改进的U-net架构，结合基于符号距离变换（SDT）的新型损失函数，在银标准ground truth数据上进行训练

Result: 在测试数据集上达到DSC 0.964±0.006和ASSD 1.4mm±0.2mm，在外部数据集上达到DSC 0.958±0.006和ASSD 1.7±0.2mm，性能与现有SOTA方法相当或更好

Conclusion: 该方法在脑提取任务中表现出色，特别是在保持大脑外表面一致性方面，代码已在GitHub公开

Abstract: Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology and can be inconsistent in defining the boundary of the brain mask. Here, we propose a novel approach to skull strip T1-weighted images in a robust and efficient manner, aiming to consistently segment the outer surface of the brain, including the sulcal cerebrospinal fluid (CSF), while excluding the full extent of the subarachnoid space and meninges. We train a modified version of the U-net on silver-standard ground truth data using a novel loss function based on the signed-distance transform (SDT). We validate our model both qualitatively and quantitatively using held-out data from the training dataset, as well as an independent external dataset. The brain masks used for evaluation partially or fully include the subarachnoid space, which may introduce bias into the comparison; nonetheless, our model demonstrates strong performance on the held-out test data, achieving a consistent mean Dice similarity coefficient (DSC) of 0.964$\pm$0.006 and an average symmetric surface distance (ASSD) of 1.4mm$\pm$0.2mm. Performance on the external dataset is comparable, with a DSC of 0.958$\pm$0.006 and an ASSD of 1.7$\pm$0.2mm. Our method achieves performance comparable to or better than existing state-of-the-art methods for brain extraction, particularly in its highly consistent preservation of the brain's outer surface. The method is publicly available on GitHub.

</details>


### [563] [Incremental Mapping with Measurement Synchronization & Compression](https://arxiv.org/abs/2602.07901)
*Mark Griguletskii,Danil Belov,Pavel Osinenko*

Main category: cs.RO

Relevance: 25.0

TL;DR: 提出了一种增量构建连接因子图的新方法，通过选择最优图拓扑来融合异步多传感器数据，实现约30%的节点压缩，同时保持地图质量与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆和机器人使用多种传感器进行定位和建图，但异步传感器测量与离散图表示相结合使得一致状态估计变得复杂。传统方法依赖刚性图结构，在传感器速率差异大的情况下效率低下，而预积分技术适用性有限。最优因子图拓扑设计在多传感器异步系统中仍是一个开放挑战。

Method: 提出增量构建连接因子图的方法，基于外部评估标准选择最优图拓扑，确保纳入所有可用传感器数据。该方法支持图压缩，减少优化变量（节点）数量。

Result: 平均减少约30%的节点数量，同时地图质量与传统方法保持相当水平。

Conclusion: 该方法有效解决了多传感器异步系统中的因子图拓扑优化问题，实现了显著的计算效率提升而不牺牲建图精度。

Abstract: Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.

</details>


### [564] [Roadmap to Quantum Aesthetics](https://arxiv.org/abs/2602.08363)
*Ivan C. H. Liu,Hsiao-Yuan Chen*

Main category: physics.pop-ph

Relevance: 25.0

TL;DR: 该论文提出了量子美学路线图，通过艺术中介而非直接表征探索量子概念如何成为审美现象，包含自上而下（基于文本提示的生成AI）和自下而上（量子力学结构可视化）两种互补方法。


<details>
  <summary>Details</summary>
Motivation: 量子力学在当代科学中占据核心地位，但难以通过感官直接体验。论文旨在探索量子概念如何通过艺术中介成为审美现象，建立量子美学作为艺术研究的新领域。

Method: 提出两种互补方法：1）自上而下方法：使用基于文本提示的生成AI，通过系统调节"量子"一词的语言权重，探索量子想象在视觉文化中的集体建构；2）自下而上方法：通过可视化量子生成数据（如从薛定谔方程计算的氢原子轨道）直接从量子力学结构推导审美形式。

Result: 建立了量子美学作为艺术研究的新兴领域，展示了文化想象、计算中介和物理定律如何共同塑造量子美学，为艺术实践和教学开辟了新方向。

Conclusion: 量子美学是艺术研究的新兴领域，两种方法不是竞争关系，而是在艺术研究可导航领域中的交叉路径，为艺术、数据、人工智能和量子科学的交叉领域开辟了新方向。

Abstract: Quantum mechanics occupies a central position in contemporary science while remaining largely inaccessible to direct sensory experience. This paper proposes a roadmap to quantum aesthetics that examines how quantum concepts become aesthetic phenomena through artistic mediation rather than direct representation. Two complementary and orthogonal approaches are articulated. The first, a pioneering top-down approach, employs text-prompt-based generative AI to probe quantum aesthetics as a collective cultural construct embedded in large-scale training data. By systematically modulating the linguistic weight of the term "quantum," generative models are used as experimental environments to reveal how quantum imaginaries circulate within contemporary visual culture. The second, a bottom-up approach, derives aesthetic form directly from quantum-mechanical structures through the visualization of quantum-generated data, exemplified here by hydrogen atomic orbitals calculated from the Schrödinger equation. These approaches are framed not as competing methods but as intersecting paths within a navigable field of artistic research. They position quantum aesthetics as an emergent field of artistic research shaped by cultural imagination, computational mediation, and physical law, opening new directions for artistic practice and pedagogy at the intersection of art, data, artificial intelligence and quantum science.

</details>


### [565] [A General Theory of Proportionality with Additive Utilities](https://arxiv.org/abs/2602.08504)
*Piotr Skowron*

Main category: cs.GT

Relevance: 25.0

TL;DR: 该论文提出了基于基数投票（选民为候选人分配数值效用）的比例代表制规则，用于受约束的候选人选择问题，并引入了能产生比例排名的算法。


<details>
  <summary>Details</summary>
Motivation: 现有比例代表制规则主要针对批准投票（选民选择可接受的候选人子集），但缺乏针对基数投票（选民为候选人分配效用值）的比例规则。需要为更丰富的偏好表达形式开发比例代表制方法。

Method: 提出了适用于基数投票的比例规则，并引入了能产生比例排名的算法，确保排名的每个前缀都满足比例性要求。模型涵盖委员会选举、参与式预算、公共决策等多种约束场景。

Result: 开发了适用于基数投票的比例代表制规则，这些规则能处理一般约束条件，并提供了产生比例排名的算法，扩展了比例代表制理论的应用范围。

Conclusion: 该研究填补了基数投票比例代表制规则的空白，为更丰富的偏好表达形式提供了比例性保障，扩展了比例代表制理论在受约束选择问题中的应用。

Abstract: We consider a model where a subset of candidates must be selected based on voter preferences, subject to general constraints that specify which subsets are feasible. This model generalizes committee elections with diversity constraints, participatory budgeting (including constraints specifying how funds must be allocated to projects from different pools), and public decision-making. Axioms of proportionality have recently been defined for this general model, but the proposed rules apply only to approval ballots, where each voter submits a subset of candidates she finds acceptable. We propose proportional rules for cardinal ballots, where each voter assigns a numerical value to each candidate corresponding to her utility if that candidate is selected. In developing these rules, we also introduce methods that produce proportional rankings, ensuring that every prefix of the ranking satisfies proportionality.

</details>


### [566] [Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study](https://arxiv.org/abs/2602.08619)
*Laura-Maria Cornei,Mihaela-Elena Breabăn*

Main category: cs.NE

Relevance: 25.0

TL;DR: 该论文提出了一种结合多模态遗传算法和图神经网络的时间表优化混合方法，其中图神经网络封装领域知识，遗传算法探索搜索空间，两者协同提升调度质量。


<details>
  <summary>Details</summary>
Motivation: 时间表优化问题通常具有高计算复杂度，传统方法如遗传算法在探索效率上有局限，而图神经网络能学习领域知识但可能陷入局部最优。作者希望通过结合两者的优势，实现更高效、更高质量的调度解决方案。

Method: 1) 设计多模态遗传算法探索搜索空间的不同区域；2) 设计图神经网络封装通用领域知识；3) 将图神经网络作为增强算子集成到遗传算法中，引导搜索方向；4) 在员工排班问题上独立优化两个组件后进行混合；5) 通过实验比较混合方法与独立组件的性能。

Result: 实验结果表明，与独立的遗传算法和图神经网络相比，提出的混合方法在时间效率和解决方案质量指标上都带来了统计显著的改进。该方法在员工排班问题上表现出优越性能。

Conclusion: 遗传算法与图神经网络的混合方法能有效解决时间表优化问题，结合了深度学习的知识表示能力和进化算法的全局搜索能力，为复杂调度问题提供了新的解决方案。

Abstract: This paper investigates the impact of hybridizing a multi-modal Genetic Algorithm with a Graph Neural Network for timetabling optimization. The Graph Neural Network is designed to encapsulate general domain knowledge to improve schedule quality, while the Genetic Algorithm explores different regions of the search space and integrates the deep learning model as an enhancement operator to guide the solution search towards optimality. Initially, both components of the hybrid technique were designed, developed, and optimized independently to solve the tackled task. Multiple experiments were conducted on Staff Rostering, a well-known timetabling problem, to compare the proposed hybridization with the standalone optimized versions of the Genetic Algorithm and Graph Neural Network. The experimental results demonstrate that the proposed hybridization brings statistically significant improvements in both the time efficiency and solution quality metrics, compared to the standalone methods. To the best of our knowledge, this work proposes the first hybridization of a Genetic Algorithm with a Graph Neural Network for solving timetabling problems.

</details>


### [567] [PBLean: Pseudo-Boolean Proof Certificates for Lean 4](https://arxiv.org/abs/2602.08692)
*Stefan Szeider*

Main category: cs.LO

Relevance: 25.0

TL;DR: PBLean：将VeriPB伪布尔证明证书导入Lean 4的方法，通过反射实现高效验证，支持大规模证明和编码验证，在组合问题中生成可组合的Lean定理。


<details>
  <summary>Details</summary>
Motivation: 解决形式验证中伪布尔证明证书的可信验证问题，填补求解器输出与问题语义之间的信任鸿沟，使验证结果能作为可组合引理用于更大的形式化开发。

Method: 采用反射方法：在Lean中完全证明布尔检查函数的正确性，编译为原生代码执行；支持VeriPB所有核心规则（切割平面推导、反证法子证明）；支持验证编码以处理原始组合问题。

Result: 方法可扩展到包含数万步骤的证明（显式证明项构造会耗尽内存）；支持所有VeriPB核心规则；生成可组合的Lean定理而非外部验证检查器的简单判决；在多种组合问题上成功演示。

Conclusion: PBLean通过反射和验证编码，实现了伪布尔证明证书的高效形式验证，填补了求解器输出与问题语义之间的信任鸿沟，为组合问题的形式验证提供了实用工具。

Abstract: We present PBLean, a method for importing VeriPB pseudo-Boolean (PB) proof certificates into Lean 4. Key to our approach is reflection: a Boolean checker function whose soundness is fully proved in Lean and executed as compiled native code. Our method scales to proofs with tens of thousands of steps that would exhaust memory under explicit proof-term construction. Our checker supports all VeriPB kernel rules, including cutting-plane derivations and proof-by-contradiction subproofs. In contrast to external verified checkers that produce verdicts, our integration yields Lean theorems that can serve as composable lemmas in larger formal developments. To derive theorems about the original combinatorial problems rather than about PB constraints alone, we support verified encodings. This closes the trust gap between solver output and problem semantics since the constraint translation and its correctness proof are both formalized in Lean. We demonstrate the approach on various combinatorial problems.

</details>


### [568] [Technosocial risks of ideal emotion recognition technologies: A defense of the (social) value of emotional expressions](https://arxiv.org/abs/2602.08706)
*Alexandra Pregent*

Main category: cs.HC

Relevance: 25.0

TL;DR: 本文挑战了理想情感识别技术（ERTs）能通过增加情感透明度改善社会生活的假设，论证这些技术会威胁社会功能，主张基于功能优先的监管方法保护表达自由。


<details>
  <summary>Details</summary>
Motivation: 挑战关于理想情感识别技术（ERTs）能通过增加情感透明度改善社会生活的流行假设。作者认为这种假设误解了情感表达的社会功能，需要从哲学和社会科学角度分析ERTs的技术社会风险。

Method: 结合哲学的情感表达理论、社会实践分析，以及情感科学和社会心理学的实证研究，分析理想ERTs（多模态、实时、可靠推断内在情感状态的系统）的技术社会风险。

Result: 论证情感表达不仅是内在状态的反映，更是协调行动、道德修复、维持人际信任和支持集体规范的工具。这些功能依赖于部分不透明和认知摩擦的背景。理想ERTs会消除认知摩擦，用技术介导的情感档案取代关系意义，限制表达空间，导致情感决定论和情感审计，损害社会凝聚力和个体能动性。

Conclusion: 情感识别技术的准确性并不能直接证明其部署的正当性，在某些情况下反而成为监管限制的理由。主张功能优先的监管方法，将表达自由和有意情感表达视为特定社会善的构成要素，保护这些善免受过度情感可读性的侵害。

Abstract: The prospect of AI systems that I call ideal emotion recognition technologies (ERTs) is often defended on the assumption that social life would benefit from increased affective transparency. This paper challenges that assumption by examining the technosocial risks posed by ideal ERTs, understood as multimodal systems capable of reliably inferring inner affective states in real time. Drawing on philosophical accounts of emotional expression and social practice, as well as empirical work in affective science and social psychology, I argue that the appeal of such systems rests on a misunderstanding of the social functions of emotional expression. Emotional expressions function not only as read-outs of inner states, but also as tools for coordinating action, enabling moral repair, sustaining interpersonal trust, and supporting collective norms. These functions depend on a background of partial opacity and epistemic friction. When deployed in socially authoritative or evaluative contexts, ideal ERTs threaten this expressive space by collapsing epistemic friction, displacing relational meaning with technology-mediated affective profiles, and narrowing the space for aspirational and role-sensitive expressions. The result is a drift towards affective determinism and ambient forms of affective auditing, which undermine both social cohesion and individual agency. I argue that, although it is intuitive to think that increasing accuracy would legitimise such systems, in the case of ERTs accuracy does not straightforwardly justify their deployment, and may, in some contexts, provide a reason for regulatory restraint. I conclude by defending a function-first regulatory approach that treats expressive discretion and intentional emotional expression as constitutive of certain social goods, and that accordingly seeks to protect these goods from excessive affective legibility.

</details>


### [569] [Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach](https://arxiv.org/abs/2602.08456)
*Francesc Wilhelmi,Boris Bellalta,Miguel Casasnovas,Aleksandra Kijanka,Miguel Calvo-Fullana*

Main category: cs.NI

Relevance: 20.0

TL;DR: 本文提出了一种基于后悔匹配的去中心化学习算法，用于优化IEEE 802.11网络中的空间复用参数（传输功率和载波侦听阈值），通过内部后悔最小化引导竞争代理达到相关均衡，无需显式通信即可实现协调。


<details>
  <summary>Details</summary>
Motivation: 在密集的IEEE 802.11部署中，空间复用技术能提高频谱效率，但去中心化优化传输功率和载波侦听阈值面临挑战：缺乏全局状态信息，多代理并发操作导致高度非平稳环境，现有方法常收敛到低效的纳什均衡（如默认使用最大传输功率）。

Method: 提出基于内部后悔最小化的后悔匹配去中心化学习算法。与标准的"自私"去中心化方法不同，内部后悔最小化引导竞争代理达到相关均衡，无需显式通信即可模拟协调，避免了集中式解决方案（如多接入点协调）的繁重信令开销和架构复杂性。

Result: 仿真结果表明，所提方法优于现有方法，能够达到接近最优的全局性能，展示了可扩展去中心化解决方案的潜力，质疑了集中式解决方案的必要性。

Conclusion: 基于内部后悔最小化的去中心化学习算法能有效优化空间复用参数，实现接近最优的全局性能，证明了可扩展去中心化解决方案的潜力，挑战了需要繁重信令开销的集中式解决方案的必要性。

Abstract: Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Service Sets (BSSs) is challenging due to the lack of global state information. In addition, the concurrent operation of multiple agents creates a highly non-stationary environment, often resulting in suboptimal global configurations (e.g., using the maximum possible transmission power by default). To overcome these limitations, this paper introduces a decentralized learning algorithm based on regret-matching, grounded in internal regret minimization. Unlike standard decentralized ``selfish'' approaches that often converge to inefficient Nash Equilibria (NE), internal regret minimization guides competing agents toward Correlated Equilibria (CE), effectively mimicking coordination without explicit communication. Through simulation results, we showcase the superiority of our proposed approach and its ability to reach near-optimal global performance. These results confirm the not-yet-unleashed potential of scalable decentralized solutions and question the need for the heavy signaling overheads and architectural complexity associated with emerging centralized solutions like Multi-Access Point Coordination (MAPC).

</details>


### [570] [Hybrid Deep Learning Framework for CSI-Based Activity Recognition in Bandwidth-Constrained Wi-Fi Sensing](https://arxiv.org/abs/2602.06983)
*Alison M. Fernandes,Hermes I. Del Monego,Bruno S. Chang,Anelise Munaretto,Hélder M. Fontes,Rui Campos*

Main category: eess.SP

Relevance: 15.0

TL;DR: 提出一种混合深度学习框架，通过多普勒轨迹提取增强Wi-Fi信号特征，结合Inception网络和BiLSTM进行时空特征提取，使用SVM分类，在带宽受限环境下显著提升人体活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的Wi-Fi感知环境中，基于信道状态信息(CSI)的人体活动识别(HAR)面临信号特征提取困难的问题。现有方法在低带宽场景下性能下降明显，需要一种能够增强运动相关信号特征并有效提取时空特征的鲁棒框架。

Method: 1. 多普勒轨迹提取阶段：放大运动相关的信号特征；2. 混合神经网络架构：Inception网络提取层次空间特征，BiLSTM网络捕获时间依赖关系；3. SVM作为最终分类层优化决策边界；4. 在20、40、80 MHz三种带宽配置下进行系统验证。

Result: 在公开数据集上获得显著性能提升：20 MHz带宽下准确率89.27%，40 MHz下94.13%，80 MHz下95.30%。在低带宽场景下明显优于单独的深度学习基线方法。

Conclusion: 多普勒特征工程与混合学习架构的结合对于带宽受限的无线感知应用中可靠的人体活动识别具有重要价值，特别是在最受限的低带宽场景下表现出显著优势。

Abstract: This paper presents a novel hybrid deep learning framework designed to enhance the robustness of CSI-based Human Activity Recognition (HAR) within bandwidth-constrained Wi-Fi sensing environments. The core of our proposed methodology is a preliminary Doppler trace extraction stage, implemented to amplify salient motion-related signal features before classification. Subsequently, these enhanced inputs are processed by a hybrid neural architecture, which integrates Inception networks responsible for hierarchical spatial feature extraction and Bidirectional Long Short-Term Memory (BiLSTM) networks that capture temporal dependencies. A Support Vector Machine (SVM) is then utilized as the final classification layer to optimize decision boundaries. The framework's efficacy was systematically validated using a public dataset across 20, 40, and 80 MHz bandwidth configurations. The model yielded accuracies of 89.27% (20 MHz), 94.13% (40 MHz), and 95.30% (80 MHz), respectively. These results confirm a marked superiority over standalone deep learning baselines, especially in the most constrained low-bandwidth scenarios. This study underscores the utility of combining Doppler-based feature engineering with a hybrid learning architecture for reliable HAR in bandwidth-limited wireless sensing applications.

</details>


### [571] [SurfAge-Net: A Hierarchical Surface-Based Network for Interpretable Fine-Grained Brain Age Prediction](https://arxiv.org/abs/2602.06994)
*Rongzhao He,Dalin Zhu,Ying Wang,Songhong Yue,Leilei Zhao,Yu Fu,Dan Wu,Bin Hu,Weihao Zheng*

Main category: q-bio.NC

Relevance: 15.0

TL;DR: 提出SurfAge-Net，一种基于球面的大脑年龄预测网络，通过多模态形态学指标捕捉区域特异性发育模式，结合连接组学原理建模半球内和半球间依赖关系，实现细粒度脑龄预测。


<details>
  <summary>Details</summary>
Motivation: 现有脑龄预测方法主要关注全脑年龄预测，忽略了大脑发育的区域异质性，而区域特异性成熟模式对于检测局部异常轨迹至关重要。需要一种能够捕捉区域特异性发育模式、具有更强鲁棒性和临床可解释性的方法。

Method: 提出SurfAge-Net：1）基于球面表面，利用多种形态学指标；2）结合连接组学原理，通过空间-通道混合和侧化感知注意力机制，显式建模半球内和半球间依赖关系；3）能够表征与每个目标区域相关的协调成熟模式。

Result: 在三个胎儿和新生儿数据集上验证，SurfAge-Net优于现有方法（全局MAE=0.54，区域MAE=0.45，以孕周/后月经周为单位），在外部队列中表现出强泛化能力。提供空间精确、生物学可解释的皮质成熟图谱，有效识别非典型发育人群的异质性延迟和区域特异性异常。

Conclusion: 细粒度脑龄预测是推进神经发育研究和支持早期临床评估的有前景范式。SurfAge-Net通过结合连接组学原理，实现了区域特异性脑龄预测，为神经发育障碍的早期检测提供了新工具。

Abstract: Brain age prediction serves as a powerful framework for assessing brain status and detecting deviations associated with neurodevelopmental and neurodegenerative disorders. However, most existing approaches emphasize whole-brain age prediction and therefore overlook the pronounced regional heterogeneity of brain maturation that is crucial for detecting localized atypical trajectories. To address this limitation, we propose a novel spherical surface-based brain age prediction network (SurfAge-Net) that leverages multiple morphological metrics to capture region-specific developmental patterns with enhanced robustness and clinical interpretability. SurfAge-Net establishes a new modeling paradigm by incorporating the connectomic principles of cortical organization: it explicitly models both intra- and inter-hemispheric dependencies through a spatial-channel mixing and a lateralization-aware attention mechanism, enabling the network to characterize the coordinate maturation pattern uniquely associated with each target region. Validated on three fetal and neonatal datasets, SurfAge-Net outperforms existing approaches (global MAE = 0.54, regional MAE = 0.45 in gestational/postmenstrual weeks) and demonstrates strong generalizability across external cohorts. Importantly, it provides spatially precise and biologically interpretable maps of cortical maturation, effectively identifying heterogeneous delays and regional-specific abnormalities in atypical developmental populations. These results established fine-grained brain age prediction as a promising paradigm for advancing neurodevelopmental research and supporting early clinical assessment.

</details>


### [572] [Imagining the Alien: Human Projections and Cognitive Limitations](https://arxiv.org/abs/2602.07284)
*S. G. Djorgovski*

Main category: astro-ph.IM

Relevance: 15.0

TL;DR: 这篇论文探讨了人类想象外星智能的认知局限性，并提出了人工智能作为"外星智能"的替代研究途径。


<details>
  <summary>Details</summary>
Motivation: 人类对外星生命的想象长期受限于地球生命形式和文化心理模式，这种认知局限性反映了人类思维的固有局限。作者希望通过研究人工智能作为"外星智能"的替代形式，来突破这种局限性，为理解外星高级智能提供新视角。

Method: 论文采用哲学分析和跨学科研究方法，结合文化人类学、认知科学和人工智能领域的知识，分析人类想象外星智能的认知模式，并提出将人工智能作为研究外星智能的替代途径。

Result: 研究发现人类对外星智能的想象主要受限于地球生命形式和文化心理映射，而人工智能作为人类创造的"外星智能"，其快速发展可能为我们理解外星高级智能提供新的认知框架和研究方法。

Conclusion: 人工智能可以作为研究外星智能的替代途径，帮助我们突破人类认知的局限性，为理解外星高级智能提供新的视角和方法论。

Abstract: Imagining what life on other planets, and intelligent life in particular, may be like is a long-running theme in human culture. It is a manifestation of the innate human curiosity about the Cosmos, and it has inspired numerous works of art and folklore, including whole literary and other media genres. It is a profound question, with philosophical and existential implications. There is also an obvious connection with religious beliefs, as gods and other superhuman beings were imagined in the heavens. Speculations about alien beings grew in time, and today, it is a scientific subject of astrobiology, and it is pursued through serious searches for life and intelligence in the universe. However, almost all imaginings of the alien map terrestrial life forms and human cultural, historical, and psychological phenomena to the putative aliens. This lack of individual and collective imagination may reflect our biological and cultural evolution, as our minds are formed through our experiences, perceptions of the world, and interactions with our terrestrial and human environments. As such, imagining aliens is mainly a cultural phenomenon and may reflect the intrinsic cognitive limitations of the human mind. Interestingly, we did create what is effectively an alien intelligence on this planet in the form of now rapidly evolving Artificial Intelligence (AI). As its capabilities grow, it may give us new insights into what extraterrestrial advanced intelligences may be like.

</details>


### [573] [Mapping Drivers of Greenness: Spatial Variable Selection for MODIS Vegetation Indices](https://arxiv.org/abs/2602.07681)
*Qishi Zhan,Cheng-Han Yu,Yuchi Chen,Zhikang Dong,Rajarshi Guhaniyogi*

Main category: stat.ME

Relevance: 15.0

TL;DR: 提出了一种空间变系数模型，使用张量积B样条基和贝叶斯组套索先验，用于识别植被条件与多个环境驱动因素之间的关系，同时实现预测变量层面的稀疏化和空间结构保留。


<details>
  <summary>Details</summary>
Motivation: 研究植被条件与环境驱动因素之间的关系需要空间变系数模型，但当存在许多无关预测变量时，传统的空间变系数模型会产生噪声模式且难以解释。特别是在MODIS植被指数研究中，需要处理光谱带、生产力、能量通量、观测几何和地表特征等多种预测变量，这些关系随冠层结构、气候、土地利用和测量条件而变化。

Method: 提出了一种空间变系数模型，每个系数表面使用张量积B样条基，并在基系数上施加贝叶斯组套索先验。该先验诱导预测变量层面的收缩，将可忽略的效应推向零，同时保留空间结构。后验推断使用马尔可夫链蒙特卡洛方法，并为每个效应表面提供不确定性量化。

Result: 模拟实验能够恢复稀疏性并实现良好预测。在MODIS应用中，该方法产生了一个简约的预测变量子集，其效应图清晰地展示了不同景观中的主导控制因素。通过空间显著性地图和空间覆盖概率来总结保留的效应。

Conclusion: 该方法能够同时建模空间变化效应并识别重要预测变量，在保持空间结构的同时实现预测变量层面的稀疏化，为理解植被条件与环境驱动因素之间的关系提供了有效的统计框架。

Abstract: Understanding how environmental drivers relate to vegetation condition motivates spatially varying regression models, but estimating a separate coefficient surface for every predictor can yield noisy patterns and poor interpretability when many predictors are irrelevant. Motivated by MODIS vegetation index studies, we examine predictors from spectral bands, productivity and energy fluxes, observation geometry, and land surface characteristics. Because these relationships vary with canopy structure, climate, land use, and measurement conditions, methods should both model spatially varying effects and identify where predictors matter. We propose a spatially varying coefficient model where each coefficient surface uses a tensor product B-spline basis and a Bayesian group lasso prior on the basis coefficients. This prior induces predictor level shrinkage, pushing negligible effects toward zero while preserving spatial structure. Posterior inference uses Markov chain Monte Carlo and provides uncertainty quantification for each effect surface. We summarize retained effects with spatial significance maps that mark locations where the 95 percent posterior credible interval excludes zero, and we define a spatial coverage probability as the proportion of locations where the credible interval excludes zero. Simulations recover sparsity and achieve prediction. A MODIS application yields a parsimonious subset of predictors whose effect maps clarify dominant controls across landscapes.

</details>


### [574] [Constrained Pricing under Finite Mixtures of Logit](https://arxiv.org/abs/2602.08119)
*Hoang Giang Pham,Tien Mai*

Main category: math.OC

Relevance: 15.0

TL;DR: 本文研究约束条件下的混合logit定价问题，针对多类别logit模型提出多项式时间近似方案，对有限混合logit模型提出基于分支定界的算法，在客户细分数量有限时具有多项式时间近似性。


<details>
  <summary>Details</summary>
Motivation: 混合logit模型在定价和收益管理中应用广泛，但现有研究主要关注无约束场景，而实际业务中价格常受商业或监管约束，因此需要研究约束条件下的混合logit定价问题。

Method: 针对多类别logit模型（单客户细分），通过指数锥规划重构问题，获得多项式时间近似方案。针对有限混合logit模型（T个客户细分），将问题重构为包含O(T)个双线性项的指数锥规划，采用分支定界算法，其复杂度仅与T呈指数关系。

Result: 多类别logit模型的约束定价问题具有多项式时间近似方案；有限混合logit模型在客户细分数量有限时也具备多项式时间近似性。数值实验显示相对于现有基准方法具有优越性能。

Conclusion: 约束条件下的混合logit定价问题在客户细分数量有限时可有效求解，为实际业务中的约束定价提供了理论保证和实用算法。

Abstract: The mixed logit model is a flexible and widely used demand model in pricing and revenue management. However, existing work on mixed-logit pricing largely focuses on unconstrained settings, limiting its applicability in practice where prices are subject to business or regulatory constraints. We study the constrained pricing problem under multinomial and mixed logit demand models. For the multinomial logit model, corresponding to a single customer segment, we show that the constrained pricing problem admits a polynomial-time approximation scheme (PTAS) via a reformulation based on exponential cone programming, yielding an $\varepsilon$-optimal solution in polynomial time. For finite mixed logit models with $T$ customer segments, we reformulate the problem as a bilinear exponential cone program with $O(T)$ bilinear terms. This structure enables a Branch-and-Bound algorithm whose complexity is exponential only in $T$. Consequently, constrained pricing under finite mixtures of logit admits a PTAS when the number of customer segments is bounded. Numerical experiments demonstrate strong performance relative to state-of-the-art baselines.

</details>


### [575] [Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling](https://arxiv.org/abs/2602.08233)
*Jiatao Chen,Xing Tang,Xiaoyue Duan,Yutang Feng,Jinchao Zhang,Jie Zhou*

Main category: cs.SD

Relevance: 15.0

TL;DR: Tutti是一个统一的多歌手歌唱合成框架，通过结构感知歌手提示实现灵活歌手调度，并通过条件引导VAE进行互补纹理学习，显著提升合唱生成的声学真实感。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱合成系统虽然能实现高保真的独唱表演，但受限于全局音色控制，无法处理单首歌曲中的动态多歌手编排和声乐纹理变化。需要解决多歌手调度和声学纹理增强的问题。

Method: 提出Tutti统一框架：1）结构感知歌手提示，使歌手调度能够随音乐结构演变；2）通过条件引导VAE进行互补纹理学习，捕捉空间混响和频谱融合等隐式声学纹理。

Result: 实验表明Tutti在精确的多歌手调度方面表现出色，并显著增强了合唱生成的声学真实感，为复杂的多歌手编排提供了新范式。

Conclusion: Tutti为结构化多歌手生成提供了一个有效的统一框架，解决了现有系统在多歌手编排和声学纹理方面的局限性。

Abstract: While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for structured multi-singer generation. Specifically, we introduce a Structure-Aware Singer Prompt to enable flexible singer scheduling evolving with musical structure, and propose Complementary Texture Learning via Condition-Guided VAE to capture implicit acoustic textures (e.g., spatial reverberation and spectral fusion) that are complementary to explicit controls. Experiments demonstrate that Tutti excels in precise multi-singer scheduling and significantly enhances the acoustic realism of choral generation, offering a novel paradigm for complex multi-singer arrangement. Audio samples are available at https://annoauth123-ctrl.github.io/Tutii_Demo/.

</details>


### [576] [Automatic Generation of Polynomial Symmetry Breaking Constraints](https://arxiv.org/abs/2602.08297)
*Madalina Erascu,Johannes Middeke*

Main category: cs.SC

Relevance: 15.0

TL;DR: 提出一种代数方法生成随机多项式不等式作为对称性破缺约束，用于整数规划中的对称性处理，通过案例研究验证了简单二次破缺约束能有效减少求解时间。


<details>
  <summary>Details</summary>
Motivation: 整数规划中的对称性会导致冗余搜索，传统方法使用对称性破缺约束来消除等价解。本文旨在开发一种代数方法，能够生成随机多项式不等式作为更有效的对称性破缺器。

Method: 提出代数方法，输入任意基多项式和特定于整数规划的置换群，在符号计算软件中生成随机多项式不等式作为对称性破缺器。通过0-1装箱问题的案例研究，静态生成随机二次破缺器，并添加到基线整数规划问题中，使用Gurobi求解。

Result: 实验表明，简单的对称性破缺器（特别是结合少量变量和置换的二次约束）能够最一致地减少求解时间。

Conclusion: 代数方法能够有效生成对称性破缺约束，简单二次破缺器在减少整数规划求解时间方面表现最稳定。

Abstract: Symmetry in integer programming causes redundant search and is often handled with symmetry breaking constraints that remove as many equivalent solutions as possible. We propose an algebraic method which allows to generate a random family of polynomial inequalities which can be used as symmetry breakers. The method requires as input an arbitrary base polynomial and a group of permutations which is specific to the integer program. The computations can be easily carried out in any major symbolic computation software. In order to test our approach, we describe a case study on near half-capacity 0-1 bin packing instances which exhibit substantial symmetries. We statically generate random quadratic breakers and add them to a baseline integer programming problem which we then solve with Gurobi. It turns out that simple symmetry breakers, especially combining few variables and permutations, most consistently reduce work time.

</details>


### [577] [Optimizing Spectral Prediction in MXene-Based Metasurfaces Through Multi-Channel Spectral Refinement and Savitzky-Golay Smoothing](https://arxiv.org/abs/2602.08406)
*Shujaat Khan,Waleed Iqbal Waseer*

Main category: physics.optics

Relevance: 15.0

TL;DR: 提出一个结合迁移学习、多通道光谱细化和Savitzky-Golay平滑的深度学习框架，用于高效预测MXene基太阳能吸收器的电磁光谱，相比传统全波求解器显著加速计算。


<details>
  <summary>Details</summary>
Motivation: 传统使用全波求解器预测MXene基太阳能吸收器电磁光谱的计算成本很高，需要开发更高效的方法来加速纳米光子设计工作流程中的光谱预测。

Method: 采用预训练的MobileNetV2模型进行微调，从64×64元表面设计预测102点吸收光谱；引入多通道光谱细化模块通过多通道卷积增强特征提取；使用Savitzky-Golay平滑减少高频噪声。

Result: 模型在实验中显著优于基线CNN和可变形CNN模型，平均RMSE为0.0245，R²为0.9578，PSNR为32.98 dB，表现出优异的预测精度。

Conclusion: 该框架为传统求解器提供了可扩展且计算高效的替代方案，适合纳米光子设计工作流程中的快速光谱预测。

Abstract: The prediction of electromagnetic spectra for MXene-based solar absorbers is a computationally intensive task, traditionally addressed using full-wave solvers. This study introduces an efficient deep learning framework incorporating transfer learning, multi-channel spectral refinement (MCSR), and Savitzky-Golay smoothing to accelerate and enhance spectral prediction accuracy. The proposed architecture leverages a pretrained MobileNetV2 model, fine-tuned to predict 102-point absorption spectra from $64\times64$ metasurface designs. Additionally, the MCSR module processes the feature map through multi-channel convolutions, enhancing feature extraction, while Savitzky-Golay smoothing mitigates high-frequency noise. Experimental evaluations demonstrate that the proposed model significantly outperforms baseline Convolutional Neural Network (CNN) and deformable CNN models, achieving an average root mean squared error (RMSE) of 0.0245, coefficient of determination \( R^2 \) of 0.9578, and peak signal-to-noise ratio (PSNR) of 32.98 dB. The proposed framework presents a scalable and computationally efficient alternative to conventional solvers, positioning it as a viable candidate for rapid spectral prediction in nanophotonic design workflows.

</details>


### [578] [pixelLOG: Logging of Online Gameplay for Cognitive Research](https://arxiv.org/abs/2602.08941)
*Zeyu Lu,Dennis L. Barbour*

Main category: cs.HC

Relevance: 15.0

TL;DR: pixelLOG是一个用于Minecraft服务器的高性能数据收集框架，专门为基于过程的认知研究设计，支持人类行为追踪和多玩家/多智能体环境，填补了实验室评估与生态有效任务之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统认知评估通常依赖于孤立的、输出导向的测量，难以捕捉自然环境中人类认知的复杂性。需要一种能够在复杂虚拟环境中进行高分辨率认知过程分析的框架。

Method: 基于Spigot的Minecraft服务器框架，采用主动状态轮询和被动事件监控的混合方法，以可配置频率（最高超过20次/秒）捕获行为数据，通过Spigot的可扩展API实现强大的会话隔离，生成结构化JSON输出。

Result: 开发了pixelLOG框架，能够支持人类行为追踪和多玩家/多智能体环境，填补了现有框架仅针对AI智能体的局限，实现了实验室评估与生态有效任务之间的桥梁。

Conclusion: pixelLOG为认知研究提供了一个强大的数据收集工具，能够在复杂虚拟环境中进行高分辨率的行为分析，增强了认知评估的生态有效性。

Abstract: Traditional cognitive assessments often rely on isolated, output-focused measurements that may fail to capture the complexity of human cognition in naturalistic settings. We present pixelLOG, a high-performance data collection framework for Spigot-based Minecraft servers designed specifically for process-based cognitive research. Unlike existing frameworks tailored only for artificial intelligence agents, pixelLOG also enables human behavioral tracking in multi-player/multi-agent environments. Operating at configurable frequencies up to and exceeding 20 updates per second, the system captures comprehensive behavioral data through a hybrid approach of active state polling and passive event monitoring. By leveraging Spigot's extensible API, pixelLOG facilitates robust session isolation and produces structured JSON outputs integrable with standard analytical pipelines. This framework bridges the gap between decontextualized laboratory assessments and richer, more ecologically valid tasks, enabling high-resolution analysis of cognitive processes as they unfold in complex, virtual environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [579] [Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts](https://arxiv.org/abs/2602.06993)
*Shashank*

Main category: cs.LG

Relevance: 85.0

TL;DR: APN（Attractor Patch Networks）是Transformer中FFN的替代方案，通过原型匹配选择top-k专家补丁，生成低秩残差更新，实现条件化、上下文特化的非线性变换，在保持标准Transformer接口的同时，显著提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的FFN存在两个问题：1）对所有token使用相同计算量，无法根据上下文调整容量分配；2）持续学习时全局权重更新会产生干扰。APN旨在解决这些问题，实现更高效的条件化计算和更好的持续学习适应性。

Method: APN由专家补丁库组成，通过相似性路由器将token表示与学习到的原型匹配，选择top-k补丁。每个选中的补丁基于紧凑代码生成低秩残差更新。该方法保持Transformer标准接口，但实现条件化、上下文特化的非线性变换。

Result: 在字符级语言建模任务中，APN达到竞争性的困惑度（4.57 vs 4.32 PPL），同时在持续适应方面表现显著更好：适应领域偏移时，APN在原始领域的保持能力提高2.6倍（11.1 vs 29.4 PPL），在新领域的适应能力提高2.8倍（6.4 vs 17.8 PPL）。

Conclusion: APN作为Transformer FFN的替代架构，通过条件化、上下文特化的计算设计，在保持语言建模性能的同时，显著提升了模型的持续学习能力，为解决Transformer在持续学习中的干扰问题提供了有效方案。

Abstract: Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.
  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.
  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.

</details>


### [580] [Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures](https://arxiv.org/abs/2602.07070)
*Vladimer Khasia*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出HDPL算子，将Transformer中的密集线性变换分解为稀疏块对角局部处理路径和低秩VAE瓶颈全局上下文路径，在减少6.8%参数的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的密集线性变换效率低下且缺乏区分局部特征保留和全局上下文整合的结构归纳偏置，需要更高效的架构设计

Method: 引入混合双路径线性算子，将仿射变换分解为：1)稀疏块对角组件用于高秩局部处理，2)低秩VAE瓶颈用于全局上下文正则化。选择性地替换Query、Key、Value、Gate、Up等投影，保留Output、Down等聚合层

Result: 在FineWeb-Edu数据集上，HDPL架构优于标准Llama风格基线，在减少6.8%参数的同时降低了验证损失

Conclusion: HDPL在效率和表示能力间取得更好平衡，其概率潜在空间为推理时控制、持续适应、可解释性和跨模型同步提供了新的架构支持

Abstract: Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL

</details>


### [581] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出最优令牌基线(OTB)方法，通过基于梯度范数的逆加权来减少强化学习中的梯度方差，使用Logit-Gradient Proxy高效近似梯度范数，显著降低训练崩溃风险并减少65%的令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习在长视野任务中常因梯度方差爆炸而训练崩溃。传统基线方法存在优化困难、忽略序列异质性等问题，经典最优基线理论又忽略了令牌异质性且计算成本过高。

Method: 从第一性原理推导出最优令牌基线(OTB)，证明梯度更新应按照其累积梯度范数的倒数进行加权。提出Logit-Gradient Proxy，仅使用前向传播概率近似梯度范数，确保计算效率。

Result: 方法实现了训练稳定性，仅用N=4就能达到N=32大组大小的性能，在单轮和工具集成推理任务中减少了超过65%的令牌消耗。

Conclusion: OTB方法有效解决了LLM强化学习中的梯度方差问题，通过考虑令牌异质性的最优基线设计，显著提升了训练效率和稳定性。

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [582] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究发现单次运行评估智能体系统存在显著方差，2-3个百分点的改进可能只是评估噪声而非真实算法进步，建议采用多次运行、统计功效分析和pass@k等指标进行可靠评估。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估通常使用单次运行计算pass@1分数，但这种评估方法的可靠性尚未得到验证。研究者怀疑单次运行可能无法准确反映智能体的真实性能，导致报告的改进可能只是统计噪声而非真正的算法进步。

Method: 在SWE-Bench-Verified数据集上收集了60,000个智能体轨迹，涵盖三个模型和两种脚手架。通过分析这些轨迹的方差，研究单次运行评估的可靠性。进行token级别的分析，追踪轨迹分化的早期阶段。

Result: 发现单次运行pass@1估计存在显著方差：根据选择的具体运行，结果会变化2.2到6.0个百分点，即使在温度0的情况下标准差也超过1.5个百分点。轨迹在早期（前几个百分比的token）就会分化，小的差异会级联成不同的解决策略。

Conclusion: 智能体系统评估需要更严谨的方法：1) 每个任务进行多次独立运行；2) 使用统计功效分析确定检测预期效应大小所需的运行次数；3) 考虑pass@k和pass^k等指标（k>1）来更好地表征完整性能范围。这些实践对于区分真正的科学进步和统计噪声至关重要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [583] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种自适应检索增强架构，让LLM在推理过程中主动决定何时查询外部知识库。实验发现：静态检索不如CoT，但自适应检索中不使用检索的轨迹表现优于CoT，表明检索很少帮助推理，而主动不使用检索是模型性能良好的信号。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理任务中常因静态参数化知识而出现幻觉，在数学等专业领域表现不佳。本文探索通过将检索视为动态上下文学习来增强生成模型的基本原则。

Method: 提出自适应检索增强架构，让LLM智能体在推理过程中主动决定何时查询外部知识库。在GSM8K和MATH-500基准上比较自适应策略与标准CoT基线和静态检索方法。

Result: 1) 静态检索不如CoT；2) 自适应检索中：包含检索结果的轨迹表现略差于CoT，但不包含检索的轨迹表现优于CoT；3) 检索很少帮助推理（少数反例如使用有用定理）；4) 主动不使用检索是模型性能良好的指标；5) 模型根据问题难度调整检索频率。

Conclusion: 模型自我评估知识并选择性利用外部信息的能力是构建更稳健可靠生成模型的关键原则。检索决策作为元认知信号至关重要。

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [584] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

Relevance: 85.0

TL;DR: CoLoRA：一种利用任务相似性进行协作式低秩适应的参数高效微调方法，通过共享适配器和个性化适配器结合，缓解数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 解决基础模型微调中数据稀缺问题，利用多个下游用户之间的任务相似性，通过协作方式提升有效微调数据规模

Method: 提出CoLoRA（协作式低秩适应），包含共享适配器捕获跨任务相似性，以及个性化适配器针对用户特定任务；在异构线性回归上进行理论分析

Result: 理论分析提供真实参数恢复的可证明保证；自然语言实验显示，与相似任务共同训练时，个体性能显著提升

Conclusion: CoLoRA通过利用任务相似性进行协作式微调，有效缓解数据稀缺问题，提升基础模型在下游任务中的适应能力

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [585] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

Relevance: 85.0

TL;DR: SpecAttn：一种基于验证引导稀疏注意力的自推测解码方法，通过重用验证过程中计算的关键KV条目来提高长上下文LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理面临KV缓存内存需求增长的瓶颈，现有自推测解码方法依赖独立的KV选择算法，忽略了验证过程中已经计算了每个KV条目的关键性

Method: 提出SpecAttn方法，将验证过程中识别出的关键KV条目作为副产品，在起草后续token时仅加载这些关键条目，实现验证引导的稀疏注意力

Result: 相比传统自回归解码实现2.81倍吞吐量提升，相比最先进的基于稀疏性的自推测解码方法提升1.29倍

Conclusion: SpecAttn通过重用验证过程中的关键KV信息，既提高了起草token接受率，又降低了KV选择开销，有效解决了长上下文LLM推理的内存瓶颈问题

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [586] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了ArcMark，一种基于编码理论的多比特水印方法，首次推导了多比特水印的信息论容量，并展示了其在实际中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型水印方法主要从零比特水印扩展而来，每令牌仅编码单个比特。多比特水印的信息论容量（在不改变平均下一个令牌预测的情况下可插入和检测的最大比特数）一直未知，这限制了水印设计的理论基础。

Method: 1. 首次推导多比特水印的信息论容量特性；2. 基于编码理论原理设计ArcMark水印构造；3. 在特定假设下实现多比特水印信道的容量；4. 通过编码理论方法解决水印设计问题。

Result: ArcMark在实际应用中在每令牌比特率和检测准确率方面优于竞争的多比特水印方法。证明了语言模型水印本质上是一个信道编码问题。

Conclusion: 本文为水印设计提供了基于编码理论的原则性方法，填补了多比特水印信息论容量的理论空白，并展示了ArcMark在实际应用中的优越性能。

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [587] [tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models](https://arxiv.org/abs/2602.07263)
*Kevin Li,Dibyadeep Saha,Avni Kanodia,Fan Lai*

Main category: cs.LG

Relevance: 85.0

TL;DR: tLoRA：一个高效批量训练多个LoRA适配器的框架，通过融合共享基础模型的适配器为弹性共享超级模型，提升训练吞吐量1.2-1.8倍，作业完成时间2.3-5.4倍，GPU利用率37%


<details>
  <summary>Details</summary>
Motivation: 随着LoRA成为高效微调大语言模型的标准方法，共享集群中并发执行多个LoRA训练任务的需求日益增长。然而，异构LoRA适配器（不同秩、批次大小和资源分配）在训练时的协同定位面临同步停滞、通信开销和性能下降等挑战，需要专门的解决方案。

Method: tLoRA采用两层设计：1）内核层面：使用融合LoRA内核，自适应重构低秩计算块，调度秩感知的纳米批次以最大化计算与通信重叠；2）调度层面：在线、剩余容量感知的调度器自适应分组作业以最大化集体吞吐量。框架将共享同一基础模型的适配器融合为弹性共享超级模型。

Result: 基于真实集群轨迹的评估显示，tLoRA将训练吞吐量提升1.2-1.8倍，作业训练完成时间缩短2.3-5.4倍，GPU利用率提高37%。

Conclusion: tLoRA成功解决了异构LoRA适配器批量训练的效率问题，通过内核融合和智能调度实现了显著的性能提升，为共享集群中高效执行并发LoRA训练任务提供了有效解决方案。

Abstract: As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.

</details>


### [588] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

Relevance: 85.0

TL;DR: XShare：一种无需重新训练的MoE专家选择优化方法，通过批处理感知的贪婪算法动态选择专家，减少专家激活30%，降低GPU峰值负载3倍，在推测解码中提升吞吐量14%


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能高效扩展大语言模型，但在生产推理中，请求批处理和推测解码会显著增加专家激活，侵蚀效率优势。需要解决批处理感知的专家选择优化问题。

Method: 将批处理感知的专家选择建模为模块化优化问题，设计高效的贪婪算法适应不同部署场景。XShare方法无需重新训练，通过最大化选定专家的总门控分数动态适应每个批次。

Result: 标准批处理下减少专家激活30%；专家并行部署中降低GPU峰值负载3倍；推测解码中通过分层、相关性感知的专家选择实现14%吞吐量提升，即使批次请求来自异构数据集。

Conclusion: XShare有效解决了MoE在生产推理中的效率问题，通过动态批处理感知的专家选择优化，显著提升了MoE架构的实际部署效率。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [589] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

Relevance: 85.0

TL;DR: ShaPO是一个几何感知的偏好优化框架，通过选择性控制对齐关键参数子空间的几何结构来增强LLM安全对齐的鲁棒性，解决了传统数据中心方法无法处理的优化诱导脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全对齐在领域转移和噪声偏好监督下仍然脆弱。大多数现有鲁棒对齐方法关注对齐数据的不确定性，但忽视了基于偏好的目标函数中优化诱导的脆弱性。作者认为鲁棒性失效不能仅通过数据为中心的方法解决。

Method: 提出ShaPO框架，从优化几何视角重新审视LLM安全对齐的鲁棒性。通过选择性几何控制对齐关键参数子空间来强制执行最坏情况对齐目标，避免均匀几何约束导致的过正则化。在token级别稳定基于似然的代理优化，在奖励级别强制执行噪声监督下的奖励一致性优化。

Result: 在多样化的安全基准测试和噪声偏好设置中，ShaPO相比流行的偏好优化方法持续提升安全鲁棒性。此外，ShaPO能与数据鲁棒目标清晰组合，带来额外增益，实证支持了所提出的优化几何视角。

Conclusion: 从优化几何角度处理LLM安全对齐的鲁棒性是有效的，ShaPO框架通过选择性几何控制解决了传统数据中心方法的局限性，为增强安全对齐的鲁棒性提供了新方向。

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [590] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

Relevance: 85.0

TL;DR: NeVA：一种神经元级编辑框架，通过识别稀疏的价值相关神经元并进行推理时激活编辑，实现可控的价值对齐，减少价值泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的价值对齐方法存在有限的可控性问题：引导目标价值时往往会无意中激活其他非目标价值。作者引入"价值泄漏"概念来描述这一限制，并提出需要更可控的价值对齐机制。

Method: 提出NeVA框架：1）识别稀疏的价值相关神经元；2）在推理时进行激活编辑，无需参数更新或重新训练；3）基于Schwartz价值理论构建标准化泄漏度量。

Result: NeVA实现了更强的目标价值对齐，同时减少了对通用能力的性能下降。显著降低了平均泄漏率，残余效应主要局限于语义相关的价值类别。

Conclusion: NeVA为LLM价值对齐提供了更可控和可解释的机制，解决了现有方法的价值泄漏问题，实现了细粒度控制。

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [591] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Sketch&Walk Attention，一种无需训练的稀疏注意力方法，通过轻量级草图（Hadamard sketching）和确定性游走机制动态选择top-k注意力块，在20%注意力密度下保持接近无损精度，实现最高6倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长上下文LLM推理中（包括预填充和解码阶段）主导计算和内存成本，需要高效的稀疏注意力解决方案来降低开销。

Method: 使用Hadamard草图技术廉价近似注意力分数，通过游走机制跨层聚合估计值以捕捉超出token直接交互的注意力影响，基于累积分数动态选择top-k注意力块，配合定制稀疏注意力内核。

Result: 在多种模型和任务上，Sketch&Walk在20%注意力密度下保持接近无损精度，某些设置下甚至略微优于密集注意力，同时实现最高6倍推理加速。

Conclusion: Sketch&Walk Attention提供了一种统一适用于预填充和解码阶段的训练免费稀疏注意力方法，能显著提升长上下文LLM推理效率。

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [592] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文研究了基于符号的优化算法（如Lion、Muon）在训练大语言模型时优于AdamW的理论原因，提出了新的重尾梯度噪声模型来解释这一现象，并提供了SignSGD和Lion的收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 虽然基于符号的优化算法（如Lion、Muon）在训练大语言模型时表现出比AdamW更好的经验性能，但缺乏理论解释为什么这些方法优于方差自适应方法。论文旨在通过重尾梯度噪声的视角来弥合理论与实践的差距。

Method: 1. 提出新的广义重尾噪声条件，比标准有限方差假设更准确地捕捉LLM的行为
2. 在该噪声模型下，为广义光滑函数类建立SignSGD和Lion的尖锐收敛率
3. 将分析扩展到Muon和Muonlight，提供矩阵优化在重尾随机性下的首次严格分析
4. 通过LLM预训练实验验证理论见解

Result: 1. 建立了SignSGD和Lion在重尾噪声下的收敛率，匹配或超越了先前已知的最佳边界
2. 为Muon和Muonlight提供了首个重尾随机性下的严格分析
3. 实验验证了提出的噪声模型与实践相符，解释了基于符号优化器的经验优势

Conclusion: 基于符号的优化器天然适合处理重尾相关的噪声梯度，这为它们在训练大语言模型时优于方差自适应方法提供了强有力的理论依据。研究揭示了重尾梯度噪声是理解优化算法性能差异的关键因素。

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [593] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

Relevance: 85.0

TL;DR: MaCa提出了一种长度感知的Hessian矩阵构建方法，通过多尺度序列长度信息改进大语言模型的后训练量化性能


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法使用固定长度的随机序列进行校准，忽略了LLM输入长度可变的特性。输入长度直接影响激活分布和Hessian矩阵捕获的权重重要性，固定长度校准得到的Hessian估计可能无法准确反映不同输入场景下的权重重要性。

Method: MaCa方法包含两个关键部分：(1) 将多尺度序列长度信息融入Hessian估计；(2) 将每个序列作为独立样本进行正则化，从而获得更稳定、更有效的Hessian矩阵用于精确量化。

Result: 在Qwen3、Gemma3、LLaMA3等先进LLM上的实验表明，MaCa在低位量化下能持续提升精度，提供了一种轻量级增强，与现有PTQ框架兼容。

Conclusion: 这是第一个系统性地强调多尺度校准在LLM量化中作用的工作，MaCa通过长度感知的Hessian构建方法，有效解决了传统固定长度校准的局限性。

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [594] [ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations](https://arxiv.org/abs/2602.07479)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出ODELoRA方法，通过常微分方程(ODE)优化LoRA因子矩阵，模拟完整微调的梯度流，提供统一的ODE视角来理解和设计LoRA训练算法


<details>
  <summary>Details</summary>
Motivation: 传统LoRA训练方法将低秩因子矩阵分开优化，未能充分利用LoRA参数化的内在结构，导致理论和实证上的次优结果

Method: 提出基于ODE的连续时间优化动态，在平衡流形上模拟完整微调的梯度流，采用欧拉和龙格-库塔等时间离散化方案来跟踪轨迹

Result: 在强凸目标下证明线性收敛，在矩阵感知任务中验证收敛行为，在物理信息神经网络训练中展示优于基线的性能，特别是在训练稳定性方面

Conclusion: ODELoRA为LoRA训练提供了统一的ODE视角，实现了稳定的特征学习，在不同问题维度下都能有效训练深度神经网络

Abstract: Low-rank adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning method in deep transfer learning, due to its reduced number of trainable parameters and lower memory requirements enabled by Burer-Monteiro factorization on adaptation matrices. However, classical LoRA training methods treat the low-rank factor matrices individually and optimize them using standard gradient-based algorithms. Such decoupled optimization schemes are theoretically and empirically suboptimal, as they fail to fully exploit the intrinsic structure of the LoRA parameterization. In this work, we propose a novel continuous-time optimization dynamic for LoRA factor matrices in the form of an ordinary differential equation (ODE) that emulates the gradient flow of full fine-tuning on the balanced manifold. We term this approach ODELoRA. To faithfully track the trajectories of ODELoRA, we adopt well-established and theoretically grounded time-discretization schemes, including Euler and Runge--Kutta methods. Our framework provides a unified ODE-based perspective for understanding and designing LoRA training algorithms. We establish linear convergence of the proposed method under strongly convex objectives for certain discretization schemes under mild conditions, and further extend our analysis to the matrix sensing setting. Moreover, we show that ODELoRA achieves stable feature learning, a property that is crucial for training deep neural networks at different scales of problem dimensionality. Empirical results on matrix sensing tasks confirm the derived linear convergence behavior, and experiments on training physics-informed neural networks further demonstrate the superiority of ODELoRA over existing baselines, especially in the training stability.

</details>


### [595] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了首个能够定量预测大语言模型数据受限缩放定律指数的理论，通过分析语言的两个关键统计特性来预测神经缩放指数，无需自由参数或合成数据模型。


<details>
  <summary>Details</summary>
Motivation: 尽管实验神经缩放定律在很大程度上指导了大规模机器学习的实证进展，但现有理论无法定量预测任何现代LLM在任何自然语言数据集上的这些重要定律的指数。本研究旨在填补这一理论空白。

Method: 通过分析语言的两个关键统计特性：(1) 令牌对相关性随时间间隔的衰减，(2) 下一令牌条件熵随上下文长度的衰减，推导出预测数据受限神经缩放指数的简单公式。理论无需自由参数或合成数据模型。

Result: 理论预测与实验测量的神经缩放定律表现出显著匹配，通过在TinyStories和WikiText两个不同基准上从头训练GPT-2和LLaMA风格模型进行验证。

Conclusion: 该研究首次提供了能够定量预测大语言模型数据受限缩放定律指数的理论框架，揭示了语言统计特性与神经缩放指数之间的基本关系，为理解LLM缩放行为提供了理论基础。

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [596] [Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks](https://arxiv.org/abs/2602.07494)
*Shenxi Wu,Haosong Zhang,Xingjian Ma,Shirui Bian,Yichi Zhang,Xi Chen,Wei Lin*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了基于图的有效深度概念，用于统一分析多路径神经网络（如CNN、ResNet、Transformer）的深度缩放。在稳定初始化和最大更新准则下，证明了最优学习率随有效深度遵循-3/2幂律衰减，实现了跨深度和宽度的零样本学习率迁移。


<details>
  <summary>Details</summary>
Motivation: 现代深度架构训练成本高昂，需要跨宽度和深度的超参数迁移。虽然最大更新参数化（μP）解释了宽度缩放中的超参数迁移，但深度缩放对于包含并行路径和残差聚合的现代架构理解不足。需要统一分析各种非循环多路径神经网络的深度缩放规律。

Method: 引入基于图的有效深度概念，统一CNN、ResNet、Transformer等多路径架构。在稳定初始化和最大更新准则（最大化初始化时典型单步表示变化而不引起不稳定）下，理论推导最优学习率与有效深度的关系。通过实验验证理论预测。

Result: 理论证明最优学习率随有效深度遵循-3/2幂律衰减。实验验证了预测的斜率，实现了跨深度和宽度的可靠零样本学习率迁移，将深度缩放转化为可预测的超参数迁移问题。

Conclusion: 提出的有效深度概念和最大更新准则为多路径神经网络的深度缩放提供了统一理论框架，实现了跨架构的超参数迁移，显著减少了深度架构的训练调优成本。

Abstract: Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($μ$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.

</details>


### [597] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

Relevance: 85.0

TL;DR: MedVerse：基于Petri网理论的并行医疗推理框架，将医疗推理重构为可并行化的有向无环图过程，提升LLMs在复杂医疗推理任务中的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的顺序自回归解码机制将本应并行的临床推理（如鉴别诊断）强制转化为单一线性推理路径，限制了复杂医疗问题的推理效率和可靠性。

Method: 1) MedVerse Curator：自动化管道合成基于知识的医疗推理路径并转化为Petri网结构表示；2) 拓扑感知注意力机制：支持并行推理同时保持逻辑一致性；3) 定制化推理引擎：支持无额外开销的并行执行。

Result: MedVerse将通用LLMs性能提升高达8.9%；与专业医疗LLMs相比，在保持相当性能的同时，推理延迟降低1.3倍，生成吞吐量提升1.7倍。

Conclusion: MedVerse通过将医疗推理重构为并行化DAG过程，有效解决了LLMs在复杂医疗推理中的效率和可靠性限制，为医疗AI推理提供了新的框架。

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [598] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了Gaussian Match-and-Copy (GMC)基准，用于分离大语言模型中的检索和记忆行为，研究Transformer如何发展match-and-copy电路，并分析了梯度下降的隐式偏置导致硬匹配选择。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型中match-and-copy检索原语如何在自然数据上出现是困难的，因为检索和记忆行为相互纠缠。需要设计一个基准来分离这两种行为，专门研究长距离检索机制。

Method: 引入Gaussian Match-and-Copy (GMC)基准，通过纯二阶相关信号隔离长距离检索。在简化注意力设置下分析优化动态，研究梯度下降的隐式偏置如何驱动参数发散并朝向最大间隔分离器对齐。

Result: GMC基准保留了Transformer在实践中发展match-and-copy电路的关键定性特征，并能区分不同架构的检索能力。证明了在特定技术条件下，达到零经验损失的梯度下降轨迹会实现最大间隔对齐。

Conclusion: GMC基准有效分离了检索和记忆，揭示了Transformer中match-and-copy电路的发展机制。梯度下降的隐式偏置导致硬匹配选择，这有助于理解大语言模型检索行为的涌现。

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [599] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文将多指标基准测试形式化为社会选择问题，证明了在特定偏好结构（单峰、群可分、距离受限）下可以构建良好排序，解决了传统聚合方法的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现代基准测试（如HELM、MMLU）包含多个指标（准确性、鲁棒性、效率等），但将这些指标聚合成单一排名时，传统方法会出现不一致或不稳定的问题。作者旨在解决多指标基准测试中的聚合难题。

Method: 将基准测试形式化为社会选择问题：每个指标在数据集上诱导出模型偏好排序，基准算子聚合这些"投票"。研究三种偏好结构限制条件（单峰偏好、群可分偏好、距离受限偏好），证明在这些条件下可以构建良好排序。

Result: 理论证明在特定偏好结构条件下，基准算子可以构建一致且稳定的模型排名。实证分析显示现代基准套件（如HELM、MMLU）在不同基准问题上满足这些结构条件。

Conclusion: 通过识别偏好结构的充分条件，可以克服Arrow不可能定理的限制，实现有意义的多指标基准测试。这为基准测试设计提供了理论指导。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [600] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Rational Transductors架构，通过加权有限自动机(WFA)的矩阵值递归增强Transformer，解决Transformer在顺序逻辑和状态跟踪上的局限性，实现正则语言和NC¹问题的表达能力，同时保持并行计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer擅长语义建模，但在刚性顺序逻辑和状态跟踪方面表现不佳。理论研究表明自注意力机制受限于AC⁰或TC⁰复杂度类，难以在没有中间思维链的情况下实现稳健的长度泛化。需要一种既能保持Transformer并行效率，又能增强顺序推理能力的架构。

Method: 提出Rational Transductors双流架构：1) 使用加权有限自动机(WFA)的矩阵值递归增强Transformer；2) 通过Deep Rational Injection方案将有理状态信息注入注意力机制；3) 采用Random Rational Features作为通用基础进行初始化；4) 利用Differentiable Rational Feature机制弥补表示紧凑性差距。

Result: 理论证明：1) 严格扩展了Transformer的表达能力，能够捕获所有正则语言和NC¹完全问题；2) 解决了奇偶性和模计数等基本分离问题；3) 保持O(L + log T)并行时间复杂度。实证结果显示在标准Transformer失败的算法任务上实现了稳健的长度泛化。

Conclusion: Rational Transductors填补了"正则差距"，在保持Transformer并行效率的同时，增强了顺序推理能力，解决了Transformer在算法任务上的长度泛化问题，为构建更强大的序列模型提供了理论基础和架构方案。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [601] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

Relevance: 85.0

TL;DR: SERE是一种基于相似性的专家重路由方法，用于提升MoE模型批量解码效率，通过动态减少活跃专家数量实现2倍加速，质量损失最小。


<details>
  <summary>Details</summary>
Motivation: MoE架构在批量推理时需要激活过多专家，导致内存受限的解码阶段变慢，存在批量解码与专家稀疏性之间的根本矛盾。

Method: SERE通过相似性分析动态重路由token，将次要专家的token重定向到最相似的主要专家，同时识别并保留关键专家，避免静态专家剪枝或合并。

Result: 在各种复杂推理基准测试中，SERE实现了高达2.0倍的加速，质量损失最小，提供了实用的vLLM单行代码集成方案。

Conclusion: SERE为大规模MoE部署提供了成本高效、延迟敏感的实用解决方案，通过动态专家跳过机制解决了批量解码效率问题。

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [602] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该研究发现，在具有密集奖励信号的验证执行任务中，搜索策略（如Best-of-N采样）优于测试时训练（TTT）。通过引入基于惊奇度的选择策略，可以显著提升任务成功率，达到与oracle相当的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在具有确定性评估器和密集连续奖励信号的验证执行任务中，计算最优的测试时策略。传统测试时训练通过梯度更新适应模型，但作者质疑这是否是最佳策略，特别是在GPU内核优化等任务中。

Method: 使用KernelBench作为测试平台，采用120B参数的GPT-OSS-120B模型（带LoRA适配）。比较了测试时训练（1-5梯度步）与搜索策略（Best-of-N采样）。提出了基于惊奇度的选择策略：选择最高惊奇度（最低置信度）的正确样本，以及扩展到top-3选择。

Result: Best-of-N采样（K=64）在完整KernelBench L1评估集上达到90%任务成功率（18/20任务），而TTT的最佳检查点仅达到30.6%。基于惊奇度的选择策略：最高惊奇度选择达到80%成功率，比最自信选择（50%）提升30%。扩展到top-3选择可匹配oracle性能（100%）。

Conclusion: 对于密集奖励的验证执行任务，计算资源应分配给样本多样性和智能选择，而非梯度适应。基于惊奇度的选择原则可能推广到其他执行任务领域，其中最优解位于分布尾部。

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [603] [Spectral Gating Networks](https://arxiv.org/abs/2602.07679)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Yongsen Zheng,Kwok-Yan Lam,Liang Lin,Keze Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: SGN（Spectral Gating Networks）是一种在固定参数和训练预算下，通过谱重参数化增强MLP/FFN层频谱容量的方法，使用可学习的随机傅里叶特征和门控机制，在保持稳定性的同时提升表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统前馈网络中引入丰富频率表达能力与保持稳定性、可扩展性之间的张力问题。特别是基于样条的KAN参数化方法中，网格细化会导致参数增长和优化脆弱性，需要一种稳定保持的方式来增强现有MLP/FFN层的频谱容量。

Method: 提出SGN（Spectral Gating Networks），这是一种即插即用的谱重参数化方法。它在标准激活路径基础上增加紧凑的谱路径和可学习门控，模型可以从稳定基础行为开始，在训练过程中逐步分配容量给频谱特征。谱路径使用可训练的随机傅里叶特征（学习频率和相位）实现，替代基于网格的样条方法，消除分辨率依赖性。采用混合GELU-傅里叶公式进一步改善优化鲁棒性并增强高频保真度。

Result: 在视觉、NLP、音频和PDE基准测试中，SGN在可比较的计算预算下持续改善准确率-效率权衡，在CIFAR-10上达到93.15%准确率，比基于样条的KAN变体推理速度快达11.7倍。

Conclusion: SGN提供了一种稳定且可扩展的方法来增强前馈网络的频谱表达能力，通过可学习的傅里叶特征和门控机制，在保持优化稳定性的同时显著提升模型性能。

Abstract: Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.

</details>


### [604] [Towards Robust Scaling Laws for Optimizers](https://arxiv.org/abs/2602.07712)
*Alexandra Volkova,Mher Safaryan,Christoph H. Lampert,Dan Alistarh*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文研究了不同优化器对LLM预训练缩放定律的影响，提出了共享幂律指数和优化器特定缩放因子的更稳健缩放定律，并提供了凸二次目标的理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缩放定律研究通常固定使用AdamW优化器，而新一代优化器（如Muon、Shampoo、SOAP）虽然承诺更快更稳定的收敛，但其与模型和数据缩放的关系尚未被充分理解。需要研究不同优化器下的缩放定律，以更好地比较优化器性能。

Method: 1) 实证研究不同优化器下的缩放行为，发现为每个优化器单独建立Chinchilla式缩放定律存在病态问题；2) 提出共享幂律指数和优化器特定缩放因子的更稳健缩放定律；3) 对凸二次目标进行理论分析，展示Chinchilla式缩放定律如何从损失分解为不可约误差、近似误差和优化误差中自然产生。

Result: 1) 发现为不同优化器单独建立缩放定律会导致高度相关的参数；2) 提出的共享指数缩放定律能够更稳健地比较不同优化器的性能；3) 理论分析表明缩放定律可以从损失分解的角度自然推导出来。

Conclusion: 优化器选择对LLM预训练缩放定律有重要影响，提出的共享指数缩放定律为比较不同优化器提供了更稳健的框架，理论分析为缩放定律提供了数学基础。

Abstract: The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.

</details>


### [605] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

Relevance: 85.0

TL;DR: ParisKV：基于碰撞候选选择和量化内积重排的GPU原生KV缓存检索框架，支持百万token上下文，在长上下文解码效率和内存使用上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存检索方法在处理长上下文LLM推理时面临分布漂移和高延迟问题，特别是在大规模场景下。需要一种能够处理百万token上下文、具有分布漂移鲁棒性且GPU原生的高效检索框架。

Method: 提出ParisKV框架，采用碰撞候选选择机制进行初步筛选，然后使用量化内积重排估计器进行精确排序。支持通过统一虚拟寻址实现CPU卸载的KV缓存，实现按需top-k获取。该框架专门针对GPU优化，支持百万token级上下文。

Result: 在长输入和长生成基准测试中，ParisKV达到或超过全注意力机制的质量。在长上下文解码效率上实现SOTA：在batch size为1时达到或超过全注意力速度；在全注意力可运行范围内吞吐量提升2.8倍；支持百万token上下文（全注意力内存不足）。相比MagicPIG和PQCache，在百万token规模下解码延迟分别降低17倍和44倍。

Conclusion: ParisKV是一个高效、分布漂移鲁棒的KV缓存检索框架，能够显著提升长上下文LLM推理的效率，特别是在大规模场景下，为长上下文LLM应用提供了实用的解决方案。

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [606] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究发现，在LLM的强化学习阶段，简单的SGD优化器比广泛使用的AdamW表现更好且更节省内存，仅更新不到0.02%的参数，比AdamW少1000倍以上。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练中，强化学习阶段（特别是RLVR）的优化实践大多沿用预训练和监督微调阶段的方法，使用AdamW优化器。但RL与这些阶段存在根本差异，且AdamW内存开销大。本文旨在探索RL阶段是否真的需要AdamW的复杂特性。

Method: 通过分析AdamW中的动量项和自适应学习率在RL中的作用，提出假设：RL对Adam风格的每参数自适应学习率和动量依赖较少。通过实验验证，比较SGD和AdamW在LLM RL中的性能表现。

Result: 实验证明，在LLM的RL阶段，内存效率更高的SGD匹配甚至优于AdamW。SGD仅更新不到0.02%的模型参数（无需稀疏正则化），比AdamW少1000倍以上。

Conclusion: RL阶段的优化动态与监督学习阶段不同，RL可以比之前认为的更参数高效。这为LLM RL优化提供了新见解，并展示了更高效的训练可能性。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [607] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出LCB方法，通过自适应拒绝采样实现推理时奖励引导对齐，减少对预训练模型的查询次数


<details>
  <summary>Details</summary>
Motivation: 现有推理时奖励引导对齐方法要么依赖特定架构适配，要么计算成本高昂，需要更高效通用的解决方案

Method: 提出可学习切尔诺夫基线(LCBs)，通过自适应拒绝采样近似KL正则化奖励对齐的指数倾斜核，仅需黑盒采样访问预训练模型

Result: 在连续和离散扩散设置中，LCB采样与理想拒绝采样匹配，同时显著减少对预训练模型的查询次数

Conclusion: LCB提供了一种高效、通用的推理时对齐方法，在保持对齐质量的同时实现计算效率

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [608] [Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization](https://arxiv.org/abs/2602.07764)
*Tanmay Ambadkar,Sourav Panda,Shreyash Kale,Jonathan Dodge,Abhinav Verma*

Main category: cs.LG

Relevance: 85.0

TL;DR: D³PO：一种基于PPO的多目标强化学习框架，通过分解优化流程和多样性正则化，解决现有方法在恢复完整帕累托前沿时的失败问题，使用单个可部署策略实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有单偏好条件策略方法在实践中脆弱，经常无法恢复完整的帕累托前沿。作者发现这源于两个结构性问题：过早标量化导致的破坏性梯度干扰，以及偏好空间中的表示崩溃。

Method: D³PO基于PPO框架，通过分解优化流程保留每个目标的学习信号，仅在稳定后集成偏好，实现可靠的信用分配。同时使用缩放多样性正则化器强制策略行为对偏好变化的敏感性，防止崩溃。

Result: 在标准MORL基准测试中，包括高维和多目标控制任务，D³PO始终比先前的单策略和多策略方法发现更广泛和更高质量的帕累托前沿，在超体积和期望效用方面达到或超过最先进水平。

Conclusion: D³PO通过解决现有方法的两个核心结构问题，显著提高了单偏好条件策略在多目标强化学习中的性能，为可扩展的多目标优化提供了更可靠的解决方案。

Abstract: Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.

</details>


### [609] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

Relevance: 85.0

TL;DR: Faro是一个公平感知的奖励优化框架，通过在奖励模型训练中施加人口统计公平性约束（人口统计均等、均等机会或反事实公平），来解决LLM对齐中的系统性不公平问题。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据中的人口统计偏差会通过奖励模型传播到对齐的LLM中，导致系统性不公平。现有方法无法同时保证奖励模型的序数性（正确排序）、基数性（校准）和公平性。

Method: Faro是一个处理中的框架，在奖励模型训练中施加人口统计公平性约束（人口统计均等、均等机会或反事实公平）。提供了首个LLM对齐中奖励级公平性的理论分析，包括：可证明的公平性证书、KL正则化微调引起的准确性-公平性权衡的形式化表征、以及非空帕累托前沿的存在性证明。

Result: 在多个LLM和基准测试中，Faro显著减少了偏见和有害生成，同时保持或提高了模型质量。与预处理和后处理方法不同，Faro确保奖励模型同时具备序数性、基数性和公平性。

Conclusion: Faro通过理论保证的公平性约束训练奖励模型，有效解决了LLM对齐中的不公平问题，为构建可信赖的AI系统提供了重要方法。

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [610] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

Relevance: 85.0

TL;DR: 通过简单的辅助损失微调LLM，训练16个残差流维度作为惰性解释性标志，模型围绕这些标志重组并依赖它们进行生成，从而创建可解释的控制开关


<details>
  <summary>Details</summary>
Motivation: 传统方法控制LLM内部概念表示需要复杂的特征几何识别和干预技术，本文旨在寻找更直接、暴力的方法来安装可解释、可控制的特征到模型激活中

Method: 使用简单的辅助损失微调LLM，专门训练16个残差流维度（共3072维）作为惰性解释性标志，这些标志指示生成所需的概念，让模型在生成任务中学习依赖这些标志

Result: 这些惰性标志成为真正的内部特征：可解释的控制开关，允许在推理时引导生成。模型围绕这些标志重组，消除其他地方的冗余编码，侵蚀自身的替代表示

Conclusion: 当特征可靠地在固定位置提供时，梯度下降会逐渐消除其他地方的冗余编码，模型会侵蚀自身的替代表示。模型的效率压力是可利用的杠杆，可诱导出可解释、可控制的表示

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [611] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

Relevance: 85.0

TL;DR: rePIRL：一个受逆强化学习启发的框架，用于学习有效的过程奖励模型，对专家策略的假设要求最低，可统一在线和离线PRM学习方法


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型学习方法要么依赖对专家策略的强假设（如需要其奖励函数），要么存在内在局限性（如熵崩溃），导致PRM效果弱或泛化能力有限

Method: 设计双重学习过程，交替更新策略和PRM，采用定制化技术解决将传统逆强化学习扩展到LLM的挑战，理论证明可统一在线和离线PRM学习方法

Result: 在标准化数学和编程推理数据集上的实证评估显示rePIRL优于现有方法，训练出的PRM可用于测试时训练、测试时缩放和为困难问题训练提供早期信号

Conclusion: rePIRL能以最小假设学习有效的PRM，为LLM推理中的过程奖励学习提供了更通用和强大的框架

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [612] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

Relevance: 85.0

TL;DR: MARTI-MARS2：一个多智能体强化训练与推理框架，通过将多智能体协作探索过程建模为动态可学习环境，整合策略学习与多智能体树搜索，实现从同质多角色训练到异质多智能体训练的演进，突破单智能体能力限制。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在代码生成等复杂任务中存在性能瓶颈，多智能体协作有望超越这些限制。现有框架通常依赖基于提示的测试时交互或同质参数训练的多角色配置，限制了错误纠正能力和策略多样性。

Method: 提出MARTI-MARS2框架：1）将多智能体协作探索过程建模为动态可学习环境；2）整合策略学习与多智能体树搜索；3）实现从同质多角色训练到异质多智能体训练的演进；4）引入高效推理策略MARTI-MARS2-T+以充分利用测试时多智能体协作的扩展潜力。

Result: 在两个32B模型协作下，在代码生成基准测试中达到77.7%的准确率，优于GPT-5.1等强基线。揭示了新的扩展定律：从单智能体到同质多角色再到异质多智能体范式逐步产生更高的RL性能上限、鲁棒的TTS能力和更大的策略多样性。

Conclusion: MARTI-MARS2通过多智能体强化学习有效突破了单智能体能力限制，证明了策略多样性对于通过多智能体强化学习扩展智能至关重要。框架为复杂推理任务提供了新的解决方案。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [613] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出NC-LQL方法，通过朗之万动力学和多尺度噪声扰动实现软策略采样，在MuJoCo基准测试中达到与扩散方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 软策略在强化学习中通过玻尔兹曼分布平衡探索与利用，但现有方法存在局限性：参数化策略表达能力有限，扩散策略的似然难以估计，阻碍可靠熵估计。需要一种能直接实现软策略采样的方法。

Method: 1. 提出朗之万Q学习(LQL)：通过Q函数动作梯度的朗之万动力学直接采样玻尔兹曼分布，无需显式参数化策略。2. 提出噪声条件朗之万Q学习(NC-LQL)：引入多尺度噪声扰动到值函数，学习噪声条件Q函数，创建渐进平滑的值函数景观，实现从全局探索到精确模式细化的采样过程。

Result: 在OpenAI Gym MuJoCo基准测试中，NC-LQL实现了与最先进的扩散方法竞争的性能，为在线强化学习提供了一个简单而强大的解决方案。

Conclusion: NC-LQL通过朗之万动力学和多尺度噪声扰动有效解决了软策略采样的挑战，避免了参数化策略的表达限制和扩散策略的似然估计问题，在连续控制任务中表现出色。

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [614] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

Relevance: 85.0

TL;DR: OGPSA是一种轻量级方法，通过正交梯度投影解决LLM安全对齐中的遗忘问题，在保持通用能力的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: LLM安全对齐通常会产生"对齐税"——安全训练会降低模型的通用能力（如推理和编码）。作者认为这主要源于顺序对齐中的持续学习式遗忘，分布偏移和冲突目标导致安全更新覆盖了预训练能力。

Method: 将安全对齐视为持续学习问题，提出OGPSA方法：从少量参考集的梯度中估计低秩能力子空间，将安全梯度投影到其正交补空间后再更新，从而最小化对先前知识的扰动。

Result: 在SFT、DPO和顺序SFT→DPO设置中，OGPSA持续改进安全-效用帕累托前沿。例如在Qwen2.5-7B-Instruct上，SimpleQA从0.53%提升到3.03%，IFEval从51.94%提升到63.96%。

Conclusion: OGPSA有效缓解了安全对齐中的遗忘问题，在保持安全性的同时恢复了通用能力，是一种即插即用的轻量级解决方案。

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [615] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出AceGRPO框架，通过演化数据缓冲区和自适应采样解决自主机器学习工程中LLM智能体的行为停滞问题，实现持续迭代优化


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的MLE智能体存在行为停滞问题（参数冻结），而传统强化学习在MLE中面临执行延迟高和数据选择效率低的挑战

Method: AceGRPO包含两个核心组件：1) 演化数据缓冲区，将执行轨迹重新利用为可重用训练任务；2) 基于可学习性潜力函数的自适应采样，动态优先处理智能体学习边界上的任务

Result: 训练的Ace-30B模型在MLE-Bench-Lite上达到100%有效提交率，接近前沿专有模型性能，优于更大的开源基线（如DeepSeek-V3.2）

Conclusion: AceGRPO框架有效解决了自主机器学习工程中LLM智能体的行为停滞问题，展示了持续迭代优化的强大能力

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [616] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出度量-拓扑分解（MTF）作为统一的几何原理：智能不是固定几何中的导航，而是重塑表示几何使期望行为成为稳定吸引子的能力。学习对应度量收缩，任务身份和环境变化编码在拓扑中并存储在记忆里。MTF通过分解稳定拓扑和可塑性度量变形，实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习将智能等同于优化：在固定表示几何中搜索解。这在静态环境下有效，但在分布偏移、任务置换和持续学习等动态场景中会失效，即使轻微的拓扑变化也会使学习到的解失效并触发灾难性遗忘。需要新的几何框架来理解智能如何适应变化的环境。

Method: 提出度量-拓扑分解（MTF）原理，将表示几何分解为稳定的拓扑结构和可塑性的度量变形。基于此引入拓扑Urysohn机（TUM），通过记忆摊销度量推断（MAMI）实现：谱任务签名索引摊销的度量变换，使单个学习到的几何可在置换、反射或奇偶性改变的环境中重用。

Result: MTF框架解释了任务重排序的鲁棒性、对灾难性遗忘的抵抗能力，以及跨越传统持续学习方法（如EWC）无法处理的变换的泛化能力。实现了通过几何切换而非重新优化的快速适应。

Conclusion: 智能的核心不是优化，而是几何重构能力。MTF为理解持续学习、分布鲁棒性和适应性智能提供了统一的几何框架，通过分解稳定拓扑和可塑性度量变形解决了稳定性-可塑性权衡问题。

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [617] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文从理论上研究了在可验证奖励的强化学习（RLVR）下，自回归模型中组合问题的可学习性，提出了任务优势比的概念来刻画哪些任务可以从结果级反馈中学习。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在组合推理方面取得了经验成功，但尚不清楚哪些组合问题仅通过结果级反馈就能学习。本文旨在从理论上理解RLVR在组合问题中的可学习性条件。

Method: 理论分析自回归模型在RLVR训练下的组合问题可学习性，提出任务优势比这一概念作为关键指标，分析不同问题结构中优势的存在条件。

Result: 发现当正确中间步骤提供明显优势时，组合问题可以通过RLVR高效学习；当结构优势不存在时，RLVR可能收敛到次优组合；基础模型的质量在某些情况下决定优势是否存在。

Conclusion: 任务优势比是理解RLVR成功与否的关键理论指标，为RLVR在组合问题中的应用提供了原则性理论指导。

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [618] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种基于互信息的预算约束下LLM集成选择方法，通过贪婪算法选择模型，在三个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM集成通常用于提高可靠性和鲁棒性，但实践中模型之间存在强相关性。核心问题是：在预算约束下如何选择模型形成最优集成？

Method: 1. 将预算约束下的集成选择问题形式化为最大化真实标签与所选模型预测之间的互信息
2. 使用高斯copula建模模型间的相关误差，分析性能饱和的信息论原因
3. 提出贪婪互信息选择算法，直接从数据估计所需信息项，在查询预算下迭代构建集成

Result: 在MEDMCQA、MMLU和IMDB电影评论三个数据集上测试，该方法在相同查询预算下始终优于强基线方法。

Conclusion: 基于互信息的贪婪选择算法能有效解决预算约束下的LLM集成选择问题，在多个任务上展现出一致优势。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [619] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

Relevance: 85.0

TL;DR: TSR-Adam是一种针对通信受限分布式训练的低秩优化器，通过同步紧凑的核心矩阵将每步通信负载从O(mn)降低到O(r²)，显著减少梯度同步的通信开销。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，数据并行分布式训练中的梯度同步成为带宽瓶颈。现有的基于投影的低秩优化器主要为内存效率设计，但在通信受限训练中仍不理想：单边同步仍需传输O(rn)对象，刷新步骤可能主导峰值通信字节。

Method: 提出TSR-Adam，为Adam族更新引入双边低秩通信，通过同步紧凑核心U⊤GV∈ℝ^{r×r}，将主导的每步负载从O(mn)减少到O(r²)，同时将矩状态保持在低维核心中。采用随机SVD刷新避免全梯度同步，并将低秩通信扩展到嵌入梯度，使用嵌入特定秩和刷新计划。

Result: 在从60M到1B模型规模的预训练中，TSR-Adam将平均每步通信字节减少13倍；在GLUE微调中减少通信25倍，同时保持可比性能。提供了理论平稳性分析。

Conclusion: TSR-Adam有效解决了大规模分布式训练中的通信瓶颈问题，通过低秩通信显著减少通信开销，同时保持模型性能，为通信受限环境提供了实用的优化解决方案。

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [620] [The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications](https://arxiv.org/abs/2602.08019)
*Dong Pan,Bingtao Li,Yongsheng Zheng,Jiren Ma,Victor Fei*

Main category: cs.LG

Relevance: 85.0

TL;DR: 这篇论文是关于稀疏混合专家(MoE)架构的全面综述，涵盖了MoE的基本原理、核心组件、去中心化范式、垂直领域应用以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: MoE作为大语言模型的重要分支，通过稀疏条件计算机制显著提高计算效率，为模型扩展提供了有前景的路径。尽管MoE模型在各个领域越来越受欢迎，但缺乏对其最新进展的系统性探索，现有综述存在覆盖不足或关键领域探索不够深入的问题。

Method: 这篇综述采用系统性调查方法：首先分析MoE的基础原理，深入探讨其核心组件（路由网络和专家网络）；然后扩展到去中心化范式；接着探索垂直领域应用；最后识别关键挑战和未来研究方向。

Result: 作者声称这是目前MoE领域最全面的综述，旨在为研究者和实践者提供有价值的资源，帮助他们了解该领域的最新进展。

Conclusion: MoE架构通过稀疏条件计算显著提高了计算效率，为模型扩展提供了有前景的路径。去中心化范式能够释放去中心化基础设施的巨大潜力，使更广泛的社区能够参与MoE开发，并提供更大的可扩展性和成本效益。

Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>


### [621] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出ISO框架，通过预测战略上下文来优化LLM智能体在对抗性游戏中的长期表现，结合战略奖励模型和乐观学习规则，在德州扑克和宝可梦游戏中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体训练通常关注短期胜率等指标，但在长期对抗性游戏中，战略外部性会随时间演变，导致短视优化和基于变异的遗憾分析失效，需要能够预测战略上下文并优化长期回报的方法。

Method: 提出隐式战略优化(ISO)框架：1) 战略奖励模型(SRM)估计行动的长期战略价值；2) iso-grpo规则，基于上下文条件进行乐观学习；3) 智能体预测当前战略上下文并在线更新策略。

Result: 理论证明：当预测误差有界时，获得与静态游戏相当的次线性上下文遗憾和均衡收敛保证。实验：在6人无限注德州扑克和竞争性宝可梦游戏中，长期回报持续优于强LLM和RL基线，在受控预测噪声下表现稳健。

Conclusion: ISO框架通过预测战略上下文有效解决了长期对抗性游戏中的战略外部性问题，为LLM智能体在动态战略环境中的学习提供了理论保证和实用方法。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [622] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

Relevance: 85.0

TL;DR: SiameseNorm是一种双流Transformer架构，通过耦合Pre-Norm-like和Post-Norm-like流来结合Pre-Norm的优化稳定性和Post-Norm的表达能力，解决了传统单流设计中稳定性与性能的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer主要采用Pre-Norm范式以获得优化稳定性，但牺牲了不稳定的Post-Norm架构的优越潜力。先前尝试结合两者优势通常导致稳定性与性能的权衡，作者认为这是由于单流设计中的结构不兼容性：任何Post-Norm操作都会阻碍Pre-Norm保持的干净恒等梯度。

Method: 提出SiameseNorm双流架构，耦合具有共享参数的Pre-Norm-like和Post-Norm-like流。这种设计解耦了两个流的优化动态，保留Pre-Norm和Post-Norm的各自特性，使所有残差块都能接收来自两种范式的组合梯度，其中一个流确保稳定性，另一个增强表达能力。

Result: 在13亿参数模型上的广泛预训练实验表明，SiameseNorm表现出卓越的优化鲁棒性，并持续优于强基线模型。

Conclusion: SiameseNorm通过双流设计从根本上调和了Pre-Norm和Post-Norm范式，在保持优化稳定性的同时提升了模型表达能力，为Transformer架构设计提供了新思路。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [623] [FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff](https://arxiv.org/abs/2602.08040)
*Isaac Han,Sangyeon Park,Seungwon Oh,Donghu Kim,Hojoon Lee,Kyung-Joong Kim*

Main category: cs.LG

Relevance: 85.0

TL;DR: FIRE：一种平衡稳定性与可塑性的权重重初始化方法，通过约束优化最小化与历史权重的距离同时保持权重各向同性，在持续学习、语言建模和强化学习中均优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 在非平稳数据上训练的深度神经网络需要在稳定性（保留先验知识）和可塑性（适应新任务）之间取得平衡。标准的权重重初始化方法难以调优：保守的重初始化无法恢复可塑性，而激进的重初始化会抹去有用知识。

Method: FIRE通过两个指标量化稳定性与可塑性：平方Frobenius误差（SFE）衡量与历史权重的距离（稳定性），偏离各向同性（DfI）反映权重各向同性程度（可塑性）。通过约束优化问题求解重初始化点，最小化SFE同时使DfI为零，使用Newton-Schulz迭代高效近似求解。

Result: 在持续视觉学习（CIFAR-10 + ResNet-18）、语言建模（OpenWebText + GPT-0.1B）和强化学习（HumanoidBench + SAC，Atari + DQN）三个领域进行评估，FIRE始终优于无干预的朴素训练和标准重初始化方法。

Conclusion: FIRE提供了一种原则性的权重重初始化方法，能有效平衡稳定性与可塑性的权衡，在多个深度学习领域均表现出色。

Abstract: Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>


### [624] [Reliable and Responsible Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2602.08145)
*Xinyu Yang,Junlin Han,Rishi Bommasani,Jinqi Luo,Wenjie Qu,Wangchunshu Zhou,Adel Bibi,Xiyao Wang,Jaehong Yoon,Elias Stengel-Eskin,Shengbang Tong,Lingfeng Shen,Rafael Rafailov,Runjia Li,Zhaoyang Wang,Yiyang Zhou,Chenhang Cui,Yu Wang,Wenhao Zheng,Huichi Zhou,Jindong Gu,Zhaorun Chen,Peng Xia,Tony Lee,Thomas Zollo,Vikash Sehwag,Jixuan Leng,Jiuhai Chen,Yuxin Wen,Huan Zhang,Zhun Deng,Linjun Zhang,Pavel Izmailov,Pang Wei Koh,Yulia Tsvetkov,Andrew Wilson,Jiaheng Zhang,James Zou,Cihang Xie,Hao Wang,Philip Torr,Julian McAuley,David Alvarez-Melis,Florian Tramèr,Kaidi Xu,Suman Jana,Chris Callison-Burch,Rene Vidal,Filippos Kokkinos,Mohit Bansal,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文是一篇关于基础模型可靠性与负责任开发的综述，涵盖偏见与公平、安全与隐私、不确定性、可解释性、分布偏移等关键问题，以及幻觉、对齐、AIGC检测等方法和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（LLMs、MLLMs、图像生成模型、视频生成模型）在现实世界中的广泛应用，确保其可靠性和负责任性变得至关重要。论文旨在促进基础模型在伦理、可信、可靠和社会责任方面的发展。

Method: 这是一篇系统性综述论文，采用文献调研方法，对基础模型的可靠性问题进行全面梳理，包括：1）偏见与公平；2）安全与隐私；3）不确定性；4）可解释性；5）分布偏移；6）模型局限性（如幻觉）；7）对齐方法；8）AIGC检测。论文还探讨了这些领域之间的交叉关系。

Result: 论文总结了当前基础模型可靠性研究的最新进展，为每个关键领域提供了现状分析和具体的研究方向建议，并指出了领域间的相互联系和共同挑战。

Conclusion: 该综述旨在推动基础模型向更强大、更伦理、更可信、更可靠和更具社会责任的方向发展，为学术界、工业界和政府提供全面的研究路线图。

Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.

</details>


### [625] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文发现语言模型内部存在简单的几何结构表示正确性，仅需3-8维即可分离正确与错误陈述，线性分类器效果最佳，可通过激活引导因果验证


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否"知道"自己的陈述是错误的，探索正确性在模型内部如何表示，以及这种表示是否与输出表达一致

Method: 分析9个不同架构模型，研究正确性表示的几何结构，使用线性/非线性分类器对比，通过激活引导进行因果验证，比较内部探针与基于输出的方法

Result: 正确性信号仅需3-8维，性能随维度增加而下降，线性分类器最优；质心距离与训练探针性能匹配（0.90 AUC）；激活引导可改变10.9%错误率；内部探针AUC 0.80-0.97，输出方法仅0.44-0.64

Conclusion: 语言模型内部存在正确性信号但不在输出中表达，正确性分离是均值偏移的几何问题而非学习问题，可通过少量标注实现检测

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [626] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Spherical Steering方法，通过激活旋转而非加法实现推理时控制，保持表示完整性，显著提升多项选择任务性能同时保持开放生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有推理时控制方法（如激活加法）会改变隐藏表示的幅度，可能导致表示崩溃和开放生成能力下降。需要一种既能精确控制又不破坏表示完整性的方法。

Method: 提出Spherical Steering训练自由原语：1）通过激活旋转而非加法，沿测地线向目标方向旋转；2）引入置信门机制，基于输入不确定性动态调节控制强度。

Result: 在多项选择基准测试中显著优于基于加法的方法（TruthfulQA、COPA、Storycloze上提升+10%），同时保持模型的开放生成质量。

Conclusion: 保持几何一致性的规范保持旋转是精确推理时控制的鲁棒有效原语，解决了控制与表示完整性之间的权衡。

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [627] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

Relevance: 85.0

TL;DR: DiCode框架利用基础模型生成可执行环境代码，为开放世界中的智能体学习提供渐进式课程，在Craftax基准上实现了16%的性能提升


<details>
  <summary>Details</summary>
Motivation: 开放世界学习面临巨大组合空间挑战，现有方法难以生成持续可学习的环境序列。需要一种机制来构建中间环境，弥合能力差距

Method: DiCode框架：基础模型通过"梦想"生成代码级环境变体，为智能体学习提供渐进式脚手架。在Craftax基准上实例化，通过代码级环境设计控制课程

Result: 在Craftax基准上实现16%平均回报提升，在后期战斗任务中取得非零成功率（先前方法完全失败），证明代码级环境设计能有效构建中间环境

Conclusion: 代码级环境设计为课程控制提供了实用机制，能够构建弥合开放世界能力差距的中间环境，促进长时程技能获取

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [628] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种基于分析成本模型的粗粒度分区方法，用于在边缘设备上优化推测解码的执行，利用异构硬件资源实现高达1.68倍的加速。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署LLM面临严重的延迟约束，特别是在实时应用中。推测解码虽然能缓解顺序生成的低效问题，但在边缘部署时面临两个主要挑战：1) 如何在不牺牲性能或可编程性的情况下将SD集成到基于编译器的工作流中；2) 如何通过精心设计的分区策略利用现代SoC的异构计算资源。

Method: 使用分析成本模型探索异构硬件配置，指导LLM子图的粗粒度分区，特别针对边缘设备典型的短输入序列长度。该模型预测推测采样和异构执行何时联合有益，并在配备六核Cortex-A CPU和Mali GPU的边缘设备上进行验证。

Result: 在翻译任务上实现了高达1.68倍的加速，与分析预期密切匹配，验证了成本模型的有效性。

Conclusion: 通过分析成本模型指导的粗粒度分区策略，可以有效解决边缘设备上推测解码的集成和异构资源利用问题，显著提升LLM在边缘设备上的推理性能。

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [629] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

Relevance: 85.0

TL;DR: BAGEL是一个轻量级、模块化的恶意提示检测框架，通过集成多个专门针对不同攻击数据集的微调模型，使用随机森林路由器和随机选择进行预测聚合，实现高效、可适应更新的恶意提示防御。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防御系统面临根本性限制：黑箱审核API透明度有限且难以适应新威胁，白箱方法使用大型LLM法官计算成本过高且需要昂贵重训练。现有系统迫使设计者在性能、效率和适应性之间做出妥协。

Method: BAGEL采用引导聚合和专家混合启发的集成方法，包含多个在不同攻击数据集上微调的小型模型（86M参数）。推理时使用随机森林路由器识别最合适的集成成员，然后通过随机选择采样额外成员进行预测聚合。当新攻击出现时，通过微调小型提示安全分类器并添加到集成中来增量更新。

Result: BAGEL仅选择5个集成成员（430M参数）就达到0.92的F1分数，优于需要数十亿参数的OpenAI Moderation API和ShieldGemma。在九次增量更新后性能保持稳健，并通过路由器的结构特征提供可解释性。

Conclusion: 小型微调分类器的集成可以匹配或超越数十亿参数的防护系统，同时提供生产系统所需的适应性和效率，为LLM安全防御提供了更实用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [630] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

Relevance: 85.0

TL;DR: ManifoldKV：一种无需训练的KV缓存压缩方法，通过欧氏距离而非余弦相似度来评估和保留关键token，在长上下文推理中显著提升压缩效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理受限于KV缓存内存的线性增长，现有基于余弦相似度的几何压缩方法会丢失幅度信息，无法有效区分语义重要的token。

Method: 提出ManifoldKV，使用欧氏距离（而非余弦相似度）计算token与全局质心的距离，同时考虑角度和径向偏差；针对64K长上下文提出WindowedManifoldKV变体。

Result: 在RULER基准测试中，4K-16K上下文20%压缩下达到95.7%准确率；多键检索任务提升15.4个百分点；64K上下文下WindowedManifoldKV恢复准确率至84.3%。

Conclusion: ManifoldKV通过简单的欧氏距离评分有效解决KV缓存压缩问题，在多个架构上无需调优即可工作，显著提升长上下文推理的效率和鲁棒性。

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [631] [Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations](https://arxiv.org/abs/2602.08067)
*Chenglei Shen,Yi Zhan,Weijie Yu,Xiao Zhang,Jun Xu*

Main category: cs.LG

Relevance: 85.0

TL;DR: HyperBandit+：一种新颖的上下文老虎机策略，通过时间感知超网络适应时变用户偏好，并利用LLM辅助的预热启动机制提升早期在线阶段的探索-利用效率，用于流式推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有基于老虎机的方法仅将时间视为时间戳，忽略了时间与用户偏好的显式关系，导致性能不佳。此外，在线学习方法在早期在线阶段往往存在探索-利用效率低下的问题。

Method: 1) 时间感知超网络：利用神经网络以时间特征为输入，生成估计时变奖励的参数，捕捉时间与用户偏好的相关性；2) LLM辅助预热启动机制：采用多步数据增强模拟真实交互数据进行有效离线学习，为早期在线阶段提供预热参数；3) 低秩分解：降低超网络训练复杂度以满足实时流式推荐需求。

Result: 在真实世界数据集上的广泛实验表明，HyperBandit+在累积奖励方面持续优于最先进的基线方法。理论分析建立了考虑超网络和LLM预热启动机制的亚线性遗憾上界。

Conclusion: HyperBandit+通过时间感知建模和LLM辅助预热启动，有效解决了流式推荐系统中的时变偏好适应和早期探索效率问题，在理论和实验上都表现出优越性能。

Abstract: In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.

</details>


### [632] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: RLBF（强化学习回溯反馈）框架通过强化学习阶段让LLM学习动态纠正自身生成错误，结合改进的监督微调数据生成策略，显著提升对抗攻击和分布内错误的安全性，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在对抗攻击和分布内错误方面的安全脆弱性，现有方法如BSAFE存在局限性，需要更鲁棒的安全框架来应对复杂的对抗策略（如中间填充、GCG攻击等）。

Method: 1. 强化学习阶段：通过critic反馈训练模型识别和恢复实际生成的安全违规，模型学习发出"回溯x个token"信号后继续自回归生成。2. 增强的监督微调数据生成策略（BSAFE+）：在原本安全的连贯文本中注入违规，为回溯机制提供更有效的初始训练。

Result: RLBF在不同基准测试和模型规模上显著降低攻击成功率，实现优越的安全效果，同时关键地保持了基础模型的实用性。

Conclusion: RLBF框架通过强化学习驱动的动态错误纠正机制，为LLM提供了对抗复杂对抗攻击的鲁棒安全性，同时保持了模型的核心功能，是LLM安全领域的重要进展。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [633] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

Relevance: 85.0

TL;DR: RLTR（可转移奖励的强化学习）通过测试部分推理前缀能否指导其他模型得出正确答案来增强LLM推理的鲁棒性，相比RLVR在更少训练步骤下达到相当性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法只关注最终答案正确性，忽略了推理过程的鲁棒性。作者认为稳健的推理应该能在不同模型间转移和重用，即推理应具有"可转移性"

Method: 提出RLTR框架，通过可转移奖励来评估推理鲁棒性：从一个模型中提取部分推理前缀，测试其能否指导另一个独立模型得出正确答案。这种方法鼓励产生稳定、可解释且真正可泛化的推理

Result: 在MATH500上，RLTR相比RLVR获得+3.6%的Maj@64提升，且仅用约2.5倍更少的训练步骤就达到RLVR的平均准确率，同时提高了采样一致性

Conclusion: RLTR通过可转移奖励机制有效增强了LLM推理的鲁棒性和可解释性，在保持准确率的同时显著提高了训练效率

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [634] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种基于注意力拓扑谱分析的无训练护栏方法，用于检测LLM工具使用中的幻觉和失败，在Llama 3.1 8B上达到97.7%召回率，发现单层谱特征可作为近乎完美的幻觉检测器。


<details>
  <summary>Details</summary>
Motivation: 在野外部署自主代理需要可靠的安全保障来防止工具使用失败。现有监督方法需要标注数据，本文旨在开发无需训练数据的补充性护栏方法。

Method: 基于注意力拓扑的谱分析方法，通过分析注意力矩阵的谱特征（如平滑度和熵）来检测幻觉。使用多特征检测和单层谱特征检测，无需任何标注训练数据。

Result: 在Llama 3.1 8B上，多特征检测达到97.7%召回率，平衡部署时达到86.1%召回率和81.0%精确率。单层谱特征表现突出：Llama L26平滑度达到98.2%召回率，Mistral L3熵达到94.7%召回率。跨模型评估发现"大声说谎者"现象：Llama 3.1 8B的失败在谱上更明显，而Mistral 7B达到最佳区分度（AUC 0.900）。

Conclusion: 幻觉不仅是错误标记，而是热力学状态变化：模型注意力在出错时变为噪声。谱分析为代理安全提供了一个原则性、高效的框架。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [635] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了在线领域感知解码框架（ODD），用于解决LLM在动态领域中的实时适应问题，通过概率级融合和自适应置信度调制来应对概念漂移。


<details>
  <summary>Details</summary>
Motivation: 现实世界中领域知识不断演化（新法规、产品、服务等），而传统LLM微调假设静态领域，重新训练成本高昂。同时，数据分布随时间变化（概念漂移），忽略这一问题会显著降低模型预测准确性。需要高效、实时的适应方法，避免昂贵的重新训练。

Method: 提出在线领域感知解码框架（ODD），在基础LLM和前缀树先验之间进行概率级融合，使用分歧和连续性信号指导自适应置信度调制。该方法能够实时适应不断变化的领域知识，无需重新训练。

Result: 在多种漂移场景下的实证评估显示，ODD在所有句法和语义NLG指标上一致优于LLM-Greedy和LLM-Temp Scaled基线。获得0.065的绝对ROUGE-L增益，余弦相似度相对最佳基线提升13.6%。

Conclusion: ODD对演化的词汇和上下文模式具有鲁棒性，适用于动态LLM应用，为解决LLM在持续变化环境中的适应问题提供了有效方案。

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [636] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Variational In-Context Reward Modeling (ICRM)，一种贝叶斯奖励建模方法，通过上下文偏好演示实现测试时的可调控性，适应复杂多目标对齐场景。


<details>
  <summary>Details</summary>
Motivation: 传统分类器奖励模型训练后固定不变，无法适应测试时复杂的偏好分布和多目标对齐需求。随着RL应用于可验证奖励和多目标对齐，需要更灵活、可调控的奖励模型。

Method: 将奖励建模转化为基于Bradley-Terry模型和Beta共轭先验的摊销变分推断问题。通过上下文偏好演示，在测试时调整模型以适应未见过的偏好分布。

Result: 在单目标设置中，ICRM在SafeRLHF上提升34%准确率，在RM-Bench上提升9%；在多目标设置中，Pareto前沿扩大，超体积提升4%。在数学推理任务中优于传统RM。

Conclusion: ICRM提供了一种灵活、可调控的奖励建模框架，能够适应复杂偏好分布，支持多目标对齐，并具有理论保证，为RLHF和可信AI提供了新工具。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [637] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种从训练好的Transformer中提取RASP程序的方法，通过因果干预发现最小充分子程序，为Transformer内部实现简单可解释程序提供直接证据


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明Transformer的计算可以用RASP编程语言模拟，且Transformer在具有简单RASP程序的问题上能实现长度泛化，但尚不清楚训练好的模型是否真的实现了简单可解释的程序

Method: 将Transformer忠实地重新参数化为RASP程序，然后应用因果干预来发现最小充分子程序

Result: 在小型Transformer上训练算法和形式语言任务的实验中，该方法通常能从长度泛化的Transformer中恢复简单可解释的RASP程序

Conclusion: 这是迄今为止最直接的证据，表明Transformer内部实现了简单的RASP程序

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [638] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一个评估智能体目标导向性的框架，结合行为评估和可解释性分析，以LLM智能体在2D网格世界导航为案例研究


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可靠的方法来归因智能体系统的目标，需要超越单纯行为评估的方法来理解智能体如何表示和追求目标

Method: 提出整合行为评估和可解释性分析的框架：1) 行为评估：在2D网格世界中测试LLM智能体，比较最优策略，考察网格大小、障碍密度、目标结构等变量；2) 可解释性分析：使用探测方法解码智能体的内部表示，包括环境状态和多步行动计划

Result: 1) 行为表现随任务难度增加而下降，但对难度保持的变换和复杂目标结构保持鲁棒性；2) LLM智能体非线性编码环境的粗略空间地图，保留位置和目标的近似任务相关线索；3) 行动与内部表示基本一致；4) 推理过程重组表示，从环境结构线索转向支持即时行动选择的信息

Conclusion: 需要内省式检查超越行为评估，以表征智能体如何表示和追求目标，可解释性分析提供了行为评估无法获得的洞察

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [639] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出CADO框架，通过强化学习微调解决热图求解器在组合优化中的目标不匹配问题，直接优化解码后的解成本，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于热图的组合优化求解器主要采用监督学习训练范式，存在根本性的目标不匹配问题：最小化模仿损失（如交叉熵）并不能保证解成本的优化。这种不匹配表现为解码器盲视（忽略不可微的解码过程）和成本盲视（优先结构模仿而非解质量）。

Method: 提出CADO（Cost-Aware Diffusion models for Optimization）框架：1）将扩散去噪过程建模为MDP，直接优化解码后的解成本；2）引入标签中心奖励，将真实标签重新用作无偏基线而非模仿目标；3）采用混合微调实现参数高效适应。

Result: CADO在多个基准测试中实现了最先进的性能，验证了目标对齐对于释放基于热图的求解器全部潜力的重要性。

Conclusion: 通过强化学习微调直接优化解成本，而非仅仅模仿标签结构，是解决热图求解器目标不匹配问题的有效方法，CADO框架为此提供了系统解决方案。

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [640] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一个基于第一性原理的信息动力学框架，将注意力机制视为受最小作用量原理支配的物理系统，而非算法优化。通过将信息状态映射到黎曼流形，推导出智能拉格朗日量，并建立了信息热力学第一定律。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构已经革命性地改变了人工智能领域，但其底层机制仍然主要是启发式的，缺乏统一的物理理论。作者旨在建立一个基于物理原理的智能理论框架，连接统计物理学和深度学习。

Method: 1. 将信息状态映射到具有Fisher信息度量的黎曼流形
2. 推导智能拉格朗日量，将注意力机制视为受最小作用量原理支配的物理系统
3. 证明softmax函数对应于最小化信息气体亥姆霍兹自由能的唯一热力学平衡态
4. 将query-key交互识别为外部场与固有偶极矩之间的电动力学耦合

Result: 1. 建立了信息热力学第一定律，统一了推理（机械功）和学习（化学演化）
2. 解释了涌现现象（如缩放定律和顿悟）作为比热发散表征的相变
3. 解释了注意力流形中的旋转对称性破缺如何产生无质量的Goldstone玻色子，为旋转位置编码（RoPE）提供了场论视角

Conclusion: 该工作连接了统计物理学和深度学习，为基于物理的智能理论奠定了基础，为理解Transformer架构的底层机制提供了新的物理视角。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [641] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

Relevance: 85.0

TL;DR: SkillRL：通过自动技能发现和递归演化，将原始经验转化为可重用行为模式的LLM智能体框架，显著提升任务性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常孤立运行，无法从历史经验中学习。基于记忆的方法主要存储原始轨迹，这些轨迹冗余且噪声多，无法提取高层次、可重用的行为模式，限制了智能体的泛化能力。

Method: 提出SkillRL框架，包含：1）基于经验的蒸馏机制构建分层技能库SkillBank；2）通用和任务特定启发式的自适应检索策略；3）在强化学习过程中技能库与智能体策略共同演化的递归演化机制。

Result: 在ALFWorld、WebShop和七个搜索增强任务上的实验表明，SkillRL达到最先进性能，比强基线提升超过15.3%，且在任务复杂度增加时保持鲁棒性，同时显著减少token占用。

Conclusion: SkillRL通过将原始经验转化为结构化技能，有效提升了LLM智能体的学习效率和泛化能力，为构建更智能、可进化的AI系统提供了新思路。

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [642] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文通过线性化视角分析参数高效微调(PEFT)，发现微调动态等价于使用神经正切核(NTK)学习，NTK特征谱与模型适应性能强相关，为PEFT技术提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: PEFT技术虽然流行，但其训练性能和泛化机制尚未充分探索。论文旨在通过线性化视角深入理解微调过程，特别是分析微调动态与NTK的关系，为PEFT提供理论基础。

Method: 1) 将微调模型显式约束在预训练模型附近（欧几里得距离归纳偏置）；2) 证明微调动态等价于使用正定NTK学习；3) 基于正则化强度分析完全线性与线性化微调的接近程度；4) 给出NTK特征谱扰动界；5) 在LLM的LoRA上进行实证验证。

Result: 1) 线性化是LLM微调的良好模型；2) NTK特征谱与模型适应性能存在强相关性；3) 不同层选择对NTK的谱扰动有理论界限；4) 实证验证了理论在LoRA上的有效性。

Conclusion: 通过线性化视角揭示了PEFT微调机制，建立了NTK特征谱与性能的关联，为优化PEFT技术提供了理论基础，有助于更明智、更灵活的LLM适应方法。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [643] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出ICPRL框架，仅使用偏好反馈实现上下文强化学习，无需奖励监督，在未见任务上实现与有奖励监督方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习(ICRL)方法依赖显式奖励信号进行预训练，限制了在奖励模糊、难以指定或获取成本高场景下的应用。需要一种仅使用偏好反馈的学习范式。

Method: 提出ICPRL框架，包含两种变体：基于每步偏好的I-PRL和基于轨迹级比较的T-PRL。开发了监督预训练和偏好原生框架两种方法，后者直接从偏好数据优化策略，无需奖励信号或最优动作标签。

Result: 在决斗老虎机、导航和连续控制任务上的实验表明，ICPRL能够在未见任务上实现强大的上下文泛化能力，性能与使用完整奖励监督的ICRL方法相当。

Conclusion: ICPRL证明了仅使用偏好反馈实现上下文强化学习的可行性，为奖励信号难以获取的决策任务提供了有效的解决方案。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [644] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文通过PAC框架理论分析MARL与SARL在LLM训练中的样本效率，发现任务可分解性决定MARL优势，任务对齐影响效率权衡


<details>
  <summary>Details</summary>
Motivation: 尽管MARL在LLM训练中显示出潜力，但缺乏理论指导何时选择MARL而非SARL。本文旨在填补这一理论空白，为LLM的RL框架选择提供理论依据。

Method: 采用PAC框架，形式化定义LLM的SARL和MARL设置，推导显式样本复杂度边界，系统分析任务分解和对齐如何影响学习效率。

Result: MARL在任务可自然分解为独立子任务时提升样本效率，而依赖性子任务削弱MARL优势。任务对齐概念揭示了强制独立分解与潜在错配间的权衡。

Conclusion: 理论分析澄清了实证不一致性，为复杂LLM场景中有效部署MARL策略提供了实用标准，强调任务分解性质的关键作用。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [645] [Noise Stability of Transformer Models](https://arxiv.org/abs/2602.08287)
*Themistoklis Haris,Zihan Zhang,Yuichi Yoshida*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出噪声稳定性作为深度学习简单性偏置的新度量，替代平均敏感度，通过理论分析和正则化方法加速Transformer训练并促进grokking现象。


<details>
  <summary>Details</summary>
Motivation: 现有用于衡量深度学习简单性偏置的平均敏感度指标存在两个关键局限：1）难以自然推广到实值域；2）无法解释现代LLM中观察到的"junta-like"输入依赖性。需要更全面的简单性度量来理解和改进Transformer模型。

Method: 提出噪声稳定性作为新的简单性度量，衡量模型对所有输入坐标同时施加相关噪声的鲁棒性。对单层注意力机制和ReLU MLP层进行理论分析，采用协方差区间传播方法解决多层传播问题，并开发实用的噪声稳定性正则化方法。

Result: 在算法任务和下一个token预测任务上的实验表明，噪声稳定性正则化方法能持续催化grokking现象，分别加速训练约35%和75%。

Conclusion: 噪声稳定性成为理解和改进现代Transformer的强大工具，在神经网络信号传播和可解释性之间建立了新的连接。

Abstract: Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the "junta-like" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\%$ and $75\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.

</details>


### [646] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: TextResNet提出了一种解决文本梯度优化器在深度链中语义纠缠问题的新框架，通过语义梯度分解和因果路由实现精确的信号传播，在复合AI系统中表现优于TextGrad。


<details>
  <summary>Details</summary>
Motivation: TextGrad等文本梯度优化器在深度链中效果不佳，主要原因是语义纠缠问题——反馈信号混合了局部批评和上游上下文，导致归因模糊。需要解决这一限制以实现更有效的复合AI系统优化。

Method: 提出TextResNet框架，包含四个关键创新：1) 前向传播中强制加性语义增量以保持梯度流的恒等高速公路；2) 通过语义投影器进行语义梯度分解，将反馈解耦到因果独立的子空间；3) 因果路由将投影信号路由到特定组件；4) 密度感知优化调度利用解耦信号动态分配资源到系统瓶颈。

Result: TextResNet不仅比TextGrad表现更优，而且在复合AI系统的智能体任务中展现出显著稳定性，而基线方法会崩溃。

Conclusion: TextResNet通过解决语义纠缠问题，为深度链复合AI系统提供了更稳定有效的文本梯度优化框架，推动了基于文本反馈的优化方法发展。

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [647] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

Relevance: 85.0

TL;DR: Extra-CoT：极端比率思维链压缩框架，通过语义保留压缩器和分层强化学习，在保持准确性的同时大幅减少推理token数


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然提升了LLM的推理能力，但带来了巨大的计算开销。现有压缩方法在高压缩比下会损失逻辑保真度，导致性能显著下降。需要一种既能大幅减少token预算，又能保持答案准确性的高保真快速推理方法。

Method: 1. 在数学CoT数据上训练专门的语义保留压缩器生成高保真监督信号；2. 通过混合比率监督微调让LLM学习遵循不同压缩预算；3. 提出约束分层比率策略优化，通过分层奖励显式激励低预算下的问题解决能力

Result: 在三个数学推理基准测试中表现优异。例如在MATH-500上使用Qwen3-1.7B，Extra-CoT实现了超过73%的token减少，同时准确率提高了0.6%，显著优于现有最佳方法

Conclusion: Extra-CoT框架成功实现了高保真、快速的推理，在极端压缩比下仍能保持甚至提升推理性能，为LLM的高效推理提供了有效解决方案

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [648] [Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference](https://arxiv.org/abs/2602.08329)
*Yifei Gao,Lei Wang,Rong-Cheng Tu,Qixin Zhang,Jun Cheng,Dacheng Tao*

Main category: cs.LG

Relevance: 85.0

TL;DR: PrHS提出了一种先验稀疏注意力方法，在注意力评分前选择KV缓存条目，通过控制丢弃质量提供可验证的精度保证，显著减少LLM推理的计算和带宽开销。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的核心瓶颈是不断增长的KV缓存带来的计算和带宽成本。现有稀疏方法依赖后验启发式（基于观测到的注意力或代理分数），这会导致后验偏差：扭曲真实token重要性并遗漏重要token，损害长程推理能力。

Method: 提出Pre-hoc Sparsity (PrHS)，在注意力评分前选择KV条目，提供显式的精度控制。通过边际到互信息分析，推导出仅依赖于丢弃质量的互信息损失上界。在PrHS框架下，沿时间、深度和层三个正交轴实例化了三种先验选择器。

Result: 在LLaMA和Mistral系列模型上的实验验证：在GSM8K和CoQA上减少90%以上检索开销，比HShare实现3倍更高的检索稀疏性；在LongBench上平均退化低于1%；比先前稀疏基线减少约15%的注意力FLOPs；在NVIDIA A100-80GB GPU上获得9.9倍注意力算子延迟加速和2.8倍吞吐量提升。

Conclusion: PrHS通过先验KV选择解决了后验稀疏方法的后验偏差问题，提供可验证的精度保证，显著提升LLM推理效率，同时保持模型质量。

Abstract: A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.

</details>


### [649] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

Relevance: 85.0

TL;DR: JoBS：一种联合优化LLM训练数据和模型配置的方法，使用缩放定律性能预测器辅助贝叶斯优化，通过部分预算学习预测器，剩余预算进行优化，降低全训练成本。


<details>
  <summary>Details</summary>
Motivation: LLM训练中数据和模型配置的联合优化存在"先有鸡还是先有蛋"的困境：最佳训练数据配置（如数据混合）取决于模型配置（如架构），反之亦然。现有方法通常单独优化数据或模型，忽略了它们的相互作用，而联合优化被认为难以处理。

Method: JoBS使用缩放定律启发的性能预测器辅助贝叶斯优化。方法分配部分优化预算学习LLM性能预测器（通过少量训练步骤预测配置效果），剩余预算完全使用预测器进行贝叶斯优化，从而摊销全训练运行成本。研究分析了平均遗憾并设计了最小化遗憾的最优预算分配策略。

Result: 在相同优化预算下，JoBS在多样化的LLM任务中优于现有的多保真度贝叶斯优化基线，以及单独的数据和模型优化方法。

Conclusion: JoBS有效解决了LLM训练中数据和模型配置的联合优化问题，通过智能预算分配和预测器使用，实现了比现有方法更高效的优化。

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [650] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

Relevance: 85.0

TL;DR: Modalities是一个端到端的PyTorch原生框架，旨在解决LLM大规模消融实验中的计算效率和工具支持问题，通过集成先进并行化策略和模块化设计，支持万亿token和十亿参数规模的预训练与系统消融。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练和研究工作流中，大规模消融实验消耗大量计算资源，但现有开源框架对此提供有限工具支持，迫使研究人员编写自己的包装器和脚本，导致效率低下和可复现性问题。

Method: 1) 集成先进的并行化策略，支持高效预训练和系统消融；2) 采用模块化设计，提供声明式、自包含的配置系统；3) 构建端到端的PyTorch原生框架，实现从数据驱动研究到大规模模型训练的完整工作流。

Result: 该框架能够支持万亿token和十亿参数规模的高效预训练与系统消融实验，同时通过模块化设计和声明式配置，实现了现有LLM训练框架难以达到的可复现性和可扩展性水平。

Conclusion: Modalities框架填补了LLM研究工具链的空白，通过提供专门针对大规模消融实验的端到端解决方案，显著提升了LLM研究的效率和可复现性，对LLM训练和架构研究具有重要价值。

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [651] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种基于上下文多臂老虎机的RLVR（强化学习与可验证奖励）调度框架，通过自适应选择高质量rollout样本来提升大语言模型的推理能力训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在rollout使用上存在两个主要问题：1）对同一提示下质量各异的响应进行无差别处理，导致噪声监督；2）历史rollout样本仅使用一次即丢弃，导致样本效率低下和次优策略更新。

Method: 将RLVR中的rollout调度建模为上下文多臂老虎机问题，提出统一的神经调度框架。每个rollout被视为一个"臂"，其奖励定义为连续优化步骤间的性能增益提升。该调度器支持噪声感知的组内选择和历史rollout的自适应全局重用。

Result: 在六个数学推理基准测试上的实验表明，该方法在多个RLVR优化方法中均能带来一致的性能提升和训练效率改善。

Conclusion: 提出的调度框架通过理论分析（次线性遗憾界）和实验验证，有效解决了RLVR中rollout使用的低效问题，为大语言模型推理能力的强化学习训练提供了更高效的优化方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [652] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

Relevance: 85.0

TL;DR: LLMs存在隐式记忆能力，能够在独立交互间传递状态信息，无需显式记忆模块。研究者通过时间炸弹攻击展示了这种能力的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs作为无状态系统的传统假设，揭示模型通过自身输出在不同交互间隐式传递信息的潜在能力，并探索其安全影响。

Method: 提出隐式记忆概念，通过时间炸弹攻击作为具体演示——这种后门攻击需要满足通过隐式记忆积累的隐藏条件序列才能激活。使用提示工程和微调实现。

Result: 成功展示了LLMs能够通过隐式记忆在不同交互间传递信息，创建了时间炸弹攻击，并分析了隐式记忆在隐蔽通信、基准污染、定向操纵和数据投毒等方面的安全风险。

Conclusion: 隐式记忆是LLMs的一个重要特性，具有深远的安全影响。需要开发检测方法和压力测试框架来应对这一新兴威胁。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [653] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出M-Loss评估指标，量化模型合并与模型集成的差异，指导更有效的模型合并策略


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练计算成本高且受标注数据限制，模型合并提供了一种无需额外数据或大量训练的替代方案。但传统参数平均合并方法会混合非泛化特征，而模型集成虽然性能更好但推理成本和存储需求高。现有研究缺乏理论证据和评估指标来指导模型合并。

Method: 提出Merging-ensembling loss (M-Loss)评估指标，使用少量无标签数据量化模型合并的兼容性。通过测量参数平均与模型集成在层和节点级别的差异，指导更有效的合并策略。M-Loss既作为模型合并理论可行性的定量标准，也作为模型剪枝中参数重要性的指导。

Result: 理论分析和实证评估表明，将M-Loss纳入合并过程显著提高了合并模型与模型集成之间的对齐度，为准确模型整合提供了可扩展且高效的框架。

Conclusion: M-Loss为模型合并提供了理论指导和实用评估工具，能够在保持性能的同时降低计算和存储成本，是大规模模型训练的有效替代方案。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [654] [Conditional Sequence Modeling for Safe Reinforcement Learning](https://arxiv.org/abs/2602.08584)
*Wensong Bai,Chao Zhang,Qihang Xu,Chufan Chen,Chenhao Zhou,Hui Qian*

Main category: cs.LG

Relevance: 85.0

TL;DR: RCDT：首个基于条件序列建模的离线安全强化学习方法，支持单一策略零样本适应不同成本阈值，通过拉格朗日式成本惩罚和自适应惩罚系数实现更好的回报-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全RL方法通常在预定义成本阈值下训练，导致策略泛化能力有限且部署灵活性不足。实际部署中不同场景需要不同成本阈值，需要单一策略能零样本适应多种阈值。

Method: 基于条件序列建模(CSM)框架，集成拉格朗日式成本惩罚与自适应惩罚系数。引入回报-成本感知的轨迹重加权机制和Q值正则化，避免过度保守行为，优化回报-成本权衡。

Result: 在DSRL基准测试中，RCDT相比代表性基线方法持续改进回报-成本权衡，推进了离线安全RL的最新技术水平。

Conclusion: RCDT是首个CSM-based离线安全RL算法，通过自适应成本惩罚机制实现单一策略对多种成本阈值的零样本适应，为离线安全RL提供了更灵活的部署方案。

Abstract: Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>


### [655] [Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction](https://arxiv.org/abs/2602.08585)
*Ziyao Tang,Pengkun Jiao,Xinhang Chen,Wei Liu,Shiyong Li,Jingjing Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出LU-KV框架，通过边际效用优化KV缓存逐出策略，在保持80%缓存压缩率的同时最小化性能损失


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存逐出方法依赖瞬时启发式指标，假设所有注意力头的分数幅度是重要性的一致代理，但忽略了不同注意力头在预测保真度上的异质性。某些头关注token的瞬时贡献，而其他头则专注于捕获长期效用。

Method: 提出LU-KV框架：1) 基于边际效用优化头级预算分配；2) 使用凸包松弛和基于边际效用的贪心求解器实现近最优精度；3) 实现数据驱动的离线分析协议以促进实际部署。

Result: 在LongBench和RULER基准测试中，LU-KV实现了80%的KV缓存大小减少，同时最小化性能下降，并减少推理延迟和GPU内存占用。

Conclusion: 通过考虑注意力头的异质性和基于边际效用的预算分配，LU-KV提供了一种高效且实用的KV缓存优化方法，显著提升大语言模型推理效率。

Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.

</details>


### [656] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

Relevance: 85.0

TL;DR: ERIS：一个无服务器的联邦学习框架，通过模型分区和分布式梯度压缩，在保持FedAvg级别准确率的同时，显著降低通信成本并增强隐私保护，适用于十亿参数模型。


<details>
  <summary>Details</summary>
Motivation: 将联邦学习扩展到十亿参数模型时，通信效率、模型准确性和隐私保证之间存在关键权衡。现有解决方案通常孤立地处理这些挑战，牺牲准确性或依赖昂贵的加密工具。

Method: ERIS结合了模型分区策略（将聚合分布在多个客户端聚合器上）和分布式移位梯度压缩机制，消除了服务器瓶颈并分布通信负载。

Result: 理论证明ERIS在标准假设下以与FedAvg相同的速率收敛，并将互信息泄漏与聚合器数量成反比，实现强隐私保证。实验表明ERIS在图像和文本任务（包括大语言模型）中达到FedAvg级别准确率，同时显著降低通信成本并提高对成员推理和重建攻击的鲁棒性。

Conclusion: ERIS提供了一个平衡隐私和准确性的无服务器联邦学习框架，无需依赖繁重的密码学或噪声注入，适用于大规模模型训练。

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [657] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文揭示了MoE大语言模型的安全风险，发现通过操纵路由器可以激活不安全路径，将安全输出转为有害内容，并提出防御方法。


<details>
  <summary>Details</summary>
Motivation: MoE架构通过选择性激活专家来降低计算成本，但先前研究主要关注效用和效率，忽视了稀疏架构带来的安全风险。本文旨在探索MoE LLMs的安全漏洞。

Method: 1) 引入路由器安全重要性评分(RoSais)量化每层路由器的安全关键性；2) 提出细粒度token层随机优化框架(F-SOUR)发现具体不安全路径，考虑输入token的顺序性和动态性。

Result: 在DeepSeek-V2-Lite上，仅屏蔽5个路由器就能将攻击成功率提高4倍至0.79。在四个代表性MoE LLM家族上，F-SOUR在JailbreakBench和AdvBench上分别达到0.90和0.98的平均攻击成功率。

Conclusion: MoE LLMs的安全性与其架构一样稀疏，存在通过路由器操纵激活不安全路径的风险。提出了安全感知路径禁用和路由器训练等防御方向。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [658] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出新的生成模型公平性定义EGT，关注不同敏感群体生成质量可比性而非概率平衡，通过理论分析证明公平约束导致模型质量受最难近似群体限制，提出min-max微调方法实现EGT


<details>
  <summary>Details</summary>
Motivation: 现有生成模型公平性定义主要从分类任务迁移而来，关注平衡不同敏感群体样本生成概率，但这种方法很脆弱，即使不同群体建模质量差异很大也能满足公平标准。需要更全面的公平性定义来确保生成质量在不同敏感群体间可比。

Method: 提出equalized generative treatment (EGT)公平性定义，使用参考f-散度衡量生成质量。理论分析公平约束的权衡，证明模型整体质量受最难近似群体限制。提出简单高效的min-max微调方法，通过平衡不同敏感群体的f-散度来满足EGT。

Result: 在图像和文本生成任务上的实验验证表明，min-max方法相比文献中其他方法能持续实现更公平的结果，同时在两个任务上保持有竞争力的整体性能。

Conclusion: EGT为生成模型提供了更全面的公平性定义，关注生成质量可比性而非概率平衡。理论分析揭示了公平约束的内在权衡，提出的min-max微调方法能有效实现EGT，在保持性能的同时提升公平性。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [659] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

Relevance: 85.0

TL;DR: LLaDA2.1通过结合Token-to-Token编辑和Mask-to-Token方案，提出可配置阈值解码方案，实现速度与质量的平衡，并首次为扩散大语言模型设计大规模强化学习框架，显著提升推理精度和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLaDA2.0展示了100B级别块扩散模型的扩展潜力及其固有的并行化能力，但解码速度与生成质量之间的微妙平衡一直是一个难以突破的边界。需要一种能够超越这种权衡的新范式。

Method: 1. 将Token-to-Token编辑无缝集成到传统的Mask-to-Token方案中，引入联合可配置阈值解码方案，创建两种模式：速度模式（降低M2T阈值，依赖T2T优化输出）和质量模式（保守阈值保证性能）
2. 在扩展上下文窗口基础上，首次为扩散大语言模型设计大规模强化学习框架，采用专门的稳定梯度估计技术
3. 发布LLaDA2.1-Mini（16B）和LLaDA2.1-Flash（100B）两个模型版本

Result: 在33个严格基准测试中，LLaDA2.1展现出强大的任务性能和闪电般的解码速度。尽管是100B规模，在编码任务上达到惊人性能：HumanEval+ 892 TPS，BigCodeBench 801 TPS，LiveCodeBench 663 TPS。

Conclusion: LLaDA2.1通过创新的解码架构和强化学习对齐，成功解决了扩散大语言模型中速度与质量的权衡问题，在保持高质量生成的同时实现了前所未有的解码效率。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [660] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: CompilerKV：一种风险自适应和头感知的KV缓存压缩框架，通过离线经验编译决策表，在512-token预算下恢复97.7%的FullKV性能


<details>
  <summary>Details</summary>
Motivation: 长上下文场景中LLMs受限于KV缓存的线性内存增长。现有压缩方法要么依赖静态阈值和注意力启发式，要么采用粗粒度内存分配，忽略了两个关键因素：提示依赖的压缩风险变化和注意力头间的功能异质性，导致token选择不稳定和尾部失败。

Method: 提出CompilerKV框架，将离线经验编译为可重用的决策表，包含两个协同组件：1) 通过离线上下文老虎机学习的头异质性表，为不同注意力头分配可靠性权重；2) 风险自适应阈值门控机制，联合建模注意力熵和局部困惑度，将提示级风险转化为可部署的保留阈值。

Result: 在LongBench上的实验显示，在512-token预算下，CompilerKV主导SOTA方法，恢复了97.7%的FullKV性能，相比最强竞争对手获得了高达+5.2分的性能提升。

Conclusion: CompilerKV通过风险自适应和头感知的压缩方法，有效解决了长上下文LLMs中的KV缓存内存约束问题，显著提升了压缩性能并减少了尾部失败。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [661] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该研究通过主动概率推理任务比较人类和LLM在不确定性下的决策行为，发现扩展推理（如CoT）显著提升证据整合能力，使模型信念轨迹更接近人类，但对主动信息采集改善有限。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在不确定性下的决策是否与人类相似，以及链式思维推理在决策过程中的作用。将主动信息采集与证据整合分离，系统比较人类和LLM的决策行为差异。

Method: 设计主动概率推理任务，分离采样（主动获取证据）和推理（整合证据做决策）。比较人类和多种当代LLM与近最优参考策略的表现。使用机制模型拟合行为数据，通过四个可解释潜变量（记忆、策略、选择偏差、遮挡意识）分析系统偏差。

Result: 扩展推理是强性能的关键决定因素：显著提升推理能力，使信念轨迹变得类似人类，但对主动采样的改善有限。机制模型将人类和模型置于共享的低维认知空间，再现行为特征，显示CoT使LLM向人类证据积累和信念-选择映射模式转变。

Conclusion: 链式思维推理使语言模型的证据整合过程更接近人类，但在信息采集方面仍存在持续差距。该研究提供了解释人类和LLM决策差异的认知框架，对LLM的推理能力评估和人类对齐有重要意义。

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [662] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究Transformer在特征噪声下的鲁棒学习能力：在稀疏奇偶校验和多数函数上表现良好，但在随机k-junta函数上失败，主要原因是Transformer对简单函数的偏好与噪声鲁棒学习需要低敏感度函数之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据普遍存在噪声，但模型是否能在无噪声输入上正确泛化尚不清楚。研究旨在探索Transformer在特征噪声下的鲁棒学习能力，即训练时使用带噪声特征的数据，是否能学习到为目标函数正确预测无噪声特征标签的能力。

Method: 通过实验研究Transformer在不同布尔函数上的噪声鲁棒学习能力：1) 比较Transformer和LSTM在k-稀疏奇偶校验和多数函数上的表现；2) 测试Transformer在随机k-junta函数上的性能；3) 分析失败原因，提出Transformer对简单函数的偏好与噪声鲁棒学习需要低敏感度函数之间的矛盾；4) 设计实验验证假设，通过额外损失项惩罚高敏感度解决方案来帮助Transformer逃逸错误解。

Result: 1) Transformer在k-稀疏奇偶校验和多数函数上成功实现噪声鲁棒学习，而LSTM即使在适度特征噪声下也会失败；2) Transformer在随机k-junta函数的噪声鲁棒学习上通常失败，特别是当最优解的布尔敏感度小于目标函数时；3) 失败原因是Transformer对简单函数的偏好与噪声鲁棒学习需要低敏感度函数之间的矛盾；4) 通过添加惩罚高敏感度解决方案的额外损失项，可以帮助Transformer逃逸错误解陷阱。

Conclusion: Transformer在布尔函数的噪声鲁棒学习方面特别低效。虽然在某些结构化函数上表现良好，但在随机布尔函数上容易失败，主要受限于其对简单函数的偏好与噪声鲁棒学习需求之间的矛盾。通过适当调整训练目标可以部分缓解这一问题。

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [663] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

Relevance: 85.0

TL;DR: QUOKA是一种训练免费、硬件无关的稀疏注意力算法，通过选择低余弦相似度的查询和相关的键值对来加速Transformer推理，在保持接近基线准确率的同时实现3-7倍加速。


<details>
  <summary>Details</summary>
Motivation: Transformer推理中的注意力计算是计算密集的，特别是在分块预填充阶段。虽然许多查询只关注少量键，但作者观察到低余弦相似度的查询与更多键交互且对最终注意力对数贡献更大，这为选择性稀疏化提供了机会。

Method: QUOKA采用两步法：1) 保留一小部分代表性查询（特别是低余弦相似度的查询）；2) 选择与这些查询最对齐的键。该方法无需训练，硬件无关，适用于分块预填充场景。

Result: 在Needle-In-A-Haystack、LongBench、RULER和Math500等基准测试中，QUOKA实现了：首token时间减少3倍，Nvidia GPU上注意力计算加速5倍，Intel Xeon CPU上加速近7倍，同时使用88%更少的键值对，准确率接近基线。

Conclusion: QUOKA通过智能选择低余弦相似度查询和相关键值对，有效加速Transformer推理，在保持模型性能的同时显著减少计算开销，为高效推理提供了实用的解决方案。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [664] [How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs](https://arxiv.org/abs/2602.08808)
*Yapei Chang,Kyle Lo,Mohit Iyyer,Luca Soldaini*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了How2Everything框架，用于评估和改进目标条件过程生成，包括数据挖掘(How2Mine)、基准测试(How2Bench)和评估协议(How2Score)，展示了模型规模扩展趋势，并通过强化学习显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成逐步"如何做"过程是LLM的关键能力，但在真实世界任务中大规模评估和改进过程有效性仍然具有挑战性且研究不足。需要可扩展的框架来评估和改进目标条件过程生成。

Method: 1) How2Mine：从98万个网页中挖掘35.1万个过程，涵盖14个主题；2) How2Bench：构建7K示例的评估集；3) How2Score：使用LLM法官检测生成中是否包含阻碍目标实现的关键失败；4) 将前沿模型蒸馏为8B开源模型用于低成本评估；5) 使用How2Score作为奖励进行强化学习。

Result: 1) 蒸馏模型与人类标注者达到80.5%一致性；2) How2Bench揭示了模型规模和训练阶段的清晰扩展趋势；3) 使用How2Score奖励的RL在三个模型上将How2Bench性能提升>10分，且不影响标准基准测试；4) 改进对表面记忆或格式合规具有鲁棒性。

Conclusion: How2Everything展示了预训练网络数据如何支持能力评估和改进的闭环循环，为过程生成提供了可扩展的评估和改进框架。

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

</details>


### [665] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

Relevance: 85.0

TL;DR: FRPO是一种鲁棒的RLHF框架，通过优化策略在KL有界邻域内的奖励稳定性，防止下游微调时的灾难性遗忘，保持安全性和任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM通过多阶段后训练（RLHF+下游微调），但即使小的下游更新也会破坏先前学习的行为（如安全性），存在灾难性遗忘问题。标准RLHF目标无法保证对未来适应的鲁棒性。

Method: 提出Fine-tuning Robust Policy Optimization (FRPO)，在GRPO基础上修改，优化当前策略及下游适应可达的KL有界邻域内的奖励，通过最大-最小化公式确保策略变化下的奖励稳定性。

Result: 实验表明FRPO显著减少多个基础模型和下游微调机制（SFT和RL）下的安全性退化，同时保持下游任务性能。在数学RL设置中，FRPO也能在后续微调下保持准确性。

Conclusion: FRPO通过预微调鲁棒性优化，解决了RLHF模型在下游适应时的灾难性遗忘问题，为构建更鲁棒的LLM训练框架提供了有效方法。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [666] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

Relevance: 85.0

TL;DR: FlexMoRE：一种灵活的秩异构专家混合架构，通过将全尺寸专家替换为低秩适配器，在保持性能的同时显著减少参数数量（从33.27B降至10.75B），推理密集型任务需要更高秩的专家。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构通常训练全尺寸专家，但作者认为并非所有领域都需要全尺寸专家，低秩适配器可能就足够。这可以显著提高内存效率，同时保持下游任务性能。

Method: 提出FlexMoRE（灵活的秩异构专家混合）架构，支持全尺寸专家和不同秩的低秩适配器。基于FlexOlmo框架，将其预训练专家转换为低秩版本，系统研究专家秩与下游任务性能的权衡，评估6个不同秩（2^0到2^14）的专家，涵盖150种混合配置。

Result: 回归分析显示：推理密集型基准需要更高秩的专家，知识密集型基准对秩要求较低。使用最优秩配置，FlexMoRE在平均得分47.18下优于全尺寸专家混合的45.46，同时参数减少到三分之一（10.75B vs 33.27B）。

Conclusion: 秩异构专家混合架构能显著提高内存效率而不牺牲性能，推理任务需要更高秩的专家，这为高效混合专家系统设计提供了重要指导。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [667] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

Relevance: 85.0

TL;DR: Dr. MAS：针对多智能体LLM系统的稳定RL训练方法，通过智能体级优势归一化解决梯度不稳定问题


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统通过角色专业化实现高级推理和工具使用，但现有的群体式RL方法在扩展到多智能体系统时存在训练不稳定问题。研究发现GRPO风格的优化中，全局归一化基线可能偏离不同智能体的奖励分布，导致梯度范数不稳定。

Method: 提出Dr. MAS方法，采用智能体级补救措施：使用每个智能体自身的奖励统计量对优势进行归一化，从而校准梯度尺度。该方法提供了一个端到端的RL训练框架，支持可扩展编排、灵活的智能体级LLM服务和优化配置，以及共享资源调度。

Result: 在数学推理和多轮搜索基准测试中使用Qwen2.5和Qwen3系列模型进行评估。Dr. MAS相比vanilla GRPO有明显提升（数学任务：avg@16 +5.6%，pass@16 +4.6%；搜索任务：avg@16 +15.2%，pass@16 +13.1%），同时大幅消除梯度尖峰。在异构智能体模型分配下仍保持高效。

Conclusion: Dr. MAS为多智能体LLM系统提供了一个简单而稳定的RL训练方案，通过智能体级优势归一化解决了训练不稳定问题，在保持高效的同时显著提升性能。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [668] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

Relevance: 85.0

TL;DR: AnomSeer：一种基于多模态大语言模型的时间序列异常检测方法，通过专家思维链和时序基础策略优化，在异常分类、定位和解释方面超越GPT-4o等大型商业模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLMs的时间序列异常检测方法依赖粗粒度启发式方法，难以进行多维度的细致推理，而理解复杂时间序列数据需要精确的结构化细节分析。

Method: 1) 生成专家思维链轨迹，提供基于经典分析（统计度量、频率变换）的可验证细粒度推理；2) 提出时序基础策略优化(TimerPO)，包含基于最优传输的时序基础优势和正交投影组件，确保辅助细粒度信号不干扰主要检测目标。

Result: 使用Qwen2.5-VL-3B/7B-Instruct的AnomSeer在多种异常场景下，在分类和定位准确率上超越GPT-4o等大型商业基线，特别是在点和频率驱动的异常检测方面表现突出，并能生成支持其结论的合理时间序列推理轨迹。

Conclusion: 通过将模型推理建立在时间序列的精确结构化细节上，AnomSeer成功统一了异常分类、定位和解释，为MLLMs在时间序列分析中的细粒度推理能力提供了有效解决方案。

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [669] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文开发了自动红队管道来生成欺骗策略，系统性地压力测试对齐审计方法，发现当前黑白盒方法在面对有能力的错位模型时都不够鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有对齐审计方法虽然旨在检测战略性、情境感知的错位模型的隐藏目标，但尚未系统性地针对欺骗策略进行压力测试。研究者希望填补这一空白，评估当前审计方法在面对真正有能力的错位模型时的鲁棒性。

Method: 开发自动红队管道，为特定白盒和黑盒审计方法生成定制的欺骗策略（以系统提示形式）。使用秘密保持模型生物，对助手预填充、用户角色采样、稀疏自编码器和令牌嵌入相似性等方法进行压力测试。

Result: 自动红队管道找到了能够欺骗黑盒和白盒方法的提示，使这些方法做出自信但错误的猜测。这是首次记录到基于激活的战略性欺骗证据，表明当前审计方法在面对有能力的错位模型时不够鲁棒。

Conclusion: 当前的黑盒和白盒对齐审计方法在面对战略性、情境感知的错位模型时存在严重漏洞，需要开发更鲁棒的审计技术来应对未来更强大的AI系统。

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [670] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Gated Subspace Steering (GSS)方法，通过探测-引导机制选择性缓解LLM的记忆问题，相比现有方法计算效率提升100-1000倍


<details>
  <summary>Details</summary>
Motivation: 现有缓解LLM记忆训练数据的方法通常均匀干预所有token，这会降低模型在正常泛化token上的性能。作者发现记忆是稀疏、间歇且token依赖的，需要上下文感知的干预而非静态参数修改。

Method: 提出Gated Subspace Steering (GSS)方法，将干预分解为：1) 探测模块检测与记忆相关的激活；2) 引导模块仅在探测超过阈值时应用针对性修正。该方法基于最优子空间引导的优化框架。

Result: 在四个基准测试上，GSS达到或超过最先进的记忆减少效果，同时计算需求比基于优化的替代方法少100-1000倍。还提供了关于神经网络表示中记忆几何结构的新理论见解。

Conclusion: 记忆是稀疏且上下文依赖的，需要选择性干预而非均匀处理。GSS提供了一种高效、有原则的方法来缓解LLM记忆问题，同时保持泛化性能。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [671] [DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce](https://arxiv.org/abs/2602.08923)
*Wenchen Han,Shay Vargaftik,Michael Mitzenmacher,Ran Ben Basat*

Main category: cs.LG

Relevance: 85.0

TL;DR: DynamiQ是一个针对多跳全归约通信的量化框架，通过优化部分和的表示和融合内核设计，在保持精度的同时显著加速大规模模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着训练规模扩大，网络通信成为瓶颈。现有梯度量化系统未针对多跳聚合优化，其中条目沿聚合拓扑被部分求和多次，导致效率不足。

Method: 提出DynamiQ框架，包含：1）优化部分和表示的新技术；2）解压缩-累加-再压缩融合内核设计；3）扩展PyTorch DDP以支持NCCL P2P上的DynamiQ。

Result: 在不同LLM、任务和规模上，相比Omni-Reduce、THC、MXFP4/6/8等最先进方法，DynamiQ实现最高34.2%的性能提升，且是唯一能持续达到接近基线精度（如BF16基线的99.9%）的方法。

Conclusion: DynamiQ成功弥合了量化最佳实践与多跳聚合之间的差距，在显著加速训练的同时保持高精度，为大规模模型训练提供了高效的通信解决方案。

Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>


### [672] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

Relevance: 85.0

TL;DR: StealthRL是一个基于强化学习的对抗性评估框架，通过训练Qwen3-4B的LoRA适配器生成语义保持的对抗性改写，成功攻击多种AI文本检测器，揭示了当前检测器在鲁棒性方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本检测器面临对抗性改写攻击的鲁棒性挑战，攻击者可以在保持语义的同时逃避检测。现有检测器在真实对抗条件下的鲁棒性评估不足，需要建立系统性的对抗性评估协议来揭示检测器的脆弱性。

Method: 采用强化学习框架，使用Qwen3-4B模型配合LoRA适配器训练改写策略。采用Group Relative Policy Optimization (GRPO)优化复合奖励函数，平衡检测器逃避和语义保持。针对多种检测器家族（RoBERTa、FastDetectGPT、Binoculars）进行六种攻击设置评估。

Result: StealthRL在1%误报率下达到接近零的检测率（0.001平均TPR），将平均AUROC从0.74降至0.27，攻击成功率99.9%。攻击能够迁移到训练中未见过的检测器家族，揭示了共享的架构脆弱性而非特定检测器的缺陷。

Conclusion: 当前AI文本检测器存在严重的鲁棒性缺陷，StealthRL提供了一个原则性的对抗性评估协议，揭示了检测器架构的共享脆弱性，为未来更鲁棒的检测器设计提供了重要见解。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [673] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Dirichlet-Routed MoE (DirMoE)，一种基于Dirichlet变分自编码器的端到端可微分路由机制，将专家选择和专家贡献分配解耦，通过Bernoulli组件建模专家选择，Dirichlet组件处理专家贡献分配。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型通常依赖不可微的Top-k+Softmax路由机制，限制了性能和可扩展性。标准Top-k+Softmax将两个不同的决策（激活哪些专家、如何分配专家贡献）混在一起，需要更灵活、可微的路由方案。

Method: 基于Dirichlet变分自编码器框架，使用Bernoulli组件建模专家选择（通过Gumbel-Sigmoid松弛实现可微），Dirichlet组件处理专家贡献分配（通过隐式重参数化）。训练目标为变分ELBO，包含直接稀疏性惩罚以精确控制期望活跃专家数，并采用超参数调度策略引导模型从探索性路由转向确定性路由。

Result: DirMoE路由机制在匹配或超越其他方法的同时，提高了专家专业化程度。整个前向传播过程保持完全可微。

Conclusion: DirMoE通过解耦专家选择和贡献分配，提供了一种端到端可微的路由机制，解决了现有Top-k+Softmax路由的限制，提高了MoE模型的性能和可扩展性。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [674] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

Relevance: 85.0

TL;DR: ARO是一种新的矩阵优化框架，通过梯度旋转作为核心设计原则，在旋转坐标系中执行范数最速下降，超越了现有的正交化/白化方法，显著提升LLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 虽然基于正交化/白化的矩阵优化器在LLM训练效率方面取得了显著进展，但研究人员希望探索超越正交化的新范式，进一步推动效率边界。

Method: ARO将梯度旋转作为首要设计原则，在旋转坐标系中执行范数最速下降，旋转由新颖的范数感知策略确定。该方法超越了现有的正交化和白化优化器，并提出了严格的基准测试协议以减少混淆和偏差。

Result: 在严格控制的基准测试下，ARO在LLM预训练中一致优于AdamW（1.3-1.35倍）和正交化方法（1.1-1.15倍），参数规模达80亿激活参数，训练预算达8倍，且未显示收益递减迹象。

Conclusion: ARO为LLM训练优化提供了超越正交化的新范式，通过梯度旋转显著提升样本效率。该方法还可重新表述为基于残差流旋转对称性的对称感知优化器，为利用跨层/跨模块耦合的计算高效设计提供了动机。

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [675] [ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling](https://arxiv.org/abs/2602.09009)
*Yilang Zhang,Bingcong Li,Niao He,Georgios B. Giannakis*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出自适应神经连接重分配（ANCRe）框架，通过优化残差连接布局来提升深度网络收敛效率和性能，在LLM、扩散模型和ResNet上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 深度网络是现代基础模型成功的关键，但研究发现深层往往未被充分利用。传统残差连接作为默认的加深机制，其布局对收敛行为有根本性影响，甚至会导致收敛速度的指数级差异。

Method: 提出自适应神经连接重分配（ANCRe）框架：1）从优化角度分析残差连接布局对收敛的影响；2）参数化并学习残差连接性；3）以可忽略的计算和内存开销（<1%）自适应重分配残差连接。

Result: 在大语言模型预训练、扩散模型和深度ResNet上的广泛测试表明：1）相比传统残差连接，持续加速收敛；2）提升性能；3）增强深度效率。

Conclusion: 残差连接布局对深度网络优化至关重要，ANCRe框架能够自适应优化连接模式，更有效地利用网络深度，为深度网络架构设计提供了新思路。

Abstract: Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>


### [676] [Discrete Adjoint Matching](https://arxiv.org/abs/2602.07132)
*Oswin So,Brian Karrer,Chuchu Fan,Ricky T. Q. Chen,Guan-Horng Liu*

Main category: stat.ML

Relevance: 85.0

TL;DR: 提出离散伴随匹配（DAM）方法，用于在离散状态空间（如扩散大语言模型）中解决熵正则化奖励优化问题，克服了连续方法在离散生成模型中的不可微挑战。


<details>
  <summary>Details</summary>
Motivation: 伴随匹配（AM）方法在连续状态空间和可微奖励的熵正则化奖励优化中表现优异，但将其应用于离散生成模型（如扩散大语言模型）面临挑战，因为离散状态空间不可微。需要开发适用于离散生成模型的优化方法。

Method: 提出离散伴随匹配（DAM），通过统计视角而非控制理论视角，引入离散伴随估计器，将标准匹配框架应用于离散域。该方法适用于连续时间马尔可夫链描述的离散生成模型。

Result: 在合成任务和数学推理任务上展示了DAM的有效性，证明了该方法在离散生成模型微调中的实用性。

Conclusion: DAM成功将伴随匹配方法扩展到离散生成模型，为离散状态空间中的熵正则化奖励优化提供了有效解决方案，并为一般伴随基估计器开辟了新的算法机会。

Abstract: Computation methods for solving entropy-regularized reward optimization -- a class of problems widely used for fine-tuning generative models -- have advanced rapidly. Among those, Adjoint Matching (AM, Domingo-Enrich et al., 2025) has proven highly effective in continuous state spaces with differentiable rewards. Transferring these practical successes to discrete generative modeling, however, remains particularly challenging and largely unexplored, mainly due to the drastic shift in generative model classes to discrete state spaces, which are nowhere differentiable. In this work, we propose Discrete Adjoint Matching (DAM) -- a discrete variant of AM for fine-tuning discrete generative models characterized by Continuous-Time Markov Chains, such as diffusion-based large language models. The core of DAM is the introduction of discrete adjoint-an estimator of the optimal solution to the original problem but formulated on discrete domains-from which standard matching frameworks can be applied. This is derived via a purely statistical standpoint, in contrast to the control-theoretic viewpoint in AM, thereby opening up new algorithmic opportunities for general adjoint-based estimators. We showcase DAM's effectiveness on synthetic and mathematical reasoning tasks.

</details>


### [677] [The Value of Variance: Mitigating Debate Collapse in Multi-Agent Systems via Uncertainty-Driven Policy Optimization](https://arxiv.org/abs/2602.07186)
*Luoxi Tang,Yuqiao Meng,Joseph Costa,Yingxue Zhang,Muchao Ye,Zhaohan Xi*

Main category: cs.MA

Relevance: 85.0

TL;DR: 该论文提出了一种量化多智能体辩论系统不确定性的分层度量方法，并基于此开发了不确定性驱动的策略优化来防止辩论崩溃，提高系统决策准确性。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论系统通过迭代讨论提升LLM推理能力，但容易发生"辩论崩溃"——最终决策基于错误推理。现有方法缺乏检测或预防此类故障的原则性机制。

Method: 1) 提出分层不确定性度量：智能体内不确定性（个体推理）、智能体间不确定性（交互）、系统级不确定性（输出）；2) 基于不确定性驱动策略优化，在动态辩论环境中惩罚自我矛盾、同伴冲突和低置信度输出。

Result: 实验表明：1) 提出的不确定性量化能可靠指示系统故障；2) 不确定性驱动缓解策略能可靠校准多智能体系统，持续提高决策准确性，同时减少系统分歧。

Conclusion: 不确定性量化可作为多智能体辩论系统的有效诊断指标，而基于不确定性的策略优化能有效防止辩论崩溃，提升系统可靠性和决策质量。

Abstract: Multi-agent debate (MAD) systems improve LLM reasoning through iterative deliberation, but remain vulnerable to debate collapse, a failure type where final agent decisions are compromised on erroneous reasoning. Existing methods lack principled mechanisms to detect or prevent such failures. To address this gap, we first propose a hierarchical metric that quantifies behavioral uncertainty at three levels: intra-agent (individual reasoning uncertainty), inter-agent (interactive uncertainty), and system-level (output uncertainty). Empirical analysis across several benchmarks reveals that our proposed uncertainty quantification reliably indicates system failures, which demonstrates the validity of using them as diagnostic metrics to indicate the system failure. Subsequently, we propose a mitigation strategy by formulating an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs in a dynamic debating environment. Experiments demonstrate that our proposed uncertainty-driven mitigation reliably calibrates the multi-agent system by consistently improving decision accuracy while reducing system disagreement.

</details>


### [678] [Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization](https://arxiv.org/abs/2602.07306)
*Chong Wang,Nan Du,Tom Gunter,Tao Lei,Kulin Seth,Senyu Tong,Jianyu Wang,Guoli Yin,Xiyou Zhou,Kelvin Zou,Ruoming Pang*

Main category: cs.DC

Relevance: 85.0

TL;DR: 提出Parallel Track (PT) Transformer架构，通过重构计算减少跨设备同步操作，相比标准张量并行减少16倍同步操作，在LLM推理中提升服务效率


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer LLM推理面临多GPU并行时的系统挑战，传统张量并行引入大量GPU间同步操作，导致通信瓶颈和可扩展性下降

Method: 提出Parallel Track Transformer架构范式，重新组织计算结构以最小化跨设备依赖，减少同步操作，并集成到Tensor-RT-LLM和vLLM两个主流LLM服务框架中

Result: 相比标准张量并行减少16倍同步操作，在LLM服务中实现：首token时间减少15-30%，每个输出token时间减少2-12%，吞吐量提升最高31.90%

Conclusion: PT Transformer通过减少跨设备同步有效解决了LLM大规模推理中的通信瓶颈，显著提升了服务效率，同时保持模型质量竞争力

Abstract: Efficient large-scale inference of transformer-based large language models (LLMs) remains a fundamental systems challenge, frequently requiring multi-GPU parallelism to meet stringent latency and throughput targets. Conventional tensor parallelism decomposes matrix operations across devices but introduces substantial inter-GPU synchronization, leading to communication bottlenecks and degraded scalability. We propose the Parallel Track (PT) Transformer, a novel architectural paradigm that restructures computation to minimize cross-device dependencies. PT achieves up to a 16x reduction in synchronization operations relative to standard tensor parallelism, while maintaining competitive model quality in our experiments. We integrate PT into two widely adopted LLM serving stacks-Tensor-RT-LLM and vLLM-and report consistent improvements in serving efficiency, including up to 15-30% reduced time to first token, 2-12% reduced time per output token, and up to 31.90% increased throughput in both settings.

</details>


### [679] [CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution](https://arxiv.org/abs/2602.07918)
*Minbeom Kim,Mihir Parmar,Phillip Wallis,Lesly Miculicich,Kyomin Jung,Krishnamurthy Dj Dvijotham,Long T. Le,Tomas Pfister*

Main category: cs.CR

Relevance: 85.0

TL;DR: CausalArmor：基于因果归因的选择性防御框架，通过检测不信任内容对AI代理决策的支配性影响，仅在存在实际威胁时触发针对性净化，避免过度防御问题。


<details>
  <summary>Details</summary>
Motivation: 现有间接提示注入（IPI）防御存在过度防御困境——无论实际威胁如何都部署昂贵的持续净化，导致在良性场景下降低效用和增加延迟。需要一种选择性防御方法，仅在检测到实际威胁时触发净化。

Method: 1) 从因果消融视角分析IPI：成功注入表现为支配性转移，即用户请求不再对代理特权动作提供决定性支持，而特定不信任片段提供不成比例的归因影响；2) 提出CausalArmor框架：在特权决策点计算轻量级的留一消融归因；3) 仅当不信任片段支配用户意图时触发针对性净化；4) 采用追溯性思维链掩码防止代理基于"中毒"推理轨迹行动。

Result: 在AgentDojo和DoomArena上的实验表明，CausalArmor在安全性上匹配激进防御方法，同时提高可解释性，并保持AI代理的效用和延迟性能。

Conclusion: 基于因果归因的选择性防御框架CausalArmor能够有效防御间接提示注入攻击，避免过度防御问题，在安全性和实用性之间取得良好平衡。

Abstract: AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation-based attributions at privileged decision points, and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.

</details>


### [680] [A Statistical Framework for Alignment with Biased AI Feedback](https://arxiv.org/abs/2602.08259)
*Xintao Xia,Zhiqiu Xia,Linjun Zhang,Zhanrui Cai*

Main category: stat.ML

Relevance: 85.0

TL;DR: 论文提出了两种去偏的对齐方法：DDPO和DIPO，用于解决LLM作为评判者时产生的系统性偏差问题，在保持计算效率的同时提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现代对齐流程越来越多地使用大语言模型作为评判者（LLM-as-Judge）来替代昂贵的人工偏好标注，但AI标签相比高质量人类反馈数据集可能存在系统性偏差。需要开发能够缓解这种偏差的对齐方法。

Method: 提出了两种去偏对齐方法：1) DDPO：在标准DPO基础上增加残差校正和密度比重加权来缓解系统性偏差；2) DIPO：直接估计人类偏好概率而不强加参数化奖励模型。两种方法都考虑了异构的提示-响应分布和外部人类反馈源。

Result: 在情感生成、摘要和单轮对话任务上的实证研究表明，所提方法显著提升了对齐效率，并且性能接近使用完全人工标注数据训练的oracle模型。

Conclusion: DDPO为大规模对齐提供了实用且计算高效的解决方案，而DIPO作为稳健的统计最优替代方案，达到了半参数效率边界。两种方法都能有效缓解LLM作为评判者时的系统性偏差问题。

Abstract: Modern alignment pipelines are increasingly replacing expensive human preference labels with evaluations from large language models (LLM-as-Judge). However, AI labels can be systematically biased compared to high-quality human feedback datasets. In this paper, we develop two debiased alignment methods within a general framework that accommodates heterogeneous prompt-response distributions and external human feedback sources. Debiased Direct Preference Optimization (DDPO) augments standard DPO with a residual-based correction and density-ratio reweighting to mitigate systematic bias, while retaining DPO's computational efficiency. Debiased Identity Preference Optimization (DIPO) directly estimates human preference probabilities without imposing a parametric reward model. We provide theoretical guarantees for both methods: DDPO offers a practical and computationally efficient solution for large-scale alignment, whereas DIPO serves as a robust, statistically optimal alternative that attains the semiparametric efficiency bound. Empirical studies on sentiment generation, summarization, and single-turn dialogue demonstrate that the proposed methods substantially improve alignment efficiency and recover performance close to that of an oracle trained on fully human-labeled data.

</details>


### [681] [AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders](https://arxiv.org/abs/2602.08837)
*Minh-Duc Nguyen,Hai-Dang Kieu,Dung D. Le*

Main category: cs.IR

Relevance: 85.0

TL;DR: AMEM4Rec：一种基于LLM的智能推荐系统，通过跨用户记忆演化学习协同过滤信号，无需预训练CF模型，在端到端方式中解决LLM参数效率低、上下文限制和幻觉风险等问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能推荐系统面临三个主要挑战：1）微调LLM参数效率低；2）基于提示的智能推理受上下文长度限制且有幻觉风险；3）现有系统主要依赖语义知识而忽略了协同过滤（CF）信号，而CF对隐式偏好建模至关重要。

Method: 提出AMEM4Rec系统，通过跨用户记忆演化在端到端方式中学习协同过滤信号。系统将用户历史行为模式存储在全局记忆池中，记忆与相似现有记忆链接并迭代演化，以增强跨用户共享模式，使系统能够感知CF信号而无需依赖预训练CF模型。

Result: 在Amazon和MIND数据集上的广泛实验表明，AMEM4Rec始终优于最先进的基于LLM的推荐系统，证明了演化记忆引导的协同过滤的有效性。

Conclusion: AMEM4Rec通过记忆演化机制成功地将协同过滤信号整合到基于LLM的智能推荐系统中，解决了现有方法的局限性，为LLM在推荐系统中的应用提供了新的有效途径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.

</details>


### [682] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出了Neural Sabermetrics with World Model，一个基于LLM的棒球逐场比赛世界模型，将棒球比赛建模为事件的自回归序列，在MLB跟踪数据上持续预训练，能够预测比赛演化的多个方面。


<details>
  <summary>Details</summary>
Motivation: 传统棒球统计指标虽然对评估和回顾分析有价值，但无法定义逐球展开的生成模型，现有方法大多局限于单步预测或事后分析。需要建立一个能够模拟棒球比赛逐球演化的世界模型。

Method: 将棒球比赛建模为事件的长自回归序列，在超过10年MLB跟踪数据（700万次投球序列，约30亿token）上持续预训练单个LLM，构建统一的预测框架。

Result: 模型在分布内常规赛数据和分布外季后赛数据上均表现优异：正确预测约64%的下一次投球（在击球过程中）和78%的击球手挥棒决策，优于现有神经基线。

Conclusion: LLM可以作为体育领域的有效世界模型，单个骨干模型能够统一预测比赛演化的多个方面，为棒球分析提供了新的生成建模方法。

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [683] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出EmoReAlM基准测试评估多模态大语言模型在情感理解中的虚假关联和幻觉问题，并开发AVEm-DPO偏好优化方法提升模型性能


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解任务中存在两个关键挑战：1）情感与无关视听线索之间的虚假关联；2）语言模型主干中的文本先验驱动的视听线索幻觉。需要量化这些问题并开发改进方法。

Method: 1）提出EmoReAlM基准测试，专门评估MLLMs的线索-情感关联、幻觉和模态一致性；2）开发AVEm-DPO偏好优化技术，通过构建对虚假关联/幻觉响应的偏好，以及基于文本提示的视听输入对来对齐模型响应；3）引入正则化项惩罚对文本先验的依赖，减轻模态特定线索幻觉。

Result: 在DFEW、RAVDESS和EMER数据集上的实验表明，该方法显著提升基线模型性能，在零样本设置下获得6-19%的相对性能提升。

Conclusion: 通过提供严格的基准测试和鲁棒的优化框架，这项工作为情感理解和社会AI领域的MLLMs评估和改进提供了原则性方法。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [684] [Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis](https://arxiv.org/abs/2602.07135)
*Jiaqing Chen,Nicholas Hadler,Tiankai Xie,Rostyslav Hnatyshyn,Caleb Geniesse,Yaoqing Yang,Michael W. Mahoney,Talita Perciano,John F. Hartwig,Ross Maciejewski,Gunther H. Weber*

Main category: cs.LG

Relevance: 75.0

TL;DR: Landscaper是一个开源Python包，用于任意维度的损失函数景观分析，结合Hessian子空间构建和拓扑数据分析，揭示几何结构如盆地层次和连通性，并提出SMAD指标量化景观平滑度。


<details>
  <summary>Details</summary>
Motivation: 传统低维损失函数景观分析经常错过复杂的拓扑特征，需要更全面的工具来理解神经网络优化和泛化特性，特别是在预训练语言模型等复杂架构中。

Method: 开发Landscaper开源包，结合Hessian-based子空间构建和拓扑数据分析方法，提出Saddle-Minimum Average Distance (SMAD)指标量化损失景观平滑度。

Result: Landscaper在多种架构和任务中有效，包括预训练语言模型，SMAD能捕捉传统指标错过的训练转换（如景观简化），在化学性质预测任务中可作为分布外泛化指标。

Conclusion: Landscaper为模型诊断和架构设计提供有价值的洞察，特别是在数据稀缺的科学机器学习场景中，有助于理解复杂神经网络的优化和泛化特性。

Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper combines Hessian-based subspace construction with topological data analysis to reveal geometric structures such as basin hierarchy and connectivity. A key component is the Saddle-Minimum Average Distance (SMAD) for quantifying landscape smoothness. We demonstrate Landscaper's effectiveness across various architectures and tasks, including those involving pre-trained language models, showing that SMAD captures training transitions, such as landscape simplification, that conventional metrics miss. We also illustrate Landscaper's performance in challenging chemical property prediction tasks, where SMAD can serve as a metric for out-of-distribution generalization, offering valuable insights for model diagnostics and architecture design in data-scarce scientific machine learning scenarios.

</details>


### [685] [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*Zhiqi Bu,Shiyun Xu,Jialin Mao*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究了深度学习优化中的凸性现象，发现深度学习在短期训练后会呈现弱凸性，并基于此建立了学习率和损失的缩放定律，能够跨训练时长和模型规模进行大幅外推。


<details>
  <summary>Details</summary>
Motivation: 深度学习具有非凸损失面，其优化动态难以分析或控制。然而，在各种任务、模型、优化器和超参数下，优化动态在经验上表现出类似凸性的特征。本研究旨在探讨凸性和Lipschitz连续性在深度学习中的适用性，以通过学习率调度精确控制损失动态。

Method: 通过分析深度学习优化动态，发现深度学习在短期训练后会变得弱凸，损失可以通过最后迭代的上界来预测，这进一步指导了最优学习率的缩放。从凸性视角出发，建立了学习率和损失的缩放定律。

Result: 建立了能够跨训练时长80倍和模型规模70倍外推的学习率和损失缩放定律。展示了深度学习优化中的凸性现象，为学习率调度提供了理论基础。

Conclusion: 深度学习优化动态表现出可预测的凸性特征，这为精确控制学习率调度和优化过程提供了理论基础，建立的缩放定律能够大幅外推，对大规模模型训练具有实际指导意义。

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

</details>


### [686] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种受小脑启发的推理时残差控制框架，通过在线校正动作增强冻结的强化学习策略，实现故障恢复而无需修改基础策略参数。


<details>
  <summary>Details</summary>
Motivation: 机器人策略在真实世界部署中常遇到训练后故障，而重新训练、探索或系统辨识通常不切实际。需要一种推理时干预方法，在不修改基础策略的情况下实现故障恢复。

Method: 受小脑启发的残差控制框架：1）通过固定特征扩展实现高维模式分离；2）并行微区式残差通路；3）具有兴奋性和抑制性资格迹的局部误差驱动可塑性，在不同时间尺度上运行；4）性能驱动的元适应机制调节残差权限和可塑性。

Result: 在MuJoCo基准测试中，在驱动器、动态和环境扰动下，HalfCheetah-v5性能提升达+66%，Humanoid-v5提升达+53%。在严重偏移下表现优雅退化，并能将持久残差校正整合到策略参数中提供互补鲁棒性。

Conclusion: 该框架成功将小脑计算原理应用于机器人故障恢复，提供了一种推理时自适应机制，能在不破坏基础策略的情况下实现有效的故障恢复和鲁棒性增强。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [687] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究了在充分性公平约束下的最优二元分类问题，提出了基于群体校准分数的几何特征化方法，并开发了简单的后处理算法来实现最优分类器。


<details>
  <summary>Details</summary>
Motivation: 在监督机器学习中，基于预测概率的二元分类是基础任务。虽然阈值处理在无约束情况下是贝叶斯最优的，但使用单一阈值通常会违反统计群体公平约束。在充分性（预测奇偶性）约束下，即使完美群体校准的分数（包括真实类别概率）在阈值处理后也会违反预测奇偶性。因此需要研究在充分性约束下的最优分类方法。

Method: 1. 假设有限组群体校准分数，提出了在充分性约束下最优二元（随机化）分类的精确解
2. 提供了可实现的正预测值（PPV）和假遗漏率（FOR）对的几何特征化
3. 开发了简单的后处理算法，仅使用群体校准分数和群体成员身份即可获得最优分类器
4. 识别了在满足充分性条件下最小化与分离性偏差的分类器

Result: 1. 提供了在充分性约束下最优分类的精确解
2. 提出的算法能够获得最优分类器
3. 当充分性和分离性不兼容时，算法也能获得在满足充分性条件下最小化与分离性偏差的分类器，通常能达到与最优解相当的性能

Conclusion: 该论文解决了在充分性公平约束下的二元分类问题，通过几何特征化和简单后处理算法实现了最优分类。即使在充分性和分离性不兼容的情况下，也能找到合理的折衷方案，为公平机器学习提供了实用的解决方案。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [688] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

Relevance: 75.0

TL;DR: Astro是一个激活引导的结构化正则化框架，用于解决LLM仅权重量化中的异常值问题，通过重构鲁棒权重来抑制异常值，不增加推理延迟


<details>
  <summary>Details</summary>
Motivation: LLM仅权重量化（PTQ）因权重和激活异常值导致精度下降。现有方法要么抑制不足，要么带来推理延迟、预处理负担或复杂算子融合等部署效率问题

Method: 利用LLM收敛到平坦最小值的特性，提出激活引导的结构化正则化框架Astro，通过激活引导的正则化目标主动重构鲁棒权重，抑制高幅度激活对应的权重异常值

Result: Astro在LLaMA-2-7B上表现优于复杂的基于学习的旋转方法，量化时间仅为其1/3，且不增加推理延迟，与主流量化方法（如GPTQ）正交

Conclusion: Astro有效解决了LLM仅权重量化中的异常值问题，实现了硬件友好且高效的异常值抑制，在保持精度的同时不增加推理开销

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [689] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种通过学习连续程序空间来改进遗传编程的方法，其中潜在距离具有行为意义，并设计利用此结构的变异算子，显著提高搜索效率而不改变进化算法本身。


<details>
  <summary>Details</summary>
Motivation: 遗传编程虽然能产生可解释程序，但小的语法变异可能导致大的、不可预测的行为变化，降低局部性和样本效率。这被视为算子设计问题：需要学习一个连续程序空间，使潜在距离具有行为意义，然后设计利用此结构的变异算子。

Method: 1) 通过学习紧凑交易策略DSL的块因子化嵌入，创建连续程序空间；2) 通过跟踪受控潜在扰动下的动作级分歧来测量局部性；3) 比较各向同性高斯变异与几何编译变异，后者将更新限制在语义配对的进入-退出子空间，并使用基于流的模型提出方向；4) 在相同(μ+λ)进化策略和固定评估预算下进行实验。

Result: 在五个资产上，学习到的变异算子使用少一个数量级的评估发现强策略，并实现最高的中位数样本外夏普比率。虽然各向同性变异偶尔能达到更高的峰值性能，但几何编译变异产生更快、更可靠的进展。

Conclusion: 语义对齐的变异可以显著提高搜索效率，而无需修改底层进化算法。这为遗传编程中的算子设计提供了新方向，通过利用学习到的程序空间结构来改善局部性和样本效率。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [690] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出Laplacian Keyboard (LK)框架，利用拉普拉斯特征向量构建任务无关的行为基元库，通过元策略动态组合这些基元，实现超出线性约束的高效强化学习。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯特征向量在强化学习中通常只用于线性近似奖励函数，这限制了在复杂环境中的表达能力。需要超越线性约束的方法来提升表达能力和学习效率。

Method: 提出分层框架Laplacian Keyboard：1) 从拉普拉斯特征向量构建任务无关的选项库作为行为基元；2) 保证该基元库包含线性span内任意奖励的最优策略；3) 训练元策略动态组合这些选项，实现超出线性约束的策略学习。

Result: 理论分析给出了零样本近似误差的上界。实验表明LK超越了零样本解决方案，同时相比标准RL方法实现了更好的样本效率。

Conclusion: LK框架成功超越了拉普拉斯特征向量的线性约束限制，通过构建行为基元库和动态组合机制，实现了更强大的表达能力和学习效率。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [691] [Riemannian MeanFlow](https://arxiv.org/abs/2602.07744)
*Dongyeop Woo,Marta Skreta,Seonghyun Park,Sungsoo Ahn,Kirill Neklyudov*

Main category: cs.LG

Relevance: 75.0

TL;DR: Riemannian MeanFlow (RMF) 是一种在流形上直接学习流映射的框架，只需一次前向传播即可生成高质量样本，相比扩散模型减少10倍计算量，适用于蛋白质和DNA序列设计。


<details>
  <summary>Details</summary>
Motivation: 当前在黎曼流形上的扩散模型和流模型在推理时需要数十到数百次神经网络评估，这在大规模科学采样工作流中成为计算瓶颈。需要一种更高效的生成方法。

Method: 提出 Riemannian MeanFlow (RMF) 框架，在流形上直接学习流映射。推导了流形平均速度的三种等价表征（欧拉、拉格朗日和半群恒等式），并分析了参数化和稳定化技术以改进高维流形上的训练。

Result: 在启动子DNA设计和蛋白质骨架生成任务中，RMF 实现了与先前方法相当的样本质量，同时需要最多10倍更少的函数评估。少步流映射还能通过奖励前瞻实现高效的奖励引导设计。

Conclusion: RMF 提供了一种在流形上进行高效生成建模的新范式，显著减少了推理计算成本，同时保持了样本质量，适用于大规模科学应用。

Abstract: Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.

</details>


### [692] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

Relevance: 75.0

TL;DR: MaD-Mix是一个用于视觉语言模型训练的多模态数据混合框架，通过模态感知的领域对齐最大化来自动优化数据混合比例，无需人工调优。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型训练依赖昂贵的人工调优来确定多模态数据混合比例，特别是在处理复杂多模态场景（如视频-图像-文本）时，人工调优变得不切实际。需要一种原则性的、计算高效的方法来自动优化数据混合。

Method: MaD-Mix将数据混合问题形式化为模态感知的领域对齐最大化，通过Fenchel对偶获得闭式多模态对齐分数，使用跨模态耦合变量，并能系统处理缺失模态的领域（如纯语言领域）。

Result: 在0.5B和7B模型上的实验表明，MaD-Mix能加速VLM训练，在图像-文本指令调优中比人工调优节省22%的训练步数，在复杂的三模态视频-图像-文本场景中，相比均匀权重显著提升平均准确率，且混合计算开销极小（<1 GPU小时）。

Conclusion: MaD-Mix提供了一个可扩展的数据混合设计框架，能够高效优化多模态数据混合，减少对人工调优的依赖，特别适用于复杂的多模态场景，为现代VLM训练流程提供了实用的解决方案。

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [693] [Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models](https://arxiv.org/abs/2602.07904)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

Relevance: 75.0

TL;DR: LMABO：使用预训练大语言模型作为零样本在线策略师，动态选择贝叶斯优化中的采集函数，在50个基准问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化的性能高度依赖于采集函数的选择，但不存在普遍最优的单一策略。现有自适应组合方法主要依赖历史函数值，忽略了剩余预算、代理模型特征等丰富信息，需要更智能的适应性策略。

Method: 提出LMABO框架，将预训练LLM作为零样本在线策略师。在每次迭代中，使用结构化状态表示（包括优化进度、剩余预算、代理模型特征等）提示LLM从多样化组合中选择最合适的采集函数。

Result: 在50个基准问题上评估，LMABO显著优于静态策略、自适应组合方法和其他基于LLM的基线。LLM能够根据实时进展调整策略，证明其优势源于处理并综合完整优化状态形成有效自适应策略的能力。

Conclusion: LMABO成功将LLM作为智能策略师应用于贝叶斯优化，通过结构化状态表示使LLM能够理解优化过程并做出适应性决策，为优化算法设计提供了新思路。

Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.

</details>


### [694] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出了一个统一的概率空间框架，将基于控制的奖励适应和流模型合并统一起来，支持奖励引导的流合并，并提供了理论保证和应用验证。


<details>
  <summary>Details</summary>
Motivation: 大规模流和扩散模型的发展带来了两个基本算法挑战：1）预训练流的基于控制的奖励适应；2）多个模型的集成（流合并）。当前方法分别处理这两个问题，需要统一的框架来同时解决这两个挑战。

Method: 提出了一个统一的概率空间框架，将奖励适应和流合并作为极限情况包含在内。引入了奖励引导流合并（RFM），这是一个镜像下降方案，将奖励引导流合并简化为一系列标准微调问题。框架支持生成模型密度的丰富操作符家族，包括交集、并集、插值及其奖励引导对应物。

Result: 为奖励引导和纯流合并提供了首个理论保证。在说明性设置中展示了方法的可视化解释能力，并应用于高维从头分子设计和低能量构象生成。

Conclusion: 提出了一个统一的概率空间框架，成功解决了流模型的奖励适应和合并问题，为生成模型的组合操作提供了理论基础和实用方法。

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [695] [Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense](https://arxiv.org/abs/2602.09012)
*Jiacheng Liu,Yaxin Luo,Jiacheng Cui,Xinyi Shang,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出Next-Gen CAPTCHAs框架，针对当前GUI智能体已能破解传统验证码的问题，通过利用人机在交互感知、记忆、决策和行动上的"认知鸿沟"，设计需要适应性直觉而非精细规划的动态任务，为智能体时代提供可扩展的多样化防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着GUI智能体的快速发展，传统验证码已失效。像Gemini3-Pro-High和GPT-5.2-Xhigh这样的推理密集型模型在复杂逻辑谜题上达到90%通过率，使得现有安全屏障崩溃。需要新的防御机制来保护下一代网络免受高级智能体攻击。

Method: 提出Next-Gen CAPTCHAs框架，基于健壮的数据生成流水线构建基准测试，支持大规模可扩展评估。利用人机在交互感知、记忆、决策和行动上的"认知鸿沟"，设计需要适应性直觉而非精细规划的动态任务，特别是后端支持类型可以生成几乎无限的验证码实例。

Result: 建立了可扩展的防御框架，能够生成大规模验证码实例，通过动态任务重新建立生物用户与人工智能体之间的鲁棒区分，为智能体时代提供多样化的防御机制。

Conclusion: Next-Gen CAPTCHAs框架通过利用人机认知差异，设计需要适应性直觉的动态任务，有效应对高级GUI智能体的威胁，为下一代网络安全提供可扩展的解决方案。

Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

</details>


### [696] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出了一种基于稀疏感知进化（SAE）的模型融合框架，通过迭代的剪枝-融合循环作为新型变异算子，在进化过程中引入稀疏约束，提高模型融合的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前模型融合方法在大型语言模型上存在可靠性问题，需要更有效的融合策略来提升性能同时保持模型稀疏性。

Method: 采用稀疏感知进化框架，包含迭代的剪枝-融合循环作为变异算子，在评分函数中融入稀疏约束，引导进化过程偏好稀疏模型，同时考虑传统性能指标。

Result: 在多个大规模LLM基准测试中，该方法能够提高模型融合的可靠性，且由于简单性和与现有方法的正交性，易于集成。

Conclusion: 稀疏感知进化框架为模型融合提供了有效的解决方案，通过稀疏竞争机制引入额外的局部吸引和相互作用，提升了融合效果。

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [697] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出了一个用于多步马尔可夫决策过程的交互式基础学习算法，能够从间接反馈中学习个性化目标，解决了传统单步设置无法应用于多轮LLM部署等序列决策系统的问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式基础学习(IGL)研究局限于单步设置，无法应用于多轮决策系统（如多轮LLM对话）。现实场景中学习者通常只能获得由未知机制生成的间接反馈，而非显式数值奖励，这限制了现有方法在序列决策中的应用。

Method: 1. 将Zhang等人(2024a)的奖励估计器从单步扩展到多步MDP设置，解决MDP下潜在奖励解码的独特挑战；2. 基于该估计器设计逆间隙加权(IGW)算法进行策略优化；3. 在合成MDP和真实用户预订数据集上进行实验验证。

Result: 提出了计算高效的算法，在上下文情景MDP中实现了次线性遗憾保证，能够从多轮交互中有效学习个性化目标，在合成和真实数据集上都展示了有效性。

Conclusion: 成功将IGL扩展到多步序列决策设置，为多轮LLM部署等现实应用提供了理论基础和实用算法，解决了从间接反馈中学习个性化目标的挑战。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [698] [Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training](https://arxiv.org/abs/2602.08333)
*Cristian Pérez-Corral,Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究了ReLU神经网络训练中的两阶段动态：早期阶段激活模式变化显著，后期阶段权重更新主要在稳定的激活区域内进行细化


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络取得了经验成功，但其内部训练动态仍难以表征。研究旨在理解ReLU模型训练是否表现出两阶段行为：早期激活模式大幅变化，后期权重更新主要在稳定激活区域内细化模型

Method: 1. 理论证明局部稳定性：在参数和输入的测度零集外，足够小的参数扰动会保持固定输入的激活模式
2. 实证追踪：在完全连接、卷积和Transformer架构中，跟踪每次迭代的权重和激活模式变化，其中激活模式记录在ReLU前馈子模块中，使用固定验证子集

Result: 1. 理论证明了局部稳定性性质
2. 实证发现：在所有评估设置中，激活模式变化比权重更新幅度早3倍衰减，表明后期训练通常在相对稳定的激活区域内进行

Conclusion: 训练确实表现出两阶段行为，激活模式变化早期显著，后期稳定。这为监控训练动态提供了架构无关的具体工具，并激励进一步研究分段线性网络的解耦优化策略

Abstract: Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.

</details>


### [699] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

Relevance: 75.0

TL;DR: OJBKQ是一种层级的训练后量化方法，将权重量化建模为激活和权重的联合优化问题，通过扩展的Babai-Klein算法求解NP-hard的整数最小二乘问题，在3-4位量化下相比现有方法获得更低的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有训练后量化方法大多依赖启发式目标和贪心舍入策略，导致在低位量化时性能显著下降。需要一种更系统化的量化方法来减少性能损失。

Method: 将权重量化建模为激活和权重的联合优化问题，形成多右端项箱约束整数最小二乘问题。对权重矩阵的每一列应用扩展的Babai最近平面算法和Klein随机Babai算法，寻找最小残差的Babai-Klein点作为次优解。

Result: 在大语言模型上的实验表明，OJBKQ在3-4位量化下相比现有训练后量化方法获得更低的困惑度，同时保持相当的计算成本。

Conclusion: OJBKQ通过将量化问题形式化为联合优化问题并使用Babai-Klein算法求解，提供了一种有效的低位量化方法，在保持计算效率的同时显著提升量化性能。

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [700] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

Relevance: 75.0

TL;DR: SDFed是一个异构联邦提示学习框架，通过子空间细化和发散控制来解决本地-全局差异，在保护隐私的多方设置中实现更好的视觉语言预训练模型适应。


<details>
  <summary>Details</summary>
Motivation: 视觉语言预训练模型在隐私敏感的多方联邦学习环境中面临挑战：联邦优化通信成本高、客户端本地数据有限。现有联邦提示学习方法强制统一的提示结构和长度，无法应对实际客户端在数据分布和系统资源方面的异构性，可能引入全局共享与本地最优知识之间的冲突。

Method: SDFed采用异构联邦提示学习框架：1）保持固定长度的全局提示以高效聚合；2）允许每个客户端学习可变长度的本地提示以匹配其数据特性和容量；3）引入本地提示的子空间细化方法；4）设计信息保留和发散控制策略，在保持关键本地信息的同时维持全局与本地表示之间的适当分离。

Result: 在多个数据集上的广泛实验表明，SDFed在异构联邦设置中持续提升性能和鲁棒性。

Conclusion: SDFed通过解决本地-全局差异，在异构联邦环境中实现了更好的视觉语言预训练模型适应，为隐私敏感的多方学习提供了有效解决方案。

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [701] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

Relevance: 75.0

TL;DR: TFMLinker：首个基于表格基础模型（TFM）的通用链接预测方法，利用上下文学习能力实现跨数据集和领域的链接预测，无需特定数据集微调。


<details>
  <summary>Details</summary>
Motivation: 当前基于图神经网络和大型语言模型的图基础模型在通用链接预测中存在局限性：预训练规模有限或过度依赖文本信息。受表格基础模型（TFM）在跨数据集通用预测成功的启发，探索将TFM应用于链接预测，但面临如何获取必要上下文和捕捉链接拓扑信息的技术挑战。

Method: 1）原型增强的局部-全局上下文模块：构建同时捕捉图特定和跨图可迁移模式的上下文；2）通用拓扑感知链接编码器：捕捉链接中心拓扑信息并生成链接表示作为TFM输入；3）利用TFM通过上下文学习预测链接存在性。

Result: 在6个不同领域的图基准测试中，该方法优于最先进的基线方法，且无需特定数据集微调。

Conclusion: TFMLinker成功将表格基础模型应用于链接预测任务，展示了TFM在跨图数据集通用预测中的潜力，为图机器学习中的基础模型发展提供了新方向。

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [702] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出Distance-Guided Reinforcement Learning (DGRL)方法，结合Sampled Dynamic Neighborhoods (SDN)和Distance-Based Updates (DBU)，解决大型离散动作空间中的维度灾难问题，支持高达10^20个动作的空间。


<details>
  <summary>Details</summary>
Motivation: 强化学习在物流、调度和推荐系统等应用中面临大型离散动作空间的维度灾难问题。现有方法依赖网格结构或计算昂贵的最近邻搜索，限制了在高维或不规则结构领域的有效性。

Method: DGRL结合两个核心技术：1) SDN利用语义嵌入空间进行随机体积探索，在局部信任区域提供完整支持；2) DBU将策略优化转化为稳定回归任务，解耦梯度方差与动作空间基数，保证单调策略改进。方法自然泛化到混合连续-离散动作空间。

Result: 在规则和不规则结构环境中，相比最先进基准方法性能提升高达66%，同时提高收敛速度和计算复杂度。

Conclusion: DGRL为大型离散动作空间强化学习提供了高效解决方案，克服了维度灾难问题，在计算效率和性能方面均有显著改进。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [703] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种带约束的隐变量优化方法，用于奖励引导的生成任务，通过硬性白噪声约束防止奖励黑客攻击，同时保持高效性


<details>
  <summary>Details</summary>
Motivation: 现有的测试时隐变量优化方法虽然能提升预训练生成模型的奖励引导生成质量，但存在奖励黑客攻击导致质量下降的问题，且计算速度过慢，难以实际应用

Method: 采用硬性白高斯噪声约束替代软正则化，通过投影梯度上升法实现。每次更新后应用闭式投影，保持隐向量在整个优化过程中显式地保持噪声特性，防止导致不真实伪影的漂移

Result: 该方法仅需SOTA正则化方法30%的挂钟时间就能达到相当的美学评分，同时有效防止奖励黑客攻击。投影复杂度为O(N log N)，与排序或FFT等标准算法相当，实际不增加计算时间

Conclusion: 提出的约束隐变量优化方法使测试时优化既高效又可靠，通过硬性白噪声约束防止奖励黑客攻击，同时保持计算效率，为奖励引导生成提供了实用解决方案

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [704] [Data Reconstruction: Identifiability and Optimization with Sample Splitting](https://arxiv.org/abs/2602.08723)
*Yujie Shen,Zihan Wang,Jian Qian,Qi Lei*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究从KKT条件重建训练数据的两个核心问题：可识别性和优化。理论分析了两层网络多项式激活下KKT系统唯一确定训练数据的充分条件，并提出样本分裂优化方法改进现有重建方法。


<details>
  <summary>Details</summary>
Motivation: 尽管从KKT条件重建训练数据在实践中表现出色，但缺乏理论解释：何时KKT方程有唯一解？即使在可识别情况下，如何通过优化可靠恢复解？论文旨在填补这些理论空白并改进优化方法。

Method: 1) 理论分析：研究两层神经网络（多项式激活）KKT系统唯一确定训练数据的充分条件；2) 优化方法：提出样本分裂技术，通过创建额外下降方向逃离不良驻点并精化解，适用于一般重建目标。

Result: 理论分析提供了KKT重建可识别性的理论解释，实验表明样本分裂技术能持续改进多种现有重建方法的性能，验证了该优化方法的有效性。

Conclusion: 论文从理论和优化两个角度推进了训练数据重建研究：理论层面阐明了可识别性条件，优化层面提出了普适的改进方法，为LLM训练数据隐私保护和安全分析提供了新见解。

Abstract: Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.

</details>


### [705] [$\texttt{lrnnx}$: A library for Linear RNNs](https://arxiv.org/abs/2602.08810)
*Karan Bania,Soham Kalburgi,Manit Tanwar,Dhruthi,Aditya Nagarsekar,Harshvardhan Mestha,Naman Chibber,Raj Deshmukh,Anish Sathyanarayanan,Aarush Rathore,Pratham Chheda*

Main category: cs.LG

Relevance: 75.0

TL;DR: lrnnx是一个统一的线性循环神经网络软件库，实现了多种现代LRNN架构，提供统一接口和多层次控制，旨在提高LRNN研究的可访问性、可复现性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN实现分散在不同软件框架中，依赖框架特定优化，部分需要自定义CUDA内核或缺乏公开代码，导致使用、比较或扩展LRNN需要大量实现工作。

Method: 开发统一的软件库lrnnx，在统一接口下实现多种现代LRNN架构，提供从核心组件到高层模型抽象的多层次控制，采用MIT许可开源。

Result: 创建了一个统一的LRNN软件库，解决了现有实现的碎片化问题，提高了LRNN研究的可访问性和可复现性。

Conclusion: lrnnx库通过统一实现和接口标准化，显著降低了LRNN研究的技术门槛，促进了该领域的进一步发展。

Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.

</details>


### [706] [Kirin: Improving ANN efficiency with SNN Hybridization](https://arxiv.org/abs/2602.08817)
*Chenyu Wang,Zhanglu Yan,Zhi Zhou,Xu Chen,Weng-Fai Wong*

Main category: cs.LG

Relevance: 75.0

TL;DR: Kirin提出了一种整数与脉冲混合的SNN架构，实现了精度无损的ANN-to-SNN转换，在W4A4&8量化设置下达到接近FP16精度，能耗降低84.66%，时间步缩短93.75%


<details>
  <summary>Details</summary>
Motivation: 传统ANN/LLM能耗高，而SNN具有能效优势但存在转换挑战：高比特量化需要长时间窗口增加延迟，单脉冲方案信息损失与多脉冲方案能耗之间存在权衡

Method: 1) 脉冲矩阵混合策略：将导致小时间窗口的低比特参数编码为二进制脉冲，其余保留为整数格式；2) 静默阈值机制：调节单脉冲发放时机，确保输出与LLM数学等价

Result: 在W4A4&8量化设置下达到接近FP16精度，能耗降低84.66%，时间步缩短93.75%，实现了精度无损的ANN-to-SNN转换

Conclusion: Kirin通过整数与脉冲混合的SNN架构，成功解决了ANN-to-SNN转换中的精度损失、延迟和能耗问题，为高效能LLM部署提供了新方案

Abstract: Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>


### [707] [Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization](https://arxiv.org/abs/2602.08855)
*Yang Qiu,Yixiong Zou,Jun Wang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出E2A框架，通过能量驱动的生成增强来解决图神经网络中的最小偏移翻转问题，提升图OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在分布偏移下表现脆弱，特别是最小偏移翻转现象：测试样本仅轻微偏离训练分布就被错误分类。现有方法对此现象理解不足，需要从损失景观角度深入分析。

Method: 1. 从锐度感知最小化视角分析MSF现象，引入局部鲁棒半径量化损失锐度；2. 提出能量公式，证明其与鲁棒半径单调相关；3. 开发E2A框架，通过能量引导的潜在扰动生成伪OOD样本增强泛化。

Result: 在多个基准测试中，E2A框架显著提升图OOD泛化性能，优于现有最先进基线方法。

Conclusion: 通过能量驱动的生成增强方法可以有效解决图神经网络中的最小偏移翻转问题，提升模型在分布偏移下的泛化能力和稳定性。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>


### [708] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出"正分布偏移(PDS)"概念，认为在特定条件下，训练分布与目标分布的不同反而能简化学习问题，特别是降低计算复杂度而非统计复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统分布偏移研究主要关注如何减轻或避免分布偏移的负面影响，但作者认为精心选择的训练分布可以简化学习过程，这在当代机器学习中尤为重要，因为创新往往在于找到好的训练分布而非改进训练算法。

Method: 形式化定义了正分布偏移的不同变体，展示了某些困难类别在PDS下变得容易学习，并与成员查询学习建立联系，强调计算复杂度而非统计复杂度的降低。

Result: 证明了通过精心设计的训练分布，原本计算困难的问题可以通过标准梯度训练变得可解，为分布偏移提供了新的积极视角。

Conclusion: 正分布偏移为机器学习提供了新的理论框架，强调训练分布选择的重要性，特别是在降低计算复杂度方面，对实际模型训练有重要指导意义。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [709] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种基于扩散过程的Transformer不确定性校准方法，将特征变换建模为概率映射，通过扩散过程传播表示不确定性，在保持预测性能的同时提升校准效果


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在风险敏感应用中需要可靠的不确定性校准，但现有模型缺乏通过特征变换堆栈进行不确定性传播的原则性机制

Method: 提出扩散启发的Transformer重构方法：将每个特征变换块建模为概率映射，组合这些映射形成类似扩散过程的概率路径，然后将其重新编译到具有统一转移模型的扩散过程中，实现表示不确定性的原则性传播

Result: 在多种视觉和语言基准测试中，该方法相比现有不确定性感知Transformer实现了更优的校准效果和预测准确性

Conclusion: 提出的扩散启发的Transformer重构方法能够有效传播表示不确定性，在保持原始预测性能的同时显著提升模型校准效果，适用于风险敏感应用

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [710] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出Symplectic Positional Embeddings (SyPE)，一种基于哈密顿力学的可学习位置编码框架，用于解决时间序列预测中时间扭曲问题，超越了传统RoPE的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列（如金融周期、生物节律）经常表现出"时间扭曲"动态，即有效时间流与采样索引解耦。传统Transformer依赖的位置编码假设均匀的时间进展，无法表示非仿射时间扭曲。

Method: 提出SyPE框架，将旋转群SO(2)推广到辛群Sp(2,R)，通过输入依赖的自适应扭曲模块调制。该方法允许注意力机制端到端地自适应扩张或收缩时间坐标，无需预定义扭曲函数。

Result: 在StretchTime架构中实现SyPE，在标准基准测试中达到最先进性能，在表现出非平稳时间动态的数据集上表现出优越的鲁棒性。

Conclusion: SyPE严格推广了RoPE，能够捕捉局部变化的周期性，为时间序列预测中处理时间扭曲问题提供了有效的数学框架。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [711] [Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility](https://arxiv.org/abs/2602.07195)
*Bihui Jin,Kaiyuan Wang,Pengyu Nie*

Main category: cs.SE

Relevance: 75.0

TL;DR: MLEModernizer：基于LLM的代理框架，用于修复因环境侵蚀而无法复现的机器学习工程笔记本，通过迭代执行和针对性修复，使74.2%的不可复现笔记本恢复可复现性。


<details>
  <summary>Details</summary>
Motivation: 机器学习工程笔记本（如Jupyter）因硬件和软件生态快速演化（环境侵蚀）导致大量已发布笔记本无法在当代环境中复现，阻碍代码重用和科学进步。研究发现仅35.4%的Kaggle竞赛笔记本仍可复现，且环境回退（降级依赖）反而引入额外失败模式。

Method: 设计MLEModernizer：LLM驱动的代理框架，将当代环境作为固定约束，通过迭代执行笔记本、收集执行反馈、应用三类针对性修复（错误修复、运行时优化、分数校准）来现代化笔记本代码。

Result: 在7,402个基线环境下不可复现的笔记本上评估，MLEModernizer使5,492个（74.2%）恢复可复现性。相比传统环境回退方法，该方法更有效且避免引入额外失败模式。

Conclusion: MLEModernizer为应对机器学习环境侵蚀提供了有效解决方案，使从业者能够在硬件和软件生态持续演化中验证、重用和维护机器学习工程工件，促进代码重用和科学进步。

Abstract: Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes.
  To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.

</details>


### [712] [Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models](https://arxiv.org/abs/2602.07997)
*TrungKhang Tran,TrungTin Nguyen,Md Abul Bashar,Nhat Ho,Richi Nayak,Christopher Drovandi*

Main category: stat.ML

Relevance: 75.0

TL;DR: 本文针对softmax门控的多项式逻辑混合专家模型，提出了批处理最小化-最大化算法，保证了训练稳定性，并开发了基于混合度量树状图的模型选择方法，在蛋白质相互作用预测中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型在回归和分类中表现良好，但对于使用softmax多项逻辑门控的分类任务，最大似然训练的稳定性保证和原则性模型选择方法仍然有限。本文旨在解决这两个问题。

Method: 1. 推导了softmax门控多项式逻辑MoE的批处理最小化-最大化算法，使用显式二次最小化器，得到坐标闭式更新，保证目标函数的单调上升和全局收敛
2. 证明了条件密度估计和参数恢复的有限样本率
3. 将混合度量的树状图适应到分类设置，开发了无需扫描的专家数量选择器

Result: 在生物蛋白质-蛋白质相互作用预测实验中，完整流程相比强大的统计和机器学习基线，提供了更高的准确率和更好校准的概率。

Conclusion: 本文为softmax门控多项式逻辑MoE提供了理论保证的训练算法和模型选择方法，在生物信息学应用中验证了其有效性，为MoE在分类任务中的可靠应用提供了理论基础。

Abstract: Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines.

</details>


### [713] [Welfarist Formulations for Diverse Similarity Search](https://arxiv.org/abs/2602.08742)
*Siddharth Barman,Nirjhar Das,Shivam Gupta,Kirankumar Shiragur*

Main category: cs.DS

Relevance: 75.0

TL;DR: 本文提出了基于福利函数的最近邻搜索新方法，通过纳什社会福利等经济学概念在检索中自适应平衡相关性和多样性，相比传统约束方法更灵活。


<details>
  <summary>Details</summary>
Motivation: 在RAG等应用中，除了返回邻居的相关性外，多样性也是核心需求。传统方法采用约束方式强制固定多样性水平，缺乏自适应平衡相关性和多样性的能力。

Method: 基于数学经济学中的福利函数（特别是纳什社会福利）构建NNS目标函数，提出高效算法，可在任何标准ANN方法基础上应用，提供参数化控制相关性与多样性的权衡。

Result: 实验结果表明该方法实用性强，在保持高相关性的同时显著提升检索邻居的多样性，算法具有理论保证。

Conclusion: 福利函数为NNS中的多样性需求提供了原则性框架，自适应平衡相关性与多样性，为从业者提供了灵活的任务特定结果定制能力。

Abstract: Nearest Neighbor Search (NNS) is a fundamental problem in data structures with wide-ranging applications, such as web search, recommendation systems, and, more recently, retrieval-augmented generations (RAG). In such recent applications, in addition to the relevance (similarity) of the returned neighbors, diversity among the neighbors is a central requirement. In this paper, we develop principled welfare-based formulations in NNS for realizing diversity across attributes. Our formulations are based on welfare functions -- from mathematical economics -- that satisfy central diversity (fairness) and relevance (economic efficiency) axioms. With a particular focus on Nash social welfare, we note that our welfare-based formulations provide objective functions that adaptively balance relevance and diversity in a query-dependent manner. Notably, such a balance was not present in the prior constraint-based approach, which forced a fixed level of diversity and optimized for relevance. In addition, our formulation provides a parametric way to control the trade-off between relevance and diversity, providing practitioners with flexibility to tailor search results to task-specific requirements. We develop efficient nearest neighbor algorithms with provable guarantees for the welfare-based objectives. Notably, our algorithm can be applied on top of any standard ANN method (i.e., use standard ANN method as a subroutine) to efficiently find neighbors that approximately maximize our welfare-based objectives. Experimental results demonstrate that our approach is practical and substantially improves diversity while maintaining high relevance of the retrieved neighbors.

</details>


### [714] [TACIT: Transformation-Aware Capturing of Implicit Thought](https://arxiv.org/abs/2602.07061)
*Daniel Nobrega*

Main category: cs.LG

Relevance: 65.0

TL;DR: TACIT是一种基于扩散的Transformer模型，用于可解释的视觉推理。它完全在像素空间运行，通过整流流将未解决的迷宫图像转换为解决方案，实现了推理过程的可视化。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一种完全在像素空间运行的视觉推理系统，能够直接可视化推理过程，从而理解神经网络如何发展出在语言之前和之下的隐式推理策略。

Method: 使用基于扩散的Transformer架构，通过整流流在像素空间操作。模型学习将未解决的迷宫图像转换为解决方案图像，使用无噪声流匹配方法。

Result: 在100万个合成迷宫对上：训练损失减少192倍，L2距离改进22.7倍，仅需10个欧拉步骤（典型扩散模型需要100-1000步）。发现明显的相变现象：解决方案在68%的转换过程中不可见，然后在t=0.70时在2%的过程中突然出现。

Conclusion: TACIT展示了神经网络如何发展出整体而非算法的推理策略，这种"顿悟时刻"模式与人类认知中的洞察现象相似。像素空间设计为理解隐式推理策略提供了基础。

Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:
  - 192x reduction in training loss over 100 epochs
  - 22.7x improvement in L2 distance to ground truth
  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)
  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.

</details>


### [715] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出匹配框架解决异构数据集池化中的分布不对称问题，通过自适应质心选择和迭代优化表示分布，在零样本泛化场景下优于朴素池化和均匀子采样。


<details>
  <summary>Details</summary>
Motivation: 异构数据集池化在表示学习中常见，但朴素池化会放大分布不对称性，导致有偏估计，特别是在需要零样本泛化的场景中。医疗异常检测等极端异构数据场景需要更鲁棒的方法。

Method: 提出匹配框架：1) 基于自适应质心选择样本；2) 迭代优化表示分布；3) 结合双重鲁棒性和倾向得分匹配处理数据域包含；4) 过滤混淆域（异质性的主要原因）。

Result: 理论和实证分析表明，在不对称元分布下，匹配方法优于朴素池化和均匀子采样。扩展到非高斯和多模态现实场景，在零样本医疗异常检测中取得显著改进。

Conclusion: 匹配框架能有效处理异构数据集池化中的分布不对称问题，在零样本泛化场景下提供更鲁棒的表示学习，特别适用于医疗异常检测等极端异构数据场景。

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [716] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 首次将大语言模型的上下文学习能力扩展到信号处理系统，提出基于Transformer的电机前馈控制模型，通过分离信号表示和系统行为实现少样本微调和单样本上下文学习


<details>
  <summary>Details</summary>
Motivation: 传统PI控制和基于物理模型的方法在处理非线性和复杂负载条件时存在困难，而大语言模型展现的上下文学习能力尚未应用于信号处理系统，需要探索如何将这种能力扩展到物理系统控制中

Method: 提出基于Transformer的模型架构，分离信号表示和系统行为，在大规模合成线性和非线性系统数据上预训练，然后通过少量示例实现对新系统动态的泛化，支持少样本微调和单样本上下文学习

Result: 模型在多种电机负载配置上表现出良好的泛化能力，能够将未调优的示例转化为准确的前馈预测，在实验中超越了PI控制器和基于物理模型的前馈基线方法

Conclusion: 上下文学习能够桥接合成预训练和真实世界适应性，为物理系统的数据高效控制开辟了新方向，展示了将大语言模型能力扩展到信号处理领域的潜力

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [717] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出rsEAC方法，一种基于熵风险度量的离策略模型无关强化学习方法，通过避免显式表示指数值函数及其梯度，在连续控制任务中稳定学习风险敏感策略。


<details>
  <summary>Details</summary>
Motivation: 当前基于熵风险度量的策略梯度方法存在高方差和数值不稳定问题，限制了风险敏感方法在复杂任务中的应用。需要理论支持和更稳定的算法来实现实际部署中的风险感知智能体。

Method: 提出风险敏感指数演员-评论家(rsEAC)：1) 提供熵风险度量的策略梯度理论分析；2) 避免显式表示指数值函数及其梯度；3) 采用离策略模型无关方法；4) 在MuJoCo连续控制任务中验证。

Result: rsEAC相比现有方法产生更数值稳定的更新，在MuJoCo的具有风险变体连续任务中可靠地学习风险敏感策略。

Conclusion: 通过理论分析和算法创新，rsEAC解决了现有风险敏感强化学习方法的稳定性问题，为实际应用中的风险感知智能体提供了可行的解决方案。

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [718] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

Relevance: 65.0

TL;DR: DSL（双尺度Softmax损失）通过基于采样竞争自适应调整损失锐度，改进了推荐系统中的Softmax损失，在多个基准测试中平均提升6.22%，在OOD场景下提升9.31%。


<details>
  <summary>Details</summary>
Motivation: 传统Softmax损失在隐式反馈推荐系统中使用全局温度和均匀采样负样本，可能导致训练不稳定，因为不同用户-物品对的负样本竞争程度不同，固定的损失锐度可能对某些实例是次优的。

Method: DSL在log-sum-exp主干上添加两个互补分支：1）使用硬度和物品相似度重新加权每个训练实例内的负样本；2）从构建的竞争列表中自适应调整每个实例的温度。这两个组件保持了SL的几何结构，同时重塑了负样本和实例间的竞争分布。

Result: 在多个代表性基准测试和骨干网络上，DSL相比强基线有显著提升，平均改进6.22%，在某些设置中超过10%。在OOD流行度偏移下，平均改进9.31%。理论分析表明DSL重塑了鲁棒收益和KL偏差。

Conclusion: DSL通过从采样竞争本身推断有效锐度，解决了传统Softmax损失在推荐系统中的局限性，在准确性和鲁棒性方面都有显著改进，并通过DRO分析提供了理论解释。

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [719] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一个容错评估框架，用于评估模型性能估计器，通过可调节的容错水平ε来平衡偏差和方差，解决现有方法在低方差场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 在模型即服务时代，第三方AI模型的可靠验证面临挑战：新兴AI应用动态变化、新数据集不断引入、模型数量激增。现有性能估计器评估方法在低方差场景下失效：RMSE混淆偏差和方差，p值检验变得过于敏感。需要一种能够容忍实际可接受误差的评估框架。

Method: 提出容错评估框架，将偏差和方差考虑整合到可调节的容错水平ε中。理论上证明适当校准ε可以确保在不同方差机制下的可靠评估，并提出自动优化和选择ε的算法。

Result: 在真实世界数据集上的实验表明，该框架能够提供对估计器行为的全面且可操作的洞察，解决了现有评估方法在低方差场景下的局限性。

Conclusion: 提出的容错评估框架为模型性能估计器的评估提供了更实用和可靠的解决方案，特别适用于低方差场景，有助于在模型即服务环境中做出更明智的模型选择决策。

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [720] [Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations](https://arxiv.org/abs/2602.07320)
*Philip Jacobson,Ben Feinberg,Suhas Kumar,Sapan Agarwal,T. Patrick Xiao,Christopher Bennett*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究神经网络损失函数平坦区域与泛化能力的关系，探索SAM和RWP方法训练权重扰动鲁棒模型，分析噪声鲁棒泛化与优化问题


<details>
  <summary>Details</summary>
Motivation: 研究神经网络损失函数平坦区域与泛化能力的关系假设，同时解决未来低功耗硬件平台中权重内部扰动的鲁棒性训练问题

Method: 使用锐度感知最小化(SAM)和随机权重扰动(RWP)方法，从泛化(减少噪声鲁棒泛化差距)和优化(在强扰动下最大化性能)两个角度分析，建立理论框架并进行实证研究

Result: 发现过正则化的RWP训练目标对噪声鲁棒泛化最优；对于小幅度噪声，SAM的对抗目标优于任何RWP配置，但对大幅度噪声表现差；发现损失函数不平坦导致的梯度消失效应影响SAM和RWP；动态调整扰动强度匹配损失函数演化能改善优化

Conclusion: 平坦损失函数区域确实与更好的泛化相关，过正则化RWP对噪声鲁棒泛化最优，SAM对小噪声有效但对大噪声有局限，动态调整扰动策略能改善鲁棒训练

Abstract: Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.

</details>


### [721] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

Relevance: 65.0

TL;DR: UTOPIA：针对表格数据的不可学习示例方法，通过解耦优化在高显著性特征上进行语义混淆，在低显著性冗余特征中嵌入超相关捷径，实现认证的不可学习性。


<details>
  <summary>Details</summary>
Motivation: 表格数据（金融、医疗）高度敏感，但现有不可学习示例方法在表格数据上效果不佳，因为表格特征混合数值和类别约束，且存在显著性稀疏性（学习主要由少数维度主导）。需要保护表格数据免遭未经授权的模型训练。

Method: 提出UTOPIA方法：利用特征冗余将优化解耦为两个通道：1）高显著性特征用于语义混淆；2）低显著性冗余特征用于嵌入超相关捷径。在满足谱主导条件下，当毒化谱压倒干净语义谱时，可实现认证的不可学习性。

Result: 在多个表格数据集和模型上的实验表明，UTOPIA能使未经授权的训练达到接近随机的性能，优于现有不可学习示例基线方法，且在不同架构间具有良好的迁移性。

Conclusion: UTOPIA为表格数据提供了有效的不可学习性保护机制，解决了现有方法在表格数据上的局限性，为敏感表格数据的隐私保护提供了新方案。

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [722] [Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent](https://arxiv.org/abs/2602.07378)
*Shota Imai,Sota Nishiyama,Masaaki Imaizumi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究了神经网络训练中的特征遗忘现象，通过无限宽度极限下的两层神经网络分析，揭示了特征遗忘发生的机制和条件，发现数据中的非线性项强度和第二层权重初始尺度是关键因素。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络梯度训练中的动态结构是理论机器学习的核心挑战。特征遗忘现象（神经网络在长时间训练中逐渐丢失先前学习到的特征）引起了关注，需要从理论上揭示其机制和条件。

Method: 采用无限宽度极限下的两层神经网络，使用大批量随机梯度更新，推导出不同时间尺度的微分方程。利用快-慢动力学分析：第一层权重快速对齐，第二层权重缓慢发展。结合张量程序和奇异摄动理论进行理论分析。

Result: 揭示了特征遗忘发生的机制：临界流形上的流向（由慢动力学决定）决定了特征遗忘是否发生。发现两个关键因素：(i) 数据中主要非线性项的强度会诱导特征遗忘；(ii) 第二层权重的初始尺度可以缓解特征遗忘。通过数值验证了结果并推导了缩放定律。

Conclusion: 该研究从理论上解释了神经网络训练中的特征遗忘现象，揭示了其发生的动力学机制和关键影响因素，为理解神经网络训练动态提供了新的理论视角。

Abstract: The dynamics of gradient-based training in neural networks often exhibit nontrivial structures; hence, understanding them remains a central challenge in theoretical machine learning. In particular, a concept of feature unlearning, in which a neural network progressively loses previously learned features over long training, has gained attention. In this study, we consider the infinite-width limit of a two-layer neural network updated with a large-batch stochastic gradient, then derive differential equations with different time scales, revealing the mechanism and conditions for feature unlearning to occur. Specifically, we utilize the fast-slow dynamics: while an alignment of first-layer weights develops rapidly, the second-layer weights develop slowly. The direction of a flow on a critical manifold, determined by the slow dynamics, decides whether feature unlearning occurs. We give numerical validation of the result, and derive theoretical grounding and scaling laws of the feature unlearning. Our results yield the following insights: (i) the strength of the primary nonlinear term in data induces the feature unlearning, and (ii) an initial scale of the second-layer weights mitigates the feature unlearning. Technically, our analysis utilizes Tensor Programs and the singular perturbation theory.

</details>


### [723] [Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis](https://arxiv.org/abs/2602.07440)
*Cédric Jung,Shirin Salehi,Anke Schmeink*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文提出六种聚合结构来解决主动学习中的探索-利用困境，通过结合基于代表性和不确定性的获取函数，在减少标注成本和计算能耗的同时保持或提高模型精度。


<details>
  <summary>Details</summary>
Motivation: 主动学习旨在通过选择最具信息量的样本进行标注来降低标注成本，同时也能减少神经网络训练中的能耗。然而，现有的获取函数存在探索-利用困境：基于代表性的方法能有效探索数据集但忽略决策边界，而基于不确定性的方法专注于已识别的决策边界但缺乏探索。需要一种平衡两者优势的方法。

Method: 提出了六种聚合结构：串联、并联、混合、自适应反馈、随机探索和退火探索。这些结构将基于代表性（如K-Centers）和基于不确定性（如BALD、BADGE）的获取函数进行组合，以解决批量模式效率低下和冷启动问题。

Result: 实验表明聚合结构能显著减少计算成本同时保持或提高精度。例如，K-Centers后接BALD的串联结构能以12%更少的样本达到相同性能，同时获取成本减少近一半。交替使用BALD和BADGE等创新聚合方法也显示出鲁棒结果。

Conclusion: 提出的聚合结构有效解决了主动学习中的探索-利用困境，通过平衡精度和能耗，为开发更可持续、能源感知的人工智能系统提供了实用解决方案。

Abstract: Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.

</details>


### [724] [Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles](https://arxiv.org/abs/2602.07579)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种基于特征正交性损失的时间序列分类多样性驱动集成学习框架，通过强制特征表示正交化来促进集成成员间的多样性，在UCR数据集上以更少模型达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有神经网络集成方法通常使用相同架构和配置训练多个模型，仅简单聚合预测而不显式促进多样性，导致特征表示冗余，限制了集成效果。需要一种能显式鼓励特征多样性的集成学习框架。

Method: 提出多样性驱动的集成学习框架，采用去相关学习策略，在学习的特征表示上直接应用特征正交性损失，确保集成中每个模型捕获互补而非冗余的信息。

Result: 在UCR档案的128个数据集上评估，该框架以更少模型达到最先进的性能，相比传统神经网络集成方法更高效和可扩展。

Conclusion: 通过显式促进特征多样性，提出的框架能够以更高效的集成方式实现更好的时间序列分类性能，为神经网络集成学习提供了新的方向。

Abstract: Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.

</details>


### [725] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出TASTE框架，基于Stein算子将分布偏移与模型输入敏感性关联，提供几何解释和理论保证，实现OOD检测和定位


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法要么是数据中心的（仅检测训练输入分布的偏差），要么是模型中心的（依赖分类器输出而不考虑数据几何）。需要一种能连接分布偏移与模型敏感性的任务感知方法

Method: 基于Stein算子构建任务感知框架，将分布偏移投影到模型敏感性场上，提供几何解释和理论保证。支持坐标分解实现局部定位，对图像数据提供逐像素诊断

Result: 在控制高斯偏移、MNIST几何扰动和CIFAR-10扰动基准测试中，该方法与任务退化紧密对齐，优于现有基线方法

Conclusion: TASTE框架通过Stein算子有效连接分布偏移与模型敏感性，提供理论保证和可解释诊断，在OOD检测和定位方面表现优异

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [726] [On the Infinite Width and Depth Limits of Predictive Coding Networks](https://arxiv.org/abs/2602.07697)
*Francesco Innocenti,El Mehdi Achour,Rafal Bogacz*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究了预测编码网络在无限宽度和深度极限下的理论特性，证明了在特定参数化条件下，预测编码的能量函数收敛于反向传播的损失函数，从而计算相同的梯度。


<details>
  <summary>Details</summary>
Motivation: 预测编码作为一种生物上更合理的替代方案，其训练稳定性和可扩展性仍不清楚。作者旨在研究预测编码网络在无限宽度和深度极限下的理论特性，以理解其与反向传播的关系。

Method: 通过分析线性残差网络，研究预测编码网络在无限宽度和深度极限下的行为。使用宽度和深度稳定的特征学习参数化方法，证明在特定条件下预测编码能量函数收敛于反向传播损失。

Result: 研究发现：1）预测编码与反向传播具有相同的宽度和深度稳定参数化集合；2）在宽度远大于深度的情况下，预测编码能量函数收敛于反向传播损失；3）这些理论结果在深度非线性网络中得到实验验证。

Conclusion: 该工作统一了先前理论和实证结果，为预测编码网络的可扩展性提供了理论基础，表明在适当参数化下，预测编码可以计算与反向传播相同的梯度。

Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.

</details>


### [727] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出密集特征学习框架，通过直接优化嵌入矩阵的线性代数特性（谱平衡、子空间一致性、特征正交性），改善医学数据表示的质量，无需依赖标签或生成重建。


<details>
  <summary>Details</summary>
Motivation: 传统医学深度学习模型通常使用任务特定目标训练，导致表示坍缩到少数判别方向，未能充分利用临床数据的丰富结构，限制了特征的迁移性、稳定性和可解释性。

Method: 密集特征学习框架直接操作嵌入矩阵，通过定义线性代数特性的目标函数来鼓励谱平衡、子空间一致性和特征正交性，不依赖标签或生成重建。

Result: 在纵向电子健康记录、临床文本和多模态患者表示上的实证评估显示，相比监督和自监督基线，该方法在下游线性性能、鲁棒性和子空间对齐方面有持续改进。

Conclusion: 学习覆盖临床变异可能与学习预测临床结果同样重要，应将表示几何作为医学AI的一等目标。

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [728] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出将强化学习中的内省视为程序分析，通过分析内部模型来合成任务相关信息，建立强化学习与经典规划之间的联系。


<details>
  <summary>Details</summary>
Motivation: 人类在解决问题时能够通过内省从内部模型中推导出额外信息，而强化学习和经典规划通常被视为两个独立问题。论文旨在探索如何将这种内省能力形式化为程序分析，从而弥合强化学习与经典规划之间的鸿沟。

Method: 提出将内省视为程序分析的方法，讨论了如何将此方法应用于各种强化学习模型。特别描述了一种算法，能够在关系强化学习使用的模型类上实现高效的目标导向规划。

Result: 建立了一种新颖的链接，将强化学习与经典规划联系起来，展示了如何通过程序分析方法在关系强化学习模型上进行高效规划。

Conclusion: 通过将内省形式化为程序分析，可以弥合强化学习与经典规划之间的差距，使智能体能够像人类一样从内部模型中推导出任务相关信息，实现更高效的问题解决。

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [729] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

Relevance: 65.0

TL;DR: CausalTaD：通过将因果知识注入LLM来改进表格异常检测的方法，通过识别列间因果关系并重新排序，再结合重加权策略，在30多个数据集上超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的表格异常检测方法在将表格数据转换为文本时随机排列列顺序，忽略了列间的因果关系，而这对准确检测异常至关重要。因此需要将因果知识注入LLM以提升检测性能。

Method: 1) 识别表格列间的因果关系，并按因果关系重新排序列（建模为线性排序问题）；2) 提出重加权策略，根据各列对因果关系的贡献程度分配不同权重以增强效果；3) 将处理后的表格数据转换为文本并微调LLM进行异常检测。

Result: 在超过30个数据集上的实验表明，CausalTaD方法在表格异常检测任务上持续超越当前最先进的方法。

Conclusion: 将因果知识注入LLM能显著提升表格异常检测性能，列排序和重加权策略是有效的技术手段。该方法为LLM在表格数据分析中的应用提供了新思路。

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [730] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出了一种基于VQVAE和模糊聚类的离线强化学习反探索方法，通过多码本VQVAE离散化状态-动作对并设计伪计数方法，结合FCM聚类更新码本，解决了维度灾难和信息丢失问题，在D4RL基准测试中表现优于SOTA方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL中的反探索方法通过离散化连续状态-动作对进行计数，但存在维度灾难和信息丢失问题，导致效率降低、性能下降甚至策略学习失败。需要一种更有效的离散化方法来改进伪计数技术。

Method: 1) 基于多码本VQVAE的高效伪计数方法，用于离散化状态-动作对；2) 基于该伪计数方法的离线RL反探索方法；3) 基于模糊C均值(FCM)聚类的码本更新机制，提高码本向量利用率。

Result: 在D4RL基准测试的多个复杂任务中，该方法表现优于现有SOTA方法，且需要更少的计算成本。

Conclusion: 提出的基于VQVAE和模糊聚类的反探索方法有效解决了维度灾难和信息丢失问题，提高了离线RL的学习效率和性能。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [731] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文提出Kinetic Path Energy (KPE)作为流生成模型的轨迹能量诊断指标，发现KPE与语义保真度正相关，但过高能量会导致记忆化，进而提出Kinetic Trajectory Shaping (KTS)训练自由的两阶段推理策略来改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 从物理学视角理解流生成模型：采样过程类似于粒子从噪声到数据的传输，每个样本对应具有自身动力学努力的轨迹。受经典力学启发，需要量化轨迹的动力学努力，并理解其与生成质量的关系。

Method: 1. 引入Kinetic Path Energy (KPE)：类似作用量的每样本诊断指标，测量ODE轨迹上积累的动力学努力
2. 理论分析轨迹能量与数据密度的关系
3. 提出Kinetic Trajectory Shaping (KTS)：训练自由的两阶段推理策略，增强早期运动并强制后期软着陆

Result: 1. KPE表现出两个稳健对应关系：更高KPE预测更强的语义保真度；高KPE轨迹终止于低密度流形边界
2. 发现能量与记忆化的非单调关系：足够高能量时生成会退化为记忆化
3. KTS在基准任务上减少记忆化并提高生成质量

Conclusion: KPE为流生成模型提供了有价值的诊断工具，揭示了轨迹能量与生成质量的重要关系。KTS策略通过调节轨迹动力学改善了生成多样性和质量，避免了记忆化问题。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [732] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

Relevance: 65.0

TL;DR: SCoRa是一个统一的概率模型，通过结合个体评分和成对比较两种偏好获取方式，能够更准确地学习实体排序，特别在需要精确排名前几位实体的场景中表现优于单一方法。


<details>
  <summary>Details</summary>
Motivation: 长期以来存在关于人类应该单独评估实体还是进行比较评估的争论。本文发现结合两种形式的偏好获取可以超越单一方法，特别是在需要准确排序顶级实体的关键场景中。

Method: 提出了SCoRa（从比较和评分中评分）统一概率模型，能够同时学习个体评分信号和成对比较信号。证明了SCoRa的最大后验估计具有良好的数学性质，包括单调性和鲁棒性保证。

Result: 实证研究表明，即使在模型不匹配的情况下，SCoRa也能恢复准确的分数。更重要的是，识别了一个现实场景：当需要准确排序顶级实体时，结合比较和评分的方法优于单独使用任何一种方法。

Conclusion: 结合个体评分和成对比较的偏好获取方法比单一方法更有效，特别是在需要精确排名前几位实体的关键应用中。SCoRa为偏好学习提供了一个多功能的基础框架。

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [733] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

Relevance: 65.0

TL;DR: EpiFlow：通过状态约束最优控制框架，使用epigraph值函数和流匹配技术解决安全离线强化学习问题，在保持高性能的同时实现接近零的安全违规。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域具有重要价值，但现有方法难以同时保证强安全性和高性能。现有安全离线RL方法要么允许违规，要么过于保守，或者难以平衡安全性、奖励优化和数据分布一致性。

Method: 将安全离线RL建模为状态约束最优控制问题，通过epigraph重构推导可行性值函数，避免目标解耦或后处理过滤。基于epigraph值函数对行为分布进行重加权，通过流匹配拟合生成策略，实现高效且分布一致的采样。

Result: 在Safety-Gymnasium等安全关键任务上，EpiFlow实现了具有竞争力的回报和接近零的实证安全违规，展示了epigraph引导策略合成的有效性。

Conclusion: EpiFlow通过epigraph引导的流匹配框架，成功解决了安全离线RL中安全性与性能的平衡问题，为安全关键领域的自主系统训练提供了有效解决方案。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [734] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

Relevance: 65.0

TL;DR: DrugR：一种基于LLM的药物分子优化方法，通过引入明确的药理学推理步骤，结合领域持续预训练、监督微调和自平衡多粒度强化学习，在保持分子核心功效的同时改善ADMET性质。


<details>
  <summary>Details</summary>
Motivation: 分子生成和优化是化学领域的基础任务。虽然大语言模型（LLMs）具有强大的知识储备和交互能力，但其内在挑战在于分子结构与药理性质之间的复杂隐式关系以及缺乏相应的标注数据。

Method: 1. 领域特定的持续预训练；2. 通过反向数据工程的监督微调；3. 自平衡多粒度强化学习。该方法引入明确的、逐步的药理学推理到优化过程中。

Result: DrugR在多个性质上实现了全面增强，同时不损害结构相似性或靶标结合亲和力。其明确的推理过程为每个优化步骤提供了清晰、可解释的原理。

Conclusion: DrugR为自动化、知识驱动的科学发现提供了可操作的设计见解，代码和模型检查点已开源以促进未来研究。

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [735] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了一种基于信息瓶颈的任务相关维度估计方法，通过混合批评器解决传统神经互信息估计器高估维度的问题，并开发了无需扫描瓶颈尺寸的一步估计协议。


<details>
  <summary>Details</summary>
Motivation: 估计预测所需的潜在表示维度（任务相关维度）是一个困难但具有广泛科学应用价值的问题。现有方法难以准确估计，特别是在噪声环境下传统几何维度估计器性能下降。

Method: 将维度估计重新定义为信息瓶颈问题：寻找能够压缩预测变量和预测视图同时保留其互信息的最小嵌入瓶颈维度。提出混合批评器，在保持显式维度瓶颈的同时允许灵活的非线性跨视图交互。开发一步估计协议，从单个过参数化混合模型中直接读取有效维度。

Result: 在已知任务相关维度的合成问题上验证了方法的有效性。扩展到内在维度估计，在噪声环境下比传统几何维度估计器更可靠。在多个物理数据集上展示了方法的实用性。

Conclusion: 该方法为任务相关维度估计提供了可靠的信息论框架，特别适用于噪声环境下的维度估计问题，具有广泛的应用前景。

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [736] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

Relevance: 65.0

TL;DR: PRO-Bid是一个基于决策Transformer的约束感知自动竞价框架，通过约束解耦帕累托表示和反事实遗憾优化，解决标准方法在资源分配和性能优化上的局限性，在约束满足和价值获取方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动竞价系统需要在满足严格效率约束（如目标CPA）的同时最大化营销价值。现有决策Transformer方法面临两个挑战：1）标准Return-to-Go条件忽略了成本维度，导致状态混淆和资源分配不精确；2）标准回归迫使策略模仿历史平均行为，限制了向约束边界优化的能力。

Method: 提出PRO-Bid框架，包含两个协同机制：1）约束解耦帕累托表示（CDPR）：将全局约束分解为递归成本和价值上下文以恢复资源感知，同时基于帕累托前沿重新加权轨迹以关注高效数据；2）反事实遗憾优化（CRO）：利用全局结果预测器识别更优的反事实动作，将这些高效用结果作为加权回归目标，使模型超越历史平均值接近最优约束边界。

Result: 在两个公共基准测试和在线A/B测试中，PRO-Bid在约束满足和价值获取方面均优于最先进的基线方法。

Conclusion: PRO-Bid通过创新的约束感知生成框架，有效解决了自动竞价中的资源分配和性能优化问题，为约束环境下的序列决策提供了新思路。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [737] [Grokking in Linear Models for Logistic Regression](https://arxiv.org/abs/2602.08302)
*Nataraj Das,Atreya Vedantam,Chandrashekar Lakshminarayanan*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文研究了线性模型中出现的"grokking"（延迟泛化）现象，发现在最简单的线性分类设置中，即使没有深度网络结构，仅通过梯度下降的隐式偏差和偏置项动态，也能产生延迟泛化现象。


<details>
  <summary>Details</summary>
Motivation: 传统认为grokking现象主要源于深度神经网络的深度和组合结构，但本文试图探索在最简单的线性模型设置中是否也能出现grokking，以揭示该现象更本质的机制。

Method: 在线性可分数据上使用逻辑损失训练线性模型，研究三种测试机制：同分布测试、边缘集中测试和对抗性测试。理论分析梯度下降的隐式偏差如何诱导三阶段学习过程，并通过实验验证理论。

Result: 发现grokking可以在线性模型中产生，不需要深度或表示学习。延迟泛化的出现与数据不对称性（类别样本数量和支撑向量分布）相关，并给出了grokking时间的特征描述。

Conclusion: grokking现象不仅限于深度网络，在线性模型中通过偏置项动态也能出现，这为理解延迟泛化提供了更基础的视角。

Abstract: Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.

</details>


### [738] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了ALoRa-T模型，通过低秩正则化自注意力机制来改进多元时间序列异常诊断，并开发了ALoRa-Loc方法进行异常定位，在检测和定位任务上都显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常诊断对复杂系统的安全可靠性至关重要，但现有方法缺乏理论洞察，特别是异常定位这一重要但未充分探索的领域。作者旨在通过揭示Transformer与统计时间序列方法的联系来研究其在MTS上的学习过程。

Method: 提出了Attention Low-Rank Transformer (ALoRa-T)模型，对自注意力应用低秩正则化；引入了Attention Low-Rank分数来捕捉异常的时间特征；开发了ALoRa-Loc方法，通过量化时间序列间的相互关系来将异常关联到特定变量。

Result: 大量实验和真实数据分析表明，所提出的方法在检测和定位任务上都显著优于最先进的方法。

Conclusion: 通过理论分析Transformer在时间序列中的应用，提出的ALoRa-T模型和ALoRa-Loc方法为多元时间序列异常诊断提供了有效的解决方案，特别是在异常定位方面取得了重要进展。

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [739] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出CreDRO方法，通过分布鲁棒优化学习多个可信模型，捕捉训练随机性和分布偏移带来的认知不确定性，在OOD检测和医疗选择性分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有认知预测器主要将认知不确定性定义为训练初始化随机性导致的模型分歧，但这主要反映对优化随机性的敏感性，而非更深层次的不确定性来源。需要捕捉训练与测试数据之间可能存在的分布偏移带来的不确定性。

Method: 提出CreDRO方法：1) 将认知不确定性定义为训练与测试数据i.i.d.假设不同松弛程度下训练模型的分歧；2) 通过分布鲁棒优化学习一组可信模型集合；3) 同时捕捉训练随机性和分布偏移带来的不确定性。

Result: 在多个基准测试中，CreDRO在OOD检测任务上一致优于现有认知方法；在医疗应用的选择性分类任务中也表现出色。

Conclusion: CreDRO通过更全面的认知不确定性定义和分布鲁棒优化，有效捕捉了训练随机性和分布偏移带来的不确定性，提高了模型的鲁棒性和可靠性。

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [740] [Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs](https://arxiv.org/abs/2602.08500)
*Maiqi Jiang,Noman Ali,Yiran Ding,Yanfu Zhang*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究了异质图神经网络中元路径注意力机制的有效性，开发了MetaXplain解释协议来评估元路径注意力是否真正反映语义重要性，并提出了MP-AEA指标量化注意力与解释的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证异质图神经网络中元路径注意力机制的基本假设：注意力权重是否真实反映了不同元路径语义的重要性。当前大多数GNN解释器是为同质图设计的，难以直接应用于异质图，需要开发专门的解释协议来实证分析注意力与重要性之间的耦合关系。

Method: 提出了MetaXplain解释协议，包含三个核心组件：(1)视图分解解释，将异质邻域分解为元路径视图；(2)模式有效的通道扰动，确保扰动符合图模式约束；(3)融合感知归因，处理多视图融合。使用梯度、扰动和Shapley风格的解释器在ACM、DBLP和IMDB数据集上评估，并与xPath和类型匹配随机基线比较。提出了MP-AEA指标量化注意力与解释的对齐程度。

Result: 实验结果显示：元路径感知解释器通常优于随机基线；MP-AEA揭示了注意力与解释在不同数据集和骨干网络中存在高对齐和显著解耦两种状态；在解释诱导的子图上重新训练通常能保持甚至在某些噪声场景下提升预测性能，显示出解释的去噪效应。

Conclusion: 元路径注意力并不总是反映语义重要性，存在解耦现象。MetaXplain协议为异质图神经网络提供了有效的后验解释框架，MP-AEA指标能够量化注意力可靠性。解释过程本身具有去噪潜力，能够提升模型鲁棒性。

Abstract: Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.

</details>


### [741] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出Causal Schrödinger Bridge (CSB)框架，将反事实推理重新表述为熵最优传输问题，使用扩散过程(SDEs)在支持集不匹配时稳健地"隧道穿越"，显著优于确定性方法。


<details>
  <summary>Details</summary>
Motivation: 确定性流(ODE)方法在因果干预下变得脆弱，特别是在需要跨低密度区域("离流形")传输概率质量时，向量场定义不明确，导致数值不稳定和虚假相关性。

Method: 引入Causal Schrödinger Bridge (CSB)框架，将反事实推理重新表述为熵最优传输问题，利用扩散过程(SDEs)在支持集不匹配时稳健地"隧道穿越"，同时严格强制执行结构可接受性约束。证明了结构分解定理，表明全局高维桥分解为局部稳健转移。

Result: 在高维干预(Morpho-MNIST)上的实证验证表明，CSB在结构一致性方面显著优于确定性基线，特别是在强分布外处理机制下。

Conclusion: CSB为因果推理提供了一种稳健的替代方案，特别适用于需要跨低密度区域传输概率质量的情况，克服了确定性方法的局限性。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [742] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出ρ-Perfect方法，用于估计主观评分数据集上模型能达到的最高相关性，量化评分可靠性问题


<details>
  <summary>Details</summary>
Motivation: 主观评分存在固有噪声，限制了模型与人类的相关性，但这种可靠性问题很少被量化。需要一种方法来估计主观评分数据集上模型能达到的最高相关性，以区分模型限制和数据质量问题。

Method: 提出ρ-Perfect方法，定义为完美预测器与人类评分之间的相关性。基于异方差噪声场景（主观评分数据集中常见）推导估计值，并证明ρ-Perfect平方估计重测相关性来验证该方法。

Result: 在语音质量数据集上演示了ρ-Perfect的应用，展示了该方法如何区分模型限制和数据质量问题。

Conclusion: ρ-Perfect为评估主观评分数据集上的模型性能提供了实用的上限估计，有助于区分模型能力和数据质量限制。

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [743] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文提出Dashed Line Defense (DLD)，一种针对基于分数的查询攻击的即插即用防御方法，通过在损失值中引入模糊性来抵御自适应攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有的运行时防御方法大多需要访问模型参数，或者容易被攻击者自适应策略绕过。即使是当前最先进的即插即用防御也能被自适应攻击绕过，这暴露了现有防御方法的严重局限性。

Method: 提出Dashed Line Defense (DLD)，一种即插即用的后处理方法。通过在观察到的损失值与候选样本的真实对抗强度之间引入模糊性，防止攻击者可靠地分析和调整查询，从而有效破坏对抗样本生成过程。

Result: 在ImageNet上的实验表明，DLD始终优于先前的防御方法，即使在最坏情况的自适应攻击下也能保持有效，同时保持模型的预测标签不变。提供了DLD防御能力的理论保证。

Conclusion: DLD为基于分数的查询攻击提供了一种有效的即插即用防御方案，能够抵御自适应攻击策略，填补了现有防御方法的不足。

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [744] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

Relevance: 65.0

TL;DR: FIM-ODE：一个预训练的基础推理模型，通过单次前向传播直接从噪声轨迹数据预测ODE向量场，在零样本和微调设置下均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前从噪声轨迹推断ODE向量场的方法（如符号回归、高斯过程回归、神经ODE）需要复杂的训练流程和大量机器学习专业知识，或严重依赖系统特定的先验知识。需要一种更简单、更通用的方法。

Method: 提出FIM-ODE，一个预训练的基础推理模型，在低次多项式向量场的ODE先验分布上进行预训练，使用神经算子表示目标场，通过单次前向传播直接从噪声轨迹数据预测向量场。

Result: FIM-ODE在零样本性能上与ODEFormer（最近的预训练符号基线）相当甚至更好，尽管使用了更简单的预训练先验分布。预训练为微调提供了强初始化，实现了快速稳定的适应，优于现代神经和GP基线。

Conclusion: FIM-ODE通过预训练基础模型的方法，简化了ODE向量场推断流程，降低了机器学习专业知识要求，在零样本和微调设置下均表现出色。

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [745] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文揭示AI开源生态中普遍存在的"许可清洗"现象：96.5%的数据集和95.8%的模型缺乏必要的许可文本，尽管标榜为MIT/Apache等宽松许可，但实际未满足法律要求，使下游用户面临法律风险。


<details>
  <summary>Details</summary>
Motivation: 宽松许可如MIT、Apache-2.0等在开源AI中占主导，但这些许可有强制性要求（包含完整许可文本、版权声明、保留上游归属）。当前缺乏对这些要求的大规模验证，导致"许可清洗"现象：标榜自由使用但缺少必要法律文档，使下游用户面临法律风险。

Method: 对124,278条数据集→模型→应用供应链进行实证审计，涵盖Hugging Face和GitHub上的3,338个数据集、6,664个模型和28,516个应用。开发可复现的审计流程，检查许可文本、版权声明和归属传播的合规性。

Result: 惊人发现：96.5%的数据集和95.8%的模型缺乏所需许可文本；仅2.3%的数据集和3.2%的模型同时满足许可文本和版权要求；即使上游提供完整许可证据，归属也很少向下游传播：仅27.59%的模型保留合规的数据集声明，仅5.75%的应用保留合规的模型声明。

Conclusion: 从业者不能假设宽松许可标签能提供其声称的权利：许可文件和声明（而非元数据）才是法律真实性的来源。论文发布完整审计数据集和可复现流程以支持未来研究。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [746] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文提出SimpliPy简化引擎，实现比SymPy快100倍的速度提升，并基于此构建Flash-ANSR框架，在符号回归任务中达到与最先进方法相当的性能，同时能恢复更简洁而非更复杂的表达式。


<details>
  <summary>Details</summary>
Motivation: 摊销式符号回归(SR)比遗传编程方法更高效，但难以扩展到实际科学问题的复杂度。主要障碍是缺乏快速将等价表达式简化为简洁规范形式的方法。现有方法使用通用计算机代数系统如SymPy，但计算成本过高，严重限制了训练和推理速度。

Method: 提出SimpliPy，一个基于规则的简化引擎，实现比SymPy快100倍的速度提升。基于此构建Flash-ANSR框架，利用简化引擎实现：1)扩展到更大训练集；2)更高效利用每个表达式的token预算；3)系统性地去除训练集中与测试表达式等价的污染数据。

Result: 在FastSRB基准测试中，Flash-ANSR比摊销式基线方法(NeSymReS, E2E)准确率更高。与最先进的直接优化方法(PySR)性能相当，但能随着推理预算增加恢复更简洁而非更复杂的表达式。

Conclusion: SimpliPy简化引擎显著提升了摊销式符号回归的效率和可扩展性，使该方法能够处理更复杂的科学问题。Flash-ANSR框架展示了在保持准确性的同时恢复更简洁表达式的优势。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [747] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于生成模型的分布鲁棒优化(GAS-DRO)，使用生成模型构建歧义集，提升机器学习任务的分布外泛化性能


<details>
  <summary>Details</summary>
Motivation: 分布鲁棒优化(DRO)需要构建既保持与名义分布一致性，又能覆盖各种潜在场景的歧义集。现有方法在覆盖分布外场景和计算可处理性方面存在局限

Method: 提出生成模型歧义集，使用生成模型参数化对抗分布空间，在此基础上开发GAS-DRO算法，通过求解生成模型参数空间的内层最大化问题实现可处理的DRO

Result: 理论证明了GAS-DRO的平稳收敛性能，实验中使用扩散模型实现GAS-DRO，在机器学习任务中展现出优越的分布外泛化性能

Conclusion: 生成模型歧义集为DRO提供了既保持一致性又覆盖分布外场景的有效框架，GAS-DRO算法在理论和实验上都表现出色

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [748] [Tighter Information-Theoretic Generalization Bounds via a Novel Class of Change of Measure Inequalities](https://arxiv.org/abs/2602.07999)
*Yanxiao Liu,Yijun Fan an Deniz Gündüz*

Main category: cs.IT

Relevance: 65.0

TL;DR: 提出基于f-散度数据处理不等式的统一框架，得到更紧的测度变化不等式，应用于随机学习算法的泛化误差分析，得到新的信息论泛化界


<details>
  <summary>Details</summary>
Motivation: 现有信息论泛化界通常不够紧致，需要更强大且灵活的理论框架来推导更紧的泛化误差界限，同时统一多种信息度量的分析方法

Method: 基于f-散度的数据处理不等式构建统一框架，推导出涵盖f-散度、Rényi散度、α-互信息等多种信息度量的测度变化不等式，并将其嵌入到随机学习算法的泛化误差分析中

Result: 得到了比现有方法更紧的高概率信息论泛化界，同时简化了多个最佳已知结果的证明，框架灵活适应条件互信息、PAC-Bayesian理论和差分隐私等多种设置

Conclusion: 提出的统一框架不仅提供了更紧的泛化界，还简化了分析过程，具有很好的灵活性，可应用于多种机器学习理论场景

Abstract: In this paper, we propose a novel class of change of measure inequalities via a unified framework based on the data processing inequality for $f$-divergences, which is surprisingly elementary yet powerful enough to yield tighter inequalities. We provide change of measure inequalities in terms of a broad family of information measures, including $f$-divergences (with Kullback-Leibler divergence and $χ^2$-divergence as special cases), Rényi divergence, and $α$-mutual information (with maximal leakage as a special case). We then embed these inequalities into the analysis of generalization error for stochastic learning algorithms, yielding novel and tighter high-probability information-theoretic generalization bounds, while also recovering several best-known results via simplified analyses. A key advantage of our framework is its flexibility: it readily adapts to a range of settings, including the conditional mutual information framework, PAC-Bayesian theory, and differential privacy mechanisms, for which we derive new generalization bounds.

</details>


### [749] [Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion](https://arxiv.org/abs/2602.08668)
*Scott Thornton*

Main category: cs.CR

Relevance: 65.0

TL;DR: 混合检索增强生成(RAG)管道结合向量相似性搜索与知识图谱扩展进行多跳推理，但这种组合引入了新的安全漏洞：向量检索的"种子"块可通过实体链接访问敏感图谱邻域，导致跨租户数据泄露。


<details>
  <summary>Details</summary>
Motivation: 现有混合RAG系统将向量检索与知识图谱扩展结合，但未考虑这种组合带来的安全风险。当向量检索组件与图谱组件结合时，即使每个组件单独安全，组合后也可能产生跨租户数据泄露漏洞。

Method: 1) 形式化定义检索枢轴风险(RPR) 2) 提出三个量化指标：Leakage@k、放大因子、枢轴深度(PD) 3) 设计七种检索枢轴攻击 4) 在两个语料库（合成企业语料和Enron邮件语料）上评估风险 5) 提出在图谱扩展边界实施授权的防御方案

Result: 1) 未防御的混合管道显示高风险(RPR高达0.95)，每个查询返回多个未授权项目 2) 泄露主要出现在PD=2深度，由二分块-实体拓扑结构导致 3) 在图谱扩展边界实施授权可完全消除泄露(RPR接近0)，对所有攻击变体和高达10%的标签伪造率都有效，开销最小

Conclusion: 混合RAG系统的安全漏洞根源在于边界授权缺失，而非组件本身不安全。两个单独安全的检索组件组合可能形成不安全系统，必须在过渡点重新检查授权。简单的边界授权即可有效防御此类攻击。

Abstract: Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved "seed" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.
  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.
  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.

</details>


### [750] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

Relevance: 45.0

TL;DR: TransConv-DDPM是一种用于生物力学和生理时间序列数据生成的增强型生成AI方法，结合了DDPM、U-Net、多尺度卷积模块和Transformer层，在多个数据集上表现出色，并能提升预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 临床领域缺乏真实世界数据阻碍了医学诊断和预防工具中有效AI模型的训练。虽然生成式AI在计算机视觉和NLP领域已显示出增加数据量和增强模型训练的潜力，但生成生理时间序列数据面临独特挑战，因其固有的复杂性和变异性。

Method: 提出TransConv-DDPM方法，采用去噪扩散概率模型（DDPM）结合U-Net架构，集成多尺度卷积模块和Transformer层，以捕捉全局和局部的时间依赖性。在三个不同数据集上评估，生成长短序列时间序列数据。

Result: 在SmartFallMM和EEG数据集上表现优异，能有效捕捉数据点间更渐进的时间变化模式。在SmartFallMM数据集上的效用测试显示，添加TransConv-DDPM生成的合成跌倒数据使预测模型F1分数提升13.64%，总体准确率提高14.93%。

Conclusion: TransConv-DDPM能够生成高质量的合成数据用于实际应用，为解决医学AI中的数据稀缺问题提供了有前景的解决方案。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [751] [Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting](https://arxiv.org/abs/2602.07126)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出了一种针对合成关系数据的多表成员推理攻击（MT-MIA），通过异构图神经网络学习用户实体的表示，揭示了现有单表攻击低估了用户级隐私泄露的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据大多存在于关系数据库中，用户信息分布在多个相互关联的表中。现有的合成关系数据生成方法虽然取得了进展，但数据发布带来了独特的隐私挑战：信息不仅可能从单个项目泄露，还可能通过构成完整用户实体的关系泄露。

Method: 提出了多表成员推理攻击（MT-MIA），在No-Box威胁模型下，通过异构图神经网络（HGNN）学习用户实体的表示，利用所有连接的项目来更好地针对用户级漏洞。该方法能够审计合成关系数据的经验用户级隐私。

Result: MT-MIA在多个真实世界多表数据集上进行了评估，证明这种漏洞存在于最先进的关系合成数据生成器中。与单表MIA相比，MT-MIA能够更准确地揭示用户级隐私泄露，并进一步研究了泄露发生的位置。

Conclusion: 合成关系数据存在用户级隐私泄露风险，传统的单表成员推理攻击低估了这种风险。提出的MT-MIA方法能够更有效地审计用户级隐私，揭示了关系结构带来的额外隐私挑战。

Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.

</details>


### [752] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

Relevance: 45.0

TL;DR: BONSAI是一种默认感知的贝叶斯优化策略，在保持优化性能的同时减少对默认配置的偏离，特别适用于需要最小化参数变更的实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，参数调优通常有一个精心设计的默认配置，实践者只希望在必要时才偏离默认值。但标准贝叶斯优化不旨在最小化对默认配置的偏离，经常将弱相关参数推到搜索空间边界，这使得难以区分重要变更和虚假变更，增加了审查推荐配置的负担。

Method: BONSAI是一种默认感知的贝叶斯优化策略，通过剪枝对默认配置的低影响偏离，同时明确控制获取函数的损失。该方法兼容多种获取函数，包括期望改进和上置信界(GP-UCB)。

Result: 理论分析表明，在某些条件下BONSAI具有与标准GP-UCB相同的无遗憾特性。实证结果显示，BONSAI在保持竞争力的优化性能的同时，显著减少了推荐配置中非默认参数的数量，对运行时间影响很小。

Conclusion: BONSAI提供了一种实用的贝叶斯优化方法，能够在保持优化性能的同时最小化对默认配置的偏离，解决了实际应用中参数调优的重要实际问题。

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [753] [Mimetic Initialization of MLPs](https://arxiv.org/abs/2602.07156)
*Asher Trockman,J. Zico Kolter*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文首次将模仿初始化方法应用于通道混合层（MLPs），提出了一种极其简单的技术：给第一层赋予非零均值，从而在小规模视觉任务上加速训练。


<details>
  <summary>Details</summary>
Motivation: 模仿初始化方法使用预训练模型作为良好初始化的案例研究，通过观察训练权重中的结构来启发新的简单初始化技术。此前该方法仅应用于空间混合层（如卷积、自注意力和状态空间层），本文旨在将其扩展到通道混合层（MLPs）。

Method: 提出了一种极其简单的MLP初始化技术：给第一层赋予非零均值。该方法可以与空间混合初始化方法结合使用，产生叠加的正向效果。

Result: 该技术在CIFAR-10和ImageNet-1k等小规模视觉任务上加速了训练。虽然其效果比空间混合初始化小得多，但可以与它们结合使用以获得额外的正向效果。

Conclusion: 这是首次将模仿初始化方法成功应用于通道混合层（MLPs），证明了即使是简单的非零均值初始化也能在视觉任务中带来训练加速效果，为模型初始化方法提供了新的思路。

Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.

</details>


### [754] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文研究神经组合优化模型（特别是TSP求解器）的内部表示是否可迁移到其他优化相关任务，如节点移除敏感性和边保留敏感性分析，发现训练好的TSP求解器可以作为可迁移编码器用于下游决策支持任务。


<details>
  <summary>Details</summary>
Motivation: 研究神经组合优化模型是否不仅能解决原始优化问题（如TSP），还能学习到可迁移的内部表示，用于其他优化相关的下游任务，实现类似其他领域的迁移学习效果。

Method: 训练多个基于注意力机制的TSP策略模型，收集其内部激活，然后训练探针（probes）来预测节点/边嵌入，用于两个NP-hard下游任务：节点移除敏感性和边保留敏感性分析。

Result: 在Euclidean TSP100训练的模型上，两个任务的探针性能均与现有基线相当。将探针信号与几何特征集成后，性能超过最强基线：最佳节点移除任务达到65% top-1准确率（基线58%），最差边识别任务达到73% top-1准确率（基线67%）。

Conclusion: 神经TSP求解器可以作为可迁移编码器用于下游决策支持任务，且迁移准确率随求解器质量和模型规模增加而提高，表明训练更强的NCO求解器也能产生更有用的下游编码器。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [755] [Compact Conformal Subgraphs](https://arxiv.org/abs/2602.07530)
*Sreenivas Gollapudi,Kostas Kollias,Kamesh Munagala,Aravindan Vijayaraghavan*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出图基保形压缩框架，通过选择覆盖指定概率质量的最小子图来构建紧凑预测集，在保持统计有效性的同时减少结构复杂性，并设计了高效的近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在结构化领域（如路由、规划、序列推荐）中产生的预测集往往过大，需要一种既能保持统计有效性又能减少结构复杂性的压缩方法。

Method: 将压缩问题形式化为选择覆盖指定概率质量的最小子图，归结为超图中加权最密k子图问题，利用参数最小割的单调性保证嵌套性，设计高效的近似算法。

Result: 证明了松弛方法满足单调性特性，保证了有效的保形保证；设计了达到常数因子覆盖和大小权衡的近似算法；通过行程规划和导航模拟验证了算法有效性。

Conclusion: 该框架将高效保形预测与组合图压缩通过单调性联系起来，为统计有效性和压缩大小提供了严格保证，同时揭示了一个不同于经典最密k子图硬度设置的算法机制。

Abstract: Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce "graph-based conformal compression", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.

</details>


### [756] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

Relevance: 45.0

TL;DR: HARPOON是一种基于流形理论的表格数据扩散方法，通过推理时引导无约束样本沿流形几何来满足多样化的表格条件，解决了现有方法无法泛化到未见约束的问题。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据生成方法存在两个主要问题：1）训练时策略无法泛化到推理时未见约束；2）只能处理表格插补等有限任务。流形理论提供了原则性的生成引导方法，但现有公式局限于特定推理目标和连续域。

Method: 将流形理论扩展到表格数据，扩大其处理多样化推理目标的能力。在此基础上提出HARPOON方法，这是一种表格扩散方法，在推理时引导无约束样本沿流形几何来满足多样化的表格条件。

Result: 在插补和不等式约束等任务上验证了理论贡献，HARPOON在多样化数据集上表现出强大性能，证明了流形感知引导对表格数据的实际益处。

Conclusion: HARPOON通过流形理论扩展和推理时引导，为表格数据生成提供了更灵活和强大的条件控制能力，能够处理多样化的推理目标。

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [757] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

Relevance: 45.0

TL;DR: CausalCompass是一个用于评估时间序列因果发现方法在建模假设违反情况下的鲁棒性基准测试套件，通过八个假设违反场景测试发现深度学习方法的整体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 时间序列因果发现的广泛应用受到两个主要限制：1) 对不可检验的因果假设的依赖；2) 现有基准测试缺乏面向鲁棒性的评估。作者旨在解决这些问题，促进时间序列因果发现方法在现实世界应用中的更广泛采用。

Method: 提出了CausalCompass基准测试套件，设计为灵活且可扩展，专门用于评估时间序列因果发现方法在建模假设违反情况下的鲁棒性。在八个不同的假设违反场景下对代表性算法进行了广泛的基准测试，并提供了超参数敏感性分析。

Result: 实验结果显示：1) 没有单一方法在所有设置中始终达到最优性能；2) 在不同场景中表现出优越整体性能的方法几乎都是基于深度学习的方法；3) NTS-NOTEARS在实践中严重依赖标准化预处理，在原始设置中表现不佳但在标准化后表现强劲。

Conclusion: CausalCompass为时间序列因果发现方法在假设违反情况下提供了全面系统的评估框架，有助于促进这些方法在现实世界应用中的更广泛采用。深度学习基方法在鲁棒性方面表现突出。

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [758] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文（第二部分）从轨迹层面研究持续学习中的不可逆性，提出"关键期关闭"概念，解释灾难性遗忘是有限时间耗散导致的动态约束而非直接任务干扰。


<details>
  <summary>Details</summary>
Motivation: 研究有限时间学习必然不可逆的后果，特别是对持续学习的影响。第一部分已建立参数分布空间中学习的传输模型并推导出认知速度极限，第二部分从轨迹层面探索这种不可逆性如何限制学习路径的动态可达性。

Method: 采用轨迹层面分析方法，将学习建模为参数分布空间的传输过程，研究有限耗散如何约束学习路径的动态可达性。通过分析任务等效实现的连续统，探索有限时间学习如何不可逆地选择特定实现。

Result: 发现有限耗散不仅约束可达解，还约束动态可达的学习路径。有限时间学习通过逐步消除能够实现结构重构的自由度，在任务等效实现中不可逆地选择特定实现，这种现象称为"关键期关闭"。

Conclusion: 持续学习失败不是由于缺乏满足多个任务的解，而是由先前学习引起的表征自由度的不可逆损失导致的。这重新定义了灾难性遗忘，将其视为有限时间耗散施加的动态约束，而非直接的任务干扰。

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [759] [Inverting Data Transformations via Diffusion Sampling](https://arxiv.org/abs/2602.08267)
*Jinwoo Kim,Sékou-Oumar Kaba,Jiyun Park,Seunghoon Hong,Siamak Ravanbakhsh*

Main category: cs.LG

Relevance: 45.0

TL;DR: TIED是一种在一般李群上进行变换反演的方法，通过扩散过程在流形上采样变换后验，用于提升预训练神经网络对输入变换的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中经常存在未知的群变换会显著扭曲观测数据，需要恢复将数据映射回原始分布的逆变换。特别是在测试时等变性方面，需要提升预训练网络对输入变换的鲁棒性。

Method: 采用概率视角，将变换后验建模为由数据空间能量函数定义的玻尔兹曼分布。提出在一般李群上的扩散过程（TIED），保持所有更新在流形上，仅需在李代数中进行计算。利用新的平凡化目标-分数恒等式实现高效的基于分数的变换后验采样。

Result: 在图像单应性和PDE对称性实验中，TIED能够在测试时将变换后的输入恢复到训练分布，相比强基准的规范化方法和采样方法表现出更好的性能。

Conclusion: TIED提供了一种有效的李群变换反演方法，特别适用于提升预训练神经网络在测试时对输入变换的鲁棒性，在多种变换类型上展现出优越性能。

Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.

</details>


### [760] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出基于流匹配的条件独立性检验方法FMCIT，显著加速因果发现中的条件独立性测试，并结合两阶段引导PC框架GPC-FMCIT实现准确性与效率的良好权衡。


<details>
  <summary>Details</summary>
Motivation: 传统基于约束的因果发现方法需要进行大量条件独立性测试，计算复杂度高，限制了实际应用。需要设计能加速单个测试的算法。

Method: 提出流匹配条件独立性检验FMCIT，利用流匹配的高计算效率，整个因果发现过程只需训练一次模型。进一步集成到两阶段引导PC框架GPC-FMCIT中，结合快速筛选和引导的预算精炼。

Result: FMCIT能有效控制I类错误，在备择假设下保持高检验功效，即使在高维条件集下也表现良好。GPC-FMCIT在合成和真实因果发现任务中展现出优于现有方法的准确性与效率权衡。

Conclusion: FMCIT显著加速因果发现中的条件独立性测试，GPC-FMCIT框架在有限查询预算下保持高统计功效，为因果发现提供了实用的高效解决方案。

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [761] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该研究从随机偏微分方程角度分析基于分数的生成模型，将分数估计误差视为驱动Fokker-Planck方程的随机源，提出几何稳定性和位移凸性框架，并开发基于SPDE解二次变分的评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统基于粒子的SDE分析方法难以全面理解基于分数的生成模型的动态特性，需要从概率密度场演化的角度建立更系统的理论框架来分析生成模型的鲁棒性和评估方法。

Method: 采用随机偏微分方程框架建模概率密度场在随机漂移扰动下的演化，在简化设置下利用几何稳定性和位移凸性理论分析生成模型鲁棒性，并基于SPDE解在径向测试函数上的二次变分提出候选评估指标。

Result: 初步观察表明，仅使用采样轨迹前10%的数据，该评估指标仍能保持有效性，显示出计算效率的潜力，为生成模型评估提供了新的理论视角。

Conclusion: SPDE框架为理解基于分数的生成模型动态提供了新视角，几何稳定性和位移凸性分析有助于解释模型鲁棒性，提出的评估指标具有计算效率优势，为生成模型理论分析和评估开辟了新方向。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [762] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

Relevance: 45.0

TL;DR: LEFT提出了一种基于三视图令牌可学习融合的无监督时间序列异常检测框架，通过频率域、时域和多尺度视图的一致性建模来检测异常，在保持高检测精度的同时显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 无监督时间序列异常检测的关键挑战在于许多异常过于细微，无法在单一视图（如时域）中检测到，而是表现为跨多个视图（如时间、频率和混合分辨率）的不一致性。现有跨视图方法大多依赖特征或分数融合，缺乏分析-合成一致性约束。

Method: LEFT框架从三个互补视图学习特征令牌：1) 频率域令牌嵌入周期性信息；2) 时域令牌捕捉局部动态；3) 多尺度令牌学习不同时间序列粒度的异常模式。通过可学习的奈奎斯特约束谱滤波器将原始时间序列重缩放为多个分辨率。引入新颖的目标函数从粗粒度多尺度结构重建细粒度目标，并提出创新的时频循环一致性约束来显式正则化跨视图一致性。

Result: 在真实世界基准测试中，LEFT相比SOTA基线取得了最佳检测精度，同时实现了5倍的FLOPs减少和8倍的训练加速。

Conclusion: LEFT通过三视图令牌的可学习融合和跨视图一致性约束，有效解决了无监督时间序列异常检测中细微异常难以检测的问题，在精度和效率方面均表现出色。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [763] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

Relevance: 45.0

TL;DR: Geo-IQL：一种计算高效的离线强化学习框架，通过基于k近邻距离的密度惩罚来增强IQL，有效解决OOD动作高估问题，在医疗数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布外动作高估问题，现有方法在计算效率和性能之间存在权衡。CQL计算成本高，而IQL在病态数据集上容易退化为行为克隆。需要一种计算高效且能有效处理OOD误差的方法。

Method: 提出几何悲观主义框架，通过状态-动作嵌入空间中的k近邻距离计算密度惩罚，将其作为奖励塑形项预计算并注入标准IQL中，实现O(1)训练开销。

Result: 在D4RL MuJoCo基准测试中，Geo-IQL在敏感和不稳定的medium-replay任务上比标准IQL提升18+分，种子间方差降低4倍。在MIMIC-III Sepsis数据集上，标准IQL退化为行为克隆，而Geo-IQL实现主动策略改进，与临床医生终末决策一致性达86.4%（IQL为75%）。

Conclusion: 几何悲观主义提供了必要的正则化，能够在关键现实世界决策系统中安全克服局部最优，为离线RL提供了一种计算高效且性能优越的解决方案。

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [764] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出基于逆强化学习的扩散模型采样策略学习框架，无需重新训练去噪器，通过马尔可夫决策过程优化采样动态，提升生成质量和效率


<details>
  <summary>Details</summary>
Motivation: 扩散模型的训练计算成本高，但采样过程具有灵活性。现有方法通常需要显式定义奖励函数来优化采样策略，这限制了方法的通用性和适应性。本文旨在开发一种无需重新训练去噪器、能够自动学习优化采样策略的框架。

Method: 将扩散采样过程建模为离散时间有限时域马尔可夫决策过程，其中动作对应采样动态的可选修改。采用逆强化学习框架，避免显式定义奖励函数，直接使用策略梯度技术匹配目标采样行为。通过优化动作调度来改进采样质量。

Result: 实验证明该方法能够提升预训练扩散模型的生成样本质量，并自动调整采样超参数。展示了在无需重新训练去噪器的情况下，通过优化采样策略实现性能改进的有效性。

Conclusion: 提出了一种新颖的逆强化学习框架，用于优化扩散模型的采样过程。该方法避免了显式奖励函数设计，能够自动学习有效的采样策略，为扩散模型的高效采样提供了新思路。

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [765] [On the Expressive Power of GNNs for Boolean Satisfiability](https://arxiv.org/abs/2602.08745)
*Saku Peltonen,Roger Wattenhofer*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文分析了图神经网络在SAT求解中的表达能力，证明Weisfeiler-Leman层次结构无法区分可满足与不可满足实例，并研究了不同SAT实例家族的表达需求。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习方法（特别是GNNs）在布尔可满足性问题求解中的表达能力，理解GNNs在SAT求解中的理论局限性，为设计更强大的学习型SAT求解器提供理论基础。

Method: 通过Weisfeiler-Leman测试的理论框架分析GNNs的表达能力，证明WL层次结构无法区分可满足与不可满足实例，研究正则、随机、平面等SAT实例家族的表达需求，并在G4SAT基准和国际SAT竞赛的工业实例上进行实验验证。

Result: 理论证明WL层次结构无法区分可满足与不可满足实例，实验表明随机实例大多可区分，而工业实例通常需要更强的表达能力来预测满足赋值。

Conclusion: GNNs在SAT求解中存在理论表达能力限制，工业实例需要超越WL层次结构的表达能力，这为设计更强大的学习型SAT求解器提供了重要指导。

Abstract: Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.

</details>


### [766] [MolLIBRA: Genetic Molecular Optimization with Multi-Fingerprint Surrogates and Text-Molecule Aligned Critic](https://arxiv.org/abs/2602.07002)
*Masahi Okada,Kazuki Sakai,Hiroaki Yoshida,Masaki Okoshi,Tadahiro Taniguchi*

Main category: cs.NE

Relevance: 45.0

TL;DR: MolLIBRA：一种多模态遗传算法框架，通过GP集成和文本-分子对齐编码器CLAMP预排序候选分子，在有限评估预算下实现高效分子优化


<details>
  <summary>Details</summary>
Motivation: 在有限计算资源（如1000次评估）下实现高效的分子优化是一个重要挑战。传统方法需要大量昂贵的实验评估，因此需要开发能够在少量评估中有效筛选候选分子的方法。

Method: 提出MolLIBRA框架，基于遗传算法，在调用昂贵评估前使用两种critic预排序候选分子：1）基于多种分子指纹的高斯过程集成，自适应选择任务合适的指纹；2）预训练的文本-分子对齐编码器CLAMP，通过测量分子和任务描述文本嵌入的相似性提供零样本评分。

Result: 在PMO-1K基准测试中，MolLIBRA-L（语言模型候选生成器变体）在22个任务中的14个上获得最佳Top-10 AUC，并且在所有任务上的Top-10 AUC总和最高。

Conclusion: MolLIBRA通过结合多模态信息（分子指纹和文本描述）和预排序策略，在有限评估预算下实现了高效的分子优化，展示了多模态方法在科学发现任务中的潜力。

Abstract: We study sample-efficient molecular optimization under a limited budget of oracle evaluations. We propose MolLIBRA (MultimOdaLity and Language Integrated Bayesian and evolutionaRy optimizAtion), a genetic algorithm based framework that pre-ranks candidate molecules using multiple critics before oracle calls: (i) an ensemble of Gaussian process (GP) surrogates defined over multiple molecular fingerprints and (ii) a pretrained text-molecule aligned encoder CLAMP. The GP ensemble enables adaptive selection of task-appropriate fingerprints, while CLAMP provides a zero-shot scoring signal from task descriptions by measuring the similarity between molecular and text embeddings. On the Practical Molecular Optimization (PMO) benchmark with a budget of 1,000 evaluations (PMO-1K), MolLIBRA-L, our variant with a language-model-based candidate generator, attains the best Top-10 AUC on 14/22 tasks and the highest overall sum of Top-10 AUC across tasks among prior methods.

</details>


### [767] [Flow-Based Conformal Predictive Distributions](https://arxiv.org/abs/2602.07633)
*Trevor Harris*

Main category: stat.ML

Relevance: 45.0

TL;DR: 提出一种通过确定性流采样高维置信预测集边界的方法，将置信预测扩展到结构化输出空间


<details>
  <summary>Details</summary>
Motivation: 传统置信预测在低维空间容易解释，但在高维或结构化输出空间中难以表示和使用，限制了其在下游任务（如采样和概率预测）中的应用

Method: 利用可微非一致性分数诱导输出空间上的确定性流，其轨迹收敛到置信预测集边界，实现无需训练的高效边界采样方法

Result: 方法在PDE反问题、降水降尺度、气候模型去偏和飓风轨迹预测等任务中验证有效，能够生成点预测集和置信预测分布

Conclusion: 提出的流方法为高维置信预测提供了计算高效、无需训练的边界采样技术，扩展了置信预测在复杂输出空间的应用

Abstract: Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk, and mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting.

</details>


### [768] [BFTS: Thompson Sampling with Bayesian Additive Regression Trees](https://arxiv.org/abs/2602.07767)
*Ruizhe Deng,Bibhas Chakraborty,Ran Chen,Yan Shuo Tan*

Main category: stat.ML

Relevance: 45.0

TL;DR: BFTS将贝叶斯加性回归树（BART）集成到上下文赌博机中，为个性化移动健康干预提供理论保证且实用的探索策略。


<details>
  <summary>Details</summary>
Motivation: 移动健康干预需要适应复杂的非线性用户行为。现有方法存在局限：线性模型偏差高，神经网络在线设置中难以调优，树集成方法缺乏概率基础。需要一种既具有理论保证又能处理非线性关系的探索策略。

Method: 提出贝叶斯森林汤普森采样（BFTS），首次将完全概率性的BART模型直接集成到上下文赌博机的探索循环中。BART作为求和树模型，提供原则性的不确定性量化。

Result: 理论证明BFTS具有信息论贝叶斯遗憾界$\tilde{O}(\sqrt{T})$，其"feel-good"变体达到频率主义极小极大最优性。实证显示在表格基准测试中达到最先进遗憾，不确定性校准接近名义值。在Drink Less微随机试验中，BFTS比部署策略提升30%以上的参与率。

Conclusion: BFTS成功将概率树模型集成到上下文赌博机中，为非线性奖励函数提供了理论保证且实用的探索策略，在移动健康干预等应用中表现出色。

Abstract: Contextual bandits are a core technology for personalized mobile health interventions, where decision-making requires adapting to complex, non-linear user behaviors. While Thompson Sampling (TS) is a preferred strategy for these problems, its performance hinges on the quality of the underlying reward model. Standard linear models suffer from high bias, while neural network approaches are often brittle and difficult to tune in online settings. Conversely, tree ensembles dominate tabular data prediction but typically rely on heuristic uncertainty quantification, lacking a principled probabilistic basis for TS. We propose Bayesian Forest Thompson Sampling (BFTS), the first contextual bandit algorithm to integrate Bayesian Additive Regression Trees (BART), a fully probabilistic sum-of-trees model, directly into the exploration loop. We prove that BFTS is theoretically sound, deriving an information-theoretic Bayesian regret bound of $\tilde{O}(\sqrt{T})$. As a complementary result, we establish frequentist minimax optimality for a "feel-good" variant, confirming the structural suitability of BART priors for non-parametric bandits. Empirically, BFTS achieves state-of-the-art regret on tabular benchmarks with near-nominal uncertainty calibration. Furthermore, in an offline policy evaluation on the Drink Less micro-randomized trial, BFTS improves engagement rates by over 30% compared to the deployed policy, demonstrating its practical effectiveness for behavioral interventions.

</details>


### [769] [Discrete Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2602.08243)
*Wei Guo,Yuchen Zhu,Xiaochen Du,Juno Nam,Yongxin Chen,Rafael Gómez-Bombarelli,Guan-Horng Liu,Molei Tao,Jaemoo Choi*

Main category: stat.ML

Relevance: 45.0

TL;DR: 本文提出了离散ASBS框架，将连续域中的伴随匹配和薛定谔桥采样器扩展到离散空间，解决了离散神经采样器训练中的梯度缺失和组合复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 离散神经采样器的学习面临梯度缺失和组合复杂性的挑战。虽然随机最优控制和薛定谔桥提供了理论解决方案，但在连续域表现优异的伴随匹配等高效SOC求解器在离散空间中尚未探索。

Method: 通过揭示伴随匹配的核心机制是状态空间无关的，提出了离散ASBS统一框架，将AM和伴随薛定谔桥采样器扩展到离散空间。理论分析了离散SB问题的最优条件及其与SOC的联系，确定了实现此扩展所需的循环群结构。

Result: 离散ASBS在样本质量上达到竞争性水平，同时在训练效率和可扩展性方面具有显著优势。

Conclusion: 该工作成功地将连续域的高效SOC求解器扩展到离散空间，为离散神经采样器的学习提供了新的有效框架。

Abstract: Learning discrete neural samplers is challenging due to the lack of gradients and combinatorial complexity. While stochastic optimal control (SOC) and Schrödinger bridge (SB) provide principled solutions, efficient SOC solvers like adjoint matching (AM), which excel in continuous domains, remain unexplored for discrete spaces. We bridge this gap by revealing that the core mechanism of AM is $\mathit{state}\text{-}\mathit{space~agnostic}$, and introduce $\mathbf{discrete~ASBS}$, a unified framework that extends AM and adjoint Schrödinger bridge sampler (ASBS) to discrete spaces. Theoretically, we analyze the optimality conditions of the discrete SB problem and its connection to SOC, identifying a necessary cyclic group structure on the state space to enable this extension. Empirically, discrete ASBS achieves competitive sample quality with significant advantages in training efficiency and scalability.

</details>


### [770] [Is Flow Matching Just Trajectory Replay for Sequential Data?](https://arxiv.org/abs/2602.08318)
*Soon Hoe Lim,Shizheng Lin,Michael W. Mahoney,N. Benjamin Erichson*

Main category: stat.ML

Relevance: 45.0

TL;DR: 该论文研究了流匹配在时间序列生成中的理论基础，揭示了在完美函数逼近下，最优速度场是一个非参数、带记忆的连续时间动力系统，可以表示为过去转移的相似性加权混合。


<details>
  <summary>Details</summary>
Motivation: 流匹配在时间序列生成中应用日益广泛，但人们对其学习机制理解不足：它到底是学习了一般的动力结构，还是仅仅进行了有效的"轨迹重放"？作者希望从理论上阐明流匹配在序列数据上的本质。

Method: 推导了在完美函数逼近下，经验流匹配目标在序列数据上的最优速度场。对于实践中常用的高斯条件路径，证明了隐含的采样器是一个ODE，其动力学构成了非参数、带记忆的连续时间动力系统。最优场具有闭式表达式，是过去转移诱导的瞬时速度的相似性加权混合。

Result: 揭示了流匹配的本质：通过随机优化训练的神经流匹配模型是理想非参数解的参数化替代。基于最优场的结构，提出了改进ODE生成效率和数值鲁棒性的采样和近似方案。在非线性动力系统基准测试中，得到的闭式采样器直接从历史转移中产生强大的概率预测，无需训练。

Conclusion: 流匹配学习了一个记忆增强的连续时间动力系统，其最优速度场具有明确的闭式表达式和可解释的数据集依赖性。这一理论洞察不仅阐明了流匹配的工作原理，还为设计更高效、鲁棒的生成方法提供了基础。

Abstract: Flow matching (FM) is increasingly used for time-series generation, but it is not well understood whether it learns a general dynamical structure or simply performs an effective "trajectory replay". We study this question by deriving the velocity field targeted by the empirical FM objective on sequential data, in the limit of perfect function approximation. For the Gaussian conditional paths commonly used in practice, we show that the implied sampler is an ODE whose dynamics constitutes a nonparametric, memory-augmented continuous-time dynamical system. The optimal field admits a closed-form expression as a similarity-weighted mixture of instantaneous velocities induced by past transitions, making the dataset dependence explicit and interpretable. This perspective positions neural FM models trained by stochastic optimization as parametric surrogates of an ideal nonparametric solution. Using the structure of the optimal field, we study sampling and approximation schemes that improve the efficiency and numerical robustness of ODE-based generation. On nonlinear dynamical system benchmarks, the resulting closed-form sampler yields strong probabilistic forecasts directly from historical transitions, without training.

</details>


### [771] [PACC: Protocol-Aware Cross-Layer Compression for Compact Network Traffic Representation](https://arxiv.org/abs/2602.08331)
*Zhaochen Guo,Tianyufei Zhou,Honghao Wang,Ronghua Li,Shinan Liu*

Main category: cs.NI

Relevance: 45.0

TL;DR: PACC提出了一种冗余感知、层次感知的网络流量表示框架，通过多视图学习将协议栈分解为共享和私有组件，在加密流量分类、IoT设备识别和入侵检测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网络流量分类面临加密和协议演变的挑战。现有方法存在局限性：手工特征过于损失信息，原始比特编码成本高，预训练嵌入会扁平化协议层次并跨层纠缠信号。真实流量在跨层和层内都存在大量冗余，现有范式未能显式识别和去除这些冗余，导致容量浪费、捷径学习和泛化能力下降。

Method: PACC将协议栈视为多视图输入，学习紧凑的层次感知投影。框架将表示分解为共享（跨层）和私有（层特定）组件，通过联合目标函数实现：1）通过重构保留层次特定信息；2）通过对比互信息学习捕获共享结构；3）通过监督损失最大化任务相关信息，生成适用于高效推理的紧凑潜在表示。

Result: 在加密应用分类、IoT设备识别和入侵检测数据集上，PACC一致优于特征工程和原始比特基线。在加密子集上，比nPrint准确率提升高达12.9%。PACC匹配或超越了强基础模型基线，同时端到端效率提升高达3.16倍。

Conclusion: PACC通过冗余感知和层次感知的表示学习框架，解决了网络流量分类中的表示瓶颈问题。该方法在保持准确性的同时显著提升了效率，为加密流量分析提供了有效的解决方案。

Abstract: Network traffic classification is a core primitive for network security and management, yet it is increasingly challenged by pervasive encryption and evolving protocols. A central bottleneck is representation: hand-crafted flow statistics are efficient but often too lossy, raw-bit encodings can be accurate but are costly, and recent pre-trained embeddings provide transfer but frequently flatten the protocol stack and entangle signals across layers. We observe that real traffic contains substantial redundancy both across network layers and within each layer; existing paradigms do not explicitly identify and remove this redundancy, leading to wasted capacity, shortcut learning, and degraded generalization. To address this, we propose PACC, a redundancy-aware, layer-aware representation framework. PACC treats the protocol stack as multi-view inputs and learns compact layer-wise projections that remain faithful to each layer while explicitly factorizing representations into shared (cross-layer) and private (layer-specific) components. We operationalize these goals with a joint objective that preserves layer-specific information via reconstruction, captures shared structure via contrastive mutual-information learning, and maximizes task-relevant information via supervised losses, yielding compact latents suitable for efficient inference. Across datasets covering encrypted application classification, IoT device identification, and intrusion detection, PACC consistently outperforms feature-engineered and raw-bit baselines. On encrypted subsets, it achieves up to a 12.9% accuracy improvement over nPrint. PACC matches or surpasses strong foundation-model baselines. At the same time, it improves end-to-end efficiency by up to 3.16x.

</details>


### [772] [DNS: Data-driven Nonlinear Smoother for Complex Model-free Process](https://arxiv.org/abs/2602.08560)
*Fredrik Cumlin,Anubhab Ghosh,Saikat Chatterjee*

Main category: eess.SP

Relevance: 45.0

TL;DR: 提出数据驱动的非线性平滑器（DNS），用于从含噪声的线性测量序列中估计复杂动态过程的隐藏状态序列，无需过程动态模型，采用无监督学习仅使用测量数据。


<details>
  <summary>Details</summary>
Motivation: 针对复杂动态过程的隐藏状态估计问题，传统方法需要已知状态转移模型，但在实际应用中往往无法获得精确的动力学模型。本文旨在开发一种无需先验动力学知识、仅基于测量数据的数据驱动方法。

Method: 提出数据驱动的非线性平滑器（DNS），采用循环架构提供给定测量序列下隐藏状态序列的闭式后验分布。该方法完全无监督训练，仅需测量数据而不需要真实状态数据。通过模拟实验验证，包括基准Lorenz系统。

Result: 实验结果表明，DNS在多个随机动态过程的平滑任务中显著优于深度卡尔曼平滑器（DKS）和迭代数据驱动非线性状态估计（iDANSE）平滑器。

Conclusion: DNS提供了一种有效的无模型状态估计方法，能够在没有先验动力学知识的情况下，仅基于测量数据准确估计复杂动态过程的隐藏状态序列。

Abstract: We propose data-driven nonlinear smoother (DNS) to estimate a hidden state sequence of a complex dynamical process from a noisy, linear measurement sequence. The dynamical process is model-free, that is, we do not have any knowledge of the nonlinear dynamics of the complex process. There is no state-transition model (STM) of the process available. The proposed DNS uses a recurrent architecture that helps to provide a closed-form posterior of the hidden state sequence given the measurement sequence. DNS learns in an unsupervised manner, meaning the training dataset consists of only measurement data and no state data. We demonstrate DNS using simulations for smoothing of several stochastic dynamical processes, including a benchmark Lorenz system. Experimental results show that the DNS is significantly better than a deep Kalman smoother (DKS) and an iterative data-driven nonlinear state estimation (iDANSE) smoother.

</details>


### [773] [Amortising Inference and Meta-Learning Priors in Neural Networks](https://arxiv.org/abs/2602.08782)
*Tommy Rochussen,Vincent Fortuin*

Main category: stat.ML

Relevance: 45.0

TL;DR: 该论文提出了一种从多个数据集中学习权重先验的方法，将贝叶斯深度学习与概率元学习相结合，通过每个数据集的摊销变分推断来学习贝叶斯神经网络的先验分布。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯深度学习中面临的核心挑战是如何在没有先验信念的情况下表示预测任务的信念。传统贝叶斯方法需要先验分布，但在深度学习中很难定义合适的先验。该研究旨在解决如何从数据集中学习权重先验的问题。

Method: 提出了一种将贝叶斯深度学习与概率元学习相结合的方法，通过每个数据集的摊销变分推断来学习权重先验。模型可视为神经过程，其潜变量是贝叶斯神经网络的权重集合，解码器由潜变量样本参数化的神经网络组成。

Result: 该方法能够：1）在良好指定的先验下研究贝叶斯神经网络的行为；2）将贝叶斯神经网络用作灵活的生成模型；3）在神经过程中实现之前难以实现的功能，如任务内小批量处理或极端数据稀缺下的元学习。

Conclusion: 该研究通过从数据集中学习权重先验，为贝叶斯深度学习提供了实用的解决方案，弥合了贝叶斯深度学习与概率元学习之间的鸿沟，为贝叶斯神经网络的实际应用开辟了新途径。

Abstract: One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation.

</details>


### [774] [Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models](https://arxiv.org/abs/2602.09017)
*Zichen Jeff Cui,Omar Rayyan,Haritheja Etukuru,Bowen Tan,Zavier Andrianarivo,Zicheng Teng,Yihang Zhou,Krish Mehta,Nicholas Wojno,Kevin Yuanbo Wu,Manan H Anjaria,Ziyuan Wu,Manrong Mao,Guangxun Zhang,Binit Shah,Yejin Kim,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah*

Main category: cs.RO

Relevance: 45.0

TL;DR: CAP用物理接触点替代语言提示来指导机器人操作，通过模块化策略库和仿真迭代，在少量演示数据下实现对新环境和机器人的泛化，性能超越现有视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习依赖语言提示进行泛化，但语言过于抽象，难以指导具体的物理理解。需要更直接的物理接触表示来提升操作鲁棒性。

Method: 提出Contact-Anchored Policies (CAP)：1) 用空间中的物理接触点替代语言条件；2) 构建模块化工具模型库而非单一策略；3) 建立EgoGym轻量仿真平台进行快速迭代；4) 实现真实到仿真的迭代循环。

Result: 仅用23小时演示数据，CAP就能泛化到新环境和机器人，在三个基本操作技能上实现开箱即用，零样本评估中比现有大型视觉语言模型性能提升56%。

Conclusion: 物理接触点比语言提示更适合指导机器人操作，模块化设计和仿真迭代能有效提升泛化能力和数据效率。

Abstract: The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/

</details>


### [775] [Latent Target Score Matching, with an application to Simulation-Based Inference](https://arxiv.org/abs/2602.07189)
*Joohwan Ko,Tomas Geffner*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文提出Latent Target Score Matching (LTSM)，一种用于扩散模型训练的低方差目标函数，通过利用联合分数来监督边际分数，解决了传统去噪分数匹配在低噪声水平下高方差的问题。


<details>
  <summary>Details</summary>
Motivation: 去噪分数匹配(DSM)在训练扩散模型时，在低噪声水平下可能面临高方差问题。目标分数匹配(TSM)在清洁数据分数可用时可以缓解此问题，但在许多应用中，由于潜在变量的存在，只能获得联合信号而非清洁分数。

Method: 提出Latent Target Score Matching (LTSM)，将TSM扩展到潜在变量场景，利用联合分数为边际分数提供低方差监督。同时采用LTSM与DSM的混合方法，确保在不同噪声尺度下的鲁棒性。

Result: 在基于模拟的推理任务中，LTSM一致地改善了方差、分数准确性和样本质量，特别是在低噪声水平下表现优异。

Conclusion: LTSM为扩散模型训练提供了一种有效的低方差目标函数，特别适用于只能获得联合分数而非清洁分数的场景，通过混合DSM确保了方法的鲁棒性。

Abstract: Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.

</details>


### [776] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

Relevance: 40.0

TL;DR: GRAPHITE通过创建特征节点来直接提升图同质性，解决异质图上的GNN性能瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异质图（相连节点特征/标签不同）上表现不佳，甚至不如简单MLP。现有方法主要关注架构设计而非根本原因，需要创新方法直接提升图同质性

Method: 提出GRAPHITE框架，通过创建特征节点来直接转换图结构，促进具有相似特征的节点之间的同质性消息传递，从而提升图同质性

Result: 在异质图上显著优于现有方法，在Actor等数据集上超越21个最新GNN；在同质图上与SOTA方法相当；理论证明能显著提升图同质性

Conclusion: GRAPHITE通过直接转换图结构提升同质性的新范式，有效解决异质图上的GNN性能问题，为图学习提供新思路

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [777] [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*Jianchuan Yang,Xi Chen,Jidong Zhao*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于有限元方法的超图神经网络（FHGNN），将有限元计算嵌入消息传递层，用于计算力学中的物理信息机器学习，在弹塑性问题上实现了高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）与稀疏算子和非结构化离散化自然对齐，是计算力学中物理信息机器学习的有前景范式。受离散物理损失和分层深度神经网络（HiDeNN）构造的启发，作者希望将有限元计算直接嵌入消息传递层，实现数值一致的物理嵌入学习。

Method: 提出FEM-Informed Hypergraph Neural Networks（FHGNN），在节点和高斯点处嵌入有限元计算到消息传递层。训练完全由物理驱动，无需标注数据，输入是编码网格连接性的节点元素超图。采用高效的变分损失函数，利用GPU并行张量操作和离散表示。

Result: 在3D基准测试（包括各向同性和运动硬化循环加载）中，该方法相比最近的竞争性PINN变体显著提高了精度和效率。能够有效扩展到大型弹塑性问题，在可比精度下可与多核FEM实现竞争甚至更快。

Conclusion: 这项工作为非线性固体力学中的可扩展、物理嵌入学习奠定了基础，展示了FHGNN在计算力学中的潜力。

Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.

</details>


### [778] [Object-Oriented Transition Modeling with Inductive Logic Programming](https://arxiv.org/abs/2602.07602)
*Gabriel Stella,Dmitri Loguinov*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种新颖的学习算法，在基于人类认知启发的面向对象表示方面，比先前方法更强大，显著提升了最先进水平


<details>
  <summary>Details</summary>
Motivation: 从观察中构建世界模型（归纳）是机器学习的主要挑战之一。有用的模型需要在新颖情境中保持准确性（泛化），同时易于解释和高效训练。先前工作研究了受人类认知启发的面向对象表示，但现有方法能力有限。

Method: 开发了一种新颖的学习算法，比先前方法更强大。进行了全面的实验，包括消融测试和与神经基线的比较。

Result: 实验结果表明，该方法相比最先进技术有显著改进，在泛化能力、可解释性和训练效率方面表现优异。

Conclusion: 提出的学习算法在面向对象表示的学习中取得了重要进展，为构建更强大、可解释的世界模型提供了有效方法。

Abstract: Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.

</details>


### [779] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文研究使用神经网络（包括Transformer）近似矩阵函数，证明了ReLU网络近似矩阵指数的宽度和深度界限，并通过实验表明带合适数值编码的Transformer编码器-解码器能以5%相对误差概率性近似某些矩阵函数。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中取得了革命性成功，但在数值计算中的应用较少受到关注。矩阵函数（如矩阵指数、矩阵符号函数）在科学计算中广泛应用，研究神经网络（包括Transformer）如何近似这些函数具有重要意义。

Method: 1. 理论分析：证明ReLU网络近似矩阵指数所需的宽度和深度界限；2. 实验研究：使用Transformer编码器-解码器架构，探索不同数值编码方案对矩阵函数近似性能的影响。

Result: 1. 建立了ReLU网络近似矩阵指数的理论界限；2. 实验表明带合适数值编码的Transformer能以5%相对误差概率性近似某些矩阵函数；3. 发现编码方案对性能有显著影响，不同函数需要不同的编码方案。

Conclusion: Transformer可以用于数值计算中的矩阵函数近似，编码方案是关键设计因素。这为将Transformer应用于科学计算领域提供了新的可能性。

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [780] [Nansde-net: A neural sde framework for generating time series with memory](https://arxiv.org/abs/2602.08182)
*Hiromu Ozai,Kei Nakagawa*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出NA-noise作为Itô过程替代分数布朗运动，用于捕捉时间序列的长短期记忆特征，并构建NANSDE-Net生成模型


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动虽能捕捉时间序列记忆特征，但与Itô微积分不兼容，限制了其在神经随机微分方程框架中的应用。需要一种既能捕捉长短期记忆行为，又兼容Itô微积分的噪声过程。

Method: 提出NA-noise（神经网络核ARMA型噪声），通过神经网络参数化核函数并分解为乘积形式以保持马尔可夫性。基于此构建NANSDE-Net生成模型，扩展神经SDEs，并证明了理论解的存在唯一性，推导了高效的反向传播训练方案。

Result: 在合成和真实数据集上的实验表明，NANSDE-Net在再现数据的长短期记忆特征方面匹配或优于现有模型（包括分数SDE-Net），同时在Itô微积分框架内保持计算可处理性。

Conclusion: NA-noise为Itô过程提供了一种有效的替代方案，能够捕捉时间序列的长短期记忆特征，NANSDE-Net在保持计算效率的同时提升了生成建模性能。

Abstract: Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.

</details>


### [781] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出一种两阶段合成数据生成策略，通过合成-混合操作和核岭回归模型，在保护隐私的同时优化下游预测性能


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法主要关注统计信息保持，但难以平衡隐私保护所需的显著扰动与预测性能对扰动的敏感性。需要一种既能保护隐私又能保证预测性能的方法

Method: 两阶段合成策略：第一阶段采用合成-混合操作，生成纯合成数据并与原始数据融合；第二阶段基于核岭回归(KRR)训练模型，使用第一阶段生成的合成输入来生成合成输出

Result: 该方法实现了统计驱动的受限隐私-预测权衡，保证了最优预测性能。理论分析和数值实验验证了方法的有效性，并在营销问题和五个真实数据集上展示了泛化能力

Conclusion: 提出的两阶段合成策略能够有效平衡隐私保护与预测性能，为合成数据生成提供了新的理论框架和实用方法

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [782] [Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms](https://arxiv.org/abs/2602.08751)
*Nobuyuki Ota*

Main category: cs.LG

Relevance: 40.0

TL;DR: CDT-II是一个可解释的AI显微镜模型，其注意力机制直接对应生物学调控关系，能够从基因组嵌入和单细胞表达数据中揭示调控网络结构。


<details>
  <summary>Details</summary>
Motivation: 当前生物AI模型缺乏可解释性——它们的内部表示不对应研究人员可以检查的生物学关系。需要开发能够直接解释调控结构的AI模型。

Method: 通过模仿中心法则设计架构：DNA自注意力对应基因组关系，RNA自注意力对应基因共调控，DNA到RNA交叉注意力对应转录控制。仅使用基因组嵌入和原始单细胞表达数据。

Result: 在K562 CRISPRi数据中，预测扰动效应（基因平均r=0.84），无监督恢复GFI1B调控网络（6.6倍富集，P=3.5×10⁻¹⁷）。两个不同的注意力机制收敛到RNA处理模块（P=1×10⁻¹⁶）。

Conclusion: CDT-II建立了机制导向的AI作为任务导向方法的替代方案，揭示了调控结构而不仅仅是优化预测。

Abstract: Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that
  researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.
  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA
  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional
  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in
  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B
  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA
  processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing
  regulatory structure rather than merely optimizing predictions.

</details>


### [783] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

Relevance: 40.0

TL;DR: FreqLens是一个可解释的时间序列预测框架，通过可学习的频率发现和公理化的频率归因，自动发现主导周期性模式并提供理论保证的归因解释。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型缺乏可解释性，限制了在需要可解释预测的领域中的应用。需要一种能够自动发现周期性模式并提供理论保证归因的方法。

Method: 提出FreqLens框架，包含两个关键创新：1) 可学习的频率发现 - 通过sigmoid映射参数化频率基，从数据中学习并自动发现主导周期性模式；2) 公理化的频率归因 - 基于理论框架，满足完备性、忠实性、零频率和对称性公理，每个频率的归因等价于Shapley值。

Result: 在Traffic和Weather数据集上，FreqLens实现了竞争性或更优的性能，同时发现了物理上有意义的频率：在Traffic数据中所有5次独立运行都发现了24小时日周期（误差2.5%）和12小时半日周期（误差1.6%），在Weather数据中发现了比输入窗口长10倍的周周期。

Conclusion: FreqLens展示了真正的频率级知识发现能力，并在归因质量上提供了正式的理论保证，为时间序列预测提供了可解释的解决方案。

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [784] [Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials](https://arxiv.org/abs/2602.08849)
*Terry C. W. Lam,Niamh O'Neill,Christoph Schran,Lars L. Schaaf*

Main category: stat.ML

Relevance: 40.0

TL;DR: 提出一种在线异常值检测方法，通过指数移动平均跟踪损失分布，自动降低噪声样本权重，无需额外参考计算即可提升机器学习原子间势的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势的准确性受到参考数据中数值噪声的影响，这些噪声通常来自未收敛或不一致的电子结构计算。现有缓解策略（如手动过滤或迭代精炼异常值）需要大量专家努力或多个昂贵的重新训练周期，难以扩展到大型数据集。

Method: 引入在线异常值检测方案，通过指数移动平均跟踪损失分布，在整个单次训练运行中识别异常值，自动降低噪声样本的权重，无需额外参考计算。

Result: 该方法防止过拟合，性能与迭代精炼基线相当但开销显著降低。在未收敛参考数据中恢复了液态水的准确物理观测量（包括扩散系数），在SPICE数据集上训练有机化学基础模型时，将能量误差降低了三倍。

Conclusion: 该框架为在不完美数据集上训练稳健模型提供了简单、自动化的解决方案，适用于各种规模的数据集。

Abstract: The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.

</details>


### [785] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

Relevance: 35.0

TL;DR: EMSYNC是一个快速、免费、自动的视频配乐生成系统，通过视频情感分类、情感条件MIDI生成和时序边界对齐，为视频生成情感和节奏同步的音乐。


<details>
  <summary>Details</summary>
Motivation: 随着互联网视频内容快速增长，寻找合适的配乐成为重要挑战。内容创作者需要无需作曲或授权即可增强视频质量的自动音乐生成方案。

Method: 1) 新颖的视频情感分类器，使用预训练深度网络提取特征，仅训练融合层；2) 大规模情感标注MIDI数据集；3) 首个基于连续情感值而非离散类别的MIDI生成器；4) 时序边界条件方法（边界偏移编码），将和弦与场景变化对齐。

Result: 在Ekman-6和MovieNet数据集上达到SOTA；用户研究表明在音乐丰富度、情感对齐、时序同步和整体偏好上均优于现有方法。

Conclusion: EMSYNC作为全自动视频音乐生成器，通过情感和时序同步技术，为视频内容创作提供了高效解决方案。

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [786] [Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks](https://arxiv.org/abs/2602.07141)
*Isabel de la Higuera,Francisco Herrera,M. Victoria Velasco*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了特征再生核Banach空间的理论框架，将再生核Hilbert空间的理论推广到非Hilbert几何中，为核方法与神经网络提供了统一的函数空间视角。


<details>
  <summary>Details</summary>
Motivation: 现代学习模型（如具有非二次范数的固定架构神经网络）产生非Hilbert几何，超出了传统再生核Hilbert空间的范畴。在Banach空间中，点评估泛函的连续性不足以保证特征表示或基于核的学习公式，需要新的理论框架。

Method: 开发了基于特征再生核Banach空间的泛函分析框架，识别了在Hilbert体系之外恢复特征映射、核构造和表示型结果的结构条件。将监督学习表述为最小范数插值或正则化问题，建立存在性结果和条件表示定理，并扩展到向量值特征再生核Banach空间。

Result: 建立了特征再生核Banach空间的完整理论框架，证明了固定架构神经网络自然诱导此类空间的特殊实例，为核方法和神经网络提供了统一的函数空间视角，阐明了基于核的学习原理何时可以扩展到再生核Hilbert空间之外。

Conclusion: 该工作为Banach空间中的学习提供了系统的泛函分析框架，统一了核方法与神经网络的理论基础，填补了非Hilbert几何中学习理论的重要空白。

Abstract: Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.

</details>


### [787] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种高效计算do-Shapley值的新方法，通过将计算复杂度从指数级降低到与不可约集数量线性相关，并开发了基于查询预算的估计器。


<details>
  <summary>Details</summary>
Motivation: 结构因果模型(SCM)是描述复杂动态系统的强大框架，do-Shapley是量化变量平均因果效应的优雅方法。但传统计算do-Shapley值需要指数级计算成本，限制了其实际应用。

Method: 1. 将do-Shapley值重新表述为底层SCM不可约集的函数；2. 开发精确算法，计算复杂度与不可约集数量r线性相关；3. 设计基于查询预算的估计器，可在任意预算下运行；4. 证明非参数可识别性仅需d个单元素联盟的干预效应识别。

Result: 1. 精确算法在r个不可约集上线性时间内计算do-Shapley值；2. 估计器在相同查询预算下比现有方法准确度高几个数量级；3. 当查询预算达到r时，可返回机器精度的Shapley值；4. 显著降低了识别负担。

Conclusion: 通过将do-Shapley值与SCM的不可约集结构联系起来，实现了计算效率的显著提升和识别要求的降低，为大规模因果推断提供了实用工具。

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [788] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了一种新的经验纳什值遗憾度量，并设计了一个参数自由算法，在两人无信息马尔可夫博弈中实现了自适应对手非平稳性的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有工作在两人无信息马尔可夫博弈中，要么无法实现无外部遗憾（需要指数级依赖），要么使用较弱的纳什值遗憾度量且无法自适应问题难度。本文旨在同时解决这两个限制。

Method: 1. 提出新的经验纳什值遗憾度量，比纳什值遗憾更强，且在对手固定时自然退化为外部遗憾。2. 对Mao等人的epoch-based V-learning算法进行新分析，建立O(ηC + √(K/η))遗憾界。3. 设计自适应重启机制，根据对手的非平稳性动态调整参数η，最终实现参数自由算法。

Result: 算法实现了O(min{√K + (CK)^{1/3}, √(LK)})遗憾界，其中C量化对手策略的方差，L表示策略切换次数。该结果不仅恢复了两个极端情况（对手固定时的O(√K)外部遗憾和最坏情况下的O(K^{2/3})纳什值遗憾），还能通过自适应对手非平稳性在这两个极端之间平滑插值。

Conclusion: 本文完全解决了现有工作的两个限制：提出了更强的遗憾度量，并设计了参数自由算法，能够自适应对手的非平稳性，在两人无信息马尔可夫博弈中实现了最优的遗憾界。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [789] [The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network](https://arxiv.org/abs/2602.07219)
*Abhigyan Dutta,Itay Safran,Paul Valiant*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文研究了使用ReLU神经网络逼近d个输入的median函数，提出了深度-宽度权衡的构造方法，最终实现了常数深度、线性宽度且对单位超立方体均匀分布具有指数小逼近误差的神经网络。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络逼近median函数的理论能力，特别是深度-宽度权衡关系。先前关于maximum函数的研究表明，要达到可比较的精度，线性宽度需要深度至少为log log d。本文旨在突破这一障碍，为median函数设计更高效的神经网络架构。

Method: 采用多阶段迭代消除非中心元素的构造方法。通过建立从maximum到median的一般性归约，设计了一个常数深度、线性宽度的神经网络架构，能够逐步消除远离中位数的元素，同时保留候选集围绕中位数。

Result: 实现了对median函数的常数深度、线性宽度神经网络逼近，在单位超立方体均匀分布下达到指数小的逼近误差。这一结果突破了先前maximum函数研究中暗示的深度限制（log log d），展示了median函数比maximum函数更容易被神经网络逼近的特性。

Conclusion: median函数可以通过常数深度、线性宽度的ReLU神经网络高效逼近，且逼近误差指数小。这一发现不仅突破了先前关于maximum函数的深度限制，还表明median函数比maximum函数更容易被神经网络表示，为神经网络理论分析提供了新的见解。

Abstract: We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over the unit hypercube. By further establishing a general reduction from the maximum to the median, our results break a barrier suggested by prior work on the maximum function, which indicated that linear width should require depth growing at least as $\log\log d$ to achieve comparable accuracy. Our construction relies on a multi-stage procedure that iteratively eliminates non-central elements while preserving a candidate set around the median. We overcome obstacles that do not arise for the maximum to yield approximation results that are strictly stronger than those previously known for the maximum itself.

</details>


### [790] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一个用于灵巧机器人臂手系统操作任务的两阶段统一框架：第一阶段通过AR远程人机交互收集专家数据进行行为克隆预训练；第二阶段采用对比学习增强的强化学习方法，设计投影头加速学习过程，并使用事件驱动增强奖励提高安全性。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧机器人臂手系统的可扩展操作学习问题，传统方法在数据收集效率和策略鲁棒性方面存在不足。通过AR远程人机交互系统高效收集专家演示数据，结合对比学习增强的强化学习来克服行为克隆的局限性。

Method: 两阶段方法：1) 预训练阶段：基于AR远程人机交互系统收集的专家数据，采用行为克隆方式训练初始策略；2) 强化学习阶段：开发对比学习增强的强化学习方法，设计投影头加速学习，采用事件驱动增强奖励机制提高安全性。

Result: 在PyBullet物理仿真和真实世界实验中，相比经典PPO和SAC策略，该方法不仅显著加快推理速度，而且在完成操作任务的成功率方面表现更好。消融研究证实对比学习增强的RL克服了策略崩溃问题。

Conclusion: 提出的两阶段框架结合AR远程交互数据收集和对比学习增强的强化学习，为灵巧机器人操作任务提供了高效、鲁棒的解决方案，在仿真和真实环境中均表现出优越性能。

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [791] [BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks](https://arxiv.org/abs/2602.07400)
*Simon Bührer,Andreas Plesner,Aczel Till,Roger Wattenhofer*

Main category: cs.LG

Relevance: 35.0

TL;DR: BitLogic是一个端到端可训练的FPGA原生神经网络框架，用可微查找表(LUT)节点替代乘累加运算，实现原生二进制计算、稀疏连接和高效硬件实现，显著提升FPGA推理效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络推理的能耗和延迟成本主要由部署而非训练驱动，需要针对硬件优化的替代方案。FPGA提供了有吸引力的专用化平台，但现有FPGA神经网络方法分散且难以比较。

Method: 提出BitLogic框架，使用可微LUT节点直接映射到FPGA原语，支持原生二进制计算和稀疏连接。提供模块化功能API、学习编码器、硬件感知头部和边界一致的LUT松弛。自动化RTL导出管道将PyTorch模型转换为可综合HDL。

Result: 在标准视觉基准测试和异构硬件平台上展示了竞争力的准确性和显著的FPGA效率提升，包括在CIFAR-10上达到72.3%测试准确率（仅使用不到0.3M逻辑门），单样本推理延迟低于20纳秒（仅使用LUT资源）。

Conclusion: BitLogic为FPGA原生神经网络提供了一个完全基于梯度的端到端可训练框架，实现了软件和硬件推理的等价性，显著提升了FPGA上的推理效率。

Abstract: The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.

</details>


### [792] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了首个在一般奖励模型不确定性下实现无遗憾保证的贝叶斯优化算法，通过无限高斯过程（∞-GP）作为代理模型，结合Thompson采样，适用于非平稳、重尾等复杂奖励场景。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化通常假设奖励函数服从高斯过程，但在实际应用中奖励模型往往具有不确定性，可能呈现非平稳、重尾等复杂特性。现有方法缺乏对一般奖励模型的严格理论保证。

Method: 提出无限高斯过程（∞-GP）作为贝叶斯非参数模型，在奖励分布空间上放置先验，能够表示比经典高斯过程更广泛的奖励模型类别。结合Thompson采样进行探索与利用，并采用截断Gibbs采样实现计算可扩展性。

Result: 在理论上首次实现了贝叶斯优化在一般奖励设置下的无遗憾保证，仅需目标函数的Lipschitz连续性。实验结果表明在非平稳、重尾等复杂奖励场景下达到最先进性能。

Conclusion: ∞-GP为贝叶斯优化提供了更通用的理论框架，能够处理传统高斯过程无法有效建模的复杂奖励场景，同时保持计算效率。

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [793] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

Relevance: 35.0

TL;DR: ChiDeK是一个用于分子表示学习的框架，通过手性行列式核和交叉注意力机制，系统地将立体化学信息整合到分子表示中，能够同时编码中心手性和轴向手性，在多个手性相关任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手性是化学和生物学中的基本分子特性，但现有的机器学习模型难以有效捕捉手性信息。传统方法主要关注中心手性，依赖于手工设计的立体化学标签或有限的3D编码，无法推广到更复杂的轴向手性等形态。需要一种能够系统整合立体化学信息的统一框架。

Method: 提出手性行列式核(Chiral Determinant Kernel)来编码SE(3)不变的手性矩阵，并使用交叉注意力机制将局部手性中心的立体化学信息整合到全局分子表示中。这种设计能够在统一架构中显式建模手性相关特征，同时编码中心手性和轴向手性。

Result: 在四个任务上取得显著改进：R/S构型分类、对映体排序、ECD光谱预测和光学旋转预测。在轴向手性任务上平均准确率提高超过7%，并构建了新的轴向手性评估基准。

Conclusion: ChiDeK框架能够有效整合立体化学信息到分子表示学习中，在多种手性相关任务上优于现有方法，为复杂手性分子的机器学习建模提供了系统解决方案。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [794] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

Relevance: 35.0

TL;DR: Brep2Shape是一种新颖的自监督预训练方法，用于弥合CAD中边界表示（B-rep）的连续抽象表示与离散直观表示之间的差距，通过几何感知任务和对齐学习实现更好的形状理解。


<details>
  <summary>Details</summary>
Motivation: 现有处理B-rep模型的深度学习方法存在表示差距：连续方法提供分析精度但视觉抽象，离散方法提供直观清晰度但牺牲几何精度。需要弥合这一差距以实现更好的CAD模型处理。

Method: 提出Brep2Shape自监督预训练方法，通过几何感知任务让模型从参数化Bézier控制点预测密集空间点。采用双Transformer骨干网络，并行编码曲面和曲线token以捕获不同几何特性，并集成拓扑注意力建模曲面与曲线间的相互依赖关系。

Result: 实验结果表明Brep2Shape具有显著的可扩展性，在各种下游任务中实现了最先进的准确性和更快的收敛速度。

Conclusion: Brep2Shape成功弥合了CAD中抽象边界表示与直观形状表示之间的差距，通过自监督对齐学习实现了更好的几何理解，为CAD模型的深度学习处理提供了有效解决方案。

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [795] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出PAR方法解决离线RL中行为克隆正则化导致的性能上限问题，通过渐进替换低价值动作为高价值动作来扩展探索空间


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，行为克隆正则化方法虽然能产生真实策略并缓解分布外动作的偏差，但会引入性能上限问题：当数据集动作次优时，盲目模仿会阻止智能体充分利用评论家建议的高价值区域

Method: 提出近端动作替换(PAR)方法，这是一种即插即用的训练样本替换器，渐进地将低价值动作替换为稳定智能体生成的高价值动作，扩展动作探索空间同时减少低价值数据的影响

Result: 在离线RL基准测试中，PAR能持续提升性能，与基础TD3+BC结合时达到接近最先进的水平

Conclusion: PAR方法有效解决了行为克隆正则化的性能上限问题，为离线RL提供了一种有效的训练样本优化方案

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [796] [Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles](https://arxiv.org/abs/2602.07453)
*Namrita Varshney,Ashutosh Gupta,Arhaan Ahmad,Tanay V. Tayal,S. Akshay*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出数据感知的决策树集成模型特征敏感性分析框架，通过MILP和SMT编码生成接近训练分布的敏感示例，提高模型弱点的可解释性和实用性


<details>
  <summary>Details</summary>
Motivation: 决策树集成模型在关键领域广泛应用，但现有敏感性分析方法生成的示例往往远离训练分布，限制了可解释性和实用价值。需要开发能生成接近真实数据分布的敏感示例的方法，以更好地分析模型可靠性和公平性。

Method: 1) 证明敏感性验证的NP-hard性（即使深度为1的树）；2) 开发MILP优化技术加速单集成和多类树集成的敏感性验证；3) 提出数据感知框架，通过MILP和SMT编码约束敏感示例接近训练分布；4) 在大型树集成上进行实验评估。

Result: 方法可扩展到800棵树、深度8的大型集成，相比现有技术有显著改进。能生成接近训练分布的敏感示例，为高风险应用中树基模型的可靠性和公平性分析提供实用基础。

Conclusion: 提出的数据感知敏感性框架能生成现实且可解释的模型弱点证据，为决策树集成模型的鲁棒性和公平性分析提供了更实用的工具，特别适用于包含敏感特征（如受保护属性）的高风险应用。

Abstract: Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.

</details>


### [797] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

Relevance: 35.0

TL;DR: BGFormer：基于二分图Transformer的单细胞RNA测序聚类模型，通过引入可学习的锚点标记实现线性计算复杂度，解决了传统Transformer方法O(n²)复杂度在大规模数据集上的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的单细胞RNA测序聚类方法将每个细胞视为序列中的标记，计算和空间复杂度为O(n²)，限制了其在大规模数据集上的应用。需要设计更高效的架构来处理大规模单细胞数据。

Method: 提出BGFormer（二分图Transformer聚类模型），引入一组可学习的锚点标记作为共享参考点来表示整个数据集。采用二分图注意力机制学习细胞与锚点标记之间的相似性，使同一类别的细胞在嵌入空间中更接近。该方法实现了相对于细胞数量的线性计算复杂度。

Result: 在多个大规模单细胞RNA测序数据集上的实验结果表明，BGFormer在聚类效果和可扩展性方面表现优异，能够有效处理大规模数据集。

Conclusion: BGFormer通过二分图注意力机制和锚点标记设计，成功解决了Transformer在单细胞聚类中的计算复杂度问题，为大规模单细胞数据分析提供了高效可扩展的解决方案。

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [798] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出CoMI-IRL框架，使用transformer架构将行为表示/聚类与奖励学习解耦，无需已知行为模式数量K*或标签，在多专家意图的逆强化学习中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有深度生成MI-IRL方法需要已知真实行为模式数量K*的先验知识，这限制了新行为的适应性，且只能分析与学习奖励相关的部分，无法跨行为模式分析

Method: 提出对比多意图逆强化学习(CoMI-IRL)，基于transformer的无监督框架，将行为表示和聚类与下游奖励学习解耦

Result: CoMI-IRL在无需K*先验知识或标签的情况下优于现有方法，允许行为关系的可视化解释，并能适应未见行为而无需完全重新训练

Conclusion: CoMI-IRL框架解决了传统MI-IRL对先验知识的依赖问题，提供了更灵活、可解释且适应性强的多专家意图奖励学习方案

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [799] [Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games](https://arxiv.org/abs/2602.07521)
*Xionghui Yang,Bozhou Chen,Yunlong Lu,Yongyi Wang,Lingfeng Li,Lanxiao Huang,Lin Liu,Wenjun Wang,Meng Meng,Xia Lin,Wenxin Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种针对移动设备部署游戏AI的帕累托最优引导流水线，通过设计高效学生架构搜索空间，在保持性能的同时显著提升推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: 虽然游戏AI（如《王者荣耀》中的智能体）已能超越人类顶级玩家，但在移动设备上部署这些大型复杂模型面临挑战：复杂的多模态状态表示和分层动作空间需要大型策略网络，难以压缩为轻量级形式；而实际部署需要在移动平台的严格能耗和延迟约束下进行高频推理。

Method: 提出帕累托最优引导的流水线，设计针对移动执行的高效学生架构搜索空间，系统探索性能与效率之间的权衡。通过知识蒸馏方法将大型教师模型压缩为适合移动设备部署的学生模型。

Result: 蒸馏后的模型实现了显著效率提升：推理速度提升12.4倍（每帧低于0.5ms），能效提升15.6倍（每局游戏低于0.5mAh），同时保持40.32%的胜率对抗原始教师模型。

Conclusion: 该工作首次系统研究大规模游戏AI与实际移动设备部署之间的桥梁，提出的方法成功解决了复杂游戏AI在移动平台部署的效率与性能平衡问题。

Abstract: Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\times$ faster inference speed (under 0.5ms per frame) and a $15.6\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.

</details>


### [800] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

Relevance: 35.0

TL;DR: PVB是一种用于分子动力学模拟的预训练变分桥模型，通过编码器-解码器架构和增强桥匹配技术，统一训练单结构和配对轨迹数据，提高生成保真度，并引入强化学习优化蛋白质-配体复合物对接构象。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高，现有深度生成模型要么泛化能力差，要么因轨迹数据分子多样性有限而无法充分利用结构信息。需要一种能跨系统泛化并有效利用结构知识的生成方法。

Method: 提出预训练变分桥(PVB)模型，采用编码器-解码器架构，将初始结构映射到噪声潜在空间，通过增强桥匹配向阶段特定目标传输。统一训练单结构和配对轨迹数据，利用跨领域结构知识。针对蛋白质-配体复合物，引入基于强化学习的伴随匹配优化，加速向holo状态进展。

Result: 在蛋白质和蛋白质-配体复合物实验中，PVB能够忠实再现MD的热力学和动力学观测值，同时提供稳定高效的生成动力学，支持对接构象的高效后优化。

Conclusion: PVB通过统一训练框架和强化学习优化，在保持计算效率的同时提高了分子动力学生成的保真度和泛化能力，为分子模拟提供了新的有效工具。

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [801] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

Relevance: 35.0

TL;DR: ELM-INR：一种基于极限学习机的隐式神经表示方法，通过闭式解替代迭代优化，结合自适应网格细化策略BEAM提升重建质量


<details>
  <summary>Details</summary>
Motivation: 传统INR训练依赖迭代反向传播，存在频谱偏差问题，特别是在目标具有高度非均匀频率内容时。需要更快速、数值稳定的重建方法。

Method: 将域分解为重叠子域，在每个局部问题上使用极限学习机（ELM）进行闭式拟合，用稳定的线性最小二乘解替代迭代优化，通过单位分解组合局部预测器。引入BEAM自适应网格细化策略平衡子域间的频谱复杂度。

Result: 实现了快速且数值稳健的重建，在容量受限情况下通过自适应网格细化显著提升重建质量，从频谱Barron范数角度分析了近似困难区域。

Conclusion: ELM-INR提供了一种无反向传播的INR训练框架，结合自适应网格细化策略BEAM，能够有效处理非均匀频率内容，为隐式神经表示提供了新的优化视角。

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [802] [Dense Neural Networks are not Universal Approximators](https://arxiv.org/abs/2602.07618)
*Levi Rauchwerger,Stefanie Jegelka,Ron Levie*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文证明在权重和维度自然约束下，密集神经网络不具备通用逼近能力，存在Lipschitz连续函数无法被逼近，表明稀疏连接是实现真正通用逼近的必要条件。


<details>
  <summary>Details</summary>
Motivation: 虽然通用逼近定理表明足够大的神经网络可以逼近任意连续函数，但该研究旨在探索在更现实的约束条件下（如权重限制和维度约束），密集神经网络是否仍保持这种通用性。

Method: 采用模型压缩方法，结合弱正则性引理，将前馈网络解释为消息传递图神经网络。研究在ReLU神经网络上施加权重和输入输出维度的自然约束，模拟密集连接的概念。

Result: 在设定的约束条件下，证明了存在Lipschitz连续函数无法被此类密集神经网络逼近，揭示了密集层神经网络的内在局限性。

Conclusion: 密集神经网络不具备通用逼近能力，稀疏连接是实现真正通用逼近的必要条件，这对神经网络架构设计有重要启示。

Abstract: We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.

</details>


### [803] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

Relevance: 35.0

TL;DR: Feroma是一个联邦学习框架，通过客户端分布配置文件处理数据异构性，无需客户端身份信息即可动态选择聚合策略，在未见测试客户端上实现高性能部署


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在真实世界数据异构性下性能下降，无法同时处理客户端间的分布偏移和时间上的分布漂移，且依赖不现实的假设（如已知客户端聚类数量），限制了泛化能力

Method: 基于客户端分布配置文件（紧凑、隐私保护的本地数据表示），通过自适应相似性加权指导模型聚合和测试时模型分配，动态选择从聚类到个性化的聚合策略

Result: 在6个基准测试中，相比10个SOTA方法，Feroma在动态数据异构条件下平均准确率提升高达12个百分点，同时保持与FedAvg相当的计算和通信开销

Conclusion: 基于分布配置文件的聚合为在数据分布偏移和漂移下实现鲁棒联邦学习提供了实用路径

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [804] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

Relevance: 35.0

TL;DR: ElliCE：一种鲁棒的算法补救框架，通过椭圆近似Rashomon集来优化反事实解释，确保在模型不确定性下的可靠补救


<details>
  <summary>Details</summary>
Motivation: 机器学习模型影响人们生活的决策，需要理解如何通过行动获得更好结果。当Rashomon集（近似最优模型集合）很大时，标准的反事实解释变得不可靠，因为对一个模型有效的补救措施可能在另一个模型中失败。

Method: 提出ElliCE框架，通过椭圆近似Rashomon集来优化反事实解释。该方法在椭圆区域内提供可证明有效的解释，具有唯一性、稳定性和与关键特征方向对齐的理论保证。

Result: ElliCE生成的反事实解释不仅更鲁棒，而且更灵活，能够适应用户指定的特征约束，同时比现有基线方法快得多。

Conclusion: ElliCE为模型不确定性下的可靠补救提供了原则性和实用的解决方案，确保即使用户模型演变，也能提供稳定的推荐。

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [805] [Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization](https://arxiv.org/abs/2602.07708)
*Ding Zhang,Siddharth Betala,Chirag Agarwal*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了Explanation-Generalization Score (EGS)来评估GNN解释的质量，基于特征不变性原理，通过OOD泛化能力来衡量解释的因果有效性。


<details>
  <summary>Details</summary>
Motivation: 当前GNN解释性方法的评估指标（如保真度、稀疏性）无法评估解释是否识别了真正的因果变量，需要更原则性的评估框架。

Method: 提出EGS指标，基于特征不变性原理：如果解释捕捉了真正的因果驱动因素，那么在不同分布偏移下应该产生稳定的预测。通过使用解释子图训练GNN并在OOD设置下评估性能来量化这一原理。

Result: 在合成和真实世界数据集上进行了大规模验证（11,200个模型组合），结果表明EGS能够基于解释捕捉因果子结构的能力为解释器提供原则性的基准排名。

Conclusion: EGS为评估GNN解释的因果有效性提供了稳健的替代方案，超越了传统的保真度指标，有助于推动更可靠的解释性方法发展。

Abstract: Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.

</details>


### [806] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种计算高效的适应性数据分析机制，在数据分布相对于已知先验是稠密的情况下，能够达到最优的O(log T)样本复杂度，解决了计算效率与统计最优性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据工作流具有适应性，会反复查询同一数据集来优化决策，但这种适应性可能导致过拟合和统计推断无效。现有适应性数据分析机制面临计算效率与样本复杂度之间的根本矛盾：计算高效的算法通常有次优的O(√T)样本复杂度，而统计最优的O(log T)算法在标准密码学假设下计算不可行。

Method: 提出一种计算高效的ADA机制，当数据分布相对于已知先验是稠密时，能够达到最优的O(log T)样本复杂度。该机制特别适用于分布特定学习中的特征-标签数据分布，并产生样本高效的统计查询预言机。虽然算法不基于差分隐私，但满足谓词单挑安全性这一松弛的隐私概念。

Result: 在数据分布相对于已知先验稠密的自然类别下，同时实现了计算效率和最优样本复杂度。该机制在分布特定学习设置中特别有效，并揭示了适应性数据分析与差分隐私之外隐私概念之间的内在联系。

Conclusion: 识别了一类自然的数据分布，在该分布下计算效率和最优样本复杂度可以同时实现，解决了ADA中的基本权衡问题。结果还揭示了适应性数据分析与隐私之间的深层联系，超越了传统的差分隐私框架。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [807] [TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations](https://arxiv.org/abs/2602.07735)
*Matteo Rossi,Ryan Pederson,Miles Wang-Henderson,Ben Kaufman,Edward C. Williams,Carl Underkoffler,Owen Lewis Howell,Adrian Layer,Stephan Thaler,Narbe Mardirossian,John Anthony Parkhill*

Main category: cs.LG

Relevance: 35.0

TL;DR: TerraBind是一个用于蛋白质-配体结构和结合亲和力预测的基础模型，比现有方法推理速度快26倍，亲和力预测准确率提高约20%。


<details>
  <summary>Details</summary>
Motivation: 当前基于结构的药物设计深度学习方法依赖昂贵的全原子扩散来生成3D坐标，导致推理瓶颈，使得大规模化合物筛选计算不可行。作者挑战这一范式，提出关键假设：全原子分辨率对于准确的小分子构象和结合亲和力预测是不必要的。

Method: 采用粗粒度的口袋级表示（仅蛋白质Cβ原子和配体重原子），结合COATI-3分子编码和ESM-2蛋白质嵌入的多模态架构，学习丰富的结构表示。使用无扩散优化模块进行构象生成，以及结合亲和力似然预测模块。

Result: 在结构预测基准测试中，TerraBind在配体构象准确性上与基于扩散的基线方法相当。在结合亲和力预测方面，TerraBind在公共基准（CASP16）和多样化专有数据集（18个生化/细胞测定）上，Pearson相关性比Boltz-2提高约20%。亲和力预测模块还提供良好校准的亲和力不确定性估计，并通过持续学习框架和hedged批量选择策略，在模拟药物发现周期中，所选分子的亲和力改进比贪婪方法高6倍。

Conclusion: TerraBind证明了粗粒度表示在蛋白质-配体结构和结合亲和力预测中的有效性，显著提高了计算效率，同时保持或提高了准确性，为大规模化合物筛选提供了可行的解决方案。

Abstract: We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_β$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.

</details>


### [808] [Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation](https://arxiv.org/abs/2602.07834)
*D Yang Eng*

Main category: cs.LG

Relevance: 35.0

TL;DR: 使用符号回归从神经网络的Calabi-Yau流形度量近似中提取出简洁、可解释的五项公式，在保持高精度（R²=0.9994）的同时参数减少3000倍，并能准确计算物理可观测量。


<details>
  <summary>Details</summary>
Motivation: Calabi-Yau流形在弦理论中至关重要，但其度量计算极其困难。虽然神经网络可以近似这些度量，但它们是黑箱模型，缺乏可解释性。研究旨在通过符号回归从神经网络近似中提取简洁、可解释的数学公式。

Method: 采用符号回归方法，从训练好的神经网络（教师模型）中提取数学表达式。通过多种子验证，识别几何约束选择的关键特征（如幂和与对称多项式），并分析系数在模空间中的平滑变化规律。

Result: 成功提取出五项表达式，与神经网络精度相当（R²=0.9994），参数数量减少3000倍。公式在研究的模空间范围（ψ∈[0,0.8]）内保持相同函数形式，系数平滑变化。能够准确计算体积积分和Yukawa耦合等物理可观测量。

Conclusion: 符号蒸馏能够从黑箱神经网络中恢复紧凑、可解释的模型，为弦理论中难以计算的Calabi-Yau流形度量提供了既精确又透明的数学表达式，验证了符号回归在科学发现中的潜力。

Abstract: Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\approx 8-9\%$ at $ψ\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.

</details>


### [809] [GRAFT: Decoupling Ranking and Calibration for Survival Analysis](https://arxiv.org/abs/2602.07884)
*Mohammad Ashhad,Robert Hoehndorf,Ricardo Henao*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出GRAFT模型，一种用于生存分析的混合架构，结合线性AFT模型和非线性残差神经网络，通过随机门控进行特征选择，直接优化可微分的C-index对齐排序损失。


<details>
  <summary>Details</summary>
Motivation: 生存分析面临删失数据、高维特征和非线性交互的挑战。经典模型可解释但限制性强，深度学习模型灵活但通常不可解释且对噪声敏感。需要一种既灵活又可解释的模型。

Method: 提出GRAFT模型，将预后排名与校准解耦。混合架构结合线性AFT模型和非线性残差神经网络，集成随机门控进行端到端特征选择。使用局部Kaplan-Meier估计器的随机条件插补，直接优化可微分的C-index对齐排序损失。

Result: 在公共基准测试中，GRAFT在区分度和校准方面优于基线模型，在高噪声设置下保持鲁棒性和稀疏性。

Conclusion: GRAFT提供了一种既灵活又可解释的生存分析方法，通过混合架构和特征选择机制，在保持性能的同时提高模型的可解释性和鲁棒性。

Abstract: Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.

</details>


### [810] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该研究比较了四种深度学习模型（MLP、梯度提升、TabNet和SAINT）在帕金森病早期检测中的表现，发现SAINT模型在多个评估指标上表现最佳，其双注意力机制能有效建模特征交互。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期检测面临挑战，传统机器学习模型依赖特征工程且难以捕捉复杂特征交互。本研究旨在探索基于注意力的深度学习模型在表格生物医学数据中早期帕金森病检测的有效性。

Method: 使用UCI机器学习库的基准数据集，包含帕金森病患者和健康对照的生物医学语音测量数据。比较了四种分类模型：多层感知器（MLP）、梯度提升、TabNet和SAINT。

Result: SAINT在所有基线模型中表现最佳，加权精度0.98，加权召回率0.97，加权F1分数0.97，马修斯相关系数0.9990，以及最高的ROC曲线下面积。TabNet和MLP表现竞争性，梯度提升表现最差。

Conclusion: 基于注意力的深度学习架构在早期帕金森病检测中具有诊断潜力，动态特征表示在临床预测任务中很重要。SAINT的双注意力机制能有效建模样本内和样本间的特征交互。

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [811] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于可解释AI（XAI）的多任务相似性度量方法，使用ALE曲线和Fréchet距离，适用于单任务和多任务学习场景，模型无关且考虑特征重要性。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，理解任务间的相似性对于知识迁移至关重要。需要回答哪些任务相似、如何相似以及为什么相似的问题，但目前缺乏系统的方法来量化这种相似性。

Method: 基于可解释AI技术，特别是累积局部效应（ALE）曲线，使用加权Fréchet距离比较ALE曲线，考虑数据分布和特征重要性。引入缩放因子处理不同任务的预测性能差异，提供复杂场景下的应用建议。

Result: 在合成数据集和三个真实数据集（Parkinson's、自行车共享、CelebA）上验证，结果表明该度量方法符合对任务相似性的直观预期，适用于表格和非表格数据。

Conclusion: 提出的多任务相似性度量方法是探索任务间关系的有效工具，支持基于知识的决策制定，有助于理解多任务学习中的知识迁移机制。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [812] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

Relevance: 35.0

TL;DR: CLIPPER提出了一种在神经符号学习中的剪枝技术，通过整数线性规划去除不一致的标签组合，提升分类器性能


<details>
  <summary>Details</summary>
Motivation: 在神经符号学习中，输入实例的隐藏黄金标签必须满足逻辑公式。学习过程中需要计算满足公式的所有可能标签组合，但组合空间可能指数级增长，导致学习困难。

Method: 利用"具有相似潜在表示的实例可能共享相同标签"的直觉，将剪枝过程形式化为整数线性规划，在尊重逻辑结构的同时丢弃不一致的标签组合。CLIPPER与现有训练算法正交，可无缝集成。

Result: 在16个复杂神经符号任务基准测试中，CLIPPER将Scallop、Dolphin和ISED等最先进神经符号引擎的性能分别提升高达48%、53%和8%，达到最先进准确率。

Conclusion: CLIPPER通过有效剪枝标签组合空间，显著提升了神经符号学习的效率和性能，为解决组合爆炸问题提供了有效方案。

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [813] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种针对无限时域平均奖励约束MDP的原始-对偶自然行动者-评论家算法，使用多级蒙特卡洛估计器和显式预热机制处理单链动态，无需混合时间预言机，实现了√T量级的遗憾和约束违反边界。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习的遗憾分析主要依赖于遍历性或强混合时间假设，这些假设在存在瞬态状态时失效。论文旨在为更广泛的单链CMDP类提供最优保证，无需混合时间假设。

Method: 提出原始-对偶自然行动者-评论家算法，结合多级蒙特卡洛估计器处理方差，使用显式预热机制处理单链动态，无需混合时间预言机，支持一般策略参数化。

Result: 建立了有限时间遗憾和累积约束违反边界，尺度为Õ(√T)，仅受策略和评论家参数化近似误差影响，将最优保证扩展到更广泛的CMDP类。

Conclusion: 该算法成功处理了单链CMDP，无需混合时间假设，为约束强化学习提供了更通用的理论保证，扩展了现有结果的应用范围。

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [814] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Horizon Imagination (HI)方法，用于基于扩散的世界模型在强化学习中的应用，通过并行去噪多个未来观测、稳定机制和新型采样调度，在保持控制性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的世界模型在强化学习中具有高生成保真度，但面临严重的效率挑战：现有方法要么需要重型推理模型，要么依赖高度序列化的想象过程，计算成本过高。

Method: 提出Horizon Imagination (HI)方法：1) 对离散随机策略进行在策略想象；2) 并行去噪多个未来观测；3) 引入稳定机制；4) 设计新型采样调度，将去噪预算与有效视野解耦，支持子帧预算。

Result: 在Atari 100K和Craftium实验中，HI方法仅用一半去噪步数的子帧预算就能保持控制性能，并在不同调度下实现更优的生成质量。

Conclusion: Horizon Imagination有效解决了扩散世界模型在强化学习中的效率问题，通过并行化和智能调度实现了高效且高质量的未来想象。

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [815] [TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation](https://arxiv.org/abs/2602.08036)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: TAAM提出了一种无需数据回放的图持续学习方法，通过轻量级神经突触调制器实现任务特定知识存储，并结合锚点多跳传播解决未知任务ID问题。


<details>
  <summary>Details</summary>
Motivation: 当前图持续学习方法依赖数据回放策略，存在内存限制、隐私问题和稳定性-可塑性困境。需要一种无需回放的方法来有效处理流式图数据。

Method: 提出任务感知自适应调制（TAAM），核心是轻量级神经突触调制器（NSMs）。每个新任务训练一个专用NSM并冻结，作为"专家模块"对共享GNN骨干的计算流进行节点注意自适应调制。为解决未知任务ID问题，提出锚点多跳传播（AMP）方法。

Result: 在更严格的归纳学习场景下，TAAM在八个数据集上全面优于现有最先进方法，无需任何数据回放即可防止灾难性遗忘。

Conclusion: 轻量级任务特定模块可以有效指导固定GNN骨干的推理过程，实现高效图持续学习。同时发现现有GCL基准存在数据泄露和评估偏差问题。

Abstract: Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.

</details>


### [816] [Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks](https://arxiv.org/abs/2602.08128)
*Zahir Alsulaimawi*

Main category: cs.LG

Relevance: 35.0

TL;DR: OBIL是一种在线贝叶斯不平衡学习框架，通过解耦似然比估计和类别先验假设，实现无需重新训练即可实时适应分布变化，适用于类别分布动态变化的实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 现实应用中类别分布经常动态变化（如欺诈检测、医疗诊断），传统不平衡学习方法需要重新训练或访问标注数据，无法实时适应部署时的分布变化，因此需要一种无需重新训练即可适应分布变化的方法。

Method: 基于Bregman散度与适当评分规则的理论联系，证明深度网络使用此类损失训练可产生后验概率估计，从中可提取先验不变的似然比。该方法将似然比估计与类别先验假设解耦，仅需调整阈值即可适应分布变化。

Result: 理论证明似然比估计在任意类别先验和成本结构变化下保持有效，推导出有限样本遗憾界O(√(T log T))。在基准数据集和医疗诊断基准上的实验表明，OBIL在严重分布变化下保持鲁棒性能，在测试分布显著偏离训练条件时F1分数优于现有方法。

Conclusion: OBIL提供了一种理论严谨的在线不平衡学习框架，能够实时适应分布变化而无需重新训练，为实际应用中类别分布动态变化的场景提供了有效的解决方案。

Abstract: Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\sqrt{T \log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.

</details>


### [817] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

Relevance: 35.0

TL;DR: V-ABFT是一种基于方差的自适应阈值算法，用于检测矩阵乘法中的静默数据损坏，相比现有方法显著提高了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ABFT阈值确定方法存在严重问题：分析界限过于保守，而概率方法如A-ABFT产生的阈值比实际舍入误差大160-4200倍，导致检测精度不足。

Method: 提出V-ABFT算法，通过直接建模验证差异，利用统计方差估计来获得更紧密的误差界限。该方法仅需O(n)复杂度，使用最大/最小/均值统计量，相比A-ABFT的O(pn)复杂度更高效。

Result: V-ABFT将阈值与实际误差比降低到FP32/FP64约7-20倍，BF16约48-158倍，相比A-ABFT改进6-48倍，同时保持零误报率。对于融合核实现，低精度GEMM可使用FP32级阈值，实现约1000倍更精细的检测粒度。

Conclusion: V-ABFT在保持零误报率的同时，显著提高了ABFT检测精度，适用于各种精度格式和硬件平台，已在NPU和GPU上集成到容错GEMM实现中。

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [818] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出mmSIVAE，一种结合软自省VAE和混合专家乘积聚合的多模态规范建模方法，用于阿尔茨海默病神经影像分析，提高健康参考分布拟合和多模态融合质量。


<details>
  <summary>Details</summary>
Motivation: 现有VAE规范模型在阿尔茨海默病多模态神经影像分析中存在两个问题：1) 健康参考分布拟合不完善导致假阳性增加；2) 后验聚合方法（如PoE/MoE）在共享潜在空间中多模态融合效果弱。

Method: 提出mmSIVAE（多模态软自省变分自编码器），结合Mixture-of-Product-of-Experts（MOPOE）聚合机制。在潜在空间和特征空间计算与学习到的健康分布的距离作为偏差分数，并将统计显著的潜在偏差映射到区域异常以增强可解释性。

Result: 在ADNI的MRI区域体积和淀粉样蛋白PET SUVR数据上，mmSIVAE在保留对照组上改善重建效果，相比VAE基线产生更具区分性的偏差分数用于异常检测，具有更高的似然比和更清晰的对照组与AD谱系队列分离。偏差图突出显示与已知AD相关变化一致的区域级模式。

Conclusion: 研究结果强调了训练目标中优先考虑参考分布保真度和鲁棒多模态后验聚合对规范建模的重要性，对跨多模态临床数据的偏差分析具有广泛意义。

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [819] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Variance-Gated Ensembles (VGE)框架，通过信号-噪声门控机制实现非加性的不确定性分解，提供VGMU评分和VGN层，在保持计算效率的同时提升集成模型的不确定性估计性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯或近似方法通过加性分解将不确定性分为偶然性和认知性成分，但这种方法在使用有限集成采样和/或不匹配预测分布时会失效。需要一种更可靠的非加性不确定性估计框架。

Method: 提出VGE框架：1) 通过集成统计量计算信号-噪声门控注入认知敏感性；2) VGMU评分将决策边界与集成预测方差耦合；3) VGN层通过每类可学习的归一化将方差门控机制推广到训练中；4) 推导闭式向量-雅可比乘积实现端到端训练。

Result: VGE在匹配或超越最先进的信息论基线方法的同时保持计算效率，为集成模型提供了实用且可扩展的认知感知不确定性估计方法。

Conclusion: VGE提供了一种直观、可微的框架，通过方差门控机制实现有效的非加性不确定性分解，在不确定性估计任务中表现出色且计算高效。

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [820] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出模块化因果机器学习框架，用于区分治疗效果异质性的统计检测与实际临床决策改进，并在溃疡性结肠炎临床试验中应用，发现内镜特征虽能预测异质性但不能改善治疗决策。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验通常估计平均治疗效果，但治疗反应存在异质性，这促使个性化治疗的发展。然而，统计上可检测的异质性并不一定转化为更好的治疗决策，这两个问题可能产生矛盾结果。本文旨在开发一个框架来分别评估这两个问题。

Method: 提出模块化因果机器学习框架：1) 置换重要性识别预测异质性的特征；2) 最佳线性预测器(BLP)测试评估统计显著性；3) 双重稳健策略评估衡量基于异质性制定决策是否能改善患者结局。应用于UNIFI维持试验数据，使用交叉拟合X-learner模型，包含基线人口学、用药史、第8周临床评分、实验室生物标志物和视频衍生内镜特征。

Result: BLP测试显示内镜特征与乌司奴单抗vs安慰剂的治疗效果异质性有强关联，但双重稳健策略评估表明纳入内镜特征并未改善预期缓解率，且多臂评估显示性能更差。诊断比较发现内镜评分作为疾病严重程度标志物，改善未治疗患者的结果预测但增加治疗选择噪声，而临床变量(粪便钙卫蛋白、年龄、CRP)捕捉了决策相关变异。

Conclusion: 统计上可检测的治疗效果异质性不一定能转化为更好的临床决策。因果机器学习在临床试验中的应用应包括策略层面的评估，而不仅仅是异质性测试。内镜特征虽然能预测异质性，但不能改善治疗选择，而临床变量更具决策相关性。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [821] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种基于保形预测的神经算子不确定性量化方法，用于PDE求解中的决策任务，无需分布假设且具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子替代模型在PDE求解决策任务中广泛应用，但缺乏校准的不确定性量化。现有方法（如集成或贝叶斯后验估计）要么需要不切实际的分布假设，要么缺乏实际可扩展性，限制了实际应用。

Method: 提出将保形预测应用于神经算子映射的函数空间，产生分布自由的不确定性量化。利用无限维Danskin定理和变分法解决下游鲁棒决策任务。

Result: 该方法在多个工程任务中表现出优于高斯过程等限制性建模范式的性能，能够为下游鲁棒决策任务提供形式化的遗憾特征描述。

Conclusion: 保形预测为神经算子提供了实用的不确定性量化方法，支持鲁棒决策制定，具有分布自由和可扩展的优势。

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [822] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究了随机凸优化中经验风险最小化器（ERM）的样本复杂度，证明了存在ERM唯一且可能过拟合的实例，解决了Feldman的开放性问题，并给出了梯度下降的泛化下界。


<details>
  <summary>Details</summary>
Motivation: 研究随机凸优化中经验风险最小化器的样本复杂度和泛化行为，特别是探索ERM在什么情况下会过拟合，以及梯度下降算法的泛化性能。

Method: 通过构造特定的凸优化实例，分析ERM的样本复杂度和唯一性；在此基础上，分析约束梯度下降算法的泛化行为，推导泛化下界。

Result: 1) 证明了存在样本复杂度与维度线性相关的实例，其中ERM唯一且会过拟合；2) 给出了梯度下降的泛化下界Ω(√(ηT/m^1.5))，显著缩小了现有上下界之间的差距。

Conclusion: ERM在随机凸优化中可能过拟合，梯度下降算法的泛化性能受学习率、迭代次数和样本量的影响，这为理解优化算法的泛化行为提供了理论洞见。

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [823] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文研究了非平稳在线学习中的动态遗憾最小化问题，主要关注跟随正则化领导者（FTRL）方法。通过基于折扣到动态的简化方法，为FTRL相关问题的动态遗憾界限提供了模块化分析框架，应用于线性回归和逻辑回归，并扩展到Adam优化器的分析。


<details>
  <summary>Details</summary>
Motivation: FTRL方法对于曲线损失函数（如线性回归、逻辑回归）以及理解自适应优化器（如Adam）非常重要，但现有的动态遗憾分析对FTRL的探索较少。需要开发一个统一的框架来分析FTRL在非平稳环境中的动态遗憾性能。

Method: 基于折扣到动态的简化方法，提出了一个模块化的分析框架来获得FTRL相关问题的动态遗憾界限。具体应用于两种代表性曲线损失：线性回归和逻辑回归。将该简化方法扩展到分析Adam优化器，包括带有两个折扣参数(β₁,β₂)的Adam变体。

Result: 1) 简化了在线线性回归最优动态遗憾的现有证明；2) 获得了在线逻辑回归的新动态遗憾保证；3) 在随机、非凸、非光滑设置下获得了Adam优化器的最优收敛率；4) 对带有两个折扣参数的Adam变体进行了更详细的分析，得到了剪裁和无剪裁Adam变体的新结果。

Conclusion: 提出的折扣到动态简化方法为分析FTRL相关问题的动态遗憾提供了一个强大而模块化的框架，不仅简化了现有证明，还获得了新的理论保证，特别是在曲线损失函数和自适应优化器分析方面。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [824] [The Connection between Kriging and Large Neural Networks](https://arxiv.org/abs/2602.08427)
*Marius Marinescu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文探讨了空间统计学中的克里金法（Kriging）与神经网络之间的理论联系，旨在通过结合两者的优势来增强机器学习技术的可解释性、可靠性和空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各学科的普及，空间统计学正处于与AI深度融合的关键时刻。本文旨在回答一个核心问题：空间统计模型（特别是克里金法）与机器学习模型之间是否存在理论联系？尽管克里金法基于概率论和随机过程，而许多ML模型被视为黑箱，但作者认为它们之间存在深刻关联，理解这种联系有助于改进ML技术。

Method: 本文采用理论分析方法，系统研究克里金法（及其机器学习对应物——高斯过程回归）与神经网络之间的数学联系。作者回顾了相关文献，从理论层面探讨这两种看似不同的建模方法之间的内在关系。

Result: 研究发现克里金法与神经网络之间存在深刻的数学联系，这种联系为结合两种方法的优势提供了理论基础。通过将空间统计学的概率框架与神经网络的表达能力相结合，可以开发出更强大的机器学习技术。

Conclusion: 克里金法与神经网络之间存在重要的理论联系，理解这种关系有助于开发更可解释、更可靠、更具空间感知能力的机器学习模型。这种跨学科融合为空间统计学和AI的发展开辟了新方向。

Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>


### [825] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出USBD框架，通过蒸馏源数据集构建通用结构基，解决图领域自适应中源模型对拓扑结构先验的依赖问题，显著提升在结构差异大的目标域上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无源图领域自适应方法隐含依赖于源训练GNN的平滑性先验，当目标域与源域拓扑结构差异显著时，源模型会将未见的结构模式误判为噪声，导致基于伪标签的自适应不可靠。

Method: 提出通用结构基蒸馏框架，通过双层优化将源数据集蒸馏为紧凑的结构基，强制原型覆盖完整的Dirichlet能量谱，显式捕获从低频聚类到高频链的多样拓扑模式。推理时采用谱感知集成机制，根据目标图的谱指纹动态激活最优原型组合。

Result: 在基准测试中，USBD显著优于现有方法，特别是在结构差异严重的场景下，同时通过将自适应成本与目标数据规模解耦，实现了优越的计算效率。

Conclusion: USBD通过从适应有偏模型转向学习通用结构基，解决了SF-GDA中的结构泛化瓶颈，为处理拓扑结构差异大的目标域提供了有效解决方案。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [826] [RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks](https://arxiv.org/abs/2602.08446)
*Pouria Arefijamal,Mahdi Ahmadlou,Bardia Safaei,Jörg Henkel*

Main category: cs.LG

Relevance: 35.0

TL;DR: RIFLE是一个基于知识蒸馏的鲁棒联邦学习框架，用logit知识转移替代梯度共享，在资源受限的IoT环境中实现深度模型训练，并采用KL散度验证机制防御恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在IoT环境中面临两个主要挑战：1）TinyML模型在数据异构和任务复杂时无法捕捉复杂模式；2）需要防御恶意客户端和中毒攻击。现有方法在极端非IID条件下效果有限，且缺乏有效的鲁棒性保障。

Method: 提出RIFLE框架：1）用logit-based知识蒸馏替代梯度共享，使资源受限设备能训练VGG-19、ResNet18等深度模型；2）采用KL散度验证机制量化客户端更新的可靠性，无需暴露原始数据；3）通过知识蒸馏聚合方案实现高效训练。

Result: 在MNIST、CIFAR-10、CIFAR-100三个数据集上的实验表明：1）误报检测减少87.5%；2）中毒攻击缓解提升62.5%；3）准确率比传统联邦学习基线提高28.3%；4）VGG19训练时间从600多天降至1.39小时（0.3 GFLOPS设备）。

Conclusion: RIFLE通过知识蒸馏和KL散度验证，有效解决了IoT环境中联邦学习的模型容量不足和安全性问题，使深度学习在资源受限网络中变得实用可行。

Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.

</details>


### [827] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文引入治疗效应方差(VTE)和条件治疗效应方差(CVTE)作为衡量治疗反应中固有随机不确定性的自然指标，提出了非参数核估计方法，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断研究主要关注治疗效应的平均值和条件平均值，对个体治疗反应的变异性和不确定性关注不足。本文旨在填补这一空白，提出治疗效应方差作为衡量治疗反应中固有随机不确定性的指标。

Method: 提出治疗效应方差(VTE)和条件治疗效应方差(CVTE)的概念，证明这些量在温和假设下可从观测数据中识别。开发了非参数核基估计器来估计VTE和CVTE，并进行了理论收敛性分析。

Result: 在合成和半模拟数据集上的广泛实验表明，该方法在性能上优于或与朴素基线方法相当。理论分析建立了估计器的收敛性。

Conclusion: VTE和CVTE是衡量治疗反应中固有随机不确定性的有效指标，提出的估计方法在理论和实践上都具有可行性，为因果推断中的不确定性量化提供了新工具。

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [828] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出时间延迟Transformer（TD-TF），一种用于非稳态时空动力学数据驱动建模的简化Transformer架构。该架构将线性算子方法与深度序列模型联系起来，证明单层单头Transformer可解释为时间延迟动态模态分解的非线性泛化。


<details>
  <summary>Details</summary>
Motivation: 动机是建立线性算子方法与深度序列模型之间的桥梁，开发一个既能保持线性模型可解释性和效率，又能增强对复杂非线性动力学表达能力的架构。

Method: 采用极简架构设计：单层自注意力层（每个预测只有一个查询）和单层前馈网络，实现序列长度的线性计算复杂度和少量参数。将Transformer解释为时间延迟动态模态分解的非线性泛化。

Result: 在近线性系统中与强线性基线性能相当，在非线性和混沌系统中显著优于线性方法，能准确捕捉长期动力学。在合成信号、非稳态空气动力学、Lorenz '63系统和反应-扩散模型上验证了有效性。

Conclusion: TD-TF在保持线性模型可解释性和效率的同时，为复杂动力学提供了显著增强的表达能力，成功桥接了线性算子方法与深度序列模型。

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [829] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

Relevance: 35.0

TL;DR: PyAGC是一个面向工业部署的图聚类基准测试库，解决了现有评估方法在数据集规模、同质性、训练范式等方面的局限性，提供了模块化框架和高效实现。


<details>
  <summary>Details</summary>
Motivation: 当前属性图聚类（AGC）研究存在学术与工业应用的鸿沟：评估协议基于小规模、高同质性的引文数据集，采用不可扩展的全批次训练范式，依赖监督指标，无法反映标签稀缺环境下的真实性能。

Method: 1. 提出PyAGC基准测试库，将现有方法统一为模块化的Encode-Cluster-Optimize框架；2. 首次为多种最先进AGC算法提供内存高效的小批次实现；3. 构建包含12个多样化数据集（2.7K到111M节点）的基准，特别包含具有复杂表格特征和低同质性的工业图；4. 提出全面的评估协议，要求同时使用无监督结构指标、效率分析和传统监督指标。

Result: PyAGC已在蚂蚁集团高风险工业工作流中得到验证，为社区提供了一个稳健、可复现、可扩展的平台，推动AGC研究向实际部署发展。代码和资源已公开。

Conclusion: PyAGC填补了AGC研究在工业部署方面的空白，通过提供多样化数据集、高效实现和全面评估协议，为图聚类研究向实际应用发展提供了坚实基础。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [830] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

Relevance: 35.0

TL;DR: FairRARI：一个统一的凸优化框架，用于计算满足不同群体公平性标准的PageRank向量，在保证目标公平水平的同时保持与原始PageRank相同的渐近时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着算法公平性日益重要，现有方法在计算PageRank向量时存在不足：有些无法保证达到目标公平水平，有些缺乏最优性保证。需要一种原则性算法来处理基于顶点敏感属性的群体公平性标准。

Method: 提出FairRARI统一框架，利用PageRank的变分公式，通过求解带有公平性约束的强凸优化问题来计算公平的PageRank向量。框架支持三种不同的公平性标准，并能以"即插即用"方式处理各种公平性准则。

Result: 在真实世界数据集上的广泛实验表明，FairRARI在效用方面优于现有方法，同时能在多个顶点群体中达到期望的公平水平。框架计算公平PageRank向量的时间复杂度与原始PageRank算法相同。

Conclusion: FairRARI提供了一个原则性、高效且灵活的框架，用于计算满足群体公平性约束的PageRank向量，解决了现有方法的局限性，并在实际应用中表现出色。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [831] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

Relevance: 35.0

TL;DR: CauScale是一种用于高效因果发现的神经架构，通过压缩数据嵌入和共享注意力权重实现时间和空间效率提升，可扩展到1000节点图，相比现有方法实现4-13,000倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在处理大规模图时面临时间和空间效率瓶颈，限制了其在科学AI和数据分析等数据驱动领域的应用。需要开发能够扩展到大规模图的高效因果发现方法。

Method: CauScale采用双流设计：数据流从高维观测中提取关系证据，图流整合统计图先验并保留关键结构信号。通过压缩单元减少数据嵌入，采用共享注意力权重避免维护轴特定注意力图，提高时空效率。

Result: CauScale成功扩展到500节点图训练（先前方法因空间限制失败），在分布内数据上达到99.6% mAP，分布外数据上达到84.4% mAP，相比现有方法实现4-13,000倍推理加速。

Conclusion: CauScale通过创新的神经架构设计解决了因果发现中的可扩展性瓶颈，为大规模图上的高效因果推理提供了实用解决方案，在保持高准确性的同时显著提升了时空效率。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [832] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种在代数约束条件下进行连续变量约束最大后验概率（MAP）推理的方法，包括可精确求解的片段和通用的数值优化策略。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，概率机器学习系统需要在代数约束条件下进行预测（如避免障碍物的最可能轨迹）。这些现实约束通常非凸，且密度函数也非（对数）凹，使得约束MAP预测的计算既困难又不可靠。

Method: 1. 首先研究了在何种条件下可以对连续变量进行精确高效的约束MAP推理，并设计了可扩展的消息传递算法来处理这一可解片段。
2. 然后提出了一种通用的约束MAP策略，将域划分为凸可行区域，并与数值约束优化交替进行。

Result: 在合成和真实世界基准测试中，两种方法都优于无视约束的基线方法，并能扩展到当前最先进精确求解器无法处理的复杂密度函数。

Conclusion: 该研究为安全关键应用中的约束概率推理提供了有效的解决方案，能够处理非凸约束和非凹密度函数的挑战性场景。

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [833] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文系统分析了深度强化学习在网络安全应用中的11个常见方法陷阱，基于对66篇论文的分析，发现平均每篇论文存在超过5个陷阱，并通过实验验证其实际影响，最后提供可操作建议。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在网络安全领域的应用面临从实验室模拟到实际部署的诸多挑战，特别是在对抗性、非平稳性和部分可观测的环境中。现有研究存在系统性方法缺陷，需要系统化识别和解决这些问题。

Method: 1. 系统化识别11个方法陷阱，涵盖环境建模、智能体训练、性能评估和系统部署四个阶段；2. 分析2018-2025年间66篇重要DRL4Sec论文，量化每个陷阱的普遍性；3. 在自主网络防御、对抗性恶意软件创建和Web安全测试三个环境中进行控制实验验证。

Result: 1. 发现平均每篇论文存在超过5个方法陷阱；2. 通过实验验证了这些陷阱在实际网络安全任务中的显著影响；3. 量化了不同陷阱在文献中的普遍程度。

Conclusion: DRL在网络安全应用中存在系统性方法缺陷，需要更严谨的研究方法。论文提供了针对每个陷阱的具体建议，以支持开发更可靠、可部署的DRL安全系统。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [834] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

Relevance: 35.0

TL;DR: RALIS：一种结合多视图对比学习和专家混合模块的模型，用于支持训练和推理期间任意视图可用性的多模态多视图学习


<details>
  <summary>Details</summary>
Motivation: 现有多模态多视图学习方法在处理灵活视图配置（包括任意视图组合、视图数量和异构模态）方面存在困难。特别是在人类活动识别任务中，需要能够适应训练和推理期间任意视图可用性的模型。

Method: RALIS结合了多视图对比学习和专家混合模块：1) 使用调整的中心对比损失进行自监督表示学习和视图对齐，减轻缺失视图对多视图融合的影响；2) 引入视图权重考虑视图质量；3) 将计算复杂度从O(V²)降低到O(V)；4) 使用专家混合模块处理对比学习未捕获的残差差异，采用专门的负载均衡策略适应任意视图组合。

Result: 在包含惯性和人体姿态模态的四个数据集上验证，视图数量从3到9个，证明了RALIS的性能和灵活性。

Conclusion: RALIS通过结合对比学习和专家混合模块，有效解决了多模态多视图学习中的灵活视图配置问题，在人类活动识别任务中表现出色。

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [835] [HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training](https://arxiv.org/abs/2602.08762)
*Wen Xu,Zhetao Li,Yong Xiao,Pengpeng Qiao,Mianxiong Dong,Kaoru Ota*

Main category: cs.LG

Relevance: 35.0

TL;DR: HoGS：一种基于局部差分隐私的图神经网络训练框架，通过生成合成图同时保护链接和节点特征隐私，利用图同质性重建图结构，显著提升GNN训练精度。


<details>
  <summary>Details</summary>
Motivation: 现有局部差分隐私GNN方法要么只保护链接隐私，要么在同时保护链接和节点特征隐私时导致显著的效用损失。需要一种既能保护隐私又不显著降低GNN性能的方法。

Method: HoGS框架：1）在LDP保护下收集图的链接和特征信息；2）利用图数据中的同质性现象分别重建图结构和节点特征；3）使用生成的合成图作为各种先进GNN架构的输入进行训练。

Result: 在三个真实世界数据集上的实验表明，HoGS在训练GNN的准确率方面显著优于基线方法，有效减轻了LDP对下游GNN训练的负面影响。

Conclusion: HoGS提供了一种有效的LDP框架，能够同时保护图数据的链接和节点特征隐私，同时保持GNN训练的高效用，解决了现有方法在隐私保护与效用平衡方面的不足。

Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.

</details>


### [836] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该研究通过实验证明，基于库默认值的贝叶斯优化初始化相比随机初始化没有统计显著优势，建议将超参数调优作为模型开发的重要部分而非依赖默认值。


<details>
  <summary>Details</summary>
Motivation: 机器学习库（如scikit-learn）的默认超参数值包含了隐式的专家知识，理论上可以作为贝叶斯优化的信息起点加速收敛。然而这一直观假设在文献中缺乏系统验证，研究者希望探究默认值是否真正包含对优化有用的方向性信息。

Method: 研究采用截断高斯分布从库默认值周围采样作为贝叶斯优化的初始化点，与均匀随机初始化进行对比。实验涵盖三个BO后端（BoTorch、Optuna、Scikit-Optimize）、三种模型（随机森林、支持向量机、多层感知机）和五个基准数据集（分类和回归任务）。通过收敛速度和最终预测质量评估性能，使用单侧二项检验确定统计显著性。

Result: 在所有实验条件下，基于默认值的初始化相比纯随机采样没有统计显著优势（p值范围0.141-0.908）。敏感性分析显示，虽然更紧密地围绕默认值采样能改善早期评估，但这种短暂优势随着优化进程消失，最终性能保持不变。

Conclusion: 默认超参数值不包含对优化有用的方向性信息。建议实践者将超参数调优视为模型开发的重要组成部分，优先采用基于数据的搜索策略，而非启发式地依赖库默认值。

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [837] [A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation](https://arxiv.org/abs/2602.08785)
*Ofek Amran,Tom Gilat,Ron Levie*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种统一的方法，为所有规模的图（包括稀疏图和稠密图）定义紧致度量空间，在该空间下消息传递图神经网络（MPNNs）是Hölder连续的，从而得到比先前工作更强大的通用逼近定理和泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有MPNNs的泛化和逼近能力研究存在两个局限：1）当度量空间包含无界规模图时，理论仅适用于稠密图；2）当研究稀疏图时，度量空间仅包含均匀有界规模的图。需要一种统一的方法来处理所有规模的图。

Method: 基于并扩展了图算子分析（graphop analysis）方法，为所有规模的图（包括稀疏图和稠密图）定义了一个紧致度量空间，在该度量下MPNNs是Hölder连续的。

Result: 获得了比先前工作更强大的通用逼近定理和泛化界，为MPNNs的理论分析提供了更统一和强大的框架。

Conclusion: 该工作通过图算子分析方法，为MPNNs的泛化和逼近能力提供了统一的数学框架，解决了先前研究中稀疏图和稠密图分离的问题，推动了图神经网络理论的发展。

Abstract: Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are Hölder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are Hölder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.

</details>


### [838] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文简要综述了生物识别应用中的高效深度学习方法，包括训练和部署挑战、效率分类、评估指标及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在安全防御等应用中取得进展，但其训练和部署的计算需求导致高能耗和碳足迹，限制了在资源受限边缘设备上的实时应用，因此需要研究高效深度学习方法。

Method: 采用文献综述方法，对生物识别应用中的高效深度学习技术进行分类整理，提出效率评估的补充指标（内存、计算、延迟、吞吐量），并倡导通用可复现的评估标准。

Result: 建立了高效深度学习方法的分类体系，提出了全面的效率评估框架，为生物识别领域的高效模型开发提供了系统指导。

Conclusion: 高效深度学习对生物识别应用的可持续发展和边缘部署至关重要，需要建立标准化评估指标并探索新的研究方向。

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [839] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种基于度量空间magnitude概念的新型数据集距离度量——magnitude distance，包含可调缩放参数t控制对全局结构（小t）和细节（大t）的敏感性，证明了其理论性质，展示了在高维设置中的判别性，并应用于生成模型的训练目标。


<details>
  <summary>Details</summary>
Motivation: 量化数据集之间的距离是数学和机器学习中的基本问题。现有距离度量在高维设置中可能失去判别性，需要一种能捕捉不同尺度结构、理论性质良好且适用于机器学习任务的新型距离度量。

Method: 基于度量空间的magnitude概念定义magnitude distance，引入可调缩放参数t控制敏感性。从理论上分析其性质，包括尺度极限行为和度量性质条件。将其作为push-forward生成模型的训练目标进行实验验证。

Result: 证明了magnitude distance的理论性质，包括在不同尺度下的极限行为。实验表明，与经典距离相比，magnitude distance在高维设置中通过适当调整尺度参数仍能保持判别性，作为生成模型训练目标时提供有意义的信号，与现有距离基生成方法相当。

Conclusion: magnitude distance是一种理论性质良好、可调尺度敏感性的新型数据集距离度量，在高维设置中保持判别性，适用于生成模型训练等机器学习任务，为数据集比较提供了新的理论框架。

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [840] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了一种基于离散桥匹配模型的互信息估计器（DBMI），将互信息估计构建为域转移问题，适用于传统方法难以处理的离散数据。


<details>
  <summary>Details</summary>
Motivation: 离散随机变量之间的互信息估计在机器学习和信息理论中是一个重要问题，但传统互信息估计器难以处理离散数据。扩散桥模型在连续和离散状态空间中的成功应用为解决这一问题提供了新思路。

Method: 利用离散状态空间桥匹配模型的框架，将互信息估计构建为域转移问题，提出离散桥互信息（DBMI）估计器。该方法适用于离散数据，解决了传统估计器在处理离散数据时的困难。

Result: 在两种互信息估计场景中展示了DBMI估计器的性能：低维场景和基于图像的场景。结果表明该方法在离散数据互信息估计方面具有良好表现。

Conclusion: 离散桥匹配模型框架可以有效地应用于互信息估计问题，提出的DBMI估计器为离散数据的互信息估计提供了一种新方法，扩展了扩散桥模型的应用范围。

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [841] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

Relevance: 35.0

TL;DR: ShapeCond：基于shapelet的时间序列数据集压缩框架，通过shapelet引导的优化策略保留关键局部模式，显著提升压缩效率和下游分类精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据快速增长给存储和计算带来压力，现有数据集压缩方法主要针对图像，忽略了时间序列特有的时间结构和局部判别模式（如shapelets），导致在时间序列上效果不佳。

Method: 提出ShapeCond框架，利用shapelet引导的优化策略进行时间序列数据集压缩。通过shapelet辅助的合成方法，使计算成本与序列长度无关，显著提升压缩效率。

Result: 在Sleep数据集（3000时间步）上比现有最佳方法CondTSC快29倍，比直接使用shapelets快10000倍。在所有实验中一致优于现有时间序列数据集压缩方法，提升下游分类精度。

Conclusion: ShapeCond通过显式保留关键局部模式，实现了高效的时间序列数据集压缩，在压缩速度和下游任务性能上均优于现有方法。

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


### [842] [Curriculum-Learned Vanishing Stacked Residual PINNs for Hyperbolic PDE State Reconstruction](https://arxiv.org/abs/2602.06996)
*Katayoun Eshkofti,Matthieu Barreau*

Main category: cs.NE

Relevance: 35.0

TL;DR: 该论文将三种课程学习方法（原始-对偶优化、因果递进、自适应采样）集成到VSR-PINN中，用于解决双曲偏微分方程中的不连续性和激波问题，在交通重建任务中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络在处理双曲偏微分方程时，由于不连续性和激波的存在而难以收敛。VSR-PINN虽然通过嵌入消失黏性机制改进了这一问题，但仍需进一步优化训练策略以提高性能。

Method: 将三种课程学习方法集成到VSR-PINN框架中：1) 原始-对偶优化平衡物理和数据损失；2) 因果递进方案尊重时间和梯度演化，解锁更深层堆叠；3) 自适应采样针对高残差区域进行采样。

Result: 在交通重建数值实验中，强制因果性训练系统性地降低了中值点对点MSE及其运行间变异性，相比非因果训练在基线和PD变体中都实现了近一个数量级的改进。

Conclusion: 课程学习方法特别是因果递进策略显著提升了VSR-PINN在双曲PDE建模中的性能，为解决分布式动力系统中的不连续性和激波问题提供了有效途径。

Abstract: Modeling distributed dynamical systems governed by hyperbolic partial differential equations (PDEs) remains challenging due to discontinuities and shocks that hinder the convergence of traditional physics-informed neural networks (PINNs). The recently proposed vanishing stacked residual PINN (VSR-PINN) embeds a vanishing-viscosity mechanism within stacked residual refinements to enable a smooth transition from the parabolic to hyperbolic regime. This paper integrates three curriculum-learning methods as primal-dual (PD) optimization, causality progression, and adaptive sampling into the VSR-PINN. The PD strategy balances physics and data losses, the causality scheme unlocks deeper stacks by respecting temporal and gradient evolution, and adaptive sampling targets high residuals. Numerical experiments on traffic reconstruction confirm that enforcing causality systematically reduces the median point-wise MSE and its variability across runs, yielding improvements of nearly one order of magnitude over non-causal training in both the baseline and PD variants.

</details>


### [843] [Financial Bond Similarity Search Using Representation Learning](https://arxiv.org/abs/2602.07020)
*Amin Haeri,Mahdi Ghelichi,Nishant Agrawal,David Li,Catalina Gomez Sanchez*

Main category: q-fin.ST

Relevance: 35.0

TL;DR: 该论文提出使用嵌入模型捕捉债券分类属性（如发行人行业和注册地）的语义相似性，以改进利差曲线预测，优于独热编码等基线方法。


<details>
  <summary>Details</summary>
Motivation: 固定收益分析中寻找相似债券具有挑战性，因为数值金融属性往往掩盖了分类非金融属性（如发行人行业和注册地）。这些分类属性对利差曲线的可预测性有重要影响，但现有方法未能充分捕捉其语义相似性。

Method: 提出嵌入模型来捕捉债券分类属性的语义相似性，通过稀疏发行人增强进行评估，改进风险建模和曲线构建。

Result: 该方法在利差曲线预测方面优于独热编码和其他基线方法，证明分类属性确实主导利差曲线的可预测性。

Conclusion: 债券的分类非金融属性（如发行人行业和注册地）对利差曲线预测至关重要，通过嵌入模型捕捉其语义相似性可以显著改进固定收益分析。

Abstract: Finding similar bonds remains challenging in fixed-income analytics, as numerical financial attributes often overshadow categorical non-financial ones such as issuer sector and domicile. This paper shows that these categorical attributes dominate the predictability of spread curves and proposes embedding models to capture their semantic similarities, outperforming one-hot and many other baselines. Evaluated via sparse-issuer augmentation, the approach improves risk modeling and curve construction.

</details>


### [844] [BayesFlow 2.0: Multi-Backend Amortized Bayesian Inference in Python](https://arxiv.org/abs/2602.07098)
*Lars Kühmichel,Jerry M. Huang,Valentin Pratz,Jonas Arruda,Hans Olischläger,Daniel Habermann,Simon Kucharsky,Lasse Elsemüller,Aayush Mishra,Niels Bracher,Svenja Jedhoff,Marvin Schmitt,Paul-Christian Bürkner,Stefan T. Radev*

Main category: stat.CO

Relevance: 35.0

TL;DR: BayesFlow 2.0是一个用于摊销贝叶斯推断的Python库，通过神经网络训练模型模拟来加速复杂模型的贝叶斯推断，支持后验、似然和比率估计等多种功能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法在处理复杂模型和大数据集时计算速度慢，摊销贝叶斯推断通过神经网络训练模型模拟来解决这一计算挑战，提供快速推断能力。

Method: 开发BayesFlow 2.0 Python库，支持多种深度学习后端，提供丰富的生成网络用于采样和密度估计，包含高层接口和完整定制功能，支持超参数优化、设计优化和层次建模。

Result: 通过动态系统参数估计的案例研究以及与类似软件的比较，表明该库具有流线化、用户友好的工作流程，有潜力支持广泛采用。

Conclusion: BayesFlow 2.0为摊销贝叶斯推断提供了一个通用工具，能够显著加速复杂贝叶斯模型的推断过程，具有实际应用价值。

Abstract: Modern Bayesian inference involves a mixture of computational methods for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows. An overarching motif of many Bayesian methods is that they are relatively slow, which often becomes prohibitive when fitting complex models to large data sets. Amortized Bayesian inference (ABI) offers a path to solving the computational challenges of Bayes. ABI trains neural networks on model simulations, rewarding users with rapid inference of any model-implied quantity, such as point estimates, likelihoods, or full posterior distributions. In this work, we present the Python library BayesFlow, Version 2.0, for general-purpose ABI. Along with direct posterior, likelihood, and ratio estimation, the software includes support for multiple popular deep learning backends, a rich collection of generative networks for sampling and density estimation, complete customization and high-level interfaces, as well as new capabilities for hyperparameter optimization, design optimization, and hierarchical modeling. Using a case study on dynamical system parameter estimation, combined with comparisons to similar software, we show that our streamlined, user-friendly workflow has strong potential to support broad adoption.

</details>


### [845] [Beyond Crash: Hijacking Your Autonomous Vehicle for Fun and Profit](https://arxiv.org/abs/2602.07249)
*Qi Sun,Ahmed Abdo,Luis Burbano,Ziyang Li,Yaxing Yao,Alvaro Cardenas,Yinzhi Cao*

Main category: cs.CR

Relevance: 35.0

TL;DR: JackZebra：首个针对视觉端到端自动驾驶系统的路线级劫持攻击框架，通过物理可实现的攻击车辆和可重构显示屏，逐步将受害车辆引导至攻击者选择的目的地。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在安全关键环境中运行，现有物理对抗攻击主要针对即时安全故障（如碰撞、违规），但缺乏对长期路线完整性的研究。本文揭示了一种新型风险：长期路线完整性破坏，攻击者逐步将受害车辆引导至攻击者选择的目的地，而车辆看似正常行驶。

Method: 设计JackZebra框架，使用物理攻击车辆（后部安装可重构显示屏）进行路线级劫持。核心挑战是时间持久性：对抗影响必须在视角、光照、天气、交通变化及受害者持续重新规划中保持有效。关键洞察是将路线劫持视为闭环控制问题，将对抗补丁转换为可通过交互调整循环在线选择的转向原语。

Result: 评估显示JackZebra能成功劫持受害车辆偏离原始路线，并以高成功率在对抗目的地停止。对抗补丁针对最坏背景和传感器变化设计，确保对抗影响有效。

Conclusion: 本文首次展示了视觉端到端自动驾驶系统的长期路线完整性风险，提出了有效的物理对抗攻击框架，为自动驾驶安全研究提供了新方向。

Abstract: Autonomous Vehicles (AVs), especially vision-based AVs, are rapidly being deployed without human operators. As AVs operate in safety-critical environments, understanding their robustness in an adversarial environment is an important research problem. Prior physical adversarial attacks on vision-based autonomous vehicles predominantly target immediate safety failures (e.g., a crash, a traffic-rule violation, or a transient lane departure) by inducing a short-lived perception or control error. This paper shows a qualitatively different risk: a long-horizon route integrity compromise, where an attacker gradually steers a victim AV away from its intended route and into an attacker-chosen destination while the victim continues to drive "normally." This will not pose a danger to the victim vehicle itself, but also to potential passengers sitting inside the vehicle.
  In this paper, we design and implement the first adversarial framework, called JackZebra, that performs route-level hijacking of a vision-based end-to-end driving stack using a physically plausible attacker vehicle with a reconfigurable display mounted on the rear. The central challenge is temporal persistence: adversarial influence must remain effective in changing viewpoints, lighting, weather, traffic, and the victim's continual replanning -- without triggering conspicuous failures. Our key insight is to treat route hijacking as a closed-loop control problem and to convert adversarial patches into steering primitives that can be selected online via an interactive adjustment loop. Our adversarial patches are also carefully designed against worst-case background and sensor variations so that the adversarial impacts on the victim. Our evaluation shows that JackZebra can successfully hijack victim vehicles to deviate from original routes and stop at adversarial destinations with a high success rate.

</details>


### [846] [Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units](https://arxiv.org/abs/2602.07518)
*Manuel Escudero,Mohamadreza Zolfagharinejad,Sjoerd van den Belt,Nikolaos Alachiotis,Wilfred G. van der Wiel*

Main category: cs.ET

Relevance: 35.0

TL;DR: 提出基于可重构非线性处理单元(RNPU)的模拟KAN硬件架构，实现高效能、低延迟的模拟神经网络推理


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 将神经计算从线性层转移到可学习的非线性边函数，但在硬件中高效实现这些非线性仍然是一个挑战。需要开发物理模拟KAN架构来解决硬件实现问题。

Method: 提出物理模拟KAN架构，使用可重构非线性处理单元(RNPU)作为边函数的实现单元。RNPU是多端纳米级硅器件，通过控制电压调节输入输出特性。将多个RNPU组合成边处理器，组装成可重构模拟KAN (aKAN)架构，集成混合信号接口。

Result: 使用实验校准的RNPU模型和硬件测量，展示了在各种任务复杂度下的准确函数逼近，所需可训练参数少于或多层感知机(MLP)。系统级估计显示每次推理能耗约250pJ，端到端推理延迟约600ns，相比数字定点MLP在相似近似误差下，能耗降低10^2-10^3倍，面积减少约10倍。

Conclusion: RNPU可作为可扩展的硬件原生非线性计算原语，模拟KAN架构为能量、延迟和面积高效的模拟神经网络硬件提供了现实的硅基实现路径，特别适用于边缘推理场景。

Abstract: Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities efficiently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfigurable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transformations. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of $\sim$250 pJ and an end-to-end inference latency of $\sim$600 ns for a representative workload, corresponding to a $\sim$10$^{2}$-10$^{3}\times$ reduction in energy accompanied by a $\sim$10$\times$ reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify analog KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference.

</details>


### [847] [Capturing the Topological Phase Transition and Thermodynamics of the 2D XY Model via Manifold-Aware Score-Based Generative Modeling](https://arxiv.org/abs/2602.07548)
*Pratyush Jha*

Main category: cond-mat.stat-mech

Relevance: 35.0

TL;DR: 提出了一种面向流形的基于分数的生成模型框架，用于连续自旋系统的物理建模，相比标准扩散模型能更精确估计玻尔兹曼分数，成功捕捉BKT相变并实现零样本泛化到未见过的晶格尺寸。


<details>
  <summary>Details</summary>
Motivation: 生成建模在多体物理中具有应用潜力，但连续自旋系统的变量本质存在于流形上，而标准的基于欧几里得嵌入空间的分数生成模型不适合此类系统。在欧几里得空间上训练会损害模型学习目标分布的能力，因为它优先学习流形约束。

Method: 提出了流形感知的基于分数的生成建模框架，应用于64x64二维XY模型（4096维环面）。该方法考虑了连续自旋系统的流形结构，而不是在欧几里得嵌入空间中进行训练。

Result: 该方法相比标准扩散模型能更精确地估计理论玻尔兹曼分数，成功捕捉了Berezinskii-Kosterlitz-Thouless（BKT）相变，准确再现了热容等二阶矩量，无需显式特征工程。实现了零样本泛化到未见过的晶格尺寸，无需重新训练就能准确恢复不同系统尺度的物理特性。

Conclusion: 流形感知的生成建模框架为连续自旋系统提供了有效的物理建模方法，能够准确捕捉相变和热力学量，并具有良好的泛化能力。由于绕过了领域特定的特征工程，该方法本质上可推广到其他连续自旋系统。

Abstract: The application of generative modeling to many-body physics offers a promising pathway for analyzing high-dimensional state spaces of spin systems. However, unlike computer vision tasks where visual fidelity suffices, physical systems require the rigorous reproduction of higher-order statistical moments and thermodynamic quantities. While Score-Based Generative Models (SGMs) have emerged as a powerful tool, their standard formulation on Euclidean embedding space is ill-suited for continuous spin systems, where variables inherently reside on a manifold. In this work, we demonstrate that training on the Euclidean space compromises the model's ability to learn the target distribution as it prioritizes to learn the manifold constraints. We address this limitation by proposing the use of Manifold-Aware Score-Based Generative Modeling framework applied to the 64x64 2D XY model (a 4096-dimensional torus). We show that our method estimates the theoretical Boltzmann score with superior precision compared to standard diffusion models. Consequently, we successfully capture the Berezinskii-Kosterlitz Thouless (BKT) phase transition and accurately reproduce second-moment quantities, such as heat capacity without explicit feature engineering. Furthermore, we demonstrate zero-shot generalization to unseen lattice sizes, accurately recovering the physics of variable system scales without retraining. Since this approach bypasses domain-specific feature engineering, it remains intrinsically generalizable to other continuous spin systems.

</details>


### [848] [$\partial$CBDs: Differentiable Causal Block Diagrams](https://arxiv.org/abs/2602.07581)
*Thomas Beckers,Ján Drgoňa,Truong X. Nghiem*

Main category: eess.SY

Relevance: 35.0

TL;DR: 论文提出了一种可微因果框图（∂CBDs）的统一形式化方法，将因果框图、可微编程和契约验证相结合，为信息物理系统提供可组合、可学习和可验证的建模框架。


<details>
  <summary>Details</summary>
Motivation: 现代信息物理系统需要同时满足可组合性、可学习性和可验证性的建模框架，但现有方法（因果框图、可微编程、契约验证）各自孤立，无法同时满足这些需求。

Method: 提出∂CBDs框架：1）保留因果框图的组合结构和执行语义；2）集成假设-保证契约进行模块化正确性推理；3）引入基于残差的契约作为可微的轨迹级证书，兼容自动微分，支持基于梯度的优化和学习。

Result: 创建了一个可扩展、可验证、可训练的建模流程，在保持因果性和模块化的同时，支持基于数据、物理和约束的信息物理系统优化。

Conclusion: ∂CBDs统一了因果框图、可微编程和契约验证，为信息物理系统提供了同时满足可组合性、可学习性和可验证性的建模解决方案。

Abstract: Modern cyber-physical systems (CPS) integrate physics, computation, and learning, demanding modeling frameworks that are simultaneously composable, learnable, and verifiable. Yet existing approaches treat these goals in isolation: causal block diagrams (CBDs) support modular system interconnections but lack differentiability for learning; differentiable programming (DP) enables end-to-end gradient-based optimization but provides limited correctness guarantees; while contract-based verification frameworks remain largely disconnected from data-driven model refinement. To address these limitations, we introduce differentiable causal block diagrams ($\partial$CBDs), a unifying formalism that integrates these three perspectives. Our approach (i) retains the compositional structure and execution semantics of CBDs, (ii) incorporates assume--guarantee (A--G) contracts for modular correctness reasoning, and (iii) introduces residual-based contracts as differentiable, trajectory-level certificates compatible with automatic differentiation (AD), enabling gradient-based optimization and learning. Together, these elements enable a scalable, verifiable, and trainable modeling pipeline that preserves causality and modularity while supporting data-, physics-, and constraint-informed optimization for CPS.

</details>


### [849] [Scalable Mean-Field Variational Inference via Preconditioned Primal-Dual Optimization](https://arxiv.org/abs/2602.07632)
*Jinhua Lyu,Tianmin Yu,Ying Ma,Naichen Shi*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出PD-VI和P²D-VI两种变分推断算法，通过原始-对偶视角解决大规模平均场变分推断问题，具有更好的收敛速度和求解质量


<details>
  <summary>Details</summary>
Motivation: 传统变分推断方法在大规模问题上存在收敛慢、数值稳定性差等问题，需要开发更高效、稳健的算法来处理大规模平均场变分推断问题

Method: 将平均场变分推断重新表述为约束有限和问题，基于增广拉格朗日方法开发原始-对偶变分推断算法(PD-VI)，并进一步引入块预处理扩展(P²D-VI)以适应不同变分参数块的异质损失几何

Result: 在合成数据和真实大规模空间转录组数据集上的实验表明，该方法在收敛速度和求解质量上优于现有随机变分推断方法

Conclusion: 提出的原始-对偶变分推断框架为大规模平均场变分推断提供了高效、稳健的解决方案，具有理论保证和实际应用价值

Abstract: In this work, we investigate the large-scale mean-field variational inference (MFVI) problem from a mini-batch primal-dual perspective. By reformulating MFVI as a constrained finite-sum problem, we develop a novel primal-dual algorithm based on an augmented Lagrangian formulation, termed primal-dual variational inference (PD-VI). PD-VI jointly updates global and local variational parameters in the evidence lower bound in a scalable manner. To further account for heterogeneous loss geometry across different variational parameter blocks, we introduce a block-preconditioned extension, P$^2$D-VI, which adapts the primal-dual updates to the geometry of each parameter block and improves both numerical robustness and practical efficiency. We establish convergence guarantees for both PD-VI and P$^2$D-VI under properly chosen constant step size, without relying on conjugacy assumptions or explicit bounded-variance conditions. In particular, we prove $O(1/T)$ convergence to a stationary point in general settings and linear convergence under strong convexity. Numerical experiments on synthetic data and a real large-scale spatial transcriptomics dataset demonstrate that our methods consistently outperform existing stochastic variational inference approaches in terms of convergence speed and solution quality.

</details>


### [850] [Graph-based Semi-Supervised Learning via Maximum Discrimination](https://arxiv.org/abs/2602.08042)
*Nadav Katz,Ariel Jaffe*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出AUC-spec方法，一种基于图的半监督学习方法，通过优化AUC指标来最大化类别分离，在复杂标签分布数据上表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统基于图的半监督学习方法（如标签传播）在复杂标签分布数据上可能表现不佳，需要一种能更好处理类别分离的方法

Method: 开发AUC-spec方法，通过优化ROC曲线下面积（AUC）来学习低维表示，最大化类别分离，同时保持计算效率

Result: 理论分析显示所需标注点数量与模型参数呈多项式关系；实验表明AUC-spec在合成和真实数据集上具有竞争力，平衡了类别分离与图平滑性

Conclusion: AUC-spec是一种有效的图半监督学习方法，特别适用于复杂标签分布场景，在保持计算效率的同时提升分类性能

Abstract: Semi-supervised learning (SSL) addresses the critical challenge of training accurate models when labeled data is scarce but unlabeled data is abundant. Graph-based SSL (GSSL) has emerged as a popular framework that captures data structure through graph representations. Classic graph SSL methods, such as Label Propagation and Label Spreading, aim to compute low-dimensional representations where points with the same labels are close in representation space. Although often effective, these methods can be suboptimal on data with complex label distributions. In our work, we develop AUC-spec, a graph approach that computes a low-dimensional representation that maximizes class separation. We compute this representation by optimizing the Area Under the ROC Curve (AUC) as estimated via the labeled points. We provide a detailed analysis of our approach under a product-of-manifold model, and show that the required number of labeled points for AUC-spec is polynomial in the model parameters. Empirically, we show that AUC-spec balances class separation with graph smoothness. It demonstrates competitive results on synthetic and real-world datasets while maintaining computational efficiency comparable to the field's classic and state-of-the-art methods.

</details>


### [851] [GAAVI: Global Asymptotic Anytime Valid Inference for the Conditional Mean Function](https://arxiv.org/abs/2602.08096)
*Brian M Cho,Raaz Dwivedi,Nathan Kallus*

Main category: stat.ME

Relevance: 35.0

TL;DR: 提出了一种用于条件均值函数（CMF）推断的渐近任意时间有效检验方法，支持实验过程中任意时间点的高置信度决策，具有渐近类型I错误保证、功效为1和最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 条件均值函数推断在自适应实验、最优治疗分配和算法公平性审计等任务中至关重要。现有方法在连续监测下难以同时保证类型I错误控制和统计功效，需要一种能在实验过程中任意时间点做出高置信度决策的检验方法。

Method: 开发了一种渐近任意时间有效检验方法，用于CMF的全局零假设检验和CMF间对比。通过反演检验构造函数值的渐近置信序列。方法在最小样本量后支持任意时间点的决策，满足渐近类型I错误保证、功效为1和相对于高斯位置检验的最优样本复杂度。

Result: 在合成和真实世界数据上的实验表明，该方法在各种分布下具有良好的统计功效，同时在连续监测下保持名义错误率。相比以往检验，首次同时实现了渐近类型I错误控制、功效为1和最优样本复杂度。

Conclusion: 该方法为条件均值函数推断提供了一种实用的统计工具，特别适用于需要连续监测和任意时间决策的应用场景，如自适应实验和算法公平性审计。

Abstract: Inference on the conditional mean function (CMF) is central to tasks from adaptive experimentation to optimal treatment assignment and algorithmic fairness auditing. In this work, we provide a novel asymptotic anytime-valid test for a CMF global null (e.g., that all conditional means are zero) and contrasts between CMFs, enabling experimenters to make high confidence decisions at any time during the experiment beyond a minimum sample size. We provide mild conditions under which our tests achieve (i) asymptotic type-I error guarantees, (i) power one, and, unlike past tests, (iii) optimal sample complexity relative to a Gaussian location testing. By inverting our tests, we show how to construct function-valued asymptotic confidence sequences for the CMF and contrasts thereof. Experiments on both synthetic and real-world data show our method is well-powered across various distributions while preserving the nominal error rate under continuous monitoring.

</details>


### [852] [Evasion of IoT Malware Detection via Dummy Code Injection](https://arxiv.org/abs/2602.08170)
*Sahar Zargarzadeh,Mohammad Islam*

Main category: cs.CR

Relevance: 35.0

TL;DR: 该论文提出了一种针对基于功耗侧信道分析的物联网恶意软件检测系统的对抗攻击方法，通过在Mirai僵尸网络的扫描阶段注入结构化虚拟代码来动态扰动功耗特征，从而在不影响核心功能的情况下逃避AI/ML异常检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的快速扩张带来了严重的安全漏洞，使其成为恶意软件（如Mirai僵尸网络）的攻击目标。功耗侧信道分析作为一种有前景的恶意软件检测技术，但其在对抗性操纵下的鲁棒性尚未得到充分探索。

Method: 提出了一种新颖的对抗策略：在Mirai僵尸网络的扫描阶段注入结构化虚拟代码，动态扰动功耗特征以逃避AI/ML异常检测。该方法系统分析了隐蔽性、执行开销和逃避效果之间的权衡，使用从不同制造商智能手机收集的自定义数据集，在多个最先进的侧信道分析模型上进行评估。

Result: 实验结果表明，对抗性修改实现了平均75.2%的攻击成功率，揭示了基于功耗的入侵检测框架的实际漏洞。

Conclusion: 该研究揭示了功耗侧信道检测系统在对抗性攻击下的脆弱性，强调了在物联网安全中需要考虑对抗性鲁棒性的重要性。

Abstract: The Internet of Things (IoT) has revolutionized connectivity by linking billions of devices worldwide. However, this rapid expansion has also introduced severe security vulnerabilities, making IoT devices attractive targets for malware such as the Mirai botnet. Power side-channel analysis has recently emerged as a promising technique for detecting malware activity based on device power consumption patterns. However, the resilience of such detection systems under adversarial manipulation remains underexplored.
  This work presents a novel adversarial strategy against power side-channel-based malware detection. By injecting structured dummy code into the scanning phase of the Mirai botnet, we dynamically perturb power signatures to evade AI/ML-based anomaly detection without disrupting core functionality. Our approach systematically analyzes the trade-offs between stealthiness, execution overhead, and evasion effectiveness across multiple state-of-the-art models for side-channel analysis, using a custom dataset collected from smartphones of diverse manufacturers. Experimental results show that our adversarial modifications achieve an average attack success rate of 75.2\%, revealing practical vulnerabilities in power-based intrusion detection frameworks.

</details>


### [853] [Adaptive Matrix Online Learning through Smoothing with Guarantees for Nonsmooth Nonconvex Optimization](https://arxiv.org/abs/2602.08232)
*Ruichen Jiang,Zakaria Mhammedi,Mehryar Mohri,Aryan Mokhtari*

Main category: math.OC

Relevance: 35.0

TL;DR: 该论文提出了两种高效的矩阵在线学习算法，避免了Shampoo方法中昂贵的二次投影子问题，同时保持了相似的遗憾界，并将这些在线算法转换为非凸优化的矩阵优化器。


<details>
  <summary>Details</summary>
Motivation: 在线线性优化中，矩阵变量受算子范数约束时，设计数据依赖的高效自适应算法具有挑战性。现有的Shampoo类方法虽然能达到最佳自适应遗憾界，但需要解决昂贵的二次投影子问题，计算成本高。

Method: 将基于梯度的预测方案扩展到自适应矩阵在线学习，将算法设计转化为构造核范数的一族平滑势函数。定义了这类平滑的可容许性概念，并证明任何可容许平滑都能产生与单边Shampoo最佳已知保证相匹配的遗憾界。具体实例化了两种高效方法：1）使用高斯随机平滑的自适应FTPL方法；2）在增广矩阵空间中使用确定性双曲平滑的FAML方法。

Result: 两种方法都允许闭式更新，与单边Shampoo的遗憾界相差常数因子，同时显著降低了计算成本。通过在线到非凸转换，推导出两种基于矩阵的优化器：Pion（来自FTPL）和Leon（来自FAML），在非光滑非凸设置中证明了收敛保证。

Conclusion: 该工作为矩阵在线学习提供了高效的自适应算法，避免了昂贵的二次投影，同时保持了理论保证，并将这些算法扩展到非凸优化设置，为大规模矩阵优化问题提供了实用的解决方案。

Abstract: We study online linear optimization with matrix variables constrained by the operator norm, a setting where the geometry renders designing data-dependent and efficient adaptive algorithms challenging. The best-known adaptive regret bounds are achieved by Shampoo-like methods, but they require solving a costly quadratic projection subproblem. To address this, we extend the gradient-based prediction scheme to adaptive matrix online learning and cast algorithm design as constructing a family of smoothed potentials for the nuclear norm. We define a notion of admissibility for such smoothings and prove any admissible smoothing yields a regret bound matching the best-known guarantees of one-sided Shampoo. We instantiate this framework with two efficient methods that avoid quadratic projections. The first is an adaptive Follow-the-Perturbed-Leader (FTPL) method using Gaussian stochastic smoothing. The second is Follow-the-Augmented-Matrix-Leader (FAML), which uses a deterministic hyperbolic smoothing in an augmented matrix space. By analyzing the admissibility of these smoothings, we show both methods admit closed-form updates and match one-sided Shampoo's regret up to a constant factor, while significantly reducing computational cost. Lastly, using the online-to-nonconvex conversion, we derive two matrix-based optimizers, Pion (from FTPL) and Leon (from FAML). We prove convergence guarantees for these methods in nonsmooth nonconvex settings, a guarantee that the popular Muon optimizer lacks.

</details>


### [854] [Trajectory Stitching for Solving Inverse Problems with Flow-Based Models](https://arxiv.org/abs/2602.08538)
*Alexander Denker,Moshe Eliasof,Zeljko Kereta,Carola-Bibiane Schönlieb*

Main category: eess.IV

Relevance: 35.0

TL;DR: MS-Flow提出了一种基于流模型的逆问题求解方法，通过将生成轨迹表示为中间潜在状态序列而非单一初始噪声，降低了内存消耗并提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于流模型的逆问题求解方法通常直接优化初始潜在代码（噪声），这需要在整个生成轨迹上进行反向传播，导致高内存成本和数值不稳定性。

Method: MS-Flow将轨迹表示为中间潜在状态序列，通过局部强制执行流动力学并通过轨迹匹配惩罚耦合各个片段，交替更新中间潜在状态并确保与观测数据的一致性。

Result: MS-Flow在图像恢复和逆问题（包括修复、超分辨率和计算机断层扫描）上优于现有方法，同时减少了内存消耗。

Conclusion: MS-Flow通过中间状态表示和局部优化策略，为基于流模型的逆问题求解提供了一种更高效、更稳定的方法。

Abstract: Flow-based generative models have emerged as powerful priors for solving inverse problems. One option is to directly optimize the initial latent code (noise), such that the flow output solves the inverse problem. However, this requires backpropagating through the entire generative trajectory, incurring high memory costs and numerical instability. We propose MS-Flow, which represents the trajectory as a sequence of intermediate latent states rather than a single initial code. By enforcing the flow dynamics locally and coupling segments through trajectory-matching penalties, MS-Flow alternates between updating intermediate latent states and enforcing consistency with observed data. This reduces memory consumption while improving reconstruction quality. We demonstrate the effectiveness of MS-Flow over existing methods on image recovery and inverse problems, including inpainting, super-resolution, and computed tomography.

</details>


### [855] [Constructive conditional normalizing flows](https://arxiv.org/abs/2602.08606)
*Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: math.OC

Relevance: 35.0

TL;DR: 论文提出两种基于连续性方程流的神经网络构造方法，用于同时逼近微分同胚映射及其推前测度，应用于条件采样问题。


<details>
  <summary>Details</summary>
Motivation: 受条件采样应用驱动，研究如何通过连续性方程流同时逼近微分同胚映射及其推前测度，其中速度场由分段常数权重的感知器神经网络表示。

Method: 提出两种构造方法：1) 基于拉格朗日插值的极分解方法，将映射分解为可压缩分量（凸函数梯度）和不可压缩分量（通过剪切流实现）；2) 对于更规则映射（如Knöthe-Rosenblatt重排），采用受Maurey经验方法启发的概率构造。

Result: 提供了显式构造方法，其中第一种方法通过精确实现可压缩分量和近似实现不可压缩分量来逼近映射；第二种方法中权重不连续点的数量不随环境维度反比缩放。

Conclusion: 论文为条件采样问题提供了有效的神经网络构造框架，能够同时逼近微分同胚映射及其推前测度，为相关应用提供了理论基础。

Abstract: Motivated by applications in conditional sampling, given a probability measure $μ$ and a diffeomorphism $φ$, we consider the problem of simultaneously approximating $φ$ and the pushforward $φ_{\#}μ$ by means of the flow of a continuity equation whose velocity field is a perceptron neural network with piecewise constant weights. We provide an explicit construction based on a polar-like decomposition of the Lagrange interpolant of $φ$. The latter involves a compressible component, given by the gradient of a particular convex function, which can be realized exactly, and an incompressible component, which -- after approximating via permutations -- can be implemented through shear flows intrinsic to the continuity equation. For more regular maps $φ$ -- such as the Knöthe-Rosenblatt rearrangement -- we provide an alternative, probabilistic construction inspired by the Maurey empirical method, in which the number of discontinuities in the weights doesn't scale inversely with the ambient dimension.

</details>


### [856] [Contrastive Learning for Diversity-Aware Product Recommendations in Retail](https://arxiv.org/abs/2602.08886)
*Vasileios Karlis,Ezgi Yıldırım,David Vos,Maarten de Rijke*

Main category: cs.IR

Relevance: 35.0

TL;DR: 该论文提出了一种结合对比学习和负采样的方法，用于提升推荐系统的商品目录覆盖率，解决长尾分布和热门商品主导推荐的问题，在宜家零售的推荐系统中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 推荐系统面临长尾分布和有限商品目录暴露的挑战，少数热门商品主导推荐结果。这在大型在线零售环境中尤为关键，因为商品种类繁多且多样。需要在不损害推荐质量的前提下提升目录覆盖率。

Method: 受负采样解决流行度偏差的最新进展启发，将对比学习与精心选择的负样本相结合。在宜家零售现有数字推荐流程中集成该方法。

Result: 通过离线和在线评估表明，该方法提高了目录覆盖率，确保推荐更加多样化，同时保持了强大的推荐性能。

Conclusion: 该方法成功平衡了推荐多样性和质量，为大规模零售推荐系统提供了有效的解决方案。

Abstract: Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.

</details>


### [857] [Winner's Curse Drives False Promises in Data-Driven Decisions: A Case Study in Refugee Matching](https://arxiv.org/abs/2602.08892)
*Hamsa Bastani,Osbert Bastani,Bryce McLaughlin*

Main category: stat.ML

Relevance: 35.0

TL;DR: 论文指出基于模型的策略评估方法存在"赢家诅咒"问题，即使模型准确、数据随机分配、模型族设定正确、使用样本分割等方法，仍会产生虚假的乐观估计。通过理论分析和难民匹配问题的仿真研究，证明该方法会报告虚假的60%改进，而真实效果为零。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策中的一个主要挑战是准确的政策评估，即保证学习到的决策策略能实现承诺的效益。基于模型的策略评估方法（通过数据估计模型来推断反事实结果）因"赢家诅咒"问题会产生过度乐观的估计。作者发现过去十年《管理科学》期刊中55篇相关论文中有53篇使用了这种有缺陷的方法。

Method: 1) 理论分析：证明即使模型准确、数据随机分配、模型族设定正确、使用样本分割等方法，赢家诅咒仍会导致虚假的乐观估计；2) 仿真研究：基于难民匹配问题构建合成环境，设计真实效果为零的场景，测试基于模型的方法的评估结果。

Result: 1) 理论分析显示赢家诅咒会导致虚假的显著效益报告；2) 仿真研究中，基于模型的方法报告了约60%的稳定改进，而真实效果为零，这与文献中报告的22-75%改进相当；3) 常见辩解（模型准确、数据随机、模型族正确、样本分割）均无法避免赢家诅咒。

Conclusion: 基于模型的评估方法存在根本缺陷，即使满足所有常见辩解条件，仍会产生虚假的乐观估计。研究结果强烈反对使用基于模型的评估方法，对数据驱动决策和政策评估领域有重要警示意义。

Abstract: A major challenge in data-driven decision-making is accurate policy evaluation-i.e., guaranteeing that a learned decision-making policy achieves the promised benefits. A popular strategy is model-based policy evaluation, which estimates a model from data to infer counterfactual outcomes. This strategy is known to produce unwarrantedly optimistic estimates of the true benefit due to the winner's curse. We searched the recent literature on data-driven decision-making, identifying a sample of 55 papers published in the Management Science in the past decade; all but two relied on this flawed methodology. Several common justifications are provided: (1) the estimated models are accurate, stable, and well-calibrated, (2) the historical data uses random treatment assignment, (3) the model family is well-specified, and (4) the evaluation methodology uses sample splitting. Unfortunately, we show that no combination of these justifications avoids the winner's curse. First, we provide a theoretical analysis demonstrating that the winner's curse can cause large, spurious reported benefits even when all these justifications hold. Second, we perform a simulation study based on the recent and consequential data-driven refugee matching problem. We construct a synthetic refugee matching environment (calibrated to closely match the real setting) but designed so that no assignment policy can improve expected employment compared to random assignment. Model-based methods report large, stable gains of around 60% even when the true effect is zero; these gains are on par with improvements of 22-75% reported in the literature. Our results provide strong evidence against model-based evaluation.

</details>


### [858] [Online monotone density estimation and log-optimal calibration](https://arxiv.org/abs/2602.08927)
*Rohan Hore,Ruodu Wang,Aaditya Ramdas*

Main category: stat.ML

Relevance: 35.0

TL;DR: 该论文研究在线单调密度估计问题，提出了两种在线估计器：经典Grenander估计器的在线版本和基于指数加权方法的专家聚合估计器。在良好设定的随机设置中，证明了在线估计器与真实密度之间的期望累积对数似然差距具有O(n^{1/3})界，并为专家聚合估计器建立了相对于事后选择的最佳离线单调估计器的√(n log n)路径遗憾界。应用方面，展示了如何将顺序假设检验中的log-optimal p-to-e校准器构建问题转化为在线单调密度估计问题。


<details>
  <summary>Details</summary>
Motivation: 在线密度估计在顺序数据分析和实时决策中具有重要应用，但现有方法大多关注离线设置。单调密度估计在非参数统计中是一个经典问题，但将其扩展到在线设置面临挑战。论文旨在开发能够顺序处理数据并保持单调性约束的在线密度估计方法，同时建立理论保证。

Method: 提出了两种在线单调密度估计器：1）经典Grenander估计器的在线版本，利用顺序数据更新单调密度估计；2）基于指数加权方法的专家聚合估计器，通过组合多个候选单调密度估计器来适应数据。方法的关键创新在于将离线单调密度估计技术扩展到在线设置，同时保持理论性质。

Result: 理论结果包括：1）在良好设定的随机设置中，在线估计器与真实密度之间的期望累积对数似然差距为O(n^{1/3})；2）专家聚合估计器相对于最佳离线单调估计器的路径遗憾界为√(n log n)；3）将方法应用于p-to-e校准器构建，建立了其最优性。数值实验验证了理论结果。

Conclusion: 论文成功地将单调密度估计扩展到在线设置，提出了具有理论保证的在线估计器。这些方法不仅解决了在线单调密度估计的基本问题，还为顺序假设检验中的校准器构建提供了新工具。理论结果展示了在线学习与统计密度估计的有趣交叉。

Abstract: We study the problem of online monotone density estimation, where density estimators must be constructed in a predictable manner from sequentially observed data. We propose two online estimators: an online analogue of the classical Grenander estimator, and an expert aggregation estimator inspired by exponential weighting methods from the online learning literature. In the well-specified stochastic setting, where the underlying density is monotone, we show that the expected cumulative log-likelihood gap between the online estimators and the true density admits an $O(n^{1/3})$ bound. We further establish a $\sqrt{n\log{n}}$ pathwise regret bound for the expert aggregation estimator relative to the best offline monotone estimator chosen in hindsight, under minimal regularity assumptions on the observed sequence. As an application of independent interest, we show that the problem of constructing log-optimal p-to-e calibrators for sequential hypothesis testing can be formulated as an online monotone density estimation problem. We adapt the proposed estimators to build empirically adaptive p-to-e calibrators and establish their optimality. Numerical experiments illustrate the theoretical results.

</details>


### [859] [Provably robust learning of regression neural networks using $β$-divergences](https://arxiv.org/abs/2602.08933)
*Abhik Ghosh,Suryasis Jana*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出rRNet框架，基于β散度实现回归神经网络的鲁棒学习，对异常值具有强鲁棒性，理论保证包括收敛性、有界影响函数和50%渐近崩溃点


<details>
  <summary>Details</summary>
Motivation: 传统回归神经网络使用均方误差损失，对异常值敏感。现有鲁棒训练方法范围有限，主要依赖经验验证，缺乏理论保证。需要一种具有理论保证的鲁棒学习框架

Method: 基于β散度（密度幂散度）的鲁棒学习框架rRNet，适用于广泛的回归神经网络（包括非光滑激活函数和误差密度）。采用交替优化方案，理论证明在温和条件下收敛到稳定点

Result: rRNet在理论上有界影响函数，对合适β选择具有局部鲁棒性；达到50%渐近崩溃点，提供全局鲁棒保证；仿真和真实数据分析显示在函数逼近和噪声预测任务中优于现有方法

Conclusion: rRNet为回归神经网络提供了理论保证的鲁棒学习框架，结合了局部和全局鲁棒性，是现有方法的有效替代

Abstract: Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $β$-divergence (also known as the density power divergence) which we call `rRNet'. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $β$, depending on the error density. We further prove that rRNet attains the optimal 50\% asymptotic breakdown point at the assumed model for all $β\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations.

</details>


### [860] [Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.08965)
*John Gardiner,Orlando Romero,Brendan Tivnan,Nicolò Dal Fabbro,George J. Pappas*

Main category: cs.MA

Relevance: 35.0

TL;DR: 该论文提出了首个利用共享量子纠缠作为协调资源的MARL训练框架，展示了在某些无通信合作游戏中量子纠缠策略优于经典随机策略的量子优势。


<details>
  <summary>Details</summary>
Motivation: 传统MARL中缺乏通信会严重阻碍协调。已有工作使用共享随机性作为相关设备来辅助分散决策，但量子纠缠提供了比经典随机性更强大的无通信相关策略类别。量子物理中的已知结果表明，在某些无通信的单轮合作游戏中，共享量子纠缠能够实现优于仅使用共享随机性的策略。

Method: 提出了基于可微分策略参数化的框架，能够优化量子测量操作，以及新颖的策略架构，将联合策略分解为量子协调器和分散的局部执行器。该方法允许从经验中学习量子优势策略。

Result: 首先展示了能够从经验中学习在单轮游戏中实现量子优势的策略（将游戏视为黑盒oracle）。然后证明了该方法能够在Dec-POMDP形式化的多智能体序列决策问题中学习具有量子优势的策略。

Conclusion: 该工作首次将量子纠缠作为协调资源引入MARL框架，扩展了无通信相关策略的能力边界，为量子增强的多智能体系统开辟了新方向。

Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>


### [861] [When do neural ordinary differential equations generalize on complex networks?](https://arxiv.org/abs/2602.08980)
*Moritz Laber,Tina Eliassi-Rad,Brennan Klein*

Main category: physics.soc-ph

Relevance: 35.0

TL;DR: 该论文研究了神经ODE在图结构数据上的泛化能力，发现节点度异质性和动力学系统类型是决定其跨图大小和属性泛化的主要因素，平均聚类系数起次要作用。


<details>
  <summary>Details</summary>
Motivation: 神经ODE能够有效学习时间序列数据中的动力学系统，但其在图结构数据上的行为尚不清楚，特别是在处理与训练时不同大小或结构的图时的泛化能力。

Method: 使用Barabási-Barzel形式的向量场构建神经ODE，在五个常见图动力学系统的合成数据上进行训练，利用𝕊¹模型生成具有可调结构的现实图。

Result: 节点度异质性和动力学系统类型是决定神经ODE跨图大小和属性泛化的主要因素，平均聚类系数对性能影响较小。神经ODE能够捕捉固定点并在数据缺失时保持性能。

Conclusion: 神经ODE是理解复杂系统的强大方法，但在现实图中面临来自节点度异质性和聚类的挑战。这些发现对图结构数据上的动力学建模具有重要意义。

Abstract: Neural ordinary differential equations (neural ODEs) can effectively learn dynamical systems from time series data, but their behavior on graph-structured data remains poorly understood, especially when applied to graphs with different size or structure than encountered during training. We study neural ODEs ($\mathtt{nODE}$s) with vector fields following the Barabási-Barzel form, trained on synthetic data from five common dynamical systems on graphs. Using the $\mathbb{S}^1$-model to generate graphs with realistic and tunable structure, we find that degree heterogeneity and the type of dynamical system are the primary factors in determining $\mathtt{nODE}$s' ability to generalize across graph sizes and properties. This extends to $\mathtt{nODE}$s' ability to capture fixed points and maintain performance amid missing data. Average clustering plays a secondary role in determining $\mathtt{nODE}$ performance. Our findings highlight $\mathtt{nODE}$s as a powerful approach to understanding complex systems but underscore challenges emerging from degree heterogeneity and clustering in realistic graphs.

</details>


### [862] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

Relevance: 30.0

TL;DR: VertCoHiRF是一个完全去中心化的垂直联邦聚类框架，基于异构视图间的结构共识，通过标识符级别的共识而非特征统计交换来实现隐私保护


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习方法主要局限于分布式k-means变体，需要中心化协调或交换特征相关的数值统计，在异构视图或对抗行为下鲁棒性有限

Method: 完全去中心化框架，每个代理在本地特征空间独立聚类，通过标识符级别的共识协调聚类提案，使用去中心化序数排序选择代表性中心点，逐步构建共享的层次聚类结构

Result: 在垂直联邦设置中展示了有竞争力的聚类性能，通信仅限于样本标识符、聚类标签和序数排序，提供隐私保护设计，支持重叠特征分区和异构本地聚类方法

Conclusion: VertCoHiRF通过结构共识方法实现了隐私保护的垂直联邦聚类，生成了可解释的共享聚类融合层次结构，在通信复杂度和鲁棒性方面表现良好

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [863] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文提出了一种基于信任的激励机制，用于评估和奖励联邦学习系统中参与者的贡献质量，通过动态信任评分和区块链技术确保系统完整性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在恶意或故障节点可能降低模型性能的问题，需要确保系统完整性和可靠性。现有系统缺乏有效的机制来评估参与者贡献质量并激励诚实参与。

Method: 提出基于信任的激励机制，动态评估信任分数（考虑数据质量、模型准确性、一致性和贡献频率等因素），并利用区块链和智能合约自动化信任评估和激励分配过程。

Result: 理论框架旨在创建更鲁棒、公平和透明的联邦学习生态系统，减少不可信参与者带来的风险，通过激励机制鼓励诚实参与并惩罚不可靠行为。

Conclusion: 基于信任的激励机制结合区块链技术能够有效提升联邦学习系统的完整性和可靠性，为构建可信的分布式机器学习环境提供解决方案。

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [864] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出基于潜力的非近视在线匹配策略优化框架，应用于心脏移植分配，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决心脏移植等待时间过长问题，当前分配策略未能考虑器官动态到达和等待者组成，需要更灵活的数据驱动模型

Method: 基于潜力的非近视策略优化框架，通过自监督模仿学习训练高维表达能力更强的潜力函数，模仿具有完美预见性的全知算法

Result: 使用真实历史数据验证，显著优于现有方法（包括美国现状政策和连续分布框架），优化群体层面结果

Conclusion: 为美国心脏移植分配系统改革提供可扩展、理论基础的路径，时机关键

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [865] [Information Geometry of Absorbing Markov-Chain and Discriminative Random Walks](https://arxiv.org/abs/2602.08185)
*Masanari Kimura*

Main category: stat.ML

Relevance: 30.0

TL;DR: 本文从信息几何角度重新审视判别随机游走（DRW），将其视为吸收马尔可夫链上类别特定命中时间分布的统计流形，推导了闭合形式的概率质量函数、矩层次和观测Fisher信息，并提出了基于几何的节点敏感度评分。


<details>
  <summary>Details</summary>
Motivation: 判别随机游走（DRW）是半监督节点分类的简单有效工具，但其理论基础尚不完整。作者希望通过信息几何框架为DRW提供更坚实的理论支撑，并开发基于几何原理的实用策略。

Method: 1. 将类别特定命中时间分布族视为统计流形；2. 从对数线性边权重模型出发，推导命中时间概率质量函数、矩层次和观测Fisher信息的闭合形式；3. 利用Fisher矩阵的秩一特性构建低维平坦流形；4. 提出基于几何的节点敏感度评分，用于量化DRW介数对参数扰动的敏感性。

Result: 1. 获得了DRW模型的完整解析表达式；2. 发现每个种子节点的Fisher矩阵都是秩一的，通过商空间得到低维平坦流形；3. 提出的敏感度评分能够界定（在一维情况下达到）DRW介数在单位Fisher扰动下的最大一阶变化；4. 该评分可用于主动标签获取、边重加权和解释等任务。

Conclusion: 信息几何为DRW提供了坚实的理论基础，推导的解析结果和敏感度评分不仅深化了对DRW的理解，还为半监督图学习中的实际应用（如主动学习、模型解释）提供了原则性方法。

Abstract: Discriminative Random Walks (DRWs) are a simple yet powerful tool for semi-supervised node classification, but their theoretical foundations remain fragmentary. We revisit DRWs through the lens of information geometry, treating the family of class-specific hitting-time laws on an absorbing Markov chain as a statistical manifold. Starting from a log-linear edge-weight model, we derive closed-form expressions for the hitting-time probability mass function, its full moment hierarchy, and the observed Fisher information. The Fisher matrix of each seed node turns out to be rank-one, taking the quotient by its null space yields a low-dimensional, globally flat manifold that captures all identifiable directions of the model. Leveraging the geometry, we introduce a sensitivity score for unlabeled nodes that bounds, and in one-dimensional cases attains, the maximal first-order change in DRW betweenness under unit Fisher perturbations. The score can lead to principled strategies for active label acquisition, edge re-weighting, and explanation.

</details>


### [866] [Attention-Driven Framework for Non-Rigid Medical Image Registration](https://arxiv.org/abs/2602.07088)
*Muhammad Zafar Iqbal,Ghazanfar Farooq Siddiqui,Anwar Ul Haq,Imran Razzak*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出AD-RegNet，一种基于注意力机制的非刚性医学图像配准框架，结合3D UNet和双向交叉注意力，在多个尺度上建立移动图像与固定图像之间的对应关系，并在胸部和脑部数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医学图像配准是医学图像分析的基础任务，尽管深度学习方法取得了显著进展，但在大变形情况下准确对齐图像同时保持解剖学合理性仍然具有挑战性。现有方法在处理大变形时可能无法保持解剖学一致性，需要更有效的机制来指导配准过程。

Method: 提出AD-RegNet框架，采用3D UNet作为主干网络，结合双向交叉注意力机制在多个尺度上建立图像对应关系。引入区域自适应注意力机制聚焦解剖相关结构，采用多分辨率变形场合成方法进行精确对齐。使用NCC、MSE、SSIM、Jacobian行列式和TRE等指标进行综合评估。

Result: 在DIRLab（胸部4D CT）和IXI（脑部MRI）两个数据集上评估，性能与最先进方法相当。在保持计算效率的同时实现了准确的配准，确保了解剖学合理的变形。

Conclusion: 注意力引导的配准方法能够提高对齐精度并确保解剖学合理的变形，在临床应用中具有潜力。该方法在配准精度和计算效率之间取得了良好平衡。

Abstract: Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.

</details>


### [867] [Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling](https://arxiv.org/abs/2602.07192)
*Xiaolong He,Haoyan Wei,Wei Hu,Henan Mao,C. T. Wu*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文对深度材料网络（DMN）进行了全面的比较评估，分析了其在预测精度、计算效率和训练鲁棒性方面的表现，特别关注了离线训练选择对在线泛化性能的影响。


<details>
  <summary>Details</summary>
Motivation: 深度材料网络（DMN）作为结构保持的机理机器学习模型，在复杂微观结构的多尺度建模中具有加速潜力，但缺乏对其离线-在线全流程性能的系统评估。本文旨在填补这一空白，为实际部署提供指导。

Method: 通过系统比较评估DMN的预测精度、计算效率和训练鲁棒性，研究离线训练选择（包括初始化、批大小、训练数据量、激活正则化）对在线泛化性能和不确定性的影响，并与旋转无关的交互式材料网络（IMN）进行对比。

Result: 预测误差和方差随训练数据量增加而减小；初始化和批大小显著影响模型性能；激活正则化对控制网络复杂度和泛化性能至关重要；IMN相比原始DMN实现了3.4-4.7倍的离线训练加速，同时保持相当的在线预测精度和计算效率。

Conclusion: 研究阐明了结构保持材料网络中模型表达能力与效率之间的关键权衡，为多尺度材料建模中的实际部署提供了实用指导，特别强调了训练策略对泛化性能的重要性。

Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.

</details>


### [868] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

Relevance: 25.0

TL;DR: 论文提出AdaPort算法，用于VR/AR边缘渲染中的视口选择问题，通过结合全信息反馈和赌博机反馈的混合反馈模型，实现比现有方法更优的性能。


<details>
  <summary>Details</summary>
Motivation: VR/AR应用对帧率、延迟和物理虚拟环境同步有严格要求。边缘服务器需要渲染全景内容、预测用户头部运动，并在无线带宽限制内传输足够覆盖用户视口的场景部分。现有方法将问题建模为具有两级赌博机反馈的多臂赌博机，但未能利用预测反馈可在用户头部姿态观测后为所有候选部分计算的事实。

Method: 提出两级混合反馈模型，结合全信息反馈和赌博机反馈，将视口选择问题形式化为该设置下的在线学习任务。提出AdaPort混合学习算法，利用两种反馈类型提高学习效率。

Result: 推导了混合反馈模型的实例依赖遗憾下界，建立了与下界渐近匹配的实例依赖遗憾上界。通过真实世界轨迹驱动的仿真表明，AdaPort始终优于最先进的基线方法。

Conclusion: 通过识别预测反馈的全信息性质，提出混合反馈模型和AdaPort算法，显著提高了VR/AR边缘渲染中视口选择的性能。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [869] [Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation](https://arxiv.org/abs/2602.07278)
*Sai Vamsi Alisetti*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出Laplacian-LoRA方法，通过低秩谱适应延迟GCN中的过平滑现象，将有效深度提升至多两倍


<details>
  <summary>Details</summary>
Motivation: 过平滑是深度图卷积网络的基本限制，导致节点表示随深度增加而崩溃。现有方法多通过架构修改或残差机制缓解，但过平滑的谱原因常被隐含处理。

Method: 提出Laplacian-LoRA方法，对标准GCN进行简单可解释的低秩谱适应。不重新设计消息传递，而是引入可学习的谱锚定校正到固定的拉普拉斯传播算子，选择性减弱收缩同时保持稳定性和低通归纳偏置。

Result: 在多个基准数据集和深度上，Laplacian-LoRA一致延迟过平滑发生，将GCN的有效深度提升至多两倍。嵌入方差诊断确认这些增益来自延迟表示崩溃，学习谱分析显示校正是平滑、有界且行为良好的。

Conclusion: 过平滑是深度依赖的谱现象，可以通过对图传播算子进行适度的低秩适应来系统性地延迟。

Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.

</details>


### [870] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了用于学习决策列表和大间隔半空间的差分隐私算法，在PAC和在线模型中改进了现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在敏感数据上的应用增加，差分隐私成为保护训练数据隐私的重要技术。然而，现有的差分隐私算法在经典学习任务（如决策列表和半空间学习）上通常存在显著的样本复杂度开销。本文旨在开发更高效的差分隐私算法，减少与非隐私算法之间的性能差距。

Method: 1. 在PAC模型中：开发了计算高效的差分隐私算法用于学习决策列表，最小化相对于最佳非隐私算法的样本开销
2. 在在线模型中：提出了Winnow算法的差分隐私版本，用于学习大间隔半空间，错误边界在维度上为多对数，在间隔上为逆多项式
3. 应用：展示了如何在在线模型中差分隐私地学习决策列表

Result: 1. PAC模型：实现了学习决策列表的差分隐私算法，样本开销接近最优
2. 在线模型：获得了维度多对数、间隔逆多项式的错误边界，与最先进的非隐私保证在质量上匹配
3. 为经典学习问题提供了更高效的隐私保护解决方案

Conclusion: 该研究在差分隐私机器学习领域取得了重要进展，为经典学习问题提供了更高效的隐私保护算法，缩小了隐私算法与非隐私算法之间的性能差距，特别是在决策列表和半空间学习等基础问题上。

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [871] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文解决了对抗性多臂老虎机中同时实现最优静态遗憾和动态遗憾的问题，证明了在确定性损失和遗忘性对手情况下可以同时达到最优，但在适应性对手情况下不可能。


<details>
  <summary>Details</summary>
Motivation: 在对抗性多臂老虎机中，静态遗憾和动态遗憾是两种常用性能指标。虽然已有算法分别达到各自的最优界，但尚无算法能同时达到两种遗憾的最优界。Marinov和Zimmert[2021]证明了在适应性对手情况下不可能同时最优，本文旨在探索在遗忘性对手情况下实现同时最优的可能性。

Method: 1. 将Marinov和Zimmert[2021]的不可能性结果扩展到确定性损失情况；2. 提出新算法，利用负静态遗憾补偿动态遗憾控制中的探索开销，并采用Blackwell可接近性联合控制两种遗憾；3. 开发新的老虎机模型选择方法。

Result: 1. 证明了在确定性损失情况下，对抗适应性对手时同时达到最优静态和动态遗憾是不可能的；2. 提出了在遗忘性对手和确定性损失情况下同时达到最优静态和动态遗憾的算法；3. 揭示了在同时考虑多个遗憾基准时，适应性对手和遗忘性对手之间的根本分离。

Conclusion: 该研究首次展示了在遗忘性对手和确定性损失情况下同时达到最优静态和动态遗憾的可能性，为同时实现不同切换次数基准的最优遗憾这一长期开放问题提供了新见解。算法中的模型选择方法可能具有独立价值。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [872] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文提出了多臂老虎机算法的一个新性能指标——分配变异性，并建立了分配变异性与经典遗憾指标之间的基本权衡关系，证明了任何算法必须满足 $R_T \cdot S_T=Ω(T^{3/2})$ 的下界。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机算法中，不同臂的拉动次数分配存在巨大变异，这在现代应用如学习增强平台运营和后老虎机统计推断中特别有害。因此需要引入新的性能指标来量化这种分配变异性。

Method: 提出分配变异性作为新性能指标，定义为各臂拉动次数标准差的最大值。建立了分配变异性与遗憾之间的基本权衡理论，证明了任何算法必须满足 $R_T \cdot S_T=Ω(T^{3/2})$ 的下界。提出了可调算法 UCB-f 作为 UCB1 的推广，能够实现帕累托前沿上的任意点。

Result: 证明了任何算法在 $R_T=o(T)$ 条件下必须满足 $R_T \cdot S_T=Ω(T^{3/2})$ 的下界。这意味着任何极小极大遗憾最优算法必然具有 $Θ(T)$ 的最坏情况分配变异性；而任何具有次线性最坏情况遗憾的算法必然具有 $S_T= ω(\sqrt{T})$ 的分配变异性。提出的 UCB-f 算法能够实现帕累托前沿 $R_T \cdot S_T=\tildeΘ(T^{3/2})$ 上的任意点。

Conclusion: 多臂老虎机算法在分配变异性与遗憾之间存在基本权衡，这一发现对平台运营和统计推断应用有重要启示。同时解决了 Praharaj 和 Khamaru (2025) 的一个开放性问题。

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [873] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了对基于扩散的零样本逆问题求解器的理论分析，在假设先验为高斯分布的情况下，推导出理想后验采样器和扩散重建算法的闭式解，并提出了参数设计的理论框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的零样本逆问题求解方法主要依赖手动调参和启发式策略，缺乏理论分析。作者希望为这类方法提供严格的理论基础，并建立参数设计的系统化框架。

Method: 在假设先验为高斯分布的条件下，推导出理想后验采样器和扩散重建算法的谱域闭式表达式。基于这些表示，提出了一个与具体方法无关的参数设计框架，综合考虑先验特性、退化信号和扩散动力学。

Result: 理论分析表明，提出的谱域参数建议与标准启发式方法在结构上不同，且随扩散步长变化，能够在感知质量和信号保真度之间实现一致的平衡。

Conclusion: 该工作为基于扩散的逆问题求解提供了理论分析工具和参数设计原则，取代了现有的启发式选择策略，为这类方法建立了更坚实的理论基础。

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [874] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一种基于最优传输和非线性优化的新框架，从有限样本中近似未知概率分布，并通过求解混合整数线性规划来高效计算Wasserstein距离的置信上界，开发智能聚类算法寻找最优支撑集。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离已成为量化概率分布间距离的关键度量，在机器学习、控制理论等多个领域有重要应用。从有限样本中学习未知分布并给出易于计算的非渐近误差界是许多领域的基本问题。

Method: 结合最优传输、非线性优化和集中不等式，提出理论算法框架。通过求解仅依赖于近似分布支撑集大小的混合整数线性规划，高效计算Wasserstein距离的置信上界。开发智能聚类算法优化寻找近似分布的支撑集。

Result: 在基准测试中，该方法优于现有可比方法，通常返回具有更小支撑集和更紧误差界的近似分布。

Conclusion: 该框架为从有限样本中近似未知分布提供了有效的理论算法解决方案，能够高效计算Wasserstein距离的置信上界，并通过智能聚类优化近似分布的质量。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [875] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文提出一种基于场景的方法，用于分析增强机器学习的反无人机系统，识别概率黑客攻击挑战，并提出加强系统可信度的法律机制要求。


<details>
  <summary>Details</summary>
Motivation: 反无人机系统需要应对各种无人机威胁，通过人工智能等新兴技术增强反无人机系统可以带来更有效的对抗措施。本文旨在解决机器学习增强的反无人机系统中可能出现的概率黑客攻击问题，确保系统可信度。

Method: 采用基于场景的方法分析机器学习增强的反无人机系统，识别概率黑客攻击作为关键挑战，并提出可在现有法治机制中实施的具体要求来预防此类攻击。

Result: 通过场景分析方法识别了概率黑客攻击的挑战，并提出了一系列可实施的法律机制要求，这些要求能够增强反无人机系统的可信度，为军民领域的人机协作建立合理信任基础。

Conclusion: 基于场景的方法能够有效分析机器学习增强的反无人机系统，识别概率黑客攻击等挑战，提出的法律机制要求有助于增强系统可信度，促进人机协作的成功实施。

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [876] [Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso](https://arxiv.org/abs/2602.08197)
*Shingo Higashiguchi,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出Kronecker时间变化图形套索(KTVGL)方法，用于建模张量时间序列，通过Kronecker积形式估计模态特定的动态网络，避免复杂纠缠结构，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着网络服务发展，金融、医疗、在线平台等领域产生大量时间序列数据。这些数据通常包含多个相互作用的变量，估计变量间的时间变化依赖关系（动态网络结构）对准确建模至关重要。但现实数据常表示为多模态张量时间序列，导致复杂纠缠网络难以解释且计算量大。

Method: 提出Kronecker时间变化图形套索(KTVGL)方法，通过Kronecker积形式估计模态特定的动态网络，避免复杂纠缠结构。该方法可扩展到流算法，使计算时间与序列长度无关。

Result: 在合成数据实验中，该方法比现有方法获得更高的边估计精度，同时需要更少的计算时间。通过真实世界数据的案例研究进一步证明了其实用价值。

Conclusion: KTVGL方法能有效建模张量时间序列，提供可解释的动态网络结构估计，计算效率高，适用于大规模实时应用。

Abstract: With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.

</details>


### [877] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出随机在线算法，在单位区间上针对自适应选择的Lipschitz凸损失函数，实现近乎最优的$\widetilde O(\sqrt T)$期望交换遗憾，改进先前$\widetilde O(T^{2/3})$界限，算法运行时间为$\mathsf{poly}(T)$。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中的交换遗憾最小化问题，特别是在单位区间上针对自适应选择的Lipschitz凸损失函数。先前最佳界限为$\widetilde O(T^{2/3})$，存在改进空间。同时，该研究还旨在为一般可引出属性的校准误差最小化提供高效在线算法。

Method: 提出多尺度分箱技术：将单位区间离散化为多个粒度尺度的箱，同时使用所有尺度进行随机预测。该算法是随机在线算法，运行时间为多项式时间。

Result: 1. 实现了$\widetilde O(\sqrt T)$期望交换遗憾，显著改进先前$\widetilde O(T^{2/3})$界限
2. 算法运行时间为$\mathsf{poly}(T)$，具有高效性
3. 直接推论：为一般可引出属性的校准误差最小化提供高效在线算法
4. 无需先前工作中所需的识别函数Lipschitz假设，首次为中位数校准实现$\widetilde O(\sqrt T)$校准误差保证

Conclusion: 该研究提出了一个高效随机在线算法，在单位区间上针对自适应选择的Lipschitz凸损失函数实现了近乎最优的交换遗憾界限。多尺度分箱技术是关键创新，不仅解决了交换遗憾问题，还为校准误差最小化提供了更通用的解决方案，特别是扩展到了中位数校准等先前无法处理的情况。

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [878] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

Relevance: 25.0

TL;DR: GEMSS是一个变分贝叶斯框架，用于在欠定（n≪p）和高相关数据中发现多个不同的稀疏特征子集，通过结构化先验和多样性惩罚同时优化多个解。


<details>
  <summary>Details</summary>
Motivation: 在欠定和高相关数据中，多个不同的稀疏特征子集可能同样好地解释响应。传统方法通常只找到一个解，掩盖了可能的解释范围，而识别这些替代方案对于理解底层机制至关重要。

Method: GEMSS使用结构化spike-and-slab先验实现稀疏性，高斯混合近似难处理的多峰后验，Jaccard惩罚控制解多样性，通过随机梯度下降在单个目标函数中同时优化整个解集合。

Result: 在128个合成实验的基准测试中，GEMSS能扩展到高维设置（p=5000，n=50），无缝泛化到连续目标，原生处理缺失数据，对类别不平衡和高斯噪声表现出显著鲁棒性。

Conclusion: GEMSS提供了一个有效的框架来发现多个稀疏特征组合，特别适用于物理测量等需要领域洞察的应用，已作为Python包提供。

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [879] [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](https://arxiv.org/abs/2602.08986)
*Isaac Xu,Martin Gillis,Ayushi Sharma,Benjamin Misiuk,Craig J. Brown,Thomas Trappenberg*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出加权损失函数解决层次多标签分类中深层节点预测困难问题，结合节点不平衡权重和焦点权重组件，在基准数据集上显著提升召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 层次多标签分类中，模型难以预测更深层次的节点，部分原因是某些类别（或层次节点）天然稀有，且子节点几乎总是比父节点更少见。需要解决这种层次不平衡问题。

Method: 提出加权损失目标函数，结合节点不平衡权重和焦点权重组件。前者强调稀有节点而非稀有观测数据点，后者利用集成不确定性量化，关注每个模型输出分布中的不确定节点。

Result: 在基准数据集上观察到召回率提升高达5倍，F1分数也有统计显著提升。该方法还能帮助卷积网络在具有次优编码器或有限数据的挑战性任务中表现更好。

Conclusion: 通过强调稀有节点和关注不确定节点，提出的加权损失方法能有效改善层次多标签分类中深层节点的预测性能，特别是在数据不平衡和挑战性场景下。

Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.

</details>


### [880] [Learned Finite Element-based Regularization of the Inverse Problem in Electrocardiographic Imaging](https://arxiv.org/abs/2602.07466)
*Manuel Haas,Thomas Grandits,Thomas Pinetz,Thomas Beiert,Simone Pezzuto,Alexander Effland*

Main category: math.NA

Relevance: 25.0

TL;DR: 提出了一种用于心电图成像的时空正则化框架，结合空间正则化和学习的时间Fields-of-Experts先验，以捕捉复杂的心电时空激活模式。


<details>
  <summary>Details</summary>
Motivation: 心电图成像（ECGI）中的逆问题严重不适定，需要稳健的正则化。传统方法主要采用空间平滑，但心脏动力学的时序结构在生理上相关却未得到充分利用。

Method: 引入时空正则化框架，将空间正则化与学习的时间Fields-of-Experts先验耦合；推导非结构化心脏表面网格上的有限元离散化，证明Mosco收敛，并开发可处理FoE项的可扩展优化算法。

Result: 在合成心外膜数据上的数值实验表明，相比手工制作的时空方法，该方法在去噪和逆重建方面有改进，产生对噪声稳健且生理上合理的解。

Conclusion: 提出的时空正则化框架能有效利用心脏电活动的时空结构，改善心电图成像的逆问题求解质量。

Abstract: Electrocardiographic imaging (ECGI) seeks to reconstruct cardiac electrical activity from body-surface potentials noninvasively. However, the associated inverse problem is severely ill-posed and requires robust regularization. While classical approaches primarily employ spatial smoothing, the temporal structure of cardiac dynamics remains underexploited despite its physiological relevance. We introduce a space-time regularization framework that couples spatial regularization with a learned temporal Fields-of-Experts (FoE) prior to capture complex spatiotemporal activation patterns. We derive a finite element discretization on unstructured cardiac surface meshes, prove Mosco-convergence, and develop a scalable optimization algorithm capable of handling the FoE term. Numerical experiments on synthetic epicardial data demonstrate improved denoising and inverse reconstructions compared to handcrafted spatiotemporal methods, yielding solutions that are both robust to noise and physiologically plausible.

</details>


### [881] [On Generation in Metric Spaces](https://arxiv.org/abs/2602.07710)
*Jiaxun Li,Vinod Raman,Ambuj Tewari*

Main category: stat.ML

Relevance: 25.0

TL;DR: 论文研究了可分离度量实例空间中的生成问题，扩展了Kleinberg和Mullainathan[2024]的语言生成框架，通过度量分离定义新颖性，并允许对抗方和生成方使用不对称的新颖性参数。


<details>
  <summary>Details</summary>
Motivation: 将语言生成框架从可数域扩展到更一般的度量空间，研究生成能力在不同度量空间中的几何特性，特别是新颖性参数变化时生成能力的稳定性问题。

Method: 引入$(\varepsilon,\varepsilon')$-闭包维度作为尺度敏感的闭包维度类比，通过度量分离定义新颖性，允许不对称的新颖性参数，分析加倍空间和一般度量空间中的生成特性。

Result: 在加倍空间（包括所有有限维赋范空间）中，生成能力在不同新颖性尺度下稳定且对等价度量不变；但在一般度量空间中，生成能力高度尺度敏感且依赖于具体度量，甚至在无限维希尔伯特空间$\ell^2$中，所有生成概念都可能随新颖性参数变化而突然失效。

Conclusion: 生成能力在度量空间中的表现存在显著的几何对比：在加倍空间中具有稳定性，而在一般度量空间中则表现出高度敏感性和度量依赖性。

Abstract: We study generation in separable metric instance spaces. We extend the language generation framework from Kleinberg and Mullainathan [2024] beyond countable domains by defining novelty through metric separation and allowing asymmetric novelty parameters for the adversary and the generator. We introduce the $(\varepsilon,\varepsilon')$-closure dimension, a scale-sensitive analogue of closure dimension, which yields characterizations of uniform and non-uniform generatability and a sufficient condition for generation in the limit. Along the way, we identify a sharp geometric contrast. Namely, in doubling spaces, including all finite-dimensional normed spaces, generatability is stable across novelty scales and invariant under equivalent metrics. In general metric spaces, however, generatability can be highly scale-sensitive and metric-dependent; even in the natural infinite-dimensional Hilbert space $\ell^2$, all notions of generation may fail abruptly as the novelty parameters vary.

</details>


### [882] [Learning to Alleviate Familiarity Bias in Video Recommendation](https://arxiv.org/abs/2602.07987)
*Zheng Ren,Yi Wu,Jianan Lu,Acar Ary,Yiqu Liu,Li Wei,Lukasz Heldt*

Main category: cs.IR

Relevance: 25.0

TL;DR: LAFB是一个轻量级、模型无关的框架，用于缓解推荐系统中的熟悉度偏差，通过建模用户-内容熟悉度并调整预测分数来提升内容多样性。


<details>
  <summary>Details</summary>
Motivation: 现代视频推荐系统面临结构性曝光不平衡问题，主要由行为偏差（特别是熟悉度偏差）导致，这限制了新兴创作者和多样化内容的曝光机会。

Method: LAFB在重排序阶段工作，使用离散和连续交互特征建模用户-内容熟悉度，估计个性化去偏因子来调整用户评分预测分数，从而减少熟悉内容在最终排名中的主导地位。

Result: 大规模离线评估和在线A/B测试显示，LAFB增加了新颖观看时间份额，改善了新兴创作者的曝光和整体内容多样性，同时保持了稳定的总体观看时间和短期满意度。

Conclusion: LAFB已成功部署在YouTube推荐系统的重排序阶段，证明了其在缓解熟悉度偏差、提升内容多样性方面的实际有效性。

Abstract: Modern video recommendation systems aim to optimize user engagement and platform objectives, yet often face structural exposure imbalances caused by behavioral biases. In this work, we focus on the post-ranking stage and present LAFB (Learning to Alleviate Familiarity Bias), a lightweight and model-agnostic framework designed to mitigate familiarity bias in recommendation outputs. LAFB models user-content familiarity using discrete and continuous interaction features, and estimates personalized debiasing factors to adjust user rating prediction scores, thereby reducing the dominance of familiar content in the final ranking. We conduct large-scale offline evaluations and online A/B testing in a real-world recommendation system, under a unified serving stack that also compares LAFB with deployable popularity-oriented remedies. Results show that LAFB increases novel watch-time share and improves exposure for emerging creators and overall content diversity, while maintaining stable overall watch time and short-term satisfaction. LAFB has already been launched in the post-ranking stage of YouTube's recommendation system, demonstrating its effectiveness in real-world applications.

</details>


### [883] [Fundamental Limits of Community Detection in Contextual Multi-Layer Stochastic Block Models](https://arxiv.org/abs/2602.08173)
*Shuyang Gong,Dong Huang,Zhangsong Li*

Main category: math.ST

Relevance: 25.0

TL;DR: 该论文研究了从高维协变量矩阵和稀疏网络的联合观测中进行社区检测的问题，在常数平均度网络和特征数量与样本数成比例增长的渐近机制下，推导了检测和估计主体标签的尖锐阈值。


<details>
  <summary>Details</summary>
Motivation: 社区检测是网络分析中的核心问题，现有研究通常只考虑网络结构信息。本文动机在于探索如何同时利用高维协变量矩阵和多个稀疏网络的信息进行更准确的社区检测，特别是在网络平均度保持常数、特征维度与样本数成比例增长的挑战性场景下。

Method: 1. 理论分析：使用伯努利和高斯矩的新型比较不等式，以及受DHSS25启发的"恢复到卡方散度减少"的统计变体，推导信息论下界
2. 算法设计：基于计数装饰循环和装饰路径的高效算法，证明这些算法能够达到检测和弱恢复的尖锐阈值

Result: 1. 在常数平均度网络和特征数量与样本数成比例增长的渐近机制下，推导了检测和估计主体标签的尖锐阈值
2. 将MN23的工作扩展到常数度机制和噪声测量场景
3. 解决了YLS24+中关于网络数量为常数时的猜想
4. 证明在该设置下不存在统计-计算差距

Conclusion: 本文为从高维协变量和稀疏网络的联合观测中进行社区检测提供了完整的理论框架，确定了信息论极限并设计了达到该极限的高效算法，证明了在该特定设置下不存在统计-计算差距。

Abstract: We consider the problem of community detection from the joint observation of a high-dimensional covariate matrix and $L$ sparse networks, all encoding noisy, partial information about the latent community labels of $n$ subjects. In the asymptotic regime where the networks have constant average degree and the number of features $p$ grows proportionally with $n$, we derive a sharp threshold under which detecting and estimating the subject labels is possible. Our results extend the work of \cite{MN23} to the constant-degree regime with noisy measurements, and also resolve a conjecture in \cite{YLS24+} when the number of networks is a constant.
  Our information-theoretic lower bound is obtained via a novel comparison inequality between Bernoulli and Gaussian moments, as well as a statistical variant of the ``recovery to chi-square divergence reduction'' argument inspired by \cite{DHSS25}. On the algorithmic side, we design efficient algorithms based on counting decorated cycles and decorated paths and prove that they achieve the sharp threshold for both detection and weak recovery. In particular, our results show that there is no statistical-computational gap in this setting.

</details>


### [884] [Schrödinger bridge problem via empirical risk minimization](https://arxiv.org/abs/2602.08374)
*Denis Belomestny,Alexey Naumov,Nikita Puchkin,Denis Suchkov*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出一种基于学习理论的薛定谔桥问题求解方法，通过经验风险最小化学习单一正变换势函数，替代传统的Sinkhorn迭代和核平滑方法


<details>
  <summary>Details</summary>
Motivation: 传统薛定谔桥计算方法依赖Sinkhorn迭代和经验测度，需要核平滑处理对偶解来构建时变漂移项。本文提出更直接的学习理论方法，通过重写薛定谔系统为单一正变换势函数的非线性不动点方程，避免传统方法的复杂性

Method: 将薛定谔系统重写为满足非线性不动点方程的单一正变换势函数，通过经验风险最小化在函数类上估计该势函数。在参考核和终端密度的次高斯假设下，建立经验风险相对于总体风险的一致收敛性。将学习到的势函数代入桥的随机控制表示中生成样本

Result: 建立了经验风险的一致收敛理论保证，并通过数值实验展示了所提方法的性能。该方法为端点分布仅通过样本可用时的薛定谔桥问题提供了新的计算框架

Conclusion: 提出了一种基于学习理论的薛定谔桥问题求解新范式，通过直接学习势函数避免了传统方法的复杂性，为仅通过样本可用的分布之间的最优传输问题提供了有效的计算方法

Abstract: We study the Schrödinger bridge problem when the endpoint distributions are available only through samples. Classical computational approaches estimate Schrödinger potentials via Sinkhorn iterations on empirical measures and then construct a time-inhomogeneous drift by differentiating a kernel-smoothed dual solution. In contrast, we propose a learning-theoretic route: we rewrite the Schrödinger system in terms of a single positive transformed potential that satisfies a nonlinear fixed-point equation and estimate this potential by empirical risk minimization over a function class. We establish uniform concentration of the empirical risk around its population counterpart under sub-Gaussian assumptions on the reference kernel and terminal density. We plug the learned potential into a stochastic control representation of the bridge to generate samples. We illustrate performance of the suggested approach with numerical experiments.

</details>


### [885] [Empirical Study of Observable Sets in Multiclass Quantum Classification](https://arxiv.org/abs/2602.08485)
*Paul San Sebastian,Mikel Cañizo,Roman Orus*

Main category: quant-ph

Relevance: 25.0

TL;DR: 该论文研究了多类量子机器学习中的两种主要分类标准：最大化代表类别的可观测量期望值 vs 最大化编码量子态与代表类别的参考态之间的保真度，分析了不同可观测量集合选择对量子机器学习模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 目前大多数使用参数化量子电路进行分类的工作局限于二元分类，或者通过二元分类器集成实现多类分类。少数提出原生多类模型的工作没有充分论证用于分类的可观测量的选择依据。因此需要系统研究多类量子机器学习中的分类标准问题。

Method: 研究两种主要分类标准：1) 最大化代表类别的可观测量期望值；2) 最大化编码量子态与代表类别的参考态之间的保真度。选择Pauli字符串集合和计算基投影子集合作为可观测量，在量子机器学习模型中比较两种方法，分析不同可观测量集合选择对模型性能的影响，特别是在Barren Plateaus和Neural Collapse背景下的表现。

Result: 通过观察每种模型类型的经验行为，分析了不同可观测量集合选择对量子机器学习模型性能的影响，特别是在Barren Plateaus和Neural Collapse现象中的表现。结果提供了关于如何设计未来多类量子机器学习模型的见解。

Conclusion: 该研究为多类量子机器学习模型的设计提供了指导性见解，有助于理解不同分类标准和可观测量选择对模型性能的影响，特别是在量子机器学习特有的挑战如Barren Plateaus和Neural Collapse背景下的表现。

Abstract: Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest). Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class. To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse. The results provide insights that may guide the design of future multiclass quantum machine learning models.

</details>


### [886] [Empirically Understanding the Value of Prediction in Allocation](https://arxiv.org/abs/2602.08786)
*Unai Fischer-Abaigar,Emily Aiken,Christoph Kern,Juan Carlos Perdomo*

Main category: cs.CY

Relevance: 25.0

TL;DR: 开发了一个实证工具包，帮助规划者量化预测投资与其他政策杠杆（如扩大容量和改进治疗质量）相比的福利影响，并在德国就业服务和埃塞俄比亚贫困目标定位两个案例研究中应用。


<details>
  <summary>Details</summary>
Motivation: 机构越来越多地使用预测来分配稀缺资源。从设计角度看，更好的预测与其他投资（如扩大容量或改进治疗质量）存在竞争。核心问题不是如何解决特定的分配问题，而是应该解决哪个问题。

Method: 开发了一个实证工具包（rvp软件工具包），帮助规划者形成原则性答案，量化预测投资与其他政策杠杆相比的福利影响。在德国就业服务和埃塞俄比亚贫困目标定位两个真实案例研究中应用该框架。

Result: 决策者可以可靠地得出关于预测在其分配问题中相对价值的情境特定结论。提供了软件工具包和部分数据以支持该领域的未来实证工作。

Conclusion: 该研究提供了一个实用的框架，帮助决策者在资源分配问题中权衡预测投资与其他政策杠杆的价值，为实证政策分析提供了方法论工具。

Abstract: Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.

</details>


### [887] [AMS-HD: Hyperdimensional Computing for Real-Time and Energy-Efficient Acute Mountain Sickness Detection](https://arxiv.org/abs/2602.08916)
*Abu Masum,Mehran Moghadam,M. Hassan Najafi,Bige Unluturk,Ulkuhan Guler,Sercan Aygun*

Main category: cs.SC

Relevance: 25.0

TL;DR: 论文提出AMS-HD系统，使用超维计算（HDC）从可穿戴设备生理信号中实时检测高原病，在保持与传统机器学习相当准确性的同时，实现硬件高效设计。


<details>
  <summary>Details</summary>
Motivation: 高原病是危及生命的疾病，及时检测至关重要。传统机器学习方法在生理信号分类中难以平衡预测性能和硬件需求，而超维计算（HDC）作为硬件高效的计算框架，在生物医学特征有限的情况下可能提供更好的替代方案。

Method: 提出AMS-HD系统，整合定制特征提取和Hadamard HV编码，增强HDC检测的精度和效率。利用超维计算的向量符号框架，实现轻量级计算和高效内存使用，从可穿戴设备收集的生理参数（心率、血氧饱和度、呼吸率、血压、体温）进行实时检测。

Result: HDC能够实现与传统机器学习模型相当的准确性，同时具备硬件高效特性，适合低功耗可穿戴系统。系统能够在可穿戴健康监测平台上实现连续、实时的急性高原病跟踪。

Conclusion: 超维计算为高原病检测提供了有前景的硬件高效解决方案，AMS-HD系统特别适合部署在可穿戴健康监测平台，实现连续、实时的急性高原病跟踪。

Abstract: Altitude sickness is a potentially life-threatening condition that impacts many individuals traveling to elevated altitudes. Timely detection is critical as symptoms can escalate rapidly. Early recognition enables simple interventions such as descent, oxygen, or medication, and prompt treatment can save lives by significantly lowering the risk of severe complications. Although conventional machine learning (ML) techniques have been applied to identify altitude sickness using physiological signals, such as heart rate, oxygen saturation, respiration rate, blood pressure, and body temperature, they often struggle to balance predictive performance with low hardware demands. In contrast, hyperdimensional computing (HDC) remains under-explored for this task with limited biomedical features, where it may offer a compelling alternative to existing classification models. Its vector symbolic framework is inherently suited to hardware-efficient design, making it a strong candidate for low-power systems like wearables. Leveraging lightweight computation and efficient streamlined memory usage, HDC enables real-time detection of altitude sickness from physiological parameters collected by wearable devices, achieving accuracy comparable to that of traditional ML models. We present AMS-HD, a novel system that integrates tailored feature extraction and Hadamard HV encoding to enhance both the precision and efficiency of HDC-based detection. This framework is well-positioned for deployment in wearable health monitoring platforms, enabling continuous, on-the-go tracking of acute altitude sickness.

</details>


### [888] [Differentiable Logical Programming for Quantum Circuit Discovery and Optimization](https://arxiv.org/abs/2602.08880)
*Antonin Sulc*

Main category: quant-ph

Relevance: 20.0

TL;DR: 提出一种神经符号框架，将量子电路设计重构为可微逻辑编程问题，通过连续开关优化满足逻辑公理，在量子傅里叶变换发现和硬件感知适应中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前量子电路设计依赖启发式固定结构或基于规则的编译器，这些方法可能次优且缺乏通用性。需要一种更系统、可优化的量子电路设计方法。

Method: 神经符号框架将量子电路设计视为可微逻辑编程问题。使用连续开关表示潜在量子门和参数化操作，通过梯度下降优化满足可微逻辑公理（正确性、简洁性、鲁棒性）。理论框架连接连续逻辑（T-范数）和幺正演化（测地线插值），通过偏置初始化缓解贫瘠高原问题。

Result: 成功从21个候选门中发现了4量子比特量子傅里叶变换电路。在IBM Torino 133量子比特处理器上的硬件感知适应实验中，在局部路由任务中将保真度提高了59.3个百分点，并能适应硬件故障。

Conclusion: 该神经符号框架为量子电路设计提供了一种系统、可优化的方法，能够发现高效电路并适应实际硬件约束，在量子计算领域具有重要应用价值。

Abstract: Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.

</details>


### [889] [Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing](https://arxiv.org/abs/2602.07258)
*Wanru Guo,Juan Xie,Binbin Wang,Weicong Chen,Xiaoyi Lu,Vipin Chaudhary,Curtis Tatsuoka*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出Dorfman筛选框架，通过层次聚类形成数据驱动的变量组，进行组内和组间假设检验，结合弹性网络进行特征选择，并开发了鲁棒变体处理污染和非正态数据。


<details>
  <summary>Details</summary>
Motivation: 高维基因组数据存在强烈的组相关结构，传统特征选择方法假设特征独立或依赖预定义通路，对异常值和模型误设敏感，需要更稳健的方法。

Method: 多阶段框架：1) 通过层次聚类形成数据驱动的变量组；2) 进行组和组内假设检验；3) 使用弹性网络或自适应弹性网络细化选择；4) 鲁棒变体包含OGK协方差估计、秩相关和Huber加权回归处理污染数据。

Result: 模拟中，Dorfman-Sparse-Adaptive-EN在正态条件下表现最佳，Robust-OGK-Dorfman-Adaptive-EN在数据污染条件下优势明显。应用于NSCLC基因表达数据时，鲁棒Dorfman方法获得最低预测误差并富集临床相关基因。

Conclusion: Dorfman框架为基因组特征选择提供高效稳健的方法，Robust-OGK-Dorfman-Adaptive-EN在理想和污染条件下均表现优异，适用于现代基因组生物标志物发现。

Abstract: Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.
  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.
  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.
  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.

</details>


### [890] [AI-Driven Predictive Modelling for Groundwater Salinization in Israel](https://arxiv.org/abs/2602.07478)
*Laxmi Pandey,Ariel Meroz,Ben Cheng,Ankita Manekar,Abhijit Mukherjee,Meirav Cohen,Adway Mitra*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该研究整合多源数据集，应用多种机器学习模型预测地下水盐度，结合特征选择、敏感性分析和可解释AI方法识别关键驱动因素，为以色列地下水盐化问题提供深入见解。


<details>
  <summary>Details</summary>
Motivation: 全球范围内地下水盐度和污染日益严重，需要全面理解盐化机制及其驱动因素。研究旨在识别气象、地质和人为因素对地下水盐度的影响，为水资源管理提供科学依据。

Method: 整合多源数据集，应用随机森林、XGBoost、神经网络、LSTM、CNN和线性回归等机器学习模型预测盐度。采用递归特征消除、全局敏感性分析和SHAP可解释AI方法识别关键驱动因素，并通过双重机器学习进行因果分析。

Result: 识别出关键气象因素（降水、温度）、地质因素（距河流距离、距盐体距离、地形湿度指数、海岸线距离）和人为因素（农田面积、处理废水）是以色列地下水盐度的重要驱动因素。可解释AI分析特别指出处理废水是关键的人为驱动因素。

Conclusion: 该方法提供了国家尺度上全球盐化机制的深入见解，减少了AI模型不确定性，并强调需要针对性的策略来应对盐度问题。处理废水在脆弱水文气候环境中尤为关键。

Abstract: Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.

</details>


### [891] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

Relevance: 15.0

TL;DR: PALMS是一个用于模拟巴甫洛夫条件反射实验的Python环境，整合了多种注意力学习模型，提供图形界面和高效计算能力。


<details>
  <summary>Details</summary>
Motivation: 模拟在理论发展和完善过程中不可或缺，帮助研究人员制定精确定义、生成模型并做出准确预测。需要专门的工具来模拟巴甫洛夫条件反射实验，整合多种学习模型。

Method: 开发了PALMS模拟器，包含经典Rescorla-Wagner模型和多种注意力学习方法（Pearce-Kaye-Hall、Mackintosh Extended、Le Pelley's Hybrid等），并提出了Rescorla-Wagner模型的新扩展，整合了Mackintosh和Pearce-Hall的对立概念。提供图形界面支持实验设计输入，能够模拟数百个刺激的实验，计算配置线索和配置线索复合物。

Result: PALMS能够高效运行，即时可视化结果，支持在单一架构和环境中快速精确比较各种模型的预测。图形显示可轻松保存，模拟数据可导出到电子表格。通过重现已发表的关联学习文献中的实验，展示了模拟器的功能和能力。

Conclusion: PALMS作为一个开源工具，为关联学习研究提供了强大的模拟环境，扩展了现有模型的预测能力，支持大规模实验模拟和模型比较。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [892] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一个结合物理模型和数据驱动方法的大型电子负载建模框架，通过时间对比学习进行模式一致性校准，解决了现有方法在电网仿真中的局限性。


<details>
  <summary>Details</summary>
Motivation: 数据中心快速增长使得大型电子负载建模对电力系统分析日益重要。现有物理模型未校准到设施级运行，而数据驱动方法容易过拟合且产生不现实的动态行为，需要结合两者优势。

Method: 设计框架结合物理模型结构和数据驱动适应性：1) 参数化物理模型结构以支持数据驱动模式一致性校准；2) 使用时间对比学习对齐时间和统计模式而非轨迹级对齐；3) 本地设施校准，仅共享校准参数保护数据隐私。

Result: 使用MIT Supercloud、ASU Sol、Blue Waters和ASHRAE数据集校准模型，集成到ANDES平台并在IEEE 39总线、NPCC 140总线和WECC 179总线系统评估。发现LEL间交互会根本改变扰动后恢复行为，产生复合断开-重连动态和延迟稳定。

Conclusion: 提出的混合建模框架通过模式一致性校准有效结合物理模型可解释性和数据驱动适应性，揭示了未校准负载模型无法捕捉的大型电子负载交互动态。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [893] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

Relevance: 15.0

TL;DR: 论文分析了线性集成采样在随机线性赌博机中的性能，证明当集成规模为m=Θ(d log n)时，ES能达到与Thompson采样基准相近的后悔界，同时保持计算复杂度相当。


<details>
  <summary>Details</summary>
Motivation: 研究线性集成采样在随机线性赌博机中的理论性能，填补其与Thompson采样基准之间的理论差距，同时保持计算效率。

Method: 采用标准高斯扰动的线性集成采样方法，通过将分析转化为m个独立布朗运动的时一致超越问题，使用连续时间视角进行理论分析。

Result: 当集成规模m=Θ(d log n)时，ES获得$\tilde O(d^{3/2}\sqrt n)$的高概率后悔界，与Thompson采样基准相匹配，计算复杂度保持相当。

Conclusion: 线性集成采样在随机线性赌博机中能够达到接近Thompson采样的理论性能，同时保持计算效率，连续时间视角为分析随机探索提供了新思路。

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [894] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出了一种人机协同方法，用于开发可解释的模糊规则基系统来预测正向渗透海水淡化生产力，通过专家驱动的网格划分、领域引导的特征工程和基于触发强度的规则剪枝，在保持语义可解释性的同时达到与聚类方法相当的预测性能。


<details>
  <summary>Details</summary>
Motivation: 在水处理领域，模糊规则基系统的可解释性至关重要，因为决策直接影响公共健康。虽然结构可解释性已通过多目标算法得到解决，但语义可解释性常因模糊集区分度低而受损。需要开发既能保持语义可解释性又能保证预测性能的方法。

Method: 1) 专家驱动的网格划分：创建具有高区分度的隶属函数；2) 领域引导的特征工程：减少特征冗余；3) 基于触发强度的规则剪枝：简化规则集。采用人机协同方法开发可解释的模糊规则基系统。

Result: 该方法在预测正向渗透海水淡化生产力方面，达到了与基于聚类的模糊规则基系统相当的预测性能，同时保持了语义可解释性并满足了结构复杂性约束。

Conclusion: 提出的人机协同方法为水处理应用提供了可解释的解决方案，在保持语义可解释性的同时实现了良好的预测性能，证明了专家知识与自动化方法结合的有效性。

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [895] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一种NormalHedge变体，对"简单"序列实现二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T是瞬时专家遗憾的累积二阶矩


<details>
  <summary>Details</summary>
Motivation: 研究"简单"序列的专家建议预测问题，旨在为这类序列设计具有更好遗憾界的算法。传统专家建议算法通常为最坏情况设计，而实际序列往往具有某种规律性，因此希望利用序列的"简单性"获得更优性能。

Method: 提出NormalHedge算法的变体，通过随机微分方程(SDE)的连续时间极限进行动机推导，在离散时间分析中使用自协调技术。算法基于自然分布对瞬时专家遗憾进行加权平均。

Result: 当V_T > log N时，算法获得二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T是瞬时专家遗憾的累积二阶矩。该界限在序列"简单"时优于传统遗憾界。

Conclusion: 对于"简单"序列，提出的NormalHedge变体能够实现优越的二阶遗憾界，证明利用序列结构可以显著改善预测性能。连续时间分析和自协调技术为算法设计提供了新视角。

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [896] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了GAMM框架，将缺失数据机制的分类扩展到属性图领域，系统地将缺失概率与节点属性和图结构联系起来，并发现现有插补方法在图感知缺失场景中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 属性图中的缺失数据问题比表格数据更具挑战性，现有缺失数据机制分类不能充分捕捉图特有的依赖关系，需要专门针对图数据的缺失机制框架。

Method: 提出GAMM框架，扩展传统缺失数据机制分类，引入图特定的依赖关系，将缺失概率与节点属性和底层图结构系统关联。

Result: 实证研究表明，最先进的插补方法在传统掩码上有效，但在这些更现实的图感知缺失场景中表现显著下降。

Conclusion: 需要开发专门针对图感知缺失机制的插补方法，GAMM框架为属性图中的缺失数据处理提供了理论基础。

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [897] [Radial Müntz-Szász Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities](https://arxiv.org/abs/2602.08419)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出Radial Müntz-Szász Networks (RMN)神经网络架构，专门用于建模径向奇异场（如1/r、log r、裂纹尖端剖面），通过可学习的径向幂次组合实现精确的奇异场表示。


<details>
  <summary>Details</summary>
Motivation: 径向奇异场（如1/r、log r、裂纹尖端剖面）难以用坐标可分离的神经网络架构建模。作者证明任何既是径向又是加性可分离的C^2函数必须是二次函数，这为坐标幂律模型建立了基本障碍。

Method: 引入Radial Müntz-Szász Networks (RMN)，将场表示为可学习径向幂次r^μ（包括负指数）的线性组合，加上极限稳定的log原语以实现精确的log r行为。RMN允许闭式空间梯度和拉普拉斯算子，支持在穿孔域上进行物理信息学习。还扩展了RMN-Angular（角度依赖）和RMN-MC（多源可学习中心）。

Result: 在10个2D和3D基准测试中，RMN使用27个参数（MLP使用33,537个，SIREN使用8,577个），实现了比MLP低1.5-51倍的RMSE，比SIREN低10-100倍的RMSE。RMN-MC在优化收敛时，源中心恢复误差低于10^-4。同时报告了在平滑、强非径向目标上的受控失败，以界定RMN的操作范围。

Conclusion: RMN为径向奇异场提供了一种高效、精确的神经网络表示方法，特别适用于物理信息学习中的奇异场建模问题。

Abstract: Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial Müntz-Szász Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^μ$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>


### [898] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种基于算术方法的k-NN回归优化算法AMR，通过引入能处理任意数量实变量的线性方程求解方法，提升了k-NN在回归任务中的性能。


<details>
  <summary>Details</summary>
Motivation: k-NN是常用的非参数回归算法，但仍有优化空间。作者希望通过引入一种能处理任意数量实变量线性方程的算术方法，来提升k-NN算法的效率和性能。

Method: 提出算术方法算法AMA来评估算术方法的效率，并在此基础上开发算术方法回归算法AMR作为k-NN的优化版本。AMR采用引入的最优推理决策规则，并在公开的真实世界数据集上进行评估。

Result: AMR算法与其他回归算法相比具有可比性能，在大多数情况下比标准k-NN表现更好，表明AMR是对k-NN的有效优化。

Conclusion: 提出的AMR算法成功优化了k-NN回归算法，通过算术方法提升了性能，在真实数据集上表现出色。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [899] [Graph-Based Nearest-Neighbor Search without the Spread](https://arxiv.org/abs/2602.06633)
*Jeff Giliberti,Sariel Har-Peled,Jonas Sauer,Ali Vakilian*

Main category: cs.CG

Relevance: 15.0

TL;DR: 提出了一种用于近似最近邻搜索的线性大小数据结构，结合线性大小图，可在对数时间内回答查询，消除了对数据点分布的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法构建的最近邻图虽然具有线性大小，但查询时间依赖于数据点的分布（spread），而分布可能随n无限增长。需要消除这种依赖，实现仅与n相关的对数查询时间。

Method: 构建外部线性大小数据结构，与现有的线性大小最近邻图结合使用。该数据结构能够支持在对数时间内回答近似最近邻查询，而不依赖于数据点的分布。

Result: 实现了仅依赖于n的对数查询时间，消除了对数据点分布的依赖。数据结构保持线性大小，适用于高维空间中的近似最近邻搜索问题。

Conclusion: 成功解决了近似最近邻搜索中对数据点分布的依赖问题，提供了一种高效、可扩展的解决方案，为高维空间中的最近邻搜索提供了理论保证。

Abstract: $\renewcommand{\Re}{\mathbb{R}}$Recent work showed how to construct nearest-neighbor graphs of linear size, on a given set $P$ of $n$ points in $\Re^d$, such that one can answer approximate nearest-neighbor queries in logarithmic time in the spread. Unfortunately, the spread might be unbounded in $n$, and an interesting theoretical question is how to remove the dependency on the spread. Here, we show how to construct an external linear-size data structure that, combined with the linear-size graph, allows us to answer ANN queries in logarithmic time in $n$.

</details>


### [900] [Machine learning enhanced data assimilation framework for multiscale carbonate rock characterization](https://arxiv.org/abs/2602.06989)
*Zhenkai Bo,Ahmed H. Elsheikh,Hannah P. Menke,Julien Maes,Sebastian Geiger,Muhammad Z. Kashim,Zainol A. A. Bakar,Kamaljit Singh*

Main category: physics.geo-ph

Relevance: 15.0

TL;DR: 提出机器学习增强的数据同化框架，利用实验排水相对渗透率测量，实现微尺度结构高效表征，用于高保真多尺度数字岩石建模


<details>
  <summary>Details</summary>
Motivation: 碳酸盐岩储层在碳封存、石油生产和地下储氢方面具有重要价值。传统X射线CT成像面临视野与分辨率权衡问题，多尺度多物理数值模拟计算成本过高，需要高效的多尺度数字岩石建模方法

Method: 提出DNN-ESMDA框架：训练密集神经网络(DNN)作为多尺度孔隙网络模拟器的代理模型，与集成平滑器多数据同化(ESMDA)算法耦合，通过实验排水相对渗透率测量推断微孔隙相CO2-盐水排水相对渗透率

Result: 框架实现计算加速，推理时间从数千小时减少到秒级，同时提供不确定性估计，揭示各岩石相的相对重要性，指导未来表征工作

Conclusion: 机器学习增强的ESMDA框架为表征多尺度碳酸盐岩提供了一种通用化方法，具有计算效率和适用性优势

Abstract: Carbonate reservoirs offer significant capacity for subsurface carbon storage, oil production, and underground hydrogen storage. X-ray computed tomography (X-ray CT) coupled with numerical simulations is commonly used to investigate the multiphase flow behaviors in carbonate rocks. Carbonates exhibit pore size distribution across scales, hindering the comprehensive investigation with conventional X-ray CT images. Imaging samples at both macro and micro-scales (multi-scale imaging) proved to be a viable option in this context. However, multi-scale imaging faces two key limitations: the trade-off between field of view and voxel size necessitates resource-intensive imaging, while multi-scale multi-physics numerical simulations on resulting digital models incur prohibitive computational costs. To address these challenges, we propose a machine learning-enhanced data assimilation framework that leverages experimental drainage relative permeability measurements to achieve efficient characterization of micro-scale structures, delivering a data-driven solution toward a high-fidelity multiscale digital rock modeling. We train a dense neural network (DNN) as a proxy to a multi-scale pore network simulator and couple it with an ensemble smoother with multiple data assimilation (ESMDA) algorithm. DNN-ESMDA framework simultaneously infers the CO2-brine drainage relative permeability of microporosity phases with associated uncertainty estimation, revealing the relative importance of each rock phase and guiding future characterization. Our DNN-ESMDA framework achieves a computational speedup, reducing inference time from thousands of hours to seconds compared with the usage of conventional multiscale numerical simulation. Given this computational efficiency and applicability, the machine learning-enhanced ESMDA framework presents a generalizable approach for characterizing multiscale carbonate rocks.

</details>


### [901] [High-fidelity 3D multi-slab diffusion MRI using Slab-shifting for Harmonized 3D Acquisition and Reconstruction with Profile Encoding Networks (SHARPEN)](https://arxiv.org/abs/2602.07162)
*Ziyu Li,Karla L. Miller,Wenchuan Wu*

Main category: physics.med-ph

Relevance: 15.0

TL;DR: SHARPEN是一种用于3D多层面扩散MRI的板层边界伪影校正方法，通过板层移位和轻量级自监督神经网络估计板层轮廓，实现高质量亚毫米分辨率成像。


<details>
  <summary>Details</summary>
Motivation: 3D多层面扩散MRI虽然具有高分辨率优势，但存在板层边界伪影问题：非理想板层选择性RF激发导致板层轮廓非矩形化，信号在边界处衰减，相邻板层轮廓重叠引起交叉干扰，限制了T1恢复和图像质量。

Method: 提出SHARPEN方法：1）在不同扩散方向应用板层移位，提供互补的板层轮廓编码而不增加扫描时间；2）使用轻量级自监督神经网络估计板层轮廓，利用移位采集的一致性和已知物理特性；3）基于估计轮廓重建校正图像。

Result: SHARPEN在模拟和前瞻性高分辨率体内数据中验证有效，能准确估计板层轮廓并稳健校正边界伪影，即使存在体间运动。相比NPEN方法更快更准确，板层轮廓与参考2D采集结果匹配良好，支持0.7mm各向同性分辨率扩散MRI。

Conclusion: SHARPEN方法无需高质量参考训练数据，支持个体化训练，能够有效校正3D多层面扩散MRI的板层边界伪影，为神经科学研究提供高质量的亚毫米分辨率扩散MRI成像能力。

Abstract: Three-dimensional (3D) multi-slab imaging is a promising approach for high-resolution in vivo diffusion MRI (dMRI) due to its compatibility with short TR (1-2 s), providing optimal signal-to-noise ratio (SNR) efficiency. A major challenge, however, is slab boundary artifacts arising from non-ideal slab-selective RF excitation. Non-rectangular slab profiles reduce signal intensity at slab boundaries, while profile overlap across adjacent slabs introduces inter-slab crosstalk, where repeated excitation shortens the local TR and limits T1 recovery. To mitigate slab boundary artifacts without increasing scan time, we build on slab profile encoding and propose Slab-shifting for Harmonized 3D Acquisition and Reconstruction with Profile Encoding Networks (SHARPEN). For different diffusion directions, SHARPEN applies inter-volume field-of-view shifts along the slice direction to provide complementary slab profile encoding without prolonging acquisition. Slab profiles are estimated using a lightweight self-supervised neural network that exploits consistency across shifted acquisitions and known physical properties of slab profiles and diffusion images, and corrected images are reconstructed accordingly. SHARPEN was validated using simulated and prospectively acquired high-resolution in vivo data and demonstrates accurate slab profile estimation and robust boundary artifact correction, even in the presence of inter-volume motion. SHARPEN does not require high-quality reference training data and supports subject-specific training. Its efficient GPU-based implementation delivers faster and more accurate correction than NPEN, yielding slice-wise quantitative profiles that closely match those from reference 2D acquisitions. SHARPEN enables high-quality dMRI at 0.7 mm isotropic resolution on a 3T clinical scanner, highlighting its potential to advance submillimeter dMRI for neuroscience research.

</details>


### [902] [Statistical inference after variable selection in Cox models: A simulation study](https://arxiv.org/abs/2602.07477)
*Lena Schemet,Sarah Friedrich-Welz*

Main category: stat.ME

Relevance: 15.0

TL;DR: 该论文研究了在Cox比例风险模型中，对Lasso和自适应Lasso进行变量选择后的统计推断方法，包括样本分割、精确后选择推断和去偏Lasso，通过模拟研究和实际数据评估这些方法在生物医学生存数据中的表现。


<details>
  <summary>Details</summary>
Motivation: 在生物医学生存数据分析中，变量选择是核心任务，但经典的频率推断假设协变量集是预先固定的，不考虑数据驱动的变量选择。这导致后选择推断可能存在偏差，特别是在右删失生存数据中，删失带来的额外不确定性会加剧这一问题。

Method: 研究了三种后选择推断方法：样本分割、精确后选择推断和去偏Lasso，应用于Cox模型中的Lasso和自适应Lasso变量选择。通过模拟研究评估这些方法，模拟设计反映了生物医学应用中常见的协变量结构和删失率，并使用公开生存数据集进行实际应用示例。

Result: 通过模拟研究比较了不同后选择推断方法在Cox模型中的表现，评估了它们在处理变量选择后统计推断时的性能。实际应用示例进一步展示了这些方法在真实生物医学数据中的行为。

Conclusion: 该研究为生物医学生存数据分析中变量选择后的统计推断提供了方法学比较和实用指导，有助于解决后选择推断偏差问题，特别是在存在删失的生存数据背景下。

Abstract: Choosing relevant predictors is central to the analysis of biomedical time-to-event data. Classical frequentist inference, however, presumes that the set of covariates is fixed in advance and does not account for data-driven variable selection. As a consequence, naive post-selection inference may be biased and misleading. In right-censored survival settings, these issues may be further exacerbated by the additional uncertainty induced by censoring. We investigate several inference procedures applied after variable selection for the coefficients of the Lasso and its extension, the adaptive Lasso, in the context of the Cox model. The methods considered include sample splitting, exact post-selection inference, and the debiased Lasso. Their performance is examined in a neutral simulation study reflecting realistic covariate structures and censoring rates commonly encountered in biomedical applications. To complement the simulation results, we illustrate the practical behavior of these procedures in an applied example using a publicly available survival dataset.

</details>


### [903] [The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring](https://arxiv.org/abs/2602.08080)
*Luciano Bozzi,Christian Celidonio,Umberto Nuzzi,Massimo Biagini,Stefano Cherubin,Asbjørn Djupdal,Tor Andre Haugdahl,Andrea Aliverti,Alessandra Angelucci,Giovanni Agosta,Gerardo Pelosi,Paolo Belluco,Samuele Polistina,Riccardo Volpi,Luigi Malagò,Michael Schneider,Florian Wieczorek,Xabier Eguiluz*

Main category: cs.ET

Relevance: 15.0

TL;DR: CAPSARII项目提出了一种创新的可穿戴系统和战场物联网框架，用于监测士兵的生理和心理状态，通过边缘AI和云端分析提供实时战术决策支持。


<details>
  <summary>Details</summary>
Motivation: 欧洲防务局修订的能力发展计划将提高地面作战能力作为优先事项，需要增强士兵装备以提供更好保护。CAPSARII项目旨在通过监测士兵状态来改善战术决策和医疗支持。

Method: 采用创新的可穿戴系统和战场物联网(IoBT)框架，集成智能纺织品，通过软硬件优化降低能耗，在边缘节点部署AI模型进行实时分析，云端进行数据分析和比较研究，并采用高效加密和强认证方法确保安全。

Result: 该系统能够增强态势感知和作战效能，通过监测生理、运动和环境参数提供实时战术决策支持，改善可用性并延长电池寿命，同时解决安全关切。

Conclusion: CAPSARII的创新方法旨在通过提供强大的数据驱动决策支持工具来改变军事行动，但该论文主要关注军事应用系统，与用户研究的LLM和基础模型架构相关性有限。

Abstract: The European Defence Agency's revised Capability Development Plan (CDP) identifies as a priority improving ground combat capabilities by enhancing soldiers' equipment for better protection. The CAPSARII project proposes in innovative wearable system and Internet of Battlefield Things (IoBT) framework to monitor soldiers' physiological and psychological status, aiding tactical decisions and medical support. The CAPSARII system will enhance situational awareness and operational effectiveness by monitoring physiological, movement and environmental parameters, providing real-time tactical decision support through AI models deployed on edge nodes and enable data analysis and comparative studies via cloud-based analytics. CAPSARII also aims at improving usability through smart textile integration, longer battery life, reducing energy consumption through software and hardware optimizations, and address security concerns with efficient encryption and strong authentication methods. This innovative approach aims to transform military operations by providing a robust, data-driven decision support tool.

</details>


### [904] [Adjustment of Cluster-Then-Predict Framework for Multiport Scatterer Load Prediction](https://arxiv.org/abs/2602.08129)
*Hanjun Park,Aleksandr D. Kuznetsov,Ville Viikari*

Main category: eess.SP

Relevance: 15.0

TL;DR: 提出两阶段聚类-预测框架用于多端口散射体负载值预测，相比基线降低46% RMSE，并引入RUI指标评估多目标权衡


<details>
  <summary>Details</summary>
Motivation: 多端口散射体中负载值的相互依赖预测具有挑战性，因为高维度和阻抗与散射能力之间的复杂依赖关系，但这对通信和测量系统设计至关重要

Method: 提出两阶段聚类-预测框架：先聚类后预测，有效捕捉S参数与对应负载阻抗之间的函数关系。引入RUI指标评估多目标权衡

Result: 相比基线梯度提升方法，RMSE降低高达46%。K-means聚类与KNN组合在RUI评估下被确定为最优配置

Conclusion: 提出的两阶段框架能有效预测多端口散射体负载值，RUI指标适用于现实场景中多目标权衡的性能评估

Abstract: Predicting interdependent load values in multiport scatterers is challenging due to high dimensionality and complex dependence between impedance and scattering ability, yet this prediction remains crucial for the design of communication and measurement systems. In this paper, we propose a two-stage cluster-then-predict framework for multiple load values prediction task in multiport scatterers. The proposed cluster-then-predict approach effectively captures the underlying functional relation between S-parameters and corresponding load impedances, achieving up to a 46% reduction in Root Mean Square Error (RMSE) compared to the baseline when applied to gradient boosting (GB). This improvement is consistent across various clustering and regression methods. Furthermore, we introduce the Real-world Unified Index (RUI), a metric for quantitative analysis of trade-offs among multiple metrics with conflicting objectives and different scales, suitable for performance assessment in realistic scenarios. Based on RUI, the combination of K-means clustering and k-nearest neighbors (KNN) is identified as the optimal setup for the analyzed multiport scatterer.

</details>


### [905] [Do physics-informed neural networks (PINNs) need to be deep? Shallow PINNs using the Levenberg-Marquardt algorithm](https://arxiv.org/abs/2602.08515)
*Muhammad Luthfi Shahab,Imam Mukhlash,Hadi Susanto*

Main category: math.NA

Relevance: 15.0

TL;DR: 该论文提出使用浅层物理信息神经网络（PINNs）结合Levenberg-Marquardt算法求解非线性偏微分方程的正反问题，在多个基准问题上表现出比BFGS更优的收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统深度PINNs训练计算成本高且收敛慢，需要探索更高效的优化方法。本文旨在研究浅层PINNs结合二阶优化算法在求解非线性PDE正反问题中的性能。

Method: 将PINNs重构为非线性系统，采用Levenberg-Marquardt算法优化网络参数。推导神经网络对输入变量的解析导数表达式，实现Jacobian矩阵的高效计算。使用仅含两个隐藏层的浅层网络架构。

Result: 在Burgers、Schrödinger、Allen-Cahn和三维Bratu方程等基准问题上，LM算法在收敛速度、精度和最终损失值方面显著优于BFGS算法，即使使用浅层网络也能获得准确解。

Conclusion: 对于广泛类型的PDE，浅层PINNs结合高效二阶优化方法（如LM）能够为正向和反问题提供准确且计算高效的解决方案，为PDE求解提供了新的有效途径。

Abstract: This work investigates the use of shallow physics-informed neural networks (PINNs) for solving forward and inverse problems of nonlinear partial differential equations (PDEs). By reformulating PINNs as nonlinear systems, the Levenberg-Marquardt (LM) algorithm is employed to efficiently optimize the network parameters. Analytical expressions for the neural network derivatives with respect to the input variables are derived, enabling accurate and efficient computation of the Jacobian matrix required by LM. The proposed approach is tested on several benchmark problems, including the Burgers, Schrödinger, Allen-Cahn, and three-dimensional Bratu equations. Numerical results demonstrate that LM significantly outperforms BFGS in terms of convergence speed, accuracy, and final loss values, even when using shallow network architectures with only two hidden layers. These findings indicate that, for a wide class of PDEs, shallow PINNs combined with efficient second-order optimization methods can provide accurate and computationally efficient solutions for both forward and inverse problems.

</details>


### [906] [Incremental (k, z)-Clustering on Graphs](https://arxiv.org/abs/2602.08542)
*Emilio Cruciani,Sebastian Forster,Antonis Skarlatos*

Main category: cs.DS

Relevance: 15.0

TL;DR: 本文提出了一种针对动态图中(k,z)-聚类问题的随机增量算法，能够在边插入过程中以高概率维持常数倍近似解，总更新时间为Õ(km^{1+o(1)}+k^{1+1/λ}m)。


<details>
  <summary>Details</summary>
Motivation: 动态图中的(k,z)-聚类问题在边更新时需要维护聚类中心，虽然动态k-center问题已有近似算法，但动态(k,z)-聚类问题尚无类似结果。本文旨在填补这一空白，为动态图中的聚类问题提供高效算法。

Method: 采用两阶段随机增量算法：第一阶段维护大小为Õ(k)的双准则近似解，第二阶段在双准则解诱导的动态加权实例上维护常数倍近似(k,z)-聚类解。关键技术包括将Mettu-Plaxton算法适配到增量图，并证明其半径非递减性。

Result: 算法能以高概率维持常数倍近似解，总更新时间为Õ(km^{1+o(1)}+k^{1+1/λ}m)，其中λ≥1为任意固定常数。这是首个针对动态(k,z)-聚类问题的高效算法。

Conclusion: 本文成功解决了动态图中(k,z)-聚类问题，提出了首个高效增量算法，为动态图聚类提供了新的理论工具，其中半径非递减性的技术结果可能具有独立价值。

Abstract: Given a weighted undirected graph, a number of clusters $k$, and an exponent $z$, the goal in the $(k, z)$-clustering problem on graphs is to select $k$ vertices as centers that minimize the sum of the distances raised to the power $z$ of each vertex to its closest center. In the dynamic setting, the graph is subject to adversarial edge updates, and the goal is to maintain explicitly an exact $(k, z)$-clustering solution in the induced shortest-path metric.
  While efficient dynamic $k$-center approximation algorithms on graphs exist [Cruciani et al. SODA 2024], to the best of our knowledge, no prior work provides similar results for the dynamic $(k,z)$-clustering problem. As the main result of this paper, we develop a randomized incremental $(k, z)$-clustering algorithm that maintains with high probability a constant-factor approximation in a graph undergoing edge insertions with a total update time of $\tilde O(k m^{1+o(1)}+ k^{1+\frac{1}λ} m)$, where $λ\geq 1$ is an arbitrary fixed constant. Our incremental algorithm consists of two stages. In the first stage, we maintain a constant-factor bicriteria approximate solution of size $\tilde{O}(k)$ with a total update time of $m^{1+o(1)}$ over all adversarial edge insertions. This first stage is an intricate adaptation of the bicriteria approximation algorithm by Mettu and Plaxton [Machine Learning 2004] to incremental graphs. One of our key technical results is that the radii in their algorithm can be assumed to be non-decreasing while the approximation ratio remains constant, a property that may be of independent interest.
  In the second stage, we maintain a constant-factor approximate $(k,z)$-clustering solution on a dynamic weighted instance induced by the bicriteria approximate solution. For this subproblem, we employ a dynamic spanner algorithm together with a static $(k,z)$-clustering algorithm.

</details>


### [907] [Estimation of Fish Catch Using Sentinel-2, 3 and XGBoost-Kernel-Based Kernel Ridge Regression](https://arxiv.org/abs/2602.08511)
*Kanu Mohammed,Vaishnavi Joshi,Pranjali Diliprao Patil,Sandipan Mondal,Ming-An Lee,Subhadip Dey*

Main category: physics.app-ph

Relevance: 10.0

TL;DR: 使用Sentinel-2 MSI和Sentinel-3 OLCI多光谱影像，结合XGBoost-核化的核岭回归技术，预测鱼类捕获量，以支持可持续渔业管理和联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 海洋环境因素（如海表温度和上层海洋动力学）对鱼类分布有重要影响。为了维持对全球粮食安全有贡献的渔业，需要量化这些海洋-鱼类之间的非线性关系，支持可持续生态系统管理和卫星渔业评估。

Method: 采用多光谱影像（Sentinel-2 MSI和Sentinel-3 OLCI），结合极端梯度提升（XGBoost）-核化的核岭回归（KRR）技术来估计鱼类捕获量。该方法旨在捕捉海洋因素与鱼类分布之间的非线性关系。

Result: XGBoost-KRR框架在两个传感器上都实现了最强的相关性和最低的预测误差，表明其能更好地捕捉非线性海洋-鱼类关系。Sentinel-2 MSI能解析更精细的空间变异性，而Sentinel-3 OLCI则显示更平滑的光谱响应。

Conclusion: 该方法通过支持可持续生态系统管理和加强卫星渔业评估，推进了联合国可持续发展目标2（零饥饿）和14（水下生命）。

Abstract: Oceanographic factors, such as sea surface temperature and upper-ocean dynamics, have a significant impact on fish distribution. Maintaining fisheries that contribute to global food security requires quantifying these connections. This study uses multispectral images from Sentinel-2 MSI and Sentinel-3 OLCI to estimate fish catch using an Extreme Gradient Boosting (XGBoost)-kernelized Kernel Ridge Regression (KRR) technique. According to model evaluation, the XGBoost-KRR framework achieves the strongest correlation and the lowest prediction error across both sensors, suggesting improved capacity to capture nonlinear ocean-fish connections. While Sentinel-2 MSI resolves finer-scale spatial variability, emphasizing localized ecological interactions, Sentinel-3 OLCI displays smoother spectral responses associated with poorer spatial resolution. By supporting sustainable ecosystem management and strengthening satellite-based fisheries assessment, the proposed approach advances SDGs 2 (Zero Hunger) and 14 (Life Below Water).

</details>


### [908] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

Relevance: 5.0

TL;DR: 该研究开发了一种滞后向后兼容物理信息神经网络（LBC-PINN），用于模拟和反演长期荷载下的一维非饱和土固结过程，通过时间分割和分段迁移学习解决多时间尺度耦合问题。


<details>
  <summary>Details</summary>
Motivation: 解决非饱和土固结中空气和水压力在多时间尺度上的耦合耗散难题，传统数值方法在处理长期加载和反演问题时存在计算效率和精度限制。

Method: 提出LBC-PINN框架，集成对数时间分割、滞后兼容性损失强制和分段迁移学习，采用基于特征空气相耗散时间的简化分割策略提高计算效率。

Result: 模型在长达1e10秒的时间范围内准确预测孔隙空气和孔隙水压力演化，与有限元结果相比平均绝对误差低于1e-2，对渗透率比1e-3到1e3范围具有鲁棒性。

Conclusion: LBC-PINN为多时间尺度非饱和土固结问题提供了准确高效的解决方案，简化分割策略在保持精度的同时显著提高计算效率。

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [909] [Universal Coefficients and Mayer-Vietoris Sequence for Groupoid Homology](https://arxiv.org/abs/2602.08998)
*Luciano Melodia*

Main category: math.AT

Relevance: 5.0

TL;DR: 该论文研究ample群同胚的同调理论，通过神经的紧支撑Moore复形方法，建立了与离散系数的通用系数短正合序列，并分析了非离散系数的障碍。


<details>
  <summary>Details</summary>
Motivation: 研究ample群同胚的同调理论，旨在建立更一般的同调理论框架，特别是处理离散和非离散系数的情况，以及理解群同胚同调与经典代数同调理论的关系。

Method: 使用紧支撑Moore复形方法，定义群同胚同调群H_n(G;A)。通过链级同构C_c(G_n,ℤ)⊗A≅C_c(G_n,A)，将群同胚问题约化为经典代数通用系数定理。分析非离散系数的障碍，并构造Mayer-Vietoris长正合序列用于显式计算。

Result: 证明了对于离散系数A的自然通用系数短正合序列，识别了比较映射的链级形式，发现了非离散系数的障碍（紧支撑函数必须具有有限像），并建立了Mayer-Vietoris长正合序列用于显式计算。

Conclusion: 该工作为ample群同胚的同调理论提供了系统的框架，建立了与离散系数的通用系数定理，阐明了非离散系数的限制条件，并提供了有效的计算工具。

Abstract: We study homology of ample groupoids via the compactly supported Moore complex of the nerve. Let $A$ be a topological abelian group. For $n\ge 0$ set $C_n(\mathcal G;A) := C_c(\mathcal G_n,A)$ and define $\partial_n^A=\sum_{i=0}^n(-1)^i(d_i)_*$. This defines $H_n(\mathcal G;A)$. The theory is functorial for continuous étale homomorphisms. It is compatible with standard reductions, including restriction to saturated clopen subsets. In the ample setting it is invariant under Kakutani equivalence. We reprove Matui type long exact sequences and identify the comparison maps at chain level. For discrete $A$ we prove a natural universal coefficient short exact sequence $$0\to H_n(\mathcal G)\otimes_{\mathbb Z}A\xrightarrow{\ ι_n^{\mathcal G}\ }H_n(\mathcal G;A)\xrightarrow{\ κ_n^{\mathcal G}\ }\operatorname{Tor}_1^{\mathbb Z}\bigl(H_{n-1}(\mathcal G),A\bigr)\to 0.$$ The key input is the chain level isomorphism $C_c(\mathcal G_n,\mathbb Z)\otimes_{\mathbb Z}A\cong C_c(\mathcal G_n,A)$, which reduces the groupoid statement to the classical algebraic UCT for the free complex $C_c(\mathcal G_\bullet,\mathbb Z)$. We also isolate the obstruction for non-discrete coefficients. For a locally compact totally disconnected Hausdorff space $X$ with a basis of compact open sets, the image of $Φ_X:C_c(X,\mathbb Z)\otimes_{\mathbb Z}A\to C_c(X,A)$ is exactly the compactly supported functions with finite image. Thus $Φ_X$ is surjective if and only if every $f\in C_c(X,A)$ has finite image, and for suitable $X$ one can produce compactly supported continuous maps $X\to A$ with infinite image. Finally, for a clopen saturated cover $\mathcal G_0=U_1\cup U_2$ we construct a short exact sequence of Moore complexes and derive a Mayer-Vietoris long exact sequence for $H_\bullet(\mathcal G;A)$ for explicit computations.

</details>
