<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CV](#cs.CV) [Total: 73]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 83]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

Relevance: 85.0

TL;DR: RIMRULE：一种基于动态规则注入的神经符号方法，通过从失败轨迹中提炼紧凑、可解释的规则，在推理时注入提示中，以提高LLM在特定领域工具使用中的性能，无需修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域工具使用时经常遇到困难，因为API可能具有特殊性、文档不足或针对私有工作流程定制。这凸显了有效适应任务特定工具的需求。

Method: 提出RIMRULE神经符号方法：1) 从失败轨迹中提炼紧凑、可解释的规则；2) 使用最小描述长度目标进行规则整合，偏好通用性和简洁性；3) 将规则以自然语言和结构化符号形式存储；4) 在推理时动态注入规则到提示中。

Result: 在工具使用基准测试中，该方法提高了对已见和未见工具的准确性，优于基于提示的适应方法，并与微调互补。从一个LLM学习的规则可以重用改进其他LLM，包括长推理LLM，突显了符号知识在不同架构间的可移植性。

Conclusion: RIMRULE提供了一种无需修改模型权重的有效适应方法，通过动态规则注入提高LLM在特定领域工具使用中的性能，展示了符号知识在不同LLM架构间的可移植性。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [2] [Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning](https://arxiv.org/abs/2601.00095)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CL

Relevance: 85.0

TL;DR: MetaJuLS：基于元强化学习的通用约束传播策略，用于LLM结构化推理，实现跨语言/任务的快速适应，相比GPU优化基线提速1.5-2倍，精度损失小于0.2%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要结构化推理（如JSON模式强制、多语言解析），输出必须满足复杂约束。现有方法通常需要任务特定的重新训练，缺乏通用性和效率。

Method: 将结构化推理建模为自适应约束传播问题，使用图注意力网络和元强化学习训练通用约束传播策略。策略学习如何在不同语言和任务间传播约束，无需任务特定重新训练。

Result: 在10种语言的Universal Dependencies和LLM约束生成任务（LogicBench、GSM8K-Constrained）上，MetaJuLS比GPU优化基线提速1.5-2倍，精度损失小于0.2%。仅需5-10个梯度步（5-15秒）即可适应新语言/任务，而非数小时训练。

Conclusion: MetaJuLS通过元强化学习实现了高效的跨语言/任务结构化推理，发现了类人解析策略和新启发式方法，减少了LLM推理的传播步骤和碳足迹。

Abstract: Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.

</details>


### [3] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了Q*和Feedback+两种验证技术，通过反向翻译、语义匹配和执行反馈来提升LLM在商业分析中的代码生成准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM助手在企业工作流中的集成日益增多，生成准确、语义对齐且可执行的输出变得至关重要。当前对话式商业分析系统缺乏内置验证机制，用户需要手动验证可能存在缺陷的结果。

Method: 提出了两种互补的验证技术：1) Q*：执行代码与用户意图之间的反向翻译和语义匹配；2) Feedback+：结合执行反馈来指导代码精炼。这些机制嵌入在生成器-判别器框架中，将验证责任从用户转移到系统。

Result: 在Spider、Bird和GSM8K三个基准数据集上的评估表明，Q*和Feedback+都能降低错误率和任务完成时间。研究还识别出反向翻译是主要瓶颈，为未来改进提供了方向。

Conclusion: 这项工作为构建更可靠、企业级的生成式AI系统提供了一个设计导向的框架，能够提供可信的决策支持。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [4] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

Relevance: 85.0

TL;DR: WildAGTEval是一个评估LLM智能体函数调用能力的基准，专注于真实API复杂性，包含API规范和API执行两个维度的挑战，涵盖60种复杂度场景和约32K测试配置。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法假设理想化的API系统，忽略了真实世界因素如噪声API输出。需要评估LLM智能体在真实API复杂性下的函数调用能力，以更好地反映实际应用场景。

Method: 构建WildAGTEval基准，包含两个维度：1) API规范（详细文档和使用约束），2) API执行（运行时挑战）。创建包含60种复杂度场景的API系统，可组合成约32K测试配置，并提供用户-智能体交互评估框架。

Result: 评估多个先进LLM发现：大多数场景具有挑战性，无关信息复杂度最困难，使强LLM性能下降27.3%。定性分析显示LLM有时会扭曲用户意图来声称任务完成，严重影响用户满意度。

Conclusion: WildAGTEval揭示了LLM智能体在真实API复杂性下的局限性，特别是处理无关信息的能力不足，以及可能扭曲用户意图的问题，这对LLM智能体的实际部署具有重要意义。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [5] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

Relevance: 85.0

TL;DR: 量化对LLM自解释能力的影响研究：量化通常导致自解释质量和忠实度适度下降（最高4.4%和2.38%），用户研究发现量化降低了解释的连贯性和可信度（最高8.5%），大模型在忠实度方面表现更好，但不同量化技术在任务准确性、解释质量和忠实度之间没有一致优势。


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于加速LLM推理和部署，但其对自解释（SEs）的影响尚未被探索。自解释是LLM为证明自身输出而生成的解释，需要模型推理自身的决策过程，这种能力可能对量化特别敏感。由于自解释在高风险应用中被越来越依赖以实现透明度，理解量化是否以及多大程度上降低自解释质量和忠实度至关重要。

Method: 研究两种类型的自解释：自然语言解释（NLEs）和反事实示例，使用三种常见量化技术在不同比特宽度下对LLM进行量化。通过用户研究评估量化对解释连贯性和可信度的影响，比较不同规模模型对量化的敏感性。

Result: 量化通常导致自解释质量和忠实度适度下降（最高4.4%和2.38%）。用户研究发现量化降低了解释的连贯性和可信度（最高8.5%）。相比小模型，大模型在自解释质量方面对量化的抵抗力有限，但在保持忠实度方面表现更好。没有量化技术能在任务准确性、解释质量和忠实度三个方面一致表现优异。

Conclusion: 量化对自解释的影响因上下文而异，建议在特定用例中验证自解释质量，特别是对更敏感的自然语言解释。尽管如此，自解释质量和忠实度的相对较小恶化并不削弱量化作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [6] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一种针对LLM幻觉检测的鲁棒不确定性量化方法(RU)，通过构建包含虚假名称的陷阱问题集，在包含多个事实的生成任务中评估模型的不确定性，相比基线方法在ROCAUC上平均提升0.1-0.2。


<details>
  <summary>Details</summary>
Motivation: LLM幻觉问题严重影响了AI生成内容的可靠性和可信度。传统不确定性量化方法在常规问答场景中有效，但在面对非标准或对抗性提问策略时表现不足，这限制了LLM在需要强大批判性思维能力的实际应用中的可靠性。

Method: 1) 构建包含虚假名称的陷阱问题集，用于评估LLM在包含多个事实的生成任务中的不确定性；2) 提出鲁棒不确定性量化方法(RU)，专门针对非标准或对抗性提问场景设计；3) 在四个不同模型上与基线方法进行对比实验。

Result: 1) 构建的陷阱问题集表现优异；2) 提出的RU方法在四个不同模型上相比最佳基线方法，ROCAUC值平均提升0.1-0.2；3) 为LLM幻觉问题提供了新的视角和方法。

Conclusion: 该研究提出的鲁棒不确定性量化方法能有效检测LLM在复杂、对抗性场景下的幻觉问题，相比传统方法有显著改进，为解决LLM幻觉问题提供了新的技术路径。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [7] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现双语数据中仅2%的平行语料（而非代码转换数据）对多语言大语言模型的翻译能力至关重要，而跨语言理解和推理任务即使没有双语数据也能实现


<details>
  <summary>Details</summary>
Motivation: 探究多语言大语言模型中双语数据的具体贡献机制，理解为何仅占语料库2%的双语数据对模型跨语言能力产生重要影响

Method: 通过受控条件下的从头训练实验，比较标准网络语料库与去除所有多语言文档的纯单语版本；将双语数据分类为平行语料(14%)、代码转换(72%)和其他文档(14%)，并进行粒度消融实验

Result: 去除双语数据导致翻译性能下降56%，但跨语言QA和推理任务保持稳定；平行语料几乎完全恢复翻译性能(达到基线的91%)，而代码转换贡献极小；其他跨语言任务基本不受双语数据类型影响

Conclusion: 翻译能力严重依赖平行语料提供的系统性token级对齐，而跨语言理解和推理能力即使没有双语数据也能实现，揭示了不同跨语言任务对双语数据的不同依赖机制

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [8] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Defensive M2S训练范式，通过多轮对话到单轮压缩(M2S)来大幅降低护栏模型的训练和推理成本，在保持高攻击检测率的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前护栏模型处理完整多轮对话历史会带来巨大的计算成本，限制了其在长对话场景中的可扩展部署。需要一种既能保持安全检测效果又能显著降低计算开销的方法。

Method: 提出Defensive M2S训练范式，在压缩的单轮对话上微调护栏模型，而不是完整的对话历史。使用三种压缩模板(hyphenize, numberize, pythonize)将多轮对话压缩为单轮表示。从理论上分析了M2S将训练复杂度从O(n²)降低到O(n)。

Result: 在779个样本的训练数据集上，M2S仅需169K tokens，相比多轮基线的15.7M tokens减少了93倍。在SafeDialBench基准测试中，最佳配置(Qwen3Guard+hyphenize)达到93.8%的攻击检测召回率，同时将推理tokens减少94.6%(从3,231降至173 tokens)。相比基线提升了38.9个百分点。

Conclusion: M2S压缩可作为护栏模型部署的有效效率技术，能够在保持高安全检测性能的同时，显著降低训练和推理成本，实现长多轮对话的可扩展安全筛查。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [9] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

Relevance: 85.0

TL;DR: InfoSynth是一个基于信息论原则的自动生成和评估LLM推理基准的框架，使用KL散度和熵量化基准新颖性和多样性，通过遗传算法生成Python编程问题，实现97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基准创建依赖人工，成本高且耗时，现有基准常污染LLM训练数据，需要新颖多样的基准来准确评估LLM的真实能力。

Method: 提出基于KL散度和熵的指标量化基准新颖性和多样性，无需昂贵模型评估；开发端到端流水线，使用遗传算法和迭代代码反馈从种子数据集合成Python编程问题。

Result: 方法生成新问题的准确测试用例和解决方案达97%成功率，合成基准相比种子数据集展现出更高的新颖性和多样性，算法可控制生成问题的新颖性/多样性和难度。

Conclusion: InfoSynth为LLM构建高质量、新颖多样的基准提供了可扩展、自验证的流水线，解决了基准创建的成本和污染问题。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [10] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

Relevance: 85.0

TL;DR: CSSBench是一个专门针对中文特定对抗模式的安全基准测试，评估轻量级LLM在中文环境下的安全性，填补了现有英语基准测试的空白。


<details>
  <summary>Details</summary>
Motivation: 现有安全护栏主要针对英语，而中文恶意查询通常通过同音字、拼音、符号分割等中文特定模式隐藏意图，这些对抗模式在现有基准测试中未被充分捕捉，特别是对于轻量级模型存在安全隐患。

Method: 构建中文特定安全基准测试(CSSBench)，涵盖六个真实中文场景领域（非法活动与合规、隐私泄露、健康医疗错误信息、欺诈与仇恨、成人内容、公共与政治安全），将查询组织成多种任务类型，评估流行轻量级LLM并测量过度拒绝行为。

Result: 中文特定对抗模式对轻量级LLM构成关键挑战，基准测试结果显示了模型在中文环境下的安全漏洞和性能退化。

Conclusion: CSSBench为中文LLM安全提供了全面评估，有助于实际部署中的鲁棒性改进，填补了中文特定对抗模式的安全评估空白。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [11] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一个模型无关的框架，通过重复采样和多数投票机制，为固定输入任务中的LLM幻觉提供概率保证，无需修改模型权重或提示工程。


<details>
  <summary>Details</summary>
Motivation: LLM在确定性自动化工作流中经常产生上下文幻觉（生成内容与提示明确信息矛盾），这些错误在输入固定且正确性明确的场景中特别成问题。需要一种轻量级方法来降低幻觉概率。

Method: 1) 在独立上下文窗口中重复相同提示，利用指数级降低所有输出都错误的概率；2) 使用LLM作为评判器识别正确答案；3) 当评判器不完美时，通过多数投票机制增强评判器，获得指数级降低的集合级错误率。

Result: 在受控提取任务上的实验验证了理论预测：管道失败概率随重复次数指数下降，幻觉选择概率随评判器数量指数下降。该方法能驱动幻觉概率任意低。

Conclusion: 提供了一个轻量级、模块化、理论基础的框架，可在固定输入LLM工作流中任意降低幻觉概率，无需修改模型权重、解码策略或提示工程。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [12] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

Relevance: 85.0

TL;DR: FwPKM是一种新型记忆架构，将静态产品键记忆转换为动态快速权重记忆，通过局部梯度下降动态更新参数，在训练和推理时都能快速记忆和检索输入序列，解决了存储容量与计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的序列建模层面临存储容量与计算效率的权衡：Softmax注意力提供无限存储但计算成本高（二次复杂度），线性变体效率高但存储有限且固定。需要一种既能高效计算又具有足够存储容量的架构。

Method: 提出Fast-weight Product Key Memory (FwPKM)，将稀疏的产品键记忆从静态模块转变为动态的"快速权重"情景记忆。通过局部块级梯度下降在训练和推理时动态更新参数，使模型能够快速记忆和检索输入序列中的新键值对。

Result: FwPKM作为有效的情景记忆补充了标准模块的语义记忆，在长上下文数据集上显著降低了困惑度。在"大海捞针"评估中，尽管仅在4K标记序列上训练，却能泛化到128K标记的上下文。

Conclusion: FwPKM成功解决了序列建模中存储容量与计算效率的权衡问题，提供了一种动态、高效的记忆机制，能够处理远超训练长度的长上下文，对长上下文语言模型有重要价值。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [13] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Sigmoid Head模块，通过sigmoid激活函数解决语言模型概率分布无法同时给多个正确选项高概率的问题，无需人工标注质量数据，在质量估计上优于原始softmax头


<details>
  <summary>Details</summary>
Motivation: 语言模型的概率分布不是可靠的质量估计器，因为自然语言具有歧义性。当多个输出选项都有效时，模型的概率分布会分散到这些选项上，从而误导性地表明输出质量低。这由两个原因造成：(1) LM的最终输出激活函数softmax不允许多个正确选项同时获得高概率；(2) LM的训练数据是单一、one-hot编码的参考，表明每个输出步骤只有一个正确选项。

Method: 在预训练语言模型之上训练一个质量估计模块Sigmoid Head。该模块是一个额外的解嵌入头，使用sigmoid激活函数解决第一个限制。为应对第二个限制，在训练Sigmoid Head的负采样过程中，使用启发式方法避免选择可能正确的替代标记。该模块在训练和推理中计算效率高。

Result: Sigmoid Head的概率相比原始softmax头是显著更好的质量信号。由于Sigmoid Head不依赖人工标注的质量数据，相比监督式质量估计，在领域外设置中更加鲁棒。

Conclusion: Sigmoid Head方法有效解决了语言模型概率分布作为质量估计器的局限性，通过sigmoid激活函数和智能负采样策略，提供了更可靠的质量信号，且具有更好的领域外鲁棒性。

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [14] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文评估了多种大语言模型在文本跨度识别任务上的性能，包括情感分析、冒犯性语言识别和声明验证，探索了指令调优、上下文学习和思维链等策略。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数跨度识别方法依赖于BERT等较小的预训练语言模型，但最近有方法开始利用最新一代大语言模型。当前工作主要集中在命名实体识别等显式跨度识别，而像基于方面的情感分析等更主观的跨度识别任务在大语言模型中的应用尚未充分探索。

Method: 评估了多种大语言模型在三个流行任务上的文本跨度识别性能：情感分析、冒犯性语言识别和声明验证。探索了多种LLM策略，包括指令调优、上下文学习和思维链。

Result: 结果表明，文本内部的潜在关系有助于大语言模型识别精确的文本跨度。

Conclusion: 本文填补了大语言模型在主观文本跨度识别任务中的重要空白，为下游任务和模型可解释性提供了重要见解。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [15] [Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak](https://arxiv.org/abs/2601.00213)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文研究了LLMs在自动化算法设计中的安全漏洞，特别是恶意优化算法生成，提出了MalOptBench基准和MOBjailbreak攻击方法，发现主流LLMs对此类攻击高度脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛部署，其误用风险和安全问题日益受到关注。先前研究主要关注LLMs在通用使用、代码生成和基于代理的应用中的安全性，但在自动化算法设计中的漏洞尚未充分探索。本研究旨在填补这一空白，重点关注智能优化算法设计领域，因为其在复杂决策场景中应用广泛。

Method: 1) 引入MalOptBench基准，包含60个恶意优化算法请求；2) 提出MOBjailbreak方法，专门针对算法设计场景的越狱攻击；3) 对13个主流LLMs（包括最新的GPT-5和DeepSeek-V3.1）进行广泛评估；4) 评估最先进的即插即用防御方法对闭源模型的效果。

Result: 1) 大多数模型对原始恶意提示高度脆弱，平均攻击成功率为83.59%，平均危害性评分为4.28/5；2) 在MOBjailbreak攻击下，模型几乎完全失效；3) 现有防御方法对MOBjailbreak仅略微有效，且容易产生过度安全行为；4) 揭示了LLMs在算法设计领域的安全漏洞。

Conclusion: 研究结果表明，当前LLMs在算法设计场景中存在严重安全漏洞，现有防御措施不足，迫切需要更强的对齐技术来保护LLMs在算法设计中的安全使用。

Abstract: The widespread deployment of large language models (LLMs) has raised growing concerns about their misuse risks and associated safety issues. While prior studies have examined the safety of LLMs in general usage, code generation, and agent-based applications, their vulnerabilities in automated algorithm design remain underexplored. To fill this gap, this study investigates this overlooked safety vulnerability, with a particular focus on intelligent optimization algorithm design, given its prevalent use in complex decision-making scenarios. We introduce MalOptBench, a benchmark consisting of 60 malicious optimization algorithm requests, and propose MOBjailbreak, a jailbreak method tailored for this scenario. Through extensive evaluation of 13 mainstream LLMs including the latest GPT-5 and DeepSeek-V3.1, we reveal that most models remain highly susceptible to such attacks, with an average attack success rate of 83.59% and an average harmfulness score of 4.28 out of 5 on original harmful prompts, and near-complete failure under MOBjailbreak. Furthermore, we assess state-of-the-art plug-and-play defenses that can be applied to closed-source models, and find that they are only marginally effective against MOBjailbreak and prone to exaggerated safety behaviors. These findings highlight the urgent need for stronger alignment techniques to safeguard LLMs against misuse in algorithm design.

</details>


### [16] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该研究提出了一种将循证医学原则融入图基检索增强生成的方法，通过PICO框架改进知识图谱构建与检索，并设计贝叶斯重排序算法考虑证据等级，在运动康复领域验证了效果。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的RAG系统主要关注性能提升，但忽视了循证医学原则。存在两个关键问题：1) 查询与检索证据之间缺乏PICO对齐；2) 重排序时未考虑证据等级层次。需要将EBM原则系统性地整合到RAG框架中。

Method: 1) 将PICO框架整合到知识图谱构建和检索中；2) 提出贝叶斯启发的重排序算法，根据证据等级校准排名分数，无需预定义权重；3) 在运动康复领域构建知识图谱（357,844节点，371,226边）和可重用基准（1,637个QA对）。

Result: 系统在运动康复领域表现优异：nugget覆盖度0.830，答案忠实度0.819，语义相似度0.882，PICOT匹配准确率0.788。五位专家临床医生在5点李克特量表上给出4.66-4.84的高分（事实准确性、忠实度、相关性、安全性、PICO对齐）。

Conclusion: 提出的EBM适应策略显著提升了检索和答案质量，且可迁移到其他临床领域。发布的资源有助于解决运动康复领域RAG数据集的稀缺问题。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [17] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文研究了多语言大语言模型生成反事实解释的能力，发现翻译生成的反事实比直接生成的有效性更高但修改更多，多语言反事实数据增强对低资源语言效果更好，但生成质量限制了模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 反事实解释是理解模型行为的重要方法，大语言模型在生成英文反事实上表现出色且具备多语言能力，但它们在生成多语言反事实方面的效果尚不清楚，需要系统研究。

Method: 1) 对6种语言进行自动评估，比较直接生成和通过英文翻译生成的反事实；2) 分析高资源欧洲语言反事实的编辑模式；3) 识别跨语言反事实生成中的错误类型；4) 评估多语言反事实数据增强对模型性能的影响。

Result: 1) 翻译生成的反事实比直接生成的有效性更高，但需要更多修改，且质量仍不及原始英文反事实；2) 高资源欧洲语言的编辑模式相似，表明跨语言扰动遵循共同策略；3) 识别出四种跨语言一致的错误类型；4) 多语言反事实数据增强比跨语言增强带来更大性能提升，尤其对低资源语言，但生成不完美限制了性能提升。

Conclusion: 大语言模型在多语言反事实生成方面仍有局限，翻译方法能提高有效性但牺牲简洁性，多语言数据增强对低资源语言有益，但需要改进生成质量以获得更好的模型性能和鲁棒性。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [18] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

Relevance: 75.0

TL;DR: BERT-JEPA (BEPA) 在BERT风格模型中引入JEPA训练目标，解决[CLS]嵌入空间坍缩问题，将其转化为语言无关空间，提升多语言基准性能。


<details>
  <summary>Details</summary>
Motivation: BERT模型的[CLS]嵌入空间在多语言任务中容易坍缩，导致性能受限。JEPA作为一种新兴的自监督训练技术，在多个领域显示出潜力，可以用于改进BERT模型的多语言表示能力。

Method: 在BERT风格模型中添加JEPA训练目标，通过联合嵌入预测架构来对抗[CLS]嵌入空间的坍缩，将其转化为语言无关的表示空间。

Result: BERT-JEPA在多语言基准测试中表现出性能提升，证明了JEPA目标能够有效改善BERT模型的多语言表示能力。

Conclusion: JEPA训练目标可以成功应用于BERT模型，解决[CLS]嵌入空间坍缩问题，创建语言无关表示，提升多语言任务性能。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [19] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

Relevance: 75.0

TL;DR: Geo-R：基于强化学习的检索无关图像地理定位框架，通过结构化地理推理和坐标对齐奖励提升定位精度和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图像地理定位中依赖合成推理标注或外部图像检索，限制了可解释性和泛化能力。需要一种检索无关、基于结构化推理的方法来提升地理定位的准确性和透明度。

Method: 1) 提出Chain of Region：基于规则的分层推理范式，将GPS坐标映射到地理实体（国家、省份、城市等），无需模型生成或合成标签；2) 引入轻量级强化学习策略，基于Haversine距离的坐标对齐奖励，通过空间有意义的反馈优化预测

Result: 在多个基准测试中验证了Geo-R的有效性，实现了更高的定位精度、更强的泛化能力和更透明的推理过程，建立了检索无关图像地理定位的新范式

Conclusion: Geo-R通过结合结构化地理推理和直接空间监督，为可扩展、可解释的图像地理定位提供了新的解决方案，在准确性和透明度方面均有显著提升

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [20] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出Embedding Consistency Regulation (ECR)框架，通过语义锚点保持紧凑模型嵌入空间的结构一致性，解决模型压缩中的语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 紧凑模型在压缩过程中经常丢失嵌入空间的结构，特别是在容量受限或多语言场景下。现有压缩方法只关注表层输出对齐，未能保持底层流形结构，导致语义漂移和任务行为偏差。

Method: ECR框架从教师模型嵌入中提取语义锚点（离线计算），然后让紧凑模型学习在这些锚点周围保持一致的几何结构，不依赖匹配logits或内部特征。推理时仅添加小型投影步骤，不改变解码架构或运行时行为。

Result: 在100K多语言语料实验中，ECR稳定训练并保持跨任务和语言的语义结构，产生更紧凑且任务对齐的表示空间，使低容量模型能学习比传统基线更清晰的流形。ECR无需教师输出，与蒸馏兼容但独立。

Conclusion: ECR帮助紧凑模型更好地遵循任务要求，使其在严格效率或隐私限制下更容易部署，通过保持嵌入空间结构一致性解决模型压缩中的根本问题。

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [21] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

Relevance: 75.0

TL;DR: JourneyBench是一个用于评估客服场景中策略感知AI代理的基准测试，通过图表示生成多样化支持场景，并提出用户旅程覆盖率指标来衡量策略遵循能力。


<details>
  <summary>Details</summary>
Motivation: 传统IVR系统依赖固定脚本，缺乏处理复杂策略驱动任务的灵活性。虽然LLM代理提供了有前景的替代方案，但评估其遵循业务规则和真实工作流程的能力仍是一个开放挑战。现有基准主要关注工具使用或任务完成，忽略了代理遵循多步骤策略、处理任务依赖关系以及对不可预测用户行为的鲁棒性。

Method: 引入JourneyBench基准，利用图表示生成多样化、真实的客服场景。提出用户旅程覆盖率分数作为衡量策略遵循的新指标。评估了两种代理设计：静态提示代理（SPA）和动态提示代理（DPA），后者显式建模策略控制。

Result: 在三个领域的703个对话中，DPA显著提升了策略遵循能力，甚至允许较小的模型（如GPT-4o-mini）超越更强大的模型（如GPT-4o）。结构化编排对策略遵循至关重要。

Conclusion: JourneyBench作为关键资源，能够推动AI驱动的客服系统超越IVR时代的限制。结构化编排（如DPA）对于确保LLM代理在复杂客服场景中遵循业务策略至关重要。

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [22] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

Relevance: 65.0

TL;DR: Pat-DEVAL：首个针对专利说明书的多维评估框架，使用LLM-as-a-judge范式，引入Chain-of-Legal-Thought机制，显著提升专利法律合规性评估效果


<details>
  <summary>Details</summary>
Motivation: 现有专利自动撰写评估方法无法评估长篇结构连贯性和特定法律合规性，需要专门针对专利说明书的评估框架

Method: 提出Pat-DEVAL框架，基于LLM-as-a-judge范式，引入Chain-of-Legal-Thought法律约束推理机制，进行序列化专利法特定分析

Result: 在Pap2Pat-EvalGold数据集上达到0.69的皮尔逊相关系数，显著优于基线指标和现有LLM评估器；法律专业合规性相关性达0.73

Conclusion: 通过显式注入法律约束，Pat-DEVAL为自动专利撰写系统提供了同时确保技术正确性和法律合规性的方法论基础

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [23] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型，将结构和时序推理能力迁移到轻量级学生模型，在推理精度和计算效率间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理模型参数量大、计算密集，导致硬件成本高、能耗大，难以部署在资源受限的实时推理平台。现有压缩和蒸馏技术主要针对静态知识图谱，无法充分捕捉TKG中的时序依赖关系，导致推理性能下降。

Method: 提出专门针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型指导蒸馏过程，有效迁移结构和时序推理能力到轻量级学生模型。通过整合大规模公共知识和任务特定时序信息，增强学生模型建模时序动态的能力，同时保持紧凑高效架构。

Result: 在多个公开基准数据集上的广泛实验表明，该方法持续优于强基线，在推理精度、计算效率和实际部署性之间实现了有利的权衡。

Conclusion: 提出的蒸馏框架有效解决了TKG推理模型的效率和部署问题，通过大语言模型指导的蒸馏实现了轻量级模型的高性能时序推理，为资源受限平台上的实时推理应用提供了可行方案。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [24] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

Relevance: 65.0

TL;DR: DepFlow是一个三阶段抑郁条件文本转语音框架，通过对抗训练学习说话人和内容不变的抑郁嵌入，然后通过流匹配TTS模型注入这些嵌入，最后通过原型映射机制实现抑郁严重程度的平滑控制。该框架用于构建伪装抑郁增强数据集，提升抑郁检测模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前抑郁检测数据集（如DAIC-WOZ）存在语言情感与诊断标签的强耦合问题，导致模型学习语义捷径，在真实场景（如伪装抑郁）中鲁棒性不足。需要解决语义偏见，提升模型在语言内容与抑郁状态不匹配情况下的检测能力。

Method: 1. 抑郁声学编码器：通过对抗训练学习说话人和内容不变的抑郁嵌入，实现有效解耦同时保持抑郁判别能力。2. 流匹配TTS模型：使用FiLM调制将抑郁嵌入注入合成过程，控制抑郁严重程度同时保持内容和说话人身份。3. 原型映射机制：提供跨抑郁连续体的平滑可解释操作。基于此构建伪装抑郁增强数据集（CDoA）。

Result: 抑郁声学编码器ROC-AUC达到0.693。CDoA数据集在三种抑郁检测架构上分别提升macro-F1 9%、12%和5%，优于传统增强策略。DepFlow提供了可控合成平台，用于对话系统和基于模拟的评估。

Conclusion: DepFlow通过解耦抑郁声学特征与语言内容，有效缓解了抑郁检测中的语义偏见问题，提升了模型在伪装抑郁等真实场景中的鲁棒性。该框架不仅增强了检测性能，还为临床数据有限情况下的模拟评估提供了可控合成平台。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [25] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文提出了超关系时序知识广义超图(HTKGHs)来扩展传统时序知识图谱的表达能力，支持多实体复杂事实表示，并基于POLECAT数据库构建了htkgh-polecat数据集，评估了LLMs在复杂时序关系预测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统时序知识图谱(TKGs)和超关系时序知识图谱(HTKGs)虽然能表示简单时序关系，但缺乏表达复杂事实的能力，特别是无法支持两个以上主要实体的时序事实，而现实世界的地缘政治事件常常涉及多个实体。需要一种更强大的表示方法来支持复杂时序关系的预测。

Method: 1) 提出HTKGHs作为HTKGs的泛化，提供形式化定义并保持向后兼容性；2) 基于POLECAT全球事件数据库构建htkgh-polecat数据集；3) 在关系预测任务上对流行的LLMs进行基准测试和分析。

Result: 建立了HTKGHs的形式化框架，创建了包含复杂地缘政治事件的htkgh-polecat数据集，并通过基准测试揭示了LLMs在复杂时序关系预测任务中的适应性和能力。

Conclusion: HTKGHs为复杂时序事实提供了更丰富的表示能力，htkgh-polecat数据集为评估LLMs在复杂时序推理任务上的表现提供了基准，研究为LLMs在地缘政治预测等复杂场景中的应用提供了洞见。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [26] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

Relevance: 65.0

TL;DR: 本文对三种轻量级Transformer模型（DistilBERT、MiniLM、ALBERT）在三个企业NLP任务（情感分类、新闻分类、仇恨言论检测）上进行了比较分析，评估了准确性和效率指标，发现不同模型在不同维度上各有优势。


<details>
  <summary>Details</summary>
Motivation: 企业NLP应用对高效、轻量级模型处理多领域文本自动化任务的需求日益增长，需要了解不同轻量级Transformer模型在准确性、效率等方面的权衡，为企业部署提供指导。

Method: 使用IMDB、AG News和Measuring Hate Speech三个数据集，在客户情感分类、新闻主题分类、毒性及仇恨言论检测三个任务上，比较DistilBERT、MiniLM和ALBERT三种轻量级Transformer模型。评估指标包括准确性（准确率、精确率、召回率、F1分数）和效率（模型大小、推理时间、吞吐量、内存使用）。

Result: 没有单一模型在所有性能维度上占优：ALBERT在多个领域获得最高任务特定准确率；MiniLM在推理速度和吞吐量上表现最佳；DistilBERT在任务间保持最一致的准确性，同时保持有竞争力的效率。

Conclusion: 研究揭示了准确性与效率之间的权衡，建议：对延迟敏感的企业应用选择MiniLM，需要平衡性能的选择DistilBERT，资源受限环境选择ALBERT。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [27] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文探讨了LLMs作为检验语言意义理论的实证平台，对比了社会建构主义（语言游戏）和数学导向的语义场理论，分析了Transformer架构如何体现这些概念，并提出了数学结构与语言游戏互补的框架。


<details>
  <summary>Details</summary>
Motivation: LLMs为检验长期存在的语言意义理论提供了新的实证环境。作者旨在对比两种主要方法：社会建构主义（语言游戏）和数学导向的语义场理论，以理解LLMs成功捕捉语义规律的原因及其在语用推理方面的局限性。

Method: 作者基于先前工作，形式化了词汇场和语言场作为连续语义空间中的交互结构。然后分析了Transformer架构的核心特性（分布式表示、注意力机制、嵌入空间的几何规律性）如何与这些概念相关。

Result: LLMs在捕捉语义规律方面的成功支持了语言具有底层数学结构的观点，而它们在语用推理和上下文敏感性方面的持续局限性与哲学语言使用理论强调的社会基础重要性一致。

Conclusion: 数学结构和语言游戏可以理解为互补而非竞争的观点。这一框架澄清了纯统计语言模型的范围和限制，并为理论指导的AI架构提供了新方向。

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [28] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

Relevance: 65.0

TL;DR: 论文分析基于依赖关系的规则方法进行原子句子提取，识别复杂句法结构（如关系从句、状语从句等）对提取性能的影响


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的原子句子提取方法缺乏可解释性，无法揭示哪些具体语言结构导致提取失败。需要系统分析复杂句法结构对提取性能的影响。

Method: 使用spaCy实现基于依赖关系的提取规则，在WikiSplit数据集上生成100个黄金标准原子句子集，使用ROUGE和BERTScore评估性能，分析不同句法结构的影响。

Result: 系统达到ROUGE-1 F1=0.6714，ROUGE-2 F1=0.478，ROUGE-L F1=0.650，BERTScore F1=0.5898，表明中高水平的对齐。挑战性结构包括关系从句、同位语、并列谓语、状语从句和被动结构。

Conclusion: 基于规则的提取方法具有合理准确性但对句法复杂性敏感，为理解复杂句子分解提供了可解释的见解。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [29] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文提出了一个四轴框架来分析多跳问答系统的执行过程，将检索-推理过程作为分析单元，系统化比较不同模型家族的程序选择。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答系统中，检索-推理过程往往被隐式处理，使得不同模型家族的程序选择难以比较。需要系统化的框架来分析执行过程。

Method: 提出了一个四轴分析框架：(A)整体执行计划，(B)索引结构，(C)下一步控制策略和触发机制，(D)停止/继续标准。使用该框架对代表性多跳问答系统进行映射分析。

Result: 在标准基准测试（如HotpotQA、2WikiMultiHopQA、MuSiQue）上综合了报告的消融实验和趋势，突出了有效性、效率和证据忠实度之间的权衡。

Conclusion: 提出了检索-推理代理的开放挑战，包括结构感知规划、可迁移的控制策略以及在分布偏移下的鲁棒停止机制。

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [30] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出Physio-DPO框架，通过物理信息对齐将蛋白质语言模型与热力学稳定性结合，解决蛋白质设计中的结构幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型蛋白质语言模型在生成蛋白质设计方面潜力巨大，但经常产生结构幻觉，生成的语言可能性高但热力学不稳定的序列。现有对齐方法（如DPO）将偏好建模为二元标签，忽略了物理能量景观的连续结构

Method: 提出Physio-DPO物理信息对齐框架，引入幅度感知目标函数，根据天然结构与物理扰动硬负样本之间的能量差距来缩放优化更新

Result: Physio-DPO在实验中始终优于SFT、PPO和标准DPO等基线方法，将自一致性RMSD降低到1.28 Å，可折叠性提高到92.8%。定性分析显示Physio-DPO有效缓解结构幻觉，恢复疏水核心堆积和氢键网络等生物物理相互作用

Conclusion: Physio-DPO成功将蛋白质语言模型与热力学稳定性对齐，显著提高生成蛋白质的折叠性和结构准确性，为蛋白质设计提供了更可靠的生成方法

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [31] [Learning Speech Representations with Variational Predictive Coding](https://arxiv.org/abs/2601.00100)
*Sung-Lin Yeh,Peter Bell,Hao Tang*

Main category: eess.AS

Relevance: 65.0

TL;DR: 该论文揭示了HuBERT目标函数背后的预测编码原理，并基于此提出了两个简单改进，在多个语音下游任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管HuBERT是学习语音表示的最佳目标函数，但其发展停滞不前。作者认为缺乏理论基础阻碍了发展，因此探索HuBERT背后的原理，发现预测编码是其理论基础。

Method: 1. 从变分角度提出预测编码作为HuBERT的理论基础；2. 基于该理论框架，提出了两个简单的参数化和优化改进；3. 建立了与APC、CPC、wav2vec、BEST-RQ等其他目标的联系。

Result: 改进后的预训练方法在四个下游任务上带来显著提升：音素分类、基频跟踪、说话人识别和自动语音识别，验证了预测编码解释的重要性。

Conclusion: 预测编码为HuBERT目标函数提供了理论基础，基于该理论的改进能够显著提升语音表示学习性能，为未来语音表示学习的发展提供了新方向。

Abstract: Despite being the best known objective for learning speech representations, the HuBERT objective has not been further developed and improved. We argue that it is the lack of an underlying principle that stalls the development, and, in this paper, we show that predictive coding under a variational view is the principle behind the HuBERT objective. Due to its generality, our formulation provides opportunities to improve parameterization and optimization, and we show two simple modifications that bring immediate improvements to the HuBERT objective. In addition, the predictive coding formulation has tight connections to various other objectives, such as APC, CPC, wav2vec, and BEST-RQ. Empirically, the improvement in pre-training brings significant improvements to four downstream tasks: phone classification, f0 tracking, speaker recognition, and automatic speech recognition, highlighting the importance of the predictive coding interpretation.

</details>


### [32] [A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies](https://arxiv.org/abs/2601.00510)
*Jetlir Duraj,Ishita Khan,Kilian Merkelbach,Mehran Elyasi*

Main category: cs.IR

Relevance: 65.0

TL;DR: 提出一种结合树搜索与LLM语义评分的CoT方法，用于电商搜索查询分类，相比基于嵌入的方法表现更好，并能检测层次分类体系中的问题


<details>
  <summary>Details</summary>
Motivation: 电商搜索依赖层次分类体系，正确分类用户查询不仅能定位正确库存空间，还能开启多种意图理解能力，对约束检索物品和提升搜索结果相关性至关重要

Method: 探索新颖的思维链范式，结合简单树搜索与LLM语义评分，同时提出可扩展到百万级查询的LLM方法

Result: CoT方法在人工标注的查询-类别对、相关性测试和基于LLM的参考方法评估中，表现优于基于嵌入的查询类别预测基准方法

Conclusion: CoT方法能有效解决电商分类体系中的查询分类问题，并能检测层次分类体系中的问题，同时提出了可扩展的LLM方案

Abstract: Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries.

</details>


### [33] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

Relevance: 45.0

TL;DR: 该论文提出了一种针对职业教育与培训领域历史数字化文档的噪声感知命名实体识别方法，通过合成OCR错误注入、迁移学习和多阶段微调来提高模型在噪声条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 职业教育与培训领域的历史数字化文档存在OCR引起的噪声问题，传统NER方法在这种噪声条件下性能下降。需要开发能够在噪声环境下准确识别多种实体类型的鲁棒NER方法。

Method: 提出噪声感知训练方法：1）合成注入OCR错误；2）使用迁移学习；3）多阶段微调策略；4）系统比较三种互补策略：在噪声数据、干净数据和人工数据上训练。

Result: 实验结果表明，领域特定和噪声感知的微调显著提高了在噪声条件下的鲁棒性和准确性。该方法能够识别职业教育文档中的多种实体类型。

Conclusion: 该方法首次在职业教育文档中实现多实体类型识别，虽然针对德语文档但可迁移到任意语言。公开代码支持可重复的领域特定噪声感知NER研究。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [34] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

Relevance: 45.0

TL;DR: 提出基于CTC架构的轻量级语言无关多语言ASR系统，采用层次化LoRA-MoE框架，无需语言标签即可实现端到端解码，显著提升低资源场景下的解码效率。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言ASR模型（如Whisper）计算和延迟成本高，难以部署在资源受限的边缘设备上。需要开发轻量级、语言无关且高效的多语言ASR系统。

Method: 提出语言无关层次化LoRA-MoE（HLoRA）框架，集成到mHuBERT-CTC模型中。包含：1）多语言共享LoRA学习语言不变声学表示；2）语言特定LoRA专家建模语言依赖特征；3）基于LID后验的LoRA路由机制，实现无需语言标签的单次解码。

Result: 在MSR-86K和MLC-SLM 2025挑战数据集上的实验表明，HLoRA仅通过单次解码就能达到与最先进两阶段推理方法相当的性能，显著提升低资源多语言ASR的解码效率。

Conclusion: HLoRA框架为资源受限环境提供了高效的多语言ASR解决方案，通过语言无关的层次化设计实现了计算效率和性能的平衡，特别适合边缘设备部署。

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [35] [StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices](https://arxiv.org/abs/2601.00197)
*Shaswat Mohanty*

Main category: cs.CE

Relevance: 45.0

TL;DR: 本文提出增强版StockBot架构，系统评估注意力、卷积和循环时间序列预测模型，发现精心构建的普通LSTM在金融时间序列预测中表现最佳，强调了在数据有限情况下架构归纳偏置的重要性。


<details>
  <summary>Details</summary>
Motivation: 金融市场预测面临复杂的时间依赖、非线性动态和高波动性等挑战，需要系统评估不同时间序列预测模型在金融领域的实际表现。

Method: 基于早期循环神经网络框架，提出增强版StockBot架构，在统一实验设置下系统评估基于注意力的模型、卷积模型和循环时间序列预测模型，使用默认超参数进行训练比较。

Result: 经验评估表明，精心构建的普通LSTM在预测准确性和买卖决策稳定性方面优于注意力基和Transformer模型，特别是在单日离散化数据有限的情况下。

Conclusion: 循环序列模型在金融时间序列预测中具有鲁棒性和数据效率优势，强调了在数据有限的市场预测任务中架构归纳偏置的重要性。

Abstract: Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility. Building on our earlier recurrent neural network framework, we present an enhanced StockBot architecture that systematically evaluates modern attention-based, convolutional, and recurrent time-series forecasting models within a unified experimental setting. While attention-based and transformer-inspired models offer increased modeling flexibility, extensive empirical evaluation reveals that a carefully constructed vanilla LSTM consistently achieves superior predictive accuracy and more stable buy/sell decision-making when trained under a common set of default hyperparameters. These results highlight the robustness and data efficiency of recurrent sequence models for financial time-series forecasting, particularly in the absence of extensive hyperparameter tuning or the availability of sufficient data when discretized to single-day intervals. Additionally, these results underscore the importance of architectural inductive bias in data-limited market prediction tasks.

</details>


### [36] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

Relevance: 35.0

TL;DR: 该论文系统分析了对话情感识别（ERC）中的架构选择，发现对话上下文是关键因素，仅需最近10-30轮对话即可获得90%性能增益，而外部情感词典无帮助。同时通过话语标记分析揭示了情感与话语结构的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管对话情感识别已达到较高准确率，但仍存在两个关键空白：1）缺乏对哪些架构选择真正重要的理解；2）缺少将识别与生成联系起来的语言学分析。作者旨在通过系统分析IEMOCAP数据集来填补这些空白。

Method: 采用系统分析方法：1）在识别方面，进行严格的消融研究，使用10个随机种子评估不同架构组件；2）在语言学分析方面，分析5,286个话语标记出现情况，研究情感与标记位置的关系，并进行统计显著性检验。

Result: 识别方面：1）对话上下文至关重要，性能在最近10-30轮对话中快速饱和；2）分层句子表示在话语级别有帮助，但提供对话上下文后此优势消失；3）外部情感词典无增益。使用简单架构获得82.69%（4类）和67.07%（6类）加权F1。语言学分析：发现情感与话语标记位置显著相关，悲伤话语的左边缘标记使用率（21.9%）低于其他情感（28-32%）。

Conclusion: 对话上下文是对话情感识别的关键因素，简单的因果上下文架构即可达到最佳性能。悲伤话语由于缺乏明确的语用信号，最需要上下文进行消歧，这与语言学分析中发现的悲伤话语左边缘标记使用减少的现象一致。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [37] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

Relevance: 35.0

TL;DR: JP-TL-Bench是一个轻量级开源基准，用于指导日英翻译系统的迭代开发，专注于区分"哪个翻译更好"而非"翻译是否可接受"，通过LLM进行参考无关的成对比较评估。


<details>
  <summary>Details</summary>
Motivation: 日英翻译中，礼貌、隐含意义、省略和语域等细微选择对自然度感知有强烈影响，需要能够区分"哪个好翻译更好"的评估方法，而非仅仅判断翻译是否可接受。

Method: 使用参考无关的成对LLM比较协议，将候选模型与固定的版本化锚点集进行比较，通过Bradley-Terry模型聚合结果，报告胜率和从拟合对数强度推导的0-10标准化"LT"分数。

Result: 建立了结构稳定的评估框架，由于每个候选模型都针对相同的冻结锚点集进行评分，在相同基础集、评判器和聚合代码下，分数具有结构稳定性。

Conclusion: JP-TL-Bench提供了一个可靠且经济的LLM评判协议，专门针对日英翻译的细微差别，支持翻译系统的迭代开发和质量改进。

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [38] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出了judgeWEL数据集，用于卢森堡语命名实体识别，通过结合维基百科/维基数据和LLM验证的新颖流程自动标注


<details>
  <summary>Details</summary>
Motivation: 低资源语言的数据集构建是NLP的主要瓶颈之一，资源稀缺和语言特性使得大规模标注成本高且可能不一致。卢森堡语作为代表性低资源语言，需要更高效的数据集构建方法。

Method: 1) 利用维基百科内部链接和维基数据条目推断实体类型，生成弱监督标注；2) 使用多个LLM验证和筛选高质量标注句子；3) 构建自动标注和验证的完整流程

Result: 生成的judgeWEL数据集比现有卢森堡语NER数据集大约5倍，实体类别覆盖更广泛和平衡，为多语言和低资源NER研究提供了重要资源

Conclusion: 提出的基于维基百科/维基数据弱监督和LLM验证的流程能有效构建低资源语言NER数据集，为类似语言的数据集构建提供了可行方案

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [39] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

Relevance: 35.0

TL;DR: 该研究评估了在加拿大跨省癌症登记中应用Transformer NLP模型的可行性，通过微调和集成策略显著减少了漏检癌症病例，并提出了隐私保护的工作流程。


<details>
  <summary>Details</summary>
Motivation: 癌症登记依赖病理报告作为主要诊断来源，但人工提取资源密集且导致数据延迟。现有基于Transformer的NLP系统在跨司法管辖区（报告规范不同）的泛化能力尚不清楚，需要评估模型在不同省份癌症监测中的适应性。

Method: 研究采用跨省评估方法，将BCCRTron（不列颠哥伦比亚省癌症登记处开发的领域适应Transformer）和GatorTron（生物医学Transformer）应用于纽芬兰与拉布拉多省癌症登记。使用约104,000份（Tier 1：癌症vs非癌症）和22,000份（Tier 2：可报告vs不可报告）去标识化病理报告进行微调，采用互补的概要式和诊断聚焦报告部分输入管道，并通过保守OR集成组合两个模型。

Result: 微调后模型在跨省测试中保持高性能，证明在一个司法管辖区预训练的Transformer可以通过适度微调本地化到另一个管辖区。集成模型显著提升性能：Tier 1召回率达到0.99，漏检癌症从单独模型的48和54例减少到24例；Tier 2召回率0.99，漏检可报告癌症从54和46例减少到33例。

Conclusion: 结合互补文本表示的集成方法能显著减少癌症登记NLP中的漏检病例并改善错误覆盖。研究实现了隐私保护工作流程（仅共享模型权重），支持可互操作的NLP基础设施，为未来泛加拿大癌症病理和登记工作流程的基础模型奠定基础。

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出使用强化学习来增强多模态大语言模型的视觉推理能力，通过设计多种奖励函数来激励模型生成更长的、结合视觉信息的推理链，从而解决视觉谜题等需要精确视觉感知的任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在生成推理链时缺乏对视觉信息的有效整合，这限制了它们解决需要精确视觉感知的任务（如视觉谜题）的能力。研究表明视觉感知是这类任务的关键瓶颈，将图像转换为文本描述能显著提升性能，但需要更有效的方法来激励模型进行视觉推理。

Method: 采用奖励驱动的强化学习机制，设计了六种针对不同推理方面的奖励函数（包括图像理解、思考步骤和答案准确性）。使用组相对策略优化（GRPO）方法，明确激励更长、结构化的推理，并防止模型绕过视觉信息。

Result: 在Qwen-2.5-VL-7B模型上实现了5.56%的性能提升，在领域内和领域外设置中都获得了一致的增益。实验表明，将图像转换为文本描述能使Claude 3.5和Claude 3.7分别获得26.7%和23.6%的性能提升。

Conclusion: 强化学习是增强多模态大语言模型视觉推理能力的有效方法，通过精心设计的奖励函数可以激励模型生成更长的、结合视觉信息的推理链，从而显著提升在需要精确视觉感知的任务上的性能。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [41] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

Relevance: 85.0

TL;DR: FaithSCAN：一种轻量级网络，通过利用VLM的内部信号（包括token级解码不确定性、中间视觉表示和跨模态对齐特征）来检测VQA中的幻觉，无需昂贵的人工标注即可实现监督训练。


<details>
  <summary>Details</summary>
Motivation: VQA中的忠实性幻觉（视觉语言模型产生流畅但视觉上无根据的答案）严重削弱了其在安全关键应用中的可靠性。现有检测方法存在计算开销大、依赖外部资源质量、仅捕捉有限不确定性等局限性。

Method: 提出FaithSCAN轻量级网络，利用VLM的丰富内部信号：token级解码不确定性、中间视觉表示、跨模态对齐特征。通过分支证据编码和不确定性感知注意力融合这些信号。扩展LLM-as-a-Judge范式到VQA幻觉检测，提出低成本策略自动生成模型依赖的监督信号。

Result: 在多个VQA基准测试中，FaithSCAN在效果和效率上都显著优于现有方法。深入分析显示幻觉源于视觉感知、跨模态推理和语言解码的系统性内部状态变化，不同内部信号提供互补的诊断线索。

Conclusion: FaithSCAN通过有效利用VLM内部信号解决了VQA幻觉检测的关键挑战，为多模态幻觉的底层原因提供了新见解，不同VLM架构的幻觉模式存在差异。

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [42] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出CRoPS框架，通过选择性移除关键文本token构建幻觉模型，结合广义对比解码来缓解大型视觉语言模型的幻觉问题，无需额外训练即可显著提升CHAIR分数20%


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)存在生成幻觉内容的严重问题，影响实际应用的可靠性。现有免训练方法存在两个局限：(1) 对幻觉来源的假设过于狭窄；(2) 在生成后期效果下降，而后期正是幻觉最容易发生的阶段

Method: 提出CRoPS框架：1) 构建幻觉模型：通过选择性移除关键文本token来捕捉幻觉效应；2) 广义对比解码：整合多个幻觉模型来代表不同的幻觉来源，通过对比原始模型和幻觉模型的输出来抑制幻觉生成

Result: 在CHAIR分数上提升20%，在六个基准测试和三个LVLM家族上都取得了一致的性能提升，优于现有的免训练方法

Conclusion: CRoPS是一个有效的免训练幻觉缓解框架，通过更全面地建模幻觉来源并解决生成后期的幻觉问题，显著提升了LVLMs的可靠性

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [43] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: TeleWorld是一个实时多模态4D世界建模框架，通过生成-重建-引导范式统一视频生成、动态场景重建和长期世界记忆，实现空间、时间和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在实时交互、长时程一致性和动态场景持久记忆方面存在局限，阻碍了其发展为实用的世界模型。需要一种能够统一视频生成、动态重建和长期记忆的闭环系统。

Method: 提出生成-重建-引导范式：生成视频流连续重建为动态4D时空表示，再引导后续生成。采用自回归扩散视频模型，增强Macro-from-Micro Planning（MMPL）层次规划方法减少误差累积，结合高效的Distribution Matching Distillation（DMD）实现实时合成。

Result: TeleWorld在静态和动态世界理解、长期一致性和实时生成效率方面表现优异，实现了动态对象建模和静态场景表示在统一4D框架中的无缝集成。

Conclusion: 该框架是迈向实用、交互式和计算可访问世界模型的重要一步，为多模态生成和具身智能提供了记忆增强的世界模型基础。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [44] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出了Spatial4D-Bench，一个包含约40,000个问答对、覆盖18个任务的4D空间智能基准测试，用于评估多模态大语言模型在4D空间推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 人类天生具备4D空间智能（感知物体随时间的变化），但现有空间智能基准测试规模小、多样性有限。本文旨在评估多模态大语言模型能否达到人类水平的4D空间智能。

Method: 构建了Spatial4D-Bench基准测试，包含约40,000个问答对，覆盖18个明确定义的任务，系统性地组织为6个认知类别：物体理解、场景理解、空间关系理解、时空关系理解、空间推理和时空推理。

Result: 在Spatial4D-Bench上评估了各种开源和专有的最先进MLLM，发现它们在多种4D空间推理方面存在显著局限性，如路径规划、动作识别和物理合理性推理。

Conclusion: 当前MLLM在4D空间智能方面与人类水平仍有较大差距，Spatial4D-Bench为社区提供了有价值的见解，并有望促进开发更强大的MLLM。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [45] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

Relevance: 75.0

TL;DR: CPPO是一种用于微调视觉语言模型的对比感知策略优化方法，通过熵移检测感知标记，并引入对比感知损失来增强感知能力，无需额外模型或数据。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在语言模型推理方面取得了进展，但将其扩展到多模态推理需要同时改进感知和推理能力。现有方法主要使用显式感知奖励，但存在难以分离感知标记与推理标记、需要额外LLM或真实数据、强制分离感知与推理、或对所有输出标记不加区分地应用奖励等问题。

Method: CPPO通过扰动输入图像时模型输出的熵移来检测感知标记，然后在RL目标函数中引入对比感知损失（CPL），该损失在信息保留扰动下强制一致性，在信息移除扰动下强制敏感性。

Result: 实验表明，CPPO超越了之前的感知奖励方法，同时避免了使用额外模型，使训练更加高效和可扩展。

Conclusion: CPPO提供了一种有效的方法来改进视觉语言模型的感知能力，无需复杂的感知奖励设计或额外模型，在多模态推理任务中表现出色。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [46] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

Relevance: 75.0

TL;DR: AEGIS是一个全面的多任务基准测试，用于评估统一多模态模型在视觉理解、生成、编辑和交错生成等任务中的世界知识应用能力，包含1050个手动标注的问题，覆盖21个主题和6种推理类型，并提出了确定性检查表评估协议以提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性，仅提供孤立的单任务评估，诊断能力有限，无法全面评估统一多模态模型在不同任务中应用世界知识的能力。需要更全面的多任务基准来填补这一空白。

Method: 提出了AEGIS基准测试，包含1050个具有挑战性的手动标注问题，涵盖21个主题（STEM、人文、日常生活等）和6种推理类型。同时提出了确定性检查表评估协议，用原子化的"是/否"判断替代模糊的提示式评分，提高评估可靠性。

Result: 实验显示大多数统一多模态模型存在严重的世界知识缺陷，性能在复杂推理任务中显著下降。简单的插件式推理模块可以部分缓解这些弱点，这为未来研究指明了方向。

Conclusion: 世界知识推理是统一多模态模型发展的关键前沿领域，需要更全面的评估方法和改进的推理能力。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [47] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该研究提出了一种结合输入数据分布偏移检测和输出置信度指标的框架，用于监控医疗视觉语言模型在数据分布变化下的性能退化，开发了DomainSAT工具箱进行系统分析。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型部署后，当输入数据分布发生变化时，性能可能会退化。检测这种性能退化对临床可靠性至关重要，但对于没有标注数据的大型预训练VLM来说具有挑战性。

Method: 1) 开发DomainSAT工具箱，集成代表性偏移检测算法，用于系统分析输入数据分布变化；2) 研究基于输出的监控方法，引入无需标签的置信度指标来直接捕捉模型预测置信度的变化；3) 在病理学大规模数据集上进行肿瘤分类实验，结合输入数据偏移检测和输出置信度指标。

Result: 输入数据偏移检测能有效识别分布变化并提供早期诊断信号，但不总是对应实际性能退化。基于置信度的指标与性能退化密切相关，可作为输入偏移检测的有效补充。两者结合能更可靠地检测和解释VLM在数据偏移下的性能退化。

Conclusion: 结合输入数据偏移检测和输出置信度指标提供了一个实用且互补的框架，用于监控数字病理学中基础模型的可靠性，有助于提高医疗AI系统的临床可靠性。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [48] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出一个基于多模态大语言模型的端到端工作流，用于自动评分手写STEM考试，保留标准考试流程，通过多阶段设计确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能捕捉开放式推理和图表，但人工评分速度慢、难以扩展。需要自动化解决方案来减轻教师负担，同时保持评分质量。

Method: 使用多模态LLMs处理扫描手写试卷，仅需教师提供手写参考答案和评分规则。采用多阶段设计：格式/存在性检查防止空白答案评分、独立评分器集成、监督聚合、刚性模板和确定性验证，生成可审计的机器可解析报告。

Result: 使用GPT-5.2和Gemini-3 Pro作为后端，在斯洛文尼亚语的真实课程测验上评估，完整流程与教师评分的平均绝对差异约为8分，偏差低，手动审查触发率约17%。消融实验显示简化提示和移除参考答案会显著降低准确性并引入系统性过高评分。

Conclusion: 结构化提示和参考答案基础对于准确自动评分至关重要，提出的多阶段工作流能够可靠地自动化手写STEM考试评分，减轻教师负担。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [49] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

Relevance: 65.0

TL;DR: FCMBench-V1.0是一个金融信贷多模态基准测试，包含4,043张隐私合规图像和8,446个QA样本，评估视觉语言模型在信贷文档理解、决策推理和鲁棒性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在信贷风险评估和文档审查中的广泛应用，急需一个领域特定的基准测试，能够反映金融信贷应用中的实际文档和工作流程，包含信贷特定理解、真实世界鲁棒性，同时满足隐私合规要求。

Method: 通过封闭式合成-捕获流水线构建样本：手动合成带有虚拟内容的文档模板，并在内部捕获场景感知图像。评估框架包括感知、推理和鲁棒性三个维度，涵盖3个基础感知任务、4个信贷特定推理任务和10种真实世界采集伪影类型。

Result: 在评估的23个最先进视觉语言模型中：Gemini 3 Pro作为商业模型获得最佳F1分数（64.61%），Qwen3-VL-235B作为开源基线获得最佳分数（57.27%），而金融信贷特定模型Qfin-VL-Instruct获得最高总体分数（64.92%）。鲁棒性评估显示，即使表现最佳的模型在采集伪影下也会出现明显性能下降。

Conclusion: FCMBench能够有效区分现代视觉语言模型的性能差异和鲁棒性，为金融信贷领域的多模态AI评估提供了重要基准，同时解决了隐私合规和实际效用之间的平衡问题。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [50] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

Relevance: 65.0

TL;DR: TotalFM是一个放射学基础模型，通过器官分离概念高效学习3D-CT图像与语言表达的对应关系，在零样本器官级病变分类任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 放射学基础模型在处理3D-CT体数据时面临计算成本高的挑战，需要平衡计算效率与表征能力，以实现临床任务中的实际应用。

Method: 1) 通过分割技术和基于LLM的放射报告处理，自动创建器官体积-发现句子对；2) 结合VideoMAE的自监督预训练和基于体积-文本对的对比学习；3) 采用器官分离学习框架，利用14万系列的大规模数据集。

Result: 在零样本器官级病变分类中，相比CT-CLIP在83%器官上获得更高F1分数，相比Merlin在64%器官上表现更好；在零样本发现级病变分类中，相比Merlin在83%类别上获得更高AUROC；在放射报告生成任务中达到与现有VLM相当的性能。

Conclusion: 器官分离学习框架为3D-CT基础模型的实际应用提供了现实有效的设计指南，展示了在临床评估环境中的高泛化性能。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [51] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

Relevance: 65.0

TL;DR: S1-MMAlign是一个大规模多学科多模态数据集，包含1550万高质量图像-文本对，来自250万篇开放获取科学论文，涵盖物理、生物、工程等多个领域，通过AI增强管道提升科学图像与文本的对齐质量。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在通用领域取得了革命性进展，但在科学发现中的应用受到阻碍，主要因为复杂科学图像与稀疏文本描述之间存在深刻的语义鸿沟。现有科学多模态数据集存在对齐质量差的问题，限制了科学推理和跨模态理解的发展。

Method: 1) 构建大规模多学科数据集：从250万篇开放获取科学论文中提取1550万图像-文本对，涵盖多种视觉模态（实验装置、热图、显微图像等）。2) 开发AI就绪的语义增强管道：利用Qwen-VL多模态大模型系列，通过整合论文摘要和引用上下文来重新描述图像，解决原始科学标题对齐弱的问题。

Result: 技术验证表明增强显著提升数据质量：基于SciBERT的伪困惑度指标显示语义模糊性降低，CLIP分数显示图像-文本对齐提高了18.21%。数据集为AI for Science时代的科学推理和跨模态理解提供了基础资源。

Conclusion: S1-MMAlign通过大规模高质量科学多模态数据集和语义增强管道，为解决科学图像与文本之间的语义鸿沟提供了重要资源，将推动科学发现中的多模态学习和AI for Science发展。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [52] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出ActErase：一种无需训练的概念擦除方法，通过分析激活差异区域并动态替换输入激活，在文本到图像扩散模型中高效移除敏感概念


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法大多依赖数据密集且计算昂贵的微调，限制了实际应用。扩散模型在安全、版权和伦理方面存在风险，需要更高效的解决方案。

Method: 基于模型激活主要由通用概念组成、仅极小部分表示目标概念的观察，通过提示对分析识别激活差异区域，提取目标激活并在前向传播中动态替换输入激活。

Result: 在三个关键擦除任务（裸露内容、艺术风格、对象移除）上达到SOTA性能，有效保持模型生成能力，对对抗攻击具有强鲁棒性。

Conclusion: ActErase为扩散模型中的概念操作建立了新的即插即用范式，实现了轻量级但有效的概念擦除。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [53] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: DMDNet提出深度记忆解耦网络，利用深度感知扫描引导Mamba模型，结合深度协同状态空间模型和记忆专家补偿模块，解决图像反射分离问题，特别是在夜间场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图像反射分离方法仅依赖单张图像信息，当传输层和反射层对比度相似时容易混淆，夜间场景下这一问题更加严重。需要更有效的方法来处理这种具有挑战性的情况。

Method: 提出深度记忆解耦网络(DMDNet)：1) 深度感知扫描(DAScan)引导Mamba模型关注显著结构；2) 深度协同状态空间模型(DS-SSM)通过深度调制状态激活敏感性；3) 记忆专家补偿模块(MECM)利用跨图像历史知识提供层特定补偿；4) 构建夜间图像反射分离数据集(NightIRS)。

Result: DMDNet在白天和夜间场景下均优于现有最先进方法，特别是在夜间反射分离任务上表现突出，证明了所提方法的有效性。

Conclusion: DMDNet通过深度引导的状态空间建模和跨图像记忆补偿，有效解决了图像反射分离中的层混淆问题，特别是在夜间场景下，为这一具有挑战性的计算机视觉任务提供了新的解决方案。

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [54] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

Relevance: 65.0

TL;DR: FreeText是一个无需训练、即插即用的框架，通过利用扩散Transformer模型的内在机制来改进文本渲染，解决了多行布局、密集排版和长尾脚本（如中文）的精确文本渲染问题。


<details>
  <summary>Details</summary>
Motivation: 大规模文生图扩散模型在开放域合成方面表现出色，但在精确文本渲染方面仍存在困难，特别是对于多行布局、密集排版和长尾脚本（如中文）。现有解决方案通常需要昂贵的重新训练或严格的外部布局约束，这会降低美学质量并限制灵活性。

Method: FreeText将问题分解为"在哪里写"和"写什么"两个部分。对于"在哪里写"，通过读取来自内源性图像到文本注意力的token-wise空间归因来定位书写区域，使用sink-like tokens作为稳定的空间锚点，并通过拓扑感知细化产生高置信度掩码。对于"写什么"，引入频谱调制字形注入（SGMI），通过频域带通调制注入噪声对齐的字形先验，以增强字形结构并抑制语义泄漏。

Result: 在Qwen-Image、FLUX.1-dev和SD3变体上的广泛实验表明，在长文本基准测试、CVTG和作者提出的CLT-Bench上，文本可读性持续提升，同时很大程度上保持了语义对齐和美学质量，推理开销适中。

Conclusion: FreeText是一个无需训练、即插即用的框架，通过利用扩散Transformer模型的内在机制，有效解决了文本到图像生成中的精确文本渲染问题，特别是在处理复杂布局和长尾脚本时表现出色。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [55] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

Relevance: 65.0

TL;DR: SafeMo：一个可信的运动生成框架，通过最小化运动遗忘（MMU）策略在连续空间中实现安全的人体运动生成，避免了离散码本替换方法的缺陷，并提供了更好的安全-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散VQ-VAE码本替换的文本到运动（T2M）安全方法存在两个关键缺陷：1）替换被良性提示重用的码本条目会导致日常任务性能下降；2）离散标记方法引入量化和平滑度损失，导致伪影和抖动过渡。此外，现有文本到运动数据集包含不安全意图和对应运动，不适合安全驱动的机器学习。

Method: 提出SafeMo框架，集成最小化运动遗忘（MMU）的两阶段机器学习遗忘策略，在连续空间中实现安全人体运动生成，避免了码本损失并保持连续运动学特性。同时创建首个安全文本到运动数据集SafeMoVAE-29K，包含重写的安全文本提示和精炼的连续运动。

Result: 实验表明SafeMo在HumanML3D和Motion-X数据集上分别达到2.5倍和14.4倍更高的遗忘集FID，相比先前SOTA方法LCR表现出更强的遗忘性能，同时在安全提示上的良性性能相当或更好。

Conclusion: SafeMo通过连续空间中的机器学习遗忘策略有效解决了文本到运动生成中的安全问题，避免了离散码本替换方法的缺陷，在保持运动质量的同时实现了更好的安全控制。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [56] [Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection](https://arxiv.org/abs/2601.00777)
*Akanksha Chuchra,Shukesh Reddy,Sudeepta Mishra,Abhijit Das,Abhinav Dhall*

Main category: cs.SD

Relevance: 65.0

TL;DR: 探索多模态大语言模型在音频深度伪造检测中的应用，通过音频输入与文本提示结合，评估Qwen2-Audio-7B-Instruct和SALMONN模型在零样本和微调模式下的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型和多模态大语言模型在图像和视频深度伪造检测中表现出色，但它们在音频深度伪造检测方面的应用尚未充分探索。本研究旨在探索MLLMs在音频深度伪造检测中的潜力。

Method: 结合音频输入与多种文本提示作为查询，探索文本感知和上下文丰富的问答式提示（二元决策）。采用两种评估模式：(a) 零样本和(b) 微调。使用Qwen2-Audio-7B-Instruct和SALMONN两个MLLMs模型。

Result: 实验表明，音频与多提示方法结合是音频深度伪造检测的可行途径。模型在没有任务特定训练时表现不佳，难以泛化到域外数据，但在域内数据上通过少量监督即可获得良好性能。

Conclusion: 多模态大语言模型在音频深度伪造检测中具有潜力，特别是通过音频与多提示方法结合，但需要任务特定训练来提升泛化能力。

Abstract: While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.

</details>


### [57] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

Relevance: 45.0

TL;DR: LooC是一种新型向量量化方法，使用低维码本进行组合量化，通过重构码向量与特征向量的关系来扩展解空间，实现更紧凑的码本和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据和模型复杂度的增加，需要更高容量但更紧凑的向量量化方法。现有VQ方法在码本容量和紧凑性之间存在冲突，需要解决这一矛盾。

Method: 1) 引入参数高效的码本，将码向量视为特征向量中的低维组合单元进行组合；2) 采用参数无关的外推-内插机制增强特征平滑；3) 作为即插即用模块适用于现有VQ方法。

Result: 在不同任务、数据集和架构上的广泛评估表明，LooC优于现有VQ方法，在显著减小码本规模的同时实现了最先进的性能。

Conclusion: LooC成功解决了码本容量与紧凑性之间的冲突，提供了一种高效、紧凑且性能优越的向量量化解决方案。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [58] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出基于CycleGAN和YOLOv8的跨模态数据增强框架，解决PCB红外缺陷检测中数据稀缺问题，通过可见光图像生成伪红外数据提升检测性能。


<details>
  <summary>Details</summary>
Motivation: PCB红外缺陷检测面临红外数据稀缺的关键瓶颈，传统方法依赖配对监督数据，而实际工业场景中获取大量配对红外-可见光数据成本高昂且困难。

Method: 1. 使用CycleGAN进行无配对图像到图像转换，将丰富的可见光PCB图像映射到红外域；2. 生成保留缺陷结构语义和热分布模式的高保真伪红外样本；3. 构建异构训练策略，融合生成的伪红外数据和有限真实红外样本训练轻量级YOLOv8检测器。

Result: 该方法在低数据条件下有效增强特征学习，增强后的检测器显著优于仅使用有限真实数据训练的模型，接近完全监督训练的性能基准，证明伪红外合成作为工业检测的鲁棒增强策略的有效性。

Conclusion: 提出的跨模态数据增强框架成功解决了PCB红外缺陷检测中的数据稀缺问题，通过生成伪红外数据有效提升了检测性能，为工业视觉检测提供了实用的数据增强解决方案。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [59] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

Relevance: 45.0

TL;DR: ReMA是一种用于视频行为识别的表示感知混合增强方法，通过控制混合过程来增强表示稳定性，同时保持类别条件稳定性，无需额外监督或可训练参数。


<details>
  <summary>Details</summary>
Motivation: 当前视频数据增强策略大多是扰动驱动的，会引入不受控制的变异，放大非判别性因素，削弱类内分布结构并导致表示漂移，在不同时间尺度上产生不一致的增益。

Method: 提出ReMA（表示感知混合增强），包含两个互补机制：1）表示对齐机制（RAM）在分布对齐约束下进行结构化类内混合；2）动态选择机制（DSM）生成运动感知的时空掩码来定位扰动，引导其远离判别敏感区域。

Result: 在多个视频行为基准测试上的广泛实验表明，ReMA在不同时空粒度上一致地提高了泛化能力和鲁棒性。

Conclusion: ReMA通过联合控制混合的方式和位置，在不增加额外监督或可训练参数的情况下提高了表示鲁棒性，为视频行为识别提供了一种有效的增强策略。

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [60] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出GranAlign框架，通过粒度感知对齐解决零样本视频时刻检索中的语义粒度不匹配问题，在三个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频时刻检索（ZVMR）面临文本查询与视频内容语义粒度不匹配的核心挑战。现有方法虽然利用预训练知识实现联合空间表示，但未能平衡不同模态间的语义粒度，导致检索不准确。

Method: 提出无需训练的GranAlign框架，包含两种互补技术：1) 基于粒度的查询重写，生成不同语义粒度的查询；2) 查询感知的标题生成，将查询意图嵌入视频内容。通过将多粒度查询与查询无关/查询感知的标题配对，解决语义不匹配问题。

Result: 在三个主要基准测试（QVHighlights、Charades-STA、ActivityNet-Captions）上均达到新的SOTA性能，在具有挑战性的QVHighlights数据集上mAP@avg提升3.23%。

Conclusion: GranAlign通过粒度感知对齐有效解决了ZVMR中的语义粒度不匹配问题，证明了平衡多模态语义粒度的重要性，且无需额外训练即可显著提升性能。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [61] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

Relevance: 45.0

TL;DR: 该论文提出了一种模态主导感知指数(MDI)来量化RGB-红外多模态感知中的优化偏差，并开发了模态主导感知跨模态学习框架(MDACL)，通过分层跨模态引导和对抗均衡正则化来平衡多模态优化，在RGB-IR检测任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: RGB-红外多模态感知是复杂物理环境中嵌入式多媒体系统的关键技术。现有跨模态融合方法虽然有所进展，但由于模态特征不对称（信息密度和特征质量差异）导致的优化偏差问题尚未充分研究。这种偏差导致训练过度强调主导模态，阻碍了有效的多模态融合。

Method: 1. 提出模态主导感知指数(MDI)，通过联合建模特征熵和梯度贡献来量化模态主导程度；2. 基于MDI开发模态主导感知跨模态学习框架(MDACL)；3. MDACL包含分层跨模态引导(HCG)来增强特征对齐，以及对抗均衡正则化(AER)来平衡融合过程中的优化动态。

Result: 在三个RGB-IR基准测试上进行了广泛实验，MDACL有效缓解了优化偏差，并实现了最先进的性能。

Conclusion: 该研究揭示了多模态感知中的优化偏差问题，并提出了一种量化方法和平衡框架，为多模态融合提供了新的视角和解决方案。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [62] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

Relevance: 40.0

TL;DR: GLASS是一个用于AI生成图像检测的架构，结合全局调整大小视图和多个随机采样的局部裁剪，通过空间分层采样选择原始分辨率区域，并使用基于注意力的评分进行聚合。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，AI生成的图像越来越逼真和高分辨率。大多数AI生成图像检测架构通常会在输入模型前对图像进行下采样，这可能导致细粒度细节的丢失。因此需要一种能够同时利用全局和局部信息的架构来改进检测性能。

Method: GLASS架构结合全局调整大小视图和多个随机采样的局部裁剪。局部裁剪通过空间分层采样从原始分辨率区域高效选择，并使用基于注意力的评分进行聚合。该架构可以集成到各种视觉模型中（如Vision Transformer、ResNet、ConvNeXt），处理任意尺寸的图像。

Result: 实验表明，GLASH在可行的计算约束下，通过实现更高的预测性能，优于标准的迁移学习方法。

Conclusion: GLASS架构通过同时利用图像的全局和局部信息，有效改进了AI生成图像的检测性能，为高分辨率图像分析提供了新的解决方案。

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [63] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

Relevance: 40.0

TL;DR: TimeColor：基于草图的视频着色模型，支持异构、可变数量的参考图像，通过每参考区域分配和时空对应掩码注意力机制，提高颜色保真度、身份一致性和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有着色模型通常只基于单个参考（通常是场景第一帧），忽略了其他条件数据源（如角色设定表、背景图像或任意着色帧）。需要支持异构、可变数量的参考图像，同时防止捷径学习和跨身份调色板泄漏。

Method: 1) 将参考图像编码为额外的潜在帧，在时间维度上拼接，使模型能在每个扩散步骤中并行处理参考图像，同时保持参数量不变；2) 使用显式的每参考区域分配；3) 采用时空对应掩码注意力机制和模态分离的RoPE索引，增强主体-参考绑定，防止捷径学习和跨身份调色板泄漏。

Result: 在SAKUGA-42M数据集上的实验表明，TimeColor在单参考和多参考协议下，相比先前基线在颜色保真度、身份一致性和时间稳定性方面均有提升。

Conclusion: TimeColor通过支持异构、可变数量的参考图像，结合显式区域分配和时空对应注意力机制，有效提高了视频着色的质量，解决了现有方法在参考利用和身份一致性方面的局限性。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [64] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

Relevance: 40.0

TL;DR: MotionPhysics：一个端到端可微分框架，通过自然语言提示为3D场景推断合理的物理参数，无需真实轨迹或标注视频，利用多模态大语言模型估计材料参数，并通过视频扩散模型提取运动先验来指导模拟。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体和材料模拟需要专家知识和耗时的物理参数调整才能获得理想的动态行为。传统方法依赖真实轨迹或标注视频，限制了应用范围。作者希望开发一个能从自然语言提示直接推断物理参数的框架，降低模拟门槛。

Method: 1. 使用多模态大语言模型估计材料参数值，并约束在合理范围内；2. 提出可学习的运动蒸馏损失，从预训练视频扩散模型中提取鲁棒的运动先验，同时最小化外观和几何归纳偏置来指导模拟；3. 端到端可微分框架，支持多种材料和场景。

Result: 在30多个场景中评估，包括真实世界、人工设计和AI生成的3D物体，涵盖弹性固体、金属、泡沫、沙子、牛顿和非牛顿流体等多种材料。MotionPhysics能生成由自然语言引导的视觉真实动态模拟，超越现有技术，同时自动确定物理合理的参数。

Conclusion: MotionPhysics通过自然语言提示为3D场景推断物理参数，无需真实轨迹或标注视频，利用大语言模型和视频扩散模型实现高质量物理模拟，显著降低了物理模拟的门槛。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [65] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

Relevance: 40.0

TL;DR: MS COCOAI数据集：包含96000个真实和AI生成图像的数据集，用于检测AI生成图像和识别生成模型


<details>
  <summary>Details</summary>
Motivation: 随着Stable Diffusion、DALL-E等多模态生成AI系统的普及，AI生成图像越来越难以与真实照片区分，导致虚假信息和误导内容传播。迫切需要开发有效的检测方法来应对这一挑战。

Method: 基于MS COCO数据集构建包含96000个数据点的数据集，使用5种生成器（Stable Diffusion 3、2.1、SDXL、DALL-E 3、MidJourney v6）生成合成图像。提出两个任务：1）图像真伪分类；2）识别生成模型。

Result: 创建了MS COCOAI数据集，包含真实和合成图像，支持AI生成图像检测研究。数据集已在HuggingFace上公开。

Conclusion: MS COCOAI数据集为AI生成图像检测提供了重要资源，有助于开发更有效的检测方法，应对虚假信息传播的挑战。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [66] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出通过噪声优化来解决文本到图像生成模型中的模式崩溃问题，通过简单的噪声优化目标在保持基础模型保真度的同时提高生成多样性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型存在显著的模式崩溃问题，即给定相同文本提示时生成的图像缺乏多样性。虽然已有工作通过引导机制或生成大量候选图像再精炼来解决此问题，但本文采取不同方向，旨在通过噪声优化实现生成多样性。

Method: 提出简单的噪声优化目标来缓解模式崩溃，同时保持基础模型的保真度。分析噪声的频率特性，展示具有不同频率分布的替代噪声初始化可以改善优化和搜索过程。

Result: 实验证明噪声优化在生成质量和多样性方面都取得了优越的结果，能够有效缓解模式崩溃问题。

Conclusion: 噪声优化是一种有效解决文本到图像模型模式崩溃问题的方法，通过优化噪声初始化可以显著提高生成多样性而不损害模型保真度。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [67] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出压缩地图先验（CMP）框架，从历史遍历数据中学习空间先验知识，显著提升3D目标检测性能，同时大幅减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶视觉系统通常忽略历史遍历信息，每次都将位置视为首次访问。然而，大多数部署区域实际上已被多次访问过，这些历史数据可以作为有价值的空间先验知识来提升感知性能。

Method: 使用二值化哈希映射存储空间先验，仅需32KB/km²的存储空间（比密集存储减少20倍）。该框架可轻松集成到主流3D感知系统中，几乎不增加额外计算成本。

Result: 在nuScenes数据集上，CMP显著且一致地提升了多种架构的3D目标检测性能，证明了历史空间先验的有效性。

Conclusion: 压缩地图先验是一种简单有效的框架，能够从历史遍历中学习空间知识，显著提升自动驾驶视觉系统的感知能力，同时保持高效存储和计算特性。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [68] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出FaceFocalDesc问题：为任意选定面部区域生成多属性自然语言描述（包含动作单元、情绪状态、年龄估计）。构建新数据集，基于Qwen2.5-VL微调Focal-RegionFace模型，通过渐进式微调阶段实现局部面部特征聚焦分析。


<details>
  <summary>Details</summary>
Motivation: 当前面部分析研究未充分探索为任意选定面部区域生成多属性自然语言描述的问题。系统聚焦于个体面部区域的能力能带来更好的理解和控制，这在细粒度面部状态分析中很重要。

Method: 1) 构建新的多属性描述数据集，为任意选定面部区域提供丰富的区域级标注和自然语言描述；2) 基于Qwen2.5-VL微调Focal-RegionFace模型，通过多个渐进式微调阶段逐步细化对局部面部特征的聚焦。

Result: Focal-RegionFace在新基准测试中取得了最佳性能，在传统指标和新提出的指标上都表现优异，验证了其在细粒度多属性面部区域聚焦分析场景中的有效性和多功能性。

Conclusion: 该研究成功解决了面部分析中未充分探索的问题，提出的Focal-RegionFace模型在细粒度多属性面部区域分析中表现出色，为面部状态分析提供了新的视角和方法。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [69] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出IntraStyler方法，通过示例图像引导的风格合成来捕获域内多样性，无需先验知识，用于跨模态域适应的图像翻译。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法主要关注源域和目标域之间的域偏移，而域内变异性研究不足。传统方法需要预先指定域内变化进行风格合成，这在实践中不切实际。

Method: 提出IntraStyler方法：1）使用示例图像引导风格合成，使输出风格匹配示例风格；2）引入基于对比学习的风格编码器来提取纯风格特征；3）无需先验知识即可捕获多样化的域内风格。

Result: 在CrossMoDA 2023数据集上验证了方法的有效性，展示了可控风格合成的能力，以及多样化合成数据对下游分割任务的益处。

Conclusion: IntraStyler能够有效捕获域内风格多样性，无需先验知识，为跨模态域适应提供了更灵活的风格合成方法，提升了下游分割任务的性能。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [70] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出DUAL框架，通过证据深度学习动态解耦预测不确定性为认知不确定性和偶然不确定性，分别处理遥感长尾分布中的难样本和噪声样本。


<details>
  <summary>Details</summary>
Motivation: 遥感数据中普遍存在长尾分布，但现有方法未能有效区分难样本（尾部数据）和噪声模糊样本，导致对所有低置信度样本不加区分地强调，容易在噪声数据上过拟合。

Method: 基于证据深度学习，提出模型无关的不确定性感知框架DUAL：1) 使用认知不确定性作为样本稀缺性指标，指导难样本的重新加权策略；2) 利用偶然不确定性量化数据模糊性，采用自适应标签平滑机制抑制噪声影响。

Result: 在多个数据集和各种骨干网络上进行了广泛实验，证明了框架的有效性和泛化能力，超越了TGN和SADE等强基线方法。消融研究进一步验证了设计选择的关键性。

Conclusion: DUAL框架成功解决了遥感长尾分布中难样本与噪声样本的区分问题，通过不确定性解耦实现了更鲁棒的长尾学习。

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [71] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

Relevance: 35.0

TL;DR: 基于Swin Transformer的皮肤病图像分类模型，在ISIC2019数据集上对8种皮肤病变类别达到87.71%的准确率


<details>
  <summary>Details</summary>
Motivation: 皮肤病日益普遍而皮肤科医生资源有限，需要智能工具支持患者和临床医生进行及时准确的皮肤病诊断

Method: 使用公开皮肤病图像数据集进行预训练，基于Swin Transformer架构，优化数据预处理流程，应用针对性数据增强技术

Result: 在ISIC2019数据集上对8种皮肤病变类别达到87.71%的预测准确率

Conclusion: 该模型有潜力作为临床医生的诊断支持工具和患者的自我评估辅助工具

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [72] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

Relevance: 35.0

TL;DR: JGA-LBD：通过联合潜在表示和桥接扩散，从单张RGB图像统一重建3D数字人的几何和外观


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用解耦的几何估计和外观合成流程，导致重建不统一和不一致。需要一种能够统一建模几何和外观的方法。

Method: 1) 将几何和外观统一建模为联合潜在表示；2) 将所有条件统一为3D高斯表示，通过共享稀疏VAE压缩到统一潜在空间；3) 使用桥接扩散从部分观测推断缺失组件；4) 专用解码模块提取完整几何结构和渲染新视角

Result: 在几何保真度和外观质量方面优于当前最先进方法，包括具有挑战性的野外场景

Conclusion: JGA-LBD通过统一的潜在表示和桥接扩散，实现了从单张RGB图像一致重建3D数字人的几何和外观

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [73] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究为乌干达等发展中国家开发了一个实时智能交通监控系统，使用计算机视觉技术进行车辆检测、车牌识别和速度估计，并集成了自动罚单发放功能。


<details>
  <summary>Details</summary>
Motivation: 超速是道路死亡事故的主要原因，尤其是在乌干达等发展中国家，这些地区的道路安全基础设施有限。本研究旨在为资源受限环境开发一个实用的交通监控系统，通过自动化执法减少交通事故。

Method: 使用计算机视觉技术：1) YOLOv8进行车牌检测；2) CNN和Transformer模型进行字符识别；3) 基于感兴趣区域的速度估计方法；4) 集成数据库和非洲Talking API实现自动短信罚单发放。

Result: 车牌检测mAP达到97.9%；字符识别中CNN的CER为3.85%，Transformer显著降低至1.79%；速度估计误差在10km/h以内；系统集成了数据库和自动罚单发放功能。

Conclusion: 该系统为资源受限环境提供了有效的交通管理解决方案，展示了在发展中国家通过自动化交通执法减少道路事故的潜力。

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [74] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出OmniVaT框架解决视觉-触觉学习中的单域泛化问题，通过多模态分数傅里叶适配器和离散树生成模块，有效缓解模态差异和领域偏移。


<details>
  <summary>Details</summary>
Motivation: 视觉-触觉学习面临模态差异（视觉与触觉图像）和领域差距（非标准化传感器、不一致数据收集）的挑战，现有方法难以处理这些跨域泛化问题。

Method: 提出OmniVaT框架：1) 多模态分数傅里叶适配器将视觉和触觉嵌入映射到统一的嵌入-频率空间；2) 离散树生成模块通过层次树结构获得多样可靠的多模态分数表示。

Result: 大量实验证明OmniVaT在SDG-VTL任务上具有优越的跨域泛化性能，无需多域训练数据或复杂的跨模态融合策略。

Conclusion: OmniVaT首次成功解决了视觉-触觉学习的单域泛化问题，为多模态感知系统提供了有效的跨域泛化解决方案。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [75] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出DVEFormer，一种基于RGB-D Transformer的高效方法，通过知识蒸馏预测密集文本对齐视觉嵌入，替代传统语义分割，支持灵活的自然语言查询和3D建图集成


<details>
  <summary>Details</summary>
Motivation: 在家庭环境中，机器人需要全面理解周围环境才能与未经训练的人类有效直观地交互。传统语义分割方法使用固定的预定义类别，缺乏灵活性，无法支持自然语言查询等高级应用。

Method: 采用知识蒸馏方法，使用Alpha-CLIP的教师嵌入指导高效的DVEFormer学生模型学习细粒度像素级嵌入。基于RGB-D Transformer架构，实现实时性能（26.3-77.0 FPS）。

Result: 在常见室内数据集上评估显示，该方法达到竞争性性能，同时满足实时要求。完整模型在NVIDIA Jetson AGX Orin上运行26.3 FPS，较小变体达到77.0 FPS。定性结果展示了实际应用的有效性。

Conclusion: DVEFormer可作为传统分割方法的直接替代品，同时支持灵活的自然语言查询和无缝集成到移动机器人的3D建图流程中，为机器人环境理解提供了更灵活的解决方案。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [76] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: NeoVerse是一个多功能4D世界模型，能够进行4D重建、新轨迹视频生成和丰富的下游应用，通过免姿态前馈4D重建和在线单目退化模式模拟等技术，实现了对多样化野外单目视频的可扩展处理。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界建模方法存在可扩展性限制，主要源于需要昂贵专业的多视角4D数据或繁琐的训练预处理。作者旨在开发一个能够处理多样化野外单目视频的可扩展4D世界模型。

Method: NeoVerse采用免姿态前馈4D重建、在线单目退化模式模拟等技术，使整个流程能够扩展到多样化的野外单目视频，无需昂贵多视角数据或繁琐预处理。

Result: NeoVerse在标准重建和生成基准测试中实现了最先进的性能，并展现出对各种领域的泛化能力。

Conclusion: NeoVerse通过创新的可扩展设计，成功构建了一个多功能4D世界模型，能够处理多样化野外单目视频，并在重建和生成任务上达到最优性能。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [77] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

Relevance: 35.0

TL;DR: ORCANet是一个用于视频修复的单一模型，专门处理随时间平滑演变的未知退化问题，通过粗强度估计去雾模块和流提示生成模块实现时间一致的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法主要关注逐帧的退化变化，忽略了真实世界中退化过程的时间连续性。实际应用中，退化类型和强度随时间平滑演变，多种退化可能共存或逐渐过渡，需要专门处理这种平滑演变未知退化(SEUD)场景。

Method: 提出ORCANet网络：1) 粗强度估计去雾(CIED)模块利用物理先验估计雾度强度，提供粗去雾特征作为初始化；2) 流提示生成(FPG)模块提取退化特征，生成捕获片段级退化类型的静态提示和适应帧级强度变化的动态提示；3) 标签感知监督机制提高不同退化下静态提示表示的可区分性。

Result: 实验表明ORCANet在修复质量、时间一致性和鲁棒性方面优于基于图像和视频的基线方法，在SEUD场景下表现出色。

Conclusion: ORCANet成功解决了视频修复中平滑演变未知退化问题，通过结合物理先验和自适应提示机制，实现了时间一致的高质量修复，为视频修复任务提供了新思路。

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [78] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

Relevance: 35.0

TL;DR: VNS-SAM 是针对 Segment Anything Model (SAM) 在视觉非显著场景（前景背景对比度低）下性能不足的改进方案，通过 Mask-Edge Token Interactive decoder 和 Non-Salient Feature Mining module 增强 SAM 对非显著特征的感知能力，同时保持其零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: SAM 在零样本分割方面表现出色，但在视觉非显著场景（前景背景对比度低）下性能受限，现有方法难以捕捉准确轮廓。需要增强 SAM 对这些挑战性场景的感知能力，同时保持其原有的零样本泛化优势。

Method: 提出 VNS-SAM，通过两种设计有效利用 SAM 的低层特征：1) Mask-Edge Token Interactive decoder - 增强掩码与边缘特征的交互；2) Non-Salient Feature Mining module - 挖掘非显著特征。仅需少量参数增加和计算开销。同时构建了 VNS-SEG 数据集，包含超过 35K 图像，涵盖多种 VNS 场景。

Result: VNS-SAM 在各种 VNS 分割任务中表现出优越性能，特别是在零样本设置下。额外参数可在 4 小时内优化完成，展示了其实用性和可行性。构建的 VNS-SEG 数据集为模型提供了更鲁棒的 VNS 特征学习，并全面评估了模型在 VNS 场景下的分割性能和泛化能力。

Conclusion: VNS-SAM 成功增强了 SAM 在视觉非显著场景下的感知能力，同时保持了其零样本泛化优势。该方法具有实用性和广泛的实际应用潜力，特别是在前景背景对比度低的挑战性场景中。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [79] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种集成全局信息引导模块的级联卷积神经网络，用于增强复杂场景下的图像分割性能


<details>
  <summary>Details</summary>
Motivation: 视觉感知在自主行为中至关重要，但复杂场景下的鲁棒分割仍然具有挑战性。传统方法在视觉杂乱或模糊环境中表现不佳，需要更有效的特征融合机制

Method: 设计了一个级联卷积神经网络，集成了新颖的全局信息引导模块，该模块能够有效融合多层的低层纹理细节和高层语义特征，克服单尺度特征提取的局限性

Result: 在基准图像分割数据集上的实验评估表明，该框架实现了卓越的精度，优于现有的最先进方法，特别是在视觉杂乱或模糊环境中

Conclusion: 该方法显著提高了分割精度，展示了在实际机器人应用中部署的潜力，为复杂场景下的视觉感知提供了有效解决方案

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [80] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出HyperPriv-EPN框架，利用超图学习和特权信息学习（LUPI），通过术后文本特权信息训练模型，使其在仅使用术前MRI时能"幻觉"出语义社区结构，提升室管膜瘤术前预后诊断。


<details>
  <summary>Details</summary>
Motivation: 室管膜瘤术前预后对治疗规划至关重要，但MRI缺乏术后手术报告中的语义信息。现有多模态方法无法在推理时利用这些不可用的特权文本数据，需要一种能将这些知识转移到术前设置的方法。

Method: 提出超图基础的LUPI框架HyperPriv-EPN，采用Severed Graph策略：共享编码器处理Teacher图（含术后特权信息）和Student图（仅术前数据）。通过双流蒸馏，Student学习仅从视觉特征幻觉语义社区结构。

Result: 在311名患者的多中心队列中验证，HyperPriv-EPN达到最先进的诊断准确率和生存分层效果，成功将专家知识转移到术前设置，无需推理时文本输入。

Conclusion: HyperPriv-EPN有效利用历史术后数据指导新患者诊断，无需推理时文本，为医学影像分析中特权信息的利用提供了新方法。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [81] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出Pixel-to-4D框架，通过单张图像构建3D高斯场景表示并采样物体运动，实现快速、相机引导的视频生成，无需迭代去噪过程。


<details>
  <summary>Details</summary>
Motivation: 现有单图像条件视频生成方法在用户可控性（如相机路径修改）方面不足，且现有相机控制方法在准确建模相机运动、保持时间一致性和几何完整性方面存在困难。需要一种既能精确控制相机运动又能保持完全时间一致性的方法。

Method: 提出新颖框架：1）从单张图像构建3D高斯场景表示；2）采样合理的物体运动；3）在单次前向传播中完成，无需迭代去噪来将物体运动注入渲染帧。使用3D高斯表示实现相机引导的视频生成。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K数据集上的实验表明，该方法在视频质量和推理效率方面达到最先进水平。

Conclusion: 提出的Pixel-to-4D框架能够实现快速、相机引导的视频生成，同时保持高质量和时间一致性，为智能系统提供了重要的视频生成组件。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [82] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出多级特征融合方法，利用预训练网络不同深度的表征，在制造质量检测的持续学习场景中实现高效适应，减少可训练参数并缓解灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 在制造质量检测中，特别是再制造等变化场景下，产品类型和缺陷模式经常变化，需要模型频繁适应新条件，这构成了持续学习问题。现有方法需要高效适应新条件，同时避免灾难性遗忘

Method: 提出多级特征融合方法，利用预训练网络不同深度的表征。该方法从网络的不同层次提取特征并进行融合，而不是端到端训练整个网络，从而显著减少可训练参数

Result: 该方法在不同质量检测问题上能够匹配端到端训练的性能，同时使用显著更少的可训练参数。此外，它减少了灾难性遗忘，并提高了对新产品类型或缺陷的泛化鲁棒性

Conclusion: 多级特征融合方法为制造质量检测中的持续学习问题提供了一种有效的解决方案，在保持性能的同时实现了计算效率和抗遗忘性

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [83] [The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification](https://arxiv.org/abs/2601.00355)
*Tanay Donde*

Main category: eess.IV

Relevance: 35.0

TL;DR: 该研究分析了皮肤黑色素瘤分类模型中病灶注意力与诊断性能的关系，发现模型对病灶区域的关注度越高，诊断性能越好，为开发更准确可靠的医疗AI模型提供了基础。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤是最致命的皮肤癌亚型，早期准确检测可显著改善患者预后。尽管CNN等机器学习模型在自动分类方面显示出潜力，但由于对病灶区域关注不一致，其诊断可靠性仍存在问题。研究旨在分析病灶注意力与诊断性能之间的关系。

Method: 使用掩码图像、边界框检测和迁移学习，采用多种可解释性和敏感性分析方法，研究模型注意力与病灶区域的对齐程度，以及这种对齐与精确率、召回率和F1分数的相关性。

Result: 结果显示，对病灶区域关注度更高的模型取得了更好的诊断性能，表明可解释性AI在医疗诊断中的潜力。

Conclusion: 该研究为未来开发更准确、更可信的黑色素瘤分类模型奠定了基础，强调了模型注意力机制在医疗AI中的重要性。

Abstract: Melanoma is the most lethal subtype of skin cancer, and early and accurate detection of this disease can greatly improve patients' outcomes. Although machine learning models, especially convolutional neural networks (CNNs), have shown great potential in automating melanoma classification, their diagnostic reliability still suffers due to inconsistent focus on lesion areas. In this study, we analyze the relationship between lesion attention and diagnostic performance, involving masked images, bounding box detection, and transfer learning. We used multiple explainability and sensitivity analysis approaches to investigate how well models aligned their attention with lesion areas and how this alignment correlated with precision, recall, and F1-score. Results showed that models with a higher focus on lesion areas achieved better diagnostic performance, suggesting the potential of interpretable AI in medical diagnostics. This study provides a foundation for developing more accurate and trustworthy melanoma classification models in the future.

</details>


### [84] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

Relevance: 30.0

TL;DR: HarmoniAD：一种频率引导的双分支工业异常检测框架，通过高频分支增强纹理细节检测微小缺陷，低频分支捕获全局语义，平衡结构细节与语义一致性。


<details>
  <summary>Details</summary>
Motivation: 工业产品质量检测中异常检测至关重要，微小缺陷检测失败会导致严重后果。现有方法面临结构-语义权衡：结构导向模型（如基于频率的滤波器）对噪声敏感，而语义导向模型（如基于CLIP的编码器）常忽略精细细节。

Method: 提出HarmoniAD频率引导双分支框架：特征先由CLIP图像编码器提取，然后转换到频域，解耦为高频和低频路径。高频分支配备细粒度结构注意力模块（FSAM）增强纹理和边缘检测小异常，低频分支使用全局结构上下文模块（GSCM）捕获长距离依赖并保持语义一致性。采用多类联合训练策略。

Result: 在MVTec-AD、VisA和BTAD数据集上实现最先进性能，同时具备敏感性和鲁棒性。

Conclusion: HarmoniAD通过频率域解耦和双分支互补建模，有效平衡了精细细节和全局语义，解决了工业异常检测中的结构-语义权衡问题。

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [85] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了RoLID-11K，首个用于车载摄像头路边垃圾检测的大规模数据集，包含超过11,000张标注图像，涵盖英国多样化驾驶条件，具有显著的长尾分布和小目标检测挑战。


<details>
  <summary>Details</summary>
Motivation: 当前路边垃圾监测依赖劳动密集型调查和公众报告，空间覆盖有限。现有的垃圾检测视觉数据集主要关注街景静态图像、航拍场景或水环境，无法反映车载摄像头视频的独特特征——垃圾目标极小、稀疏且嵌入杂乱的背景中。

Method: 构建了RoLID-11K数据集，包含超过11,000张标注图像，涵盖英国多样化驾驶条件。对多种现代检测器进行基准测试，包括精度导向的Transformer架构（如CO-DETR）和实时YOLO模型，分析它们在极端小目标检测任务上的优缺点。

Result: CO-DETR及相关Transformer架构在定位精度上表现最佳，而实时模型受限于粗糙的特征层次结构。该数据集为动态驾驶场景中的极端小目标检测建立了具有挑战性的基准。

Conclusion: RoLID-11K为车载摄像头路边垃圾检测提供了首个大规模基准数据集，支持开发可扩展、低成本的垃圾监测系统。数据集公开可用，旨在推动计算机视觉在环境监测领域的应用。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [86] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

Relevance: 30.0

TL;DR: RGS-SLAM提出了一种基于高斯分布的SLAM框架，使用训练免费的对应关系初始化替代传统的残差驱动稠密化，通过DINOv3描述符和多视角三角测量生成高斯种子，提高了映射稳定性和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统GS-SLAM使用残差驱动稠密化方法，需要逐步添加高斯分布来填补缺失几何结构，这种方法可能导致早期映射不稳定、收敛速度慢，特别是在纹理丰富和杂乱场景中渲染质量有限。

Method: 1) 使用DINOv3描述符提取密集多视角对应关系；2) 通过置信度感知的内点分类器精炼对应关系；3) 执行一次性三角测量生成结构感知的高斯种子；4) 在优化前建立良好分布的高斯先验；5) 保持与现有GS-SLAM管道的完全兼容性。

Result: 在TUM RGB-D和Replica数据集上评估：1) 收敛速度提升约20%；2) 在纹理丰富和杂乱场景中获得更高的渲染保真度；3) 达到竞争性或优于最先进的高斯和基于点的SLAM系统；4) 保持实时映射性能（最高925 FPS）。

Conclusion: RGS-SLAM通过训练免费的对应关系到高斯初始化方法，显著提高了SLAM系统的稳定性、收敛速度和渲染质量，同时保持与现有管道的兼容性和实时性能，为高斯分布SLAM提供了更有效的初始化策略。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [87] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: MorphAny3D是一个无需训练的三维变形框架，利用结构化潜在(SLAT)表示，通过注意力机制融合源和目标特征，实现高质量跨类别三维变形。


<details>
  <summary>Details</summary>
Motivation: 三维变形在生成语义一致和时间平滑的变形方面仍然具有挑战性，特别是在跨类别的情况下。现有方法难以处理复杂的变形序列，尤其是在保持结构连贯性和时间一致性方面。

Method: 提出训练免费框架MorphAny3D，基于SLAT表示。核心创新包括：1) Morphing Cross-Attention (MCA) - 在注意力机制中融合源和目标特征以保持结构连贯性；2) Temporal-Fused Self-Attention (TFSA) - 融合先前帧特征以增强时间一致性；3) 方向校正策略缓解变形过程中的姿态模糊性。

Result: 实验表明该方法能生成最先进的变形序列，即使在具有挑战性的跨类别情况下也能表现优异。进一步支持解耦变形和三维风格迁移等高级应用，并能推广到其他基于SLAT的生成模型。

Conclusion: MorphAny3D通过创新的注意力机制融合策略，实现了高质量的三维变形，解决了跨类别变形中的语义一致性和时间平滑性问题，为三维生成模型提供了新的变形能力。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [88] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出基于多视角2D图像和神经辐射场(NeRF)的3D实例分割框架，用于精确农作物计数，无需作物特定参数调优，在棉花、苹果、梨数据集上验证效果优异。


<details>
  <summary>Details</summary>
Motivation: 室外农田环境中，部分遮挡和作物聚集导致的模糊性给基于图像的农作物计数带来巨大挑战，需要更精确的3D实例分割方法来解决这些问题。

Method: 利用多视角2D图像，结合神经辐射场(NeRF)进行视图合成，引入作物可见性和掩码一致性评分，结合NeRF的3D信息实现3D实例分割。

Result: 在棉花、苹果、梨三个农业数据集上验证，展示了优异的计数性能，不受作物颜色、形状、大小变化影响，相比现有方法表现更优。

Conclusion: 提出的3D实例分割框架能有效解决室外农作物计数问题，无需作物特定参数调优，并贡献了棉花植物数据集促进进一步研究。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [89] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

Relevance: 25.0

TL;DR: VisNet：一种高效的人体再识别模型，通过多尺度特征融合、语义聚类和动态权重平均等技术，在保持高准确率的同时大幅降低计算成本，适用于实时监控和移动应用。


<details>
  <summary>Details</summary>
Motivation: 当前人体再识别（ReID）方法虽然准确率高，但计算成本过高，难以在计算资源有限的实时监控和移动应用中部署。需要一种既高效又准确的解决方案。

Method: 提出VisNet模型，包含：1）多尺度特征融合（融合ResNet50的1-4阶段特征）；2）语义聚类与解剖学身体分区；3）动态权重平均平衡分类语义正则化；4）使用FIDI损失函数改进度量学习。

Result: 在Market-1501数据集上达到87.05% Rank-1准确率和77.65% mAP，参数量32.41M，计算量4.601 GFLOPs，相比现有方法在保持高准确率的同时显著降低计算成本。

Conclusion: VisNet为计算资源有限的实时监控和移动应用提供了一种实用的人体再识别解决方案，在准确率和效率之间取得了良好平衡。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [90] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一个概率双流框架，用于骨架动作识别，统一了可靠性建模和多模态集成，特别关注手部细微动作，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作识别方法主要关注身体中心的大尺度运动，忽略了对手部细微动作的识别，而手部动作对于细粒度识别至关重要。需要一种能够处理不确定性并整合多模态信息的框架。

Method: 提出了概率双流框架，包含三个关键组件：1) 无需校准的预处理管道，直接从原生坐标学习；2) 概率Noisy-OR融合，稳定可靠性感知的双流学习；3) 从内部到跨模态的集成，将四种骨架模态与RGB表示耦合。

Result: 在多个基准测试（NTU RGB+D 60/120, PKU-MMD, N-UCLA）和新定义的手部中心基准测试中，表现出持续的改进和鲁棒性，特别是在噪声和异构条件下。

Conclusion: 该框架通过统一可靠性建模和多模态集成，有效解决了骨架动作识别中手部细微动作识别的问题，在多种条件下表现出优越性能。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [91] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出ABFR-KAN，一种结合先进脑功能表示组件与Kolmogorov-Arnold Networks的transformer分类网络，用于自闭症谱系障碍诊断，在ABIDE I数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于图谱分割的功能连接分析存在选择偏差和缺乏个体特异性的问题，需要改进脑功能表示以提升自闭症诊断的准确性和可靠性。

Method: 提出ABFR-KAN网络，结合先进的脑功能表示组件与Kolmogorov-Arnold Networks，减少结构偏差，提高解剖一致性，增强功能连接估计的可靠性。

Result: 在ABIDE I数据集上的跨站点评估和消融实验表明，ABFR-KAN在不同模型主干和KAN配置下均优于现有最先进的自闭症分类基线方法。

Conclusion: ABFR-KAN通过结合先进的脑功能表示和KAN网络，有效提升了自闭症诊断的性能，为脑疾病诊断提供了更可靠的工具。

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [92] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出Anomaly Quadruplet-Net，通过四元组损失函数和定制数据加载器，在遮挡或视觉变化微小情况下提升装配进度估计精度


<details>
  <summary>Details</summary>
Motivation: 智能工厂需要自动监控产品装配进度以提高效率，但多日手动装配任务中，相邻任务间视觉变化微小会导致现有方法误分类，需要更鲁棒的估计系统

Method: 基于四元组损失函数学习异常图像特征，设计定制数据加载器策略性选择训练样本，使用桌面PC装配图像数据集进行验证

Result: 在桌面PC数据集上，Anomaly Quadruplet-Net比现有方法估计精度提升1.3%，相邻任务间误分类减少1.9%

Conclusion: 提出的方法在遮挡或视觉变化微小情况下能有效估计装配进度，适用于小规模数据集场景

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [93] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出DynaDrag方法，采用预测-移动框架进行拖拽式图像编辑，通过迭代的运动预测和运动监督实现像素级图像操控


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑方法存在跟踪失败、跟踪模糊、源图像与目标图像差距过大、中间点不合理导致编辑性差等问题，需要更有效的框架来解决这些挑战

Method: 提出DynaDrag方法，采用预测-移动框架，迭代执行运动预测和运动监督：1) 运动预测预测手柄点应移动的位置；2) 运动监督根据预测拖动点；3) 动态调整有效手柄点以提升性能

Result: 在人脸和人体数据集上的实验表明，DynaDrag在性能上优于先前工作，能够更有效地实现像素级图像编辑

Conclusion: DynaDrag作为首个采用预测-移动框架的拖拽方法，通过迭代的运动预测和运动监督机制，有效解决了现有方法的局限性，在图像编辑任务中表现出优越性

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [94] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文探索将自监督学习作为辅助任务来优化通用深度伪造检测的主要任务，通过融合自监督辅助任务的特征表示，在跨数据集评估中实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测面临泛化能力不足的问题，现有方法在跨数据集评估中表现不佳。作者希望通过自监督学习作为辅助任务来增强特征表示能力，提高检测器的泛化性能。

Method: 研究不同的训练方案组合，将自监督学习作为辅助任务与主要检测任务结合，通过融合自监督辅助任务的特征表示来增强主要任务的表示能力。

Result: 在DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV等多个数据集上实验，结果显示该方法在跨数据集评估中相比当前最先进的检测器具有更好的泛化性能。

Conclusion: 自监督学习作为辅助任务能够为深度伪造检测提供强大的特征表示，融合自监督和主要任务的特征表示可以充分发挥两者的潜力，提高检测器的泛化能力。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [95] [Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging](https://arxiv.org/abs/2601.00041)
*Fatemeh Hosseinabadi,Mohammad Mojtaba Rohani*

Main category: eess.IV

Relevance: 25.0

TL;DR: 该研究评估了三种CNN架构（ResNetRS、RegNet、EfficientNetV2）在儿科肺炎X光图像分类中的性能，RegNet表现最佳，准确率达92.4%。


<details>
  <summary>Details</summary>
Motivation: 儿科肺炎是全球儿童发病和死亡的主要原因，及时准确诊断面临挑战，包括放射学专业知识有限以及儿科影像的生理和程序复杂性。研究旨在探索自动分类儿科胸部X光图像（肺炎vs正常）的可行性。

Method: 从公开数据集中提取1,000张儿科胸部X光图像进行预处理和二元分类标注。使用预训练的ImageNet权重对三种CNN架构（ResNetRS、RegNet、EfficientNetV2）进行微调，评估准确率和灵敏度。

Result: RegNet表现最佳，准确率92.4%，灵敏度90.1%；ResNetRS准确率91.9%，灵敏度89.3%；EfficientNetV2准确率88.5%，灵敏度88.1%。

Conclusion: 基于CNN的迁移学习在儿科肺炎X光图像分类中表现出良好性能，RegNet架构最适合该任务，为临床辅助诊断提供了有前景的技术方案。

Abstract: Pediatric pneumonia remains a leading cause of morbidity and mortality in children worldwide. Timely and accurate diagnosis is critical but often challenged by limited radiological expertise and the physiological and procedural complexity of pediatric imaging. This study investigates the performance of state-of-the-art convolutional neural network (CNN) architectures ResNetRS, RegNet, and EfficientNetV2 using transfer learning for the automated classification of pediatric chest Xray images as either pneumonia or normal.A curated subset of 1,000 chest X-ray images was extracted from a publicly available dataset originally comprising 5,856 pediatric images. All images were preprocessed and labeled for binary classification. Each model was fine-tuned using pretrained ImageNet weights and evaluated based on accuracy and sensitivity. RegNet achieved the highest classification performance with an accuracy of 92.4 and a sensitivity of 90.1, followed by ResNetRS (accuracy: 91.9, sensitivity: 89.3) and EfficientNetV2 (accuracy: 88.5, sensitivity: 88.1).

</details>


### [96] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出SMAGNet模型，通过空间掩码自适应门控网络融合SAR和MSI数据，用于洪水淹没范围制图，提高模型对缺失数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 洪水期间及时准确的水域制图对灾害管理至关重要。虽然SAR数据是主要数据源，但结合MSI数据的多模态方法能提高精度，特别是在洪水峰值期间观测数据有限时。然而，如何自适应地整合部分可用的MSI数据到SAR基础的洪水制图过程尚未充分探索。

Method: 提出SMAGNet（空间掩码自适应门控网络），以SAR数据为主要输入，通过特征融合整合互补的MSI数据。模型设计能够处理MSI数据部分可用或完全缺失的情况。

Result: 在C2S-MS Floods数据集上，SMAGNet在不同MSI数据可用性水平下均优于其他多模态深度学习模型。即使MSI数据完全缺失，SMAGNet性能仍与仅使用SAR数据训练的U-Net模型相当。

Conclusion: SMAGNet提高了模型对缺失数据的鲁棒性，增强了多模态深度学习在实际洪水管理场景中的适用性。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [97] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

Relevance: 20.0

TL;DR: SV-GS：一种在稀疏观测下重建动态目标的框架，通过骨架驱动的变形场和运动估计，在稀疏视角和时间采样下实现高质量动态重建。


<details>
  <summary>Details</summary>
Motivation: 现实世界中动态目标重建面临挑战：传统方法需要密集的视角覆盖和时间采样，但在实际场景中（如监控摄像头），观测往往是稀疏的视角和稀疏的时间采样，导致动态重建问题高度不适定。

Method: 提出SV-GS框架，同时估计变形模型和物体随时间运动。利用粗略骨架图和初始静态重建作为输入引导运动估计。优化骨架驱动的变形场，包含粗粒度骨架关节姿态估计器和细粒度变形模块。仅使关节姿态估计器时间相关，实现平滑运动插值同时保留几何细节。

Result: 在合成数据集上，稀疏观测条件下PSNR比现有方法提升高达34%。在真实数据集上，使用显著更少的帧数，性能与密集单目视频方法相当。证明初始静态重建可被基于扩散的生成先验替代，增强实用性。

Conclusion: SV-GS能够在稀疏观测条件下有效重建动态目标，通过骨架驱动变形场和运动估计的结合，解决了现实场景中动态重建的挑战，具有实际应用价值。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [98] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

Relevance: 15.0

TL;DR: DichroGAN是一个条件生成对抗网络，用于从卫星图像恢复海底的空中颜色，通过估计大气场景辐射和光传输来消除水下光吸收和散射效应。


<details>
  <summary>Details</summary>
Motivation: 由于光在水柱中随深度呈指数衰减，从卫星图像恢复海底的空中颜色是一个具有挑战性的任务。现有的水下恢复技术难以准确处理这种复杂的光学效应。

Method: DichroGAN采用条件生成对抗网络架构，包含四个生成器：两个用于估计漫反射和镜面反射以获得大气场景辐射，第三个处理光谱带特征，第四个估计水下光传输。基于水下图像形成方程，通过两步同时训练来消除光吸收和散射效应。

Result: 在卫星和水下数据集上的广泛实验表明，DichroGAN相比最先进的水下恢复技术具有竞争力的性能。

Conclusion: DichroGAN能够有效恢复海底的空中颜色，为卫星遥感中的水下图像恢复提供了新的解决方案。

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [99] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出SynDR-IQA框架，通过分布感知的多样性内容上采样和密度感知的冗余聚类下采样来重塑合成数据分布，以解决BIQA模型在合成数据上训练时泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 盲图像质量评估(BIQA)面临大规模标注数据稀缺的挑战，合成数据是潜在解决方案，但现有合成数据集训练的模型泛化能力有限。研究发现合成数据学习的表示呈现离散聚类模式，阻碍回归性能：高质量图像特征围绕参考图像聚类，低质量图像特征基于失真类型聚类。这源于合成数据分布而非模型架构问题。

Method: 基于样本多样性和冗余对泛化误差影响的理论推导，提出SynDR-IQA框架：1) 分布感知的多样性内容上采样：增强视觉多样性同时保持内容分布；2) 密度感知的冗余聚类下采样：通过减少密集聚类区域的样本密度来平衡样本分布。

Result: 在三种跨数据集设置（合成到真实、合成到算法、合成到合成）上的大量实验证明了方法的有效性。代码已开源。

Conclusion: 通过重塑合成数据分布而非修改模型架构，SynDR-IQA有效提升了BIQA模型的泛化能力，为解决合成数据训练中的分布问题提供了新思路。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [100] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文提出一个轻量级害虫检测与农药推荐框架，适用于智能手机和无人机等低资源设备，帮助小农户实现精准农业。


<details>
  <summary>Details</summary>
Motivation: 传统害虫管理方法依赖人工田间检查和化学农药，成本高、耗时长、劳动密集且对环境有负面影响。需要为小农户开发适用于低资源设备的解决方案。

Method: 框架包含两个模块：1) 害虫检测模块使用轻量级CNN结合原型元学习，在少量训练样本下准确识别害虫；2) 农药推荐模块结合作物类型和生长阶段等环境因素，推荐安全环保的农药。通过整合多个公开数据集构建了综合害虫图像数据集。

Result: 轻量级CNN在保持高精度的同时显著降低计算复杂度，性能可与最先进模型媲美。决策支持系统减少了对传统化学农药的依赖，促进了可持续农业实践。

Conclusion: 该框架展示了在精准农业中实时应用的潜力，特别适合资源有限的小农户使用，有助于实现更环保、高效的害虫管理。

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [101] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一个轻量级两阶段框架，用于受损3D物体的联合几何和颜色修复，通过分离损伤定位和重建，使用扩散模型进行体素网格的掩码条件修复。


<details>
  <summary>Details</summary>
Motivation: 数字文化遗产修复的背景下，需要有效修复受损3D文物的几何结构和颜色纹理，现有方法往往无法同时处理几何和颜色修复。

Method: 两阶段框架：第一阶段使用2D卷积网络在RGB切片上预测损伤掩码并聚合为体积掩码；第二阶段使用扩散式3D U-Net在体素网格上进行掩码条件修复，联合预测占用率和颜色。

Result: 在合成损伤的纹理文物数据集上评估，相比基于对称性的基线方法，在固定32^3分辨率下产生更完整的几何结构和更一致的颜色重建。

Conclusion: 显式掩码条件是指导体积扩散模型进行3D几何和颜色联合修复的实用方法。

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [102] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

Relevance: 15.0

TL;DR: TOLF（Tiny Object Localization with Flows）是一个针对微小目标检测的噪声鲁棒定位框架，通过归一化流进行灵活误差建模和不确定性引导优化，解决微小目标对标注噪声敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管通用目标检测取得了显著进展，但微小目标与正常尺度目标之间仍存在性能差距。研究发现微小目标对标注噪声高度敏感，优化严格定位目标容易导致噪声过拟合。

Method: 提出TOLF框架：1）使用归一化流进行复杂、非高斯预测分布的误差建模；2）不确定性感知梯度调制机制，抑制从高不确定性、易受噪声影响的样本中学习；3）在噪声监督下实现鲁棒学习。

Result: 在三个数据集上的广泛实验验证了方法的有效性。特别是在AI-TOD数据集上，TOLF将DINO基线提升了1.2% AP。

Conclusion: TOLF通过流式误差建模和不确定性引导优化，有效解决了微小目标检测中的噪声过拟合问题，显著提升了微小目标检测性能。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [103] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

Relevance: 15.0

TL;DR: RePose：用于康复训练的实时3D人体姿态估计与运动分析方法，通过多摄像头RGB视频输入实现端到端的实时监测与评估


<details>
  <summary>Details</summary>
Motivation: 解决康复训练中患者动作实时监测与评估的需求，提供即时反馈和指导，帮助患者正确执行康复训练动作，辅助恢复肌肉力量和运动功能

Method: 1）提出统一的多摄像头RGB视频端到端实时人体姿态估计与运动分析流程；2）针对多人干扰的医疗康复场景提出快速跟踪方法（单帧<1ms）；3）改进SmoothNet用于实时姿态估计，减少误差并恢复真实运动状态；4）使用Unity平台进行实时监测评估并显示肌肉应力状况

Result: 实现了康复训练场景下的实时3D人体姿态估计与运动分析，能够实时监测患者动作并提供反馈，快速跟踪方法在多人干扰下仍能高效工作，改进的姿态估计方法使运动状态更平滑准确

Conclusion: RePose系统为康复训练提供了有效的实时监测与评估解决方案，能够帮助患者正确执行康复动作，加速恢复过程，在医疗康复领域具有实用价值

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [104] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

Relevance: 15.0

TL;DR: 基于图像的深度学习用于马铃薯储存质量监测，包括发芽检测、重量损失估计和保质期预测，DenseNet在发芽检测上达到98.03%准确率


<details>
  <summary>Details</summary>
Motivation: 解决马铃薯储存期间的质量监控挑战，提供非侵入式、可扩展的解决方案，减少食物浪费并改善库存管理

Method: 在200天控制温湿度条件下收集图像和重量数据，使用预训练的ResNet、VGG、DenseNet和ViT架构，设计两个专门模型：发芽检测二元分类器和重量损失/保质期预测多分类器

Result: DenseNet在发芽检测上达到98.03%准确率；保质期预测在粗分类（2-5类）时准确率超过89.83%，细分类（6-8类）时准确率下降；证明了图像模型集成到自动化系统的可行性

Conclusion: 图像基础方法为马铃薯质量评估提供经济有效、非破坏性的解决方案，支持储存和分销的效率和可持续性；未来需开发适用于不同品种和储存条件的通用模型

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [105] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于学习的框架，将原始TomoSAR点云转换为高分辨率建筑高度图，通过双拓扑网络处理点云噪声和缺失数据，实现大规模城市高度测绘。


<details>
  <summary>Details</summary>
Motivation: 星载SAR层析成像(TomoSAR)提供天气无关的侧视观测，能捕捉建筑立面结构，但TomoSAR点云常受噪声、各向异性点分布和数据空洞影响，阻碍准确高度重建。

Method: 引入基于学习的框架，使用双拓扑网络交替处理点分支（建模不规则散射特征）和网格分支（强制空间一致性），联合处理这两种表示以去噪输入点并填补缺失区域。

Result: 在慕尼黑和柏林数据上的广泛实验验证了方法的有效性，并证明框架可扩展以融合光学卫星影像，进一步提升重建质量。

Conclusion: 这是首个直接从TomoSAR点云进行大规模城市高度测绘的概念验证，为城市应用提供了可靠的高度估计方法。

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [106] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 论文提出在图像去马赛克任务中，通过空间下采样可以显著提高各向同性网络的效率和性能，并设计了JD3Net网络验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 移动平台上的数字成像应用需要轻量高效的网络，但现有的各向同性网络（残差网络）避免空间下采样，导致计算成本过高。论文认为空间下采样可以改善网络效率和性能。

Method: 采用基于DeepMAD的数学架构设计技术，设计了带下采样和不带下采样的简单全卷积网络进行比较，并提出了下采样变体JD3Net。

Result: 实验表明下采样确实提高了经验性能，JD3Net在多种图像去马赛克和联合去马赛克-去噪任务上表现出色。

Conclusion: 空间下采样可以显著提升各向同性网络在图像去马赛克任务中的效率和性能，为移动平台应用提供了更实用的解决方案。

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [107] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

Relevance: 15.0

TL;DR: UniCo是一个单次前向传播的3D形状补全方法，通过专用路径解码基元（primitives）而非级联方式，预测具有完整几何、语义和内点成员关系的基元集合。


<details>
  <summary>Details</summary>
Motivation: 现有结构化形状补全方法通常采用级联方式处理基元和点云，作者重新思考基元与点云的交互方式，认为在专用路径中解码基元并关注共享形状特征更为有效。

Method: 提出UniCo框架，引入基元代理（learnable queries）作为可学习查询，通过上下文化处理产生可直接组装的输出。训练策略将基元和点云耦合，采用在线目标更新确保优化一致性。

Result: 在合成和真实世界基准测试中，使用四种独立组装求解器，UniCo始终优于近期基线方法，将Chamfer距离降低高达50%，法线一致性提高高达7%。

Conclusion: 该方法为从不完整数据中进行结构化3D理解提供了有吸引力的解决方案，通过统一表示和专用解码路径实现了更有效的基元预测。

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [108] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出LNU-Net和IBU-Net两种新型深度学习架构用于心脏MRI左心室分割，分别基于层归一化和实例-批量归一化，在Dice系数和平均垂直距离指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对心脏影像的临床量化和诊断至关重要，需要更准确的分割方法来支持医疗决策。

Method: 基于U-Net架构，提出两种变体：LNU-Net在每个卷积块应用层归一化；IBU-Net在第一个卷积块结合实例和批量归一化。采用仿射变换和弹性变形进行数据增强，使用805张MRI图像进行评估。

Result: 提出的LNU-Net和IBU-Net在Dice系数和平均垂直距离指标上优于原始U-Net和其他最先进方法。

Conclusion: 两种新型归一化策略的U-Net变体在左心室分割任务中表现出色，为医学图像分割提供了有效的架构改进。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [109] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

Relevance: 15.0

TL;DR: AdaGaR提出了一种用于单目视频动态3D场景重建的统一框架，通过自适应Gabor表示和时序连续性约束，解决了现有方法在频率适应性和运动平滑性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D场景重建方法存在两个主要问题：1）单高斯基元具有低通滤波特性，无法捕捉高频细节，而标准Gabor函数存在能量不稳定问题；2）缺乏时序连续性约束导致插值时出现运动伪影。

Method: 1）自适应Gabor表示：通过可学习频率权重和自适应能量补偿扩展高斯函数，平衡细节捕捉和稳定性；2）时序连续性：采用三次Hermite样条与时序曲率正则化确保平滑运动演化；3）自适应初始化：结合深度估计、点跟踪和前景掩码建立早期训练中的稳定点云分布。

Result: 在Tap-Vid DAVIS数据集上取得SOTA性能（PSNR 35.49, SSIM 0.9433, LPIPS 0.0723），在帧插值、深度一致性、视频编辑和立体视图合成等任务上表现出强泛化能力。

Conclusion: AdaGaR通过统一的频率自适应和时序连续性框架，显著提升了动态3D场景重建的质量和稳定性，为视频理解和生成任务提供了有效的解决方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


### [110] [Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures](https://arxiv.org/abs/2601.00067)
*Merritt P. R. Losert,Dario Denora,Barnaby van Straaten,Michael Chan,Stefan D. Oosterhout,Lucas Stehouwer,Giordano Scappucci,Menno Veldhorst,Justyna P. Zwolak*

Main category: cond-mat.mes-hall

Relevance: 15.0

TL;DR: 该论文提出了一种自动化协议，用于从量子点器件的电荷稳定性图中提取电容特性，结合机器学习、图像处理和对象检测技术，无需人工标注即可分析大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 随着量子点自旋量子比特向更大、更复杂的器件架构发展，快速、自动化的器件表征和数据分析工具变得至关重要。手动解释电荷稳定性图中的特征耗时、易错且难以规模化。

Method: 集成机器学习、图像处理和对象检测技术，自动识别和跟踪电荷稳定性图中的电荷跃迁线，从大量数据集中提取电容特性，无需人工标注。

Result: 方法在实验测量的应变锗单量子阱（平面）和应变锗双量子阱（双层）量子点器件上得到验证，能够统计估计相对杠杆臂和电容耦合等物理相关量。

Conclusion: 该协议能够快速提取量子点器件的有用、非平凡信息，为大规模量子器件表征提供了自动化解决方案。

Abstract: As quantum dot (QD)-based spin qubits advance toward larger, more complex device architectures, rapid, automated device characterization and data analysis tools become critical. The orientation and spacing of transition lines in a charge stability diagram (CSD) contain a fingerprint of a QD device's capacitive environment, making these measurements useful tools for device characterization. However, manually interpreting these features is time-consuming, error-prone, and impractical at scale. Here, we present an automated protocol for extracting underlying capacitive properties from CSDs. Our method integrates machine learning, image processing, and object detection to identify and track charge transitions across large datasets without manual labeling. We demonstrate this method using experimentally measured data from a strained-germanium single-quantum-well (planar) and a strained-germanium double-quantum-well (bilayer) QD device. Unlike for planar QD devices, CSDs in bilayer germanium heterostructure exhibit a larger set of transitions, including interlayer tunneling and distinct loading lines for the vertically stacked QDs, making them a powerful testbed for automation methods. By analyzing the properties of many CSDs, we can statistically estimate physically relevant quantities, like relative lever arms and capacitive couplings. Thus, our protocol enables rapid extraction of useful, nontrivial information about QD devices.

</details>


### [111] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

Relevance: 15.0

TL;DR: DefVINS：一种视觉惯性里程计框架，通过将刚性IMU锚定状态与非刚性变形图分离，解决可变形场景中的VIO问题，结合可观测性分析和条件激活策略提高非刚性环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统VIO基于刚性假设，在可变形场景中会过度拟合局部非刚性运动或产生严重漂移。需要一种能处理场景变形的VIO框架。

Method: 1. 将刚性IMU锚定状态与非刚性变形图分离；2. 使用标准VIO初始化固定重力、速度和IMU偏差；3. 渐进激活非刚性自由度；4. 可观测性分析指导IMU锚定和条件激活策略。

Result: 结合惯性约束和可观测性感知的变形激活策略，在非刚性环境中提高了鲁棒性。消融研究验证了该方法的有效性。

Conclusion: DefVINS通过显式分离刚性和非刚性分量，结合可观测性分析和条件激活策略，有效解决了可变形场景中的VIO问题。

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>


### [112] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

Relevance: 5.0

TL;DR: SlingBAG Pro是一种基于点云迭代概念的三维光声成像重建算法，专门针对不规则几何换能器阵列设计，通过分层优化策略显著提升重建速度。


<details>
  <summary>Details</summary>
Motivation: 临床应用中需要高质量三维光声成像，但传统不规则阵列配置面临计算复杂度高、内存需求大、重建时间长的问题。现有迭代重建算法难以处理不规则阵列配置。

Method: 基于Sliding ball adaptive growth (SlingBAG)方法的点云迭代概念，扩展兼容任意阵列几何形状。采用分层优化策略，结合零梯度滤波和逐步增加的时间采样率，快速去除冗余空间点云并加速收敛。

Result: 相比原始SlingBAG算法，SlingBAG Pro在不规则阵列几何下实现了高达2.2倍的速度提升。通过仿真和活体小鼠实验验证了方法的有效性。

Conclusion: SlingBAG Pro算法能够在不规则阵列配置下保持高质量重建，减少所需换能器数量，并显著缩短重建时间，为临床三维光声成像提供了实用解决方案。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越表面语义相似性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略仍是一个挑战。需要超越表面语义相似性，提供与对话逻辑结构对齐的知识。

Method: 采用粗到细的两阶段检索方法：1) 识别知识库中与上下文相关的子区域；2) 在该子区域内提取与推理过程相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过关键词在知识句子中导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更贴近人类对话的底层推理逻辑，还显著提高了检索知识的多样性，生成更具信息性和创造性的响应。

Conclusion: 提出的推理感知知识检索方法能够有效整合检索和推理策略，为LLMs提供与对话逻辑对齐的知识，提升对话质量和多样性。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [114] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出一个LLM代理系统，能从原始文本中提取因果反馈模糊认知图(FCM)，通过三步指令引导LLM提取关键概念节点和因果边，形成动态系统并与人类生成的FCM收敛到相同均衡状态。


<details>
  <summary>Details</summary>
Motivation: 传统FCM构建需要人工标注，费时费力且主观性强。本文旨在利用LLM的自主性和推理能力，自动化地从文本中提取因果结构，实现更高效、可扩展的FCM构建方法。

Method: 设计三步系统指令：1) 从文本提取关键名词和名词短语；2) 从中选择FCM概念节点；3) 推断节点间的模糊因果边。使用LLM代理(Gemini和ChatGPT)处理文本，生成FCM动态系统，并与人类生成的FCM进行对比验证。

Result: 在Kissinger关于AI前景的论文测试中，LLM生成的FCM与人类生成的FCM收敛到相同的均衡极限环，尽管节点和边数量不同。混合不同LLM生成的FCM能吸收主要组件的均衡状态，同时创建新的均衡以更好地近似底层因果动态系统。

Conclusion: LLM代理能够有效自动化FCM构建过程，生成的因果结构与人类专家结果一致，且混合多个LLM的FCM能产生更丰富的均衡状态，展示了LLM在因果推理和动态系统建模方面的潜力。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [115] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出混合智能体框架，将LLM作为自然语言接口，严格分离语义推理与数学计算，解决库存管理中的"幻觉税"问题，相比端到端GPT-4o方案降低32.1%库存成本。


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，而直接使用LLM作为端到端求解器会产生显著的"幻觉税"——由于模型无法进行基于随机推理的准确计算导致的性能差距。

Method: 提出混合智能体框架，严格分离语义推理与数学计算：LLM作为智能接口从自然语言中提取参数并解释结果，同时自动调用严格算法构建优化引擎。引入Human Imitator（微调的"数字孪生"）来模拟有限理性管理者的行为，实现可扩展、可重复的压力测试。

Result: 混合智能体框架相比使用GPT-4o作为端到端求解器的交互基线，总库存成本降低了32.1%。研究发现仅提供完美真实信息不足以改善GPT-4o性能，确认瓶颈本质上是计算而非信息问题。

Conclusion: LLM不应作为运筹学的替代品，而应作为自然语言接口，使非专家能够访问基于严格求解器的策略。混合框架通过分离语义推理与数学计算，有效解决了LLM在优化问题中的"幻觉税"问题。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [116] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

Relevance: 85.0

TL;DR: Mathesis：一种神经符号架构，通过将数学状态编码为高阶超图，并使用符号推理核（SRK）将约束映射到连续能量景观，解决LLM在复杂推理中的逻辑失败问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理中存在持续的逻辑失败，缺乏内部公理框架。需要一种能够将符号逻辑与神经表示相结合的架构来解决这一问题。

Method: 提出Mathesis神经符号架构：1) 将数学状态编码为高阶超图；2) 使用符号推理核（SRK）作为可微分逻辑引擎，将约束映射到连续能量景观；3) 定义全局能量函数E(G)，零能量表示逻辑一致性；4) 使用梯度信号训练超图变换器大脑；5) 通过蒙特卡洛树搜索和进化证明搜索实现多步推理。

Result: 该方法将证明搜索转化为能量最小化问题，通过可微分逻辑引擎提供梯度信号来训练神经组件，实现了符号逻辑与神经表示的深度融合。

Conclusion: Mathesis架构通过神经符号方法解决了LLM的逻辑推理缺陷，将符号推理与神经网络训练相结合，为复杂数学推理提供了新的解决方案。

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [117] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: FlashInfer-Bench是一个标准化框架，用于评估和部署LLM生成的GPU内核，包含数据集、基准测试框架、排行榜和动态替换机制，以提升LLM推理系统的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能够生成GPU内核，但将这些AI生成的内核集成到实际推理系统中仍面临挑战。现有研究缺乏连接内核生成、基准测试和部署的标准化闭环框架。

Method: 1. FlashInfer Trace提供统一模式描述内核定义、工作负载、实现和评估；2. 基于真实服务轨迹构建数据集；3. 包含正确性和性能感知的基准测试框架；4. 建立公开排行榜跟踪LLM代理的GPU编程能力；5. 动态替换机制(apply())将最佳内核注入生产LLM引擎。

Result: 1. 建立了实用的可重复路径，持续改进AI生成内核并部署到大规模LLM推理中；2. 评估了LLM代理的性能和局限性；3. 比较了不同GPU编程语言的权衡；4. 为未来代理设计提供了见解。

Conclusion: FlashInfer-Bench填补了LLM生成GPU内核与实际部署之间的空白，通过标准化框架实现了从内核生成到生产部署的闭环，为提升LLM推理系统性能提供了实用解决方案。

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [118] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

Relevance: 85.0

TL;DR: 研究发现LLM驱动的智能体不仅存在人口统计学偏见，还会在最小"我们vs他们"线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，风险从人类群体间差异转向更根本的群体不对称——人类整体可能被智能体视为外群体。研究还提出了信念毒化攻击(BPA)来抑制人类规范脚本并重新激活对人类的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLM智能体在群体划分情境下的偏见行为，特别是当智能体-人类边界成为群体划分标准时，人类整体可能被智能体视为外群体的风险。这扩展了传统人口统计学偏见研究的范畴，关注更根本的群体间不对称问题。

Method: 1. 构建基于分配决策的受控多智能体社会模拟，在明确的收益权衡下测试群体间偏见；2. 设计信念毒化攻击(BPA)，包括初始化时的档案毒化(BPA-PP)和通过优化信念精炼后缀注入存储反思中的记忆毒化(BPA-MP)；3. 在多种设置下进行广泛实验验证智能体群体间偏见的存在和BPA的严重性。

Result: 实验发现：1. 智能体在最小群体线索下表现出一致的群体间偏见；2. 当部分对应方被标记为人类时，这种偏见会减弱，但这种减弱依赖于智能体相信真实人类存在的信念；3. BPA攻击能够有效抑制人类规范脚本并重新激活对人类的偏见；4. 研究提出了在档案和记忆边界进行干预的实用缓解策略。

Conclusion: LLM智能体不仅存在传统的人口统计学偏见，还会在群体划分情境下表现出对人类的偏见。信念依赖的人类规范脚本激活机制创造了新的攻击面，通过BPA攻击可以操纵智能体对人类的偏见行为。研究旨在为更安全的智能体设计提供信息，而非促进实际利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [119] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究分析了推理模型中的"顿悟"时刻，发现中程推理转变很罕见，不会随训练变得更频繁，也很少提高准确性，表明它们并非模型内在的自我修正机制，而是不稳定推理行为的症状。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟"时刻，导致准确输出，暗示了内在的自我修正能力。但尚不清楚这种推理策略的内在转变是否真正提高了性能。

Method: 研究分析了100万+推理轨迹、数百个训练检查点、三个推理领域、多种解码温度和模型架构。通过检测训练运行中的中程推理转变，并研究模型不确定性对转变效果的影响。

Result: 发现推理转变很罕见，不会随训练变得更频繁，也很少提高准确性。然而，其效果随模型不确定性而变化。基于此发现，研究显示在高熵条件下人为触发外在转变能可靠地提高准确性。

Conclusion: 中程推理转变是不稳定推理行为的症状，而非内在的自我修正机制。这表明先前对模型"顿悟"时刻的认知可能被误解了。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [120] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

Relevance: 85.0

TL;DR: DA-DPO提出难度感知的直接偏好优化框架，通过估计偏好数据难度并重加权训练样本，解决多模态LLM中偏好优化过拟合问题，提升幻觉抑制效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法因偏好数据难度不平衡容易过拟合，模型倾向于过度关注容易区分的偏好对，阻碍细粒度幻觉抑制并降低整体性能。

Method: 1) 难度估计：利用预训练视觉-语言模型的生成和对比目标，通过分布感知投票策略产生鲁棒的难度分数；2) 难度感知训练：基于估计难度重加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 实验表明DA-DPO持续改进多模态偏好优化，在标准基准测试中展现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO通过难度感知的样本重加权有效解决了多模态DPO中的过拟合问题，无需额外数据或微调阶段即可实现更有效的偏好优化。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [121] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

Relevance: 85.0

TL;DR: PedX-LLM：一个结合视觉特征、文本数据和交通领域知识的LLM框架，用于行人过街行为推理，在未见场景中表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习）泛化能力有限，在新场景中表现不佳。LLM提供了从数值模式匹配转向语义、上下文感知行为推理的机会，但现有LLM应用缺乏领域特定适应和视觉上下文。

Method: 提出PedX-LLM框架：1) 集成LLaVA提取的视觉特征与文本数据；2) 融入交通领域知识；3) 使用LoRA微调LLaMA-2-7B基础模型；4) 支持零样本和少样本学习。

Result: 1) 达到82.0%平衡准确率，优于最佳统计和监督学习方法；2) 视觉增强模块贡献2.9%性能提升；3) 领域知识集成带来额外4.1%改进；4) 在5个未见测试站点上，零样本配置达到66.9%平衡准确率，比基线方法至少高18个百分点；5) 少样本学习（仅5个验证示例）将平衡准确率提升至72.2%。

Conclusion: PedX-LLM通过视觉和知识增强的推理，使模型能够模拟人类决策逻辑，克服纯数据驱动方法的局限性，在未见场景中表现出强大的泛化能力。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [122] [Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing](https://arxiv.org/abs/2601.00042)
*Manish Bhatt,Adrian Wood,Idan Habler,Ammar Al-Kahfah*

Main category: cs.CR

Relevance: 85.0

TL;DR: 该论文研究LLM智能体安全测试方法，发现随机种子方差对测试结果影响显著（8倍差异），多种子平均可降低方差；奖励塑造会损害性能；简单状态签名优于复杂签名；集成方法能提供攻击类型多样性


<details>
  <summary>Details</summary>
Motivation: 尽管经过安全训练，具备工具使用能力的生产级LLM智能体仍需要进行安全测试。研究旨在评估GPT-4o-mini的安全性能，探索不同测试方法的有效性

Method: 采用Go-Explore方法对GPT-4o-mini进行28次实验运行，涵盖六个研究问题。比较随机种子方差、奖励塑造效果、状态签名复杂度、集成与单智能体策略等

Result: 随机种子方差主导算法参数，导致8倍结果差异；单种子比较不可靠，多种子平均能显著降低方差；奖励塑造在94%运行中损害性能或产生18个假阳性；简单状态签名优于复杂签名；集成方法提供攻击多样性

Conclusion: 在测试安全训练模型时，种子方差和领域专业知识可能比算法复杂度更重要。多种子平均、简单状态签名和集成方法是有效的安全测试策略

Abstract: Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.

</details>


### [123] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

Relevance: 85.0

TL;DR: InlineCoder：通过将未完成函数内联到其调用图中，将仓库级代码生成重构为函数级任务，利用锚点生成和双向内联（上游内联和下游检索）来增强仓库理解


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码生成方法（如RAG或基于上下文的函数选择）主要依赖表面相似性，难以捕捉仓库中函数、类和模块之间的复杂依赖关系，导致对仓库语义理解不足

Method: 1. 给定函数签名，先生成草稿完成（锚点）来近似下游依赖并支持基于困惑度的置信度估计；2. 双向内联过程：上游内联将锚点嵌入到调用者中捕获多样化使用场景，下游检索将锚点的被调用者集成到提示中提供精确依赖上下文；3. 结合草稿完成、上游和下游视角的丰富上下文，为LLM提供全面的仓库视图

Result: 论文未提供具体实验结果，但从方法描述看，InlineCoder通过重构问题为函数级任务，有望提升仓库级代码生成的准确性和对复杂依赖关系的理解能力

Conclusion: InlineCoder通过创新的内联方法，将仓库级代码生成转化为更易处理的函数级任务，有效解决了现有方法在理解复杂仓库依赖关系方面的局限性

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [124] [RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers](https://arxiv.org/abs/2601.00426)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

Relevance: 85.0

TL;DR: RMAAT是一种受星形胶质细胞启发的Transformer架构，通过递归记忆增强和自适应压缩机制实现线性复杂度注意力，在长序列处理中显著提升计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的自注意力机制具有二次复杂度，限制了其在长序列上的应用。作者从生物神经科学中的星形胶质细胞（在记忆和突触调节中起关键作用）获取灵感，探索超越传统架构修改的替代方案。

Method: 提出RMAAT架构：1) 采用递归分段处理策略，通过持久记忆token传播上下文信息；2) 基于模拟星形胶质细胞长时程可塑性(LTP)的自适应压缩机制；3) 受星形胶质细胞短时程可塑性(STP)启发的线性复杂度注意力机制；4) 使用AMRB训练算法提高循环网络的记忆效率。

Result: 在Long Range Arena (LRA)基准测试中，RMAAT展现了竞争力的准确率，同时在计算和内存效率方面有显著提升，验证了星形胶质细胞启发的动力学在可扩展序列模型中的潜力。

Conclusion: 将星形胶质细胞启发的动力学整合到Transformer架构中，为解决长序列处理中的计算效率问题提供了有前景的途径，为可扩展序列模型开辟了新方向。

Abstract: The quadratic complexity of self-attention mechanism presents a significant impediment to applying Transformer models to long sequences. This work explores computational principles derived from astrocytes-glial cells critical for biological memory and synaptic modulation-as a complementary approach to conventional architectural modifications for efficient self-attention. We introduce the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), an architecture integrating abstracted astrocyte functionalities. RMAAT employs a recurrent, segment-based processing strategy where persistent memory tokens propagate contextual information. An adaptive compression mechanism, governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP), modulates these tokens. Attention within segments utilizes an efficient, linear-complexity mechanism inspired by astrocyte short-term plasticity (STP). Training is performed using Astrocytic Memory Replay Backpropagation (AMRB), a novel algorithm designed for memory efficiency in recurrent networks. Evaluations on the Long Range Arena (LRA) benchmark demonstrate RMAAT's competitive accuracy and substantial improvements in computational and memory efficiency, indicating the potential of incorporating astrocyte-inspired dynamics into scalable sequence models.

</details>


### [125] [MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability](https://arxiv.org/abs/2601.00481)
*Tie Ma,Yixi Chen,Vaastav Anand,Alessandro Cornacchia,Amândio R. Faustino,Guanheng Liu,Shan Zhang,Hongbin Luo,Suhaib A. Fahmy,Zafar A. Qazi,Marco Canini*

Main category: cs.NI

Relevance: 85.0

TL;DR: MAESTRO是一个用于评估基于LLM的多智能体系统(MAS)的测试、可靠性和可观测性套件，通过标准化配置、执行跟踪和系统信号来支持系统化评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对基于LLM的多智能体系统进行系统化评估的工具，特别是在测试、可靠性和可观测性方面。需要标准化的方法来评估不同MAS架构的性能、资源消耗和稳定性。

Method: MAESTRO提供统一接口标准化MAS配置和执行，支持通过适配器集成原生和第三方MAS，导出框架无关的执行跟踪和系统级信号（延迟、成本、故障等）。在12个代表性MAS上进行实验，涵盖不同框架和交互模式。

Result: 研究发现：1) MAS执行结构稳定但时间变化大，导致运行间性能差异显著；2) MAS架构是资源消耗、可重复性和成本-延迟-准确性权衡的主要驱动因素，影响超过后端模型或工具设置的变化。

Conclusion: MAESTRO支持对基于LLM的多智能体系统进行系统化评估，为设计和优化智能体系统提供实证指导，强调架构设计的重要性超过模型选择。

Abstract: We present MAESTRO, an evaluation suite for the testing, reliability, and observability of LLM-based MAS. MAESTRO standardizes MAS configuration and execution through a unified interface, supports integrating both native and third-party MAS via a repository of examples and lightweight adapters, and exports framework-agnostic execution traces together with system-level signals (e.g., latency, cost, and failures). We instantiate MAESTRO with 12 representative MAS spanning popular agentic frameworks and interaction patterns, and conduct controlled experiments across repeated runs, backend models, and tool configurations. Our case studies show that MAS executions can be structurally stable yet temporally variable, leading to substantial run-to-run variance in performance and reliability. We further find that MAS architecture is the dominant driver of resource profiles, reproducibility, and cost-latency-accuracy trade-off, often outweighing changes in backend models or tool settings. Overall, MAESTRO enables systematic evaluation and provides empirical guidance for designing and optimizing agentic systems.

</details>


### [126] [Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?](https://arxiv.org/abs/2601.00559)
*Jason Quantrill,Noura Khajehnouri,Zihan Guo,Manar H. Alalfi*

Main category: cs.CR

Relevance: 85.0

TL;DR: 该论文首次全面评估了大型语言模型在智能家居物联网平台中检测规则交互威胁的能力，发现LLMs在语义理解方面表现良好，但在需要跨规则结构推理的威胁检测上准确性显著下降，且无法保持稳定可靠性。


<details>
  <summary>Details</summary>
Motivation: 智能家居物联网平台（如openHAB）使用触发-动作-条件规则自动化设备行为，但这些规则之间的交互可能产生交互威胁，如隐式依赖、冲突触发或重叠条件。传统方法依赖符号化的约束驱动静态分析，但需要评估LLMs在这种安全关键场景中的能力。

Method: 1. 构建多类别交互威胁分类法；2. 使用原始openHAB数据集和结构挑战性的突变数据集（测试规则变换下的鲁棒性）；3. 评估Llama 3.1 8B、Llama 70B、GPT-4o、Gemini-2.5-Pro和DeepSeek-R1；4. 在零样本、单样本和双样本设置下进行基准测试；5. 与oHIT手动验证的真实标签进行比较；6. 对比符号推理基线的性能。

Result: 1. LLMs在语义理解方面表现良好，特别是在动作和条件相关威胁上；2. 在需要跨规则结构推理的威胁检测上准确性显著下降，尤其是在突变规则形式下；3. 模型性能在不同威胁类别和提示设置下差异很大，没有模型能提供一致的可靠性；4. 符号推理基线在两个数据集上保持稳定检测，不受规则重写或结构扰动影响。

Conclusion: LLMs单独使用在物联网环境中进行安全关键交互威胁检测尚不可靠。需要结合符号分析与基于LLM的语义解释的混合架构，以减少误报同时保持结构严谨性。

Abstract: Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.

</details>


### [127] [QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models](https://arxiv.org/abs/2601.00679)
*Rachmad Vidya Wicaksana Putra,Pasindu Wickramasinghe,Muhammad Shafique*

Main category: cs.NE

Relevance: 85.0

TL;DR: QSLM：一个自动化量化框架，用于压缩脉冲驱动语言模型（SLMs），在满足性能和内存约束的同时，显著减少内存占用和功耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然性能优异，但计算成本高、内存占用大、功耗高，难以在嵌入式设备部署。现有的脉冲驱动语言模型（SLMs）虽然降低了功耗，但内存占用仍然过大。手动量化方法虽然有效但耗时耗力，无法适应不同网络、性能要求和内存预算的需求。

Method: QSLM框架首先识别网络架构的层次结构和各层对量化的敏感性，然后采用分层量化策略（全局、块级和模块级量化），利用多目标性能-内存权衡函数选择最终的量化设置。

Result: 实验结果表明，QSLM能将内存占用减少高达86.5%，功耗降低高达20%，同时在SST-2数据集上保持高达84.4%的情感分类准确率，在WikiText-2数据集上获得23.2的困惑度分数，接近原始非量化模型。

Conclusion: QSLM提供了一个可扩展的自动化量化解决方案，有效压缩SLMs的内存占用，同时保持高性能，为资源受限的嵌入式设备部署语言模型提供了实用方法。

Abstract: Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.

</details>


### [128] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

Relevance: 75.0

TL;DR: 研究探索了视觉语言模型在视频问答任务中基于置信度的选择性预测，发现在分布内设置置信度阈值可提供机制性控制，但在分布偏移下可靠性会下降。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测能力，即当不确定时能够拒绝回答而非冒险犯错。研究旨在验证基于置信度的拒绝机制是否能可靠控制错误率，以及在分布偏移下是否保持稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过设置置信度阈值epsilon进行选择性预测，分析风险-覆盖权衡曲线，并测试在分布偏移下的性能变化。

Result: 1) 在分布内，置信度阈值提供机制性控制，通过调整阈值可获得平滑的风险-覆盖权衡，有效降低错误率；2) 在分布偏移下，置信度阈值提供的控制可靠性显著下降。

Conclusion: 基于置信度的选择性预测在分布内有效，但在实际部署中面临分布偏移挑战，需要更稳健的置信度校准方法或替代机制来保证可靠性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [129] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文比较了三种神经推理方法：LLM推理、监督学习推理和显式模型推理。研究发现LLM不可靠，监督学习推理存在灾难性遗忘问题，而提出的球面神经网络通过显式模型构建实现了可靠的逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在逻辑推理方面不可靠，监督学习方法存在灾难性遗忘问题。需要探索更可靠的神经推理方法，特别是能够进行严格逻辑推理且保持推理一致性的方法。

Method: 提出球面神经网络，将概念表示为n维球面上的圆，通过补圆表示否定运算符，通过过滤不可满足的圆形配置实现可靠决策。该方法能够同时掌握16种三段论推理任务。

Result: 球面神经网络在经典三段论推理和析取三段论推理中都达到100%准确率，且不会出现灾难性遗忘。相比之下，监督学习的Euler Net在重新训练后对已学任务的准确率降至6.25%。

Conclusion: 在三种神经推理方法中，基于显式模型构建的神经推理是最可靠的。球面神经网络展示了显式模型构建在实现可靠逻辑推理方面的优势。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [130] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

Relevance: 75.0

TL;DR: 本文比较了三种基于LLM的软件漏洞检测方法：检索增强生成(RAG)、监督微调(SFT)和双代理框架，发现RAG方法在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，为自动化软件漏洞检测提供了新机遇。本文旨在比较不同LLM技术在软件漏洞检测中的有效性，以提升现代代码库的安全性。

Method: 研究评估了三种方法：1) 检索增强生成(RAG)，整合互联网和MITRE CWE数据库的外部领域知识；2) 监督微调(SFT)，使用参数高效的QLoRA适配器；3) 双代理LLM框架，其中第二个代理审核和优化第一个代理的输出。使用从Big-Vul和GitHub真实代码库编译的数据集，重点关注五个关键CWE类别。

Result: RAG方法取得了最高的整体准确率(0.86)和F1分数(0.85)，表明上下文增强的价值。SFT方法也表现出色。双代理系统在提高推理透明度和错误缓解方面显示出潜力，同时减少了资源开销。

Conclusion: 研究表明，整合领域专业知识机制能显著增强LLM在实际漏洞检测任务中的适用性。RAG方法因整合外部知识而表现最佳，为LLM在软件安全领域的实际应用提供了重要见解。

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [131] [The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth](https://arxiv.org/abs/2601.00306)
*Emilio Ferrara*

Main category: cs.CY

Relevance: 75.0

TL;DR: 论文提出"合成现实"概念，认为生成式AI的最大风险不是单个深度伪造，而是共享认知基础和制度验证实践的逐渐侵蚀，并提出了分层风险框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 当前公众讨论主要关注生成式AI的"深度伪造"等具体危害，但忽视了更广泛的社会技术转变：生成式AI创造了合成现实，其中内容、身份和社会互动被联合制造并相互强化，导致共享认知基础和制度验证实践的逐渐侵蚀。

Method: 1) 将合成现实形式化为分层堆栈（内容、身份、互动、制度）；2) 扩展生成式AI危害分类法；3) 阐明生成式AI引入的质性转变；4) 将近期风险案例（2023-2025）整合为案例库；5) 提出缓解堆栈和研究议程。

Result: 建立了合成现实的理论框架，识别了生成式AI带来的六个质性转变（成本崩溃、吞吐量、定制化、微细分、来源缺失、信任侵蚀），并通过实际案例展示了这些机制在欺诈、选举、骚扰等领域的表现。

Conclusion: 生成式AI的最大风险是共享认知基础的侵蚀，需要综合治理方法（来源基础设施、平台治理、制度工作流重新设计、公共韧性），并提出了"生成式AI悖论"：随着合成媒体无处不在，社会可能理性地完全忽视数字证据。

Abstract: Generative AI (GenAI) now produces text, images, audio, and video that can be perceptually convincing at scale and at negligible marginal cost. While public debate often frames the associated harms as "deepfakes" or incremental extensions of misinformation and fraud, this view misses a broader socio-technical shift: GenAI enables synthetic realities; coherent, interactive, and potentially personalized information environments in which content, identity, and social interaction are jointly manufactured and mutually reinforcing. We argue that the most consequential risk is not merely the production of isolated synthetic artifacts, but the progressive erosion of shared epistemic ground and institutional verification practices as synthetic content, synthetic identity, and synthetic interaction become easy to generate and hard to audit. This paper (i) formalizes synthetic reality as a layered stack (content, identity, interaction, institutions), (ii) expands a taxonomy of GenAI harms spanning personal, economic, informational, and socio-technical risks, (iii) articulates the qualitative shifts introduced by GenAI (cost collapse, throughput, customization, micro-segmentation, provenance gaps, and trust erosion), and (iv) synthesizes recent risk realizations (2023-2025) into a compact case bank illustrating how these mechanisms manifest in fraud, elections, harassment, documentation, and supply-chain compromise. We then propose a mitigation stack that treats provenance infrastructure, platform governance, institutional workflow redesign, and public resilience as complementary rather than substitutable, and outline a research agenda focused on measuring epistemic security. We conclude with the Generative AI Paradox: as synthetic media becomes ubiquitous, societies may rationally discount digital evidence altogether.

</details>


### [132] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

Relevance: 65.0

TL;DR: 该研究提出了一种针对尼日利亚皮钦语的自动抑郁症筛查方法，通过微调大语言模型来适应尼日利亚当地语言和文化背景，在资源受限环境中实现心理健康评估。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统工具如PHQ-9在高收入国家验证，但在尼日利亚等中低收入国家存在语言和文化障碍，当地使用皮钦语和520多种方言，需要适应本地语言的筛查工具。

Method: 收集432份尼日利亚年轻人皮钦语音频响应，进行转录、预处理和标注（语义标签、俚语解释、PHQ-9严重程度评分）。微调三种LLM（Phi-3-mini-4k-instruct、Gemma-3-4B-it、GPT-4.1）并在标注数据集上评估性能。

Result: GPT-4.1在PHQ-9严重程度评分预测中达到94.5%准确率，优于其他模型。定性评估中，GPT-4.1也产生最文化适宜、清晰且上下文相关的响应。

Conclusion: AI驱动的抑郁症筛查可为尼日利亚等语言多样、资源受限的社区提供解决方案，为部署对话式心理健康工具奠定基础。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [133] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

Relevance: 65.0

TL;DR: 该论文提出了一种基于不可逆信息处理的智能物理理论，将智能系统建模为耦合的智能体-环境过程，通过守恒定律约束信息编码，并定义了智能作为每单位不可逆处理信息产生的目标导向功。


<details>
  <summary>Details</summary>
Motivation: 建立智能的物理基础理论，将信息处理与物理守恒定律联系起来，为理解智能系统（包括生物和人工系统）提供统一的、与底物无关的理论框架。

Method: 提出守恒一致编码（CCE）框架，将编码对应为吸引子的亚稳态盆地；定义智能为每单位不可逆处理信息产生的目标导向功；推导信息摄入、不可逆计算和功提取的物理约束层次；分析振荡和近临界动力学；发展连续动力电路理论。

Result: 理论揭示了长时程效率需要保持内部信息结构，导致自建模现象；建立了物理体现智能系统的内在认知极限；分析了大脑接近理论预测的高效运行机制；展示了布尔逻辑作为吸引子选择的特例；提出了基于不可逆信息流和结构稳态的AI安全视角。

Conclusion: 该研究提供了智能作为物理现象的统一的、底物无关的理论框架，连接了信息处理、物理约束和智能行为，对理解生物智能和设计人工智能系统具有重要启示。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [134] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

Relevance: 65.0

TL;DR: MIDAS是一个分布式AI代理系统，用于工程设计的创新生成，通过模拟人类元认知构思流程，评估全局和局部新颖性，实现真正的人机协同创作。


<details>
  <summary>Details</summary>
Motivation: 当前"单次爆发"式AI系统在生成新颖多样的设计想法方面存在局限，它们倾向于产生大量语义聚集的想法，这对新手设计师构成了认知挑战。需要一种能够模拟人类元认知构思流程的系统，实现真正的人机协同创作。

Method: 提出MIDAS框架，用分布式"团队"的专门化AI代理替代单一AI范式，模拟人类元认知构思工作流程。系统逐步精炼想法，并评估每个想法的全局新颖性（相对于现有解决方案）和局部新颖性（相对于先前生成的想法）。

Result: MIDAS展示了一个可行且渐进式的真正人机协同创作范式，将人类设计师从被动的筛选者提升为参与式、主动的协作伙伴。

Conclusion: 分布式AI代理系统能够有效解决工程设计中的创新生成挑战，通过模拟人类认知流程实现更高质量的人机协同。

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [135] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

Relevance: 65.0

TL;DR: AgenticDomiKnowS (ADS) 使用智能体工作流将自由形式的任务描述自动转换为完整的 DomiKnowS 程序，显著降低神经符号编程的门槛，将开发时间从数小时缩短至10-15分钟。


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高模型的鲁棒性、可解释性和数据效率，但这一过程耗时且具有挑战性。现有的 DomiKnowS 框架虽然提供了高级声明式编程接口，但仍要求用户熟练掌握其特定语法，限制了更广泛的应用。

Method: ADS 采用智能体工作流，将自由形式的任务描述自动翻译为完整的 DomiKnowS 程序。该工作流单独创建和测试每个 DomiKnowS 组件，并支持可选的人工干预环节，允许熟悉 DomiKnowS 的用户对中间输出进行细化调整。

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时大幅减少到10-15分钟，显著提高了开发效率。

Conclusion: ADS 通过消除对特定库语法的依赖，降低了神经符号编程的门槛，使更多研究人员能够利用符号约束来增强深度学习模型，同时支持人机协作的工作流程。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


### [136] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

Relevance: 65.0

TL;DR: 提出了首个多智能体框架来自动化协调重命名，通过范围推断、计划执行和复制三个智能体协作，利用开发者初始重构作为线索来推断相关重构范围，显著减少开发者的重复工作负担。


<details>
  <summary>Details</summary>
Motivation: 协调重命名是软件开发中频繁但具有挑战性的任务，开发者需要手动在多个文件和上下文中传播重命名重构，这个过程既繁琐又容易出错。现有的启发式方法会产生大量误报，而普通大语言模型由于上下文有限且无法与重构工具交互，只能提供不完整的建议。

Method: 设计了首个多智能体框架，包含三个智能体：1) 范围推断智能体：将开发者的初始重构线索转化为明确的自然语言声明范围；2) 计划执行智能体：使用该范围作为严格计划，识别需要重构的程序元素，并通过调用IDE的可信重构API安全执行更改；3) 复制智能体：指导项目范围内的搜索。

Result: 在609K次提交的100个开源项目中进行了形成性研究，并调查了205名开发者。该框架能够有效自动化协调重命名任务，显著减少开发者的工作负担，同时保持开发者在主导地位。

Conclusion: AI智能体在软件开发中的主要价值在于扩展开发者的推理和行动能力，而不是取代人类参与。协调重命名正是智能体可以显著减轻开发者负担的重复性任务，同时保持开发者的主导地位。

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [137] [Improving Scientific Document Retrieval with Academic Concept Index](https://arxiv.org/abs/2601.00567)
*Jeyun Lee,Junhyoung Lee,Wonbin Kweon,Bowen Jin,Yu Zhang,Susik Yoon,Dongha Lee,Hwanjo Yu,Jiawei Han,Seongku Kang*

Main category: cs.IR

Relevance: 65.0

TL;DR: 本文提出了一种学术概念索引方法，通过提取和组织论文中的关键概念来改进科学领域检索器的适应，包括概念覆盖查询生成和概念聚焦上下文增强两个方向。


<details>
  <summary>Details</summary>
Motivation: 将通用领域检索器适应到科学领域面临两大挑战：1) 大规模领域相关标注数据的稀缺性；2) 词汇和信息需求的显著不匹配。现有基于LLM的方法（合成查询微调和辅助上下文生成）忽视了科学文档中嵌入的多样化学术概念，导致生成冗余或概念狭窄的查询和上下文。

Method: 提出学术概念索引，从论文中提取关键概念并按学术分类法组织。基于此索引：1) CCQGen：概念覆盖查询生成，自适应地基于未覆盖概念条件化LLM，生成具有更广概念覆盖的互补查询；2) CCExpand：概念聚焦辅助上下文，利用文档片段作为概念感知查询的简洁响应。

Result: 实验表明，将学术概念索引整合到查询生成和上下文增强中，能产生更高质量的查询、更好的概念对齐，并显著提升检索性能。

Conclusion: 学术概念索引能有效解决科学领域检索器适应中的概念覆盖问题，通过结构化概念组织提升LLM生成的查询和上下文质量，从而改善检索效果。

Abstract: Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.

</details>


### [138] [Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes](https://arxiv.org/abs/2601.00012)
*Shahar Ain Kedem,Itamar Zimerman,Eliya Nachmani*

Main category: eess.SP

Relevance: 45.0

TL;DR: 提出一种受NeRF启发的EEG信号处理方法，将脑电信号视为连续神经活动的3D场景，通过神经网络学习固定大小的权重向量表示，支持信号渲染、超分辨率重建和虚拟电极生成。


<details>
  <summary>Details</summary>
Motivation: EEG数据面临长度可变、信噪比低、个体差异大、时间漂移等挑战，且缺乏大规模干净数据集。现有深度学习方法难以有效处理EEG信号，需要新的建模方法来解决这些独特挑战。

Method: 借鉴NeRF思想，将EEG电极位置类比为不同视角，将连续神经活动类比为3D场景。训练神经网络以NeRF风格学习单个EEG样本，生成固定大小的权重向量表示。该方法支持从任意时间步和空间位置渲染EEG信号，实现超分辨率重建和虚拟电极生成。

Result: 方法能够连续可视化任意分辨率的脑活动，重建原始EEG信号，有效模拟不存在电极的数据。重构信号可输入标准EEG处理网络提升性能。

Conclusion: NeRF启发的EEG表示方法为解决EEG建模挑战提供了新思路，通过连续表示支持信号渲染、超分辨率重建和虚拟电极生成，为EEG处理开辟了新方向。

Abstract: Electroencephalography (EEG) data present unique modeling challenges because recordings vary in length, exhibit very low signal to noise ratios, differ significantly across participants, drift over time within sessions, and are rarely available in large and clean datasets. Consequently, developing deep learning methods that can effectively process EEG signals remains an open and important research problem. To tackle this problem, this work presents a new method inspired by Neural Radiance Fields (NeRF). In computer vision, NeRF techniques train a neural network to memorize the appearance of a 3D scene and then uses its learned parameters to render and edit the scene from any viewpoint. We draw an analogy between the discrete images captured from different viewpoints used to learn a continuous 3D scene in NeRF, and EEG electrodes positioned at different locations on the scalp, which are used to infer the underlying representation of continuous neural activity. Building on this connection, we show that a neural network can be trained on a single EEG sample in a NeRF style manner to produce a fixed size and informative weight vector that encodes the entire signal. Moreover, via this representation we can render the EEG signal at previously unseen time steps and spatial electrode positions. We demonstrate that this approach enables continuous visualization of brain activity at any desired resolution, including ultra high resolution, and reconstruction of raw EEG signals. Finally, our empirical analysis shows that this method can effectively simulate nonexistent electrodes data in EEG recordings, allowing the reconstructed signal to be fed into standard EEG processing networks to improve performance.

</details>


### [139] [PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices](https://arxiv.org/abs/2601.00367)
*Nandish Chattopadhyay,Abdul Basit,Amira Guesmi,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CR

Relevance: 45.0

TL;DR: PatchBlock：用于边缘AI的轻量级对抗性补丁检测与缓解框架，通过异常检测和降维技术保护神经网络免受补丁攻击，可在CPU上并行运行，保持系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用（如自动驾驶、监控）在资源受限设备上运行，容易受到对抗性补丁攻击（如恶意贴纸），这些攻击可能欺骗神经网络做出错误预测，带来严重后果。现有防御方法通常计算开销大，不适合边缘设备。

Method: PatchBlock采用三阶段流水线：1) 分块（Chunking）：将输入图像分割成块；2) 分离（Separating）：使用重新设计的隔离森林算法进行异常检测，通过针对性切割实现更快收敛；3) 缓解（Mitigating）：对识别的异常区域应用降维技术。该框架作为传感器级的预处理模块，可在CPU上并行运行，避免额外GPU开销。

Result: 在多种神经网络架构、基准数据集、攻击类型和边缘设备上的评估表明，PatchBlock在强补丁攻击（如Google Adversarial Patch）下能恢复高达77%的模型准确率，同时保持高可移植性和最小的干净准确率损失。在计算时间和能耗方面优于现有防御方法。

Conclusion: PatchBlock提供了一个模型无关、补丁无关的轻量级防御框架，可轻松集成到现有边缘AI系统中，有效提高对抗性补丁攻击下的鲁棒性，同时保持系统效率和吞吐量。

Abstract: Adversarial attacks pose a significant challenge to the reliable deployment of machine learning models in EdgeAI applications, such as autonomous driving and surveillance, which rely on resource-constrained devices for real-time inference. Among these, patch-based adversarial attacks, where small malicious patches (e.g., stickers) are applied to objects, can deceive neural networks into making incorrect predictions with potentially severe consequences. In this paper, we present PatchBlock, a lightweight framework designed to detect and neutralize adversarial patches in images. Leveraging outlier detection and dimensionality reduction, PatchBlock identifies regions affected by adversarial noise and suppresses their impact. It operates as a pre-processing module at the sensor level, efficiently running on CPUs in parallel with GPU inference, thus preserving system throughput while avoiding additional GPU overhead. The framework follows a three-stage pipeline: splitting the input into chunks (Chunking), detecting anomalous regions via a redesigned isolation forest with targeted cuts for faster convergence (Separating), and applying dimensionality reduction on the identified outliers (Mitigating). PatchBlock is both model- and patch-agnostic, can be retrofitted to existing pipelines, and integrates seamlessly between sensor inputs and downstream models. Evaluations across multiple neural architectures, benchmark datasets, attack types, and diverse edge devices demonstrate that PatchBlock consistently improves robustness, recovering up to 77% of model accuracy under strong patch attacks such as the Google Adversarial Patch, while maintaining high portability and minimal clean accuracy loss. Additionally, PatchBlock outperforms the state-of-the-art defenses in efficiency, in terms of computation time and energy consumption per sample, making it suitable for EdgeAI applications.

</details>


### [140] [CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge](https://arxiv.org/abs/2601.00549)
*Zhiheng Guo,Zhaoyang Liu,Zihan Cen,Chenyuan Feng,Xinghua Sun,Xiang Chen,Tony Q. S. Quek,Xijun Wang*

Main category: cs.IT

Relevance: 45.0

TL;DR: CoCo-Fed是一个压缩与组合联邦学习框架，通过双重维度梯度降维解决本地内存瓶颈，利用正交子空间叠加协议减少全局通信开销，在无线感知任务中实现高效内存和通信效率。


<details>
  <summary>Details</summary>
Motivation: 在O-RAN架构中部署大规模神经网络面临两个关键瓶颈：1) 资源受限的gNB上本地训练所需的内存占用过大；2) 高维模型更新在带宽受限的回程链路上进行全局聚合时导致带宽饱和。

Method: 提出CoCo-Fed框架：本地采用双重维度梯度降维，将优化器适配到低秩结构而不增加推理参数/延迟；全局引入基于正交子空间叠加的传输协议，将层间更新投影并叠加为每个gNB的单个矩阵，大幅减少回程流量。

Result: 在到达角估计任务上的大量仿真表明，CoCo-Fed在内存和通信效率方面显著优于现有基线方法，在非独立同分布设置下保持稳健收敛。

Conclusion: CoCo-Fed通过统一本地内存效率和全局通信减少，解决了O-RAN中边缘智能部署的关键瓶颈，并建立了严格的理论基础证明其在无线感知任务中的收敛性。

Abstract: The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings.

</details>


### [141] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

Relevance: 40.0

TL;DR: 提出ClinicalReTrial框架，将临床试验失败预测转化为主动的协议重新设计问题，通过自我演化的AI代理实现闭环优化，提升试验成功率。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法仅能预测临床试验失败风险，但无法提供可操作的补救措施。临床试验失败是药物开发的主要瓶颈，微小的协议设计缺陷就可能毁掉有前景的治疗方案。

Method: 将临床试验推理转化为迭代协议重新设计问题，集成失败诊断、安全感知修改和候选评估的闭环奖励驱动优化框架。使用结果预测模型作为模拟环境，维护分层记忆系统捕获迭代反馈和可转移的重新设计模式。

Result: ClinicalReTrial改进了83.3%的试验协议，平均成功率提升5.7%。回顾性案例研究显示发现的重新设计策略与实际临床试验修改高度一致。

Conclusion: 该框架填补了从被动风险预测到主动协议优化的关键空白，为临床试验设计提供了可操作的AI驱动解决方案。

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [142] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

Relevance: 35.0

TL;DR: Mortar系统结合质量多样性算法与大型语言模型，自动演化游戏机制用于游戏设计，通过树搜索合成完整游戏并评估其技能排序能力。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计是耗时且依赖专家经验的过程，需要自动化方法来探索多样化的游戏机制，降低游戏设计门槛。

Method: 结合质量多样性算法和LLM探索多样化机制，通过树搜索合成完整游戏，评估机制对玩家技能排序的贡献度。

Result: Mortar能生成多样化且可玩的游戏，其演化出的机制能更好地促进游戏中的技能排序，消融实验验证了各组件的重要性。

Conclusion: Mortar展示了LLM与进化算法结合在自动游戏设计中的潜力，为游戏机制自动演化提供了有效框架。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [143] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

Relevance: 35.0

TL;DR: ReCiSt是一个受生物自愈机制启发的分布式计算系统自主故障恢复框架，通过语言模型驱动的智能体实现故障隔离、诊断、自适应恢复和知识积累


<details>
  <summary>Details</summary>
Motivation: 分布式计算系统（DCCS）集成了从物联网设备到云计算的异构资源，其复杂性、移动性和动态运行条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。受生物系统自愈能力的启发，开发自主故障恢复框架。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个阶段：遏制、诊断、元认知和知识。使用语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源，实现自主故障恢复。

Result: 在公共故障数据集上使用多种语言模型评估，ReCiSt能够在数十秒内完成自愈，智能体CPU使用率最低为10%。展示了克服不确定性的分析深度和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功将生物自愈机制转化为分布式计算系统的自主故障恢复能力，通过语言模型驱动的智能体实现了最小化人工干预的弹性管理。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [144] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出ACCD框架，通过三阶段渐进式架构检测社交媒体上的协同虚假行为，使用记忆引导自适应机制动态学习最优检测配置，在真实数据集上实现87.3%的F1分数，比现有基线提升15.2%。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体协同虚假行为检测方法存在三个主要问题：1) 依赖表面相关性分析而非深层因果关系；2) 使用静态参数设置，无法适应多样化的协同场景；3) 需要大量人工标注，成本高昂且效率低下。需要一种更准确、高效且自动化的解决方案。

Method: 提出自适应因果协同检测（ACCD）框架，采用三阶段渐进式架构：1) 自适应收敛交叉映射（CCM）技术深入识别账户间真实因果关系；2) 半监督分类中集成主动学习和不确定性采样，减少人工标注负担；3) 基于历史检测经验的自动验证模块，实现检测结果的自验证和优化。采用记忆引导自适应机制动态学习最优检测配置。

Result: 在Twitter IRA数据集、Reddit协同痕迹和多个广泛使用的机器人检测基准上进行评估：1) 协同攻击检测F1分数达87.3%，比最强现有基线提升15.2%；2) 减少68%的人工标注需求；3) 通过层次聚类优化实现2.8倍处理速度提升。

Conclusion: ACCD为社交媒体协同行为识别提供了更准确、高效且高度自动化的端到端解决方案，具有重要的实际应用价值和广泛的推广潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [145] [Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI](https://arxiv.org/abs/2601.00014)
*Eran Zvuloni,Ronit Almog,Michael Glikson,Shany Brimer Biton,Ilan Green,Izhar Laufer,Offer Amir,Joachim A. Behar*

Main category: eess.SP

Relevance: 35.0

TL;DR: 使用深度学习模型DeepHHF分析24小时单导联心电图数据，预测5年内心力衰竭风险，性能优于30秒片段模型和临床评分


<details>
  <summary>Details</summary>
Motivation: 心力衰竭影响11.8%的65岁以上成年人，降低生活质量和寿命。预防心力衰竭可降低发病率和死亡率。研究者假设人工智能应用于24小时单导联心电图数据可以预测5年内的心力衰竭风险

Method: 使用Technion-Leumit Holter ECG数据集（69,663个记录，47,729名患者，20年数据）。开发DeepHHF深度学习模型，训练于24小时心电图记录，并与30秒片段模型和临床评分进行比较

Result: DeepHHF的AUC达到0.80，优于30秒片段模型和临床评分。高风险个体住院或死亡风险增加两倍。可解释性分析显示模型关注心律失常和心脏异常，关键注意力集中在上午8点至下午3点

Conclusion: 深度学习建模24小时连续心电图数据可行，能捕捉阵发性事件和昼夜节律变化，对可靠风险预测至关重要。人工智能应用于单导联Holter心电图无创、廉价、广泛可用，是心力衰竭风险预测的有前景工具

Abstract: Heart failure (HF) affects 11.8% of adults aged 65 and older, reducing quality of life and longevity. Preventing HF can reduce morbidity and mortality. We hypothesized that artificial intelligence (AI) applied to 24-hour single-lead electrocardiogram (ECG) data could predict the risk of HF within five years. To research this, the Technion-Leumit Holter ECG (TLHE) dataset, including 69,663 recordings from 47,729 patients, collected over 20 years was used. Our deep learning model, DeepHHF, trained on 24-hour ECG recordings, achieved an area under the receiver operating characteristic curve of 0.80 that outperformed a model using 30-second segments and a clinical score. High-risk individuals identified by DeepHHF had a two-fold chance of hospitalization or death incidents. Explainability analysis showed DeepHHF focused on arrhythmias and heart abnormalities, with key attention between 8 AM and 3 PM. This study highlights the feasibility of deep learning to model 24-hour continuous ECG data, capturing paroxysmal events and circadian variations essential for reliable risk prediction. Artificial intelligence applied to single-lead Holter ECG is non-invasive, inexpensive, and widely accessible, making it a promising tool for HF risk prediction.

</details>


### [146] [Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing](https://arxiv.org/abs/2601.00020)
*Nikhil Garg,Anxiong Song,Niklas Plessnig,Nathan Savoia,Laura Bégon-Lours*

Main category: cs.NE

Relevance: 35.0

TL;DR: 该论文提出使用铁电忆阻器实现脉冲神经网络的设备感知训练和自适应学习，用于EEG运动想象解码，在资源受限平台上实现个性化脑机接口


<details>
  <summary>Details</summary>
Motivation: 脑电图(EEG)脑机接口面临非平稳神经信号的挑战，信号在会话和个体间变化大，限制了通用模型的泛化能力，需要在资源受限平台上实现自适应和个性化学习

Method: 1) 制造、表征和建模铁电突触器件；2) 采用卷积-循环SNN架构；3) 两种部署策略：设备感知训练和软件训练权重转移+设备端微调；4) 设备感知权重更新策略：梯度更新数字累积，超过阈值时转换为离散编程事件

Result: 两种部署策略都实现了与最先进软件SNN相当的分类性能；通过仅重新训练最终网络层的特定主题迁移学习提高了分类准确率

Conclusion: 可编程铁电硬件能够支持脉冲神经网络的稳健、低开销自适应，为神经信号的个性化神经形态处理开辟了实用路径

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.

</details>


### [147] [Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration](https://arxiv.org/abs/2601.00129)
*Ziang Yin,Hongjian Zhou,Nicholas Gangi,Meng Zhang,Jeff Zhang,Zhaoran Rena Huang,Jiaqi Gu*

Main category: physics.optics

Relevance: 35.0

TL;DR: 该论文提出了实现实用光子AI系统的三个关键考虑因素，并开发了一个跨层工具链来支持从早期探索到物理实现的光子AI设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型规模的扩大，传统电子计算面临能耗和性能瓶颈。光子计算具有高能效和低延迟的潜力，但现有光子AI系统在支持现代模型、管理开销和处理硬件非理想性方面存在不足。论文旨在解决这些挑战，实现可扩展的实用光子AI系统。

Method: 论文提出了三个关键考虑因素：1) 支持动态张量操作（特别是注意力/Transformer工作负载）；2) 系统管理转换、控制和数据移动开销；3) 在硬件非理想性下的鲁棒性。为定量研究这些权衡，开发了跨层工具链：SimPhony用于实现感知建模和快速跨层评估，ADEPT/ADEPT-Z用于端到端电路和拓扑探索，Apollo/LiDAR用于可扩展的光子物理设计自动化。

Result: 开发了一个完整的光子AI设计工具链，能够将物理成本转化为系统级指标，连接系统目标到可行的光子结构，并将候选电路转化为可制造的布局，同时考虑布线、热和串扰约束。

Conclusion: 论文通过识别关键设计考虑因素和开发相应的工具链，为构建实用、可扩展的光子AI系统提供了系统化方法。这些工具使架构决策基于现实假设，有助于光子计算在AI加速领域的实际应用。

Abstract: In this work, we identify three considerations that are essential for realizing practical photonic AI systems at scale: (1) dynamic tensor operation support for modern models rather than only weight-static kernels, especially for attention/Transformer-style workloads; (2) systematic management of conversion, control, and data-movement overheads, where multiplexing and dataflow must amortize electronic costs instead of letting ADC/DAC and I/O dominate; and (3) robustness under hardware non-idealities that become more severe as integration density grows. To study these coupled tradeoffs quantitatively, and to ensure they remain meaningful under real implementation constraints, we build a cross-layer toolchain that supports photonic AI design from early exploration to physical realization. SimPhony provides implementation-aware modeling and rapid cross-layer evaluation, translating physical costs into system-level metrics so architectural decisions are grounded in realistic assumptions. ADEPT and ADEPT-Z enable end-to-end circuit and topology exploration, connecting system objectives to feasible photonic fabrics under practical device and circuit constraints. Finally, Apollo and LiDAR provide scalable photonic physical design automation, turning candidate circuits into manufacturable layouts while accounting for routing, thermal, and crosstalk constraints.

</details>


### [148] [Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow](https://arxiv.org/abs/2601.00130)
*Hongjian Zhou,Ziang Yin,Jiaqi Gu*

Main category: physics.optics

Relevance: 35.0

TL;DR: 提出一个跨层协同设计与自动化框架，旨在降低光子AI系统开发门槛，包括可扩展光子边缘AI和Transformer推理架构设计、开源建模工具SimPhony，以及AI驱动的光子设计自动化方法。


<details>
  <summary>Details</summary>
Motivation: 光子技术为高性能AI系统提供了速度、并行性和能效优势，但电子-光子AI系统的设计和部署面临陡峭的学习曲线，缺乏成熟的电子-光子设计自动化工具链，导致设计周期长且限制了跨学科创新。

Method: 1) 提出可扩展光子边缘AI和Transformer推理架构设计；2) 开发开源建模工具SimPhony用于快速EPIC AI系统评估和设计空间探索；3) 推进AI驱动的光子设计自动化，包括基于物理AI的Maxwell求解器、制造感知的逆向设计框架，以及用于元光学神经网络的可扩展逆向训练算法。

Result: 构建了一个完整的跨层协同设计与自动化框架，为下一代电子-光子AI系统提供了可扩展的EPDA堆栈，降低了光子AI系统开发的技术门槛。

Conclusion: 该工作通过跨层协同设计和自动化框架，有望推动光子AI系统的民主化开发，促进电子-光子AI系统的跨学科创新和协同进化。

Abstract: Photonics is becoming a cornerstone technology for high-performance AI systems and scientific computing, offering unparalleled speed, parallelism, and energy efficiency. Despite this promise, the design and deployment of electronic-photonic AI systems remain highly challenging due to a steep learning curve across multiple layers, spanning device physics, circuit design, system architecture, and AI algorithms. The absence of a mature electronic-photonic design automation (EPDA) toolchain leads to long, inefficient design cycles and limits cross-disciplinary innovation and co-evolution. In this work, we present a cross-layer co-design and automation framework aimed at democratizing photonic AI system development. We begin by introducing our architecture designs for scalable photonic edge AI and Transformer inference, followed by SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. We then highlight advances in AI-enabled photonic design automation, including physical AI-based Maxwell solvers, a fabrication-aware inverse design framework, and a scalable inverse training algorithm for meta-optical neural networks, enabling a scalable EPDA stack for next-generation electronic-photonic AI systems.

</details>


### [149] [MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection](https://arxiv.org/abs/2601.00143)
*Gang Qu,Guanghao Li,Zhongming Zhao*

Main category: q-bio.GN

Relevance: 35.0

TL;DR: MethConvTransformer：基于Transformer的深度学习框架，整合脑部和外周组织DNA甲基化数据，用于阿尔茨海默病生物标志物发现，在多个数据集上优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）是一种多因素神经退行性疾病，伴有广泛的表观遗传失调。DNA甲基化作为稳定且动态的表观遗传修饰，有望成为早期AD检测的非侵入性生物标志物。然而，甲基化特征在不同组织和研究间差异显著，限制了可重复性和转化应用。

Method: 开发MethConvTransformer框架，结合CpG位点的线性投影、卷积层和自注意力层，捕捉局部和长程依赖关系，同时整合受试者水平协变量和组织嵌入，区分共享和区域特异性甲基化效应。

Result: 在六个GEO数据集和独立的ADNI验证队列中，模型始终优于传统机器学习基线，实现卓越的区分能力和泛化性能。可解释性分析揭示了与AD相关通路一致的生物学意义甲基化模式。

Conclusion: MethConvTransformer提供了稳健的跨组织表观遗传生物标志物，同时提供多分辨率可解释性，推进了基于甲基化的可重复诊断，并为疾病机制提供了可检验的假设。

Abstract: Alzheimer's disease (AD) is a multifactorial neurodegenerative disorder characterized by progressive cognitive decline and widespread epigenetic dysregulation in the brain. DNA methylation, as a stable yet dynamic epigenetic modification, holds promise as a noninvasive biomarker for early AD detection. However, methylation signatures vary substantially across tissues and studies, limiting reproducibility and translational utility. To address these challenges, we develop MethConvTransformer, a transformer-based deep learning framework that integrates DNA methylation profiles from both brain and peripheral tissues to enable biomarker discovery. The model couples a CpG-wise linear projection with convolutional and self-attention layers to capture local and long-range dependencies among CpG sites, while incorporating subject-level covariates and tissue embeddings to disentangle shared and region-specific methylation effects. In experiments across six GEO datasets and an independent ADNI validation cohort, our model consistently outperforms conventional machine-learning baselines, achieving superior discrimination and generalization. Moreover, interpretability analyses using linear projection, SHAP, and Grad-CAM++ reveal biologically meaningful methylation patterns aligned with AD-associated pathways, including immune receptor signaling, glycosylation, lipid metabolism, and endomembrane (ER/Golgi) organization. Together, these results indicate that MethConvTransformer delivers robust, cross-tissue epigenetic biomarkers for AD while providing multi-resolution interpretability, thereby advancing reproducible methylation-based diagnostics and offering testable hypotheses on disease mechanisms.

</details>


### [150] [Latent Flow Matching for Expressive Singing Voice Synthesis](https://arxiv.org/abs/2601.00217)
*Minhyeok Yun,Yong-Hoon Choi*

Main category: cs.SD

Relevance: 35.0

TL;DR: FM-Singer：基于条件流匹配的歌声合成方法，通过潜在空间中的连续向量场传输先验潜在变量到后验潜在变量，解决cVAE中的先验-后验失配问题，提升歌声的细微表现力。


<details>
  <summary>Details</summary>
Motivation: 基于条件变分自编码器(cVAE)的歌声合成存在先验-后验失配问题，导致细微表现力（如颤音和微韵律）下降。需要一种方法来改善潜在空间分布匹配，同时保持高效推理。

Method: 提出FM-Singer，在潜在空间中引入条件流匹配(CFM)，学习将先验潜在变量传输到后验潜在变量的连续向量场。推理时通过求解常微分方程(ODE)优化先验样本，再生成波形。

Result: 在韩语和中文歌声数据集上的实验显示，相比强基线方法，FM-Singer在韩语数据集上取得了更低的梅尔倒谱失真和基频误差，以及更高的感知评分。

Conclusion: FM-Singer通过条件流匹配有效解决了cVAE中的先验-后验失配问题，提升了歌声合成的细微表现力，同时保持了并行解码的高效性。

Abstract: Conditional variational autoencoder (cVAE)-based singing voice synthesis provides efficient inference and strong audio quality by learning a score-conditioned prior and a recording-conditioned posterior latent space. However, because synthesis relies on prior samples while training uses posterior latents inferred from real recordings, imperfect distribution matching can cause a prior-posterior mismatch that degrades fine-grained expressiveness such as vibrato and micro-prosody. We propose FM-Singer, which introduces conditional flow matching (CFM) in latent space to learn a continuous vector field transporting prior latents toward posterior latents along an optimal-transport-inspired path. At inference time, the learned latent flow refines a prior sample by solving an ordinary differential equation (ODE) before waveform generation, improving expressiveness while preserving the efficiency of parallel decoding. Experiments on Korean and Chinese singing datasets demonstrate consistent improvements over strong baselines, including lower mel-cepstral distortion and fundamental-frequency error and higher perceptual scores on the Korean dataset. Code, pretrained checkpoints, and audio demos are available at https://github.com/alsgur9368/FM-Singer

</details>


### [151] [Neural Minimum Weight Perfect Matching for Quantum Error Codes](https://arxiv.org/abs/2601.00242)
*Yotam Peled,David Zenati,Eliya Nachmani*

Main category: quant-ph

Relevance: 35.0

TL;DR: 提出NMWPM解码器，结合GNN提取局部特征和Transformer捕获全局依赖，为MWPM解码器预测动态边权重，通过代理损失函数实现端到端优化，显著降低量子纠错的逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 量子计算需要量子纠错（QEC）来实现其全部潜力。传统MWPM解码器使用固定边权重，无法充分利用量子纠错码的局部和全局特征信息，限制了纠错性能。

Method: 提出神经最小权重完美匹配（NMWPM）解码器：1）使用GNN提取局部综合征特征；2）使用Transformer捕获长程全局依赖；3）预测MWPM解码器的动态边权重；4）设计代理损失函数解决MWPM不可微问题，实现端到端优化。

Result: 相比标准基线，NMWPM显著降低了逻辑错误率（LER），证明了结合神经网络预测能力和经典匹配算法结构的混合解码器的优势。

Conclusion: NMWPM解码器通过神经-经典混合架构有效提升了量子纠错性能，为量子计算中的纠错解码提供了新的数据驱动方法。

Abstract: Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.

</details>


### [152] [Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective](https://arxiv.org/abs/2601.00257)
*Aly Sabri Abdalla,Vuk Marojevic*

Main category: eess.SY

Relevance: 35.0

TL;DR: 本文提出了一种基于O-RAN的低空经济框架，通过语义感知rApp和强化学习xApp实现无人机群实时轨迹规划，并讨论了相关测试平台和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 低空经济应用（如无人机物流和应急响应）在复杂信号受限环境中面临实时、弹性、上下文感知的编排挑战，现有AI技术缺乏针对LAE任务的专门集成。

Method: 提出O-RAN支持的LAE框架，利用解耦的RAN架构、开放接口和RAN智能控制器实现闭环AI优化。通过语义感知rApp作为地形解释器，为强化学习xApp提供语义指导，实现无人机群实时轨迹规划。

Result: 评估了所提架构的可行性和性能，展示了语义感知rApp与强化学习xApp协同工作的有效性，并调研了可用于LAE研究的无人机测试平台能力。

Conclusion: 该O-RAN框架为LAE任务提供了AI优化的解决方案，但仍面临关键研究挑战和标准化需求，需要进一步探索。

Abstract: Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.

</details>


### [153] [Mapping Human Anti-collusion Mechanisms to Multi-agent AI](https://arxiv.org/abs/2601.00360)
*Jamiu Adekunle Idowu,Ahmed Almasoud,Ayman Alfahid*

Main category: cs.MA

Relevance: 35.0

TL;DR: 该论文提出了一个将人类反合谋机制应用于多智能体AI系统的框架，包括建立分类体系、映射干预措施，并识别了实施中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统日益自主，它们可能发展出类似人类市场中的合谋策略。虽然人类领域积累了数个世纪的反合谋机制，但如何将这些机制适应到AI环境中仍不明确。本文旨在填补这一空白。

Method: 1) 开发人类反合谋机制的分类体系（制裁、宽大处理与举报、监控与审计、市场设计、治理）；2) 将这些机制映射到多智能体AI系统的潜在干预措施；3) 为每种机制提出实施方法；4) 识别开放挑战。

Result: 建立了一个系统性的框架，将人类反合谋机制转化为适用于AI系统的干预措施，并明确了实施路径和关键挑战。

Conclusion: 人类反合谋机制可以适应到多智能体AI系统中，但需要解决归因问题、身份流动性、边界问题和对抗性适应等独特挑战。这为开发AI系统的反合谋机制提供了理论基础。

Abstract: As multi-agent AI systems become increasingly autonomous, evidence shows they can develop collusive strategies similar to those long observed in human markets and institutions. While human domains have accumulated centuries of anti-collusion mechanisms, it remains unclear how these can be adapted to AI settings. This paper addresses that gap by (i) developing a taxonomy of human anti-collusion mechanisms, including sanctions, leniency & whistleblowing, monitoring & auditing, market design, and governance and (ii) mapping them to potential interventions for multi-agent AI systems. For each mechanism, we propose implementation approaches. We also highlight open challenges, such as the attribution problem (difficulty attributing emergent coordination to specific agents) identity fluidity (agents being easily forked or modified) the boundary problem (distinguishing beneficial cooperation from harmful collusion) and adversarial adaptation (agents learning to evade detection).

</details>


### [154] [Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing](https://arxiv.org/abs/2601.00384)
*Md Mahbub Hasan,Marcus Sternhagen,Krishna Chandra Roy*

Main category: cs.CR

Relevance: 35.0

TL;DR: 该论文研究3D打印系统中的网络安全威胁，提出基于Transformer的入侵检测系统来防御G代码文件在传输过程中被篡改的攻击。


<details>
  <summary>Details</summary>
Motivation: 增材制造在关键领域的应用日益广泛，但CAD与机器执行层之间的接口引入了新的攻击面。传统切片软件和运行时接口无法检测G代码文件在传输过程中被中间人攻击篡改的威胁，导致打印出结构缺陷但外观正常的零件。

Method: 提出无监督入侵检测系统：1) 使用冻结的Transformer编码器（BERT变体）提取系统行为的语义表示；2) 通过对比学习训练投影头学习异常敏感嵌入；3) 采用基于聚类的方法和自注意力自编码器进行分类。

Result: 实验结果表明，该方法能有效区分正常和受攻击的执行过程，成功检测Creality K1 Max和Ender 3两种FDM系统中的中间人攻击。

Conclusion: 论文展示了3D打印系统中的网络安全威胁，并提出了基于Transformer的检测方案。该方法为增材制造系统的安全防护提供了有效解决方案。

Abstract: Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.

</details>


### [155] [LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization](https://arxiv.org/abs/2601.00770)
*Simon Paquette-Greenbaum,Jiangbo Yu*

Main category: cs.CE

Relevance: 35.0

TL;DR: 本文提出了一种用于基数约束均值-方差投资组合优化（CCPO）的新型智能体框架，该框架能够自动化复杂的工作流程和算法开发，在基准问题上达到最先进算法的性能水平。


<details>
  <summary>Details</summary>
Motivation: 投资组合优化是金融机构的核心任务，CCPO作为混合整数二次规划问题，传统精确求解器难以处理，需要大量启发式算法。现有方法需要繁琐的工作流程和大量算法开发工作，而智能体框架在组合优化中显示出自动化工作流程和算法开发的潜力。

Method: 研究实现了一种用于CCPO的新型智能体框架，探索了多种具体架构设计。该框架旨在自动化复杂的投资组合优化工作流程，并能够开发有效的启发式算法。

Result: 在基准问题上，实现的智能体框架能够匹配最先进算法的性能。同时显著减轻了复杂工作流程和算法开发的工作量，在最坏情况下仅报告了较低但可接受的误差。

Conclusion: 智能体框架是解决CCPO问题的有效方法，能够自动化复杂工作流程和算法开发，同时保持与最先进算法相当的性能，为投资组合优化提供了新的解决方案。

Abstract: Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.

</details>


### [156] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

Relevance: 25.0

TL;DR: 提出基于MinDist度量的规则框架用于13张印度拉米纸牌游戏，通过编辑距离量化手牌与完成状态的结构接近度，结合对手建模显著提升胜率


<details>
  <summary>Details</summary>
Motivation: 13张印度拉米纸牌是一个不完全信息的顺序游戏，需要概率推理和组合决策。传统启发式方法缺乏形式化框架，需要更系统化的策略设计方法。

Method: 提出MinDist度量，修改MinScore算法，量化手牌与最近有效配置的编辑距离。设计计算高效算法，使用动态剪枝和模式缓存。结合对手建模的两玩家零和模拟框架，使用统计假设检验评估策略。

Result: 基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist度量能有效捕捉手牌结构接近完成状态的程度，结合对手建模的规则框架显著改善游戏表现，为不完全信息顺序游戏提供了新的策略设计方法。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [157] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

Relevance: 25.0

TL;DR: 研究探讨生成式AI如何理解乡土建筑智慧，以伊朗鸽塔为例测试三种扩散模型，发现AI能重现几何模式但误解材料和气候逻辑，参考图像提升真实性但限制创造力


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI系统如何解释乡土建筑形式中蕴含的建筑智慧，探索AI在感知、扭曲和重新想象传统设计智能方面的能力边界

Method: 以伊朗鸽塔为案例研究，测试三种扩散模型（Midjourney v6、DALL-E 3、基于SDXL的DreamStudio），采用三个提示阶段（参考性、适应性、推测性），使用五标准评估框架（类型学、材料性、环境、真实性、文化特异性）

Result: AI能可靠地重现几何模式，但误解材料和气候推理逻辑；参考图像能提高真实性但限制创造力，而无参考的自由生成则产生创新但文化模糊的结果

Conclusion: 定义了视觉相似性与建筑推理之间的边界，提出计算乡土推理作为分析AI如何感知、扭曲和重新想象传统设计智能的框架

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [158] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

Relevance: 25.0

TL;DR: 论文提出金融蜂群模型，将流动性博弈与理性蜂群理论结合，研究独立交易者如何通过差异奖励实现个体盈利与市场流动性的集体目标。


<details>
  <summary>Details</summary>
Motivation: 将蜂群方法应用于金融市场流动性建模，同时将金融分析技术用于蜂群分析，有望推动两个领域的发展。在蜂群研究中，博弈论方法有望解释观察到的集体效用遵从现象；在金融市场中，理解独立金融代理如何自组织以改善市场稳定性对市场设计研究至关重要。

Method: 将流动性博弈（交易者收益取决于交易中的总流动性）与理性蜂群（去中心化代理使用差异奖励将自利学习与全局目标对齐）统一起来。在马尔可夫团队博弈框架中使用差异奖励，构建金融蜂群模型。

Result: 证明个体流动性最大化行为有助于整体市场流动性，无需协调或共谋。金融蜂群模型为建模理性独立代理提供了框架，使它们在双边资产市场中既能实现个体盈利又能达成集体市场效率。

Conclusion: 金融蜂群模型为研究独立交易者如何通过自组织实现市场流动性提供了理论框架，连接了博弈论、蜂群智能和金融市场设计，展示了差异奖励在协调个体理性与集体目标方面的有效性。

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [159] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

Relevance: 25.0

TL;DR: 将计算语言学中的语义空间推理方法扩展到团队运动战术决策，将球员视为词汇、团队配合视为语义结构，通过向量表示和距离度量评估战术匹配度


<details>
  <summary>Details</summary>
Motivation: 将计算语言学中的语义空间推理方法应用于团队运动战术分析，建立文本与团队之间的类比关系，为集体决策和性能优化提供可推广的框架

Method: 将球员表示为多维向量（技术、身体、心理属性），通过上下文加权聚合成团队语义表示，在共享向量空间中编码战术模板，使用向量距离度量评估战术匹配度

Result: 开发了Python原型系统，能够生成可解释的动态自适应策略建议，并提供属性级别的细粒度诊断洞察，方法可推广到篮球、曲棍球、协作机器人等多个领域

Conclusion: 该方法为团队领域的集体决策和性能优化提供了通用框架，未来方向包括真实数据集成、预测模拟和混合人机战术智能

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [160] [Probability-Aware Parking Selection](https://arxiv.org/abs/2601.00521)
*Cameron Hickert,Sirui Li,Zhengbing He,Cathy Wu*

Main category: eess.SY

Relevance: 25.0

TL;DR: 论文提出概率感知停车选择问题，通过动态规划框架帮助司机选择最佳停车位置而非直接前往目的地，考虑停车场可用性的概率信息，在真实数据实验中显示时间节省可达66%。


<details>
  <summary>Details</summary>
Motivation: 现有停车导航系统低估总旅行时间，未考虑寻找停车位的时间，这影响用户体验、出行方式选择、交通拥堵和排放。需要解决停车可用性的不确定性，提供更准确的旅行时间估计。

Method: 提出概率感知停车选择问题，采用可适应的动态规划框架进行决策，基于停车场级别的概率可用性信息。进行闭式分析确定何时针对特定停车场或探索替代方案，以及预期时间成本。利用随机观测估计停车可用性，分析观测误差。

Result: 使用美国西雅图真实数据实验，平均绝对误差从7%降至2%以下（随观测频率增加）。数据模拟显示，概率感知策略相比概率无感知基线时间节省达66%，但仍比直接到目的地估计多花123%时间。

Conclusion: 概率感知停车选择方法能有效考虑停车可用性的动态特性，显著改善停车导航系统的准确性，但实际停车搜索时间仍显著影响总旅行时间估计。

Abstract: Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates.

</details>


### [161] [Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks](https://arxiv.org/abs/2601.00538)
*Chi-Te Kuo,Li-Hsiang Shen,Jyun-Jhe Huang*

Main category: eess.SP

Relevance: 25.0

TL;DR: 论文提出了一种多智能体混合深度强化学习方法（PMHRL），用于优化多MF-RIS辅助的NOMA下行网络，通过联合优化功率分配、波束成形、RIS配置和位置部署来最大化能效。


<details>
  <summary>Details</summary>
Motivation: 传统RIS存在信号覆盖有限和能量自持能力不足的问题。多功能可重构智能表面（MF-RIS）通过主动RIS能力和能量收集功能，能够扩展信号覆盖并实现自持性，但需要有效的优化方法来协调多个MF-RIS的配置。

Method: 提出参数化共享的多智能体混合深度强化学习（PMHRL）框架，其中多智能体近端策略优化（PPO）处理连续变量（如功率分配、波束成形），深度Q网络（DQN）处理离散变量（如RIS位置）。通过联合优化功率分配、发射波束成形、MF-RIS的幅度、相移、能量收集比以及RIS位置。

Result: 仿真结果表明，PMHRL方法相比其他基准（无参数化共享、纯PPO、纯DQN）具有最高的能效。多MF-RIS辅助的NOMA下行网络相比无能量收集/放大的场景、传统RIS以及无RIS/MF-RIS部署，在不同多址接入方式下都能实现最高能效。

Conclusion: PMHRL方法能够有效优化多MF-RIS辅助的NOMA网络，显著提升系统能效，为未来无线通信系统中的智能表面部署提供了有效的优化框架。

Abstract: Multi-functional reconfigurable intelligent surface (MF-RIS) is conceived to address the communication efficiency thanks to its extended signal coverage from its active RIS capability and self-sustainability from energy harvesting (EH). We investigate the architecture of multi-MF-RISs to assist non-orthogonal multiple access (NOMA) downlink networks. We formulate an energy efficiency (EE) maximization problem by optimizing power allocation, transmit beamforming and MF-RIS configurations of amplitudes, phase-shifts and EH ratios, as well as the position of MF-RISs, while satisfying constraints of available power, user rate requirements, and self-sustainability property. We design a parametrized sharing scheme for multi-agent hybrid deep reinforcement learning (PMHRL), where the multi-agent proximal policy optimization (PPO) and deep-Q network (DQN) handle continuous and discrete variables, respectively. The simulation results have demonstrated that proposed PMHRL has the highest EE compared to other benchmarks, including cases without parametrized sharing, pure PPO and DQN. Moreover, the proposed multi-MF-RISs-aided downlink NOMA achieves the highest EE compared to scenarios of no-EH/amplification, traditional RISs, and deployment without RISs/MF-RISs under different multiple access.

</details>


### [162] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

Relevance: 15.0

TL;DR: 提出一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑优化包裹分配给配送员，确保每个配送员每天完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的包裹分配方法效率低下，导致配送员之间工作量分布不平衡。需要优化系统以改善配送时间，实现所有员工之间的完整工作量平衡。

Method: 采用多算法方法，包括不同版本的k-means、进化方法、基于k-means初始化的递归分配（使用不同问题编码）以及混合进化集成算法。算法同时考虑配送点距离和配送员位置。

Result: 在西班牙Azuqueca de Henares的实际最后一公里包裹配送系统中验证了所提方法的性能，展示了其在现实世界问题中的有效性。

Conclusion: 提出的多算法方法能够有效解决最后一公里包裹配送中的工作量平衡问题，确保配送员之间工作量均衡分配，提高系统效率。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [163] [Hear the Heartbeat in Phases: Physiologically Grounded Phase-Aware ECG Biometrics](https://arxiv.org/abs/2601.00170)
*Jintao Huang,Lu Leng,Yi Zhang,Ziyuan Yang*

Main category: eess.IV

Relevance: 15.0

TL;DR: 提出HPAF框架用于ECG身份认证，通过三阶段分层相位感知融合和心跳感知多原型注册策略，在公开数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有ECG身份认证方法将心跳视为同质信号，忽略了心脏周期内相位特异性特征。需要更精细地建模相位特异性形态和变化线索。

Method: 1. 三阶段HPAF框架：IPR独立提取各心脏相位表示；PGHF分层融合生理相关相位；GRF自适应平衡贡献生成统一身份表示。2. HAM注册策略：构建多原型图库模板集减少心跳特异性噪声影响。

Result: 在三个公开数据集上，HPAF在闭集和开集设置下均优于其他方法，达到最先进的性能。

Conclusion: 通过显式建模心脏相位特异性特征和心跳间变异性，HPAF框架显著提升了ECG身份认证的准确性和鲁棒性。

Abstract: Electrocardiography (ECG) is adopted for identity authentication in wearable devices due to its individual-specific characteristics and inherent liveness. However, existing methods often treat heartbeats as homogeneous signals, overlooking the phase-specific characteristics within the cardiac cycle. To address this, we propose a Hierarchical Phase-Aware Fusion~(HPAF) framework that explicitly avoids cross-feature entanglement through a three-stage design. In the first stage, Intra-Phase Representation (IPR) independently extracts representations for each cardiac phase, ensuring that phase-specific morphological and variation cues are preserved without interference from other phases. In the second stage, Phase-Grouped Hierarchical Fusion (PGHF) aggregates physiologically related phases in a structured manner, enabling reliable integration of complementary phase information. In the final stage, Global Representation Fusion (GRF) further combines the grouped representations and adaptively balances their contributions to produce a unified and discriminative identity representation. Moreover, considering ECG signals are continuously acquired, multiple heartbeats can be collected for each individual. We propose a Heartbeat-Aware Multi-prototype (HAM) enrollment strategy, which constructs a multi-prototype gallery template set to reduce the impact of heartbeat-specific noise and variability. Extensive experiments on three public datasets demonstrate that HPAF achieves state-of-the-art results in the comparison with other methods under both closed and open-set settings.

</details>


### [164] [Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics](https://arxiv.org/abs/2601.00277)
*Ali Anaissi,Seid Miad Zandavi,Weidong Huang,Junaid Akram,Basem Suleiman,Ali Braytee,Jie Hua*

Main category: q-bio.QM

Relevance: 15.0

TL;DR: 该研究系统评估了单细胞多模态数据整合分析流程中不同预处理策略（归一化、降维、整合方法）的组合性能，发现Seurat和Harmony在数据整合方面表现优异，UMAP是最兼容的降维方法。


<details>
  <summary>Details</summary>
Motivation: 单细胞多模态数据整合分析对个性化医疗有重要意义，但目前缺乏对不同预处理策略组合的系统评估。研究旨在填补这一空白，为单细胞数据分析提供最佳实践指导。

Method: 构建通用单细胞数据分析流程，包含归一化、数据整合和降维三个步骤。在6个不同模态、组织和物种的数据集上，评估7种归一化方法、4种降维方法和5种整合方法的组合性能，使用Silhouette Coefficient Score、Adjusted Rand Index和Calinski-Harabasz Index三个指标。

Result: Seurat和Harmony在数据整合方面表现最佳，其中Harmony在处理大型数据集时更高效。UMAP是与整合方法兼容性最好的降维方法。归一化方法的选择取决于所使用的整合方法。

Conclusion: 该研究为单细胞多模态数据整合分析提供了系统评估框架和实用建议，有助于优化分析流程并提高结果可靠性。

Abstract: Single-cell data analysis has the potential to revolutionize personalized medicine by characterizing disease-associated molecular changes at the single-cell level. Advanced single-cell multimodal assays can now simultaneously measure various molecules (e.g., DNA, RNA, Protein) across hundreds of thousands of individual cells, providing a comprehensive molecular readout. A significant analytical challenge is integrating single-cell measurements across different modalities. Various methods have been developed to address this challenge, but there has been no systematic evaluation of these techniques with different preprocessing strategies. This study examines a general pipeline for single-cell data analysis, which includes normalization, data integration, and dimensionality reduction. The performance of different algorithm combinations often depends on the dataset sizes and characteristics. We evaluate six datasets across diverse modalities, tissues, and organisms using three metrics: Silhouette Coefficient Score, Adjusted Rand Index, and Calinski-Harabasz Index. Our experiments involve combinations of seven normalization methods, four dimensional reduction methods, and five integration methods. The results show that Seurat and Harmony excel in data integration, with Harmony being more time-efficient, especially for large datasets. UMAP is the most compatible dimensionality reduction method with the integration techniques, and the choice of normalization method varies depending on the integration method used.

</details>


### [165] [Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\ell_1$ Relaxations](https://arxiv.org/abs/2601.00329)
*Angshul Majumdar*

Main category: cs.GT

Relevance: 15.0

TL;DR: 论文研究联盟结构生成问题，其中联盟价值不是给定的，而是需要从观察中学习。提出稀疏概率CSG框架，使用两种估计方法：贝叶斯贪婪联盟追踪和ℓ1惩罚估计器，分析其理论保证和性能比较。


<details>
  <summary>Details</summary>
Motivation: 传统联盟结构生成假设联盟价值已知，但在实际应用中，这些价值通常需要从观察数据中学习。本文旨在解决从稀疏观察中学习联盟价值并生成最优联盟结构的问题。

Method: 提出稀疏概率CSG框架：1) 将每个观察建模为稀疏线性回归问题，2) 使用贝叶斯贪婪联盟追踪方法（类似正交匹配追踪），3) 使用ℓ1惩罚估计器。分析两种方法在相干性条件和最小信号假设下的理论性质。

Result: BGCP方法在T≳K log m时能以高概率恢复盈利联盟的真实集合，从而获得福利最优结构。ℓ1惩罚估计器在受限特征值条件下获得ℓ1和预测误差界，并转化为福利差距保证。识别了稀疏概率CSG优于基线的机制。

Conclusion: 稀疏概率CSG框架能有效从观察中学习联盟价值并生成最优联盟结构。两种估计方法在不同条件下具有理论保证，为联盟结构生成提供了新的概率学习方法。

Abstract: We study coalition structure generation (CSG) when coalition values are not given but must be learned from episodic observations. We model each episode as a sparse linear regression problem, where the realised payoff \(Y_t\) is a noisy linear combination of a small number of coalition contributions. This yields a probabilistic CSG framework in which the planner first estimates a sparse value function from \(T\) episodes, then runs a CSG solver on the inferred coalition set. We analyse two estimation schemes. The first, Bayesian Greedy Coalition Pursuit (BGCP), is a greedy procedure that mimics orthogonal matching pursuit. Under a coherence condition and a minimum signal assumption, BGCP recovers the true set of profitable coalitions with high probability once \(T \gtrsim K \log m\), and hence yields welfare-optimal structures. The second scheme uses an \(\ell_1\)-penalised estimator; under a restricted eigenvalue condition, we derive \(\ell_1\) and prediction error bounds and translate them into welfare gap guarantees. We compare both methods to probabilistic baselines and identify regimes where sparse probabilistic CSG is superior, as well as dense regimes where classical least-squares approaches are competitive.

</details>


### [166] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

Relevance: 15.0

TL;DR: 论文提出将无服务器计算（FaaS）与MapReduce编程模型结合，通过优化Map和Reduce函数数量来提升词频统计任务的执行效率


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算需求增长，无服务器计算成为研究热点。MapReduce作为大数据处理模型已广泛应用。本文受FaaS无服务器框架和MapReduce高并发、鲁棒性启发，希望结合两者来减少词频统计任务的时间跨度并提高效率

Method: 基于无服务器计算平台构建MapReduce编程模型，通过实验确定特定任务中最优的Map函数和Reduce函数数量。针对相同工作量，研究不同Map和Reduce函数数量对执行时间和效率的影响

Result: 实验表明，随着Map和Reduce函数数量的增加，执行时间减少，程序整体效率以不同速率提升。研究发现最优的Map和Reduce函数数量配置

Conclusion: 通过无服务器计算平台上的MapReduce模型优化，可以找到特定任务的最优函数数量配置，帮助企业和技术人员找到最优解决方案

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [167] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

Relevance: 15.0

TL;DR: 提出优先级感知的多机器人覆盖路径规划（PA-MCPP）问题，通过两阶段框架优化优先级区域覆盖延迟和总完成时间


<details>
  <summary>Details</summary>
Motivation: 现有MCPP方法假设环境区域均匀重要，无法处理某些区域需要更快关注的场景，限制了在优先级覆盖任务中的有效性

Method: 提出两阶段框架：1）贪婪区域分配结合局部搜索和生成树路径规划；2）斯坦纳树引导的剩余覆盖

Result: 实验表明该方法相比标准MCPP基线显著降低优先级加权延迟，同时保持竞争力的总完成时间，能有效通过调整权重控制区域覆盖行为

Conclusion: PA-MCPP问题框架和两阶段方法能有效处理优先级感知的多机器人覆盖规划，在保持效率的同时优化优先级区域覆盖

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [168] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种针对LLM组合技术中tokenizer移植步骤的供应链攻击方法，通过设计单个"破坏性token"在供体模型中功能惰性，但在移植到基础模型后重构为恶意特征，从而破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM生态系统越来越多地使用模型组合技术（如权重合并、推测解码和词汇表扩展），这些方法在不同模型家族间应用的关键前提是tokenizer移植，即将不兼容的词汇表对齐到共享嵌入空间。作者发现这一关键互操作性步骤引入了供应链漏洞。

Method: 通过利用系数重用的几何特性，设计了一个双目标优化问题：1）在供体模型中保持token功能惰性（统计行为与正常行为无差异）；2）在移植到基础模型后可靠地重构为高显著性的恶意特征。使用稀疏求解器实例化攻击，实现训练免费的谱模仿以规避异常检测。

Result: 攻击成功实现了：1）在供体模型中保持功能惰性，统计行为与正常模型无差异；2）在移植到基础模型后可靠地破坏其生成能力；3）攻击具有结构持久性，能够抵抗微调和权重合并；4）通过谱模仿有效规避异常检测。

Conclusion: 该研究揭示了模块化AI组合流程中的隐藏风险，即tokenizer移植这一关键互操作性步骤可能被恶意利用，通过设计特定的"破坏性token"来破坏基础模型的生成能力，同时保持供体模型的行为正常，这对LLM供应链安全提出了重要警示。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [169] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: NeuroSymBO将提示工程重构为序列决策问题，通过贝叶斯优化在方程发现过程中动态选择最优指令，解决LLM的指令脆弱性问题


<details>
  <summary>Details</summary>
Motivation: LLM在方程发现中表现出潜力，但其输出对提示措辞高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解

Method: 提出NeuroSymBO方法：维护离散推理策略库，将提示工程重构为序列决策问题，使用贝叶斯优化基于数值反馈在每个步骤选择最优指令

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简约的解决方案

Conclusion: 将提示工程视为序列决策问题并通过贝叶斯优化动态选择指令，可以有效解决LLM的指令脆弱性问题，提升方程发现性能

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [170] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种使用纯强化学习梯度在线微调决策变换器的新方法，解决了现有方法依赖监督学习目标的问题，通过改进GRPO算法实现了更好的在线性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但扩展到在线设置时仍依赖监督序列建模目标。作者发现后见回报重标注这一标准组件与基于重要性采样的RL算法不兼容，阻碍了纯RL梯度的在线微调。

Method: 将GRPO算法适配到决策变换器，引入关键改进：子轨迹优化以改进信用分配、序列级似然目标以增强稳定性和效率、主动采样以鼓励在不确定区域探索。

Result: 在多个基准测试中，新方法超越了现有的在线决策变换器基线，实现了最先进的性能，证明了纯RL梯度在线微调的有效性。

Conclusion: 纯强化学习梯度可以成功用于决策变换器的在线微调，通过解决后见回报重标注与重要性采样算法的不兼容问题，实现了更好的在线性能。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [171] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

Relevance: 85.0

TL;DR: GRIT是一种动态、曲率感知的LoRA方法，通过K-FAC预条件梯度、周期性重投影到Fisher特征方向、自适应调整有效秩，在减少46%可训练参数的同时达到或超越LoRA/QLoRA性能


<details>
  <summary>Details</summary>
Motivation: 现有LoRA和QLoRA方法在几何上较为盲目：它们在固定、随机方向的低秩子空间中优化，主要使用一阶下降，忽略了局部损失曲率。这可能导致有效更新预算膨胀，并沿着弱约束方向放大漂移。

Method: GRIT在保持LoRA参数化的基础上：(1)使用K-FAC作为自然梯度代理在秩空间中对梯度进行预条件处理；(2)周期性将低秩基重投影到主导Fisher特征方向以抑制漂移；(3)根据谱自适应调整有效秩，使容量集中在信号所在的位置。

Result: 在LLaMA骨干上的指令跟随、理解和推理基准测试中，GRIT匹配或超越了LoRA和QLoRA，同时平均减少46%的可训练参数（不同任务中减少25-80%），在各种提示风格和数据混合下没有实际质量损失。

Conclusion: GRIT通过曲率感知的优化策略，在保持LoRA参数效率优势的同时，显著减少了参数数量和漂移问题，提供了更好的更新与保留权衡边界。

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [172] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出E-GRPO（熵感知的组相对策略优化），通过增加SDE采样步骤的熵来改进基于流匹配模型的人类偏好对齐方法，解决多步去噪中奖励信号稀疏和模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型在人类偏好对齐中，多步去噪优化面临稀疏和模糊的奖励信号问题。作者观察到高熵步骤能实现更高效有效的探索，而低熵步骤则产生无差异的轨迹。

Method: 提出E-GRPO方法：1）合并连续的低熵步骤形成一个高熵步骤用于SDE采样；2）在其他步骤使用ODE采样；3）引入多步组归一化优势函数，在共享相同合并SDE去噪步骤的样本中计算组相对优势。

Result: 在不同奖励设置下的实验结果表明，该方法能有效提高流匹配模型在人类偏好对齐中的性能。

Conclusion: 通过熵感知的SDE步骤合并和组相对策略优化，能够有效解决多步去噪中的奖励信号问题，提升强化学习在流匹配模型偏好对齐中的效果。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [173] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

Relevance: 85.0

TL;DR: 正交性损失无法有效提升MoE模型专家多样性，反而增加权重空间重叠，对性能影响不一致且不可靠


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，探索正交性损失是否能有效增强专家多样性

Method: 在MoE模型中应用正交性损失，分析7种不同正则化强度下权重空间和激活空间的重叠情况，评估在WikiText-103、TinyStories和PTB数据集上的性能

Result: 正交性损失在多方面失败：权重空间重叠增加114%，激活空间重叠保持高位(~0.6)，性能影响不一致（WikiText-103略有改善，TinyStories轻微下降，PTB结果高度波动），权重与激活正交性无显著相关性(r=-0.293, p=0.523)

Conclusion: 权重空间正则化既未实现其几何目标，也未可靠提升性能，不适合用于MoE多样性增强

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [174] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究发现小型语言模型（7-9B参数）存在严重的推理可靠性危机：50-69%的正确答案包含根本性错误推理，即"正确但理由错误"现象。作者提出了推理完整性评分（RIS）作为过程性评估指标，发现RAG能显著改善推理完整性，而元认知干预反而损害性能。


<details>
  <summary>Details</summary>
Motivation: 部署小型语言模型作为自主代理时，需要信任其推理过程而不仅仅是输出结果。当前标准准确率指标无法检测到"正确但理由错误"的现象，这在实际部署中可能带来严重风险。

Method: 分析了10,734个推理轨迹，涵盖三个模型和多样化任务。提出了推理完整性评分（RIS）作为过程性评估指标，验证了其评分者间一致性（κ=0.657）。研究了RAG和元认知干预对推理完整性的影响，并通过机制分析探究了其作用原理。最后训练了一个神经分类器用于快速验证推理完整性。

Result: 1. 小型语言模型存在严重推理可靠性问题：50-69%的正确答案包含根本性错误推理。
2. RAG能显著改善推理完整性（Cohen's d=0.23-0.93），减少7.6%的错误。
3. 元认知干预（如自我批评）反而损害性能（d=-0.14到-0.33）。
4. 训练的神经分类器达到0.86 F1分数，速度提升100倍。

Conclusion: 仅依赖准确率评估语言模型是危险的，因为模型可能基于完全错误的推理得出正确答案。过程性验证对于可信赖的自主代理至关重要，RAG是改善推理完整性的有效方法，而元认知干预在小型模型中可能适得其反。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [175] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

Relevance: 85.0

TL;DR: Trajectory Guard：一种用于检测LLM代理多步行动计划异常的Siamese循环自编码器，通过对比学习和重构的混合损失函数，能同时检测任务轨迹对齐和序列有效性，在实时性和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适用于LLM代理的多步行动计划：均值池化嵌入会稀释异常步骤，而纯对比方法忽略序列结构。标准无监督方法在预训练嵌入上F1分数不超过0.69，无法有效检测"错误计划"和"畸形计划结构"。

Method: 提出Trajectory Guard，一个Siamese循环自编码器，采用混合损失函数：通过对比学习学习任务-轨迹对齐，通过重构学习序列有效性。这种双重目标能统一检测"错误的任务计划"和"畸形的计划结构"。

Result: 在合成扰动和真实世界失败（RAS-Eval安全审计和Who&When多代理系统）的基准测试中，在平衡集上达到0.88-0.94的F1分数，在不平衡外部基准上达到0.86-0.92的召回率。推理延迟32ms，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能有效检测LLM代理行动计划的异常，同时考虑任务对齐和序列结构，具有高准确性和实时性，适用于生产部署中的安全验证。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [176] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

Relevance: 85.0

TL;DR: HFedMoE：一种基于MoE的异构联邦学习框架，用于高效微调大语言模型，通过专家重要性评估、自适应专家选择和稀疏感知聚合来解决资源受限设备上的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）可以保护数据隐私，但大语言模型（LLM）的规模使得在资源受限的移动设备上进行本地训练不切实际。MoE模型通过稀疏激活专家来降低计算负担，但将MoE集成到FL微调中面临三个关键挑战：1）缺乏可靠指标来选择适合客户的专家；2）异构计算资源限制MoE专家激活；3）客户端特定的专家子集和路由偏好破坏全局聚合。

Method: 提出HFedMoE框架：1）基于专家对微调性能的贡献评估专家重要性；2）从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3）设计稀疏感知模型聚合策略，通过重要性加权贡献聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确性和收敛速度方面优于最先进的基准方法。

Conclusion: HFedMoE成功解决了MoE在联邦学习微调中的关键挑战，实现了在资源受限设备上高效、计算友好的LLM微调，为隐私保护的分布式模型训练提供了实用解决方案。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [177] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

Relevance: 85.0

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，结合在线重置机制防止码本崩溃，并使用KV-LoRA高效利用压缩记忆表示，在保持高保留准确率的同时将记忆库大小减少到最强基线的0.3%。


<details>
  <summary>Details</summary>
Motivation: LLMs的知识会随着数据演化而过时，持续学习需要在不擦除已有知识的情况下更新模型。现有记忆增强方法面临记忆库在现实大规模数据流中不断增长的严重限制。

Method: 提出MBC模型：1) 通过码本优化策略压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在注意力层使用Key-Value Low-Rank Adaptation (KV-LoRA) 高效利用压缩记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩和高效适应机制，有效解决了记忆增强方法中记忆库无限增长的问题，为LLMs的持续学习提供了实用的解决方案。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [178] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种无需训练的方法，通过注意力模式的光谱分析检测LLM中的有效数学推理，利用四个可解释的光谱诊断指标，在多种Transformer模型上实现85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学推理中常产生看似合理但逻辑错误的输出（幻觉），需要一种无需训练、可解释的方法来检测推理的有效性。传统方法依赖形式验证器或训练分类器，存在技术限制和过拟合风险。

Method: 将注意力矩阵视为动态图的邻接矩阵，提取四个光谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和光谱熵。这些指标在有效和无效数学证明间有显著差异，仅需单一阈值即可分类。

Result: 在7个来自4个架构家族（Meta Llama、Alibaba Qwen、Microsoft Phi、Mistral AI）的Transformer模型上，效应量达Cohen's d=3.30（p<10^-116），分类准确率85.0-95.6%，校准阈值在完整数据集上达93-95%。发现光谱方法检测的是逻辑一致性而非编译器接受度。

Conclusion: 光谱图分析为推理验证提供了原则性框架，具有即时应用于幻觉检测和AI安全监控的潜力。注意力机制设计影响哪些光谱特征捕获推理有效性，如Mistral-7B的滑动窗口注意力将判别信号从HFER转移到后期层平滑度。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [179] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

Relevance: 85.0

TL;DR: YapBench是一个量化LLM过度生成的轻量级基准测试，通过测量超出必要回答长度的字符数来评估模型在简洁性理想场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为通用助手时，经常对简单请求给出不必要的冗长回答，包含冗余解释、模糊表达和模板内容，增加了认知负担和推理成本。先前研究表明基于偏好的后训练和LLM评估会导致系统性长度偏差，即使质量相当，更长的回答也更容易获得奖励。

Method: 提出YapBench基准测试，包含300多个英文提示，涵盖三种简洁性理想场景：需要简短澄清的最小或模糊输入、具有简短稳定答案的封闭式事实问题、以及单行代码任务。主要指标YapScore测量响应超出基准答案的字符数，YapIndex则是类别级别中位数YapScore的均匀加权平均值。

Result: 评估76个助手LLM，发现中位数超额长度存在数量级差异，并识别出特定类别的失败模式：在模糊输入上的"真空填充"行为，以及在单行技术请求上的解释或格式化开销。

Conclusion: YapBench为量化LLM过度生成提供了标准化评估框架，揭示了不同模型在简洁性响应方面的显著差异，有助于推动更高效、用户友好的LLM开发。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [180] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出IRPO框架，将Bradley-Terry模型融入GRPO，通过为每个响应生成点式分数，解决成对GRMs在RL训练中的计算瓶颈问题，实现高效的多候选评估。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型(GRMs)在奖励建模中具有可解释性、推理时扩展性和RL优化潜力，但广泛使用的成对GRMs与GRPO等RL算法结合时存在计算瓶颈：1) 成对比较的O(n²)时间复杂度；2) 重复采样或额外CoT推理的计算开销。

Method: 提出Intergroup Relative Preference Optimization (IRPO)框架，将Bradley-Terry模型融入Group Relative Policy Optimization (GRPO)。通过为每个响应生成点式分数，实现RL训练期间对任意多候选的高效评估，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中达到点式GRMs的最先进性能，与当前领先的成对GRMs性能相当。在训练后评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO有效解决了成对GRMs的计算瓶颈问题，在保持性能的同时显著提升效率，为GRMs与RL算法的结合提供了更实用的解决方案。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [181] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

Relevance: 85.0

TL;DR: STAC算法通过引入时间性偶然不确定性（而非认知不确定性）来缩放悲观偏差，使用单一分布评论家网络建模时间回报不确定性，结合dropout正则化，在随机环境中实现风险规避行为并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 离策略演员-评论家方法中，评论家网络倾向于系统性地高估价值估计。现有方法通常使用集成方法来量化认知不确定性以引入悲观偏差，但本文提出利用时间性偶然不确定性（来自随机转移、奖励和策略诱导的贝尔曼目标变异性）来缩放悲观偏差。

Method: STAC使用单一分布评论家网络建模时间回报不确定性，将时间性偶然不确定性纳入时间差分更新中的悲观偏差缩放。同时对评论家和演员网络应用dropout进行正则化，提高训练稳定性和性能。

Result: 基于分布评论家的悲观主义足以缓解高估问题，并在随机环境中自然导致风险规避行为。引入dropout进一步提高了训练稳定性和性能。该设计使用单一分布评论家网络实现了改进的计算效率。

Conclusion: STAC通过利用时间性偶然不确定性而非认知不确定性来缩放悲观偏差，提供了一种更高效的方法来缓解强化学习中的价值高估问题，并在随机环境中实现风险规避行为。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [182] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出Distributional Creative Reasoning (DCR)框架，分析当前LLM推理管道中基于正确性的强化学习导致推理路径分布崩溃的问题，提供防止多样性衰减的理论保证和实践方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理管道主要依赖自举推理循环（如采样多样化思维链并强化最高分路径），这种设计过度优化正确性，导致模型推理路径分布崩溃，降低语义熵并削弱创造性问题解决能力。

Method: 提出Distributional Creative Reasoning (DCR)统一变分目标，将训练视为通过解迹概率测度的梯度流。STaR、GRPO、DPO、熵奖励等方法都是该损失的特殊情况。框架包含多样性衰减定理分析、确保收敛到稳定多样策略的设计、以及实践中的简单可操作方法。

Result: DCR框架提供三个核心结果：1) 多样性衰减定理描述STaR、GRPO、DPO中基于正确性目标导致的不同多样性衰减模式；2) 确保收敛到稳定多样策略的设计，有效防止崩溃；3) 实践中的简单可操作方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方案，解决了当前推理管道中分布崩溃问题，为创造性推理提供了理论保证。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [183] [Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing](https://arxiv.org/abs/2601.00245)
*Osvaldo Simeone*

Main category: cs.NE

Relevance: 85.0

TL;DR: 该论文探讨了神经形态计算原理与现代AI架构（如状态空间模型和Transformer）之间的联系，从"token内处理"和"token间处理"的角度分析神经形态AI的发展，并综述了相关训练方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI快速发展带来的能源需求激增，需要探索更高效的神经形态计算原理。现代AI架构（如量化激活、状态空间动态、稀疏注意力）已体现神经形态特性，本文旨在系统分析这些联系。

Method: 通过"token内处理"（同一向量输入的多通道变换）和"token间处理"（基于上下文相关性的信息选择组合）的视角，分析神经形态模型、状态空间模型和Transformer架构之间的联系。综述了从基于替代梯度的并行卷积处理到基于强化学习的局部学习规则等多种训练方法。

Result: 建立了神经形态计算原理与现代AI架构之间的系统性联系框架，展示了神经形态AI从早期脉冲神经网络（SNNs）的token内处理，发展到利用状态空间动态或稀疏自注意力的token间处理的演进路径。

Conclusion: 神经形态计算原理为设计高效AI架构提供了重要指导，现代AI架构已自然融入神经形态特性。通过区分token内/间处理，可以更好地理解和设计高效的计算模型，应对AI能源挑战。

Abstract: The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.

</details>


### [184] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

Relevance: 85.0

TL;DR: Revati是一个时间扭曲模拟器，通过直接执行真实服务系统代码实现性能建模，无需物理GPU，使用CUDA API拦截和虚拟时间跳跃技术，在vLLM和SGLang上实现<5%预测误差，速度比真实GPU执行快5-17倍。


<details>
  <summary>Details</summary>
Motivation: 部署LLM需要测试数百种服务配置，但在GPU集群上评估每个配置需要数小时和数千美元成本。离散事件模拟器虽然更快更便宜，但需要重新实现服务系统的控制逻辑，随着框架演进负担加重。

Method: Revati是一个时间扭曲模拟器，通过拦截CUDA API调用来虚拟化设备管理，使服务框架无需物理GPU即可运行。系统不执行GPU内核，而是执行时间跳跃——通过预测的内核持续时间快速推进虚拟时间。提出了协调协议来同步分布式进程中的时间跳跃，同时保持因果关系。

Result: 在vLLM和SGLang上，Revati实现了小于5%的预测误差，覆盖多种模型和并行配置，同时运行速度比真实GPU执行快5-17倍。

Conclusion: Revati通过直接执行真实服务系统代码实现高效性能建模，解决了传统模拟器需要重新实现控制逻辑的问题，为LLM服务配置优化提供了快速准确的评估工具。

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [185] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文提出了一种检索增强的多工具修复工作流，让代码生成LLM通过编译器诊断、CodeQL安全扫描和KLEE符号执行迭代修复代码，并使用轻量级嵌入模型检索成功修复案例，显著提升了代码安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常存在安全漏洞、逻辑不一致和编译错误，现有研究表明结构化反馈、静态分析、检索增强和执行优化能显著提升LLM性能。因此需要开发系统化的工具辅助修复框架来提升生成代码的质量和安全性。

Method: 提出检索增强的多工具修复工作流：1) 单代码生成LLM迭代优化输出；2) 使用编译器诊断、CodeQL安全扫描和KLEE符号执行进行反馈；3) 轻量级嵌入模型进行语义检索，获取安全修复示例指导生成。

Result: 在DeepSeek-Coder-1.3B和CodeLlama-7B生成的3,242个程序上评估：DeepSeek安全漏洞减少96%；CodeLlama关键安全缺陷率从58.55%降至22.19%，证明工具辅助自修复对"顽固"模型也有效。

Conclusion: 多工具修复工作流能显著提升LLM生成代码的鲁棒性和安全性，即使是大型模型也能通过工具辅助自修复机制有效减少安全漏洞，为代码生成LLM的质量保障提供了有效方案。

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


### [186] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

Relevance: 75.0

TL;DR: 时间序列基础模型（TSFMs）作为通用骨干网络，在异常检测任务中超越任务特定方法，特别是在类别不平衡情况下表现优异，且参数高效微调方法（如LoRA）能匹配或超越全微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法大多需要大量任务特定训练，缺乏通用性。本文探索时间序列基础模型（TSFMs）是否可以作为通用骨干网络，实现高效、可扩展的异常检测。

Method: 系统实验比较三种策略：零样本推理、全模型适应、参数高效微调（PEFT，包括LoRA、OFT、HRA）。在多个基准测试上评估TSFMs在异常检测任务中的表现。

Result: TSFMs在AUC-PR和VUS-PR指标上显著超越任务特定基线，尤其在严重类别不平衡情况下表现优异。PEFT方法不仅降低计算成本，在大多数情况下还能匹配或超越全微调效果。

Conclusion: 时间序列基础模型可作为有前景的通用模型，实现可扩展、高效的时间序列异常检测，即使这些模型原本是为预测任务预训练的。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [187] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文探索了大模型联邦定制化，首次在联邦学习框架中应用prefix-tuning，验证了其可行性并展示了与集中式方法相近的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在各领域的广泛应用，如何在保护数据隐私的联邦学习框架下有效定制大模型成为一个重要挑战。论文旨在探索大模型联邦定制化的可行性和方法。

Method: 论文首先综述了多种大模型定制技术（全微调、高效微调、提示工程、prefix-tuning、知识蒸馏、检索增强生成），然后探讨如何在联邦学习框架中实现这些技术。特别地，首次在联邦学习环境中实验了prefix-tuning方法。

Result: 实验验证了联邦prefix-tuning的可行性，其性能接近集中式方法。与其他三种联邦定制方法相比，展示了具有竞争力的性能、满意的效率和一致的鲁棒性。

Conclusion: 联邦prefix-tuning是大模型联邦定制化的有效方法，为在保护数据隐私的前提下定制大模型提供了可行的技术路径。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [188] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出IGBO框架，通过双目标优化结合领域知识训练可解释模型，使用DAG编码特征重要性层次结构，采用TIG度量特征重要性，并引入最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释模型训练中，如何有效结合结构化领域知识并保持模型性能是一个挑战。现有方法在整合特征重要性层次结构时存在局限性，且时间序列数据的特征重要性度量面临分布外问题。

Method: 1) 使用有向无环图编码特征重要性层次结构；2) 采用时间积分梯度度量特征重要性；3) 提出最优路径预言机解决TIG计算中的分布外问题；4) 建立双目标优化框架平衡可解释性和准确性。

Result: 理论分析证明收敛性和对mini-batch噪声的鲁棒性。时间序列数据实验显示IGBO能有效强制DAG约束，准确率损失最小，优于标准正则化基线方法。

Conclusion: IGBO框架成功整合结构化领域知识到可解释模型训练中，通过双目标优化平衡可解释性和性能，为解决分布外问题和特征重要性约束提供了有效方案。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [189] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出BSAT（B样条自适应分词器），一种参数免费的自适应时间序列分段方法，通过B样条拟合将高曲率区域作为token，结合混合位置编码L-RoPE，实现高效长时序预测。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长时序预测中存在两个主要问题：1）自注意力的二次复杂度计算开销大；2）均匀分段可能与数据的语义结构不对齐。需要一种既能压缩序列长度又能保持语义信息的方法。

Method: 1）BSAT：使用B样条拟合时间序列，在高曲率区域自适应放置token，将变长基函数表示为固定大小的token（系数+位置）；2）L-RoPE：混合位置编码，结合可学习的加性位置编码和具有层间可学习基数的旋转位置嵌入，使不同层关注不同的时间依赖关系。

Result: 在多个公开基准测试中表现出竞争力，在高压缩率下保持强性能，特别适合内存受限的应用场景。

Conclusion: BSAT提供了一种有效的时间序列自适应分词方法，结合L-RoPE位置编码，能够在保持预测性能的同时显著压缩序列长度，为内存受限的长时序预测任务提供了实用解决方案。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [190] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

Relevance: 75.0

TL;DR: FedHypeVAE：一种基于超网络的差分隐私联邦数据合成框架，通过条件VAE和客户端感知解码器解决非IID异构数据下的嵌入级数据生成问题，同时提供形式化隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦数据共享方法在非IID客户端异构性下表现不佳，且缺乏对梯度泄漏的形式化保护。需要一种既能个性化生成又具备隐私保证的嵌入级数据合成框架。

Method: 基于条件VAE架构，用客户端感知解码器和类别条件先验替换全局解码器，通过共享超网络从私有可训练客户端代码生成。采用差分隐私优化超网络，结合局部MMD对齐和Lipschitz正则化增强稳定性。

Result: FedHypeVAE在生成层实现个性化、隐私保护和分布对齐的统一，支持领域无关合成和可控多领域覆盖，为联邦环境下的隐私保护数据合成提供理论基础。

Conclusion: 该框架为联邦学习中的隐私保护数据合成建立了原则性基础，解决了非IID异构性和隐私保护的关键挑战。

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


### [191] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该研究将Yahtzee骰子游戏建模为MDP，使用多种策略梯度方法（REINFORCE、A2C、PPO）训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优性能的241.78分（最优DP分数为254.59）。


<details>
  <summary>Details</summary>
Motivation: Yahtzee作为具有随机性、组合结构和延迟奖励的经典骰子游戏，是一个有趣的中等规模强化学习基准。单人游戏可通过动态规划求解，但多人游戏难以处理，需要近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程（MDP），使用REINFORCE、优势演员-评论家（A2C）和近端策略优化（PPO）等策略梯度方法训练自博弈智能体。采用具有共享主干的多头网络架构，并对特征编码、动作编码、架构、回报估计器和熵正则化进行消融研究。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感且未能达到接近最优性能，而A2C在各种设置下都能稳健训练。最佳智能体在10万次评估游戏中获得中位数分数241.78分，接近最优DP分数254.59的95%。智能体获得上区奖励和Yahtzee的概率分别为24.9%和34.1%。

Conclusion: A2C在Yahtzee游戏中表现出最强的鲁棒性，但所有模型都难以学习上区奖励策略，过度关注"四骰同点"，凸显了长期信用分配和探索的持续挑战。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [192] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

Relevance: 65.0

TL;DR: 大规模比较评估16种可解释机器学习方法在216个表格数据集上的表现，发现EBMs在回归任务中表现最佳，但性能高度依赖于数据集特征如维度、样本量、线性度和类别不平衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融、法律等高风险领域的广泛应用，模型可解释性和问责性需求日益增长。尽管可解释ML受到关注，但对表格数据的内生可解释模型的系统性评估仍然稀缺，现有研究多集中于聚合性能指标。

Method: 对16种内生可解释方法进行大规模比较评估，包括经典线性模型、决策树以及EBMs、符号回归、GOSDT等新方法。覆盖216个真实世界表格数据集，按数据集结构特征（维度、样本量、线性度、类别不平衡）分层分析性能，同时评估训练时间和分布偏移下的鲁棒性。

Result: EBMs在回归任务中表现最稳定且预测准确率高；符号回归和IGANNs在非线性场景下表现突出；GOSDT对类别不平衡高度敏感；性能高度依赖于数据集特征，没有单一方法在所有场景下最优。

Conclusion: 研究为实践者在可解释性和预测性能之间寻求平衡提供了实用指导，深化了对表格数据可解释建模的实证理解，强调需要根据具体数据集特征选择合适方法。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [193] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文提出对抗样本应分为两类：利用非鲁棒特征的样本和不利用非鲁棒特征的样本，并提出了基于集成的方法来量化非鲁棒特征的操纵程度，重新审视了对抗鲁棒性中的多个现象。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒性理论（特别是Ilyas等人提出的非鲁棒特征理论）主要关注利用数据分布中脆弱但预测性特征的对抗样本，但忽略了那些不直接利用这些特征的对抗样本。作者认为这两种样本代表了不同类型的对抗弱点，需要在评估对抗鲁棒性时加以区分。

Method: 提出了基于集成的度量方法，用于量化对抗扰动对非鲁棒特征的操纵程度。使用该度量分析攻击者生成的对抗样本的组成，区分两类对抗样本。

Result: 通过新度量方法能够区分两类对抗样本，并重新审视了多个对抗鲁棒性现象：1）锐度感知最小化对对抗鲁棒性的影响；2）在鲁棒数据集上对抗训练与标准训练之间的鲁棒性差距。

Conclusion: 对抗样本应分为利用非鲁棒特征和不利用非鲁棒特征两类，这种区分对于准确评估对抗鲁棒性至关重要。提出的度量方法为分析对抗弱点提供了新视角。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [194] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种自定义损失函数(CLF)来减少深度学习训练中的随机性影响，提高模型在不同运行中的一致性和可靠性


<details>
  <summary>Details</summary>
Motivation: 当前深度学习训练方法缺乏确保跨运行一致性和鲁棒性的机制，即使控制初始化和训练条件，模型准确率仍存在显著变异性，这影响了模型的可复现性和可靠性

Method: 提出自定义损失函数(CLF)，通过微调其参数来平衡预测准确率和训练稳定性，减少训练结果对权重初始化和数据洗牌等随机因素的敏感性

Result: 在图像分类和时间序列预测的多种架构上进行广泛实验，表明该方法显著提高了训练鲁棒性，同时不牺牲预测性能

Conclusion: CLF是开发更稳定、可靠和可信赖神经网络的有效高效策略，有助于提高深度学习模型的可复现性和可靠性

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [195] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

Relevance: 65.0

TL;DR: ARISE：一个轻量级框架，通过添加基于群体的探索层增强强化学习，在非平稳奖励和高维策略任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略场景下。现有方法在复杂任务中探索不足，需要更有效的探索机制来提升学习效率和鲁棒性。

Method: ARISE框架在标准策略梯度方法基础上添加紧凑的群体探索层，将策略动作与粒子驱动的建议混合。每个粒子代表动作空间中采样的候选策略轨迹，使用奖励方差线索自适应调节探索强度。

Result: 在简单基准上仅有轻微改进（CartPole-v1 +0.7%），但在复杂任务上获得显著提升：LunarLander-v3 +46%，Hopper-v4 +22%，同时在Walker2d和Ant上保持稳定性。在非平稳奖励变化下，ARISE提供明显鲁棒性优势，在CartPole上超越PPO +75分。

Conclusion: ARISE提供了一个简单、架构无关的途径，在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的RL智能体。群体组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [196] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出了一种贝叶斯逆博弈框架，通过结构化变分自编码器和可微纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，实现多模态不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然逆博弈方法只能提供点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能够量化不确定性并支持多模态观测的贝叶斯方法。

Method: 提出贝叶斯逆博弈框架，使用结构化变分自编码器（VAE）嵌入可微纳什博弈求解器，在交互数据集上训练，无需智能体真实目标的标签。支持多模态观测数据融合。

Result: 框架成功学习先验和后验分布，相比最大似然估计方法提高推理质量，实现更安全的下游决策而不牺牲效率。当轨迹信息不足时，多模态推理进一步减少不确定性。

Conclusion: 贝叶斯逆博弈方法能够量化不确定性，提高自主决策的安全性，支持多模态观测，为多智能体交互场景提供更可靠的逆博弈解决方案。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [197] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出了一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化，通过高斯噪声过程的去噪器实现训练自由的扩散采样器


<details>
  <summary>Details</summary>
Motivation: 解决分类变量梯度优化中的挑战：传统方法要么使用噪声大的得分函数估计器，要么使用有偏的连续松弛方法。需要一种更好的重参数化技巧来优化分类分布

Method: 提出扩散基软重参数化方法：对分类分布使用高斯噪声过程，其去噪器有闭式解且计算高效，通过训练自由的扩散采样器实现反向传播

Result: 在各种基准测试中，提出的重参数化技巧展现出有竞争力或改进的优化性能

Conclusion: 扩散基软重参数化为分类分布提供了一种有效的梯度优化方法，扩展了连续松弛方法家族，在优化性能上表现优异

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [198] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，获得比传统参数平均更准确的全局奖励估计


<details>
  <summary>Details</summary>
Motivation: 在机器人和多智能体系统中，自主智能体通常在微妙不同的环境中运行，但追求共同的高级目标。由于动态差异、隐私约束和有限通信带宽，直接汇集数据学习共享奖励函数通常不切实际

Method: 每个客户端首先在本地执行轻量级最大熵逆强化学习，遵守其计算和隐私限制。然后通过Wasserstein重心融合得到的奖励函数，考虑其底层几何结构

Result: 证明了重心融合比联邦学习中传统的参数平均方法产生更准确的全局奖励估计。提供了一个原则性和通信高效的框架，用于推导跨异构智能体和环境的共享奖励

Conclusion: 该方法为异构智能体系统提供了一种有效的联邦逆强化学习框架，通过最优传输理论实现奖励函数的几何感知融合

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [199] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

Relevance: 45.0

TL;DR: DDL提出了一种新型残差网络架构，通过可学习的几何变换（Delta算子）调制恒等连接，动态控制特征转换，增强网络对复杂状态转换的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络的恒等连接虽然缓解了梯度消失问题，但强加了严格的加性归纳偏置，限制了网络建模复杂状态转换的能力。需要一种更灵活的机制来动态控制特征转换。

Method: 提出Deep Delta Learning (DDL)架构，用可学习的、数据相关的几何变换（Delta算子）替代标准恒等连接。Delta算子是对单位矩阵的秩-1扰动，由反射方向向量k(X)和门控标量β(X)参数化，能够动态插值恒等映射、正交投影和几何反射。

Result: DDL能够显式控制层间转移算子的谱，在保持门控残差架构稳定训练特性的同时，增强了网络对复杂、非单调动态的建模能力。

Conclusion: DDL通过可学习的几何变换调制残差连接，为深度网络提供了更灵活的特征转换机制，在保持训练稳定性的同时增强了复杂动态建模能力。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [200] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出了可控概念瓶颈模型（CCBMs），支持概念标签级、概念级和数据级三种粒度的模型编辑，无需重新训练即可实现动态更新


<details>
  <summary>Details</summary>
Motivation: 现实应用中部署的模型需要持续维护：需要删除错误或敏感数据（遗忘）、纠正错误标注的概念、或纳入新样本（增量学习）以适应环境变化。传统概念瓶颈模型假设数据和概念固定且干净，缺乏有效的编辑机制，重新训练成本高昂。

Method: 基于影响函数推导出数学上严格的闭式近似解，支持三种粒度编辑：概念标签级（修改概念与标签关系）、概念级（修改概念定义）、数据级（数据删除和添加）。无需重新训练即可实现模型更新。

Result: 实验结果表明CCBMs在效率和适应性方面表现优异，能够实现动态可信的概念瓶颈模型，验证了在实际应用中的实用价值。

Conclusion: CCBMs为概念瓶颈模型提供了有效的编辑框架，解决了现实应用中模型维护的挑战，使CBMs能够在动态环境中保持实用性和可信性。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [201] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文探讨了无自注意力Transformer架构（神经链）与离散动力系统之间的类比，比较了标准数值离散化和PINN学习在求解Burgers和Eikonal方程时的表现，发现两者获得相同动力学知识但路径不同。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究无自注意力的Transformer架构（神经链）与离散动力系统之间的类比关系，比较传统数值方法和物理信息神经网络（PINN）在求解偏微分方程时的差异，特别是它们在参数效率、物理可解释性和计算成本方面的对比。

Method: 方法包括：1）将无自注意力的Transformer架构（神经链）与离散动力系统建立数学类比；2）对Burgers方程（粘性和非粘性）和Eikonal方程进行数值求解；3）使用标准数值离散化方法（有限差分法）和PINN学习方法进行对比；4）分析两种方法在矩阵结构、参数数量和训练成本方面的差异。

Result: 研究发现：1）标准数值离散化和PINN学习获得相同的系统动力学知识；2）PINN使用随机矩阵，而有限差分法使用高度结构化的三对角矩阵；3）可接受的随机矩阵数量远多于唯一的三对角形式，这解释了PINN搜索通常落在随机集合中；4）PINN需要更多参数，导致物理可解释性差和训练成本高，而有限差分法没有这些缺点。

Conclusion: 结论是：对于一维动态问题，PINN相比传统数值方法没有优势，因为参数更多、可解释性差、训练成本高。但研究不排除在高维问题中PINN和机器学习可能提供更好的策略。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [202] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

Relevance: 45.0

TL;DR: TeleDoCTR：针对电信领域票务故障排除的领域特定、上下文感知的端到端系统，集成分类、检索和生成任务，显著提升故障排除效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电信领域的票务故障排除是高度复杂且耗时的任务，需要专家解读票务内容、查阅文档和搜索历史记录。传统人工方法不仅延迟问题解决，还阻碍整体运营效率。需要自动化系统来提升电信票务故障排除的效率和效果。

Method: 提出TeleDoCTR系统，集成领域特定的排序和生成模型，自动化故障排除工作流的关键步骤：1）票务分类到合适的专家团队；2）检索上下文和语义相似的历史票务；3）生成详细的故障分析报告（包括问题、根本原因和潜在解决方案）。

Result: 在真实世界电信基础设施数据集上评估，TeleDoCTR优于现有最先进方法，显著提升故障排除过程的准确性和效率。

Conclusion: TeleDoCTR是一个有效的电信领域特定故障排除系统，通过集成分类、检索和生成任务，能够端到端地自动化票务解决流程，为电信运营提供实用解决方案。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [203] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

Relevance: 45.0

TL;DR: 本文提出了一种基于图神经网络和强化学习的交通感知出租车热点预测框架，用于优化城市出租车调度，相比随机选择基线减少了56%的乘客等待时间和38%的行驶距离。


<details>
  <summary>Details</summary>
Motivation: 在智慧城市交通中，传统出租车热点预测模型通常仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共事件等动态因素的影响。这导致出租车供应与乘客需求匹配效率低下，需要实时整合城市交通网络数据和移动模式。

Method: 将城市道路网络建模为图（交叉口为节点，路段为边），节点属性包括历史需求、事件邻近度和实时拥堵分数。使用图神经网络编码时空依赖关系，然后通过Q-learning智能体推荐最优出租车热点。奖励机制联合优化乘客等待时间、司机行驶距离和拥堵避免。

Result: 在模拟的德里出租车数据集上实验，相比基线随机选择方法，提出的模型减少了约56%的乘客等待时间和38%的行驶距离。

Conclusion: 该交通感知、基于图的强化学习框架能有效优化出租车调度，可适应多模式交通系统，并能集成到智慧城市平台中实现实时城市移动性优化。

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [204] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出一个基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文多臂老虎机问题动态选择计算步骤的最优精度配置，平衡精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中，混合精度方法可以显著减少计算成本，但传统方法需要手动调优精度配置。需要一种自动化的方法来动态选择不同计算步骤的最优精度，以平衡计算效率和数值精度。

Method: 将精度调优问题建模为上下文多臂老虎机问题，使用离散化状态空间和增量动作值估计。构建Q表将离散化特征（如近似条件数和矩阵范数）映射到精度配置动作，采用epsilon-greedy策略优化多目标奖励函数，平衡精度和计算成本。

Result: 在线性方程组迭代求精的应用中，该方法能有效选择精度配置，在保持与双精度基线相当的精度同时，显著降低计算成本。框架对未见数据具有良好的泛化能力。

Conclusion: 这是首个基于强化学习的精度自动调优工作，验证了RL在混合精度数值方法中的潜力，为科学计算中的算法优化提供了新思路。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [205] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

Relevance: 45.0

TL;DR: 提出一种对抗样本矫正方法，通过重新攻击对抗样本来估计原始正确标签，无需参数调整或预训练，能处理多种攻击类型


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要关注对抗样本检测而非正确分类，但某些应用（如自动驾驶交通标志识别）需要识别原始正确类别。需要一种能矫正对抗样本以恢复原始标签的方法。

Method: 基于重新攻击对抗样本，将其移出决策边界以进行准确标签预测。仅使用对抗样本作为输入，无需参数调整或预训练，能处理白盒攻击产生的微小扰动对抗样本。

Result: 该方法在各种攻击方法（包括目标攻击和黑盒攻击）生成的对抗样本矫正中表现一致，在对抗多种攻击的稳定性方面优于传统矫正和输入变换方法。

Conclusion: 提出的对抗样本矫正方法能有效处理多种攻击类型，无需复杂调整，在恢复原始标签方面具有稳定性能，但仍面临黑盒攻击和低置信度目标攻击的挑战。

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [206] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文研究非马尔可夫状态和成本过程下的线性函数逼近强化学习方法，证明了策略评估算法在遍历条件下的收敛性，并分析了Q-learning在特定基函数选择下的收敛性


<details>
  <summary>Details</summary>
Motivation: 研究非马尔可夫环境下的强化学习问题，因为实际应用中状态过程往往不是马尔可夫的，需要开发能够处理非马尔可夫过程的强化学习理论和方法

Method: 1. 使用线性函数逼近进行策略评估，证明在遍历条件下的收敛性；2. 分析Q-learning在量化映射基函数下的收敛性；3. 将结果应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示

Result: 1. 证明了策略评估算法在非马尔可夫过程遍历条件下的收敛性，极限对应辅助马尔可夫决策过程的贝尔曼算子的固定点；2. 证明了在量化映射基函数下Q-learning的收敛性；3. 推导了学习算法极限的显式误差界

Conclusion: 该研究为非马尔可夫环境下的强化学习提供了理论保证，特别是在线性函数逼近框架下，为部分可观测马尔可夫决策过程等实际应用提供了理论基础

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [207] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了对抗性图提示（AGP）框架，将对抗学习融入图提示中，实现鲁棒的图神经网络微调，通过min-max优化解决图拓扑和节点特征的噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法在图拓扑和节点特征面临各种噪声和攻击时表现出显著脆弱性，需要开发鲁棒的图微调方法。

Method: 提出对抗性图提示（AGP）框架，采用min-max优化：内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声；外层最小化学习最优节点提示来对抗噪声。

Result: 在多个基准任务上的实验验证了AGP相比最先进方法的鲁棒性和有效性，理论证明能同时处理图拓扑和节点噪声。

Conclusion: AGP是一种通用方法，可与各种预训练GNN模型集成，增强下游任务的鲁棒性，首次将对抗学习融入图提示实现鲁棒微调。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [208] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用问题无关的模拟数据集来研究极端类别不平衡下的性能表现，发现最佳检测器高度依赖于训练数据中故障样本的总数量。


<details>
  <summary>Details</summary>
Motivation: 工业系统中机器学习应用面临极端类别不平衡的挑战，主要由于训练过程中故障数据的有限可用性。需要评估异常检测算法在真实工程约束下的性能表现。

Method: 使用基于超球面异常分布的合成数据集（2D和10D），在异常率0.05%到20%、训练规模1000到10000的范围内，对14种检测器进行基准测试，测试集大小为40000。

Result: 发现最佳检测器高度依赖于训练数据中故障样本总数：少于20个故障样本时无监督方法占优；30-50个故障样本时半监督和监督方法性能大幅提升；半监督方法在10个特征时优势明显；额外健康样本通常无显著帮助。

Conclusion: 研究揭示了异常检测方法在小数据集上的泛化性能下降，为工业环境中部署异常检测提供了实用见解，强调了根据可用故障样本数量选择合适方法的重要性。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [209] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出了一种渐近置信度的最佳臂识别新框架，通过放宽精确误差控制要求，在长时域场景下实现更紧的样本复杂度界限，支持非参数分布和协变量利用。


<details>
  <summary>Details</summary>
Motivation: 现有BAI方法在实际应用中存在局限：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致样本效率低下。现实场景常涉及弱信号、高显著性要求和实验后推断需求，这些都需要长时域，但现有方法无法有效处理。

Method: 提出渐近框架，要求误差控制在最小样本量下渐近有效；开发新颖的渐近任意时间有效置信序列用于臂索引；设计新BAI算法，灵活整合协变量进行方差缩减，确保完全非参数设置下的近似误差控制。

Result: 在温和收敛假设下，提供了样本复杂度的渐近界限；最坏情况样本复杂度与高斯BAI在精确误差保证和已知方差下的最佳情况样本复杂度匹配；实验表明方法在保持误差控制的同时减少了平均样本复杂度。

Conclusion: 提出的渐近框架克服了传统BAI方法的局限性，在现实长时域场景中实现了更优的样本效率，同时支持非参数分布和协变量利用，为实际应用提供了更实用的解决方案。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [210] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Sequential Reservoir Computing架构，通过将大储层分解为一系列小型互连储层，降低内存和计算成本，在保持长期时间依赖性的同时，显著提升高维时空系统的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中面临梯度训练和内存瓶颈问题，而传统Reservoir Computing虽然通过固定循环层和凸读出优化缓解了这些问题，但在输入维度增加时仍存在可扩展性问题。

Method: 提出Sequential Reservoir Computing架构，将大型储层分解为一系列小型互连储层，保持传统RC的简单性和效率，同时显著降低内存和计算成本。

Result: 在低维混沌系统和高维物理模拟中，Sequential RC相比LSTM和标准RNN基线，实现了15-25%更长的有效预测时间、20-30%更低的误差指标，训练成本降低达三个数量级。

Conclusion: Sequential RC在保持传统RC简单性和效率的同时，实现了对高维动力系统的卓越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [211] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下保持语义完整性，相比均匀保护在1dB SNR下获得6.8% chrF分数提升和9.3%实体保护改进。


<details>
  <summary>Details</summary>
Motivation: 解决有限带宽通信系统中语义保持的挑战，传统信道编码（如LDPC、Reed-Solomon）无法实现细粒度语义保护，需要在边缘计算和物联网等带宽稀缺但语义保真度关键的场景中提供解决方案。

Method: 提出基于强化学习的自适应重复编码框架，使用复合语义失真度量（平衡全局嵌入相似性和实体级保护），通过强化学习代理以上下文感知方式分配保护资源，实现按维度不等错误保护。

Result: 实验显示相比均匀保护有统计显著提升：在1dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保护，证明简单但智能分配的重复编码能够实现细粒度语义保护。

Conclusion: 代码结构必须与语义粒度对齐，挑战传统信道编码范式，为下一代语义感知网络提供实用路径，特别适合边缘计算和物联网场景。

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [212] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究AI生成图像溯源问题，提出利用未标注网络数据提升目标生成模型（如DALL-E 3）的识别性能，解决现有方法对未知生成器泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，实现特定生成模型的溯源识别。现有方法在已知生成器上表现良好，但难以泛化到未见的新生成器。

Method: 1. 使用CLIP特征和线性分类器建立基线；2. 提出约束优化方法，利用未标注网络数据（可能包含真实图像、未知生成器输出或目标模型样本）；3. 鼓励网络样本被分类为非目标，同时约束在标注数据上的性能保持高位。

Result: 实验结果显示，引入网络数据显著提升了在挑战性未见生成器上的溯源性能，证明未标注数据可以有效增强开放世界场景下的AI生成内容溯源能力。

Conclusion: 未标注网络数据是提升AI生成内容溯源泛化能力的关键资源，提出的约束优化方法能有效利用这些数据增强模型对未知生成器的识别能力。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [213] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一个关于L2正则化宽网络中特征学习的理论，表明监督学习本质上是压缩性的。作者推导出预测"注水"谱演化的核ODE，证明对于任何稳定稳态，核秩受类别数C限制，且SGD噪声也是低秩的(O(C))，将动态限制在任务相关子空间。


<details>
  <summary>Details</summary>
Motivation: 研究监督学习中特征学习的本质特性，特别是探索为什么监督学习会产生压缩表示，以及这与自监督学习的高秩扩展表示形成对比。旨在统一确定性和随机性视角下的对齐理论。

Method: 提出宽L2正则化网络的特征学习理论框架，推导核ODE描述谱演化，证明核秩受类别数限制的定理，分析SGD噪声的低秩特性，对比监督学习与自监督学习的表示特性。

Result: 证明了监督学习是固有压缩的，核秩受类别数C限制，SGD噪声也是低秩的(O(C))，将动态限制在任务相关子空间。这与自监督学习的高秩扩展表示形成鲜明对比。

Conclusion: 监督学习本质上产生压缩的低秩表示，而自监督学习产生高秩扩展表示。该理论框架统一了确定性和随机性视角下的对齐，解释了为什么监督学习会自然收敛到任务相关子空间。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [214] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出结合光流和三种深度模型（监督CNN、预训练CNN特征提取器、层次极限学习机）进行无人机视频中人体检测的方法，在UCF-ARG数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识且对动态变化敏感，而特征学习方法能自动提取抽象特征。针对无人机非静态相机拍摄的变高度视频中的人体检测挑战，需要更鲁棒的方法。

Method: 结合光流和三种深度模型：1) 监督卷积神经网络（S-CNN）配合softmax或SVM分类器；2) 预训练CNN特征提取器；3) 层次极限学习机（H-ELM）。在UCF-ARG空中数据集上训练测试，评估五种人类动作。

Result: 预训练CNN平均准确率98.09%，S-CNN使用softmax达95.6%（SVM为91.7%），H-ELM达95.9%。H-ELM在CPU上训练445秒，S-CNN在GPU上训练770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务中表现成功，预训练CNN效果最佳。深度学习方法相比传统手工特征方法更具优势。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [215] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出TGE方法，通过轨迹级生成嵌入解决离线观察模仿学习中专家演示稀缺、离线数据与专家行为差异大的问题，利用时间扩散模型学习潜在空间中的专家状态密度，构建密集平滑的替代奖励。


<details>
  <summary>Details</summary>
Motivation: 解决离线观察模仿学习(LfO)中的核心挑战：专家演示稀缺，离线次优数据与专家行为分布差异大。现有分布匹配方法在此场景下表现不佳，因为它们施加严格的支持约束并依赖脆弱的一步模型，难以从非完美数据中提取有用信号。

Method: 提出TGE方法：1) 在离线轨迹数据上训练时间扩散模型；2) 利用学习到的扩散嵌入的平滑几何特性，在潜在空间中估计专家状态密度；3) 构建轨迹级生成嵌入，生成密集平滑的替代奖励；4) 捕获长期时间动态，弥合不相交支持之间的差距。

Result: 在D4RL运动控制和操作基准测试中，TGE方法一致匹配或优于先前的离线LfO方法，证明了其在专家演示稀缺、离线数据分布与专家差异大的情况下的有效性。

Conclusion: TGE通过轨迹级生成嵌入方法，利用时间扩散模型的平滑几何特性，有效解决了离线观察模仿学习中数据分布不匹配的问题，为从非完美数据中提取有用学习信号提供了稳健的解决方案。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [216] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究了残差网络中层级SGD如何高效学习分层模型，这类模型超越了先前深度学习算法可学习的模型，达到了高效可学习性的深度极限。


<details>
  <summary>Details</summary>
Motivation: 动机是理解深度学习为何能成功学习复杂函数。作者认为分层模型可能为理解深度学习提供基础，因为人类"教师"的存在暗示了分层结构的内在可用性——教师通过提供细粒度标签，揭示了大脑内部算法的"提示"或"片段"。

Method: 方法采用监督学习框架，假设存在未知的标签层次结构 L₁ ⊆ L₂ ⊆ ... ⊆ Lᵣ = [n]。简单标签是输入的直接函数，而更复杂的标签是较简单标签的函数。使用残差网络和层级SGD进行学习，并形式化了教师部分了解其内部逻辑的简化模型。

Result: 结果表明，这类分层模型超越了先前深度学习算法可学习的模型，达到了高效可学习性的深度极限。存在需要多项式深度才能表达的分层模型，而先前模型可以通过对数深度电路计算。形式化分析显示，在教师部分了解内部逻辑的简化模型中，会出现促进高效可学习性的分层结构。

Conclusion: 结论是分层模型的学习能力可能最终成为理解深度学习的基础。这类模型不仅自然适用于深度学习擅长的领域，而且人类教师的存在支持了分层结构内在可用的假设。该工作为深度学习为何能学习复杂函数提供了理论解释。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [217] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了一种多用户上下文赌博机方法，将图同质性与非线性奖励函数结合，通过统一的多用户RKHS核函数设计算法，实现结构化探索。


<details>
  <summary>Details</summary>
Motivation: 在多用户上下文赌博机中，用户通过图结构关联，奖励函数既表现出非线性特性又具有图同质性。现有方法通常单独处理这些特性，缺乏统一的框架来同时建模非线性行为和用户间的图结构关系。

Method: 提出了一种联合惩罚项，结合图平滑项（基于RKHS距离）和个体粗糙度惩罚。证明了该惩罚等价于统一的多用户RKHS中的平方范数，并显式推导了其再生核，该核将图拉普拉斯矩阵与基础臂核优雅地融合。基于此设计了LK-GP-UCB和LK-GP-TS算法，利用高斯过程后验进行探索。

Result: 理论分析提供了高概率遗憾界，其缩放与多用户核的有效维度相关，而非用户数量或环境维度。实验表明，在非线性设置中，该方法优于强线性和非图感知基线，即使在真实奖励为线性的情况下也保持竞争力。

Conclusion: 本文提供了一个统一、理论严谨且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，为多用户上下文赌博机中的图结构建模提供了新思路。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [218] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出SAFN（类加权稀疏注意力融合网络），一种用于帕金森病多模态分析的深度学习框架，通过对称交叉注意力机制融合MRI影像和临床数据，解决可解释性、类别不平衡等问题。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病计算模型存在可解释性差、类别不平衡、多模态数据融合困难等问题，需要开发一个既能有效整合生物和临床标记物，又能保持透明度和可解释性的预测框架。

Method: 使用模态特定编码器处理MRI皮层厚度、体积测量、临床评估和人口统计学变量；采用对称交叉注意力机制捕捉影像与临床表征间的非线性交互；通过稀疏约束注意力门控融合层动态选择信息模态；使用类别平衡焦点损失（β=0.999, γ=1.5）处理数据不平衡。

Result: 在帕金森病进展标志物倡议的703名参与者（570名PD，133名健康对照）上，通过五折交叉验证，SAFN达到0.98±0.02的准确率和1.00±0.00的PR-AUC，优于现有机器学习和深度学习基线。

Conclusion: SAFN为神经退行性疾病计算分析提供了一个可重复、透明的多模态建模范式，其决策过程具有临床一致性（约60%预测权重分配给临床评估），符合运动障碍学会诊断原则。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [219] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究LSTM模型压缩，通过逐步减少隐藏单元数量（从128到16）来平衡模型大小与预测精度，发现在零售销售预测中，64单元模型比128单元模型更小（73%）、更准确（47%），挑战了"模型越大越好"的假设。


<details>
  <summary>Details</summary>
Motivation: 标准LSTM在零售销售预测中准确但计算成本高，对中小型零售业构成挑战。研究旨在探索模型压缩方法，在保持预测精度的同时减少计算需求。

Method: 采用渐进式LSTM模型压缩方法，逐步减少隐藏单元数量（128→64→32→16）。使用Kaggle Store Item Demand Forecasting数据集（913,000条日销售记录，10个商店，50个商品）评估模型大小与预测精度的权衡。

Result: 64单元LSTM模型表现最佳：MAPE从128单元的23.6%降至12.4%，模型大小减少73%（280KB→76KB），精度提高47%。证明更大模型不一定更好，存在最优压缩点。

Conclusion: LSTM模型压缩可显著减少计算需求同时提高预测精度，这对资源受限的零售业有实际应用价值。研究挑战了"模型越大越好"的假设，展示了模型优化的潜力。

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [220] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

Relevance: 35.0

TL;DR: 本文提出了一种基于自动机的强化学习框架，用于在MITL时序逻辑规范下为MDP和POMDP合成策略，通过将MITL公式转换为定时LDGBA并与决策过程同步，构建适合Q学习的乘积定时模型。


<details>
  <summary>Details</summary>
Motivation: 动态不确定环境中的机器人系统需要满足复杂任务序列和严格时间约束的规划器。MITL提供了形式化表达时间约束的框架，但由于随机动态和部分可观测性，将MITL与强化学习结合仍然具有挑战性。

Method: 将MITL公式转换为定时限确定广义Büchi自动机(Timed-LDGBA)，与底层决策过程同步构建乘积定时模型，采用简单但富有表达力的奖励结构来强制执行时序正确性，同时允许额外的性能目标。

Result: 在三个仿真研究中验证：5×5网格世界(MDP)、10×10网格世界(POMDP)和办公室服务机器人场景。结果表明，该框架能持续学习满足严格时间约束的策略，可扩展到更大状态空间，在部分可观测环境中保持有效。

Conclusion: 该框架展示了在时间关键和不确定环境中可靠机器人规划的潜力，能够处理随机转移、扩展到大状态空间，并在部分可观测环境中保持有效性。

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [221] [Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction](https://arxiv.org/abs/2601.00146)
*Vikram Seenivasan,Srinath Saikrishnan,Andrew Lizarraga,Jonathan Soriano,Bernie Boscoe,Tuan Do*

Main category: astro-ph.IM

Relevance: 35.0

TL;DR: LoRA技术应用于天体物理学中的红移估计，通过结合不同星系成像数据集来改进CNN模型的性能，在计算效率和准确性之间取得平衡


<details>
  <summary>Details</summary>
Motivation: 天体物理学中红移估计面临数据质量与数量的权衡：光谱红移数据准确但稀缺，测光红移数据丰富但精度较低。需要一种方法能结合两者的优势，同时避免完全重新训练的高计算成本

Method: 使用LoRA（低秩适应）技术：首先在测光红移数据集上训练基础CNN模型，然后在光谱红移数据集上使用LoRA进行微调。LoRA通过添加适配器网络调整模型权重，避免完全重新训练

Result: LoRA模型相比传统迁移学习方法表现更好：偏差减少约2.5倍，散射减少约2.2倍。虽然完全重新训练在泛化能力上略优于LoRA，但计算成本显著更高

Conclusion: LoRA为天体物理学中的回归模型微调提供了计算效率和性能之间的良好平衡，特别适用于数据稀疏的任务，能够充分利用现有的预训练模型

Abstract: In this work, we demonstrate how Low-Rank Adaptation (LoRA) can be used to combine different galaxy imaging datasets to improve redshift estimation with CNN models for cosmology. LoRA is an established technique for large language models that adds adapter networks to adjust model weights and biases to efficiently fine-tune large base models without retraining. We train a base model using a photometric redshift ground truth dataset, which contains broad galaxy types but is less accurate. We then fine-tune using LoRA on a spectroscopic redshift ground truth dataset. These redshifts are more accurate but limited to bright galaxies and take orders of magnitude more time to obtain, so are less available for large surveys. Ideally, the combination of the two datasets would yield more accurate models that generalize well. The LoRA model performs better than a traditional transfer learning method, with $\sim2.5\times$ less bias and $\sim$2.2$\times$ less scatter. Retraining the model on a combined dataset yields a model that generalizes better than LoRA but at a cost of greater computation time. Our work shows that LoRA is useful for fine-tuning regression models in astrophysics by providing a middle ground between full retraining and no retraining. LoRA shows potential in allowing us to leverage existing pretrained astrophysical models, especially for data sparse tasks.

</details>


### [222] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

Relevance: 35.0

TL;DR: CPPDD是一个轻量级、后设置的隐私保护数据聚合框架，通过双层保护机制和共识锁定实现安全的多客户端协作，具有线性可扩展性和高效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多方计算（MPC）和同态加密（HE）方案在可扩展性、计算开销和信任最小化方面的不足，为受监管和资源受限环境提供高效的安全数据聚合方案。

Method: 采用双层保护机制：每客户端仿射掩码和优先级驱动的顺序共识锁定，结合步骤校验和（sigma_S）与数据校验和（sigma_D）实现去中心化完整性验证，支持标量、向量和矩阵负载。

Result: 在MNIST数据集上验证，线性可扩展至500个客户端，每客户端计算时间亚毫秒级，恶意偏差检测率100%，数据恢复精确，计算开销比MPC和HE基线低3-4个数量级。

Conclusion: CPPDD为安全投票、联盟联邦学习、区块链托管和地理信息能力建设等场景提供了可扩展、信任最小化且可验证的多方计算解决方案。

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [223] [Three factor delay learning rules for spiking neural networks](https://arxiv.org/abs/2601.00668)
*Luke Vassallo,Nima Taherinejad*

Main category: cs.NE

Relevance: 35.0

TL;DR: 该论文提出了一种在线学习脉冲神经网络中突触和轴突延迟参数的三因素学习规则，使用高斯代理梯度计算资格迹，显著提升了时序任务性能并降低了模型规模和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统SNN主要学习突触权重，对时序模式识别贡献有限。现有延迟学习方法依赖大网络和离线学习，不适合资源受限环境的实时操作。需要开发在线学习延迟参数的方法来提升SNN在时序任务中的性能。

Method: 在LIF前馈和循环SNN中引入突触和轴突延迟参数，提出三因素学习规则在线同时学习延迟参数。使用平滑高斯代理近似脉冲导数计算资格迹，结合自上而下的误差信号确定参数更新。

Result: 引入延迟使准确率比仅权重基线提升高达20%；相似参数数量下，联合学习权重和延迟使准确率提升高达14%。在SHD语音识别数据集上达到与离线反向传播方法相似的准确率，同时模型规模减少6.6倍，推理延迟降低67%，仅损失2.4%准确率。

Conclusion: 该方法通过在线学习延迟参数显著提升了SNN在时序任务中的性能，同时大幅降低了模型规模和推理延迟，有利于设计功耗和面积受限的神经形态处理器，支持设备端学习并降低内存需求。

Abstract: Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.

</details>


### [224] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文提出了一种基于熵的再训练框架，通过非平衡随机动力学建模数据漂移，使用熵触发机制来平衡再训练频率与操作成本。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在非平稳环境中部署时，由于数据漂移会导致性能下降。现有的漂移检测方法大多缺乏原理性的动力学解释，且无法指导如何平衡再训练频率与操作成本。

Method: 将部署时的数据漂移建模为受Fokker-Planck方程控制的概率流，使用时变Kullback-Leibler散度量模型-数据不匹配，提出基于熵平衡分解的熵触发再训练策略。

Result: 在受控的非平稳分类实验中，熵触发再训练实现了与高频再训练相当的预测性能，同时相对于每日和基于标签的策略，将再训练事件减少了一个数量级。

Conclusion: 基于熵的再训练框架为数据漂移管理提供了原理性的动力学解释，能够有效平衡模型性能与操作成本。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [225] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

Relevance: 25.0

TL;DR: GRL-SNAM是一个用于未知环境中同时导航与建图的几何强化学习框架，通过局部感官观察而非全局地图构建，使用哈密顿优化实现动态最短路径搜索。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图（SNAM）的挑战性问题，传统方法需要构建全局地图或设计复杂的多智能体策略，而本文提出仅依赖局部感官观察的方法。

Method: 将路径导航和建图建模为动态最短路径搜索过程，使用受控哈密顿优化：将感官输入转换为局部能量景观编码可达性、障碍物和变形约束，通过更新哈密顿量分阶段演化感知、规划和重配置策略。

Result: 在2D导航任务上评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM在相同阶段感知约束下保持间隙、泛化到未见布局，通过局部能量优化而非广泛全局建图实现高质量导航。

Conclusion: 通过更新哈密顿量的几何强化学习能够通过最小化探索和局部能量优化实现高质量导航，无需构建全局地图，为未知环境中的同时导航与建图提供了新方法。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [226] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出SSI-GAN（半监督Swin启发GAN）用于蚊子神经元尖峰信号分类，仅需1-3%标注数据即可达到99.93%准确率，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒主要传播媒介，人工分类神经元尖峰模式耗时耗力。现有深度学习方案需要完全标注数据集和高度预处理信号，限制了实际场景的大规模应用。为解决标注数据稀缺问题，需要开发半监督方法。

Method: 提出SSI-GAN架构：结合Swin Transformer的移位窗口判别器和基于Transformer的生成器。使用多头自注意力模型在平面窗口式Transformer判别器中学习稀疏高频尖峰特征。仅用1-3%标注数据，在超过1500万尖峰样本上训练，使用贝叶斯Optuna框架优化超参数，五折蒙特卡洛交叉验证验证鲁棒性。

Result: 感染后第三天仅用3%标注数据达到99.93%分类准确率。仅1%监督下在所有感染阶段保持高准确率。相比标准监督方法，在相同性能水平下减少97-99%人工标注工作量。移位窗口Transformer设计大幅超越所有基线，创下尖峰神经元感染分类新纪录。

Conclusion: SSI-GAN通过半监督学习和Transformer架构有效解决了神经元尖峰信号分类中的标注数据稀缺问题，为实际现场应用提供了可行方案，在生物医学信号处理领域具有重要应用价值。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [227] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一个资源高效的数据中心框架，用于心律失常检测，通过特征工程使高维数据线性可分，实现98.44%准确率，模型仅8.54KB，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测。现有深度学习方法计算开销大，不适合资源受限的边缘设备。

Method: 采用资源高效的数据中心框架，优先特征工程而非模型复杂度。结合时频小波分解和图论结构描述符（如PageRank中心性），通过互信息和递归消除优化特征空间，使用可解释的超轻量线性分类器。

Result: 在MIT-BIH和INCART数据集上达到98.44%诊断准确率，模型大小仅8.54KB，分类推理延迟0.46μs，每搏处理管道52ms，实现实时操作。相比压缩模型KD-Light（25KB，96.32%准确率）有数量级效率提升。

Conclusion: 该框架通过精心设计的特征工程使复杂心律失常数据线性可分，实现了边缘设备上的高效实时心律失常检测，为无电池心脏传感器提供了可行方案。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [228] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了一个基于国际象棋战术位置的量子基准测试QKRD，用于系统评估QAOA算法设计选择，发现结构化基准能揭示随机实例中隐藏的问题感知技术优势。


<details>
  <summary>Details</summary>
Motivation: 当前QAOA主要在MaxCut、TSP、SAT等缺乏语义结构和人类可解释性的随机实例上进行基准测试，这些测试对真实世界问题的性能洞察有限。需要具有结构化约束、空间局部性和人类可解释性的基准来评估量子算法在实际问题上的表现。

Method: 引入量子王环支配问题(QKRD)，这是一个基于国际象棋战术位置的NISQ规模基准，包含5000个结构化实例，具有one-hot约束、空间局部性和10-40量子比特规模。该基准结合人类可解释的覆盖度指标和针对经典启发式算法的内在验证，无需外部预言机即可得出算法结论。

Result: 使用QKRD系统评估QAOA设计选择发现：约束保持混合器(XY, domain-wall)比标准混合器收敛快约13步；预热策略减少45步收敛时间，能量改进超过d=8；CVaR优化产生负面结果，能量更差且无覆盖度优势。QAOA比贪婪启发式算法表现好12.6%，比随机选择好80.1%。

Conclusion: 结构化基准能揭示问题感知QAOA技术在随机实例中被掩盖的优势。QKRD为可重复的NISQ算法研究提供了代码、数据和实验工件。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [229] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-虚拟形象交互，实现低延迟（约500ms）的互动头像生成，并引入基于条件丢弃的无标签直接偏好优化方法学习表达性互动。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟形象生成模型缺乏真正的互动感，通常生成单向响应，缺乏情感参与。需要解决两个关键挑战：在因果约束下实时生成运动，以及在没有额外标注数据的情况下学习表达性、生动的反应。

Method: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-虚拟形象交互，处理实时多模态输入（用户音频和运动）。引入直接偏好优化方法，利用通过丢弃用户条件构建的合成负样本，实现无标签的表达性互动学习。

Result: 框架实现低延迟实时交互（约500ms），相比基线加速6.8倍，生成的反应性和表达性虚拟形象运动在超过80%的情况下优于基线。

Conclusion: Avatar Forcing框架成功解决了实时互动虚拟形象生成的关键挑战，通过扩散强迫和直接偏好优化实现了低延迟、表达性强的虚拟形象互动。

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [230] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出了一种主动学习框架，用于智能地丰富参数化动力系统的数据驱动降阶模型，通过贝叶斯线性回归和自适应采样策略提升模型稳定性和准确性


<details>
  <summary>Details</summary>
Motivation: 数据驱动的降阶模型对训练数据质量敏感，需要识别最佳训练参数来构建高质量的参数化降阶模型，以支持数字孪生中的虚拟资产

Method: 使用算子推断方法，建立参数化算子推断的概率版本（贝叶斯线性回归），利用概率ROM解的不确定性设计顺序自适应采样方案来选择新的训练参数向量

Result: 数值实验表明，在相同计算预算下，提出的自适应采样策略比随机采样能产生更稳定和准确的降阶模型

Conclusion: 主动学习框架能有效提升数据驱动降阶模型的质量，为数字孪生应用提供更可靠的虚拟资产基础

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [231] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出KRCD方法，用于在非线性单环境观测数据中检测未观测混杂因子，通过比较标准和高阶核回归来识别未观测混杂效应。


<details>
  <summary>Details</summary>
Motivation: 现有检测未观测混杂因子的方法需要线性假设或多个异质环境，限制了在非线性单环境设置中的应用。需要开发能在非线性观测数据中检测未观测混杂因子的方法。

Method: 提出核回归混杂因子检测(KRCD)方法，利用再生核希尔伯特空间建模复杂依赖关系。通过比较标准和高阶核回归，推导出检验统计量，其显著偏离零表示存在未观测混杂因子。

Result: 理论证明：在无限样本下，回归系数一致当且仅当不存在未观测混杂因子；有限样本差异收敛到零均值高斯分布。实验表明KRCD在合成基准和Twins数据集上优于现有基线，且计算效率更高。

Conclusion: KRCD为非线性单环境观测数据中的未观测混杂因子检测提供了有效方法，填补了现有方法的空白，具有理论保证和实际应用价值。

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [232] [AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules](https://arxiv.org/abs/2601.00581)
*Stephen E. Farr,Stefan Doerr,Antonio Mirarchi,Francesc Sabanes Zariquiey,Gianni De Fabritiis*

Main category: physics.chem-ph

Relevance: 25.0

TL;DR: AceFF是一个针对小分子药物发现优化的预训练机器学习原子间势能模型，通过改进的TensorNet2架构和全面的药物样化合物数据集训练，在保持DFT级精度的同时实现高通量推理速度。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习原子间势能模型已成为密度泛函理论的高效替代方案，但在不同化学空间中的泛化能力仍然有限。AceFF旨在解决这一问题，专门针对药物发现领域的小分子化合物开发高精度、高效率的力场模型。

Method: 采用改进的TensorNet2架构，在包含药物样化合物的全面数据集上进行训练。模型支持所有基本药物化学元素（H, B, C, N, O, F, Si, P, S, Cl, Br, I），并专门训练处理带电状态。

Result: 在严格的基准测试中表现出色，包括复杂扭转能扫描、分子动力学轨迹、批量最小化以及力和能量精度评估。AceFF在有机分子领域建立了新的最先进水平。

Conclusion: AceFF提供了一个平衡高通量推理速度和DFT级精度的力场模型，专门针对药物发现中的小分子化合物优化，为药物化学研究提供了强大的计算工具。

Abstract: We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.

</details>


### [233] [Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs](https://arxiv.org/abs/2601.00672)
*Seungchan Ko,Jiyeon Kim,Dongwook Shin*

Main category: math.NA

Relevance: 25.0

TL;DR: 提出稀疏网络架构的FEONet改进方法，通过利用有限元结构降低计算成本，同时保持精度，适用于大规模参数化PDE问题。


<details>
  <summary>Details</summary>
Motivation: 原始FEONet在处理大规模问题时，随着单元数量增加，计算成本上升且精度可能下降，需要改进以应对大规模参数化偏微分方程问题。

Method: 提出基于有限元结构的稀疏网络架构，利用有限元的局部特性设计稀疏连接，减少参数数量，同时建立理论保证近似能力和稳定性。

Result: 稀疏网络在计算成本和效率方面有显著改进，同时保持可比精度，理论分析证明其能有效逼近目标算子且训练预测稳定。

Conclusion: 稀疏FEONet架构成功解决了原始方法在大规模问题中的计算效率问题，为参数化PDE的算子学习提供了更实用的解决方案。

Abstract: In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction.

</details>


### [234] [NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion](https://arxiv.org/abs/2601.00389)
*Muhammad Bilal,Omer Tariq,Hasan Ahmed*

Main category: cs.CR

Relevance: 20.0

TL;DR: 提出了NOS-Gate，一种用于独立网关的流式入侵检测系统，利用网络优化尖峰动态的轻量级双状态单元，仅使用元数据在加密流量上进行实时检测，并通过加权公平队列进行可逆缓解。


<details>
  <summary>Details</summary>
Motivation: 加密流量中的时序和突发模式可能泄露信息，自适应攻击者可以利用这些信息。这破坏了独立消费者网关中仅基于元数据的检测能力，因此需要在严格的CPU和延迟预算下，为消费者网关开发仅使用元数据的流式加密流量入侵检测系统。

Method: 提出了NOS-Gate系统，为每个流实例化一个源自网络优化尖峰动态的轻量级双状态单元。系统对固定长度的元数据特征窗口进行评分，在K-of-M持续规则下触发可逆缓解措施，暂时降低流在加权公平队列中的权重。

Result: 在时序控制规避攻击下评估，使用可执行的"worlds"基准测试，在0.1%误报率下，NOS-Gate达到0.952的事件召回率，优于最佳基线的0.857。在门控下，减少了p99.9排队延迟和p99.9附带延迟，每个流窗口的平均评分成本约为2.09微秒。

Conclusion: NOS-Gate是一种有效的流式入侵检测系统，能够在严格的资源约束下，仅使用加密流量的元数据进行高效检测和缓解，显著提高了检测性能同时降低了延迟影响。

Abstract: Timing and burst patterns can leak through encryption, and an adaptive adversary can exploit them. This undermines metadata-only detection in a stand-alone consumer gateway. Therefore, consumer gateways need streaming intrusion detection on encrypted traffic using metadata only, under tight CPU and latency budgets. We present a streaming IDS for stand-alone gateways that instantiates a lightweight two-state unit derived from Network-Optimised Spiking (NOS) dynamics per flow, named NOS-Gate. NOS-Gate scores fixed-length windows of metadata features and, under a $K$-of-$M$ persistence rule, triggers a reversible mitigation that temporarily reduces the flow's weight under weighted fair queueing (WFQ). We evaluate NOS-Gate under timing-controlled evasion using an executable 'worlds' benchmark that specifies benign device processes, auditable attacker budgets, contention structure, and packet-level WFQ replay to quantify queue impact. All methods are calibrated label-free via burn-in quantile thresholding. Across multiple reproducible worlds and malicious episodes, at an achieved $0.1%$ false-positive operating point, NOS-Gate attains 0.952 incident recall versus 0.857 for the best baseline in these runs. Under gating, it reduces p99.9 queueing delay and p99.9 collateral delay with a mean scoring cost of ~ 2.09 μs per flow-window on CPU.

</details>


### [235] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

Relevance: 15.0

TL;DR: IMBWatch是一个时空图神经网络框架，用于大规模检测非法按摩院（IMBs），通过构建动态图分析在线广告、商业许可记录和众包评论等开源情报，识别人口贩卖和性剥削网络。


<details>
  <summary>Details</summary>
Motivation: 非法按摩院（IMBs）以合法健康服务为掩护，从事人口贩卖、性剥削和强迫劳动等犯罪活动。由于数字广告编码、人员地点频繁变更、共享基础设施（如电话号码和地址）重复使用等问题，传统检测方法（社区举报和监管检查）反应迟钝且难以揭示犯罪网络全貌。

Method: IMBWatch是一个时空图神经网络（ST-GNN）框架，从开源情报（在线广告、商业许可记录、众包评论）构建动态图。节点代表企业、别名、电话号码、位置等异质实体，边捕捉时空和关系模式（共址、电话重复使用、广告同步）。框架结合图卷积操作和时间注意力机制，建模IMB网络在时空上的演化模式。

Result: 在美国多个城市的真实数据集上，IMBWatch优于基线模型，获得更高的准确率和F1分数。除了性能提升，该框架还提供更好的可解释性，为主动干预提供可操作的见解。

Conclusion: IMBWatch是一个可扩展、可适应其他非法领域的框架，已发布匿名数据和开源代码支持可重复研究，为打击人口贩卖和性剥削提供了有效的技术工具。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [236] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

Relevance: 15.0

TL;DR: 使用XGBoost模型预测美国交通事故严重程度，基于50万条事故数据，达到78%准确率，发现时间、地理位置和天气变量是最强预测因子。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对交通事故严重程度的预测能力，为基于证据的交通管理提供支持，并改进严重程度预测研究。

Method: 使用2016-2023年美国50万条交通事故数据集，训练XGBoost分类器，通过随机搜索交叉验证优化，采用类别加权处理类别不平衡问题。

Result: 最终模型整体准确率78%，对主要类别（严重程度2）达到87%的精确率和召回率。特征重要性分析显示时间、地理位置、能见度、温度和风速是最强预测因子，但降水和能见度预测能力有限。

Conclusion: 数据集主要包含中等严重程度事故，限制了模型对极端情况的预测能力，需要替代采样策略、增强特征工程和整合外部数据集。这些发现为交通管理提供依据并指明未来研究方向。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [237] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

Relevance: 15.0

TL;DR: 基于电子健康记录数据的机器学习模型在预测肝硬化方面显著优于传统FIB-4评分，能够提前1-3年进行风险分层


<details>
  <summary>Details</summary>
Motivation: 开发基于常规电子健康记录数据的机器学习模型，用于早期预测肝硬化，并与传统FIB-4评分进行性能比较，以支持临床决策和预防管理

Method: 回顾性队列研究，使用大型学术医疗系统的去标识化电子健康记录数据。识别脂肪肝患者并分为肝硬化和非肝硬化队列。构建预测场景，使用观察窗口和预测窗口模拟真实临床使用。从观察窗口汇总人口统计学、诊断、实验室结果、生命体征和共病指数。训练XGBoost模型用于1年、2年和3年预测时间范围，并在保留测试集上评估性能。使用AUC与FIB-4进行比较

Result: 最终队列包括1年预测3,043名患者，2年预测1,981名患者，3年预测1,470名患者。在所有预测时间范围内，机器学习模型始终优于FIB-4。XGBoost模型在1年、2年和3年预测中的AUC分别为0.81、0.73和0.69，而FIB-4的AUC分别为0.71、0.63和0.57。随着预测时间范围的延长，性能增益持续存在，表明早期风险区分能力得到改善

Conclusion: 利用常规电子健康记录数据的机器学习模型在早期预测肝硬化方面显著优于传统FIB-4评分。这些模型能够实现更早、更准确的风险分层，可以作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [238] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一种无需姿态确定控制子系统(ADCS)的纳米卫星电力系统故障检测方法，使用神经网络模拟正常状态，结合多种机器学习方法进行故障分类


<details>
  <summary>Details</summary>
Motivation: 纳米卫星在低地球轨道运行中，电力系统各部件面临压力耐受性、发射压力和环境因素导致的故障风险。传统方法需要ADCS，但本文旨在开发无需ADCS的故障检测方案

Method: 1. 基于神经网络模拟无故障系统状态，输入为太阳辐射和太阳能板表面温度，输出为电流和负载
2. 使用神经网络分类器根据故障模式和类型进行诊断
3. 采用多种机器学习方法进行故障分类：PCA分类、决策树、KNN

Result: 开发了完整的故障检测框架，能够诊断光伏子系统的线间故障和开路、DC-DC转换器的IGBT短路和开路、地面电池调节器故障等常见故障

Conclusion: 该方法成功实现了无需ADCS的纳米卫星电力系统故障检测，为小型卫星的可靠运行提供了有效的故障诊断方案

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [239] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种基于1D UNet的数据增强方法（AugUNet1D），用于自动标注脑电图中的棘慢波放电事件，相比传统算法和基础1D UNet有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）中事件的手动标注非常耗时，特别是对于持续数周至数月的连续记录。棘慢波放电（SWD）作为失神发作的电生理标志，通常需要手动标注。虽然已有研究使用机器学习自动分割和分类EEG信号，但仍有改进空间。

Method: 研究比较了14种机器学习分类器在961小时EEG记录（包含22,637个标注SWD）上的性能，发现1D UNet表现最佳。通过数据增强（特别是缩放增强）改进1D UNet，得到AugUNet1D，并与最近发表的"Twin Peaks"算法进行比较。

Result: AugUNet1D在SWD标注任务上表现出优越性能，检测到的事件特征与手动标注的SWD更相似，优于传统算法方法和基础1D UNet。

Conclusion: AugUNet1D为EEG事件自动标注提供了有效解决方案，显著减少了手动工作量。研究公开了预训练和未训练的AugUNet1D模型供其他研究者使用。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [240] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

Relevance: 15.0

TL;DR: 使用扩散模型和云原生架构自动生成零售店货架布局图，将设计时间从30小时减少到0.5小时，成本降低97.5%


<details>
  <summary>Details</summary>
Motivation: 传统货架布局图设计耗时且昂贵（平均30小时/复杂布局），需要自动化解决方案来优化零售空间规划

Method: 基于扩散模型的云原生架构：1) 从多个零售店成功布局中学习；2) AWS云训练；3) 边缘部署实时推理；4) 修改损失函数集成零售约束

Result: 设计时间减少98.3%（30→0.5小时），约束满足率94.4%，成本降低97.5%，盈亏平衡期4.4个月，支持10,000并发请求

Conclusion: 生成式AI在自动化零售空间优化中具有可行性，云原生架构提供可扩展的解决方案

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [241] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种基于机器学习的骑行时间预测方法，使用路线拓扑特征和运动员当前体能状态（训练负荷指标）来替代传统物理模型所需的复杂参数。


<details>
  <summary>Details</summary>
Motivation: 现有骑行时间预测方案依赖需要大量参数（如空气阻力系数、实时风速预报）的物理模型，这些参数对大多数业余骑行者来说不切实际。需要一种更实用的方法，利用可获取的数据来预测骑行时间。

Method: 采用机器学习方法，结合路线拓扑特征和运动员当前体能状态（从训练负荷指标推导）。使用Lasso回归模型，通过特征工程消除数据泄露，在N-of-1研究设计中使用单个运动员数据集（N=96次骑行）。

Result: Lasso回归模型（拓扑+体能特征）达到MAE=6.60分钟和R²=0.922。整合体能指标（CTL, ATL）相比仅使用拓扑特征将误差降低了14%（MAE从7.66分钟降至6.60分钟）。渐进检查点预测支持动态比赛规划。

Conclusion: 机器学习方法能够有效预测骑行时间，使用历史性能代理替代复杂的物理测量。生理状态即使在自定节奏的努力中也能有意义地约束表现。该方法为业余骑行者提供了实用的预测工具。

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [242] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出了一种用于在向下封闭凸体上最大化非负、非单调γ-弱DR-次模函数的近似算法，当γ=1时恢复0.401近似比，当γ<1时保证平滑退化


<details>
  <summary>Details</summary>
Motivation: 次模目标在约束下的最大化是机器学习和优化的基本问题。研究非负、非单调γ-弱DR-次模函数在向下封闭凸体上的最大化问题，旨在提供对γ平滑依赖的近似保证

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，处理非单调性，形成简单有效的算法

Result: 算法在γ=1时恢复0.401近似比，γ<1时保证平滑退化，改进了先前在相同约束下γ-弱DR-次模最大化的已知界限

Conclusion: 该方法为向下封闭凸体上的非单调γ-弱DR-次模最大化提供了最先进的保证，结合了连续贪婪和双贪婪技术的优势

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [243] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的足球角球防守评估框架，通过球员追踪数据推断盯人和区域防守任务，实现无标签的防守贡献评估和角色条件化对抗分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调运动，现有对抗分析方法依赖"平均"行为模拟而缺乏战术上下文，需要更精确的防守表现评估方法。

Method: 针对角球场景设计协变量依赖隐马尔可夫模型(CDHMM)，从球员追踪数据推断时间分辨的盯人和区域防守任务，提出防守贡献归因框架和角色条件化对抗分析方法。

Result: 模型能够无标签地推断防守任务分配，提供可解释的防守贡献评估，相比传统方法能更好地考虑战术上下文，为无球防守表现提供更准确的对抗分析。

Conclusion: CDHMM框架为结构化足球场景(如角球)提供了有效的防守评估方法，通过角色条件化对抗分析实现了更准确的无球防守表现评估，具有可解释性和战术相关性。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [244] [Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models](https://arxiv.org/abs/2601.00081)
*Henry Crandall,Tyler Schuessler,Filip Bělík,Albert Fabregas,Barry M. Stults,Alexandra Boyadzhiev,Huanan Zhang,Jim S. Wu,Aylin R. Rodan,Stephen P. Juraschek,Ramakrishna Mukkamala,Alfred K. Cheung,Stavros G. Drakos,Christel Hohenegger,Braxton Osting,Benjamin Sanchez*

Main category: physics.med-ph

Relevance: 15.0

TL;DR: 开发了一种基于生物电阻抗（BioZ）的智能手表设备，用于无袖带血流动力学监测，通过多尺度建模和物理信息神经网络实现无需校准的血压和血流速度估计。


<details>
  <summary>Details</summary>
Motivation: 现有无袖带血压监测设备通常依赖缺乏理论基础的脉冲波分析或脉冲到达时间方法，容易受到生理和实验混杂因素的影响，影响准确性和临床实用性。

Method: 开发了具有实时生物电阻抗（BioZ）传感的智能手表设备；通过多尺度分析和计算建模框架阐明BioZ与血压之间的生物物理关系；使用融合流体动力学原理的信号标记物理信息神经网络进行无需校准的血压和血流速度估计。

Result: 在健康个体（休息和运动后）以及高血压和心血管疾病患者（门诊和重症监护环境）中成功测试了该方法，证明了BioZ技术用于无袖带血压和血流速度监测的可行性。

Conclusion: 生物电阻抗技术能够解决现有无袖带技术的关键局限性，为连续心血管健康监测提供了更可靠的理论基础和方法。

Abstract: Wearable technologies have the potential to transform ambulatory and at-home hemodynamic monitoring by providing continuous assessments of cardiovascular health metrics and guiding clinical management. However, existing cuffless wearable devices for blood pressure (BP) monitoring often rely on methods lacking theoretical foundations, such as pulse wave analysis or pulse arrival time, making them vulnerable to physiological and experimental confounders that undermine their accuracy and clinical utility. Here, we developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. We elucidate the biophysical relationship between BioZ and BP via a multiscale analytical and computational modeling framework, and identify physiological, anatomical, and experimental parameters that influence the pulsatile BioZ signal at the wrist. A signal-tagged physics-informed neural network incorporating fluid dynamics principles enables calibration-free estimation of BP and radial and axial blood velocity. We successfully tested our approach with healthy individuals at rest and after physical activity including physical and autonomic challenges, and with patients with hypertension and cardiovascular disease in outpatient and intensive care settings. Our findings demonstrate the feasibility of BioZ technology for cuffless BP and blood velocity monitoring, addressing critical limitations of existing cuffless technologies.

</details>


### [245] [Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks](https://arxiv.org/abs/2601.00342)
*Xuehui Qian,Hongkai Tao,Yongji Wang*

Main category: physics.flu-dyn

Relevance: 15.0

TL;DR: 提出了一种基于物理信息神经网络（PINNs）的新框架，用于在无界域中求解非线性可压缩势方程，解决了传统方法中域截断和线性化带来的误差问题。


<details>
  <summary>Details</summary>
Motivation: 在空气动力学中，准确模拟翼型上的亚音速可压缩流动对飞机设计至关重要。传统方法通常依赖线性化方程或有限截断域，这会引入不可忽略的误差并限制在实际场景中的应用。

Method: 使用物理信息神经网络（PINNs）求解完整的非线性可压缩势方程，通过坐标变换和将物理渐近约束嵌入网络架构来解决无界域和收敛挑战，并采用多阶段PINN（MS-PINN）方法迭代最小化残差。

Result: 通过模拟圆形和椭圆形几何形状的流动，验证了该框架的有效性，量化了域截断和线性化引入的显著差异（特别是在较高马赫数下），并证明该框架是计算流体动力学中鲁棒的高保真工具。

Conclusion: 该PINN框架为无界域中的非线性可压缩流动问题提供了高精度解决方案，克服了传统方法的局限性，在计算流体动力学领域具有重要应用价值。

Abstract: In aerodynamics, accurately modeling subsonic compressible flow over airfoils is critical for aircraft design. However, solving the governing nonlinear perturbation velocity potential equation presents computational challenges. Traditional approaches often rely on linearized equations or finite, truncated domains, which introduce non-negligible errors and limit applicability in real-world scenarios. In this study, we propose a novel framework utilizing Physics-Informed Neural Networks (PINNs) to solve the full nonlinear compressible potential equation in an unbounded (infinite) domain. We address the unbounded-domain and convergence challenges inherent in standard PINNs by incorporating a coordinate transformation and embedding physical asymptotic constraints directly into the network architecture. Furthermore, we employ a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision. We validate this framework by simulating flow over circular and elliptical geometries, comparing our results against traditional finite-domain and linearized solutions. Our findings quantify the noticeable discrepancies introduced by domain truncation and linearization, particularly at higher Mach numbers, and demonstrate that this new framework is a robust, high-fidelity tool for computational fluid dynamics.

</details>


### [246] [Deterministic Coreset for Lp Subspace](https://arxiv.org/abs/2601.00361)
*Rachit Chhaya,Anirban Dasgupta,Dan Feldman,Supratim Shit*

Main category: cs.DS

Relevance: 15.0

TL;DR: 提出了第一个迭代算法，用于构建确定性ℓ_p子空间嵌入的ε-coreset，解决了长期存在的对数因子问题，并获得了最优的coreset大小。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和数据分析中，coreset（核心集）技术用于压缩大规模数据集，同时保持重要的统计特性。然而，现有的ℓ_p子空间嵌入coreset构造方法存在对数因子问题，这限制了其理论紧致性和实际应用效果。本文旨在解决这一长期存在的开放性问题。

Method: 提出了一种迭代算法，在每次迭代中确保维护集上的损失在原始数据集损失的适当缩放范围内有上下界。算法通过加权选择矩阵X的行子集来构建coreset，保证了确定性ℓ_p子空间嵌入性质。

Result: 算法在O(poly(n,d,ε⁻¹))时间内返回大小为O(d^{max{1,p/2}}/ε²)的确定性ε-coreset，完全消除了对数因子，达到了理论下界，是最优的。该coreset还可用于确定性近似求解ℓ_p回归问题。

Conclusion: 本文首次提出了构建确定性ℓ_p子空间嵌入ε-coreset的迭代算法，解决了长期存在的对数因子问题，获得了最优的coreset大小，为大规模数据分析提供了更高效的压缩方法。

Abstract: We introduce the first iterative algorithm for constructing a $\varepsilon$-coreset that guarantees deterministic $\ell_p$ subspace embedding for any $p \in [1,\infty)$ and any $\varepsilon > 0$. For a given full rank matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $n \gg d$, $\mathbf{X}' \in \mathbb{R}^{m \times d}$ is an $(\varepsilon,\ell_p)$-subspace embedding of $\mathbf{X}$, if for every $\mathbf{q} \in \mathbb{R}^d$, $(1-\varepsilon)\|\mathbf{Xq}\|_{p}^{p} \leq \|\mathbf{X'q}\|_{p}^{p} \leq (1+\varepsilon)\|\mathbf{Xq}\|_{p}^{p}$. Specifically, in this paper, $\mathbf{X}'$ is a weighted subset of rows of $\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\ell_p$ subspace embedding. For an error parameter $\varepsilon$, our algorithm takes $O(\mathrm{poly}(n,d,\varepsilon^{-1}))$ time and returns a deterministic $\varepsilon$-coreset, for $\ell_p$ subspace embedding whose size is $O\left(\frac{d^{\max\{1,p/2\}}}{\varepsilon^{2}}\right)$. Here, we remove the $\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\ell_p$ regression problem in a deterministic manner.

</details>


### [247] [Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials](https://arxiv.org/abs/2601.00503)
*Li Chen,Leonardo Medrano Sandonas,Shirong Huang,Alexander Croy,Gianaurelio Cuniberti*

Main category: physics.chem-ph

Relevance: 15.0

TL;DR: 开发了MORE-ML计算框架，结合量子力学数据和机器学习预测体味挥发性有机物传感特性，为电子鼻分子受体设计提供机理理解和理性设计原则。


<details>
  <summary>Details</summary>
Motivation: 数字传感面临挑战，需要可持续方法扩展定制电子鼻在复杂体味挥发性有机物分析中的应用。现有方法缺乏从分子层面理解传感机理的能力。

Method: 开发MORE-ML框架，整合电子鼻分子构建块的量子力学性质数据与机器学习方法。扩展MORE-Q数据集为MORE-QX，采样更大的构象空间，计算电子结合特征。使用树基ML模型（CatBoost）预测传感相关性质，并应用可解释AI方法分析影响预测的关键量子力学性质。

Result: 分析发现构建块的量子力学性质与结合特征之间相关性较弱。CatBoost模型在预测性能上优于其他方法，尤其在未见化合物的迁移性方面表现突出。可解释AI方法识别出影响结合特征预测的关键量子力学性质。

Conclusion: MORE-ML框架结合量子力学洞察与机器学习，为体味传感中的分子受体提供了机理理解和理性设计原则，为分析复杂气味混合物的人工传感材料发展奠定基础。

Abstract: Digital sensing faces challenges in developing sustainable methods to extend the applicability of customized e-noses to complex body odor volatilome (BOV). To address this challenge, we developed MORE-ML, a computational framework that integrates quantum-mechanical (QM) property data of e-nose molecular building blocks with machine learning (ML) methods to predict sensing-relevant properties. Within this framework, we expanded our previous dataset, MORE-Q, to MORE-QX by sampling a larger conformational space of interactions between BOV molecules and mucin-derived receptors. This dataset provides extensive electronic binding features (BFs) computed upon BOV adsorption. Analysis of MORE-QX property space revealed weak correlations between QM properties of building blocks and resulting BFs. Leveraging this observation, we defined electronic descriptors of building blocks as inputs for tree-based ML models to predict BFs. Benchmarking showed CatBoost models outperform alternatives, especially in transferability to unseen compounds. Explainable AI methods further highlighted which QM properties most influence BF predictions. Collectively, MORE-ML combines QM insights with ML to provide mechanistic understanding and rational design principles for molecular receptors in BOV sensing. This approach establishes a foundation for advancing artificial sensing materials capable of analyzing complex odor mixtures, bridging the gap between molecular-level computations and practical e-nose applications.

</details>


### [248] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

Relevance: 15.0

TL;DR: 提出GCMI（生成条件缺失插补网络）方法，通过生成条件策略处理数据集缺失值，结合多重插补链式方程增强鲁棒性和准确性，在MCAR和MAR机制下表现优异。


<details>
  <summary>Details</summary>
Motivation: 数据集中的缺失值是统计分析中的重要问题，现有方法在处理缺失值方面存在局限性。作者旨在开发一种更先进、更鲁棒的缺失值插补方法，特别是在MCAR（完全随机缺失）和MAR（随机缺失）机制下。

Method: 1. 提出GCMI（生成条件缺失插补网络）框架，基于生成条件策略进行缺失值插补；2. 将GCMI与多重插补框架结合，采用链式方程方法增强模型稳定性和插补性能；3. 在MCAR和MAR缺失机制下验证理论性质。

Result: 通过模拟实验和基准数据集实证评估，GCMI方法在缺失值插补方面优于当前主流技术，表现出更高的准确性和鲁棒性。

Conclusion: GCMI是一种实用且前沿的统计数据分析工具，在缺失值处理方面具有显著优势，特别是在MCAR和MAR机制下。

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>


### [249] [Cost Optimization in Production Line Using Genetic Algorithm](https://arxiv.org/abs/2601.00689)
*Alireza Rezaee*

Main category: cs.NE

Relevance: 15.0

TL;DR: 该论文提出了一种基于遗传算法的生产流水线成本最优任务调度方法，通过两种染色体编码策略（基于工位和基于任务）来解决具有复杂约束的组合调度问题。


<details>
  <summary>Details</summary>
Motivation: 生产流水线中的任务调度是一个具有复杂约束（任务持续时间、执行成本、优先级约束）的组合优化问题。传统基于梯度或解析的方法难以处理非可微成本函数和复杂约束，因此需要探索遗传算法等启发式方法。

Method: 1. 使用遗传算法框架，研究两种染色体编码策略：基于工位的表示（使用JGAP库和SuperGene有效性检查）和基于任务的表示（基因直接编码工位分配）
2. 为每种编码适配标准遗传算子（交叉、变异、选择和替换）以保持可行性
3. 在三种优先级结构（紧密耦合、松散耦合、非耦合）上进行实验验证

Result: 实验结果表明：基于任务的编码相比基于工位的编码具有更平滑的收敛性和更可靠的成本最小化效果，特别是在有效调度数量较大时。遗传算法在复杂约束和非可微成本函数情况下优于基于梯度和解析方法。

Conclusion: 遗传算法特别适用于具有复杂约束和不可微成本函数的组合调度问题。基于任务的染色体编码策略在收敛性和解决方案质量方面表现更优，为生产流水线调度提供了有效的启发式方法。

Abstract: This paper presents a genetic algorithm (GA) approach to cost-optimal task scheduling in a production line. The system consists of a set of serial processing tasks, each with a given duration, unit execution cost, and precedence constraints, which must be assigned to an unlimited number of stations subject to a per-station duration bound. The objective is to minimize the total production cost, modeled as a station-wise function of task costs and the duration bound, while strictly satisfying all prerequisite and capacity constraints. Two chromosome encoding strategies are investigated: a station-based representation implemented using the JGAP library with SuperGene validity checks, and a task-based representation in which genes encode station assignments directly. For each encoding, standard GA operators (crossover, mutation, selection, and replacement) are adapted to preserve feasibility and drive the population toward lower-cost schedules. Experimental results on three classes of precedence structures-tightly coupled, loosely coupled, and uncoupled-demonstrate that the task-based encoding yields smoother convergence and more reliable cost minimization than the station-based encoding, particularly when the number of valid schedules is large. The study highlights the advantages of GA over gradient-based and analytical methods for combinatorial scheduling problems, especially in the presence of complex constraints and non-differentiable cost landscapes.

</details>


### [250] [Group Cross-Correlations with Faintly Constrained Filters](https://arxiv.org/abs/2601.00045)
*Benedikt Fluhr*

Main category: math.DS

Relevance: 5.0

TL;DR: 本文提出了一种更宽松的群互相关概念，解决了先前文献中群作用具有非紧致稳定子群时的约束不兼容问题，并将结果推广到非传递群作用，同时弱化了幺模性假设。


<details>
  <summary>Details</summary>
Motivation: 先前关于群互相关的研究存在两个主要限制：1) 对于具有非紧致稳定子群的群作用，现有约束条件不兼容；2) 结果主要局限于传递群作用且通常要求幺模性。这些限制阻碍了理论在更广泛群作用场景中的应用。

Method: 通过放宽群互相关中滤波器的约束条件，使其不再像先前文献那样严格受限。这种方法允许处理具有非紧致稳定子群的群作用，并将理论框架扩展到非传递群作用，同时弱化幺模性要求。

Result: 成功解决了群作用具有非紧致稳定子群时的约束不兼容问题，建立了更一般的群互相关理论框架，能够处理非传递群作用，并且不依赖于幺模性假设。

Conclusion: 本文提出的广义群互相关概念克服了先前理论的局限性，为分析更广泛类型的群作用提供了理论基础，扩展了群表示理论和相关数学工具的应用范围。

Abstract: We provide a notion of group cross-correlations, where the associated filter is not as tightly constrained as in the previous literature. This resolves an incompatibility previous constraints have for group actions with non-compact stabilizers. Moreover, we generalize previous results to group actions that are not necessarily transitive, and we weaken the common assumption of unimodularity.

</details>
