<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
*Debdeep Sanyal,Manodeep Ray,Murari Mandal*

Main category: cs.CL

Relevance: 85.0

TL;DR: AntiDote是一种双层优化方法，通过对抗性超网络训练LLM抵抗恶意微调攻击，在保持模型能力的同时显著提升安全性


<details>
  <summary>Details</summary>
Motivation: 解决开源权重LLM面临的安全风险，防止恶意攻击者通过全参数微调绕过现有安全措施，同时保持模型通用能力

Method: 采用双层优化：1) 辅助对抗性超网络学习生成恶意LoRA权重 2) 防御模型训练目标抵消对抗权重影响，保持安全对齐

Result: 在52种红队攻击测试中，AntiDote比基准方法强27.4%，在MMLU等能力基准上性能下降小于0.5%

Conclusion: AntiDote为构建安全且实用的开源权重模型提供了计算高效的方法，使安全性成为更内在和弹性的属性

Abstract: The release of open-weight large language models (LLMs) creates a tension
between advancing accessible research and preventing misuse, such as malicious
fine-tuning to elicit harmful content. Current safety measures struggle to
preserve the general capabilities of the LLM while resisting a determined
adversary with full access to the model's weights and architecture, who can use
full-parameter fine-tuning to erase existing safeguards. To address this, we
introduce AntiDote, a bi-level optimization procedure for training LLMs to be
resistant to such tampering. AntiDote involves an auxiliary adversary
hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)
weights conditioned on the defender model's internal activations. The defender
LLM is then trained with an objective to nullify the effect of these
adversarial weight additions, forcing it to maintain its safety alignment. We
validate this approach against a diverse suite of 52 red-teaming attacks,
including jailbreak prompting, latent space manipulation, and direct
weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial
attacks compared to both tamper-resistance and unlearning baselines. Crucially,
this robustness is achieved with a minimal trade-off in utility, incurring a
performance degradation of upto less than 0.5\% across capability benchmarks
including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute
efficient methodology for building open-weight models where safety is a more
integral and resilient property.

</details>


### [2] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

Relevance: 85.0

TL;DR: MVPBench是一个包含24,020个实例的基准测试，系统评估LLM在75个国家中的多维人类价值对齐表现，揭示了跨地域和人口统计线的显著差异，并证明轻量级微调方法可显著提升价值对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往忽视文化和人口多样性，导致对价值对齐在全球范围内泛化能力的理解有限，需要更全面的评估资源来支持全球对齐研究。

Method: 构建MVPBench基准测试，包含精细价值标签、个性化问题和丰富人口统计元数据；使用LoRA和DPO等轻量级微调方法进行实验验证。

Result: 发现SOTA LLM在地理和人口统计线上存在显著对齐性能差异；轻量级微调方法在域内和域外设置中都能显著提升价值对齐效果。

Conclusion: 需要进行人口感知的对齐评估，MVPBench为全球对齐、个性化价值建模和公平AI发展提供了实用基础。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [3] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文介绍了NOWJ团队在COLIEE 2025竞赛中五个任务的方法与结果，重点在法律案例蕴含任务(Task 2)中取得第一名，F1分数0.3195。团队结合了传统检索技术(BM25、BERT等)与先进大语言模型(Qwen-2、DeepSeek-V3等)的混合方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何将传统信息检索技术与现代生成式大语言模型相结合，提升法律信息处理的性能，特别是在法律案例蕴含任务中的表现。

Method: 两阶段检索系统：第一阶段使用词汇语义过滤(BM25、BERT、monoT5)，第二阶段使用上下文LLM分析。整合了预排序模型、语义表示(BGE-m3、LLM2Vec)和先进LLM(Qwen-2、QwQ-32B、DeepSeek-V3)进行摘要、相关性评分和上下文重排序。

Result: 在法律案例蕴含任务(Task 2)中获得第一名，F1分数0.3195。在其他四个任务(法律案例检索、法规检索、法律文本蕴含、法律判决预测)中也表现出稳健性能。

Conclusion: 混合模型结合传统IR技术和现代生成模型具有巨大潜力，为法律信息处理的未来发展提供了有价值的参考。

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [4] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
*Fengyu She,Nan Wang,Hongfei Wu,Ziyi Wan,Jingmian Wang,Chang Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: SciGPT是基于Qwen3架构的科学领域专用大语言模型，通过领域蒸馏、稀疏专家混合注意力机制和知识感知适应来提升科学文献理解能力，在ScienceBench基准测试中超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 解决通用大语言模型在科学领域处理技术术语、方法严谨性和复杂科学任务时的局限性，为跨学科研究提供更有效的AI工具。

Method: 采用三阶段方法：1）低成本领域蒸馏平衡性能与效率；2）稀疏专家混合注意力机制减少长文档推理内存消耗55%；3）知识感知适应整合领域本体以弥合跨学科知识鸿沟。

Result: 在ScienceBench基准测试中，SciGPT在序列标注、生成和推理等核心科学任务上超越GPT-4o，并在未见科学任务中表现出强大鲁棒性。

Conclusion: SciGPT证明了领域适应方法在科学LLM中的有效性，具有促进AI辅助科学发现的潜力。

Abstract: Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.

</details>


### [5] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
*Flor Miriam Plaza-del-Arco,Paul Röttger,Nino Scherrer,Emanuele Borgonovo,Elmar Plischke,Dirk Hovy*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文研究LLM个性化中的人物提示对错误拒绝率的影响，发现模型能力和任务类型比人物特征对错误拒绝的影响更大


<details>
  <summary>Details</summary>
Motivation: 量化LLM个性化中人物提示导致的错误拒绝问题，探究不同社会人口统计特征人物对模型拒绝行为的影响

Method: 使用15种社会人口统计人物特征、16个不同模型、3个任务类型和9种提示改写，提出基于蒙特卡洛的方法进行高效量化分析

Result: 模型能力越强，人物特征对拒绝率的影响越小；某些社会人口统计特征在某些模型中会增加错误拒绝率；模型选择和任务类型对错误拒绝有显著影响

Conclusion: 人物效应可能被高估，错误拒绝更多受模型能力和任务敏感性影响，而非人物特征本身

Abstract: Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

</details>


### [6] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

Relevance: 15.0

TL;DR: 该论文提出了针对埃塞俄比亚南部Wolaita和Gofa语言的双语识别(BLID)方法，结合BERT预训练模型和LSTM取得了0.72的F1分数


<details>
  <summary>Details</summary>
Motivation: 解决多语言社区中文本包含多种语言时的语言识别问题，特别是在Wolaita和Gofa这两种相似语言之间的区分挑战

Method: 采用多种实验方法，最终确定BERT预训练语言模型与LSTM的组合方法

Result: 在测试集上获得了0.72的F1分数，表现最佳

Conclusion: 该方法能有效处理社交媒体中的语言识别问题，并为该领域进一步研究奠定基础

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)
*Hyungjin Chung,Hyelin Nam,Jiyeon Kim,Hyojun Go,Byeongjun Park,Junho Kim,Joonseok Lee,Seongsu Ha,Byung-Hoon Kim*

Main category: cs.CV

Relevance: 75.0

TL;DR: Video Parallel Scaling (VPS) 是一种推理时方法，通过并行处理视频帧子集并聚合输出概率，在不增加上下文窗口的情况下提升VideoLLMs的时序推理能力


<details>
  <summary>Details</summary>
Motivation: VideoLLMs在处理长视频时面临计算成本过高和长上下文性能下降的问题，需要一种高效的方法来提升时序细节捕捉能力

Method: VPS运行多个并行推理流，每个流处理视频帧的不同子集，然后聚合这些互补流的输出概率来整合更丰富的视觉信息

Result: 在Video-MME和EventHallusion等基准测试中，VPS在不同模型架构和规模(2B-32B)上均显著提升性能，比Self-consistency等其他并行方法扩展性更好

Conclusion: VPS提供了一种内存高效且鲁棒的框架，通过有效利用不相关的视觉证据来增强VideoLLMs的时序推理能力，且与其他解码策略互补

Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck:
increasing the number of input frames to capture fine-grained temporal detail
leads to prohibitive computational costs and performance degradation from long
context lengths. We introduce Video Parallel Scaling (VPS), an inference-time
method that expands a model's perceptual bandwidth without increasing its
context window. VPS operates by running multiple parallel inference streams,
each processing a unique, disjoint subset of the video's frames. By aggregating
the output probabilities from these complementary streams, VPS integrates a
richer set of visual information than is possible with a single pass. We
theoretically show that this approach effectively contracts the Chinchilla
scaling law by leveraging uncorrelated visual evidence, thereby improving
performance without additional training. Extensive experiments across various
model architectures and scales (2B-32B) on benchmarks such as Video-MME and
EventHallusion demonstrate that VPS consistently and significantly improves
performance. It scales more favorably than other parallel alternatives (e.g.
Self-consistency) and is complementary to other decoding strategies, offering a
memory-efficient and robust framework for enhancing the temporal reasoning
capabilities of VideoLLMs.

</details>


### [8] [3D and 4D World Modeling: A Survey](https://arxiv.org/abs/2509.07996)
*Lingdong Kong,Wesley Yang,Jianbiao Mei,Youquan Liu,Ao Liang,Dekai Zhu,Dongyue Lu,Wei Yin,Xiaotao Hu,Mingkai Jia,Junyuan Deng,Kaiwen Zhang,Yang Wu,Tianyi Yan,Shenyuan Gao,Song Wang,Linfeng Li,Liang Pan,Yong Liu,Jianke Zhu,Wei Tsang Ooi,Steven C. H. Hoi,Ziwei Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 这是一篇关于3D和4D世界建模的综述论文，首次系统性地整理了该领域的研究现状，提出了明确的定义和分类体系，涵盖了视频、占据网格和LiDAR三种主要方法。


<details>
  <summary>Details</summary>
Motivation: 当前世界建模研究主要集中在2D图像和视频生成，忽视了快速发展的3D/4D表示方法（如RGB-D、占据网格、LiDAR点云）。同时缺乏标准化的定义和分类体系，导致文献中的研究结果分散且不一致。

Method: 通过建立精确的定义体系，提出结构化分类法（VideoGen视频生成、OccGen占据网格生成、LiDARGen激光雷达生成），系统总结3D/4D场景下的数据集和评估指标。

Result: 创建了首个专门针对3D和4D世界建模与生成的全面综述，提供了该领域的系统性参考框架，包括分类体系、数据集和评估标准。

Conclusion: 该综述为3D/4D世界建模领域提供了基础性参考，有助于推动该领域的标准化发展，并指出了未来的研究方向和应用前景。

Abstract: World modeling has become a cornerstone in AI research, enabling agents to
understand, represent, and predict the dynamic environments they inhabit. While
prior work largely emphasizes generative methods for 2D image and video data,
they overlook the rapidly growing body of work that leverages native 3D and 4D
representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds
for large-scale scene modeling. At the same time, the absence of a standardized
definition and taxonomy for ``world models'' has led to fragmented and
sometimes inconsistent claims in the literature. This survey addresses these
gaps by presenting the first comprehensive review explicitly dedicated to 3D
and 4D world modeling and generation. We establish precise definitions,
introduce a structured taxonomy spanning video-based (VideoGen),
occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and
systematically summarize datasets and evaluation metrics tailored to 3D/4D
settings. We further discuss practical applications, identify open challenges,
and highlight promising research directions, aiming to provide a coherent and
foundational reference for advancing the field. A systematic summary of
existing literature is available at https://github.com/worldbench/survey

</details>


### [9] [An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities](https://arxiv.org/abs/2509.08003)
*Shahid Shafi Dar,Bharat Kaurav,Arnav Jain,Chandravardhan Singh Raghaw,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

Relevance: 25.0

TL;DR: XFloodNet是一个用于城市洪水分类的新型深度学习框架，通过层次化跨模态门控注意力、异构卷积多尺度注意力和级联卷积Transformer特征精炼技术，在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统洪水检测方法依赖单模态数据和静态规则系统，无法捕捉洪水事件的动态非线性关系。现有注意力机制和集成学习方法在层次精炼、跨模态特征集成和对噪声环境的适应性方面存在局限，导致洪水分类性能不佳。

Method: 提出了XFloodNet框架，包含三个核心组件：1）层次化跨模态门控注意力机制，动态对齐视觉和文本特征；2）异构卷积自适应多尺度注意力模块，利用频率增强的通道注意力和频率调制的空间注意力；3）级联卷积Transformer特征精炼技术，通过自适应缩放和级联操作协调层次特征。

Result: 在Chennai Floods、Rhine18 Floods和Harz17 Floods三个基准数据集上分别达到了93.33%、82.24%和88.60%的F1分数，显著超越了现有方法。

Conclusion: XFloodNet通过先进的深度学习技术重新定义了城市洪水分类，解决了传统方法的局限性，在洪水检测方面表现出卓越的性能和鲁棒性。

Abstract: In an era of escalating climate change, urban flooding has emerged as a
critical challenge for sustainable cities, threatening lives, infrastructure,
and ecosystems. Traditional flood detection methods are constrained by their
reliance on unimodal data and static rule-based systems, which fail to capture
the dynamic, non-linear relationships inherent in flood events. Furthermore,
existing attention mechanisms and ensemble learning approaches exhibit
limitations in hierarchical refinement, cross-modal feature integration, and
adaptability to noisy or unstructured environments, resulting in suboptimal
flood classification performance. To address these challenges, we present
XFloodNet, a novel framework that redefines urban flood classification through
advanced deep-learning techniques. XFloodNet integrates three novel components:
(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically
aligns visual and textual features, enabling precise multi-granularity
interactions and resolving contextual ambiguities; (2) a Heterogeneous
Convolutional Adaptive Multi-Scale Attention module, which leverages
frequency-enhanced channel attention and frequency-modulated spatial attention
to extract and prioritize discriminative flood-related features across spectral
and spatial domains; and (3) a Cascading Convolutional Transformer Feature
Refinement technique that harmonizes hierarchical features through adaptive
scaling and cascading operations, ensuring robust and noise-resistant flood
detection. We evaluate our proposed method on three benchmark datasets, such as
Chennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves
state-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,
surpassing existing methods by significant margins.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)
*Linyao Chen,Zimian Peng,Yingxuan Yang,Yikun Wang,Wenzheng Tom Tang,Hiroki H. Kobayashi,Weinan Zhang*

Main category: cs.AI

Relevance: 65.0

TL;DR: EnvX是一个利用Agentic AI将GitHub仓库转化为智能代理的框架，通过自然语言交互和代理间协作实现软件组件的自动化重用。


<details>
  <summary>Details</summary>
Motivation: 解决开源软件组件重用过程中手动操作、易出错和割裂的问题，降低开发者理解API和编写集成代码的门槛。

Method: 采用三阶段方法：1) TODO引导的环境初始化；2) 人类对齐的代理自动化；3) 代理间(A2A)协作协议，结合大语言模型能力和结构化工具集成。

Result: 在GitTaskBench基准测试中，EnvX达到74.07%的执行完成率和51.85%的任务通过率，优于现有框架，并能实现多仓库协作。

Conclusion: EnvX将仓库从被动代码资源转变为智能交互代理，提升了开源生态系统的可访问性和协作性。

Abstract: The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

</details>


### [11] [Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI](https://arxiv.org/abs/2509.08151)
*Botao Zhu,Jeslyn Wang,Dusit Niyato,Xianbin Wang*

Main category: cs.AI

Relevance: 45.0

TL;DR: 基于大AI模型的师生代理架构，通过任务特定信任语义蒸馏(2TSD)模型，实现高效准确的协作设备信任评估和选择


<details>
  <summary>Details</summary>
Motivation: 解决传统协作设备信任评估中数据交换频繁、推理复杂、动态变化带来的开销大和评估质量下降问题

Method: 采用LAM驱动的师生代理架构，教师代理负责多维信任数据收集和任务特定信任语义提取，学生代理接收语义进行快速协作选择

Result: 实验表明2TSD模型能减少评估时间、降低设备资源消耗，并提高协作选择准确性

Conclusion: 提出的2TSD模型有效解决了分布式协作环境中的信任评估挑战

Abstract: Accurate trustworthiness evaluation of potential collaborating devices is
essential for the effective execution of complex computing tasks. This
evaluation process involves collecting diverse trust-related data from
potential collaborators, including historical performance and available
resources, for collaborator selection. However, when each task owner
independently assesses all collaborators' trustworthiness, frequent data
exchange, complex reasoning, and dynamic situation changes can result in
significant overhead and deteriorated trust evaluation. To overcome these
challenges, we propose a task-specific trust semantics distillation (2TSD)
model based on a large AI model (LAM)-driven teacher-student agent
architecture. The teacher agent is deployed on a server with powerful
computational capabilities and an augmented memory module dedicated to
multidimensional trust-related data collection, task-specific trust semantics
extraction, and task-collaborator matching analysis. Upon receiving
task-specific requests from device-side student agents, the teacher agent
transfers the trust semantics of potential collaborators to the student agents,
enabling rapid and accurate collaborator selection. Experimental results
demonstrate that the proposed 2TSD model can reduce collaborator evaluation
time, decrease device resource consumption, and improve the accuracy of
collaborator selection.

</details>


### [12] [Learning-Based Planning for Improving Science Return of Earth Observation Satellites](https://arxiv.org/abs/2509.07997)
*Abigail Breitfeld,Alberto Candela,Juan Delfa,Akseli Kangaslahti,Itai Zilberstein,Steve Chien,David Wettergreen*

Main category: cs.AI

Relevance: 35.0

TL;DR: 该论文提出了两种基于学习的方法（强化学习和模仿学习）来解决卫星动态目标选择问题，相比现有启发式方法，模仿学习性能提升10.0%，强化学习提升13.7%，且都能用少量数据有效训练。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星在数据收集方面存在局限性，包括轨道固定、传感器视野有限和操作资源消耗大。需要优化数据收集策略，只采集最重要的信息。动态目标选择是一种新兴概念，利用卫星资源和前瞻仪器数据智能调整主仪器配置。

Method: 提出了两种学习方法：1）强化学习方法；2）模仿学习方法。这些方法基于动态规划解决方案来规划采样位置序列，并与现有的启发式动态目标选择方法进行比较。

Result: 模仿学习比最佳启发式方法平均性能提升10.0%，强化学习平均提升13.7%。两种学习方法都能用相对较少的数据有效训练。

Conclusion: 学习型方法在卫星动态目标选择应用中表现出显著优势，特别是强化学习方法效果最佳，且数据效率高，适合实际卫星应用场景。

Abstract: Earth observing satellites are powerful tools for collecting scientific
information about our planet, however they have limitations: they cannot easily
deviate from their orbital trajectories, their sensors have a limited field of
view, and pointing and operating these sensors can take a large amount of the
spacecraft's resources. It is important for these satellites to optimize the
data they collect and include only the most important or informative
measurements. Dynamic targeting is an emerging concept in which satellite
resources and data from a lookahead instrument are used to intelligently
reconfigure and point a primary instrument. Simulation studies have shown that
dynamic targeting increases the amount of scientific information gathered
versus conventional sampling strategies. In this work, we present two different
learning-based approaches to dynamic targeting, using reinforcement and
imitation learning, respectively. These learning methods build on a dynamic
programming solution to plan a sequence of sampling locations. We evaluate our
approaches against existing heuristic methods for dynamic targeting, showing
the benefits of using learning for this application. Imitation learning
performs on average 10.0\% better than the best heuristic method, while
reinforcement learning performs on average 13.7\% better. We also show that
both learning methods can be trained effectively with relatively small amounts
of data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [How Far Are We from True Unlearnability?](https://arxiv.org/abs/2509.08058)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文研究发现现有不可学习样本方法在多任务场景下失效，从模型优化角度分析了损失景观与不可学习性的关系，提出了基于参数锐度感知可学习性(SAL)的不可学习距离(UD)来衡量数据不可学习性，并对主流方法进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 高质量数据在大模型时代至关重要，但未经授权的数据使用损害数据所有者利益。现有不可学习方法生成的不可学习样本在多任务数据集上仍然有效，未能展现跨任务不可学习性，这促使研究者探索真正不可学习样本的实现距离。

Method: 1) 观察干净模型和中毒模型在收敛过程中的差异；2) 从损失景观角度分析关键参数优化路径的差异；3) 提出锐度感知可学习性(SAL)来量化参数的不可学习性；4) 基于SAL分布提出不可学习距离(UD)来衡量数据的不可学习性；5) 对主流不可学习方法进行基准测试。

Result: 研究发现现有不可学习方法在多任务场景下存在局限性，损失景观与不可学习性密切相关，提出的UD指标能够有效衡量数据的不可学习性，揭示了现有不可学习方法的能力边界。

Conclusion: 当前距离实现真正不可学习的样本还有差距，损失景观为理解不可学习性提供了新视角，SAL和UD指标为评估和改进不可学习方法提供了有效工具，需要社区进一步关注现有方法的能力边界。

Abstract: High-quality data plays an indispensable role in the era of large models, but
the use of unauthorized data for model training greatly damages the interests
of data owners. To overcome this threat, several unlearnable methods have been
proposed, which generate unlearnable examples (UEs) by compromising the
training availability of data. Clearly, due to unknown training purposes and
the powerful representation learning capabilities of existing models, these
data are expected to be unlearnable for models across multiple tasks, i.e.,
they will not help improve the model's performance. However, unexpectedly, we
find that on the multi-task dataset Taskonomy, UEs still perform well in tasks
such as semantic segmentation, failing to exhibit cross-task unlearnability.
This phenomenon leads us to question: How far are we from attaining truly
unlearnable examples? We attempt to answer this question from the perspective
of model optimization. To this end, we observe the difference in the
convergence process between clean and poisoned models using a simple model
architecture. Subsequently, from the loss landscape we find that only a part of
the critical parameter optimization paths show significant differences,
implying a close relationship between the loss landscape and unlearnability.
Consequently, we employ the loss landscape to explain the underlying reasons
for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the
unlearnability of parameters based on this explanation. Furthermore, we propose
an Unlearnable Distance (UD) to measure the unlearnability of data based on the
SAL distribution of parameters in clean and poisoned models. Finally, we
conduct benchmark tests on mainstream unlearnable methods using the proposed
UD, aiming to promote community awareness of the capability boundaries of
existing unlearnable methods.

</details>


### [14] [Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization](https://arxiv.org/abs/2509.07993)
*Federico Fontana,Anxhelo Diko,Romeo Lanzino,Marco Raoul Marini,Bachir Kaddar,Gian Luca Foresti,Luigi Cinque*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文将深度伪造检测重新定义为持续学习问题，提出了一个高效框架来增量适应新兴视觉操纵技术，同时保留对过去生成器的知识。


<details>
  <summary>Details</summary>
Motivation: 深度伪造生成技术的快速发展给检测系统带来严峻挑战，传统非持续学习方法需要频繁且昂贵的重新训练。

Method: 提出基于轻量级视觉骨干网络的高效持续学习框架，模拟7年间深度伪造技术的真实时间演化，并引入两个新评估指标C-AUC和FWT-AUC。

Result: 实现了高效适应（比完全重新训练快155倍）和稳健的历史知识保留，但对未来生成器的泛化能力接近随机水平（FWT-AUC≈0.5）。

Conclusion: 提出了非通用深度伪造分布假设，指出当前方法在没有额外训练的情况下对未来生成器的泛化能力有限。

Abstract: The rapid evolution of deepfake generation technologies poses critical
challenges for detection systems, as non-continual learning methods demand
frequent and expensive retraining. We reframe deepfake detection (DFD) as a
Continual Learning (CL) problem, proposing an efficient framework that
incrementally adapts to emerging visual manipulation techniques while retaining
knowledge of past generators. Our framework, unlike prior approaches that rely
on unreal simulation sequences, simulates the real-world chronological
evolution of deepfake technologies in extended periods across 7 years.
Simultaneously, our framework builds upon lightweight visual backbones to allow
for the real-time performance of DFD systems. Additionally, we contribute two
novel metrics: Continual AUC (C-AUC) for historical performance and Forward
Transfer AUC (FWT-AUC) for future generalization. Through extensive
experimentation (over 600 simulations), we empirically demonstrate that while
efficient adaptation (+155 times faster than full retraining) and robust
retention of historical knowledge is possible, the generalization of current
approaches to future generators without additional training remains near-random
(FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing
generator. Such observations are the foundation of our newly proposed
Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}

</details>


### [15] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

Relevance: 20.0

TL;DR: JEL是一个新颖的计算高效端到端多神经网络实体链接模型，在实体链接任务上超越了当前最先进模型


<details>
  <summary>Details</summary>
Motivation: 知识图谱需要将文本中的提及正确链接到实体，这是自然语言处理中的基础任务，对新闻分析等应用至关重要。摩根大通每年花费超过200万美元用于外部供应商成本，实体链接能够桥接非结构化新闻文本与知识图谱

Method: 基于多神经网络的端到端实体链接模型，计算高效

Result: 超越了当前最先进的实体链接模型

Conclusion: JEL模型在实体链接任务上表现出色，能够有效支持新闻分析等实际应用

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>
