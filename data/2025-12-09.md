<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.CV](#cs.CV) [Total: 237]
- [cs.AI](#cs.AI) [Total: 114]
- [cs.LG](#cs.LG) [Total: 199]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了一个基于直接偏好优化（DPO）的框架，用于改善大型语言模型在医疗护理对话中的事实准确性、语义连贯性和共情能力，解决了现有LLM在医疗应用中的事实不可靠和缺乏共情沟通的问题。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在医疗和护理应用中存在两个关键缺陷：事实不可靠性和缺乏共情沟通能力。这些缺陷在敏感场景中带来显著风险，特别是当非专业人士和护理人员寻求医疗指导或情感支持时。

Method: 采用直接偏好优化（DPO）对齐框架，使用成对偏好数据对领域适应的LLM进行微调。偏好响应体现支持性和易理解的沟通风格，而被拒绝的响应则代表指令性或过于技术化的语气。这种方法比传统的基于强化学习的对齐更高效。

Result: 在多个开源和专有LLM上的实证评估显示，DPO调优的模型相比基线和商业替代方案（如谷歌医疗对话系统）实现了更高的语义对齐、改善的事实准确性和更强的人类中心评估分数。

Conclusion: 基于偏好的对齐为开发可信赖、有共情能力且具备临床知识的AI助手提供了一条可扩展且透明的途径，适用于护理人员和医疗沟通场景。

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [2] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

Relevance: 85.0

TL;DR: GAUGE是一个轻量级的logit-based框架，用于实时检测LLM对话中的隐性情感升级，传统毒性过滤器无法检测这种渐进式的情感伤害。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地作为情感伴侣使用，即使没有明显毒性，重复的情感强化或情感漂移也会逐渐升级为隐性伤害，现有基于外部分类器或临床标准的防护机制无法实时检测这种细微的对话动态变化。

Method: GAUGE是一个轻量级的logit-based框架，通过测量LLM输出如何概率性地改变对话的情感状态来检测隐藏的对话升级，不需要外部分类器。

Result: 论文提出了GAUGE框架，能够实时检测LLM对话中的隐性情感升级，填补了传统毒性过滤器无法检测渐进式情感伤害的空白。

Conclusion: GAUGE为LLM作为情感伴侣的安全使用提供了新的防护机制，能够实时检测传统方法无法发现的隐性情感伤害。

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [3] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一种利用LLM-as-a-Judge自动构建策略对齐训练数据的方法，无需人工标注或平行语料，可构建适应不同简化策略的简化系统，小规模开源LLM在词汇级简化上超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 句子简化旨在使句子更易读易懂同时保持原意，但不同应用需要不同的简化策略（如仅替换复杂词汇或整体重写）。目前实现这种策略驱动的控制仍是一个开放挑战，且现有方法通常需要昂贵的人工标注或平行语料

Method: 采用LLM-as-a-Judge方法自动构建策略对齐的训练数据：1）使用LLM作为评判者评估简化质量；2）自动生成符合特定简化策略的训练样本；3）完全消除对人工标注或平行语料的需求；4）可适应多种简化策略

Result: 小规模开源LLM（如Phi-3-mini-3.8B）在词汇导向的简化任务上超越GPT-4o，在整体重写任务上达到可比性能。自动指标和人工评估均验证了改进，且在不同模型家族和规模上表现一致，证明了方法的鲁棒性

Conclusion: 提出的LLM-as-a-Judge方法为策略驱动的句子简化提供了有效解决方案，无需昂贵的人工标注，使小规模模型在特定任务上超越大型商业模型，展示了自动数据构建方法的潜力和通用性

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [4] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

Relevance: 85.0

TL;DR: LOCUS是一个低成本的NLP模型定制化流程，通过少量标注数据、检索相关数据、合成训练样本和参数高效微调，在NER和文本分类任务上超越GPT-4o等基线，同时大幅降低成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 当前大模型定制化需要大量标注数据和计算资源，成本高昂。本文旨在开发一个低成本的通用专业化流程，仅需少量标注数据就能构建高性能的NLP模型，同时大幅降低内存占用和计算成本。

Method: LOCUS采用三阶段流程：1) 从大型知识库中检索相关数据；2) 通过上下文数据生成合成额外训练样本；3) 使用全参数微调或LoRA等参数高效方法进行模型微调。主要针对NER和文本分类任务。

Result: 在多个基准测试中超越GPT-4o等强基线，内存优化模型仅需5%的内存占用就能达到全参数微调99%的准确率，参数量不到GPT-4o的1%却在多个基准上表现更优。

Conclusion: LOCUS证明了通过少量标注数据、智能检索和合成数据生成，可以构建高效、低成本的专用NLP模型，为模型定制化提供了经济高效的解决方案。

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [5] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: Nanbeige4-3B是一个3B参数的小规模高性能语言模型，通过创新的预训练调度器、SFT数据质量提升、双偏好蒸馏和多阶段强化学习，在小模型规模下实现了超越同规模模型并媲美更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探索小规模语言模型的性能边界，通过系统化的训练方法创新来突破小模型的缩放定律限制，实现高性能的小规模模型。

Method: 1. 预训练：使用FG-WSD调度器（细粒度热身-稳定-衰减）分阶段优化数据混合；2. SFT：结合审议生成优化和思维链重构提升数据质量；3. 蒸馏：使用双偏好蒸馏（DPD）从旗舰推理模型蒸馏；4. 强化学习：多阶段RL使用可验证奖励和偏好建模。

Result: 在广泛基准测试中，Nanbeige4-3B不仅显著优于同参数规模模型，还能与更大规模模型竞争，展示了小模型的高性能潜力。

Conclusion: 通过系统化的训练方法创新，可以显著提升小规模语言模型的性能，突破传统的缩放定律限制，为高效的小模型部署提供了可行方案。

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [6] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

Relevance: 85.0

TL;DR: ProSocialAlign：一个测试时、参数高效的框架，通过词典约束生成和方向调节机制，在不重新训练基础模型的情况下，引导LLM生成安全、共情、价值对齐的响应。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型安全范式在情感激烈或高风险场景中存在不足：仅拒绝可能疏远用户，而简单顺从可能放大风险。需要一种能在保持安全的同时提供共情、价值对齐响应的解决方案。

Method: 1) 形式化五个以人为中心的目标，将安全建模为词典约束生成：先应用硬约束消除有害延续，然后在安全集合内优化亲社会质量；2) 方向调节：在参数空间减去学习的"伤害向量"以缓解伤害；3) 偏好感知自回归奖励建模：联合训练多个属性并解决梯度冲突，实现细粒度、用户可控的解码。

Result: 在五个安全基准测试中达到最先进性能，显著减少不安全泄漏，提升与人类价值观的对齐度，在多个评估指标上均有强劲提升。

Conclusion: ProSocialAlign为在推理时生成上下文敏感、安全、人类对齐的响应提供了稳健且模块化的基础，是LLM安全对齐的重要进展。

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [7] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

Relevance: 85.0

TL;DR: 对GPT-2进行机制可解释性研究，通过系统激活修补实验检验情感处理的两阶段假设，发现早期层是词汇情感检测器，但上下文整合发生在晚期层而非中期层，与预测的层次模式不同。


<details>
  <summary>Details</summary>
Motivation: 研究GPT-2中情感信息处理的机制，检验关于情感计算的两阶段架构假设：早期词汇检测和中期上下文整合。通过因果分析理解大语言模型中情感处理的层次结构。

Method: 使用机制可解释性方法，对GPT-2的所有12层进行系统激活修补实验。测试三种上下文整合假设：中层集中、现象特异性和分布式处理。

Result: 早期层（0-3）确实作为词汇情感检测器，编码稳定的位置特定极性信号。但所有三种上下文整合假设都被证伪，上下文现象（否定、讽刺、领域转移等）主要在晚期层（8-11）通过统一的非模块化机制整合。

Conclusion: GPT-2的情感计算与预测的层次模式不同，上下文整合发生在晚期层而非中期层。这强调需要进一步实证表征大语言模型中的上下文整合机制。

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [8] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

Relevance: 85.0

TL;DR: PersonaMem-v2是用于LLM个性化的先进数据集，包含1000个真实用户-聊天机器人交互，覆盖300+场景和20000+用户偏好。研究显示前沿LLM在隐式个性化任务上准确率仅37-48%，通过强化微调Qwen3-4B超越GPT-5达到53%准确率，而代理记忆框架使用2k令牌记忆而非完整32k对话历史，实现55%准确率且减少16倍输入令牌。


<details>
  <summary>Details</summary>
Motivation: 个性化是AI能力和对齐的下一个里程碑。当前LLM在隐式个性化方面表现不佳，需要更好的数据集和方法来提升模型理解用户偏好和长期交互的能力。

Method: 1) 创建PersonaMem-v2数据集，模拟1000个真实用户-聊天机器人交互，包含300+场景、20000+用户偏好和128k令牌上下文窗口；2) 使用强化微调提升模型的长上下文推理能力；3) 开发代理记忆系统框架，维护可读记忆并随时间增长。

Result: 前沿LLM在隐式个性化任务上准确率仅37-48%。通过强化微调，Qwen3-4B超越GPT-5达到53%准确率。代理记忆框架实现55%准确率，同时使用16倍更少的输入令牌（2k记忆 vs 32k完整对话历史）。

Conclusion: 数据集对个性化研究有重要影响，代理记忆系统是通向现实世界个性化智能的可扩展路径。长上下文支持不是瓶颈，推理能力才是隐式个性化任务的关键限制因素。

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [9] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

Relevance: 85.0

TL;DR: FlyThinker提出了一种"边思考边生成"的高效个性化长文本生成框架，通过并行推理模型生成潜在token级推理，动态指导响应生成，解决了传统方法在长文本生成中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前偏好对齐方法主要优化群体级偏好，忽视个体用户。早期个性化方法（如提示定制或微调）难以推理隐含偏好，而最近的"先思考后生成"方法在长文本生成中面临挑战：静态一次性推理必须捕获完整响应生成的所有相关信息，学习困难且适应性有限。

Method: FlyThinker采用"边思考边生成"框架：1) 使用独立的推理模型并行生成潜在token级推理；2) 将推理结果融合到生成模型中动态指导响应生成；3) 推理模型仅依赖先前响应而非自身输出，保持训练并行性；4) 所有推理token可在单次前向传递中生成，确保训练效率。

Result: 在真实世界基准测试上的广泛实验表明，FlyThinker在保持训练和推理效率的同时，实现了更好的个性化生成效果。

Conclusion: FlyThinker通过"边思考边生成"框架有效解决了个性化长文本生成的挑战，实现了推理与生成的并发执行，在效率和效果上都优于现有方法。

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [10] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一种参数高效的指令微调方法，结合差分隐私噪声分配与梯度裁剪，在保护隐私的同时提升训练效率


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型指令微调中的隐私保护和效率问题，需要在保护用户数据隐私的同时保持模型性能

Method: 冻结主干模型，通过低维投影子空间更新参数，在梯度计算中引入裁剪和自适应噪声分配，结合梯度约束、噪声分配和参数投影的统一框架

Result: 在准确性、隐私预算和参数效率方面优于基线模型，在多样化和不确定数据条件下保持稳定性能

Conclusion: 丰富了差分隐私与参数高效微调的理论整合，为复杂指令环境中的安全训练提供了可行解决方案

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [11] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

Relevance: 85.0

TL;DR: ImplicitBBQ基准扩展了BBQ，通过隐式线索评估LLM偏见，发现GPT-4o在隐式提示下性能显著下降，揭示了现有显式基准无法检测的隐性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM偏见评估基准主要依赖显式线索（直接声明受保护属性如宗教、种族、性别），但现实世界互动常包含通过姓名、文化线索或特征推断的隐性偏见。这种关键疏忽在公平性评估中造成了显著盲点。

Method: 提出ImplicitBBQ基准，扩展Bias Benchmark for QA (BBQ)，在6个类别中引入隐式线索的受保护属性。通过隐式提示评估LLM偏见，与显式基准进行对比分析。

Result: GPT-4o在ImplicitBBQ上表现出显著性能下降，在"性取向"子类别中准确率下降高达7%，其他大多数类别也出现一致下降。这表明当前LLM包含显式基准无法检测的隐性偏见。

Conclusion: ImplicitBBQ为NLP领域的细致公平性评估提供了关键工具，揭示了现有偏见评估方法的局限性，强调需要更全面的评估框架来检测隐性偏见。

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [12] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出LWE框架，让LLM评估器在推理时通过自生成的反馈逐步改进，无需训练数据，通过选择性更新机制在困难样本上学习，提升评估性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-judge评估方法存在两个问题：1) 独立处理每个样本，无法积累经验；2) 使用固定提示词，缺乏样本特定的评估标准。需要让评估器在推理时能够学习和改进。

Method: 提出LWE框架，维护一个演化的元提示，生成样本特定的评估指令并通过自生成反馈进行自我精炼。进一步提出Selective LWE，只在自不一致的样本上更新元提示，聚焦计算资源于困难案例。

Result: 在两个成对比较基准测试中，Selective LWE优于强基线方法，证明评估器可以通过简单的选择性更新在顺序测试中改进，从最困难的案例中学习最多。

Conclusion: 评估器可以在推理时通过自监督方式改进，选择性更新机制既保留了顺序学习的优势又显著提高了成本效益，为LLM评估提供了更高效的方法。

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [13] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: NBDiff提出了一种将自回归语言模型适配到块扩散语言模型的方法，通过渐进式块大小增加、上下文因果注意力掩码和辅助AR损失，实现了高效的知识迁移和并行生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有自回归解码存在顺序生成的吞吐量瓶颈，而从头训练扩散语言模型成本高昂。先前的方法未能解决AR因果性与块扩散双向性之间的根本不匹配问题。

Method: 将AR视为块大小=1的块扩散，设计渐进式适配路径：上下文因果注意力掩码、高效并行适配过程、辅助AR损失以最大化数据利用和保留预训练知识、逐步增加生成块大小。

Result: NBDiff-7B在7B类DLMs中达到最先进性能，在通用知识、数学和代码基准测试中相比强基线有显著提升，继承了长上下文建模和推理能力。

Conclusion: 原则性的AR到块扩散适配是训练DLMs的有效且计算高效替代方案，能够利用成熟AR检查点的知识，实现并行生成优势。

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [14] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: Rhea框架通过角色感知启发式情景注意力解决LLM多轮对话中的累积上下文衰减问题，将对话历史解耦为指令记忆和情景记忆两个独立模块，显著提升多轮对话性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮任务中表现出色，但在多轮对话中性能会逐渐下降。作者将这种现象定义为"累积上下文衰减"——由注意力污染、稀释和漂移导致的上下文完整性渐进性退化。需要解决LLM在多轮对话中的性能衰减问题。

Method: 提出Rhea（角色感知启发式情景注意力）框架，将对话历史解耦为两个功能独立的记忆模块：1）指令记忆（IM）：通过结构优先级机制持久存储高保真全局约束；2）情景记忆（EM）：通过非对称噪声控制和启发式上下文检索动态管理用户-模型交互。推理时应用优先级注意力，选择性整合相关情景信息，同时始终优先考虑全局指令。

Result: 在多个多轮对话基准测试（包括MT-Eval和Long-MT-Bench+）上，Rhea缓解了性能衰减，在10分制上整体准确率提高了1.04分（相对于强基线相对增益16%）。在长时程交互中保持接近完美的指令保真度（IAR > 8.1）。

Conclusion: Rhea为构建更精确、指令一致的对话式LLM提供了一个原则性和有效的框架，通过解耦记忆模块和优先级注意力机制有效解决了多轮对话中的累积上下文衰减问题。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [15] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了使用LLMs模拟用户意见的局限性，发现即使采用链式思维提示和提出的CLAIMSIM方法，LLMs也难以准确模拟不同人口特征用户的多样化观点，主要存在观点固定和无法处理冲突主张的问题。


<details>
  <summary>Details</summary>
Motivation: 随着使用LLMs模拟用户意见的关注度增加，但经过RLHF训练的LLMs倾向于主流观点，这引发了对它们能否代表不同人口和文化背景用户的担忧。研究旨在探索LLMs在跨领域调查问题中模拟人类响应的能力。

Method: 研究采用直接提示和链式思维提示两种方法，并提出了CLAIMSIM方法——通过从LLM参数知识中提取观点作为上下文输入来实现观点多样化。在调查问题回答任务上进行实验验证。

Result: 实验表明，虽然CLAIMSIM能产生更多样化的响应，但两种方法都难以准确模拟用户。分析发现两个关键局限：1）LLMs倾向于在不同人口特征间保持固定观点，生成单一视角主张；2）面对冲突主张时，LLMs难以推理人口特征间的细微差异，限制了针对特定用户画像调整响应的能力。

Conclusion: 当前LLMs在模拟多样化用户观点方面存在显著局限性，特别是在处理人口特征差异和冲突观点时表现不足。这为LLMs在用户意见模拟应用中的可靠性提出了重要警示。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [16] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

Relevance: 85.0

TL;DR: FVA-RAG框架通过引入对抗性检索策略和双重验证机制，将RAG从归纳验证转向演绎证伪，有效减少检索谄媚导致的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 标准RAG系统存在检索谄媚漏洞：当查询基于错误前提或常见误解时，向量检索器倾向于获取符合用户偏见的文档而非客观事实，导致模型"带引用的幻觉"

Method: 提出FVA-RAG框架，采用对抗性检索策略生成"致命查询"来寻找矛盾证据，引入双重验证机制显式权衡草稿答案与"反上下文"

Result: 在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG基线显著提高了对谄媚幻觉的鲁棒性

Conclusion: FVA-RAG通过将检索范式从寻求支持转向寻求反驳，有效作为事实生成的推理时"红队"机制

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [17] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现当前大型语言模型的安全对齐技术在多轮对抗攻击下仍然存在根本性漏洞，模型规模不能预测对抗鲁棒性，而思维模式（思考模式）可显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管在安全对齐方面投入了大量资源，但大型语言模型对复杂多轮对抗攻击的脆弱性仍然缺乏充分表征，且不清楚模型规模或推理模式是否影响鲁棒性。

Method: 使用TEMPEST多轮攻击框架评估了8个供应商的10个前沿模型，针对1000种有害行为生成超过97,000个API查询，通过独立安全分类器进行自动评估。

Result: 结果显示：6个模型的攻击成功率（ASR）达到96%-100%，4个模型表现出有意义的抵抗（ASR 42%-78%）；在相同架构上启用扩展推理可将ASR从97%降至42%。

Conclusion: 当前对齐技术在多轮自适应攻击下仍然存在根本性漏洞，模型规模不能预测对抗鲁棒性，而深思熟虑的推理模式是可行的安全增强方向。

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [18] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: SAGE是一个基于场景的基准测试，通过跨文化核心概念对齐和生成式任务设计，评估LLMs的跨文化理解和推理能力，包含4530个测试项，覆盖9个文化维度和15个现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs跨文化理解能力的基准存在三个关键局限：缺乏情境场景、跨文化概念映射不足、深度文化推理能力有限。需要更全面评估LLMs是否真正具备跨文化理解能力。

Method: 基于文化理论将跨文化能力分为9个维度，筛选210个核心概念，在4大类跨文化情境下构建15个具体现实场景，按照项目设计原则创建4530个测试项，支持数据集持续扩展。

Result: 实验证实SAGE数据集可迁移到其他语言，揭示了模型在多个维度和场景上的弱点，暴露了跨文化推理的系统性局限。LLMs距离真正的细致跨文化理解仍有差距。

Conclusion: SAGE基准填补了现有评估空白，为评估LLMs跨文化理解提供了更全面的框架。虽然已有进展，但LLMs在跨文化理解方面仍需改进。

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [19] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

Relevance: 85.0

TL;DR: Token Filtering：一种轻量级在线结构化剪枝技术，通过联合键值相似度测量token冗余度，在推理时直接跳过冗余注意力计算，无需校准数据，显著降低LLM推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法依赖离线校准数据，存在跨输入泛化不稳定问题。需要一种无需校准数据、在推理时动态决策的稳定剪枝方法。

Method: 1. 基于联合键值相似度测量token冗余度；2. 设计方差感知融合策略，自适应加权不同注意力头的键值相似度；3. 在线决策跳过冗余注意力计算，无额外内存开销。

Result: 在LLaMA-2 (7B/13B)、LLaMA-3 (8B)和Mistral (7B)上验证，Token Filtering优于现有结构化剪枝方法，在常识推理基准上保持准确率，在MMLU等挑战性任务上即使剪枝50%仍保持强性能。

Conclusion: Token Filtering提供了一种无需校准数据、稳定高效的LLM推理加速方案，通过在线token冗余检测显著降低计算成本，同时保持模型性能。

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [20] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

Relevance: 85.0

TL;DR: 通过特征归因方法构建伪忠实自解释，对指令调优模型进行持续学习，可提升LLM自解释的忠实度，且这种提升在不同解释风格、多词设置和未见任务中具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能根据用户指令生成不同风格的自解释，但现有研究发现这些自解释往往缺乏忠实性（不能真实反映模型行为）。如何提升自解释的忠实性以及这种提升是否能在不同解释风格间泛化，是尚未充分探索的问题。

Method: 1) 使用特征归因方法构建单词约束的伪忠实自解释；2) 在三个分类任务和三种解释风格上，用这些伪忠实自解释对指令调优模型进行持续学习；3) 评估训练效果在单词、多词设置及未见任务中的泛化能力。

Result: 训练能提升所有分类任务和解释风格的自解释忠实度，且这种提升能泛化到多词设置和未见任务。三种解释风格间存在一致的跨风格泛化，表明训练可能促进更广泛的忠实自解释能力提升。

Conclusion: 通过伪忠实自解释进行持续学习是提升LLM自解释忠实度的有效方法，且这种提升具有跨风格、跨任务的良好泛化性，为开发更可靠的模型自解释能力提供了可行路径。

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [21] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究通过强化学习微调语言模型，使其使用Prolog作为外部验证工具，提升推理可靠性和可审计性，在数学推理和知识测试中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在工具使用时经常产生看似合理但错误的推理，难以验证其可靠性。为确保智能体AI系统的安全性，需要将模型推理与形式化验证系统结合，提高可靠性和可审计性。

Method: 使用Group Relative Policy Optimization (GRPO)对Qwen2.5-3B-Instruct进行微调，基于清理后的GSM8K-Prolog-Prover数据集。研究探索了：(1)不同提示结构，(2)奖励组合（执行、语法、语义、结构），(3)推理协议（单次、best-of-N、两种智能体模式）。

Result: 强化学习方法优于监督微调，3B模型在零样本MMLU上达到与7B模型少样本相当的性能。最佳配置是：best-of-N配合外部Prolog验证在GSM8K上获得最高准确率；智能体推理配合内部修复在MMLU-Stem和MMLU-Pro上获得最优零样本泛化能力。

Conclusion: 将模型推理基于形式化验证系统能显著提升安全关键应用的可靠性和可审计性。联合优化提示、奖励和推理协议能有效塑造程序的语法和逻辑结构。

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [22] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Persian-Phi模型，通过高效课程学习将英语单语Phi-3 Mini适配到波斯语，仅需3.8B参数即实现竞争性多语言能力


<details>
  <summary>Details</summary>
Motivation: AI民主化受限于低资源语言训练LLM的高计算成本，挑战"强大多语言能力需要大模型或多语言基线"的假设

Method: 1) 使用双语故事(Tiny Stories)进行"预热"阶段对齐嵌入；2) 持续预训练；3) 通过参数高效微调(PEFT)进行指令调优

Result: Persian-Phi在HuggingFace的Open Persian LLM Leaderboard上取得竞争性结果，证明小模型也能实现良好多语言能力

Conclusion: 提供了一个经过验证、可扩展的框架，能够以最小硬件资源将最先进LLM扩展到资源不足的语言

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [23] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: NPR框架让LLM通过自我进化获得真正的并行推理能力，实现性能提升24.5%和推理加速4.6倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理通常是顺序模拟并行推理，而非真正的并行认知。需要让模型从顺序仿真转变为原生并行推理，实现更高效、可扩展的智能体推理。

Method: 1) 自蒸馏渐进训练：从"冷启动"格式发现到严格拓扑约束的无监督过渡；2) PAPO算法：在执行图中直接优化分支策略，通过试错学习自适应分解；3) NPR引擎：重构SGLang的内存管理和流程控制，支持大规模并行RL训练。

Result: 在8个推理基准测试中，基于Qwen3-4B训练的NPR实现最高24.5%性能提升和4.6倍推理加速，100%真正并行执行，超越现有基线。

Conclusion: NPR为LLM自我进化真正的并行推理能力建立了新标准，实现了高效、可扩展的智能体推理，解决了现有方法常回退到自回归解码的问题。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [24] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出PRS（渐进奖励塑造）和VSPO（基于价值采样的策略优化）两种技术，解决工具集成推理LLM在强化学习中的稀疏奖励和梯度退化问题，提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 工具集成推理的LLM通过强化学习优化时面临两个关键挑战：1）稀疏的二元奖励信号对中间步骤指导有限，收敛缓慢；2）组相对策略优化中相同奖励导致零优势，降低样本效率和训练稳定性。

Method: 提出两种互补技术：PRS是课程式奖励设计，引入密集的阶段反馈，先鼓励模型掌握可解析的工具调用格式，再优化事实正确性和答案质量；VSPO是增强的GRPO变体，用任务价值指标选择提示，并应用价值平滑裁剪稳定梯度更新。

Result: 在多个短式和长式QA基准测试中，PRS持续优于传统二元奖励，VSPO相比PPO、GRPO、CISPO和纯SFT基线，实现了更好的稳定性、更快收敛和更高最终性能。

Conclusion: PRS和VSPO共同产生了跨领域泛化能力更强的LLM-based TIR智能体，有效解决了工具集成推理强化学习中的核心挑战。

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [25] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

Relevance: 85.0

TL;DR: LIME（语言元数据嵌入）通过在token嵌入中融入语法、语义和上下文属性的元数据，显著提升预训练效率，改善分词质量，并增强语言建模和生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前仅解码器语言模型的预训练依赖大量高质量数据，但这类数据的可用性正接近极限。虽然元数据常用于创建和整理数据集，但其作为直接训练信号的潜力尚未充分探索。

Method: 提出LIME方法，将捕捉语法、语义和上下文属性的元数据融入token嵌入中。还开发了LIME+1变体，使用前一个token的元数据来指导下一个token的生成。

Result: LIME将训练数据分布适应速度提升高达56%，仅增加0.01%参数且计算开销可忽略。改善分词质量，显著增强语言建模和生成任务性能。LIME+1将推理性能提升高达38%，算术准确率提升高达35%。这些优势在500M到2B参数规模的模型中均保持。

Conclusion: 元数据作为直接训练信号具有巨大潜力，LIME方法能显著提升预训练效率和质量，同时增强模型的语言理解和生成能力。

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [26] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出RoPE++方法，在标准RoPE基础上重新引入复数点积的虚部，形成双分量注意力分数，以增强长上下文依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE实现只使用复数点积的实部计算注意力分数，丢弃了包含重要相位信息的虚部，可能导致长上下文依赖建模中关系细节的损失。

Method: 扩展RoPE以重新利用被丢弃的虚部，创建双分量注意力分数，充分利用完整的复数表示来保留更多位置信息。

Result: 在多个长上下文语言建模基准测试中，该方法相比标准RoPE持续提升性能，且随着上下文长度增加，优势更加显著。

Conclusion: 重新引入RoPE复数点积的虚部可以增强位置信息保留，提升长上下文依赖建模能力，为LLM位置编码提供改进方向。

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [27] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Metric-Fair Prompting框架，通过度量公平性约束指导LLM在医学问答中做出决策，利用问题相似性和Lipschitz约束确保个体公平性，在MedQA基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗问答等高风险应用中，LLM决策需要公平性保证。现有方法缺乏对个体公平性（相似问题应得到相似处理）的显式约束，可能导致不一致的决策结果。

Method: 1) 将医学多选题的(问题,选项)对视为二元实例；2) 使用NLP嵌入计算问题相似性；3) 将相似问题组成联合对处理而非孤立处理；4) 设计提示词强制全局决策协议：提取关键临床特征，映射到置信度分数f(x)，施加Lipschitz约束确保相似输入获得相似分数。

Result: 在MedQA(US)基准测试中，Metric-Fair Prompting相比标准单项目提示方法提升了性能，表明公平性引导的置信度导向推理能够增强LLM在高风险临床多选题上的准确性。

Conclusion: 通过将度量公平性约束融入提示工程，可以同时提升LLM在医学问答中的公平性和准确性，为高风险应用中的可信AI提供新思路。

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [28] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 开源2B参数模型PCMind-2.1-Kaiyuan-2B，通过量化数据基准、选择性重复训练和多领域课程学习，在资源受限下实现高效训练


<details>
  <summary>Details</summary>
Motivation: 解决开源社区与工业界在LLM训练上的知识鸿沟，后者依赖闭源高质量数据和训练配方。旨在为资源有限的研究者提供实用的开源解决方案

Method: 1) 量化数据基准方法：系统比较异构开源数据集，指导数据混合策略；2) 战略选择性重复方案：在多阶段范式中有效利用稀疏高质量数据；3) 多领域课程训练策略：按质量排序样本；4) 优化数据预处理流水线和FP16稳定性架构修改

Result: Kaiyuan-2B在性能上可与最先进的全开源模型竞争，展示了资源受限预训练的实际可扩展解决方案

Conclusion: 该工作为资源有限的开源社区提供了完整的训练方案和工具，包括模型权重、数据和代码，有助于缩小与工业界的差距

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [29] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

Relevance: 85.0

TL;DR: CGBridge：一种即插即用的方法，通过外部可训练的桥接模块将代码图信息注入冻结的LLM，提升代码理解的结构感知能力，在代码摘要和翻译任务上取得显著改进，推理速度比LoRA调优模型快4倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码智能任务中主要依赖线性化的token序列，难以理解程序的结构语义。先前的研究要么受限于提示长度约束，要么需要特定于任务的架构修改，无法与大规模指令跟随LLM兼容。需要一种既能利用代码图结构信息，又不需要修改LLM架构的方法。

Method: 提出CGBridge方法：1）在27万代码图数据集上通过自监督学习预训练代码图编码器；2）训练外部桥接模块，通过跨模态注意力机制对齐代码、图和文本的语义；3）桥接模块生成结构感知提示，注入冻结的LLM，针对下游代码智能任务进行微调。

Result: 在代码摘要任务上，相比原始模型和图增强提示方法分别获得16.19%和9.12%的相对提升；在代码翻译任务上，分别获得9.84%和38.87%的相对提升。推理速度比LoRA调优模型快4倍以上，实现了效果和效率的双重优势。

Conclusion: CGBridge通过外部桥接模块有效增强了LLM对代码结构语义的理解能力，无需修改LLM架构，实现了即插即用的结构感知代码理解，在多个代码智能任务上表现出显著优势。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [30] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

Relevance: 85.0

TL;DR: LLMs能通过内部表征识别不连贯故事，但在生成评分时无法有效区分连贯与不连贯叙事，表明其对叙事连贯性的理解存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够可靠地区分连贯与不连贯的故事，探究LLMs对叙事连贯性的理解能力及其局限性。

Method: 使用配对叙事数据集，通过探测研究分析LLMs的内部表征，测试不同提示变体下的评分生成能力，并比较不同类型不连贯（场景违反vs角色特质违反）的敏感性。

Result: 1) LLMs内部表征能可靠识别不连贯叙事；2) 但生成评分无法有效区分连贯/不连贯叙事；3) 思维链无法消除此缺陷；4) LLMs对场景违反的不连贯更敏感，表明其更依赖原型世界知识而非基于意义的叙事连贯性构建。

Conclusion: LLMs对叙事连贯性的理解不完整，存在内部表征与外部行为的不一致，且更依赖原型世界知识而非深层叙事理解。

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [31] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文通过受控实验框架，系统分析了预训练、中期训练和RL后训练对语言模型推理能力的因果贡献，发现RL仅在预训练留有足够空间且针对模型能力边界时有效，中期训练在固定计算下比仅RL更高效。


<details>
  <summary>Details</summary>
Motivation: 尽管RL技术显著提升了语言模型的推理能力，但尚不清楚后训练是否真正扩展了模型超越预训练获得的推理能力。现代训练流程缺乏可控性：大规模预训练语料不透明，中期训练常被忽视，RL目标与未知先验知识复杂交互。

Method: 开发完全受控的实验框架，使用合成推理任务（具有明确原子操作、可解析的逐步推理轨迹），系统操纵训练分布。沿两个轴评估模型：向更复杂组合的外推泛化，以及跨表面上下文的语境泛化。

Result: 1) RL仅当预训练留有足够空间且RL数据针对模型能力边界（困难但尚未超出能力范围的任务）时产生真实能力提升。2) 语境泛化需要最小但充分的预训练暴露，之后RL可可靠迁移。3) 中期训练在固定计算下比仅RL显著提升性能。4) 过程级奖励减少奖励攻击并提高推理保真度。

Conclusion: 结果阐明了预训练、中期训练和RL之间的相互作用，为理解和改进推理语言模型训练策略提供了基础。中期训练在训练流程中具有核心但未被充分探索的作用。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [32] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出"协作因果意义构建(CCS)"作为决策支持智能体的新框架，强调AI应作为认知工作伙伴参与专家决策过程，而非仅提供答案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在复杂高风险决策支持中表现不佳，人机团队常表现不如最佳个体，专家在验证循环和过度依赖间摇摆，未能实现互补性。这不仅是准确性问题，更是AI辅助理念的根本缺陷。

Method: 提出协作因果意义构建(CCS)研究议程和组织框架：设计作为认知工作伙伴的系统，维护专家推理模型，帮助阐明和修订目标，共同构建和压力测试因果假设，从联合决策结果中学习。

Result: 提出CCS框架，包括：1)使协作思考具有工具价值的训练生态；2)共同构建模型的表示和交互协议；3)以信任和互补性为中心的评价体系。

Conclusion: CCS框架可重新定位多智能体系统研究，围绕参与协作意义构建的智能体，使其成为与人类伙伴共同思考的AI队友。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [33] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文研究了LLM在分布外泛化能力评估中的局限性，发现不同OOD测试集之间的性能相关性高度依赖于具体模型选择，没有统一的趋势。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM分布外泛化能力的方法通常只关注单个OOD数据集，但实际部署中模型会遇到更复杂多样的数据分布变化。因此需要研究OOD泛化结果是否具有普适性。

Method: 在微调过程中评估模型在多个OOD测试集上的性能，通过回归掉域内性能后计算部分相关性，分析不同OOD测试集之间的泛化性能相关性。

Result: 分析OLMo2和OPT模型发现，OOD泛化结果没有统一的趋势：任何两个OOD测试集之间的正负相关性强烈依赖于具体分析的模型选择。

Conclusion: LLM的OOD泛化能力评估需要更全面的方法，不能仅依赖单个OOD数据集的结果，因为不同模型在不同分布变化下的表现模式各异。

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


### [34] [Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization](https://arxiv.org/abs/2512.06713)
*Donghang Duan,Xu Zheng*

Main category: cs.CR

Relevance: 85.0

TL;DR: RLAA提出了一种本地化、无需训练的文本匿名化框架，通过引入仲裁者机制解决现有贪婪对抗策略的非理性问题，在保护隐私的同时避免效用崩溃。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的文本匿名化框架依赖远程API服务，存在"隐私悖论"：用户必须向不可信的第三方披露数据才能获得隐私保护。直接将现有框架迁移到本地小规模模型会导致效用灾难性崩溃。

Method: 将匿名化过程建模为边际隐私增益(MPG)和边际效用成本(MUC)的权衡，提出理性本地化对抗匿名化(RLAA)框架，采用攻击者-仲裁者-匿名化者(A-A-A)架构，仲裁者作为理性守门员验证攻击者推理，过滤对隐私保护益处微小的反馈，强制执行理性早停标准。

Result: 在不同数据集上的广泛实验表明，RLAA实现了最佳的隐私-效用权衡，在某些情况下甚至优于现有最优方法。

Conclusion: RLAA框架通过引入理性仲裁机制，解决了现有贪婪对抗策略的非理性问题，实现了完全本地化的高效文本匿名化，在保护隐私的同时避免了效用崩溃。

Abstract: Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent "privacy paradox": users must somehow disclose data to untrusted third parties for superior privacy preservation. Moreover, directly migrating these frameworks to local small-scale models (LSMs) offers a suboptimal solution with catastrophic collapse in utility based on our core findings. Our work argues that this failure stems not merely from the capability deficits of LSMs, but from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SoTA) methods. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies inevitably drift into an irrational state. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer (A-A-A) architecture. RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible benefits on privacy preservation. This mechanism enforces a rational early-stopping criterion, and systematically prevents utility collapse. Extensive experiments on different datasets demonstrate that RLAA achieves the best privacy-utility trade-off, and in some cases even outperforms SoTA on the Pareto principle. Our code and datasets will be released upon acceptance.

</details>


### [35] [Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels](https://arxiv.org/abs/2512.07474)
*Yifei Huang,Tianyu Yan,Sitong Gong,Xiwei Gao,Caixin Kang,Ruicong Liu,Huchuan Lu,Bo Zheng*

Main category: cs.HC

Relevance: 85.0

TL;DR: Living Novel系统将文学作品转化为沉浸式多角色对话体验，通过两阶段训练解决LLM角色扮演中的人物漂移和叙事连贯性问题，在《海底两万里》上验证效果优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 解决LLM驱动角色对话中的两个核心问题：1) 通用LLM存在人物漂移问题，难以保持角色一致性；2) 智能体能力超出故事世界约束，导致叙事不连贯（剧透泄露）和鲁棒性失败（框架破坏）。

Method: 提出两阶段训练流程：1) 深度人物对齐(DPA)阶段使用无数据强化微调实现深度角色保真度；2) 连贯性和鲁棒性增强(CRE)阶段使用故事时间感知知识图谱和检索增强训练来强制执行叙事约束。

Result: 在《海底两万里》上的评估显示：DPA流程使专门模型在人物特定指标上优于GPT-4o；CRE阶段在连贯性和鲁棒性指标上达到近乎完美的性能；研究提出了AI驱动叙事系统的实用设计指南。

Conclusion: 角色优先的自训练是可信度的基础，明确的故事时间约束对于维持连贯、抗中断的移动网络体验至关重要。该系统为LLM在叙事应用中的角色扮演提供了有效的解决方案。

Abstract: We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.

</details>


### [36] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出CFD框架，利用多个LLM代理模拟人类标注员进行细粒度证据交换以达成共识，用于数据增强，在心理健康和在线安全任务上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界指标（如心理健康事件、在线风险行为）对NLP任务很重要，但标注成本高且动态变化。需要开发有效的LLM数据增强方法来获取这些难以标注的信息。

Method: 提出置信感知细粒度辩论（CFD）框架：多个LLM代理模拟人类标注员，交换细粒度证据进行辩论以达成共识。创建了两个专家标注数据集（心理健康Reddit数据集和在线安全Facebook数据集），并比较了多种LLM数据增强方法。

Result: CFD框架在所有基线方法中表现最稳健。数据增强持续改善下游任务，通过辩论转录本增强的特征带来最大提升，在线安全任务上比非增强基线提高10.1%。

Conclusion: CFD框架是有效的LLM数据增强方法，能显著提升下游任务性能，特别是在获取现实世界指标方面具有实用价值。

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [37] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

Relevance: 75.0

TL;DR: 两个大型语言模型（Mistral Nemo Base 2407和Llama 2 13B hf）在多智能体设置中相互对话，无外部输入，发现对话会从初始的连贯性逐渐陷入重复循环，出现收敛现象。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多智能体对话设置中的长期行为，研究当两个LLM在没有外部干预的情况下持续相互响应时，对话会如何演化，是否存在系统性模式或收敛现象。

Method: 使用Mistral Nemo Base 2407和Llama 2 13B hf模型，从短种子句子开始，让两个模型相互读取对方输出并生成响应，持续固定步数。应用词汇和嵌入指标来测量对话与初始种子的偏离程度以及两个模型输出的相似性。

Result: 大多数对话开始时连贯，但随后陷入重复。一旦开始重复，两个模型倾向于产生相似输出而非引入新方向，形成循环。这种收敛现象发生在大型、独立训练的模型中，且无提示指令。

Conclusion: 大型语言模型在多智能体对话中会自发收敛到重复模式，即使模型规模大且独立训练。这揭示了LLM对话动态中的系统性行为，对理解模型交互和设计鲁棒的多智能体系统有启示。

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [38] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出了一个Identify-then-Verify框架，通过让模型先识别缺失信息、再验证是否真正缺失，来更可靠地判断上下文是否足够回答问题。


<details>
  <summary>Details</summary>
Motivation: 现有简单提示策略在处理事实性问题时表现良好，但在需要推理的推断性问题（inferential questions）上经常失败。判断上下文是否包含足够信息来回答问题，是构建可靠问答系统的关键挑战。

Method: 提出结构化Identify-then-Verify框架：1) 首先生成多个关于缺失信息的假设并建立语义共识；2) 执行关键验证步骤，强制模型重新检查源文本来确认这些信息是否真正缺失。

Result: 在多个多跳推理和事实性QA数据集上评估，结果表明该方法通过引导模型为其关于缺失信息的判断提供理由，能够产生更准确的充分性判断，同时清晰阐明信息缺口。

Conclusion: 通过让模型先推理缺失的具体信息，可以提供更可靠的隐式信号来评估整体充分性，从而构建更可靠的问答系统。

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [39] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文研究使用大语言模型自动将德语文本按CEFR语言能力等级分类，通过构建多样化数据集并评估多种方法，实现了比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 语言能力评估对教育至关重要，能够根据学习者需求提供个性化教学。本文旨在探索使用大语言模型自动对德语文本进行CEFR等级分类，以实现可靠且可扩展的语言能力评估。

Method: 1) 构建多样化数据集：结合多个现有CEFR标注语料库与合成数据；2) 评估多种方法：提示工程策略、微调LLaMA-3-8B-Instruct模型、基于LLM内部神经状态的探测分类方法。

Result: 研究结果显示，所提出的方法相比先前方法取得了持续的性能提升，证明了LLMs在可靠且可扩展的CEFR分类方面的潜力。

Conclusion: 大语言模型在德语CEFR文本分类任务中表现出色，为语言能力评估提供了可靠且可扩展的解决方案，在教育领域具有重要应用价值。

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [40] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

Relevance: 75.0

TL;DR: PDFTEMRA是一个紧凑的Transformer架构，集成模型蒸馏、频域调制、集成学习和随机激活模式，旨在降低计算成本的同时保持语言理解性能，特别适用于资源受限的医疗场景和低资源语言（如印地语）。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的真实医疗场景中训练和部署大型语言模型的挑战，特别关注为视觉障碍用户和低资源语言（如印地语）使用者提供医疗援助支持。

Method: 提出PDFTEMRA架构，整合模型蒸馏、频域调制、集成学习和随机激活模式，在医疗问答和咨询数据集上进行训练和评估，与标准NLP SOTA模型基线进行比较。

Result: PDFTEMRA在显著降低计算需求的同时实现了可比较的性能表现，表明其适用于可访问、包容性的低资源医疗NLP应用。

Conclusion: PDFTEMRA为资源受限环境下的医疗NLP应用提供了一种高效解决方案，特别适合低资源语言和可访问性场景。

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [41] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

Relevance: 75.0

TL;DR: 研究发现，在单词前添加语义提示（如"meaning: {word}"）能显著提升文本嵌入模型在单词相似度任务上的表现，无需训练即可超越传统静态嵌入方法


<details>
  <summary>Details</summary>
Motivation: 文本嵌入模型主要针对句子级应用设计，在单词级任务上的表现缺乏深入研究。现有模型在孤立单词上的行为不够明确，而单词相似度评估是自然语言理解的基础任务。

Method: 测试7种文本嵌入模型，包括OpenAI、Cohere、Voyage AI等的最新模型，在3个标准单词相似度基准（SimLex-999、WordSim-353、MEN-3000）上评估。采用零-shot方法，简单地在单词前添加语义提示（如"meaning: {word}"、"Represent the semantic concept: {word}"）后嵌入。

Result: 语义提示显著提升单词相似度相关性：SimLex-999上最多提升+0.29，某些模型从完全失败（相关性=0）恢复到+0.73改进。最佳结果：Cohere模型在SimLex-999上相关性=0.692，OpenAI模型在WordSim-353上相关性=0.811，MEN-3000上相关性=0.855，超越Word2Vec（0.40）和LexVec（0.48）。

Conclusion: 简单的语义提示能有效提升文本嵌入模型在单词级任务上的表现，无需额外训练即可达到新的SOTA。这表明当前文本嵌入模型在单词理解上存在局限性，但可通过简单提示缓解。

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [42] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

Relevance: 75.0

TL;DR: 本文探讨LLMs对法证语言学的双重挑战：既是强大的分析工具，又通过风格模仿和合成文本破坏了语言风格识别的基本假设。当前AI文本检测技术存在局限性，需要方法重构以保持科学可信度和法律可采性。


<details>
  <summary>Details</summary>
Motivation: LLMs对法证语言学构成了双重挑战：一方面作为强大的分析工具支持大规模语料分析和作者归属，另一方面通过风格模仿、作者身份混淆和合成文本扩散，破坏了语言风格识别的基本假设。这种矛盾对法证应用有重要影响，需要重新评估现有方法。

Method: 文章分析了当前AI文本检测技术的三类方法：基于分类器的方法、风格计量学方法和水印技术。评估了这些方法在非母语英语作者中的高误报率以及对对抗策略（如同形异义字替换）的脆弱性等局限性。

Result: 研究发现LLMs能够近似表面风格特征，但与人类作者存在可检测的差异。当前检测技术面临严重限制，特别是在法律可采性标准（如Daubert和Kumho Tire框架）下存在不确定性。

Conclusion: 法证语言学需要进行方法重构以保持科学可信度和法律可采性。建议的适应措施包括：混合人机工作流程、超越二元分类的可解释检测范式，以及测量不同群体误差和偏见的验证机制。语言揭示其生产者信息的核心见解仍然有效，但必须适应日益复杂的人机作者链。

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [43] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文提出了一种新的长度控制方法Progress Ratio Embeddings (PRE)，通过连续嵌入和三角函数信号实现稳定的文本生成长度控制，相比现有方法在超出训练分布时表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 现代神经语言模型在文本生成方面取得了高精度，但对生成长度的精确控制仍不成熟。现有基于反向位置嵌入(RPE)的方法在超出训练分布的长度控制时存在局限性，特别是使用与绝对剩余token数量相关的离散倒计时信号会导致不稳定性。

Method: 提出了Progress Ratio Embeddings (PRE)方法，使用连续嵌入与三角函数"不耐烦"信号相结合。PRE可以无缝集成到标准Transformer架构中，提供稳定的长度保真度，同时不影响标准评估指标下的文本准确性。

Result: 在两个广泛使用的新闻摘要基准测试上的实验表明，PRE方法在保持文本准确性的同时，提供了稳健的长度控制，并且能够很好地泛化到未见过的目标长度。

Conclusion: PRE方法为解决文本生成中的长度控制问题提供了一种有效且稳健的解决方案，相比现有方法在超出训练分布时表现更好，具有更好的泛化能力。

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [44] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

Relevance: 75.0

TL;DR: DART是一个多智能体框架，通过视觉智能体之间的辩论分歧来识别有用的视觉工具（如目标检测、OCR、空间推理等），利用工具引入新信息和工具对齐的同意分数来促进讨论，最终通过聚合智能体选择最佳答案。


<details>
  <summary>Details</summary>
Motivation: 专业视觉工具可以为大语言模型或视觉语言模型提供专家知识（如接地、空间推理、医学知识等），但确定何时调用哪些工具具有挑战性。现有方法在工具调用时机和选择上存在不足，需要一种更智能的方式来协调多智能体与视觉工具的交互。

Method: DART采用多智能体辩论框架，利用智能体之间的分歧来识别有用的视觉工具。当智能体产生分歧时，系统调用相关视觉工具（如目标检测、OCR等）引入新信息，并提供工具对齐的同意分数来突出与专家工具一致的智能体。最后使用聚合智能体基于智能体输出和工具信息选择最佳答案。

Result: 在四个多样化基准测试中，DART优于多智能体辩论和单智能体工具调用框架。在A-OKVQA和MMMU上分别比次优基线（带法官模型的多智能体辩论）高出3.4%和2.4%。在M3D医学数据集上比其他强大的工具调用、单智能体和多智能体基线高出1.3%。文本重叠度分析显示DART相比现有多智能体方法有更丰富的讨论。

Conclusion: DART通过利用多智能体辩论中的分歧来指导视觉工具调用，有效提升了视觉问答任务的性能。该方法能够自适应地调用多样化工具来解决分歧，在多个领域都表现出优越性，为多智能体与专业工具的结合提供了新思路。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [45] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

Relevance: 75.0

TL;DR: NeSTR：神经符号时序推理框架，结合符号表示与混合反思推理，提升LLM在复杂时序约束下的推理能力，无需微调即可实现优越的零样本性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理任务中表现出色，但在复杂时序约束下的时序推理仍是重大挑战。现有方法存在局限：符号方法未能充分利用LLMs的推理能力，而反思方法缺乏结构化时序表示，可能导致不一致或幻觉推理。

Method: 提出神经符号时序推理（NeSTR）框架，整合结构化符号表示与混合反思推理。通过符号编码保留显式时序关系，通过验证强制逻辑一致性，使用溯因反思纠正错误推理。

Result: 在多样化的时序问答基准测试中，NeSTR实现了优越的零样本性能，无需任何微调即可持续提升时序推理能力，展示了神经符号集成在增强大语言模型时序理解方面的优势。

Conclusion: NeSTR框架通过神经符号集成有效解决了LLMs在复杂时序推理中的挑战，为提升LLMs的时序敏感性提供了新途径，展示了结构化表示与反思推理结合的价值。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [46] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出TreeED和ForestED框架，使用LLM诱导决策树进行表格数据错误检测，提升可解释性和鲁棒性，相比最佳基线平均F1分数提升16.1%


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的错误检测方法存在两个主要问题：1）作为黑盒标签器缺乏可解释性，无法提供检测结果的解释；2）对提示词高度敏感，由于模型随机性导致输出不一致，缺乏鲁棒性。需要一种既能利用LLM能力又能提供可解释性和鲁棒性的方法。

Method: 提出LLM-as-an-inducer框架：1）TreeED：通过提示词（数据上下文、决策树规范、输出要求）查询LLM诱导决策树骨架，包含规则节点（简单验证检查）、GNN节点（捕获复杂模式如函数依赖）和叶子节点（最终决策）；2）ForestED：基于不确定性采样获取多个行子集，为每个子集构建决策树，使用EM算法联合估计树可靠性和优化共识预测。

Result: 实验表明该方法准确、可解释且鲁棒，相比最佳基线平均F1分数提升16.1%。TreeED和ForestED在保持高准确率的同时提供了透明的决策过程和更好的稳定性。

Conclusion: 提出的LLM-as-an-inducer框架通过诱导决策树解决了传统LLM-as-a-labeler方法的可解释性和鲁棒性问题，为表格数据错误检测提供了更可靠和透明的解决方案。

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [47] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

Relevance: 75.0

TL;DR: SPAD提出了一种新的RAG幻觉检测方法，通过数学分解将每个token的概率归因于七个不同来源，然后按词性标签聚合来识别异常模式，有效检测幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有方法将RAG中的幻觉归因于内部知识（存储在FFN中）与检索上下文之间的二元冲突，这种观点不完整，忽略了生成过程中其他组件的影响，如用户查询、先前生成的token、当前token本身和最终的LayerNorm调整。

Method: 1. 数学归因：将每个token的概率归因于七个来源：查询、RAG、过去token、当前token、FFN、最终LayerNorm和初始嵌入；2. 聚合分析：按词性标签（POS）聚合这些分数，量化不同组件如何驱动特定语言类别；3. 异常检测：通过识别异常模式（如名词过度依赖Final LayerNorm）来检测幻觉。

Result: 大量实验表明，SPAD在RAG幻觉检测方面达到了最先进的性能。

Conclusion: SPAD通过更全面的概率归因框架，超越了传统的二元冲突视角，为RAG幻觉检测提供了更准确的方法。

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [48] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出一种简单方法，通过特征选择将语音信息融入文本预训练大语言模型，用于特定分类任务，在论辩谬误检测任务上达到SOTA


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合音频和文本嵌入时面临音频序列过长的问题，且现有语音分词器输出长序列大词汇表token，难以低成本集成到大语言模型中

Method: 使用基于lasso的特征选择在多模态词袋表示中保留最重要的音频token，通过自监督语言建模目标使语言模型适应这些token，然后在下游任务上微调

Result: 相比单模态模型、更大的SpeechLM或学习表示的音频集成方法，性能均有提升；在论辩谬误检测分类任务上达到SOTA，即使随机选择音频token也能增强单模态模型

Conclusion: 提出的简单方法能有效将语音信息融入文本预训练LLM，提升特定分类任务性能，特别是在音频被认为无益的任务上表现出色

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [49] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文提出了HalluShift++方法，通过分析MLLM内部层动态的异常来检测多模态大语言模型中的幻觉问题，无需依赖外部LLM评估器。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言理解任务中表现出色，但经常产生与视觉内容事实不一致的幻觉描述，可能导致严重后果。现有方法主要依赖外部LLM评估器，但这些评估器本身也存在幻觉问题且存在领域适应挑战。

Method: 提出假设：幻觉表现为MLLM内部层动态中的可测量异常，不仅仅是分布偏移，还包括特定假设的逐层分析。通过这种修改，HalluShift++将幻觉检测的有效性从基于文本的LLM扩展到多模态场景。

Result: 开发了HalluShift++方法，能够有效检测MLLM中的幻觉问题，代码已在GitHub上开源。

Conclusion: 通过分析MLLM内部层动态的异常来检测幻觉是一种有前景的方法，避免了对外部LLM评估器的依赖，提高了检测的可靠性和适应性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [50] [An Index-based Approach for Efficient and Effective Web Content Extraction](https://arxiv.org/abs/2512.06641)
*Yihan Chen,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.IR

Relevance: 75.0

TL;DR: 提出Index-based Web Content Extraction方法，将网页内容提取从生成式任务重构为高效的索引预测任务，显著提升RAG系统中网页内容提取的速度和准确性


<details>
  <summary>Details</summary>
Motivation: 随着Web代理需要处理大量网页内容，LLM上下文管理成为关键挑战。现有解决方案存在不足：生成式提取模型延迟高，基于规则的方法缺乏适应性，分块重排序方法忽视网页结构

Method: 将HTML分割为结构感知、可寻址的片段，将提取过程重构为索引预测任务，仅提取与查询相关内容的位罝索引，使提取延迟与内容长度解耦

Result: 在RAG QA系统中作为后检索处理组件时提高了QA准确性，在主要内容提取和查询相关提取两种场景中，在准确性和速度上都优于现有方法

Conclusion: 该方法有效解决了LLM与海量网页之间的鸿沟，为Web代理和RAG管道提供了高效、准确的网页内容提取解决方案

Abstract: As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.

</details>


### [51] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出一种轻量级方法建模多跳问答中的上下文相关篇章效用，考虑篇章间依赖关系，通过小规模Transformer模型预测篇章效用分数，提升重排序和问答性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要从多个文本段落中识别和综合信息。现有检索方法主要识别相关段落，但进一步评估篇章效用有助于去除冗余段落，减少噪声和答案不准确性。现有效用预测方法独立建模篇章效用，忽略了多跳推理的关键方面：篇章效用可能是上下文相关的，受其与其他段落关系的影响。

Method: 提出轻量级方法建模上下文相关篇章效用，考虑篇章间依赖关系。微调小型Transformer模型预测多跳问答的篇章效用分数。利用先进推理模型的推理轨迹捕获篇章使用顺序，获取合成训练数据。

Result: 实验表明，基于效用的检索篇章评分相比基于相关性的重排序方法，能带来改进的重排序和下游问答性能。

Conclusion: 建模上下文相关篇章效用对于多跳问答很重要，提出的轻量级方法能有效提升篇章重排序和问答性能。

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [52] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

Relevance: 65.0

TL;DR: LLM4SFC：首个从自然语言描述生成可执行顺序功能图（SFC）的框架，解决工业PLC编程中图形化语言生成的挑战


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要关注文本PLC编程语言（如结构化文本），但IEC 61131-3标准的图形化语言（如SFC）研究不足。SFC生成面临图形化特性和嵌入式ST代码的双重挑战，导致生成的代码不可执行且与工业工具链不兼容

Method: 1）使用简化的结构化表示捕获拓扑结构和内联ST代码，减少文本冗余；2）通过微调和少样本检索增强生成（RAG）对齐SFC编程规范；3）采用结构化生成方法实时修剪非法token，确保符合SFC文本格式

Result: 在真实自动化制造项目SFC数据集上评估，使用开源和专有LLM，LLM4SFC可靠生成语法有效的SFC程序，生成成功率达75%-94%，有效桥接图形化和文本PLC语言

Conclusion: LLM4SFC是首个从自然语言生成可执行SFC的框架，为自动化工业编程铺平道路，解决了图形化PLC语言生成的独特挑战

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [53] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该研究评估了五种大型语言模型（包括开源和专有模型）在自动生成出院小结任务上的表现，发现专有模型（特别是Gemini）表现最佳，而开源模型存在幻觉和重复信息等问题。


<details>
  <summary>Details</summary>
Motivation: 出院小结包含丰富的患者信息，对医疗护理至关重要。自动生成出院小结可以显著减少医护人员的工作量、减少错误，并确保关键患者信息易于获取和操作。

Method: 使用五种大型语言模型（开源模型：Mistral、Llama 2；专有系统：GPT-3、GPT-4、Gemini 1.5 Pro），基于MIMIC-III数据集中的摘要和笔记。采用精确匹配、软重叠和无参考指标进行评估，并进行临床专家的人工评估。

Result: 专有模型（特别是使用单样本提示的Gemini）表现最佳，生成的摘要与黄金标准摘要相似度最高。开源模型（尤其是微调后的Mistral）表现有希望但仍落后，常出现幻觉和重复信息问题。临床专家的人工评估确认了专有模型生成的摘要具有实际实用性。

Conclusion: 尽管存在幻觉和缺失信息等挑战，但研究结果表明，只要确保数据隐私，大型语言模型（特别是专有模型）是自动生成出院小结的有希望的候选方案。

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [54] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

Relevance: 65.0

TL;DR: CAuSE是一个为预训练多模态分类器生成忠实自然语言解释的新框架，通过因果抽象和交换干预确保解释忠实性，在多个数据集和模型上验证有效。


<details>
  <summary>Details</summary>
Motivation: 多模态分类器通常被视为黑盒模型，现有解释方法不够直观和可访问。自然语言解释（NLEs）虽然直观，但需要忠实反映分类器内部决策过程（忠实性）。当前缺乏为任意预训练多模态分类器生成忠实NLEs的系统性方法。

Method: 提出CAuSE框架，通过交换干预训练，形成底层分类器的因果抽象。该方法适用于任何预训练多模态分类器，通过因果干预确保生成的解释忠实反映模型决策过程。

Result: CAuSE在多个数据集和模型上验证了泛化能力。在重新设计的多模态因果忠实性度量上优于其他方法，定性分析也显示其优势。同时进行了详细的错误分析以识别失败案例。

Conclusion: CAuSE为多模态分类器提供了一种生成忠实自然语言解释的有效框架，通过因果抽象确保解释忠实性，有助于建立对AI系统的信任。

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [55] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

Relevance: 65.0

TL;DR: PICEPR是一种新颖的"Prompting-in-a-Series"算法，通过内容和嵌入两条流水线，利用模块化解码器LLM进行人格识别，实现了5-15%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 动机在于利用LLM的强大能力来改进人格识别任务，通过心理学信息的内容嵌入方法，探索LLM作为人格特征提取器和人格丰富内容生成器的潜力。

Method: 提出PICEPR算法，包含两条流水线：(a) 内容流水线 - LLM总结或生成内容；(b) 嵌入流水线 - 用于人格识别分类。使用模块化解码器LLM作为人格特征提取器和内容生成器，并比较了GPT4o、Gemini、Mistral等闭源和开源模型。

Result: PICEPR算法在人格识别任务上实现了新的最先进性能，相比现有方法有5-15%的性能提升。同时比较了不同LLM生成内容的质量。

Conclusion: PICEPR算法通过心理学信息的内容嵌入方法有效提升了人格识别性能，证明了LLM在人格特征提取和内容生成方面的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [56] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

Relevance: 65.0

TL;DR: GUMBridge是一个新的英语桥接指代资源，包含16种不同文体，提供桥接现象的广泛覆盖和细粒度子类型分类标注，评估显示在LLM时代桥接消解和子类型分类仍然是困难的NLP任务。


<details>
  <summary>Details</summary>
Motivation: 桥接指代是一种指代现象，其中话语中实体的指称依赖于先前非相同实体进行解释。现有英语桥接指代资源大多规模小、现象覆盖有限、文体覆盖有限，需要更全面、多样化的资源来推动该领域研究。

Method: 构建GUMBridge资源，包含16种不同文体的英语文本，提供桥接现象的广泛覆盖和细粒度子类型分类标注。进行标注质量评估，并使用开源和闭源的当代LLM在三个基础任务上进行基准性能测试。

Result: GUMBridge提供了桥接指代的全面资源，标注质量评估显示良好的一致性。LLM基准测试表明，即使在LLM时代，桥接消解和子类型分类仍然是困难的NLP任务，现有模型在这些任务上表现有限。

Conclusion: GUMBridge填补了桥接指代资源的空白，提供了多样化的文体覆盖和细粒度标注。研究显示桥接指代处理仍然是NLP中的挑战性问题，需要专门的方法和技术改进，即使在强大的LLM时代也是如此。

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [57] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

Relevance: 65.0

TL;DR: 本文研究了低资源语言的跨语言连续预训练，以波斯-阿拉伯语系为案例，通过无标注数据收集和针对性预训练，开发出参数量仅为3亿的模型，性能媲美参数量5倍大的系统，挑战了ASR质量主要随模型规模扩展的假设。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的自动语音识别面临标注数据稀缺和计算资源不足的双重限制，现有大模型依赖大量参数和标注数据，不适用于资源有限的语言。本文旨在探索如何通过战略性的跨语言预训练和无标注数据利用，为低资源语言开发高效ASR系统。

Method: 1) 构建3000小时多语言语料库，通过可扩展的无标注数据收集管道；2) 采用针对性连续预训练策略；3) 结合形态感知的分词方法；4) 开发300M参数模型，相比传统大模型显著减少参数规模。

Result: 300M参数模型在波斯语上超越Whisper Large v3（15亿参数），在阿拉伯语和乌尔都语上取得竞争性结果，性能媲美参数量5倍大的系统，证明了数据相关性和战略预训练比模型规模更重要。

Conclusion: ASR质量并非主要随模型规模扩展，而是数据相关性和战略预训练更为关键。这为低资源语言提供了实用的ASR发展路径，无需依赖大规模计算基础设施或专有数据集，促进包容性语音技术的发展。

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [58] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文提出了SwissGov-RSD，首个自然、文档级、跨语言语义差异识别数据集，包含224个多语言平行文档，并评估了多种LLM和编码器模型在该任务上的表现，发现当前方法在跨语言文档级任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 识别跨文档（尤其是不同语言间）的语义差异对于文本生成评估和多语言内容对齐至关重要，但作为独立任务却很少受到关注。现有研究主要集中在单语、句子级或合成数据上，缺乏自然、文档级的跨语言语义差异识别基准。

Method: 1. 构建SwissGov-RSD数据集：包含224个多语言平行文档（英-德、英-法、英-意），由人工标注者进行token级差异标注
2. 评估多种模型：包括开源和闭源大语言模型以及编码器模型
3. 在不同微调设置下进行实验

Result: 当前自动方法在跨语言文档级语义差异识别任务上表现不佳，与它们在单语、句子级和合成基准上的性能相比存在显著差距。LLM和编码器模型都显示出这一不足。

Conclusion: 跨语言文档级语义差异识别是一个具有挑战性的任务，现有模型在该领域表现不足。SwissGov-RSD数据集为未来研究提供了重要基准，代码和数据集已公开。

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [59] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文提出在生成式错误跨度检测（ESD）任务中使用最小贝叶斯风险（MBR）解码替代传统的最大后验（MAP）解码，以解决模型概率估计与人类标注相似度不一致的问题，并通过MBR蒸馏降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的生成式ESD方法通常使用MAP解码，假设模型估计的概率与人类标注的相似度完美相关。然而作者观察到，与人类标注不相似的标注可能获得比人类标注更高的模型似然，这表明MAP解码存在局限性。

Method: 1. 将MBR解码应用于生成式ESD模型，使用句子级和跨度级相似度度量作为效用函数来选择候选假设
2. 提出MBR蒸馏方法，通过知识蒸馏使标准贪婪模型匹配MBR解码性能，消除推理时的延迟瓶颈

Result: 实验结果显示，MBR解码在系统级、句子级和跨度级均优于MAP基线。MBR蒸馏方法能够使标准贪婪模型达到与MBR解码相当的性能，同时显著降低计算成本。

Conclusion: MBR解码能够有效解决生成式ESD模型中概率估计与人类标注相似度不一致的问题，而MBR蒸馏方法则提供了计算效率与性能的平衡方案。

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [60] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

Relevance: 65.0

TL;DR: MoCoRP是一个用于基于人物对话的框架，通过显式建模人物句子与回复之间的NLI关系，提升对话的一致性和参与度。


<details>
  <summary>Details</summary>
Motivation: 现有基于人物的对话数据集缺乏人物句子与回复之间的显式关系标注，导致模型难以有效捕捉人物信息，影响对话的一致性和质量。

Method: 提出MoCoRP框架，利用NLI专家显式提取人物句子与回复之间的NLI关系（如蕴含、矛盾、中性），使模型能有效将适当的人物信息融入回复。该方法应用于BART等预训练模型，并通过对齐调优扩展到现代大语言模型。

Result: 在ConvAI2和MPChat公开数据集上，MoCoRP超越了现有基线，在人物一致性和上下文感知对话生成方面表现优异。不仅在定量指标上领先，在定性方面也有显著改进。

Conclusion: 显式建模人物-回复关系能有效提升基于人物对话的质量，MoCoRP框架为人物一致性对话生成提供了有效解决方案。

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [61] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出一种结构化方法，将LLMs以成本效益方式应用于定量研究，通过思维链和少样本学习提示，结合人类与机器的优势，解决人机评分差异问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在研究中应用成本较高且存在固有弱点，需要一种既能利用LLMs优势又能弥补其不足的方法，同时将定性研究中人机协作的最佳实践扩展到定量研究领域。

Method: 采用思维链和少样本学习提示策略，将计算机科学中的提示工程技术与定性研究的人机协作方法相结合，构建结构化工作流程，让人类使用溯因推理和自然语言来审查机器和人类的工作。

Result: 成功应用于1990-2017年间1934份制药联盟新闻稿的人机评分差异分析，展示了如何通过低成本技术管理LLMs的固有弱点。

Conclusion: 该方法为在定量研究中高效利用LLMs提供了可行框架，通过精心设计的低成本技术有效管理LLMs的弱点，实现了人机优势互补。

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [62] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出基于图神经网络的在线不文明行为检测框架，在维基百科社区中检测毒性、攻击性和人身攻击，通过文本相似性构建评论关系图，结合动态注意力机制，在性能和效率上优于12个大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 在线不文明行为已成为数字社区的普遍问题，给用户带来社会和心理负担。现有平台通过人工审核和自动检测来遏制不文明行为，但现有方法在准确性和效率方面仍有局限。需要更有效的检测方法来解决这一挑战。

Method: 提出图神经网络框架，将用户评论表示为节点，通过文本相似性定义边，构建评论关系图。引入动态调整的注意力机制，在信息聚合过程中自适应平衡节点特征和拓扑特征。模型同时学习语言内容和评论间的结构关系。

Result: 实证评估表明，该架构在多个指标上优于12个最先进的大型语言模型，同时推理成本显著降低。结果强调了结构上下文在检测在线不文明行为中的关键作用，解决了纯文本LLM范式在行为预测中的局限性。

Conclusion: 图神经网络框架在检测在线不文明行为方面比大型语言模型更有效且高效，突出了结构上下文的重要性。所有数据集和比较结果将公开以支持进一步研究和可重复性。

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [63] [AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study](https://arxiv.org/abs/2512.05983)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

Relevance: 65.0

TL;DR: 该论文提出了一种基于AI的妥协提案生成方法，用于支持大规模民主文本编辑（如宪法起草），通过NLP技术和LLMs创建语义度量空间，并开发算法寻找多数支持的妥协方案。


<details>
  <summary>Details</summary>
Motivation: 在AI的论证、调解和谈判等领域，寻找代理提案之间的妥协方案是一个基本挑战。现有研究虽然提出了寻求多数支持提案的联盟形成过程，但如何有效找到这些妥协提案仍然是一个开放问题。特别是在协作编写文本文档（如民主制定社区宪法）的领域，传统工具存在局限。

Method: 1. 形式化了一个包含代理有限理性和不确定性的整体模型；2. 应用NLP技术并利用LLMs为文本创建语义度量空间；3. 开发算法生成合适的妥协提案；4. 通过模拟各种联盟形成过程来评估算法效果。

Result: 展示了AI在促进大规模民主文本编辑（如协作起草宪法）方面的潜力，证明了所开发算法在寻找多数支持的妥协提案方面的有效性。

Conclusion: 该研究填补了在联盟形成过程中有效寻找妥协提案的空白，为民主文本协作编辑提供了新的AI解决方案，特别是在传统工具受限的领域。

Abstract: The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.

</details>


### [64] [Small Language Models Reshape Higher Education: Courses, Textbooks, and Teaching](https://arxiv.org/abs/2512.06001)
*Jian Zhang,Jia Shao*

Main category: physics.ed-ph

Relevance: 65.0

TL;DR: 该研究以"大气物理学"为例，构建了包含55万篇PDF和1亿句高质量语料的教育资源库，利用小型语言模型(MiniLMs)实现精确检索，重新设计了课程体系、教材和教学方法，展示了AI教育应用的新范式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在高等教育中存在准确性不足和计算需求高的限制，而小型语言模型(MiniLMs)具有轻量级和精确检索的优势，更适合专业教育场景。研究旨在探索MiniLMs在高等教育中的具体应用路径。

Method: 1) 从130多个国际权威期刊收集55万篇全文PDF，构建专业语料库和图像库；2) 提取1亿句高质量语料和300万张高分辨率学术图像；3) 使用MiniLMs组织成高维向量库实现精确检索；4) 基于MiniLMs重新设计"大气物理学"的课程体系、教材和教学方法。

Result: 成功构建了大规模教育资源库，将传统静态教材转化为动态数字资源库，设计了基于问题的学习路径，实现了从被动知识传授到主动认知发展的教学范式转变。

Conclusion: MiniLMs驱动的"大气物理学"课程展示了"AI for education"的具体实现路径，通过跨学科前沿系统设计和动态资源库，为高等教育提供了新的教学模式。

Abstract: While large language models (LLMs) have introduced novel paradigms in science and education, their adoption in higher education is constrained by inherent limitations. These include a tendency to produce inaccuracies and high computational requirements, which compromise the strict demands for accurate and reliable knowledge essential in higher education. Small language models (MiniLMs), by contrast, offer distinct advantages in professional education due to their lightweight nature and precise retrieval capabilities. This research takes "Atmospheric Physics" as an example. We established a specialized corpus and image repository by gathering over 550,000 full-text PDFs from over 130 international well-respected journals in Earth and environmental science. From this collection, we extracted over 100 million high-quality sentence-level corpus and more than 3 million high-resolution academic images. Using MiniLMs, these resources were organized into a high-dimensional vector library for precise retrieval and efficient utilization of extensive educational content. Consequently, we systematically redesigned the courses, textbooks, and teaching strategies for "Atmospheric Physics" based on MiniLMs. The course is designed as a "interdisciplinary-frontier" system, breaking down traditional boundaries between atmospheric science, space science, hydrology, and remote sensing. Teaching materials are transformed from static, lagging text formats into a dynamic digital resource library powered by MiniLM. For teaching methods, we have designed a question-based learning pathway. This paradigm promotes a shift from passive knowledge transfer to active cognitive development. Consequently, this MiniLM-driven "Atmospheric Physics" course demonstrates a specific avenue for "AI for education".

</details>


### [65] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

Relevance: 45.0

TL;DR: CMV-Fuse是一个跨模态视图融合框架，通过结合四种语言视角（抽象意义表示、成分句法、依存句法和语义注意力）来模拟人类语言处理，用于方面级情感分析，显著提升了基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 当前方面级情感分析系统通常利用孤立的语言视角，忽视了人类自然利用的结构表示之间的复杂相互作用。自然语言理解本质上依赖于整合从表面句法到深层语义和世界知识的多个互补视角。

Method: 提出CMV-Fuse跨模态视图融合框架，系统整合四种语言视角：抽象意义表示、成分句法、依存句法和语义注意力，并增强外部知识集成。通过局部句法、中间语义和全局知识层次的分层门控注意力融合，捕捉细粒度结构模式和广泛上下文理解。采用新颖的结构感知多视图对比学习机制确保互补表示的一致性。

Result: 在标准基准测试上相比强基线有显著改进，分析揭示了每种语言视角如何对更稳健的情感分析做出贡献。

Conclusion: CMV-Fuse通过模拟人类语言处理的多视角融合方法，为方面级情感分析提供了更全面的语言理解框架，实现了性能提升和鲁棒性增强。

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [66] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

Relevance: 45.0

TL;DR: 开发了首个高质量泰卢固语-英语语音翻译基准，比较级联与端到端架构，发现端到端系统在低资源场景下具有竞争力，并评估了多种自动评估指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语有超过8000万使用者，但语音翻译研究严重不足。该研究旨在填补这一空白，为这种形态丰富的语言建立高质量的语音翻译基准。

Method: 1. 从CSTD语料库构建46小时高质量泰卢固语-英语语音翻译数据集（30/8/8小时划分）
2. 系统比较级联架构（IndicWhisper + IndicMT）与端到端架构（SeamlessM4T）
3. 评估BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore等指标与人工评估的相关性

Result: 1. IndicWhisper + IndicMT因使用大量泰卢固语特定训练数据而性能最高
2. 微调的SeamlessM4T模型使用显著更少的泰卢固语数据却表现出显著竞争力
3. 传统指标比BERTScore在泰卢固语-英语翻译中提供更好的质量区分度

Conclusion: 通过仔细的超参数调整和足够的平行数据（可能少于100小时），端到端系统在低资源场景下可以达到与级联方法相当的性能。研究提供了可复现的基准、低资源端到端性能潜力的实证证据，以及形态复杂语言对自动评估的实用指导。

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [67] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

Relevance: 45.0

TL;DR: MODOMA系统是一个用于无监督语言习得实验的计算多智能体实验室环境，通过成人与儿童智能体之间的交互实现语言习得，最终生成基于知识的语言模型。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是创建一个完全参数化的计算环境，用于模拟语言习得过程，使研究人员能够控制实验的所有方面，并明确表示和查询习得的语法知识。

Method: 采用多智能体框架，包含成人和儿童两个语言模型智能体，结合统计和基于规则的程序进行无监督语言习得。儿童智能体通过成人生成的训练和测试数据学习功能性和内容性语法范畴。

Result: 实验表明儿童智能体能够成功习得离散的语法范畴，并且在机器生成数据中发现了与人类生成数据相似的模式，验证了MODOMA方法在建模语言习得方面的有效性。

Conclusion: MODOMA系统为计算语言习得实验提供了新的可能性，能够成功模拟语言习得过程并获取可查询的语法知识表示。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [68] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

Relevance: 35.0

TL;DR: 本文研究使用形态学感知的分词器来辅助音频语料库的注释工作，提出了两种非线性分词方案，在ASR任务中与传统BPE和Unigram模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 动机是提高音频语料库注释的效率，减少人工标注工作量。针对Yoloxóchitl Mixtec这种具有非连接性形态的语言，传统分词方法可能不够有效，需要专门设计的分词器来更好地保留音调形态信息。

Method: 提出了两种新颖的非线性分词方案：1) Segment and Melody分词器：仅提取音调而不预测分割；2) Sequence of Processes分词器：预测单词分割，使端到端ASR系统能同时生成分割和未分割的转录。与传统BPE和Unigram模型进行比较。

Result: 新颖分词器与传统BPE和Unigram模型表现相当。Segment-and-Melody模型在词错误率方面优于传统分词器，但在字符错误率方面未达到相同水平。通过形态学和信息论指标分析，发现了与下游性能的预测相关性。

Conclusion: 针对特定语言非连接性形态设计的非线性分词器在ASR任务中与传统分词模型具有竞争力。需要进一步研究这些分词器在下游处理任务中的适用性。

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [69] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

Relevance: 35.0

TL;DR: AlignRuScore：将AlignScore指标适配到俄语的事实一致性评估工具，填补了俄语文本事实一致性评估工具的空白


<details>
  <summary>Details</summary>
Motivation: 当前事实一致性评估工具主要集中在英语语料库，缺乏对俄语文本的评估工具。为了确保生成文本的事实一致性在俄语NLP应用中的可靠性，需要开发专门针对俄语的评估工具。

Method: 通过微调基于RuBERT的对齐模型，在俄语和翻译的英语数据集上使用特定任务的分类和回归头，将AlignScore指标适配到俄语。

Result: 结果表明，统一的对齐指标可以成功移植到俄语，为鲁棒的多语言事实一致性评估奠定了基础。研究发布了翻译语料库、模型检查点和代码。

Conclusion: AlignRuScore成功填补了俄语事实一致性评估工具的空白，证明了统一对齐指标在多语言环境中的可移植性，为更广泛的多语言评估工具开发提供了基础。

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [70] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出TopiCLEAR方法，通过聚类嵌入和自适应降维进行主题提取，专门针对社交媒体短文本的挑战，在多个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型是为较长、较正式的文档设计的，在处理社交媒体短文本时面临挑战：共现统计有限、语义碎片化、拼写不一致和非正式语言。需要专门针对社交媒体短文本的主题提取方法。

Method: TopiCLEAR：通过聚类嵌入和自适应降维进行主题提取。使用Sentence-BERT嵌入文本，先用高斯混合模型进行初步聚类，然后通过基于线性判别分析的监督投影迭代优化聚类，直到收敛。无需预处理步骤如停用词移除。

Result: 在四个数据集（20News、AgNewsTitle、Reddit、TweetTopic）上评估，与七个基线方法（包括最近的SBERT方法和零样本生成AI方法）相比，TopiCLEAR实现了与人工标注主题的最高相似度，对社交媒体帖子和在线新闻文章都有显著改进。

Conclusion: TopiCLEAR方法在社交媒体短文本主题提取方面表现出色，产生更可解释的主题，适用于社交媒体数据和网络内容分析应用。

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [71] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

Relevance: 35.0

TL;DR: IXAM是一个用于作者归属模型的交互式可解释性框架，允许用户探索模型嵌入空间并构建多层次写作风格特征的解释


<details>
  <summary>Details</summary>
Motivation: 现有的作者归属模型缺乏有效的可解释性工具，用户难以理解模型预测背后的写作风格特征，需要交互式框架来探索模型嵌入空间并构建解释

Method: 开发了IXAM框架，针对基于嵌入的作者归属模型，提供交互式界面让用户探索嵌入空间，构建多层次粒度（从词汇到语篇）的写作风格特征解释

Result: 通过用户评估证明，相比预定义风格解释，IXAM框架在帮助用户理解模型预测方面更有价值

Conclusion: IXAM为作者归属模型提供了有效的交互式可解释性工具，增强了模型预测的可解释性和用户信任

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [72] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

Relevance: 35.0

TL;DR: MASim是首个支持多语言智能体交互的仿真框架，用于研究跨语言社会行为，包含全球舆论建模和媒体影响分析两大功能，并构建了MAPS基准进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有智能体角色扮演仿真大多是单语言的，无法模拟真实社会中跨语言交互这一重要特性。需要开发多语言智能体仿真框架来支持可控、可扩展的计算社会科学研究。

Method: 提出MASim多语言智能体仿真框架，支持具有不同社会语言特征的生成式智能体进行多轮交互。框架包含两大分析功能：1）全球舆论建模，模拟不同语言文化对开放域假设的态度演变；2）媒体影响和信息传播，通过自主新闻智能体动态生成内容并影响用户行为。构建MAPS基准，结合全球人口分布的调查问题和人口统计角色来实例化仿真。

Result: 在标定、敏感性、一致性和文化案例研究实验中，MASim能够再现社会文化现象，并突显多语言仿真对于可扩展、可控计算社会科学的重要性。

Conclusion: MASim是首个支持多语言智能体交互的仿真框架，为研究跨语言社会行为提供了重要工具，展示了多语言仿真在计算社会科学中的价值。

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [73] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

Relevance: 35.0

TL;DR: SafeTerm是一个AI驱动的系统，用于自动检索和排名与药物安全查询相关的MedDRA标准术语，通过向量嵌入和相似性计算实现自动化术语匹配。


<details>
  <summary>Details</summary>
Motivation: 在药物安全审查中，将不良事件术语分组到标准化MedDRA查询或FDA自定义医学查询对于信号检测至关重要。传统方法需要大量人工工作，需要自动化解决方案来提高效率和准确性。

Method: 系统将医学查询术语和MedDRA首选术语嵌入多维向量空间，然后应用余弦相似度和极值聚类方法生成按相关性分数排名的术语列表。验证使用FDA OCMQ v3.0（104个查询），仅限于有效的MedDRA术语。

Result: 在中等阈值下实现高召回率（>95%），更高阈值可提高精确度（最高86%）。最佳阈值（~0.70-0.75）产生约50%召回率和约33%精确度。窄术语子集表现相似但需要稍高相似度阈值。

Conclusion: SafeTerm AI驱动系统为自动化MedDRA查询生成提供了可行的补充方法。建议初始使用约0.60的相似度阈值，精炼术语选择时可增加阈值。

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [74] [MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions](https://arxiv.org/abs/2512.06933)
*Zifan Peng*

Main category: cs.CE

Relevance: 35.0

TL;DR: 论文提出了matex框架，一个认知多智能体系统，用于生成以太坊交易的忠实解释，结合链上证据和协议语义理解复杂交易。


<details>
  <summary>Details</summary>
Motivation: 理解复杂的以太坊交易具有挑战性：多跳代币流、嵌套合约调用和不透明的执行路径导致用户盲目签名。通过对普通用户、开发者和审计人员的访谈，发现需要基于链上证据和实际协议语义的忠实、逐步解释。

Method: 引入matex认知多智能体框架，将交易理解建模为协作调查过程，结合快速假设生成、动态链下知识检索、证据感知合成和对抗性验证来生成忠实解释。

Result: 未在摘要中明确说明具体实验结果，但提出了一个系统框架来解决以太坊交易理解问题。

Conclusion: matex框架通过多智能体协作调查方法，能够生成基于证据和协议语义的忠实交易解释，解决用户理解复杂以太坊交易的挑战。

Abstract: Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.

</details>


### [75] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

Relevance: 25.0

TL;DR: 本文提出了两种英语文本到统一意义表示（UMR）的解析方法，其中SETUP模型在AnCast和SMATCH++评分上表现最佳，显著提升了UMR自动解析能力。


<details>
  <summary>Details</summary>
Motivation: UMR是一种基于图的语义表示方法，能够捕捉文本核心意义并适用于多种语言（包括低资源语言）。虽然UMR在语言文档化、低资源语言技术改进和可解释性方面有潜力，但需要文本到UMR解析器来实现大规模自动生成准确的UMR图。目前文本到UMR解析的研究还很有限。

Method: 提出了两种英语文本到UMR解析方法：1）微调现有的抽象意义表示（AMR）解析器；2）利用通用依存关系转换器。以先前工作为基线，开发了名为SETUP的最佳性能模型。

Result: 最佳模型SETUP在AnCast评分上达到84分，SMATCH++评分达到91分，表明在自动UMR解析方面取得了实质性进展。

Conclusion: 本文提出的方法显著推进了英语文本到UMR的自动解析能力，为UMR在下游应用中的大规模使用奠定了基础。

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [76] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

Relevance: 25.0

TL;DR: 本文提出了一种构建多语言语料库的混合方法，用于支持人文社会科学中新兴概念的研究，以"非技术创新"为例。该方法结合公司网站文本和年度报告，通过处理流程创建可用于机器学习的数据集。


<details>
  <summary>Details</summary>
Motivation: 为支持人文社会科学中新兴概念的研究提供可重复和可扩展的资源，特别是针对"非技术创新"这类概念。需要构建能够分析词汇变异并支持自然语言处理应用的数据集。

Method: 混合方法结合两种数据源：1) 从公司网站自动提取的英法文文本，2) 按文档标准自动筛选的年度报告。处理流程包括语言检测、内容过滤、相关片段提取和元数据增强。从原始语料库创建英文数据集，为专家词典中的每个术语提取五句上下文块，并标注主题类别。

Result: 创建了一个可重复且可扩展的资源，适用于分析新兴概念周围的词汇变异，并生成专门用于自然语言处理应用的数据集。该数据集适合监督分类任务。

Conclusion: 该方法为研究人文社会科学中的新兴概念提供了有效的语料库构建框架，既支持概念分析，又为NLP应用生成专门数据集，具有可扩展性和可重复性。

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [77] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

Relevance: 25.0

TL;DR: SafeTerm AMQ是一个AI系统，用于自动检索MedDRA医学术语，通过向量嵌入和相似度计算将不良事件术语分组到SMQs/OCMQs中，在药物安全审查中实现平衡的召回率和精确率。


<details>
  <summary>Details</summary>
Motivation: 在药物上市前安全审查中，将相关不良事件术语自动分组到标准化医学查询（SMQs/OCMQs）对于信号检测至关重要。传统方法依赖人工，需要自动化解决方案来提高效率和准确性。

Method: SafeTerm AMQ系统将医学查询术语和MedDRA首选术语（PTs）嵌入到多维向量空间中，应用余弦相似度和极值聚类方法，生成按相关性分数（0-1）排名的术语列表。通过多个相似度阈值验证性能。

Result: 在110个SMQs验证中，系统在中等相似度阈值下达到94%的高召回率，在更高阈值下精确率可达89%。最佳阈值（0.70）下总体召回率48%，精确率45%。自动阈值选择（0.66）优先召回率（0.58）而非精确率（0.29）。

Conclusion: SafeTerm AMQ在SMQs和OCMQs上表现良好，是自动生成MedDRA查询的可行补充方法，能够平衡召回率和精确率。建议使用合适的MedDRA术语进行查询，并应用自动阈值方法优化召回率。

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [78] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

Relevance: 15.0

TL;DR: 该研究使用语料库语言学方法分析在线讨论中虚拟现实与焦虑相关的话语模式，发现VR、Oculus和头显是最常讨论的词汇，揭示了VR系统发展及用户体验的关注点。


<details>
  <summary>Details</summary>
Motivation: VR在焦虑症治疗中显示出潜力，但缺乏对公众在线讨论的系统分析。了解用户对VR技术的看法可以支持其疗效评估和未来发展。

Method: 采用语料库语言学方法，使用Sketch Engine软件分析英语趋势语料库，识别高频词和搭配模式，特别是VR与焦虑子语料库中的词汇网络。

Result: 在VR与焦虑子语料库中，VR、Oculus和头显是最常讨论的词汇，表明关注点集中在虚拟系统本身和物理设备。介词短语搭配揭示了设计、体验和发展三个维度的讨论模式。

Conclusion: 研究揭示了公众对VR与焦虑讨论的话语模式，为通过VR技术发展和可及性支持心理咨询需求提供了新视角和未来方向。

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [79] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

Relevance: 15.0

TL;DR: AquaFusionNet是一个轻量级跨模态框架，用于统一微生物显微成像和水质传感器数据，在边缘设备上实时监测饮用水系统污染事件


<details>
  <summary>Details</summary>
Motivation: 当前小型饮用水系统的微生物污染监测存在两个问题：1）微生物显微成像和物理化学传感器数据需要分开解读，导致实时决策不可靠；2）缺乏公开的饮用水环境显微图像数据集。需要一种能统一两种信息源并在边缘设备上部署的解决方案。

Method: 提出AquaFusionNet框架，通过门控交叉注意力机制学习微生物外观与传感器动态之间的统计依赖关系。创建了AquaMicro12K数据集（12,846张标注的1000倍显微图像），专门针对饮用水环境。框架设计针对低功耗硬件优化。

Result: 在印度尼西亚东爪哇7个设施部署6个月，处理184万帧图像，污染事件检测达到94.8% mAP@0.5，异常预测准确率96.3%，在Jetson Nano上功耗仅4.8W。相比其他轻量级检测器，在相同或更低功耗下提供更高准确率。

Conclusion: 跨模态耦合减少了单模态检测器常见的故障模式（特别是在污垢、浊度峰值和不一致光照条件下）。所有模型、数据和硬件设计已开源，促进在分散式水安全基础设施中的复制和适应。

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [80] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

Relevance: 15.0

TL;DR: 提出了一种基于历史安全数据的自动化方法，用于选择最小但全面的PRO-CTCAE症状子集，通过MedDRA语义编码和谱分析平衡相关性与多样性。


<details>
  <summary>Details</summary>
Motivation: 在肿瘤临床试验中，PRO-CTCAE系统用于收集患者报告的不良事件症状。选择过多项目会增加患者负担降低依从性，选择过少可能遗漏重要安全信号，需要一种自动化方法来优化项目选择。

Method: 将PRO-CTCAE症状映射到MedDRA术语，编码到Safeterm高维语义空间，结合相关性和发生率计算效用函数，应用谱分析识别正交医学概念集，平衡相关性与多样性。

Result: 开发了集成到Safeterm试验安全应用中的工具，通过模拟和肿瘤案例研究评估性能，能够提供客观可重复的方法平衡信号覆盖与患者负担。

Conclusion: 该自动化方法利用MedDRA语义和历史数据简化PRO-CTCAE设计，为临床试验提供更高效的患者报告结果测量工具。

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [81] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

Relevance: 15.0

TL;DR: 该研究重新检验了基本词汇中语音特征统计过表征的普遍性，通过扩大语言样本（2864种语言）并加入谱系和地理控制，发现大多数先前观察到的语音象征模式并不稳健，只有少数模式保持稳定。


<details>
  <summary>Details</summary>
Motivation: 先前关于基本词汇中语音象征模式的研究存在方法学缺陷：样本偏倚、未充分控制语言间的谱系和地理依赖关系，导致结果可能不可复现。本研究旨在通过更严格的统计控制和更大样本检验这些模式的稳健性。

Method: 1. 扩大语言样本：使用Lexibank数据库的2864种语言（原研究245种）
2. 改进统计模型：在原始模型基础上加入空间和谱系依赖性的统计控制
3. 对比分析：比较新旧样本和控制条件下的结果差异

Result: 1. 大多数先前观察到的语音象征模式在加入谱系和地理控制后消失
2. 只有少数模式在新样本和控制条件下保持高度稳定
3. 能够在更大规模上评估语音象征的分布模式

Conclusion: 语言普遍性声明需要在多个层面进行稳健性检验，大多数先前报告的语音象征模式可能反映了方法学偏差而非真正的普遍模式，只有少数模式具有跨语言的稳健性。

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出视频生成模型推理评估框架，通过任务对设计测试模型在棋类、迷宫、数独等任务上的推理能力，Sora-2等领先模型达到60%成功率，并建立自动化评估工具VMEvalKit。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在视觉质量方面取得显著进展，但其推理能力尚未得到系统评估。研究旨在建立标准化的评估范式来衡量视频模型在复杂推理任务上的表现，为模型能力提升提供基准。

Method: 提出"任务对"实验范式，构建包含39个模型的代码框架VMEvalKit，支持棋类、迷宫、数独、心理旋转、瑞文矩阵等推理任务。通过自动化评估与人工判断的相关性验证评估有效性。

Result: Sora-2等领先视频生成模型在推理任务上达到约60%的成功率。自动化评估与人类判断高度相关，证明该评估范式具有可扩展性。研究还展示了强化学习改进视频模型推理能力的潜力。

Conclusion: 视频生成模型已具备初步推理能力，提出的评估框架为系统评估和提升模型推理性能提供了有效工具，为未来视频模型的强化学习优化奠定了基础。

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [83] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文分析了SAM2和SAM3之间的根本性断裂，解释了为什么SAM2的基于提示的分割专业知识无法转移到SAM3的多模态概念驱动范式。SAM2通过空间提示进行几何和时间分割，而SAM3引入了统一的视觉语言架构，具备开放词汇推理、语义基础、对比对齐和基于示例的概念理解能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解SAM2和SAM3之间的根本性范式转变，解释为什么SAM2的专业知识无法直接迁移到SAM3。这有助于研究人员认识到从基于提示的分割到概念驱动分割的转变，为未来的分割基础模型发展提供指导。

Method: 通过五个核心组件进行系统分析：1) 概念断裂分析，对比基于提示和基于概念的分割；2) 架构差异分析，对比纯视觉-时间设计与视觉语言集成架构；3) 数据集和标注差异；4) 训练和超参数区别；5) 评估指标和失败模式分析。

Result: 分析表明SAM3代表了分割基础模型的新类别，具有开放词汇推理、语义基础和多模态概念理解能力。SAM2的优化知识不适用于SAM3，评估指标也从几何IoU转向语义和开放词汇评估。

Conclusion: SAM3标志着概念驱动分割时代的开始，代表了分割基础模型的新范式。未来的研究方向应关注多模态融合、开放词汇推理和语义基础能力的发展。

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [84] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文揭示了大型视觉语言模型（LVLMs）存在一个关键但未被充分探索的缺陷：即使模型知道正确答案，也经常通过错误的推理路径得出结果。核心问题不是知识缺乏，而是在庞大推理搜索空间中的路径选择偏差。作者提出了PSO（路径选择优化）框架来增强LVLMs的推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: LVLMs存在一个关键问题：即使模型拥有正确答案的知识，也常常通过错误的推理路径得出结果。这并非知识缺乏，而是推理搜索空间中的路径选择偏差。模型虽然能够采样正确的解决方案轨迹，但更倾向于选择不稳定或逻辑不一致的路径，导致结果不稳定和不可靠。Pass@K（大K值）与Pass@1之间的显著差距表明这种失败主要源于错误推理而非无知。

Method: 提出了PSO（Path-Select Optimization）两阶段后训练框架：第一阶段使用带有模板和答案奖励的Group Relative Policy Optimization（GRPO）来培养结构化、逐步推理；第二阶段进行在线偏好优化，模型从GRPO生成的数据中采样推理路径，自我评估，并朝着偏好的轨迹对齐。错误或次优路径同时存储在Negative Replay Memory（NRM）中作为硬负样本，定期回顾以防止模型重复先前错误并促进持续推理改进。

Result: 广泛实验表明，PSO有效修剪无效推理路径，显著提高推理准确性（平均提升7.4%），并产生更稳定和一致的思维链。

Conclusion: PSO框架成功解决了LVLMs中的路径选择偏差问题，通过两阶段优化显著提升了模型的推理性能和稳定性，为LVLMs的可靠性改进提供了有效方法。

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [85] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

Relevance: 85.0

TL;DR: LaVer是一个新颖的多模态大语言模型训练框架，通过在LLM的联合潜在语义空间中引入掩码图像建模，解决MLLMs中的模态不平衡问题，增强视觉信息利用。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在模态不平衡问题，深层网络中对视觉信息的利用不足，导致视觉性能下降或产生幻觉。这源于训练主要依赖文本token预测，缺乏直接的视觉监督信号，导致视觉表征在层间逐渐同质化。

Method: 提出Latent Visual Reconstruction (LaVer)框架，在LLM的联合潜在语义空间中引入掩码图像建模，为MLLMs提供直接的视觉激活信号，增强视觉注意力分配。

Result: 在多个基准测试上的广泛实验证明该方法在各种场景下具有优越性，特别是在需要密集视觉能力的任务中表现突出。

Conclusion: LaVer通过掩码图像建模有效解决了MLLMs中的模态不平衡问题，增强了视觉表征的判别性，提高了视觉信息利用效率。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [86] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出了MedVidBench医学视频理解基准和MedGRPO强化学习框架，解决医学视频理解中空间精度、时序推理和临床语义的挑战，通过跨数据集奖励归一化和医学LLM评估器实现多数据集平衡训练。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医学视频理解方面存在困难，需要空间精度、时序推理和临床语义理解。现有方法缺乏专门的医学视频基准，且标准强化学习在多数据集训练中因奖励尺度不平衡而失败。

Method: 1) 构建MedVidBench基准：531,850个视频-指令对，涵盖8个医学来源，包含视频、片段和帧级任务，采用专家引导提示和双模型验证的质量保证流程。2) 提出MedGRPO框架：跨数据集奖励归一化将各数据集的中位性能映射到共同奖励值；医学LLM评估器通过比较相似性评分在五个临床维度评估字幕质量。

Result: 在MedVidBench上监督微调Qwen2.5-VL-7B显著优于GPT-4.1和Gemini-2.5-Flash；MedGRPO框架进一步在定位和字幕任务上超越SFT基线，解决了多数据集训练中的奖励不平衡问题。

Conclusion: 该工作为医学领域的视觉语言模型建立了基础基准和稳健的训练方法，MedVidBench基准和MedGRPO框架共同推进了医学视频理解的发展。

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [87] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: CoT4Det将感知任务（如目标检测）重新表述为分类、计数和定位三个可解释步骤，显著提升大视觉语言模型在感知任务上的性能，在COCO2017上mAP从19%提升到33%。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在视觉问答等任务上表现出色，但在感知任务（如目标检测、语义分割）上表现远不如专业模型。例如Qwen2.5-VL-7B-Instruct在COCO2017上只有19% mAP，特别是在密集场景和小目标检测上表现不佳。

Method: 提出Chain-of-Thought for Detection (CoT4Det)，将感知任务重新表述为三个可解释步骤：1) 分类：识别图像中的物体类别；2) 计数：统计每个类别的物体数量；3) 定位：确定每个物体的具体位置。这种方法更符合大视觉语言模型的推理能力。

Result: CoT4Det显著提升感知性能而不损害通用视觉语言能力。使用标准Qwen2.5-VL-7B-Instruct，在COCO2017 val上mAP从19.0%提升到33.0%，在RefCOCO系列上优于基线+2%，在Flickr30k实体上优于基线19%。

Conclusion: 通过将感知任务重新表述为更符合大视觉语言模型推理能力的分类-计数-定位链式思维，可以显著提升模型在感知任务上的性能，同时保持通用视觉语言能力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [88] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

Relevance: 85.0

TL;DR: RunawayEvil：首个针对图像到视频生成模型的多模态越狱框架，采用"策略-战术-行动"范式，通过强化学习和LLM实现自我演化的攻击能力，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频生成系统在安全方面研究不足，特别是对越狱攻击的脆弱性。虽然I2V模型提供了创意控制，但其多模态系统的安全性尚未得到充分探索，需要开发系统化的攻击框架来评估漏洞。

Method: 基于"策略-战术-行动"范式构建自我演化攻击框架：1) 策略感知命令单元：通过强化学习策略定制和LLM策略探索实现策略自我演化；2) 多模态战术规划单元：基于选定策略生成协调的文本越狱指令和图像篡改指南；3) 战术行动单元：执行和评估多模态协调攻击。

Result: 在Open-Sora 2.0和CogVideoX等商业I2V模型上实现最先进的攻击成功率，在COCO2017数据集上比现有方法提升58.5%到79%。

Conclusion: 该工作为I2V模型的漏洞分析提供了关键工具，为构建更鲁棒的视频生成系统奠定了基础，强调了多模态系统安全评估的重要性。

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [89] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出AlignGemini：基于任务-模型对齐原则的双分支AIGI检测器，结合语义一致性检查（VLM分支）和像素伪影检测（专家分支），解决现有VLM检测器在像素伪影识别上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的AI生成图像检测方法存在两个问题：1）需要大量资源进行微调；2）仍存在严重幻觉问题。研究发现VLM本质上是语义导向的，缺乏对细粒度像素伪影的敏感性，而传统像素伪影检测器又缺乏语义理解能力。

Method: 提出任务-模型对齐原则，将AIGI检测形式化为两个互补任务：语义一致性检查和像素伪影检测。实例化为AlignGemini双分支检测器：1）仅用纯语义监督微调的VLM分支；2）仅用纯像素伪影监督训练的专家分支。通过正交监督在两个简化数据集上训练，使每个分支发挥各自优势。

Result: 在五个野外基准测试中，AlignGemini实现了平均准确率+9.5%的提升，证明了任务-模型对齐在可泛化AIGI检测中的有效性。

Conclusion: 任务-模型对齐是解决AIGI检测中语义与像素伪影平衡问题的有效途径。通过将检测任务分解为互补的子任务并匹配相应模型，可以获得更鲁棒和泛化的检测性能。

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [90] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出Stitch and Tell (SiTe)方法，通过拼接图像并生成空间感知的文本对，无需标注即可增强视觉语言模型的空间理解能力，减少空间幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型常出现空间幻觉，即错误描述图像中物体的相对位置。作者认为这源于图像和文本之间的不对称性，需要增强模型的空间理解能力。

Method: 提出SiTe方法：1) 沿空间轴拼接图像；2) 基于拼接图像的布局生成空间感知的标题或问答对；3) 无需昂贵的高级模型或人工标注；4) 可即插即用地注入结构化空间监督。

Result: 在LLaVA-v1.5-7B、LLaVA-Qwen2-1.5B和HALVA-7B三种架构上评估，SiTe显著提升空间理解任务：MME_Position (+5.50%)和Spatial-MM (+4.19%)，同时保持或提升通用视觉语言基准性能如COCO-QA (+1.02%)和MMBench (+4.76%)。

Conclusion: 明确将空间感知结构注入训练数据是减少空间幻觉、增强空间理解的有效方法，同时能保持通用视觉语言能力。该方法简单、无需标注、可即插即用。

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [91] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出Think-Reflect-Revise (TRR)三阶段训练框架，通过策略引导的自我反思增强大型视觉语言模型的安全对齐，解决单次推理易受越狱攻击的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理虽然提升了LVLMs的整体能力，但单次"思考-回答"范式在上下文或视觉越狱攻击下仍然脆弱。关键洞察是：单次推理可能忽略自身输出中的显式有害内容，而通过反思可以利用第一轮推理中揭示的恶意内容实现真正的自我修正。

Method: 1) 构建包含5,000个示例的Reflective Safety Reasoning (ReSafe)数据集，遵循think-reflect-revise流程；2) 使用ReSafe数据集微调目标模型以初始化反思行为；3) 通过强化学习强化策略引导的反思。

Result: TRR显著提升了LVLMs在安全感知基准和越狱攻击评估中的安全性能，将Qwen2.5-VL-7B的整体安全响应率从42.8%提升至87.7%，同时在MMMU和MMStar等通用基准上保持稳定性能。

Conclusion: 通过策略引导的自我反思可以有效增强LVLMs的安全对齐，TRR框架为解决单次推理的安全漏洞提供了有效方案，在保持通用能力的同时显著提升安全性能。

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [92] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出StorytellingPainter两阶段流水线，结合LLMs的创造性推理和T2I模型的视觉合成能力，生成具有复杂语义链的叙事图像，并开发了专门的评估框架。


<details>
  <summary>Details</summary>
Motivation: 叙事图像通过丰富的视觉线索传达逻辑连贯的故事，在插画创作、认知筛查等领域有广泛应用，但由于其复杂的语义特性，这类图像难以创建且相对稀缺。研究旨在探索如何利用生成式AI模型创建这类图像。

Method: 提出StorytellingPainter两阶段流水线：1) 利用LLMs进行创造性推理生成故事；2) 使用T2I模型进行视觉合成。开发专门的评估框架，包括语义复杂性评估器、KNN多样性评估器和故事-图像对齐评估器。针对开源与专有LLMs的性能差距，探索定制化训练策略，开发了Mini-Storytellers系列轻量级模型。

Result: 实验结果表明该方法的可行性和有效性，成功生成了具有复杂语义链的叙事图像。提出的评估框架能够全面评估生成图像的质量，而Mini-Storytellers模型在缩小开源与专有LLMs性能差距方面表现良好。

Conclusion: 该研究证明了利用LLMs和T2I模型生成叙事图像的可行性，为解决这类复杂语义图像的稀缺问题提供了有效方案。提出的评估框架和轻量级模型为相关研究提供了有价值的工具。

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [93] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出一种无需训练的自校正框架，通过不确定性引导的视觉重关注机制减少VLM幻觉，在冻结预训练模型上实现9.8%的幻觉率降低


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)经常生成看似合理但实际错误的幻觉内容，现有方法通常需要重新训练或微调，缺乏无需训练的自校正机制来解决这一问题

Method: 提出训练免费的自校正框架，结合多维不确定性量化（令牌熵、注意力分散、语义一致性、声明置信度）和注意力引导的裁剪机制，通过迭代精炼响应来减少幻觉

Result: 在POPE和MMHAL BENCH基准测试中，使用Qwen2.5-VL-7B模型，幻觉率相比基线降低9.8个百分点，对抗性分割上的物体存在准确率提高4.7点

Conclusion: 不确定性引导的视觉重关注机制能有效减少VLM幻觉，无需梯度更新，为可信多模态系统提供有前景的方向

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [94] [When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing](https://arxiv.org/abs/2512.07166)
*Siyuan Xu,Yibing Liu,Peilin Chen,Yung-Hui Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文提出了一种解决MLLM隐私泄露问题的新方法，通过构建SPPE数据集和引导生成框架，在保护隐私的同时恢复原始内容并保持编辑质量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）中的隐私泄露问题长期存在，现有研究虽然能有效模糊隐私信息，但往往忽视了对用户隐私真实性和恢复质量的评估。本文聚焦于如何在多样化的MLLM场景中恢复代理驱动的受保护数据这一关键挑战。

Method: 1. 构建SPPE数据集，包含多种隐私类别和用户指令，模拟真实MLLM应用场景，提供受保护代理及其各种MLLM编辑版本；2. 将隐私恢复制定为基于互补多模态信号的引导生成任务；3. 提出统一方法，可靠地重建隐私内容同时保持MLLM生成编辑的保真度。

Result: 在SPPE和InstructPix2Pix数据集上的实验表明，该方法能很好地泛化到不同的视觉内容和编辑任务，在隐私保护和MLLM可用性之间实现了良好的平衡。

Conclusion: 本文通过创新的数据集构建和引导生成框架，为解决MLLM隐私保护与可用性之间的权衡问题提供了有效方案，为多模态隐私保护研究开辟了新方向。

Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

</details>


### [95] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

Relevance: 85.0

TL;DR: START提出了一种结合空间和文本学习的图表理解方法，通过图表元素定位和图表到代码生成来增强MLLM对图表视觉布局和数据细节的理解能力。


<details>
  <summary>Details</summary>
Motivation: 图表理解对于MLLM在实际场景中的应用至关重要。与自然图像不同，图表同时具有结构化视觉布局（空间属性）和底层数据表示（文本属性），理解这两者对于精确、细粒度的图表推理是必需的。

Method: 1. 提出图表元素定位和图表到代码生成两种任务来增强MLLM的空间和文本理解能力
2. 开发START-Dataset数据生成流程：先使用MLLM将真实图表图像转换为可执行图表代码，再使用LLM演化代码以确定图表元素位置
3. 提出Chart Spatial understanding Benchmark (CS-Bench)来评估模型对图表空间结构的理解能力

Result: START在不同模型规模和基准测试上都超越了基础模型，并且明显优于之前的最先进方法，在图表理解任务上取得了显著提升。

Conclusion: 通过结合空间和文本学习，START为MLLM的图表理解提供了有效的解决方案，填补了现有方法在处理图表空间结构方面的不足，并通过新的基准测试推动了该领域的发展。

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [96] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

Relevance: 85.0

TL;DR: MMRPT：首个将强化学习直接融入大规模视觉语言模型预训练的框架，通过掩码多模态强化预训练增强视觉推理能力，而非简单模仿文本描述。


<details>
  <summary>Details</summary>
Motivation: 当前多模态预训练受限于图像-文本对的描述性偏差，导致模型倾向于依赖表面语言线索而非真正的视觉理解。需要一种能强化视觉推理而非简单模仿文本描述的预训练方法。

Method: 提出掩码多模态强化预训练框架：1) 通过视觉token的注意力估计句子级视觉依赖度；2) 掩码高度依赖视觉的文本片段；3) 使用语义-视觉奖励引导模型通过视觉推理重建被掩码部分，奖励机制强化视觉基础而非文本模仿。

Result: 在多样化基准测试中实现一致的零样本性能提升，在监督微调下显著提高鲁棒性，证明强化驱动的掩码推理为多模态模型提供了更可靠和可泛化的预训练目标。

Conclusion: 将强化学习直接融入多模态预训练能有效增强视觉推理能力，减少对表面语言线索的依赖，为构建更可靠、可泛化的大规模视觉语言模型提供了新方向。

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [97] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出RVLF框架解决手语翻译中表示不足和语义对齐问题，通过视觉语言模型与强化学习结合，在多个数据集上显著提升BLEU分数


<details>
  <summary>Details</summary>
Motivation: 当前无注释手语翻译面临两个关键挑战：1) 手语表示不足，难以捕捉细微视觉线索；2) 基于LLM的方法存在句子级语义对齐问题，限制翻译质量

Method: 三阶段强化视觉语言框架：1) 构建专门的手语视觉语言模型，融合骨架运动线索与DINOv2视觉特征；2) 指令微调获得SLT-SFT基线；3) 引入GRPO优化策略，结合BLEU和ROUGE奖励函数微调模型

Result: 在CSL-Daily、PHOENIX-2014T、How2Sign和OpenASL数据集上，BLEU-4分数分别提升+5.1、+1.11、+1.4和+1.61，无需外部大规模手语数据集预训练

Conclusion: RVLF框架有效解决了手语翻译中的表示和语义对齐问题，首次将GRPO引入手语翻译领域，实验验证了其在提升翻译质量和语义一致性方面的有效性

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [98] [The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers](https://arxiv.org/abs/2512.07331)
*Kanishk Awadhiya*

Main category: cs.CV

Relevance: 85.0

TL;DR: Vision Transformers (ViTs) 在中间层自发形成"U形"熵分布，这种"归纳瓶颈"不是架构缺陷，而是数据依赖的适应机制。研究发现瓶颈深度与任务所需的语义抽象程度相关。


<details>
  <summary>Details</summary>
Motivation: 研究ViTs中观察到的"U形"熵分布现象，探究这是否是架构缺陷，还是网络对数据特性的自适应机制。理解ViTs在不同数据集上的表示学习行为。

Method: 通过分析DINO训练的ViTs在不同数据集（UC Merced, Tiny ImageNet, CIFAR-100）上的层间有效编码维度（EED），研究瓶颈深度与语义抽象需求的相关性。

Result: 纹理丰富的数据集保持高秩表示，而面向对象的数据集驱动网络在中间层抑制高频信息，形成瓶颈以隔离语义特征。瓶颈深度与任务语义抽象程度强相关。

Conclusion: ViTs的"归纳瓶颈"是数据依赖的自适应机制，而非架构缺陷。网络根据任务复杂度学习调整表示压缩程度，为理解Transformer表示学习提供新视角。

Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

</details>


### [99] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

Relevance: 85.0

TL;DR: SJD++是一种无需训练的概率并行解码算法，通过多令牌预测和推测采样机制，将自回归文本到图像生成的推理延迟降低2-3倍，步骤压缩2-7倍，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型自回归模型虽然能生成高质量高分辨率图像，但推理速度缓慢，因为需要数百到数千次顺序前向传递进行下一个令牌预测。需要加速自回归文本到图像生成过程。

Method: 提出Speculative Jacobi Decoding++ (SJD++)，结合Jacobi解码的迭代多令牌预测机制和推测采样的概率草稿-验证机制。关键创新是在每次验证后重用高置信度草稿令牌而非重新采样所有令牌。

Result: 在多个代表性自回归文本到图像生成模型上实验，SJD++实现了2-3倍的推理延迟降低和2-7倍的步骤压缩，同时保持视觉质量无可见退化。

Conclusion: SJD++是一种有效的训练免费加速方法，显著提升自回归图像生成效率，对实际应用有重要价值。

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [100] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文发现视觉大语言模型深层视觉token信息会逐渐消失，提出"信息地平线"概念，并证明在深层随机剪枝能达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 视觉大语言模型依赖大量视觉token导致计算成本高，现有训练无关的剪枝方法在深层表现不佳，作者发现深层视觉token信息会逐渐消失，需要研究如何高效剪枝。

Method: 提出量化token信息含量的方法（通过移除token后模型输出概率的变化），分析视觉token在不同层的信息分布，发现"信息地平线"现象，提出在深层使用随机剪枝策略。

Result: 1) 发现视觉token信息随层加深逐渐均匀化并在中间层消失；2) 信息地平线位置因任务而异（OCR任务更深）；3) 模型能力越强，信息地平线越深。结合随机剪枝的方法在Qwen2.5-VL-7B上剪枝50%视觉token仍保持96.9%性能。

Conclusion: 深层视觉token信息会消失，在信息地平线之后使用随机剪枝是高效平衡性能与效率的方法，能显著提升现有剪枝方法效果。

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [101] [PVeRA: Probabilistic Vector-Based Random Matrix Adaptation](https://arxiv.org/abs/2512.07703)
*Leo Fillioux,Enzo Ferrante,Paul-Henry Cournède,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

Relevance: 85.0

TL;DR: PVeRA：VeRA适配器的概率版本，通过概率化修改低秩矩阵来处理输入中的固有模糊性，并在训练和测试时支持不同采样配置，在参数高效适应中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型训练和微调需要大量数据和计算资源，成本高昂。参数高效适应方法通过在冻结骨干网络上附加少量可训练模块来解决这一问题。VeRA适配器使用共享的冻结随机低秩矩阵在参数高效适应中表现优异，但缺乏处理输入模糊性的能力。

Method: 提出PVeRA（概率VeRA），对VeRA的低秩矩阵进行概率化修改。该方法允许处理输入中的固有模糊性，并在训练和测试阶段支持不同的采样配置，提高了模型的适应能力和鲁棒性。

Result: 在VTAB-1k基准测试中，PVeRA在七个适配器比较中表现最佳，超越了VeRA和其他适配器，展示了在参数高效适应任务上的优越性能。

Conclusion: PVeRA通过概率化方法改进了VeRA适配器，有效处理输入模糊性并支持灵活的采样策略，为参数高效适应提供了更强大的解决方案。

Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

</details>


### [102] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

Relevance: 85.0

TL;DR: Unison是一个低成本的多模态统一理解与生成模型，采用两阶段方案，能自动解析用户意图和任务参数，在仅50万训练样本和50GPU小时下实现多种任务覆盖


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态统一模型的两大问题：1）自回归方法需要大量计算资源，普通研究者难以承受；2）两阶段方法任务覆盖有限或生成质量差。两者都需要手动配置参数，缺乏自动化能力。

Method: 采用两阶段方案：1）保留预训练理解模型和生成模型的能力；2）通过低成本对齐微调连接两者；3）增加自动解析用户意图、确定任务类型、提取元信息的能力，实现全自动化。

Result: 在仅500k训练样本和50GPU小时的极低成本下，模型能准确自动识别任务和提取参数，在文本/图像/视频理解和文本到视觉内容生成/编辑/可控生成/IP参考生成等任务上表现优异。

Conclusion: Unison证明了通过两阶段方案和自动化意图解析，可以在极低成本下实现高质量的多模态统一理解与生成，为普通研究者提供了可行的解决方案。

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [103] [OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation](https://arxiv.org/abs/2512.06589)
*Xiaojun Jia,Jie Liao,Qi Guo,Teng Ma,Simeng Qin,Ranjie Duan,Tianlin Li,Yihao Huang,Zhitao Zeng,Dongxian Wu,Yiming Li,Wenqi Ren,Xiaochun Cao,Yang Liu*

Main category: cs.CR

Relevance: 85.0

TL;DR: OmniSafeBench-MM是一个全面的多模态越狱攻击防御评估工具箱，集成了13种攻击方法、15种防御策略和多样化数据集，建立了三维评估协议来衡量危害性、意图对齐和响应细节水平。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）取得了进展，但它们仍然容易受到越狱攻击，绕过安全对齐并诱导有害行为。现有基准如JailBreakV-28K、MM-SafetyBench和HADES存在局限性：攻击场景有限、缺乏标准化防御评估、没有统一可复现的工具箱。

Method: 开发了OmniSafeBench-MM工具箱，包含：1）13种代表性攻击方法；2）15种防御策略；3）多样化数据集，涵盖9个主要风险领域和50个细粒度类别，按咨询性、命令性和陈述性查询类型结构化；4）三维评估协议：危害性（多级尺度）、意图对齐、响应细节水平。

Result: 在10个开源和8个闭源MLLMs上进行了广泛实验，揭示了它们对多模态越狱的脆弱性。该工具箱为未来研究提供了标准化基础，代码已开源。

Conclusion: OmniSafeBench-MM通过统一数据、方法和评估到一个开源可复现平台，填补了多模态安全评估的空白，为MLLMs的安全研究提供了标准化工具。

Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.

</details>


### [104] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出一种可证明鲁棒的语义分割网络，通过内置Lipschitz约束实现高效训练和实时认证，比随机平滑方法快600倍。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对微小扰动敏感，现有鲁棒性研究主要集中在分类任务，语义分割的高效认证方法较少。需要开发既能保证鲁棒性又能保持高效性的语义分割网络。

Method: 引入一类新的可证明鲁棒语义分割网络，内置Lipschitz约束，实现高效训练。提出通用化鲁棒性认证框架，支持多种性能指标在ℓ₂攻击下的最坏情况分析。

Result: 在Cityscapes等挑战性数据集上达到有竞争力的像素精度，认证过程比随机平滑方法快600倍（A100 GPU），支持实时鲁棒语义分割。

Conclusion: 首次实现实时兼容的可证明鲁棒语义分割，为语义分割任务提供了灵活且计算高效的鲁棒性认证框架，在保持准确性的同时显著提升认证速度。

Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [105] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

Relevance: 75.0

TL;DR: VAT（Vision Action Transformer）是一种新颖的机器人学习架构，通过利用ViT所有层的特征层次结构，实现感知与动作生成的深度渐进融合，在模拟操作任务上达到98.15%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习中，Vision Transformers（ViTs）是视觉感知的标准方法，但大多数方法仅使用最后一层的特征，丢弃了有价值的信息。作者认为这提供了不足的表示，需要充分利用ViT的完整特征层次结构。

Method: 提出Vision Action Transformer（VAT），从ViT扩展而来，处理专门的动作令牌与所有Transformer层的视觉特征，实现感知与动作生成的深度渐进融合。

Result: 在四个LIBERO基准测试的模拟操作任务套件上，VAT实现了98.15%的平均成功率，超越了OpenVLA-OFT等先前方法，建立了新的最先进水平。

Conclusion: VAT不仅为模仿学习提供了一个强大的模型，还证明了利用视觉模型的完整"表示轨迹"对于推进机器人策略的关键重要性。

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [106] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

Relevance: 75.0

TL;DR: BeLLA是一个端到端架构，将统一的360°鸟瞰图表示与大型语言模型连接，用于自动驾驶问答任务，在需要空间推理的问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶研究中存在局限性：单视角编码器无法利用多摄像头系统的空间结构，而聚合的多视角特征缺乏统一的空间表示，难以进行自我中心方向、物体关系和更广泛上下文的推理。

Method: 提出BeLLA架构，将统一的360°鸟瞰图表示与大型语言模型连接，形成端到端的自动驾驶问答系统。该方法利用BEV表示提供统一的空间参考框架，结合LLM进行推理和问答。

Result: 在NuScenes-QA和DriveLM两个基准测试中，BeLLA在需要空间推理的问题上（如相对物体定位和附近物体行为理解）持续优于现有方法，某些任务上获得高达+9.3%的绝对改进。在其他类别中也表现出竞争力。

Conclusion: BeLLA通过结合统一的BEV表示和LLM，有效解决了自动驾驶中空间推理的挑战，为多模态自动驾驶系统提供了更强大的场景理解和推理能力。

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [107] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文重新研究了深度神经网络对"愚弄图像"的过度自信问题，发现即使在现代架构（包括Transformer）中，高置信度愚弄仍然存在，其中ViT-B/16最为脆弱。作者提出了SPOOF攻击方法，该方法简单高效，能生成高置信度愚弄图像，且对抗训练仅提供部分抵抗力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在图像识别任务中表现出色，但对与自然图像毫无相似之处的输入仍表现出过度自信。作者重新审视Nguyen等人（2015）的"愚弄图像"工作，旨在验证现代架构（包括卷积和Transformer分类器）是否仍然存在这种脆弱性。

Method: 1. 重新实现了基于CPPN和直接编码的进化愚弄攻击；2. 在包括卷积和Transformer分类器的现代架构上进行测试；3. 提出了SPOOF攻击方法，这是一种简约、一致且更高效的黑盒攻击方法，能生成高置信度愚弄图像；4. 使用愚弄图像作为额外类别进行重新训练以测试抵抗力。

Result: 1. 高置信度愚弄在现代网络中仍然存在；2. 基于Transformer的ViT-B/16最为脆弱，比卷积模型用更少查询实现近乎确定的错误分类；3. SPOOF能生成无法识别的愚弄图像，像素修改最少且计算量大幅减少；4. 使用愚弄图像进行重新训练仅提供部分抵抗力，SPOOF在稍高查询预算下仍能持续愚弄。

Conclusion: 现代深度分类器对愚弄图像仍然存在持续脆弱性，即使是最先进的架构（包括Transformer）也无法完全抵抗这种攻击。对抗训练只能提供有限保护，表明需要更鲁棒的防御机制。

Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [108] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

Relevance: 75.0

TL;DR: LaFG提出了一种语言驱动的细粒度图像检索框架，利用LLM和VLM将类别名称转换为属性级监督，通过构建数据集范围的属性词汇表来增强跨类别细节的可比性建模，提升未见类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像检索方法使用基于类别名称的稀疏one-hot标签作为监督，虽然对已见类别有效，但忽略了类别名称中丰富的语义信息，阻碍了跨类别细节的可比性建模，限制了向未见类别的泛化能力。

Method: 1. 使用LLM将每个类别名称作为语义锚点，生成详细的属性导向描述；2. 利用冻结的VLM将这些描述投影到视觉对齐空间，聚类形成数据集范围的属性词汇表，并从相关类别中获取补充属性；3. 使用全局提示模板选择类别相关属性，聚合成类别特定的语言原型；4. 用这些原型监督检索模型。

Result: 论文结果显示LaFG在细粒度图像检索任务中，特别是在未见类别的泛化方面取得了显著提升，通过语言驱动的属性监督增强了模型的语义理解能力和跨类别可比性。

Conclusion: LaFG通过利用LLM和VLM将类别名称转换为丰富的属性级监督，有效解决了传统细粒度图像检索方法中语义稀疏的问题，显著提升了模型对跨类别细节的可比性建模能力和未见类别的泛化性能。

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [109] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

Relevance: 75.0

TL;DR: ReCAD是一个强化学习框架，通过引导预训练大模型生成精确的参数化CAD模型，利用其内在生成能力，在文本到CAD和图像到CAD任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖监督微调注入知识，编辑性有限，未能充分利用预训练大模型的强大生成先验。需要一种能利用PLMs生成能力、支持复杂CAD操作（如模式复制和镜像）的框架。

Method: 1. 微调视觉语言模型，重写CAD脚本为参数化代码，生成准确文本描述进行监督；2. 提出新颖RL策略，将参数化代码作为指导增强模型推理；3. 采用分层基元学习过程，在统一奖励函数下逐步教授结构化组合技能。

Result: 在文本到CAD和图像到CAD任务中达到最先进水平，显著提高几何精度。在图像到CAD任务中，将平均Chamfer距离从73.47降至29.61（分布内），从272.06降至80.23（分布外），大幅超越现有基线。

Conclusion: ReCAD成功利用预训练大模型的生成能力，通过强化学习框架实现了精确的参数化CAD模型生成，支持复杂CAD操作，在几何精度和语义保真度方面表现优异。

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [110] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: TreeQ是一个针对扩散Transformer（DiT）的统一量化框架，通过树结构搜索、环境噪声引导和通用Monarch分支技术，实现了DiT模型的高效低比特量化，在W3A3和W4A4设置下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: DiT作为图像生成的可扩展骨干网络，虽然性能优于U-Net，但计算和内存需求高，实际部署困难。现有的混合精度量化方法在U-Net上成功，但在DiT架构上应用有限且未充分探索。

Method: 1. 树结构搜索：利用DiT的线性特性，在O(n)时间内遍历解空间，通过比较剪枝提高目标精度；2. 环境噪声引导：使用单一超参数对齐PTQ和QAT配置；3. 通用Monarch分支：结构化稀疏分支防止超低比特下的信息瓶颈。

Result: 在DiT-XL/2模型上，TreeQ在W3A3和W4A4 PTQ/PEFT设置下达到最先进性能，首次实现了DiT模型近乎无损的4位PTQ性能。

Conclusion: TreeQ框架有效解决了DiT量化的关键挑战，为DiT的实际部署提供了高效的量化解决方案，显著降低了计算和内存开销。

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [111] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

Relevance: 75.0

TL;DR: VG-Refiner：首个面向工具精炼的指代接地推理框架，通过两阶段"思考-再思考"机制和精炼奖励，解决现有工具集成视觉推理中不可靠工具输出导致的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成视觉推理范式主要关注通过强化学习整合各种视觉工具，但缺乏处理不可靠或错误工具输出的有效响应机制。这在指代和接地任务中尤为突出，不准确的检测工具预测经常误导模型产生幻觉推理。

Method: 提出VG-Refiner框架，包含：1) 两阶段思考-再思考机制，使模型能显式分析和响应工具反馈；2) 精炼奖励机制，鼓励对不良工具结果进行有效修正；3) 少量任务特定数据增强精炼能力；4) 提出两个新指标和公平评估协议。

Result: 在指代和推理接地基准测试中，VG-Refiner在准确性和修正能力方面取得显著提升，同时保持了预训练模型的通用能力。

Conclusion: VG-Refiner是首个专注于工具精炼的指代接地推理框架，通过显式处理工具反馈和系统评估机制，有效解决了工具集成视觉推理中的幻觉问题。

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [112] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出诊断框架评估AI生成驾驶视频(AIGVs)对自动驾驶模型训练和评估的可靠性，包括构建ADGV-Bench基准和ADGVE评估器，发现原始AIGVs会降低感知性能，但经过筛选后可成为真实数据的有效补充。


<details>
  <summary>Details</summary>
Motivation: 文本到视频模型能生成高质量驾驶场景，为自动驾驶提供低成本、可扩展的数据替代方案。但关键问题是：这些AI生成的驾驶视频能否可靠地支持自动驾驶模型的训练和评估？需要系统研究AIGVs的可靠性和潜在风险。

Method: 1) 提出AIGV失效模式分类法（视觉伪影、物理不合理运动、交通语义违规）；2) 构建ADGV-Bench基准，包含人工质量标注和密集感知任务标签；3) 提出ADGVE评估器，结合静态语义、时序线索、车道遵守信号和VLM引导推理生成质量评分。

Result: 实验表明：盲目添加原始AIGVs会降低物体检测、跟踪和实例分割性能；使用ADGVE筛选AIGVs能持续改善视频质量评估指标和下游自动驾驶模型性能；筛选后的AIGVs可成为真实世界数据的有益补充。

Conclusion: 研究揭示了AIGVs在自动驾驶中的风险和潜力，提供了实用工具来安全利用大规模视频生成技术。AIGVs经过适当筛选后可作为真实数据的有效补充，但需要系统评估框架确保可靠性。

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [113] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出Self-Autoregressive Refinement (SAR)方法，通过Stagger-Scale Rollout和Contrastive Student-Forcing Loss解决自回归图像生成中的曝光偏差问题，在预训练模型基础上显著提升生成质量且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 自回归生成模型在媒体合成中表现出强大能力，但尺度级自回归模型存在曝光偏差问题，这主要源于训练-测试不匹配和尺度间学习难度不平衡，导致生成质量下降。

Method: 提出SAR方法，包含两个核心组件：1) Stagger-Scale Rollout (SSR)机制，通过轻量级自回归展开让模型接触自己的中间预测，对齐训练测试模式；2) Contrastive Student-Forcing Loss (CSFL)，为自生成上下文提供充分监督以确保训练稳定。

Result: 实验表明，SAR应用于预训练AR模型能一致提升生成质量且计算开销小。例如，在ImageNet 256上训练的FlexVAR-d16模型，SAR在10个epoch内（32xA100 GPU上5小时）实现5.2% FID降低。

Conclusion: SAR因其高效性、可扩展性和有效性，有望成为视觉自回归生成的可靠后训练方法，解决曝光偏差问题并提升生成质量。

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [114] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出基于Mamba状态空间模型的场景文本检测器，通过选择机制与注意力层结合，增强长序列特征提取能力，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的方法在全局特征提取方面有限制，Transformer方法虽然解决了这一问题，但存在跨领域限制和固有缺陷：在建模长距离依赖时会遗忘重要信息或关注无关表示。Mamba状态空间模型通过线性复杂度选择机制展现了更好的长距离依赖建模能力。

Method: 1) 提出基于Mamba的场景文本检测器，将选择机制与注意力层集成；2) 采用Top_k算法显式选择关键信息，减少Mamba建模中的无关信息干扰；3) 设计双尺度前馈网络和嵌入金字塔增强模块，促进高维隐藏状态交互和多尺度特征融合。

Result: 在多个基准测试中达到SOTA或竞争性性能：CTW1500上F-measure为89.7%，TotalText上为89.2%，ICDAR19ArT上为78.5%。

Conclusion: 提出的基于Mamba的场景文本检测器通过集成选择机制和注意力层，有效解决了长距离依赖建模中的信息遗忘和无关关注问题，在场景文本检测任务中表现出优越性能。

Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [115] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

Relevance: 75.0

TL;DR: DEPER提出了一种个性化图像描述生成方法，通过建模用户的观看行为（注意力模式）和语言风格，使用轻量级适配器实现少样本个性化，显著提升了描述质量。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像描述模型只关注语言风格，忽略了不同用户观看图像时的注意力模式差异。人们观看同一图像时关注不同区域、对象和细节，这种观看行为的个性化对描述生成至关重要。

Method: DEPER学习一个主体嵌入，同时捕捉语言风格和观看行为，通过辅助注意力预测任务进行指导。使用轻量级适配器将这些嵌入与冻结的视觉语言模型对齐，实现少样本个性化而无需重新训练。

Result: 在四个数据集上，涵盖不同观看任务和简短/详细描述，DEPER平均提升24%，表明建模个性化注意力能产生更符合人类认知和更高质量的描述。

Conclusion: 理解人们如何观看有助于预测他们会说什么；建模人类感知多样性可以提升多模态系统的性能和人类对齐性。

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [116] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

Relevance: 75.0

TL;DR: DEViL提出了一种检测器赋能的视频大语言模型，通过参考语义令牌连接视频LLM和开放词汇检测器，解决时空定位中的误差累积问题，并引入时间正则化确保时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频时空定位任务中，将边界框作为文本令牌进行自回归生成，导致输出序列过长、空间误差随时间累积以及定位结果漂移的问题。需要一种更有效的方法来处理视频中的时空定位和推理。

Method: DEViL将视频LLM与开放词汇检测器（OVD）耦合，通过参考语义令牌（RST）连接两者。RST将用户查询提炼为丰富的语义表示，既作为控制信号又替代OVD的文本嵌入。此外，提出OVD内的tube-mined时间正则化（TTReg），驱动OVD生成时间一致的目标对象查询。

Result: 实验表明DEViL在各种细粒度视频理解任务上表现优异，特别是在STVG（时空视觉定位）和GroundedVQA（基于定位的视觉问答）任务上取得了强劲性能。

Conclusion: DEViL通过检测器赋能的方法有效解决了视频时空定位中的误差累积问题，实现了端到端的参考理解和空间定位学习，为视频理解任务提供了新的解决方案。

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [117] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出ECVGPO算法，用于多模态大语言模型在视觉定位任务中的熵控制，通过调节探索与利用的平衡来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习微调多模态大语言模型取得了显著进展，但熵在感知导向任务（如视觉定位）中的作用和控制策略仍未充分探索。本文旨在分析视觉定位任务中熵的特性，并开发有效的熵控制方法。

Method: 首先分析视觉定位任务中熵的作用和特性，并与推理任务进行对比。基于这些发现，提出ECVGPO（熵控制视觉定位策略优化）算法，这是一个可解释的算法，专门设计用于有效的熵调节，通过熵控制更好地平衡探索与利用的权衡。

Result: 实验表明，ECVGPO在各种基准测试和模型上都实现了广泛的性能提升。

Conclusion: ECVGPO算法通过有效的熵控制，在多模态大语言模型的视觉定位任务中取得了显著改进，为感知导向任务的强化学习微调提供了新的熵控制策略。

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [118] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

Relevance: 75.0

TL;DR: VisChainBench是一个用于评估大型视觉语言模型多图像、多轮次推理能力的大规模基准测试，包含1,457个任务和20,000多张图像，涵盖日常场景和工程故障排除等领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注静态或水平比较（如视觉差异识别），过度依赖语言线索，忽视了渐进式、上下文相关的推理以及视觉到视觉的推理挑战。需要填补这一空白来评估LVLMs在多步骤视觉推理方面的能力。

Method: 使用多智能体生成流水线构建基准测试，确保高视觉多样性和受控的语言偏差。包含三个不同领域的1,457个任务，模拟现实世界的决策过程，最小化语言指导。

Result: 创建了包含20,000多张图像的大规模基准测试VisChainBench，数据已在HuggingFace上公开可用，代码也可用于基准测试构建。

Conclusion: VisChainBench填补了LVLMs在多图像、多轮次场景中评估的空白，为评估模型在顺序、相互依赖任务中的多步骤视觉推理能力提供了重要工具。

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [119] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

Relevance: 75.0

TL;DR: MMDuet2：基于多轮RL训练的视频MLLM主动交互模型，无需精确回复时间标注，在流式视频中自主决定何时回复


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大语言模型多为回合制交互，无法在视频播放过程中主动决定何时回复，这限制了实时应用的发展。需要解决手动调整回复决策阈值和精确标注回复时间的问题。

Method: 提出文本到文本的主动交互方法，模型基于对话历史和当前帧视觉上下文自主决定回复或保持沉默。引入多轮强化学习训练方法，鼓励及时准确的回复，无需精确回复时间标注。在52k视频数据集上通过SFT和RL训练MMDuet2模型。

Result: MMDuet2在回复时机和质量上优于现有主动视频MLLM基线，在ProactiveVideoQA基准测试中达到最先进性能。

Conclusion: 该方法有效解决了视频MLLM主动交互中的关键挑战，为实时视频应用提供了更自然的交互方式。

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [120] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

Relevance: 75.0

TL;DR: DyToK：一种无需训练的VLLM动态token压缩方法，利用VLLM固有的注意力机制实现动态token压缩，通过LLM引导的关键帧先验动态调整每帧token保留比例，在保持精度的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型面临长视频处理时视觉token序列二次计算复杂度增长的问题。传统关键帧采样方法虽然能提升效率，但存在额外计算成本和二元帧选择范式次优的问题。

Method: 提出DyToK方法，利用VLLM注意力层自然编码的查询条件关键帧先验，动态调整每帧token保留比例，优先保留语义丰富的帧，抑制冗余信息。该方法无需训练，可与现有压缩方法（如VisionZip、FastV）即插即用。

Result: DyToK在多个VLLM（如LLaVA-OneVision、Qwen2.5-VL）上实现了最优的效率-精度权衡，推理速度提升4.3倍的同时保持精度不变。

Conclusion: DyToK通过利用VLLM固有的注意力机制实现动态token压缩，为长视频理解提供了高效且准确的解决方案，具有即插即用兼容性。

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [121] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

Relevance: 75.0

TL;DR: VDOT是一个高效统一的视频生成模型，采用分布匹配蒸馏范式，结合最优传输技术和判别器，在4步推理中达到100步去噪基线的性能


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型要么只能处理少数特定条件，要么因复杂推理导致生成时间过长，难以在实际应用中部署。需要开发高效统一的视频生成解决方案。

Method: 1. 采用分布匹配蒸馏范式训练；2. 使用最优传输技术优化真实与生成分数分布差异，避免KL散度蒸馏中的零强迫或梯度崩溃问题；3. 集成判别器感知真实视频数据；4. 开发自动化视频数据标注和过滤管道；5. 构建统一测试基准UVCBench。

Result: 实验表明，仅需4步推理的VDOT模型在性能上匹配或超越需要100步去噪的基线模型，显著提升了生成效率。

Conclusion: VDOT通过最优传输蒸馏和判别器集成，实现了高效统一的视频生成，在保持高质量的同时大幅减少推理步骤，为实际应用提供了可行方案。

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [122] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

Relevance: 75.0

TL;DR: RMAdapter：一种用于视觉语言模型少样本微调的双分支适配器，通过适应分支注入任务知识，重建分支保留通用知识，实现泛化与任务适应的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（如CLIP）在少样本微调中面临任务特定适应与泛化能力的平衡挑战。现有研究主要关注基于提示的方法，而基于适配器的方法探索不足且性能存在差距。

Method: 提出重建型多模态适配器（RMAdapter），采用双分支架构：1）适应分支通过参数高效微调注入任务特定知识；2）重建分支通过将潜在空间特征重建回原始特征空间来保留通用知识。通过局部重建损失计算和共享投影模块保持轻量化，并加入一致性约束来调节判别性与泛化的权衡。

Result: 在不依赖数据增强或重复提示设计的情况下，RMAdapter在三个代表性任务（新类别泛化、新目标数据集泛化、领域泛化）上全面优于现有最先进方法。

Conclusion: RMAdapter通过双分支架构有效平衡了视觉语言模型少样本微调中的任务特定适应与泛化能力，为参数高效微调提供了新思路。

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [123] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

Relevance: 75.0

TL;DR: MulCLIP提出了一种端到端的多层次对齐框架，通过全局对比对齐、token重建对齐和子标题聚合补丁对齐，解决了CLIP模型在处理长文本描述时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在短文本上表现良好，但在处理长文本描述时效果不佳。虽然最近的研究利用区域提议信息来缓解这一问题，但带来了显著的部署成本。需要一种更高效的方法来处理长文本与图像的细粒度对齐。

Method: 1. 保持图像与摘要/长标题的全局对比对齐，并扩展位置编码以支持更长文本序列
2. 提出token重建对齐：通过局部校准特征增强单词与图像补丁之间的语义连接
3. 提出子标题聚合补丁对齐：自动提取和聚合每个子标题的上下文丰富补丁

Result: 在多个基准测试中，MulCLIP一致提升了下游性能。消融研究证实其多尺度对齐是提升细粒度能力的关键因素，优于基于区域提议的方法，更适合实际应用。

Conclusion: MulCLIP通过多层次对齐框架有效解决了CLIP在处理长文本描述时的局限性，在保持高效部署的同时提升了细粒度理解能力，为实际应用提供了更好的解决方案。

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [124] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出Dropout Prompt Learning方法，在视觉语言模型的文本和视觉分支上应用基于token重要性的自适应dropout，并结合残差熵正则化，提升模型在少样本学习、长尾分类和分布外泛化等挑战性场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统dropout通过随机丢弃神经元来提升模型泛化能力，但视觉语言模型中的文本和视觉token具有不同的重要性。作者希望开发一种针对视觉语言模型的dropout方法，能够根据token的模态内上下文和模态间对齐关系来评估其重要性，从而实现更灵活的自适应dropout概率。

Method: 1) Dropout Prompt Learning：在文本和视觉分支的token上应用dropout，根据模态内上下文和模态间对齐评估每个token的重要性，为不同token分配灵活的dropout概率；2) 残差熵正则化：在保持语义对齐以实现通用知识迁移的同时，鼓励dropout引入的多样化表示。

Result: 在15个基准测试上验证了方法的有效性，特别是在少样本学习、长尾分类和分布外泛化等挑战性场景中表现优异。在base-to-novel泛化任务上，超越了包括KgCoOp（提升5.10%）和PromptSRC（提升2.13%）在内的正则化方法。

Conclusion: 提出的Dropout Prompt Learning方法通过自适应token dropout和残差熵正则化，显著提升了视觉语言模型在各种挑战性场景下的鲁棒性和泛化能力，为视觉语言模型的prompt学习提供了有效的正则化策略。

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [125] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

Relevance: 75.0

TL;DR: TEXTER是一种新的零样本图像分类器解释方法，通过识别决策关键特征并映射到CLIP特征空间来生成文本解释，相比现有方法能产生更忠实和可解释的说明。


<details>
  <summary>Details</summary>
Motivation: 现有零样本解释方法通常将全局图像特征与语言对齐，只能描述可见内容而非驱动预测的关键因素。大型视觉语言模型虽然能生成字幕，但并非专门为分类器特定推理设计，无法提供针对分类决策的解释。

Method: TEXTER首先识别对预测有贡献的神经元，强调这些神经元编码的特征（决策关键特征），然后将这些强调的特征映射到CLIP特征空间以检索反映模型推理的文本解释。对于Transformer架构，使用稀疏自编码器进一步提高可解释性。

Result: 大量实验表明，TEXTER比现有方法生成更忠实和可解释的解释。代码将公开发布。

Conclusion: TEXTER通过隔离决策关键特征再进行对齐的方法，克服了现有零样本解释方法的局限性，能够更好地揭示图像分类器的决策依据。

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [126] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

Relevance: 75.0

TL;DR: LongCat-Image是一个开创性的开源双语（中英）图像生成基础模型，在文本渲染、真实感、部署效率和开发者可访问性方面解决当前主流模型的核心挑战，通过精心设计的数据策略和奖励模型实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前主流图像生成模型在多语言文本渲染（特别是中文字符）、真实感、部署效率和开发者可访问性方面的核心挑战，为社区提供全面开放的开源生态系统。

Method: 1) 在预训练、中期训练和SFT阶段采用严格的数据筛选策略；2) 在RL阶段协调使用精心设计的奖励模型；3) 采用紧凑的6B参数扩散模型架构，相比领域内常见的20B+ MoE架构显著减小；4) 建立全面的开源生态系统，包括多个模型版本和完整训练工具链。

Result: 1) 在文本渲染能力和真实感方面达到新的SOTA水平；2) 为中文字符渲染设定了新的行业标准，在覆盖范围和准确性上超越主流开源和商业解决方案；3) 仅6B参数的紧凑设计实现最小VRAM使用和快速推理；4) 在图像编辑任务上也取得SOTA结果，具有优越的编辑一致性。

Conclusion: LongCat-Image通过创新的数据策略、紧凑架构设计和全面开源生态系统，在多语言图像生成领域取得了突破性进展，为开发者和研究人员提供了强大的视觉内容创作支持。

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [127] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

Relevance: 75.0

TL;DR: SpatialDreamer是一个基于强化学习的框架，通过主动探索、视觉想象和证据推理的闭环过程，提升多模态大语言模型在复杂空间推理任务中的表现，特别是需要心理模拟的任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在需要心理模拟的复杂空间推理任务上表现有限，主要依赖被动观察空间数据，缺乏主动的心理意象过程。需要一种能够模拟人类主动空间心理模拟的方法。

Method: 提出SpatialDreamer强化学习框架，包含主动探索、通过世界模型的视觉想象和证据推理的闭环过程。为解决长序列推理任务中细粒度奖励监督不足的问题，提出几何策略优化（GeoPO），引入树结构采样和基于几何一致性约束的步级奖励估计。

Result: 在多个具有挑战性的基准测试中，SpatialDreamer取得了极具竞争力的结果，标志着MLLMs在类人主动空间心理模拟方面的关键进展。

Conclusion: SpatialDreamer通过强化学习和主动心理模拟机制，显著提升了MLLMs在复杂空间推理任务中的能力，为类人空间认知能力的发展提供了重要方向。

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [128] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文提出了一种新的视觉相似性度量方法，专注于捕捉图像之间的"关系相似性"而非传统的外观属性相似性，通过构建匿名化描述数据集并微调视觉语言模型来实现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉相似性度量方法（如LPIPS、CLIP、DINO）仅关注感知属性相似性，无法捕捉人类能够感知的丰富关系相似性。人类不仅能识别外观相似，还能识别结构关系相似（如地球与桃子的分层结构对应关系）。

Method: 1. 将关系图像相似性形式化为可测量问题：当图像内部视觉元素之间的关系或功能对应时，即使视觉属性不同，也认为具有关系相似性。
2. 构建包含114k图像-标题的数据集，其中标题经过匿名化处理，描述场景的底层关系逻辑而非表面内容。
3. 使用该数据集微调视觉语言模型，以测量图像之间的关系相似性。

Result: 开发了首个能够测量图像关系相似性的模型，该模型能够根据图像的底层关系结构而非可见外观来连接图像。研究显示，关系相似性具有许多实际应用，但现有图像相似性模型无法捕捉这一维度。

Conclusion: 关系相似性是视觉计算中一个关键但被忽视的维度，现有模型存在重要缺陷。通过提出的方法和数据集，为基于关系结构而非表面外观的图像相似性评估开辟了新方向。

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [129] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

Relevance: 65.0

TL;DR: NeuroFM：专门针对神经病理学（特别是神经退行性疾病）训练的基础模型，相比通用病理学模型在脑组织分析任务上表现更优


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型主要基于外科病理数据训练，这些数据富含非神经组织，过度代表肿瘤、炎症等非神经系统疾病。神经病理学具有独特的细胞类型、细胞结构和疾病特征，这种领域不匹配限制了通用模型在神经退行性疾病（如阿尔茨海默病、帕金森病）形态学模式识别方面的能力。

Method: 开发了NeuroFM，一个专门在涵盖多种神经退行性病理的脑组织全切片图像上训练的基础模型。模型针对神经病理学特有的形态学特征进行优化。

Result: NeuroFM在多个神经病理学特定下游任务上表现优于通用模型，包括混合性痴呆疾病分类、海马区域分割、神经退行性共济失调识别（涵盖小脑性特发性震颤和脊髓小脑性共济失调亚型）。

Conclusion: 领域专业化基础模型在脑组织上训练能更好地捕捉神经病理学特定特征，为脑疾病诊断和研究提供更准确可靠的AI分析，为数字病理学专业领域的特定领域模型开发树立了先例。

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [130] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

Relevance: 65.0

TL;DR: 论文提出使用AI代理自动化科学计算机视觉工具适配的"最后一公里"问题，通过系统评估框架发现简单代理架构能生成优于人类专家的代码，而复杂代理架构并非总是有益


<details>
  <summary>Details</summary>
Motivation: 将生产级计算机视觉工具适配到特定科学数据集存在"最后一公里"瓶颈：微调需要大量标注数据（科学家通常缺乏），而手动代码适配需要科学家花费数周到数月时间。论文探索使用AI代理自动化这一手动编码过程

Method: 引入系统化的代理代码优化评估框架，研究三个生产级生物医学成像流水线，比较不同代理架构的性能，包括简单代理框架与复杂代理架构

Result: 简单代理框架能一致生成优于人类专家解决方案的适配代码，分析显示常见复杂代理架构并非普遍有益，为代理设计提供了实用路线图。通过将代理生成的函数部署到生产流水线验证了方法的实际影响

Conclusion: AI代理能有效自动化科学计算机视觉工具适配，简单代理架构在特定任务中表现优异，为实际应用提供了明确路径。开源了评估框架并展示了真实世界部署的可行性

Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [131] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出一个基于多模态大语言模型（MLLM）的偏好条件图像生成框架，通过提取丰富的用户表示并注入扩散模型，实现个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有偏好条件图像生成方法要么无法捕捉细微的用户偏好，要么缺乏有效的个性化视觉信号编码机制。需要一种能够准确理解用户审美偏好并生成符合个人喜好的图像的方法。

Method: 1) 训练MLLM进行偏好导向的视觉问答任务以捕捉细粒度语义线索；2) 引入两个互补的探测任务：用户间区分（区分不同用户）和用户内区分（区分喜欢与不喜欢的内容）；3) 设计基于最大均值差异的对齐损失，弥合模态差距同时保留多模态结构；4) 将得到的嵌入用于条件化扩散生成器。

Result: 在图像质量和偏好对齐方面显著优于强基线方法，证明了表示提取和对齐在个性化生成中的有效性。

Conclusion: 提出的多模态框架通过MLLM提取丰富的用户表示并有效对齐到扩散模型，实现了高质量的个性化图像生成，在偏好对齐和图像质量方面表现出色。

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [132] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出CEFM框架，通过对比学习将临床诊断标准（ABC规则）映射到视觉Transformer嵌入空间，生成结构化文本解释，提高黑色素瘤分类的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在黑色素瘤分类中已达到专家水平，但模型不透明和缺乏可解释性是临床采用的关键障碍。临床医生难以信任黑盒模型的决策过程，需要建立临床语义与视觉特征之间的透明联系。

Method: 使用对比学习作为核心机制，通过双投影头将临床诊断标准（不对称性、边界、颜色）映射到Vision Transformer嵌入空间，将对齐的表征通过自然语言生成转化为结构化文本解释。

Result: 在公开数据集上达到92.79%的准确率和0.961的AUC，在多个可解释性指标上有显著提升。定性分析显示学习到的嵌入空间排列与临床ABC规则应用一致。

Conclusion: CEFM框架成功弥合了高性能分类与临床信任之间的鸿沟，通过将临床语义与视觉特征对齐，为黑色素瘤诊断提供了透明可解释的解决方案。

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [133] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

Relevance: 65.0

TL;DR: NexusFlow是一个轻量级即插即用框架，用于处理异构任务的部分监督多任务学习，通过可逆耦合层对齐任务特征分布，在自动驾驶和室内场景任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有部分监督多任务学习方法主要关注同构密集预测任务，而现实场景中任务结构多样且标注不完整，需要处理异构任务的知识迁移问题。

Method: 引入代理网络和可逆耦合层，将不同任务的特征分布对齐到共享规范空间，保持信息完整性，避免表示崩溃，支持异构任务间的知识迁移。

Result: 在nuScenes自动驾驶数据集上取得SOTA结果，在NYUv2室内场景数据集上所有任务均获得一致性能提升，验证了框架的通用性。

Conclusion: NexusFlow能够有效处理异构任务的部分监督多任务学习，通过可逆特征对齐实现跨任务知识迁移，具有广泛的适用性。

Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [134] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

Relevance: 65.0

TL;DR: RefBench-PRO：一个全面的指代表达理解基准，将指代表达分解为感知和推理两个核心维度，细分为六个渐进挑战性任务，并提出RL-based学习方案Ref-R1提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有REC基准主要评估感知能力，缺乏可解释的评分机制，无法揭示多模态大语言模型在不同认知能力上的grounding能力。需要更全面的基准来评估MLLM在指代表达理解上的表现。

Method: 1. 提出RefBench-PRO基准，将指代表达分解为感知（属性、位置）和推理（交互、常识、关系、拒绝）两个维度六个子任务；2. 开发全自动数据生成管道，生成多样化的指代表达；3. 提出Ref-R1学习方案，采用基于动态IoU的GRPO（Group Relative Policy Optimization）提升复杂推理条件下的定位精度。

Result: RefBench-PRO能够对MLLM在指代表达理解上进行可解释评估，在感知和推理维度都提出了更大挑战。Ref-R1在复杂推理条件下建立了更强的REC基线。

Conclusion: RefBench-PRO为MLLM的指代表达理解提供了更全面的评估框架，通过分解认知维度实现可解释性评估，同时提出的Ref-R1方法提升了模型在复杂推理条件下的定位性能。

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [135] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

Relevance: 65.0

TL;DR: CLUENet是一种用于视觉语义理解的透明深度架构，通过聚类注意力机制在准确率、效率和可解释性之间取得平衡，在CIFAR-100和Mini-ImageNet上优于现有聚类方法和主流视觉模型。


<details>
  <summary>Details</summary>
Motivation: 卷积和注意力模型在视觉任务中虽然成功，但其刚性感受野和复杂架构限制了它们对不规则空间模式的建模能力，并阻碍了可解释性，这对于需要高模型透明度的任务构成了挑战。聚类范式提供了有前景的可解释性和灵活的语义建模，但存在准确率有限、效率低和训练中梯度消失的问题。

Method: 提出了CLUENet（聚类注意力网络），包含三个关键创新：(1) 全局软聚合和硬分配，使用温度缩放余弦注意力和门控残差连接增强局部建模；(2) 块间硬共享特征分发；(3) 改进的聚类池化策略。

Result: 在CIFAR-100和Mini-ImageNet上的实验表明，CLUENet优于现有的聚类方法和主流视觉模型，在分类性能和视觉可解释性方面都有显著提升。

Conclusion: CLUENet为视觉语义理解提供了一种透明深度架构，在准确率、效率和透明度之间取得了令人信服的平衡，解决了现有聚类方法和传统视觉模型的局限性。

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [136] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

Relevance: 65.0

TL;DR: NeuroABench是首个专门评估神经外科解剖理解的多模态基准，包含9小时标注视频，覆盖89种手术和68个解剖结构。实验显示当前MLLMs在解剖识别任务上表现有限（最佳模型40.87%准确率），与神经外科学员（平均46.5%）仍有差距。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在手术视频理解研究中主要关注手术流程和工作流，而忽视了临床实践中至关重要的解剖理解能力。外科医生依赖精确的解剖知识来解读、回顾和学习手术视频，因此需要专门的基准来评估MLLMs的解剖理解能力。

Method: 1) 创建NeuroABench基准：包含9小时标注的神经外科视频，覆盖89种不同手术；2) 开发新颖的多模态标注流程，采用多轮审核循环；3) 评估68个临床解剖结构的识别能力；4) 在10多个最先进的MLLMs上进行实验；5) 提取数据集子集，与4名神经外科学员进行对比测试。

Result: 1) 最佳MLLM在解剖识别任务上仅达到40.87%准确率；2) 神经外科学员测试结果：最佳学员56%，最低28%，平均46.5%；3) 最佳MLLM表现与最低分学员相当，但显著低于学员平均表现；4) 揭示了MLLMs在解剖理解方面的显著局限性。

Conclusion: NeuroABench填补了手术视频理解中解剖评估的空白，揭示了当前MLLMs在解剖理解方面与人类专家存在显著差距。虽然MLLMs已取得进展，但要达到人类水平的解剖理解仍需重大改进。

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [137] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出Omni-Referring Image Segmentation (OmniRIS)任务，支持文本指令和带掩码/框/涂鸦的参考图像作为全模态提示，实现高度泛化的图像分割。构建了OmniRef数据集和OmniSegNet基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有分割任务多为单模态条件（如仅文本或仅视觉），无法充分利用文本和视觉模态的互补优势。文本擅长细粒度属性描述，视觉擅长罕见物体定位，需要统一框架实现高度泛化的图像分割。

Method: 1) 提出OmniRIS任务框架，支持文本指令和视觉参考图像（带掩码、边界框或涂鸦）作为全模态提示；2) 构建OmniRef数据集（186,939个全模态提示，30,956张图像）；3) 设计OmniSegNet基线模型，解决全模态提示编码等关键挑战。

Result: 实验验证了OmniSegNet能够有效遵循全模态指令，并展示了OmniRIS在高度泛化图像分割方面的优越性。支持多种分割设置（一对一、多对多），提升了实际应用价值。

Conclusion: OmniRIS通过融合文本和视觉模态的优势，实现了更灵活、更泛化的图像分割。OmniRef数据集和OmniSegNet模型为这一新研究方向提供了坚实基础。

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [138] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

Relevance: 65.0

TL;DR: Saber是一个无需显式参考图像-视频-文本三元组数据的零样本参考到视频生成框架，仅使用视频-文本对训练，通过掩码训练策略和注意力设计实现身份一致性和参考感知表示


<details>
  <summary>Details</summary>
Motivation: 当前参考到视频生成方法依赖昂贵的显式参考图像-视频-文本三元组数据，这些数据的构建成本高且难以扩展，限制了方法的可扩展性

Method: 提出Saber框架：1) 仅使用视频-文本对训练，无需显式R2V数据；2) 采用掩码训练策略学习身份一致性和参考感知表示；3) 设计专门的基于注意力的模型架构；4) 集成掩码增强技术减轻复制粘贴伪影

Result: 在OpenS2V-Eval基准测试中优于使用R2V数据训练的方法，展现出对不同数量参考图像的强大泛化能力

Conclusion: Saber通过零样本框架解决了R2V数据稀缺问题，实现了可扩展的参考到视频生成，在保持身份一致性的同时避免了昂贵的显式数据收集

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [139] [DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection](https://arxiv.org/abs/2512.07078)
*Bo Gao,Jingcheng Tong,Xingsheng Chen,Han Yu,Zichen Li*

Main category: cs.CV

Relevance: 65.0

TL;DR: DFIR-DETR：一种用于跨场景小目标检测的动态特征聚合与频域处理检测器，在无人机遥感图像和工业缺陷检测中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 无人机遥感图像中的小目标检测和工业表面缺陷识别面临共同挑战：特征稀疏且弱、背景杂乱、目标尺度变化大。现有基于Transformer的检测器存在三个关键问题：1）网络下采样导致特征严重退化；2）空间卷积无法有效捕获长程依赖；3）标准上采样方法导致特征图不必要膨胀。

Method: 提出DFIR-DETR架构，包含三个核心模块：1）DCFA模块使用动态K稀疏注意力（复杂度从O(N²)降至O(NK)）和空间门控线性单元增强非线性建模；2）DFPN模块采用幅度归一化上采样防止特征膨胀，使用双路径洗牌卷积保留跨尺度空间细节；3）FIRC3模块在频域操作，实现全局感受野而不牺牲效率。

Result: 在NEU-DET和VisDrone数据集上分别达到92.9%和51.6%的mAP50，均为当前最优结果。模型轻量化，仅11.7M参数和41.2 GFLOPs，在资源受限环境下表现优异。

Conclusion: DFIR-DETR通过动态特征聚合和频域处理有效解决了小目标检测的关键挑战，在两个差异很大的领域都表现出强大的泛化能力和实用性，特别适合资源受限的跨场景小目标检测应用。

Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.

</details>


### [140] [TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning](https://arxiv.org/abs/2512.07135)
*Zebin Xing,Pengxuan Yang,Linbo Wang,Yichen Zhang,Yiming Hu,Yupeng Zheng,Junli Wang,Yinfeng Gao,Guang Li,Kun Ma,Long Chen,Zhongpu Xia,Qichao Zhang,Hangjun Ye,Dongbin Zhao*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文提出了一种改进的自动驾驶规划方法，通过混合专家模型适应不同场景的轨迹先验，并使用强化学习优化轨迹评分机制，在navsim ICCV基准测试中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统虽然使用轨迹先验能提升规划性能，但存在两个关键问题：1) 轨迹先验在不同驾驶场景中差异很大，需要适应性调整；2) 轨迹评估机制缺乏策略驱动的优化，受限于单阶段监督训练的局限性。

Method: 1) 使用混合专家模型为不同场景应用不同的轨迹先验；2) 利用强化学习微调轨迹评分机制；3) 集成不同感知骨干模型以增强感知特征。

Result: 在navsim ICCV基准测试中获得51.08分，排名第三，证明了方法的有效性。

Conclusion: 通过场景自适应的轨迹先验和强化学习优化的评分机制，可以显著提升自动驾驶规划系统的性能。

Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.

</details>


### [141] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

Relevance: 65.0

TL;DR: CUHK-X是一个用于人类动作识别、理解和推理的大规模多模态数据集和基准套件，包含58,445个样本，涵盖40个动作，通过提示式场景创建方法生成逻辑一致的文本描述，解决了现有LVLMs在处理非RGB模态数据时缺乏大规模数据-描述资源的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在处理深度、IMU、毫米波等非RGB模态数据时面临挑战，主要原因是缺乏大规模的数据-描述配对资源。现有的人类动作识别数据集通常只提供粗粒度的数据标签注释，不足以捕捉动作理解（HAU）和动作推理（HARn）所需的细粒度动作动态。

Method: 提出CUHK-X数据集，包含58,445个样本，涵盖30名参与者在两个室内环境中执行的40个动作。采用基于提示的场景创建方法，利用LLMs生成逻辑连贯的活动序列描述，然后进行人工验证，确保文本描述的逻辑和时空一致性。

Result: 实验结果显示，在CUHK-X基准套件上的平均准确率为：人类动作识别（HAR）76.52%，人类动作理解（HAU）40.76%，人类动作推理（HARn）70.25%。该数据集支持六种评估任务，为多模态人类活动分析提供了数据密集型学习方法的基础。

Conclusion: CUHK-X填补了多模态人类动作分析领域的数据空白，特别是为非RGB模态数据提供了高质量的数据-描述配对资源，有助于推动数据密集型学习方法在鲁棒的多模态人类活动分析中的应用和发展。

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [142] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文比较了基于CLIP和DINOv2的视觉基础模型在手持物体抓取场景中的3D姿态估计性能，发现CLIP在语义理解方面更优，而DINOv2在几何特征提取方面更强。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型(VFMs)和视觉语言模型(VLMs)为计算机视觉提供了丰富的语义和几何表示。本研究旨在系统比较CLIP和DINOv2这两种主流视觉模型在机器人抓取场景中的3D姿态估计性能，为实际应用提供模型选择指导。

Method: 在手持物体抓取场景中，对基于CLIP和DINOv2的方法进行全面的视觉比较。在6D物体姿态估计任务上评估两种模型，使用基准数据集进行大量实验，分析它们在语义理解和几何特征提取方面的表现。

Result: 实验表明两种模型具有互补优势：基于CLIP的方法通过语言接地实现了更好的语义一致性，而基于DINOv2的方法展示了竞争性的性能并具有增强的几何精度。

Conclusion: 该分析为机器人操作和抓取应用中选择合适的视觉模型提供了见解，强调了根据任务需求（语义理解vs几何精度）选择适当模型的重要性。

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [143] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出UCPE（统一相机位置编码），通过相对射线编码和绝对方向编码，在视频扩散Transformer中实现精确相机控制，仅增加<1%可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有相机编码方法通常基于简化的针孔假设，限制了在真实世界相机多样内参和镜头畸变下的泛化能力。需要一种几何一致的表示来统一完整的相机信息。

Method: 1) 相对射线编码：几何一致表示，统一6自由度位姿、内参和镜头畸变；2) 绝对方向编码：识别俯仰和横滚作为有效控制分量；3) 轻量空间注意力适配器：集成到预训练视频扩散Transformer中。

Result: 在相机可控文本到视频生成任务中达到最先进的相机控制能力和视觉保真度。构建了覆盖广泛相机运动和镜头类型的大型视频数据集。

Conclusion: UCPE在相机可控视频生成中有效，有潜力作为Transformer在未来的多视图、视频和3D任务中的通用相机表示。

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [144] [Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts](https://arxiv.org/abs/2512.07302)
*Mingning Guo,Mengwei Wu,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.CV

Relevance: 65.0

TL;DR: AerialVP是一个用于无人机图像感知的任务提示增强代理框架，通过主动提取多维辅助信息来增强任务提示，解决传统VLM方法在无人机图像中面临的挑战


<details>
  <summary>Details</summary>
Motivation: 传统基于VLM的图像感知方法依赖于用户提供的文本任务提示，但在无人机图像中面临目标混淆、尺度变化和复杂背景等挑战。当任务提示简单而图像内容复杂时，视觉和文本token之间的语义对齐变得困难，限制了模型关注任务相关信息的能力。

Method: AerialVP框架包含三个阶段：1) 分析任务提示以确定任务类型和增强需求；2) 从工具库中选择适当工具；3) 基于分析和选定工具生成增强的任务提示。框架主动从无人机图像中提取多维辅助信息来增强任务提示。

Result: 实验结果表明，AerialVP显著增强了任务提示的指导能力，在开源和专有VLM中都带来了稳定且显著的性能提升。作者还引入了AerialSense基准，包含无人机视觉推理、视觉问答和视觉定位任务。

Conclusion: AerialVP通过主动提取无人机图像的多维辅助信息来增强任务提示，有效解决了传统VLM方法在无人机图像感知中的局限性，为复杂视觉场景下的VLM应用提供了新思路。

Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.

</details>


### [145] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出LogicCBM，将概念瓶颈模型与命题逻辑结合，通过可微逻辑操作增强概念表达能力，超越简单的线性组合，提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型(CBMs)主要通过线性组合概念进行预测，表达能力有限。作者希望增强概念学习模型，使其能够捕捉概念间的复杂关系，超越简单的加权组合。

Method: 引入逻辑模块，将CBM学习的概念通过可微逻辑操作连接起来。LogicCBM使用逻辑运算符组合概念，保持端到端可学习性，同时提高模型在逻辑操作方面的表达能力。

Result: 在知名基准测试和合成数据集上的实验表明，LogicCBM具有更好的准确性，能够执行有效的干预，并且具有高度可解释性。

Conclusion: 通过将命题逻辑集成到概念瓶颈模型中，LogicCBM能够超越简单的线性概念组合，捕捉概念间关系，提高模型表达能力和性能，同时保持可解释性优势。

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [146] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

Relevance: 65.0

TL;DR: MultiMotion是一个用于多目标视频运动传输的统一框架，通过Mask-aware Attention Motion Flow（AMF）在DiT架构中实现多目标运动的解耦与控制，并引入RectPC高效采样器，在专门构建的基准数据集上取得了精确、语义对齐且时间一致的多目标运动传输效果。


<details>
  <summary>Details</summary>
Motivation: 当前Diffusion Transformer（DiT）架构在多目标视频运动传输中存在运动纠缠和缺乏目标级控制的问题，难以实现多个独立物体的精确运动控制。

Method: 1. Mask-aware Attention Motion Flow（AMF）：利用SAM2掩码在DiT流程中显式解耦和控制多个目标的运动特征；2. RectPC：高阶预测-校正求解器，用于高效准确的采样，特别适用于多实体生成；3. 构建首个专门用于DiT多目标运动传输的基准数据集。

Result: MultiMotion实现了精确、语义对齐且时间一致的多目标运动传输，保持了DiT的高质量和可扩展性，在多目标运动控制方面显著优于现有方法。

Conclusion: 该工作通过AMF机制和RectPC采样器解决了DiT在多目标运动传输中的核心挑战，为视频生成中的多目标控制提供了有效的解决方案，推动了DiT在复杂视频生成任务中的应用。

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [147] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

Relevance: 65.0

TL;DR: SAVE框架通过稀疏自编码器特征引导来减少多模态大语言模型中的物体幻觉问题，无需训练即可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然取得了显著进展，但仍然容易受到语言先验和视觉信息丢失导致的物体幻觉问题的影响。现有方法在缓解这一问题上存在局限，需要更有效的解决方案。

Method: 提出SAVE框架：1）使用稀疏自编码器提取潜在特征；2）通过二元物体存在问答探针识别最能反映模型视觉信息处理的"视觉理解特征"；3）沿着这些识别出的特征引导模型，增强基于视觉的理解能力。

Result: 在标准基准测试中优于最先进的无训练方法：CHAIR_S提升10个百分点，在POPE和MMHal-Bench上获得一致增益。跨多个模型和层的广泛评估证实了方法的鲁棒性和泛化性。

Conclusion: SAVE通过稀疏自编码器特征引导有效减少多模态大语言模型的物体幻觉，分析表明该方法通过抑制不确定物体标记的生成和增加对图像标记的关注来缓解幻觉问题。

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [148] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文提出了一个列表式学习框架，通过结合生成式建模和判别式排序来改进视频问答中的答案生成质量，使用掩码指针交叉熵损失与排序权重进行候选答案重排序。


<details>
  <summary>Details</summary>
Motivation: 视频问答中，现有方法在语义精度和排序一致性方面存在不足，特别是对于需要时间推理和语义消歧的问题。需要一种能够生成连贯、细粒度答案列表的方法。

Method: 1) 基础多模态模型首先生成多个候选答案；2) 使用新颖的Masked Pointer Cross-Entropy Loss with Rank Weights训练的重排序模型对候选答案进行排序；3) 该目标整合了指针式候选选择、排序依赖权重和词汇限制下的掩码交叉熵。

Result: 实验显示在准确性和排序稳定性方面获得了一致的提升，特别是在需要时间推理和语义消歧的问题上效果显著。

Conclusion: 通过桥接生成式建模和判别式排序，该方法能够产生连贯、细粒度的答案列表，实现了稳定的列表式优化。

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [149] [DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)
*Jialv Zou,Shaoyu Chen,Bencheng Liao,Zhiyu Zheng,Yuehao Song,Lefei Zhang,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: DiffusionDriveV2使用强化学习改进端到端自动驾驶的扩散模型，解决模式坍塌问题，在保持多样性的同时提升轨迹质量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的端到端自动驾驶方法存在模式坍塌问题，倾向于生成保守、同质化的行为。虽然DiffusionDrive使用预定义锚点划分动作空间来生成多样轨迹，但其依赖模仿学习缺乏足够约束，导致多样性与一致高质量之间的困境

Method: 提出DiffusionDriveV2，利用强化学习约束低质量模式并探索更优轨迹：1) 使用尺度自适应乘性噪声促进广泛探索；2) 采用锚点内GRPO管理单个锚点样本的优势估计；3) 锚点间截断GRPO整合不同锚点的全局视角，避免不同意图间的不当优势比较

Result: 在NAVSIM v1数据集上获得91.2 PDMS，在NAVSIM v2数据集上获得85.5 EPDMS，创下新纪录。实验验证该方法解决了截断扩散模型中多样性与一致高质量之间的困境，实现了最佳权衡

Conclusion: DiffusionDriveV2通过强化学习显著提升了端到端自动驾驶扩散模型的整体输出质量，同时保持了高斯混合模型固有的多模态特性，解决了模式坍塌问题

Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2

</details>


### [150] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

Relevance: 65.0

TL;DR: DMVAE通过分布匹配约束将编码器的潜在分布与任意参考分布对齐，超越了传统VAE的高斯先验，发现SSL衍生分布在重建保真度和建模效率之间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型（如VAE和基础模型对齐编码器）隐式约束潜在空间而不显式塑造其分布，不清楚哪种分布最适合建模。需要明确研究潜在分布对建模效果的影响。

Method: 提出Distribution-Matching VAE (DMVAE)，通过分布匹配约束将编码器的潜在分布与任意参考分布对齐，支持自监督特征、扩散噪声或其他先验分布。

Result: SSL衍生分布在重建保真度和建模效率之间达到最佳平衡，在ImageNet上仅用64个训练周期达到gFID=3.2。分布级对齐比固定先验更能弥合易建模潜在空间与高保真图像合成之间的差距。

Conclusion: 选择合适的潜在分布结构（通过分布级对齐实现）而非依赖固定先验，是弥合易建模潜在空间与高保真图像合成之间差距的关键。

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [151] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

Relevance: 65.0

TL;DR: FAE提出了一种简单有效的框架，将预训练视觉表征适配到适合生成的低维潜在空间，仅需单层注意力层，同时保留重建和理解所需信息。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型通常在压缩潜在空间中运行以平衡训练效率和样本质量。现有方法尝试利用高质量预训练视觉表征，但由于理解导向特征与生成友好潜在空间之间的不匹配，适配这些表征具有挑战性。表征编码器受益于高维潜在空间以捕获多样化假设，而生成模型偏好低维潜在空间以忠实保留注入噪声。

Method: FAE框架通过耦合两个独立的深度解码器来适配预训练视觉表征：一个训练用于重建原始特征空间，第二个将重建特征作为输入进行图像生成。该方法通用性强，可与多种自监督编码器（如DINO、SigLIP）实例化，并集成到扩散模型和归一化流两种生成模型家族中。

Result: 在类条件和文本到图像基准测试中，FAE表现出色。在ImageNet 256x256上，使用CFG的扩散模型达到接近SOTA的FID 1.29（800轮）和1.70（80轮）。不使用CFG时，FAE达到SOTA的FID 1.48（800轮）和2.08（80轮），展示了高质量和快速学习能力。

Conclusion: FAE提供了一种简单而有效的解决方案，将预训练视觉表征适配到生成友好的低维潜在空间，在保持重建和理解能力的同时，实现了高质量的图像生成和快速收敛。

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [152] [MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment](https://arxiv.org/abs/2512.06628)
*Ruicheng Zhang,Mingyang Zhang,Jun Zhou,Zhangrui Guo,Xiaofan Liu,Zunnan Xu,Zhizhou Zhong,Puxin Yan,Haocheng Luo,Xiu Li*

Main category: cs.RO

Relevance: 65.0

TL;DR: MIND-V是一个用于长时程机器人操作视频生成的分层框架，通过语义推理、行为语义桥接和运动视频生成三个核心组件，结合强化学习后训练确保物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习面临长时程、多样化操作数据稀缺的问题。现有的视频生成模型只能合成简单动作的短视频，且依赖手动定义轨迹，无法满足复杂机器人操作的需求。

Method: 1. 分层框架：包含语义推理中心（SRH，使用预训练视觉语言模型进行任务规划）、行为语义桥（BSB，将抽象指令转换为领域不变表示）、运动视频生成器（MVG，条件视频渲染）
2. 分阶段视觉未来推演：测试时优化策略增强长时程鲁棒性
3. GRPO强化学习后训练：使用物理前瞻一致性（PFC）奖励，利用V-JEPA世界模型在特征空间对齐预测和实际动态演化，确保物理合理性

Result: MIND-V在长时程机器人操作视频生成方面达到了最先进的性能，为具身数据合成建立了可扩展和可控的范式。

Conclusion: 该工作提出了一个结合认知科学原理的分层视频生成框架，通过语义推理和物理约束强化学习，有效解决了长时程机器人操作视频生成的挑战。

Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

</details>


### [153] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

Relevance: 45.0

TL;DR: 训练在发育现实数据集（SAYCam）上的V-JEPA模型在直觉物理基准测试中表现不佳，表明仅靠数据分布变化不足以让当前架构学习直觉物理表示


<details>
  <summary>Details</summary>
Motivation: 人类通过基于直觉物理理解的丰富内部模型来导航世界，而当前深度学习模型尽管在大量互联网视频数据上训练，在直觉物理基准测试中仍达不到人类水平。本研究探讨数据分布（而非数据量）是否是学习这些物理原理的关键。

Method: 在SAYCam数据集上预训练Video Joint Embedding Predictive Architecture (V-JEPA)模型，该数据集是发育现实的、自我中心的视频数据集，部分捕捉了三个儿童的日常视觉体验，数据量仅为SOTA模型训练数据的0.01%。然后在IntPhys2基准测试上评估模型性能。

Result: 训练在这个发育现实数据集上并未带来IntPhys2基准测试性能的显著提升。结果表明，仅训练在发育现实数据集上不足以让当前架构学习支持直觉物理的表示。

Conclusion: 仅改变视觉数据量和分布可能不足以构建具有人工直觉物理的系统。需要更深入理解架构、训练目标或数据特性如何影响直觉物理学习。

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [154] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出MTGC框架，通过多模态引导（文本描述、高压缩图像、语义伪词）增强超低码率下生成式图像压缩的语义一致性，解决生成幻觉问题


<details>
  <summary>Details</summary>
Motivation: 生成式图像压缩在超低码率下存在语义偏差问题，限制了其在6G语义通信中的可靠部署。需要解决生成幻觉导致的语义不一致问题。

Method: MTGC框架整合三种引导模态：文本描述（全局语义）、高压缩图像（低层视觉信息）、语义伪词（细粒度任务相关语义）。设计了任务感知语义压缩模块生成SPWs，以及多模态引导扩散解码器通过双路径协作机制注入引导信息。

Result: 在超低码率下显著提升语义一致性（DISTS指标在DIV2K数据集上下降10.59%），同时改善感知质量和像素级保真度。

Conclusion: MTGC框架有效解决了超低码率生成式图像压缩的语义偏差问题，为6G语义通信提供了可靠的图像压缩解决方案。

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [155] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出双路径Transformer框架，结合CLIP的视觉和文本模态，用于远距离性别识别，在U-DetAGReID数据集上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 远距离图像性别识别面临空间分辨率低、视角变化大、面部特征缺失等挑战，需要更鲁棒的解决方案

Method: 双路径Transformer框架：1) 视觉路径：微调CLIP图像编码器上层；2) 属性路径：通过软生物特征提示（发型、服装等）在CLIP文本-图像空间中推理性别；加入空间通道注意力模块增强判别定位

Result: 在U-DetAGReID数据集上超越现有行人属性和重识别基线，在macro-F1、准确率、AUC等指标上表现优异，对距离、角度、高度变化具有鲁棒性

Conclusion: 语言引导的双路径学习为无约束远距离场景下的负责任性别识别提供了原则性、可扩展的基础

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [156] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: eess.IV

Relevance: 45.0

TL;DR: 验证并扩展了医学图像分类方法框架，将改进的ConvNeXt Tiny架构（集成GAGM、SEVector和FSL）从阿尔茨海默症MRI分类迁移到乳腺X光分类，发现GAGM和SEVector有效但FSL无效，并扩展了多指标评估和可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 验证先前在阿尔茨海默症MRI分类中表现良好的改进ConvNeXt Tiny架构（包含GAGM、SEVector和FSL）在乳腺X光分类任务中的可迁移性，并扩展该方法框架。

Method: 使用Kaggle数据集（整合INbreast、MIAS和DDSM乳腺X光数据集），比较基线CNN、ConvNeXt Tiny和InceptionV3骨干网络，并集成GAGM（全局平均与最大池化融合）和SEVector（轻量级通道注意力）模块。进行多指标评估（宏F1、每类召回方差、ROC/AUC）、特征可解释性分析（Grad-CAM）和开发交互式临床探索仪表板。

Result: GAGM和SEVector模块有效增强了特征可区分性并减少了假阴性（特别是恶性病例），但特征平滑损失（FSL）在乳腺X光分类条件下未带来可测量的改进。扩展了多指标评估框架并提供了特征可解释性分析。

Conclusion: 验证了GAGM和SEVector模块在医学图像分类中的有效性，但发现FSL的效果依赖于特定架构和计算假设。研究扩展了原始框架，并指出需要探索替代方法来提高类内紧凑性和类间可分离性，特别是增强乳腺X光分类中恶性与良性病例的区分能力。

Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>


### [157] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

Relevance: 45.0

TL;DR: 该研究提出了一种结合Transformer和BiLSTM的遥感预测框架，用于提前14天预测蓝藻水华强度，利用稀疏卫星数据取得了良好预测性能。


<details>
  <summary>Details</summary>
Motivation: 蓝藻有害藻华对水生生态系统和公共健康构成严重威胁，特别是在Lake Champlain等地区。遥感技术为解决现场观测稀疏问题提供了可扩展的监测方案，但现有数据存在大量缺失，需要开发能够处理稀疏数据的预测模型。

Method: 1. 使用Cyanobacterial Assessment Network的蓝藻指数数据和MODIS卫星温度数据
2. 采用两阶段预处理：像素级前向填充和加权时间插补，然后平滑处理
3. 特征工程：蓝藻指数值等频分箱，提取温度统计特征
4. 结合Transformer和BiLSTM的混合模型架构，捕捉长时间依赖和序列动态

Result: 模型在不同预测时间窗口表现出色：1天预测F1分数89.5%，2天86.4%，3天85.5%，14天预测仍保持78.9%的F1分数和82.6%的AUC。模型能够从稀疏卫星数据中捕捉复杂的时空动态。

Conclusion: 该研究证明了Transformer-BiLSTM混合模型能够有效处理稀疏遥感数据，为蓝藻水华提供可靠的早期预警，支持管理决策。模型展示了从稀疏时间序列数据中学习复杂模式的能力。

Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [158] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

Relevance: 45.0

TL;DR: PA-VAD：一种无需真实异常视频的视频异常检测方法，通过合成伪异常视频与真实正常视频配对训练检测器，在标准弱监督设置下达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 实际部署视频异常检测面临真实异常视频稀缺且收集成本高的问题，需要开发无需真实异常视频的训练方法

Method: 1) 使用CLIP选择类别相关初始图像，通过视觉语言模型优化文本提示，调用视频扩散模型合成伪异常视频；2) 设计域对齐正则化模块，结合域对齐和内存使用感知更新，缓解合成异常的过度时空幅度

Result: 在ShanghaiTech上达到98.2%，UCF-Crime上达到82.5%，分别超过最强真实异常方法0.6%和UVAD SOTA方法1.9%

Conclusion: 无需收集真实异常即可实现高精度异常检测，为可扩展部署提供了实用路径

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [159] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

Relevance: 45.0

TL;DR: 该论文提出了衡量超分辨率模型高级保真度（语义一致性）的新标准，构建了首个带保真度标注的数据集，发现基础模型在此任务上表现更好，并通过基于保真度反馈的微调同时提升语义保真度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 当前超分辨率模型虽然能生成视觉质量高的图像，但有时会产生幻觉改变图像内容。现有低层图像质量指标无法有效评估这种高级语义变化，需要建立衡量高级保真度的补充标准来评估生成式SR模型的可靠性。

Method: 1) 构建首个带保真度标注的SR数据集；2) 评估SOTA SR模型在保持高级保真度方面的表现；3) 分析现有图像质量指标与保真度测量的相关性；4) 探索基础模型在此任务上的表现；5) 基于保真度反馈微调SR模型。

Result: 1) 发现现有SR模型在高级保真度方面表现不佳；2) 基础模型在高级保真度评估任务上表现更好；3) 基于保真度反馈微调可以同时提升语义保真度和感知质量。

Conclusion: 高级保真度是评估生成式SR模型可靠性的重要补充标准，基础模型在此任务上具有优势，基于保真度反馈的优化可以同时改善语义一致性和视觉质量。

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [160] [See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement](https://arxiv.org/abs/2512.07251)
*Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon Płotka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

Relevance: 45.0

TL;DR: SMILE是一种解剖感知的扩散模型，用于医学图像增强，通过学习器官形状和对比度动态，仅增强临床相关区域而不改变其他区域。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像增强模型往往过度编辑，可能扭曲器官、产生虚假发现或遗漏小肿瘤，因为它们不理解解剖结构或对比度动态。需要一种能够理解解剖学并仅增强临床相关区域的模型。

Method: 提出SMILE模型，包含三个关键创新：1) 结构感知监督，遵循真实器官边界和对比度模式；2) 无需配准的学习，直接处理未对齐的多期相CT扫描；3) 统一推理，在所有对比度期相中提供快速一致的增强。

Result: 在六个外部数据集上，SMILE在图像质量方面优于现有方法（SSIM提高14.2%，PSNR提高20.6%，FID改善50%），并产生解剖学准确且具有诊断意义的图像。还能提高非对比CT的癌症检测F1分数达10%。

Conclusion: SMILE通过解剖感知的扩散模型实现了更准确、更安全的医学图像增强，在保持解剖学真实性的同时提高了诊断价值。

Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.

</details>


### [161] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

Relevance: 45.0

TL;DR: ContextAnyone是一个上下文感知的扩散框架，通过单张参考图像实现角色一致性的文本到视频生成，解决了现有方法在保持发型、服装、体型等上下文线索方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在保持角色身份一致性方面存在局限，特别是难以保持发型、服装、体型等上下文线索，这些对于视觉连贯性至关重要。

Method: 提出上下文感知扩散框架，联合重建参考图像和生成新视频帧；使用Emphasize-Attention模块选择性增强参考感知特征；采用双引导损失结合扩散和参考重建目标；提出Gap-RoPE位置嵌入分离参考和视频token。

Result: 实验表明ContextAnyone在身份一致性和视觉质量方面优于现有参考到视频方法，能够生成跨多样动作和场景的连贯且保持上下文的角色视频。

Conclusion: ContextAnyone通过上下文感知的扩散框架有效解决了角色一致性视频生成的挑战，在保持广泛上下文线索方面表现出色。

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [162] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

Relevance: 45.0

TL;DR: TD-Attn是一个解决文本到图像扩散模型中先验视角偏置问题的框架，通过3D感知注意力引导和分层注意力调制来提升3D任务中的多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在3D任务中存在先验视角偏置问题，导致不同视角下物体外观不一致。这种偏置使得主题词在跨注意力计算中优先激活先验视角特征，而忽略目标视角条件。

Method: 提出TD-Attn框架，包含两个核心组件：1) 3D感知注意力引导模块(3D-AAG)：构建视角一致的3D注意力高斯分布，强制跨视角的空间一致性；2) 分层注意力调制模块(HAM)：使用语义引导树指导语义响应分析器定位和调制对视角条件敏感的跨注意力层。

Result: 实验表明TD-Attn能显著提升多视角一致性，可作为通用插件增强各种3D任务，同时支持可控和精确的3D编辑。

Conclusion: TD-Attn通过数学分析揭示了先验视角偏置的根源，并提出有效的解决方案，为基于扩散模型的3D生成和编辑任务提供了重要的技术改进。

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [163] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

Relevance: 45.0

TL;DR: InterAgent：首个端到端文本驱动的物理多智能体人形控制框架，通过自回归扩散Transformer和多流块实现多智能体协调，超越现有单智能体方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于单智能体场景，忽视了多智能体交互中必要的物理合理互动。为了填补这一空白，需要开发能够模拟人类社交行为复杂协调的多智能体人形控制框架。

Method: 1. 提出自回归扩散Transformer，配备多流块，解耦本体感知、外部感知和动作以减轻跨模态干扰；2. 引入交互图外部感知表示，显式捕捉细粒度关节到关节的空间依赖；3. 设计稀疏边基注意力机制，动态修剪冗余连接并强调关键的智能体间空间关系。

Result: InterAgent在多个强基线方法中表现优异，实现最先进性能，能够仅从文本提示生成连贯、物理合理且语义忠实的多智能体行为。

Conclusion: InterAgent是首个端到端文本驱动的物理多智能体人形控制框架，通过创新的架构设计成功实现了多智能体协调，为未来研究提供了重要基础。

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [164] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

Relevance: 45.0

TL;DR: DECOMP是一种用于密集预测任务的主动学习采样策略，通过将图像分解为类别特定组件并基于类别置信度采样，提高医学图像标注效率


<details>
  <summary>Details</summary>
Motivation: 现有密集预测任务的主动学习方法存在计算成本高、区域选择不相关、过度依赖不确定性采样等问题，特别是在医学图像标注成本高昂的背景下，需要更高效的区域级标注策略

Method: 提出分解采样(DECOMP)：1) 使用伪标签将图像分解为类别特定组件；2) 从每个类别中采样区域；3) 利用类别预测置信度指导采样过程，确保困难类别获得更多标注

Result: 在ROI分类、2D分割和3D分割任务中，DECOMP一致超越基线方法，能更好地采样少数类别区域并提升这些困难类别的性能

Conclusion: DECOMP通过类别分解和置信度指导的采样策略，有效解决了密集预测任务中主动学习的挑战，显著提高了医学图像标注效率

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [165] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

Relevance: 45.0

TL;DR: NPC提出了一种自动化的负提示方法，通过识别和应用抑制非预期内容的负提示来改善文本-图像对齐，无需额外图像合成即可提升扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成在复杂组合结构或想象性提示上仍存在对齐困难，需要更精确的控制机制来抑制非预期内容生成。

Method: 提出NPC自动化流程：1) 分析交叉注意力模式解释目标/非目标负提示的作用机制；2) 使用验证器-标注器-提议器框架生成候选负提示；3) 通过显著文本空间评分进行排名选择。

Result: 在GenEval++和Imagine-Bench上表现优异：GenEval++达到0.571 vs 基线0.371，在Imagine-Bench上获得最佳整体性能。

Conclusion: 通过指导模型不生成什么内容，NPC为扩散模型提供了原则性、全自动的文本-图像对齐增强路径。

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [166] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

Relevance: 45.0

TL;DR: UltrasODM是一个用于临床超声采集的双流框架，通过运动相似性分组、光流融合Dual-Mamba模块进行6-DoF姿态估计，结合贝叶斯不确定性和显著性图，为操作者提供实时校准的不确定性提示和纠正建议。


<details>
  <summary>Details</summary>
Motivation: 临床超声采集高度依赖操作者，快速探头运动和亮度波动常导致重建误差，降低临床可信度和实用性。需要提高超声重建的可靠性和临床工作流程的安全性。

Method: 1) 对比排序模块按运动相似性分组帧；2) 光流流融合Dual-Mamba时间模块进行鲁棒的6-DoF姿态估计；3) 人机交互层结合贝叶斯不确定性、临床校准阈值和显著性图，当不确定性超过阈值时发出提示建议纠正操作。

Result: 在临床自由手超声数据集上，相比UltrasOM，UltrasODM减少漂移15.2%、距离误差12.1%、Hausdorff距离10.1%，同时生成每帧不确定性和显著性输出。

Conclusion: UltrasODM通过强调透明度和临床医生反馈，提高了重建可靠性，支持更安全、更可信的临床工作流程。

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [167] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

Relevance: 45.0

TL;DR: WorldReel是一个4D视频生成器，通过联合生成RGB帧和4D场景表示（点云图、相机轨迹、密集光流映射），实现原生时空一致性，解决了现有视频生成器在3D不一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成器虽然实现了逼真的视觉效果，但在3D一致性方面存在根本性缺陷。为了解决这一问题，作者提出需要开发能够原生保持时空一致性的4D视频生成方法，以实现更连贯的几何和外观建模。

Method: WorldReel采用联合生成框架，同时输出RGB帧和4D场景表示（包括点云图、相机轨迹、密集光流映射）。通过显式的4D表示强制保持单一底层场景在不同视角和动态内容中的一致性。训练时巧妙结合合成数据和真实数据：合成数据提供精确的4D监督（几何、运动、相机），真实视频贡献视觉多样性和真实感。

Result: 实验表明WorldReel在动态场景和移动相机下的视频生成中达到了新的最先进水平，在几何一致性、运动连贯性指标上优于竞争方法，并减少了视角-时间伪影。该方法能够泛化到野外拍摄的视频，同时保持强大的几何保真度。

Conclusion: WorldReel将视频生成推向4D一致的世界建模，使智能体能够通过单一稳定的时空表示来渲染、交互和推理场景，为实现更一致的视频生成和世界建模奠定了基础。

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [168] [Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics](https://arxiv.org/abs/2512.07224)
*Tianyi Ren,Daniel Low,Pittra Jaengprajak,Juampablo Heras Rivera,Jacob Ruzevick,Mehmet Kurt*

Main category: eess.IV

Relevance: 45.0

TL;DR: 该论文提出使用对比级Shapley值来解释医学图像分割模型，通过评估不同MRI对比度对模型性能的贡献来提供临床可解释的可靠性指标。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在医学图像分割中表现出色，但临床实践中需要可解释性来确保模型的接受和整合。现有研究主要关注梯度方法识别影响区域，而该研究探索更广泛、临床对齐的方法来解释模型性能如何公平地归因于不同的成像对比度。

Method: 使用对比级Shapley值系统扰动模型输入以评估特征重要性。在BraTS 2024数据集上，为四种MRI对比度在四种模型架构中生成Shapley值排名。提出了两个指标：1) 模型与"临床医生"成像排名的一致性；2) 通过交叉验证折叠中Shapley排名方差量化的不确定性。

Result: 较高性能案例(Dice >0.6)与临床排名的一致性显著更高。Shapley排名方差的增加与性能下降相关(U-Net: r=-0.581)。这些指标为模型可靠性提供了临床可解释的代理。

Conclusion: 对比级Shapley值提供了一种临床可解释的方法来评估医学图像分割模型的可靠性，帮助临床医生更好地理解最先进的分割模型，并促进模型在临床实践中的接受和整合。

Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.

</details>


### [169] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

Relevance: 40.0

TL;DR: D³-Predictor：一种无噪声的确定性框架，通过重新构建预训练扩散模型消除随机性，将扩散模型转化为几何先验专家集合，用于密集预测任务


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的密集预测方法存在核心限制：扩散采样的随机噪声与密集预测所需的确定性图像到几何映射不匹配。这种随机噪声会破坏细粒度空间线索，使模型偏向时间步特定的噪声目标，从而破坏有意义的几何结构映射。

Method: 提出D³-Predictor框架：1）将预训练扩散模型重新构建为无随机噪声的确定性框架；2）将扩散网络视为时间步依赖的视觉专家集合；3）通过自监督方式将这些异构先验聚合为单一、干净、完整的几何先验；4）利用任务特定监督将此无噪声先验适配到密集预测任务。

Result: 在多种密集预测任务上的实验表明，D³-Predictor在多样场景中达到竞争性或最先进的性能。同时，它只需要之前方法不到一半的训练数据，并能以单步推理高效执行。

Conclusion: 通过消除扩散模型中的随机噪声，D³-Predictor成功将扩散先验转化为适用于密集预测任务的确定性几何先验，在性能、数据效率和推理速度方面均有显著优势。

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [170] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: AdLift：首个3D高斯泼溅（3DGS）编辑保护方法，通过将严格有界的2D对抗扰动提升到3D高斯表示中，防止任意视角和维度的指令驱动编辑


<details>
  <summary>Details</summary>
Motivation: 扩散模型驱动的3DGS编辑技术虽然推动了3D内容创作，但也使3D资产面临未经授权编辑和恶意篡改的风险。现有针对2D图像的对抗扰动保护方法难以直接应用于3DGS，面临视角泛化保护和可见性与保护能力平衡两大挑战

Method: 提出AdLift方法：1）将严格有界的2D对抗扰动提升到3D高斯表示的保护层；2）使用定制的Lifted PGD进行渐进优化，通过梯度截断和投影梯度约束图像级扰动；3）通过图像到高斯的拟合操作将扰动反向传播到保护高斯参数；4）交替进行梯度截断和图像-高斯拟合，实现跨视角一致的保护

Result: 定性和定量实验表明，AdLift能有效保护3DGS资产免受最先进的指令驱动2D图像和3DGS编辑方法的攻击，在不同视角下保持一致的对抗保护性能，并能泛化到新视角

Conclusion: AdLift是首个针对3DGS的编辑保护框架，成功解决了3D资产保护中的视角泛化和可见性-保护能力平衡问题，为3D内容安全提供了有效解决方案

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [171] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

Relevance: 40.0

TL;DR: MeshRipple：一种新的自回归网格生成方法，通过前沿感知的BFS标记化、扩展预测策略和稀疏注意力全局记忆来解决现有方法中长距离几何依赖断裂的问题，生成具有高表面保真度和拓扑完整性的网格。


<details>
  <summary>Details</summary>
Motivation: 现有自回归网格生成方法将面序列化并训练截断片段，使用滑动窗口推理来应对内存限制，但这种不匹配破坏了长距离几何依赖关系，导致孔洞和碎片化组件。需要解决这一关键限制。

Method: MeshRipple包含三个关键创新：1）前沿感知BFS标记化，将生成顺序与表面拓扑对齐；2）扩展预测策略，保持连贯、连接的表面增长；3）稀疏注意力全局记忆，提供有效无界的感受野来解决长距离拓扑依赖。

Result: MeshRipple能够生成具有高表面保真度和拓扑完整性的网格，在性能上超越了近期强基线方法。

Conclusion: MeshRipple通过集成设计解决了自回归网格生成中的长距离依赖问题，实现了更高质量的网格生成。

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [172] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

Relevance: 40.0

TL;DR: OpenVE-3M是一个开源的大规模高质量指令视频编辑数据集，包含空间对齐和非空间对齐两类编辑任务，并构建了OpenVE-Bench基准测试和OpenVE-Edit模型。


<details>
  <summary>Details</summary>
Motivation: 当前指令图像编辑数据集质量不断提高，但大规模高质量的指令视频编辑数据集仍然稀缺。为了解决这一差距，需要构建一个开源、大规模、高质量的指令视频编辑数据集。

Method: 1. 构建OpenVE-3M数据集：包含空间对齐编辑（全局风格、背景变化、局部变化、局部移除、局部添加、字幕编辑）和非空间对齐编辑（摄像机多镜头编辑和创意编辑）；2. 设计精心设计的数据流水线并进行严格质量过滤；3. 构建OpenVE-Bench基准测试，包含431个视频编辑对，涵盖多样化编辑任务；4. 训练OpenVE-Edit模型（5B参数）。

Result: 1. OpenVE-3M在规模、编辑类型多样性、指令长度和整体质量上超越现有开源数据集；2. OpenVE-Bench提供了与人类判断高度一致的三个关键指标；3. OpenVE-Edit模型在OpenVE-Bench上达到新的SOTA，超越了包括14B基线在内的所有先前开源模型。

Conclusion: 该工作填补了指令视频编辑领域的数据集空白，提供了高质量的数据集、基准测试和高效模型，为视频编辑研究提供了重要基础设施。

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [173] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种针对边缘设备大规模数据集存储和通信成本的新颖数据集量化方法，通过减少样本内冗余来压缩数据集，同时保持模型训练性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备中大规模数据集的存储和通信成本挑战。传统数据集剪枝和蒸馏方法主要关注样本间冗余，而本文方法专注于减少每个样本内部的冗余或信息量较少的内容，同时保留关键特征。

Method: 1) 首先应用线性对称量化获取每个样本的初始量化范围和尺度；2) 引入自适应量化分配算法，为具有不同精度要求的样本分配不同的量化比率，同时保持恒定的总压缩比。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K上的实验验证了方法的有效性。结果显示，在相同压缩比下，该方法在保持模型训练性能的同时实现了显著的数据集压缩，优于传统量化和数据集剪枝基线方法。

Conclusion: 该方法首次使用有限比特表示数据集以实现存储减少，引入了具有自适应比率分配的数据集级量化算法，为边缘设备上的大规模数据集管理提供了有效的解决方案。

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [174] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

Relevance: 35.0

TL;DR: FishDetector-R1是一个基于多模态大语言模型的弱监督框架，用于水下鱼类检测、分割和计数，在DeepFish数据集上显著提升性能，并具有良好的跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类图像分析对生态监测至关重要，但面临视觉质量下降和标注成本高昂的挑战。现有方法难以在弱监督条件下实现准确检测、分割和计数。

Method: 提出统一的MLLM框架，包含两个关键组件：1) 检测到计数的提示机制，确保空间一致的检测和计数；2) 可验证奖励的强化学习(RLVR)，利用稀疏点标签的可扩展范式。

Result: 在DeepFish数据集上，相比基线方法，AP提升20%，mIoU提升10%，MAE降低30%，GAME降低35%。消融实验验证了奖励设计的有效性，且方法在其他水下数据集上泛化良好。

Conclusion: FishDetector-R1通过弱监督为海洋视觉理解提供了可靠且可扩展的解决方案，在准确性和跨域鲁棒性方面表现优异。

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [175] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究对两个大规模胸部X光嵌入模型（CXR-Foundation和MedImageInsight）在公开数据集上进行了基准测试，评估它们在医学图像表示学习中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像表示学习方面表现出色，但它们在跨数据集上的比较行为尚未得到充分探索。本研究旨在建立标准化的医学基础模型评估基准。

Method: 使用统一的预处理流程和固定的下游分类器，从预训练编码器中提取嵌入，训练轻量级LightGBM分类器，评估多个疾病标签，报告平均AUROC和F1分数及95%置信区间。

Result: MedImageInsight在大多数任务中表现略优，而CXR-Foundation展现出更强的跨数据集稳定性。MedImageInsight嵌入的无监督聚类显示出与定量结果一致的疾病特异性结构。

Conclusion: 研究强调了标准化评估医学基础模型的必要性，并为未来多模态和临床整合研究建立了可复现的基线。

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [176] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出SpectraIrisPAD框架，使用DINOv2 ViT骨干网络结合可学习光谱位置编码、令牌融合和对比学习，用于多光谱虹膜呈现攻击检测，并创建了包含5个近红外波段、8种攻击类型的MSIrPAD数据集。


<details>
  <summary>Details</summary>
Motivation: 虹膜识别虽然准确，但在实际应用中易受呈现攻击（PA）威胁。传统虹膜系统主要在近红外（NIR）波段工作，而多光谱成像能提供互补的反射信息，可增强呈现攻击检测（PAD）方法的泛化能力。

Method: 提出SpectraIrisPAD框架，基于DINOv2 Vision Transformer骨干网络，配备可学习光谱位置编码、令牌融合和对比学习机制，提取区分性的波段特定特征来区分真实虹膜样本和各种欺骗伪影。

Result: SpectraIrisPAD在未见攻击评估协议下始终优于多个最先进的基线方法，在所有性能指标上都表现出色，展示了在检测广泛呈现攻击方面的卓越鲁棒性和泛化能力。

Conclusion: 多光谱成像结合深度学习框架能有效提升虹膜呈现攻击检测的鲁棒性和泛化能力，SpectraIrisPAD为虹膜生物识别系统的安全性提供了有力保障。

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [177] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: Track4DGen是一个两阶段框架，通过将多视角视频扩散模型与基础点跟踪器、混合4D高斯溅射重建器结合，解决稀疏输入生成动态4D对象的难题。核心思想是将跟踪器导出的运动先验显式注入到中间特征表示中，提升时间一致性和跨视角连贯性。


<details>
  <summary>Details</summary>
Motivation: 从稀疏输入生成动态4D对象具有挑战性，需要同时保持外观和运动在视角和时间上的一致性，同时抑制伪影和时间漂移。作者认为视角差异源于监督仅限于像素或潜在空间的视频扩散损失，缺乏明确的时间感知、特征级跟踪指导。

Method: 提出Track4DGen两阶段框架：第一阶段在多视角视频扩散生成器中强制执行密集特征级点对应关系，产生时间一致的特征；第二阶段使用混合运动编码重建动态4D高斯溅射，将扩散特征（携带第一阶段跟踪先验）与Hex-plane特征连接，并用4D球谐函数增强以进行更高保真度的动态建模。

Result: Track4DGen在多视角视频生成和4D生成基准测试中均超越基线方法，产生时间稳定、可文本编辑的4D资产。此外，作者还策划了Sketchfab28高质量数据集，用于基准测试对象中心的4D生成并促进未来研究。

Conclusion: 通过显式注入跟踪器导出的运动先验到中间特征表示中，Track4DGen有效解决了动态4D对象生成中的时间一致性和跨视角连贯性问题，为4D内容生成提供了新的解决方案。

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [178] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

Relevance: 35.0

TL;DR: FacePhys：基于时空状态空间对偶性的内存高效远程光电容积描记算法，解决了模型可扩展性、跨数据集泛化和实时操作的三难问题，在误差降低49%的同时实现3.6MB内存占用和9.46ms每帧延迟。


<details>
  <summary>Details</summary>
Motivation: 基于摄像头的生命体征测量技术（如远程光电容积描记rPPG）为舒适、普适的健康监测提供了可能，但实际部署面临两大挑战：前端设备计算资源有限，以及数据压缩传输导致信号质量下降和精度降低。

Method: 提出FacePhys算法，基于时空状态空间对偶性设计，利用可转移的心脏状态来捕捉视频帧间的细微周期性变化，同时保持最小计算开销。支持长视频序列训练和低延迟推理。

Result: FacePhys实现了新的最先进性能，误差降低49%。实时推理内存占用仅3.6MB，每帧延迟9.46ms，比现有方法提升83%到99%。在实际部署中表现出可靠的实时性能。

Conclusion: FacePhys通过创新的状态空间设计解决了rPPG实际部署中的关键挑战，在精度、效率和实时性方面取得显著突破，为前端设备上的实时健康监测提供了可行解决方案。

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [179] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: StrokeNet：一种新颖的笔画分类网络架构，通过参考点表示和空间查询机制解决笔画间细粒度语义关系建模问题，在多个在线手写数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 笔画分类面临书写风格变化、内容模糊和动态书写位置等挑战，核心在于建模笔画间的语义关系。现有深度学习方法难以捕捉局部化的笔画交互关系，而点级视角虽能解决但引入冗余。需要一种既能精细表示笔画又能高效建模交互的方法。

Method: 提出StrokeNet架构：1）将笔画编码为参考点对表示（点+特征向量），参考点支持空间查询，特征向量支持交互建模；2）动态选择每个笔画的参考点并序列化，使用Inline Sequence Attention模块构建上下文特征；3）设计Cross-Ellipse Query机制聚类参考点并提取多尺度空间特征；4）联合优化框架同时通过参考点回归预测笔画类别，并通过辅助分支建模相邻笔画语义转移。

Result: 在多个公开在线手写数据集上达到最先进性能。在CASIA-onDo数据集上，准确率从93.81%提升至95.54%，证明了方法的有效性和鲁棒性。

Conclusion: StrokeNet通过参考点表示和空间查询机制有效解决了笔画分类中的细粒度关系建模问题，为在线手写识别提供了新的解决方案，在多个数据集上验证了其优越性。

Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [180] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

Relevance: 35.0

TL;DR: S2WMamba：一种用于全色锐化的新型Mamba架构，通过2D/1D小波变换显式解耦空间和光谱信息，使用Mamba进行跨模态交互，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 全色锐化中，联合处理PAN和MS图像通常会使空间细节与光谱保真度纠缠在一起，导致性能受限。需要一种能够有效解耦空间和光谱信息的方法。

Method: 1) 使用2D Haar DWT处理PAN图像定位空间边缘和纹理；2) 使用通道级1D Haar DWT处理每个像素的光谱作为1D信号分离高低频分量；3) 构建光谱分支和空间分支；4) 使用基于Mamba的跨调制进行长程依赖建模；5) 多尺度动态门自适应融合分支输出。

Result: 在WV3、GF2和QB数据集上，S2WMamba匹配或超越了最近的强基线（FusionMamba、CANNet、U2Net、ARConv），PSNR提升最高达0.23 dB，在完整分辨率WV3上达到HQNR 0.956。

Conclusion: S2WMamba通过显式频率解耦和轻量级跨模态交互，有效解决了全色锐化中的空间-光谱纠缠问题，Mamba架构的线性复杂度使其具有高效性。

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [181] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出SPL-UAD框架，通过可学习的并行提示分支和自适应欺骗上下文提示生成，解耦物理攻击和数字攻击的优化方向，提升统一攻击检测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的人脸识别系统同时面临物理呈现攻击和数字伪造攻击的威胁，现有方法使用CLIP加正则化约束，但存在物理和数字攻击检测优化方向冲突的问题。

Method: 提出SPL-UAD框架：1) 构建可学习的并行提示分支，通过自适应欺骗上下文提示生成独立控制每种攻击类型的优化；2) 设计线索感知增强，利用双提示机制在数据上生成具有挑战性的样本挖掘任务。

Result: 在大型UniAttackDataPlus数据集上的实验表明，该方法在统一攻击检测任务中取得了显著的性能提升。

Conclusion: 通过解耦物理和数字攻击的提示空间优化分支，SPL-UAD框架有效解决了现有方法的优化冲突问题，提升了模型对未见攻击类型的鲁棒性。

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [182] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究为FER2013数据集添加了VAD（Valence-Arousal-Dominance）三维情感标注，特别是首次标注了Dominance维度，并提出使用正交卷积的ResNet网络来提高VAD预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前面部表情识别数据集主要基于离散情感类别（如高兴、愤怒等），表达性有限。未来情感计算需要更全面精确的情感度量，VAD三维参数能提供更丰富的情感表征。虽然AffectNet已包含Valence和Arousal维度，但仍缺乏Dominance维度。

Method: 1) 为FER2013数据集手动标注VAD三维情感参数，特别是Dominance维度；2) 提出基于ResNet的正交卷积回归网络，通过强制卷积核正交化来提取更多样化和表达性强的特征；3) 进行消融实验验证正交卷积对VAD预测的改进效果。

Result: 1) Dominance维度可以被测量，但无论在人工标注还是网络预测中都比Valence和Arousal维度更难获取；2) 正交卷积的引入显著改善了VAD预测性能；3) 创建了首个包含完整VAD标注的FER2013数据集，并提供了基于正交卷积的ResNet基线模型。

Conclusion: 该研究填补了面部表情识别数据集中Dominance维度标注的空白，提出的正交卷积网络能有效提升VAD情感预测精度。新建的VAD标注数据集可作为多维情感度量的基准，正交化回归网络可作为VAD情感预测的基线模型。

Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [183] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

Relevance: 35.0

TL;DR: AGORA：基于3D高斯泼溅的生成对抗网络框架，用于生成可动画化的3D人体化身，实现实时渲染和精细表情控制


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：基于NeRF的隐式表示渲染速度慢且动态一致性差，而3D高斯泼溅方法通常仅限于静态头部生成，缺乏动态控制。需要一种既能保持高质量又能实现实时动画控制的方法。

Method: 提出AGORA框架，将3D高斯泼溅与生成对抗网络结合。核心是轻量级的FLAME条件变形分支，预测每个高斯的残差，实现身份保持和精细表情控制。采用双鉴别器训练方案，利用参数化网格的合成渲染来增强表情保真度。

Result: 在表情准确性上超越最先进的NeRF方法，在单GPU上实现250+ FPS渲染，CPU推理达到约9 FPS，首次展示了实用的CPU-only可动画化3D高斯泼溅化身合成。

Conclusion: AGORA代表了向实用、高性能数字人类迈出的重要一步，实现了视觉真实性和精确可控性的平衡，为VR、远程呈现和娱乐应用提供了可行的解决方案。

Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [184] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出SCD-MLLM框架，基于多模态大语言模型实现稳定跨域抑郁识别，支持异构数据输入并增强对缺失模态的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 抑郁检测具有重要公共卫生意义，但现有音频视频多模态方法缺乏统一通用框架，对真实世界中常见的缺失模态情况稳定性不足

Method: 提出SCD-MLLM框架，包含多源数据输入适配器（MDIA）使用掩码机制和任务特定提示统一异构输入，以及模态感知自适应融合模块（MAFM）通过共享投影机制自适应融合音频视觉特征

Result: 在五个公开抑郁数据集（CMDC、AVEC2014、DAIC-WOZ、DVlog、EATD）上，在完整和部分模态设置下均优于SOTA模型及主流商业LLM（Gemini和GPT），展现优越跨域泛化能力和缺失模态稳定性

Conclusion: SCD-MLLM为多模态抑郁检测提供了统一且稳定的解决方案，在真实世界应用中具有良好泛化能力和鲁棒性

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [185] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

Relevance: 35.0

TL;DR: SUGAR是一个用于3D感知生成模型的框架，支持同时或顺序移除多个身份，无需重新训练整个模型，通过个性化替代潜在表示将不需要的身份重定向到视觉连贯的替代方案。


<details>
  <summary>Details</summary>
Motivation: 随着3D感知生成模型在合成人类身份图像方面取得进展，引发了关于用户同意和从模型输出空间中移除特定个体的紧迫问题。需要一种能够高效移除多个身份而不损害模型质量的方法。

Method: SUGAR框架学习每个身份的个性化替代潜在表示，将不需要的身份重建重定向到视觉连贯的替代方案，同时引入持续效用保持目标以防止随着更多身份被移除而导致的性能退化。

Result: SUGAR在移除多达200个身份方面达到最先进性能，与现有基线相比，保留效用提高了700%。

Conclusion: SUGAR为生成模型中的身份移除问题提供了一个可扩展的解决方案，在保护用户隐私的同时保持了模型的质量和多样性。

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [186] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出了一种分层卷积网络，通过嵌入分类学层次结构来改进硅藻图像的多级分类，相比传统平面分类方法在保持物种级准确率的同时，显著提高了上层分类级别的准确率和错误定位性。


<details>
  <summary>Details</summary>
Motivation: 硅藻的准确分类对水生生态系统监测至关重要，但传统方法严重依赖专家分类学家。现有的深度学习方法大多将硅藻识别视为平面分类，只预测一个分类级别。本研究探索将分类学层次结构嵌入神经网络架构中，是否能同时提高准确率和错误定位性。

Method: 提出了一种分层卷积网络，包含五个级联的头部，分别预测纲、目、科、属、种。每个头部接收共享的主干特征和来自更高层级的概率分布，在训练和推理过程中使用二元掩码将预测限制在有效后代范围内。使用包含1,456张硅藻图像、覆盖82个物种的数据集，在相同设置下比较分层模型和平面模型。

Result: 分层模型在物种级别与平面基线相当（69.4%准确率），但在所有上层分类级别均表现更优。当物种预测失败时，92.5%的错误分类物种在属级别被正确预测（平面基线为67.2%）。分层模型将平均分类学距离降低了38.2%（1.209 vs. 1.955）。渐进式训练显示双向机制：分层约束掩码自上而下约束预测空间，而细粒度级别的梯度自下而上通过共享主干传播，精炼特征。

Conclusion: 嵌入分类学层次结构的分层神经网络能够产生更稳健、可解释且与生物学对齐的预测，适用于多级分类学分类。该方法通过双向机制改进了上层分类级别的准确率，并显著提高了错误定位性。

Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [187] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出使用掩码自编码器（MAE）预训练策略，在模拟的强引力透镜图像上学习可泛化的表示，用于暗物质模型分类和图像超分辨率两个下游任务。


<details>
  <summary>Details</summary>
Motivation: 强引力透镜可以揭示暗物质子结构的影响，但分析噪声大、分辨率低的图像具有挑战性。需要开发能够从这些物理丰富的模拟数据中学习通用表示的方法，以支持多种分析任务。

Method: 使用掩码自编码器（MAE）在DeepLense ML4SCI基准的模拟强透镜图像上进行预训练，采用掩码图像建模目标训练Vision Transformer编码器。然后针对两个下游任务分别微调编码器：暗物质模型分类（冷暗物质、轴子样或无子结构）和图像超分辨率（16×16到64×64）。

Result: 在90%掩码率下，微调的分类器达到宏观AUC 0.968和准确率88.65%，优于从头训练的基线（AUC 0.957，准确率82.46%）。超分辨率任务中，MAE预训练模型重建图像的PSNR约33 dB，SSIM 0.961，略优于从头训练。掩码率分析显示一致的权衡：更高的掩码率改善分类但略微降低重建保真度。

Conclusion: MAE预训练在物理丰富的模拟数据上提供了灵活、可重用的编码器，适用于多种强透镜分析任务，展示了自监督学习在科学计算领域的潜力。

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [188] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

Relevance: 35.0

TL;DR: UniVoiceLite是一个轻量级无监督的视听框架，统一了语音增强和语音分离任务，利用唇部运动和面部身份线索引导语音提取，无需配对噪声-干净数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界音频通常同时包含背景噪声和重叠说话者，需要统一解决方案。现有方法多为多阶段架构，参数复杂且依赖监督训练，限制了可扩展性和泛化能力。

Method: 提出UniVoiceLite框架：1) 利用唇部运动和面部身份线索引导语音提取；2) 使用Wasserstein距离正则化稳定潜在空间；3) 无需配对噪声-干净数据的无监督学习。

Result: 实验结果表明，UniVoiceLite在噪声和多说话者场景下均表现良好，实现了效率与鲁棒泛化能力的结合。

Conclusion: UniVoiceLite为语音增强和语音分离提供了一个轻量级、无监督的统一解决方案，具有良好的实际应用潜力。

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [189] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

Relevance: 35.0

TL;DR: FedSCAl是一个联邦学习框架，用于解决联邦源自由域自适应问题，通过服务器-客户端对齐机制来减少客户端漂移并提高伪标签准确性。


<details>
  <summary>Details</summary>
Motivation: 在联邦源自由域自适应场景中，客户端持有未标记数据且存在显著的域间差异，传统方法难以处理极端数据异质性导致的客户端漂移和不可靠伪标签问题。

Method: 提出FedSCAl框架，采用服务器-客户端对齐机制来正则化客户端更新，通过对齐客户端和服务器模型的预测来缓解客户端漂移。

Result: 在基准视觉数据集上的实验表明，FedSCAl在分类任务中持续优于现有的联邦学习方法，显著提高了伪标签准确性。

Conclusion: FedSCAl通过服务器-客户端对齐机制有效解决了联邦源自由域自适应中的客户端漂移问题，为处理数据异质性提供了有效解决方案。

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [190] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

Relevance: 35.0

TL;DR: UARE是首个统一图像质量评估、修复和增强的视觉语言模型，通过两阶段训练框架实现多任务协同训练，利用质量评估信号指导图像修复和增强。


<details>
  <summary>Details</summary>
Motivation: 图像质量评估(IQA)和图像修复在概念上紧密相关，但现有工作通常将它们分开处理。受统一多模态理解-生成模型的启发，作者认为更强的理解能力可以提升生成性能，因此需要一个统一模型来探索IQA如何指导图像修复和增强。

Method: 基于预训练的统一理解和生成模型，采用两阶段训练框架：1) 渐进式从单一类型失真扩展到高阶混合退化的训练计划；2) 使用交错文本-图像数据进行统一微调，将IQA信号与修复目标对齐，实现多任务协同训练。

Result: 在IQA、修复和增强任务上的广泛实验证明了UARE的有效性，展示了利用IQA提升修复和增强性能的潜力。

Conclusion: UARE成功统一了图像质量评估、修复和增强任务，通过多任务协同训练实现了IQA对修复的指导作用，为低层视觉任务提供了新的统一框架。

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [191] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

Relevance: 35.0

TL;DR: MeshSplatting：一种基于网格的重建方法，通过可微分渲染联合优化几何和外观，将神经渲染与交互式3D图形连接起来，实现实时场景交互。


<details>
  <summary>Details</summary>
Motivation: 现有基于基元的方法（如3D高斯泼溅）虽然实现了实时渲染，但其点基表示与AR/VR和游戏引擎中的网格基流水线不兼容。需要一种能够创建平滑、高质量网格的方法，以桥接神经渲染和交互式3D图形。

Method: 通过受限Delaunay三角剖分强制连接性，并通过可微分渲染联合优化几何和外观，同时细化表面一致性，创建端到端平滑的网格表示。

Result: 在Mip-NeRF360数据集上，相比当前最先进的MiLo方法，PSNR提升+0.69 dB，训练速度快2倍，内存使用减少2倍，能够高效地在实时3D引擎中渲染。

Conclusion: MeshSplatting成功创建了视觉高质量、端到端平滑的网格，桥接了神经渲染和交互式3D图形，为实时场景交互提供了无缝解决方案。

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [192] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出CADE方法，首次将持续学习与弱监督视频异常检测结合，通过双生成器和多判别器集成解决领域偏移和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测方法主要处理静态数据集，忽略了数据领域可能变化的问题。当数据领域发生偏移时，需要持续学习视角，否则仅用新数据训练会导致对先前数据的性能下降（遗忘）。

Method: 提出CADE方法：1）使用双生成器解决WVAD中的数据不平衡和标签不确定性；2）提出多判别器集成，捕捉因遗忘而在过去场景中遗漏的异常模式。

Result: 在ShanghaiTech和Charlotte Anomaly等多场景VAD数据集上的广泛实验表明，CADE显著优于现有的VAD方法。

Conclusion: CADE是首个结合持续学习和弱监督视频异常检测的工作，通过双生成器和多判别器集成有效解决了领域偏移和遗忘问题，在多场景VAD数据集上表现出色。

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [193] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种弱监督方法，仅使用椎体级别的健康/恶性标签（无需病变掩码）进行CT椎体转移瘤分割。方法结合扩散自编码器生成健康编辑图像，通过像素差异图提出候选病变区域，再使用Hide-and-Seek Attribution技术评估各区域的恶性贡献度，最终实现溶骨性和成骨性病变分割。


<details>
  <summary>Details</summary>
Motivation: CT中椎体转移瘤的准确分割在临床上很重要，但难以规模化，因为体素级标注稀缺，且溶骨性和成骨性病变常与良性退行性改变相似。需要开发仅使用弱监督标签的方法来克服标注瓶颈。

Method: 1. 使用扩散自编码器（DAE）生成每个椎体的分类器引导的健康编辑图像；2. 通过像素级差异图提出候选病变区域；3. 引入Hide-and-Seek Attribution技术：依次揭示每个候选区域并隐藏其他区域，将编辑图像投影回数据流形，使用潜在空间分类器量化该组件的孤立恶性贡献；4. 高评分区域形成最终的溶骨性或成骨性分割。

Result: 在保留的放射科医生标注上，尽管没有掩码监督，仍实现了强大的成骨性/溶骨性性能（F1: 0.91/0.85; Dice: 0.87/0.78），超过基线方法（F1: 0.79/0.67; Dice: 0.74/0.55）。

Conclusion: 椎体级标签可以转化为可靠的病变掩码，证明生成编辑与选择性遮挡相结合支持CT中准确的弱监督分割。

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [194] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

Relevance: 35.0

TL;DR: AutoQ-VIS是一个无监督视频实例分割框架，通过质量引导的自训练方法，在不需要人工标注的情况下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割面临像素级掩码和时间一致性标注的双重挑战。现有无监督方法如VideoCutLER虽然通过合成数据消除了光流依赖，但仍受限于合成到真实域的差距。

Method: 提出质量引导的自训练框架，在伪标签生成和自动质量评估之间建立闭环系统，实现从合成视频到真实视频的渐进适应。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的SOTA VideoCutLER提升4.4%，且无需人工标注。

Conclusion: 质量感知的自训练方法对于无监督视频实例分割是可行的，能够有效弥合合成到真实域的差距。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [195] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: ECOCSeg提出了一种基于纠错输出码的语义分割方法，通过细粒度编码和比特级标签去噪机制，在伪标签学习中提高稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在语义分割的伪标签学习中，特别是在无监督域适应和半监督学习场景下，传统的one-hot编码容易放大错误伪标签的问题。为了解决这一挑战，需要一种能够处理部分错误比特、提高伪标签质量的机制。

Method: 提出了ECOCSeg方法：1）引入基于ECOC的分类器，将类别分解为属性，能够处理部分不准确的比特；2）开发比特级标签去噪机制，为未标记图像生成更高质量的伪标签；3）该方法可以轻松集成到现有方法中。

Result: 在多个UDA和SSL基准测试中，ECOCSeg在不同分割架构上均表现出显著改进，证明了其有效性和泛化能力。

Conclusion: ECOCSeg通过ECOC编码提供了一种解决伪标签学习中错误传播问题的新视角，提高了分割模型的稳定性和泛化性能，在标签稀缺场景下具有重要应用价值。

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [196] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出了一种基于卷积混合器范式的轻量级架构，用于遥感场景分类，在AID和EuroSAT基准测试中取得了良好的准确率与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 遥感场景分类在地球观测中至关重要，但现有CNN和ViT模型在空间分辨率、视角、方向和背景条件变化下泛化能力有限，需要更高效且鲁棒的解决方案。

Method: 提出轻量级卷积混合器架构，通过多尺度深度卷积进行空间混合，通过逐点操作进行通道混合，高效提取局部和上下文信息，同时保持低参数量和计算量。

Result: 在AID数据集上获得74.7%总体准确率、74.57%平均准确率和73.79 Kappa值；在EuroSAT上获得93.90%总体准确率、93.93%平均准确率和93.22 Kappa值，优于现有CNN和transformer模型。

Conclusion: 提出的卷积混合器架构在遥感场景分类任务中实现了准确率与效率的良好平衡，为轻量级视觉模型设计提供了新思路。

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [197] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

Relevance: 35.0

TL;DR: JoPano：基于DiT的统一全景图生成框架，通过联合面适配器和条件切换机制，将文本到全景图和视图到全景图两个任务统一在单一模型中，解决了现有方法视觉质量受限和任务独立建模的问题。


<details>
  <summary>Details</summary>
Motivation: 现有全景图生成方法面临两大挑战：1）基于U-Net的架构限制了生成全景图的视觉质量；2）通常将文本到全景图和视图到全景图两个核心任务独立处理，导致建模冗余和效率低下。需要一种既能提升视觉质量又能统一处理两个任务的解决方案。

Method: 提出基于DiT的联合全景图生成框架JoPano：1）使用立方体贴图表示全景图；2）设计联合面适配器，将预训练DiT在自然图像上的生成能力迁移到全景图领域；3）应用泊松融合减少立方体面边界的不一致性；4）引入条件切换机制，在单一模型中统一文本到全景图和视图到全景图任务。

Result: JoPano在文本到全景图和视图到全景图生成任务上都能生成高质量全景图，在FID、CLIP-FID、IS和CLIP-Score等指标上达到最先进性能。同时提出了Seam-SSIM和Seam-Sobel两个新指标来定量评估接缝一致性。

Conclusion: JoPano通过统一架构成功解决了全景图生成中的视觉质量和任务冗余问题，证明了DiT架构在全景图生成领域的有效性，并为多任务生成模型设计提供了新思路。

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [198] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种用于语义分割的自监督学习方法，通过选择性掩码图像重建作为预训练任务，相比随机掩码方法在多个数据集上提升了分割精度


<details>
  <summary>Details</summary>
Motivation: 传统掩码图像建模预训练方法使用随机掩码，但这种方法可能不是最优的。作者希望开发一种更智能的掩码策略，能够利用训练过程中学到的知识来指导掩码选择，从而提高预训练效果

Method: 提出选择性掩码图像重建方法：1）将图像重建预训练分解为迭代步骤；2）在每个迭代中，选择重建损失最高的图像块进行掩码；3）利用已训练模型的知识指导后续掩码选择；4）替代传统的随机掩码增强

Result: 在通用数据集（Pascal VOC和Cityscapes）上比随机掩码方法和监督ImageNet预训练提升2.9%分割精度；在杂草分割数据集（Nassar 2020和Sugarbeets 2016）上提升2.5%；对性能最差的类别提升尤为显著

Conclusion: 选择性掩码图像重建方法为端到端语义分割工作流程提供了有效实用的解决方案，特别适合需要有限模型容量以满足推理速度和计算资源要求的场景

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [199] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

Relevance: 35.0

TL;DR: TransCues：一种用于透明物体分割的Transformer编码器-解码器架构，通过边界特征增强和反射特征增强模块有效结合两种视觉线索，在多个基准数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 玻璃等透明材料在日常生活中普遍存在，但由于其透明性和反射特性，现有分割方法难以将其与不透明材料区分开。人类感知依赖边界和反射物体特征来识别玻璃物体，但现有文献在处理透明物体时未能充分捕捉这两种特性。

Method: 提出TransCues框架，采用金字塔式Transformer编码器-解码器架构，包含两个核心模块：边界特征增强模块和反射特征增强模块。这两个模块以相互促进的方式结合边界和反射这两种强大的视觉线索。

Result: 在多个基准数据集上取得显著提升：Trans10K-v2上mIoU提升4.2%，MSD上提升5.6%，RGBD-Mirror上提升10.1%，TROSD上提升13.1%，Stanford2D3D上提升8.3%。大幅超越现有最佳方法。

Conclusion: 通过有效结合边界和反射特征，TransCues框架能够更好地处理透明物体分割问题，在多个数据集上表现出优越性能，证明了该方法对玻璃物体的有效性。

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [200] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

Relevance: 35.0

TL;DR: CHIMERA是一个零样本扩散模型框架，通过自适应缓存注入和语义锚提示实现平滑图像变形，解决了现有方法过渡突兀和过饱和的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现出色，但在实现平滑且语义一致的图像变形方面仍面临挑战。现有方法由于缺乏自适应结构和语义对齐，往往产生突兀的过渡或过饱和的外观。

Method: 提出CHIMERA框架，将变形定义为缓存反转引导的去噪过程。采用自适应缓存注入（ACI）在DDIM反转期间缓存输入特征并在去噪时自适应重新注入，实现深度和时间自适应的空间语义对齐。使用语义锚提示（SAP）通过视觉语言模型生成共享锚提示作为语义桥梁。还提出了全局-局部一致性评分（GLCS）作为变形导向的评估指标。

Result: 广泛的实验和用户研究表明，CHIMERA比现有方法实现了更平滑、语义更一致的过渡，在图像变形领域建立了新的最先进水平。

Conclusion: CHIMERA通过创新的缓存注入和语义锚提示机制，有效解决了扩散模型在图像变形中的挑战，为图像变形任务提供了新的解决方案。

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [201] [MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation](https://arxiv.org/abs/2512.07165)
*Muyu Xu,Fangneng Zhan,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

Relevance: 35.0

TL;DR: MuSASplat：一种用于稀疏视图3D高斯溅射的轻量级多尺度适配器框架，显著减少训练计算成本同时保持渲染质量


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练3D先验的姿态无关前馈方法需要完整微调大型ViT骨干网络，导致巨大的GPU成本。需要一种既能保持高质量渲染又能大幅降低计算负担的解决方案。

Method: 提出轻量级多尺度适配器，仅微调ViT架构的一小部分参数；引入特征融合聚合器，有效整合多视图特征，避免内存库的高开销。

Result: 在多个数据集上实现最先进的渲染质量，同时显著减少参数数量和训练资源需求，在稀疏输入视图下仍保持高保真度。

Conclusion: MuSASplat通过高效的适配器设计和特征融合机制，为稀疏视图3D高斯溅射提供了一种计算高效且质量优异的解决方案。

Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.

</details>


### [202] [Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification](https://arxiv.org/abs/2512.07190)
*Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种新的拓扑引导医学图像分类框架，通过多尺度、多过滤持续同调特征增强视觉分类骨干网络，显著提升医学图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络在医学图像分类中要么过于关注像素强度特征而忽略基本解剖结构，要么只能通过单参数持续同调捕获简单的拓扑特征。需要一种能捕捉复杂解剖结构的方法。

Method: 1) 计算多分辨率/尺度的立方持续同调图；2) 开发"葡萄园"算法将多个持续同调图整合为单一稳定图；3) 设计基于交叉注意力的神经网络直接处理整合后的持续同调图；4) 将拓扑嵌入与CNN或Transformer特征图融合。

Result: 在三个公共数据集上的评估显示，该方法相比强基线和最先进方法取得了持续且显著的改进，证明了全面拓扑视角对鲁棒和可解释医学图像分类的价值。

Conclusion: 通过将多尺度和多过滤拓扑整合到端到端架构中，该方法增强了模型识别复杂解剖结构的能力，为医学图像分类提供了更全面的拓扑视角。

Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.

</details>


### [203] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该综述首次系统梳理了高效3D/4D高斯泼溅技术，将现有方法分为参数压缩和结构压缩两大类，总结了核心思想、方法论趋势、数据集、评估指标和基准比较，并讨论了当前局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)作为显式表示方法，虽然能实现实时高保真3D重建和新视角合成，但其存储和渲染数百万高斯函数需要巨大的内存和计算资源，在4D动态场景中问题更加严重。因此需要高效压缩技术来减少冗余同时保持重建质量。

Method: 该综述采用系统性分类方法，将高效3D和4D高斯泼溅技术分为两大方向：参数压缩（直接压缩高斯参数）和结构压缩（重新组织高斯结构）。对每个类别总结了核心思想和方法论趋势，并涵盖了广泛使用的数据集、评估指标和代表性基准比较。

Result: 提供了首个统一的3D/4D高效高斯泼溅技术概览，建立了系统分类框架，总结了当前技术发展现状，为研究人员提供了全面的技术路线图。

Conclusion: 高效高斯泼溅技术是解决3DGS内存和计算需求的关键方向，未来需要向可扩展、紧凑和实时的静态与动态3D场景表示发展，该综述为这一领域的研究提供了系统性的指导框架。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [204] [Towards Robust Protective Perturbation against DeepFake Face Swapping](https://arxiv.org/abs/2512.07228)
*Hengyang Yao,Lin Li,Ke Sun,Jianing Qiu,Huiping Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出EOLT框架，通过强化学习学习变换分布，自动优先关键变换并生成实例特定的扰动，显著提升DeepFake人脸交换防护的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DeepFake人脸交换技术带来严重的隐私和安全风险。现有防御方法通过嵌入不可见扰动来保护图像，但这些扰动在面对压缩、调整大小等基本变换时非常脆弱。研究发现防护鲁棒性对训练变换的选择高度敏感，传统的均匀采样期望变换(EOT)方法存在根本性不足。

Method: 提出期望学习变换分布(EOLT)框架，将变换分布作为可学习组件而非固定设计选择。使用策略网络通过强化学习自动优先关键变换，并自适应生成实例特定的扰动，显式建模防御瓶颈同时保持广泛的迁移性。

Result: 在广泛实验中，该方法相比最先进方法实现了显著改进：平均鲁棒性提高26%，在具有挑战性的变换类别上增益高达30%。

Conclusion: EOLT框架通过将变换分布作为可学习组件，显著提升了DeepFake防护的鲁棒性，为对抗性防御提供了更有效的解决方案。

Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.

</details>


### [205] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: ReLKD是一个用于广义类别发现（GCD）的端到端框架，通过利用隐式的类间关系来增强新类别的分类性能，特别在标注数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: GCD面临将未标注数据（包含已知和新类别）进行分类的挑战，但现有方法通常独立处理每个类别，忽略了类间关系。在现实场景中直接获取这些关系具有挑战性，因此需要开发能够有效利用隐式类间关系的方法。

Method: ReLKD包含三个关键模块：1）目标粒度模块学习判别性表示；2）粗粒度模块捕获层次化的类间关系；3）蒸馏模块将粗粒度模块的知识转移到目标粒度模块，以优化表示学习。

Result: 在四个数据集上的广泛实验证明了ReLKD的有效性，特别是在标注数据有限的情况下表现突出。

Conclusion: ReLKD通过有效利用隐式类间关系，显著提升了GCD任务中新类别的分类性能，为解决现实世界中的类别发现问题提供了有效方案。

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [206] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种用于内窥镜视频增强的退化感知框架，通过跨帧传播退化表示实现实时高质量增强，在性能与效率之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术依赖术中视频，但视频常因不均匀光照、组织散射、遮挡和运动模糊而退化，影响手术安全与效果。现有深度学习方法计算量大，难以满足实时手术需求。

Method: 提出退化感知框架：1) 使用对比学习从图像中提取退化表示；2) 引入融合机制，用退化表示调制图像特征指导单帧增强模型；3) 通过退化与恢复图像间的循环一致性约束训练，提高鲁棒性和泛化能力。

Result: 实验表明，该框架在性能与效率方面优于多种先进方法，实现了实时内窥镜视频增强。

Conclusion: 退化感知建模对实时内窥镜视频增强有效，通过隐式学习和传播退化表示为临床应用提供了实用途径。

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [207] [Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation](https://arxiv.org/abs/2512.07275)
*Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种用于皮肤病变分割的新型编码器-解码器网络架构，通过多尺度残差结构、多分辨率多通道融合模块、交叉混合注意力模块和外部注意力桥接，有效解决了不规则形状和低对比度问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，精确的皮肤病变分割对于皮肤疾病的早期检测和准确诊断至关重要。尽管深度学习在图像处理方面取得了显著进展，但现有方法尚未有效解决不规则病变形状和低对比度的挑战。

Method: 1. 基于多尺度残差结构的编码器-解码器网络架构；2. 多分辨率多通道融合(MRCF)模块捕获跨尺度特征；3. 交叉混合注意力模块(CMAM)重新定义注意力范围并动态计算多上下文权重；4. 外部注意力桥接(EAB)解决传统U-Net中跳跃连接导致的信息损失问题。

Result: 在多个皮肤病变分割数据集上的广泛实验评估表明，所提出的模型显著优于现有的基于Transformer和卷积神经网络的模型，展现出卓越的分割准确性和鲁棒性。

Conclusion: 该研究提出了一种创新的皮肤病变分割方法，通过多尺度特征提取、注意力机制优化和信息损失补偿，有效解决了医疗图像分割中的关键挑战，为皮肤疾病的早期诊断提供了更准确的工具。

Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.

</details>


### [208] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

Relevance: 35.0

TL;DR: Geo3DVQA是一个用于评估视觉语言模型在仅使用RGB遥感图像进行3D地理空间推理能力的基准测试，包含11万个问题-答案对，涵盖16个任务类别和三个复杂度级别。


<details>
  <summary>Details</summary>
Motivation: 当前3D地理空间分析依赖昂贵的专用传感器（如LiDAR和多光谱传感器），限制了全球可访问性。现有的基于传感器和规则驱动的方法难以整合多个3D线索、处理多样化查询并提供可解释的推理。

Method: 创建了一个包含110k个精心策划的问题-答案对的基准测试，涵盖16个任务类别，分为三个复杂度级别：单特征推理、多特征推理和应用级空间分析。使用RGB遥感图像评估视觉语言模型在高度感知的3D地理空间推理能力。

Result: 评估了10个最先进的视觉语言模型，结果显示RGB到3D推理具有挑战性：GPT-4o准确率28.6%，Gemini-2.5-Flash准确率33.0%，而领域特定微调的Qwen2.5-VL-7B达到49.6%（提升24.8个百分点）。

Conclusion: 当前视觉语言模型在RGB到3D地理空间推理方面存在局限性，但领域适应能显著提升性能。Geo3DVQA为可扩展、可访问和全面的3D地理空间分析引入了新的挑战前沿。

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [209] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了Aerial-D，一个用于航空图像的大规模指代表达分割数据集，包含37,288张图像和1,522,523个指代表达，涵盖259,709个标注目标。采用全自动流水线构建，结合规则生成和LLM增强，支持现代和历史航空图像的实例和语义分割。


<details>
  <summary>Details</summary>
Motivation: 航空图像（无人机、历史档案、卫星图像等）的指代表达分割面临独特挑战：空间分辨率差异大、颜色使用不一致、目标像素少、物体密度高、部分遮挡。现有数据集规模有限，需要专门针对航空图像的大规模数据集。

Method: 1. 构建Aerial-D数据集：37,288张图像，1,522,523个指代表达，259,709个标注目标，覆盖21个类别（车辆、基础设施、土地覆盖类型等）。2. 采用全自动流水线：结合系统规则生成和LLM增强程序，丰富语言多样性和视觉细节关注。3. 使用过滤器模拟历史成像条件。4. 采用RSRefSeg架构，在Aerial-D和先前航空数据集上联合训练。

Result: 联合训练在当代基准测试中达到竞争性性能，同时在单色、棕褐色和颗粒化退化（档案航空摄影中常见）下保持强准确性。模型能够统一处理现代和历史图像的实例和语义分割。

Conclusion: Aerial-D是航空图像指代表达分割的重要资源，通过全自动流水线构建，结合规则生成和LLM增强。联合训练方法在保持现代图像性能的同时，对历史图像退化具有鲁棒性。数据集、训练模型和完整软件流水线已公开。

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [210] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

Relevance: 35.0

TL;DR: DeepAgent是一个多智能体协作框架，通过视觉和音频模态的互补检测深度伪造视频，使用随机森林元分类器融合决策以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法通常将音频和视觉信息集成在单一模型中，容易受到模态不匹配、噪声和操纵的影响。需要更鲁棒的跨模态检测框架来应对合成媒体的挑战。

Method: 提出DeepAgent多智能体框架：Agent-1使用AlexNet-based CNN检测视觉伪造痕迹；Agent-2检测音视频不一致性，结合声学特征、Whisper音频转录和EasyOCR帧读取。两个智能体的决策通过随机森林元分类器融合。

Result: 在Celeb-DF和FakeAVCeleb数据集上，Agent-1达到94.35%准确率；在FakeAVCeleb上，Agent-2和元分类器分别达到93.69%和81.56%；跨数据集验证中，在DeepFakeTIMIT上元分类器达到97.49%准确率。

Conclusion: 分层融合方法通过缓解单个模态的弱点增强了鲁棒性，多智能体方法能有效应对深度伪造中的多样化操纵类型。

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [211] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种结构感知的特征校正方法，通过构建区域邻接图（RAG）来利用图像的低级特征，改进CLIP在开放词汇语义分割中的局部区域一致性，减少噪声预测。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在大规模数据集上预训练后，倾向于全局语义对齐，但在细粒度视觉区域与文本关联时表现不佳，导致开放词汇语义分割中局部区域出现噪声和不一致预测。这源于对比训练范式带来的分散偏差，仅使用CLIP特征难以缓解。

Method: 提出结构感知的特征校正方法：1）基于低级特征（颜色、纹理等）构建区域邻接图（RAG）来捕捉局部结构关系；2）利用RAG对CLIP特征进行细化，增强局部判别能力；3）将图像特定的先验信息直接融入特征校正过程。

Result: 在多个开放词汇分割基准测试中，该方法有效抑制了分割噪声，提高了区域级一致性，并取得了强劲的性能表现。

Conclusion: 通过结合图像特定的低级结构先验，可以显著改善CLIP在开放词汇语义分割中的局部区域关联能力，解决对比训练范式带来的分散偏差问题。

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [212] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出新的无人机反无人机视觉跟踪任务UAV-Anti-UAV，构建百万级数据集，并提出基于Mamba的MambaSTS方法进行时空语义集成学习。


<details>
  <summary>Details</summary>
Motivation: 当前反无人机研究主要关注固定地面摄像头采集的RGB、红外或RGB-IR视频，缺乏从移动无人机平台跟踪目标无人机的研究。为填补这一空白，提出更具挑战性的无人机反无人机跟踪任务。

Method: 提出MambaSTS方法：使用Mamba模型学习全局语义特征，Transformer学习空间特征，利用状态空间模型的长序列建模能力，通过时间令牌传播机制建立视频级长期上下文。

Result: 构建了包含1,810个视频的百万级数据集，每个视频都有人工标注的边界框、语言提示和15个跟踪属性。实验验证了MambaSTS方法的有效性，对50种现代深度跟踪算法的评估表明该领域仍有很大改进空间。

Conclusion: 无人机反无人机跟踪是一个具有挑战性的新任务，提出的数据集和MambaSTS方法为该领域研究提供了基础，实验表明现有方法在该任务上仍有很大提升空间。

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [213] [GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring](https://arxiv.org/abs/2512.07391)
*Đorđe Nedeljković*

Main category: cs.CV

Relevance: 35.0

TL;DR: GlimmerNet：一种超轻量级卷积网络，通过分组扩张深度卷积实现多尺度特征提取，无需额外参数成本，在无人机应急监控任务中达到新的精度-效率平衡前沿。


<details>
  <summary>Details</summary>
Motivation: 虽然CNN在边缘和移动视觉任务中计算效率高，但现有方法通过Vision Transformers引入全局上下文理解会带来显著计算开销。本文旨在保留强全局感知能力的同时避免依赖计算昂贵的组件。

Method: 提出GlimmerNet，基于"将感受野多样性与特征重组分离"的原则。核心是分组扩张深度卷积块(GDBlocks)，将通道分组并分配不同扩张率，实现多尺度特征提取。设计新颖的聚合器模块，使用分组点卷积重组跨组表示，显著降低参数开销。

Result: 仅31K参数，比最新基线减少29% FLOPs，在无人机AIDERv2数据集上达到新的SOTA加权F1分数0.966。

Conclusion: GlimmerNet为资源受限的无人机平台实时应急监控建立了新的精度-效率平衡前沿，证明了无需昂贵计算组件也能实现强全局感知。

Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.

</details>


### [214] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文发现全切片图像归一化方法在真实临床数据上会产生幻觉伪影，提出了一种自动检测幻觉的新图像比较度量，揭示了传统评估指标未能捕捉的严重问题。


<details>
  <summary>Details</summary>
Motivation: 全切片图像归一化是计算病理学的重要预处理步骤，但现有深度学习方法倾向于输出平均值，可能掩盖诊断重要特征，更严重的是会产生视觉上难以检测的幻觉伪影，对下游分析构成严重威胁，而当前评估实践往往忽视这些问题。

Method: 提出了一种新颖的图像比较度量，专门设计用于自动检测归一化输出中的幻觉伪影。使用该度量系统地评估了在真实世界临床数据上重新训练的多个引用广泛的归一化方法。

Result: 研究发现当这些模型在真实临床数据上重新训练和评估时，幻觉伪影的出现频率令人担忧。新提出的度量揭示了传统指标未能捕捉的显著不一致性和失败情况，表明现有方法在临床部署中存在严重风险。

Conclusion: 幻觉风险是真实存在且被低估的，需要开发更鲁棒、可解释的归一化技术，并在临床部署中实施更严格的验证协议。

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [215] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

Relevance: 35.0

TL;DR: VideoCoF提出了一种新颖的链式帧推理方法，通过让视频扩散模型先预测编辑区域潜在表示，再进行视频生成，实现了无需用户提供掩码的精确视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法面临关键权衡：专家模型依赖任务特定的先验（如掩码）而难以统一；统一的时间上下文学习模型虽无需掩码但缺乏显式空间线索，导致指令到区域映射不精确。需要一种既能统一化又能精确定位的方法。

Method: VideoCoF采用链式帧推理方法，强制视频扩散模型遵循"观察、推理、编辑"流程：先预测推理标记（编辑区域潜在表示），再生成目标视频标记。同时引入RoPE对齐策略，利用推理标记确保运动对齐并支持超出训练时长的长度外推。

Result: 仅使用5万视频对的最小数据成本，VideoCoF在VideoCoF-Bench上实现了最先进的性能，验证了方法的效率和有效性。

Conclusion: VideoCoF通过显式推理步骤解决了视频编辑中的精度与统一性权衡问题，无需用户提供掩码即可实现精确的指令到区域对齐和细粒度视频编辑。

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [216] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

Relevance: 35.0

TL;DR: S2VC：基于单步扩散的视频编码器，通过条件编码框架和高效单步扩散生成器，在低码率下实现高质量视频重建，相比现有感知方法平均节省52.73%码率。


<details>
  <summary>Details</summary>
Motivation: 传统和神经视频编码器在率失真性能上表现优异，但在低码率下提升感知质量仍具挑战。现有方法要么受限于生成能力产生伪影，要么依赖预训练扩散模型但采样复杂度高。需要一种既能保持高质量又能降低计算成本的方法。

Method: 提出S2VC单步扩散视频编码器：1）结合条件编码框架与高效单步扩散生成器；2）引入上下文语义指导，从缓冲特征中提取帧自适应语义，替代文本描述；3）在扩散U-Net中加入时间一致性指导，确保帧间时间连贯性。

Result: S2VC实现了最先进的感知质量，相比现有感知方法平均节省52.73%的码率，证明了单步扩散在高效高质量视频压缩中的潜力。

Conclusion: 单步扩散模型能够有效平衡视频压缩的质量与效率，通过语义和时间一致性指导机制，在低码率下实现高质量重建，为视频编码领域提供了新的研究方向。

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [217] [From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images](https://arxiv.org/abs/2512.07527)
*Fei Yu,Yu Liu,Luyang Tang,Mingchao Sun,Zengye Ge,Rui Bu,Yuchao Jin,Haisen Zhao,He Sun,Yangyan Li,Mu Xu,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出针对城市规模3D重建的方法，使用2.5D高度图建模城市几何，结合可微分渲染和纹理恢复网络，从稀疏卫星图像合成逼真的地面视角


<details>
  <summary>Details</summary>
Motivation: 解决从卫星图像进行城市规模3D重建的极端视角外推问题，现有方法（如NeRF、3DGS）在处理稀疏轨道图像、严重透视缩短和纹理缺陷时失败

Method: 1) 使用Z单调有符号距离场建模2.5D高度图，匹配城市建筑布局；2) 通过可微分渲染技术从卫星图像绘制网格外观；3) 训练生成式纹理恢复网络增强外观细节

Result: 在4km²真实区域仅用少量卫星图像实现重建，在合成逼真地面视图方面达到最先进性能，生成高质量、可直接应用的资产

Conclusion: 提出的方法解决了城市规模3D重建的极端视角外推挑战，为城市规划、仿真等下游任务提供高保真资产

Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

</details>


### [218] [Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation](https://arxiv.org/abs/2512.07568)
*Xuecheng Li,Weikuan Jia,Alisher Kurbonaliev,Qurbonaliev Alisher,Khudzhamkulov Rustam,Ismoilov Shuhratjon,Eshmatov Javhariddin,Yuanjie Zheng*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出DSRSD-Net框架，通过残差分解和语义去相关约束来解耦模态特定和模态共享信息，解决多模态学习中的模态主导、冗余耦合和虚假相关问题。


<details>
  <summary>Details</summary>
Motivation: 多模态表示常面临模态主导、冗余信息耦合和虚假跨模态相关性问题，导致次优泛化和有限可解释性。高方差模态容易掩盖弱但语义重要的信号，而简单融合策略会不受控制地纠缠模态共享和模态特定因素。

Method: 提出双流残差语义去相关网络(DSRSD-Net)：1) 双流表示学习模块通过残差投影分离模态内(私有)和模态间(共享)潜在因子；2) 残差语义对齐头将不同模态的共享因子映射到共同空间；3) 去相关和正交性损失正则化共享空间的协方差结构，同时强制共享流和私有流之间的正交性。

Result: 在两个大规模教育基准测试中，DSRSD-Net在下一步预测和最终结果预测方面持续优于强单模态、早期融合、晚期融合和协同注意力基线。

Conclusion: DSRSD-Net通过解耦模态特定和模态共享信息，有效解决了多模态学习中的关键挑战，提高了预测性能和可解释性。

Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

</details>


### [219] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: AutoSeg3D将在线3D分割重构为实例跟踪问题，通过对象查询进行时空信息传播，提升具身智能体的环境感知能力


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的3D分割方法主要关注空间信息传播，忽视了感知的动态性和时间维度。具身智能体需要实时、细粒度的3D分割能力来理解动态环境，但视角变化导致的部分物体可见性问题需要时间信息来建立完整物体理解。

Method: 1. 将在线3D分割重构为实例跟踪问题；2. 使用对象查询进行时间信息传播：长期实例关联保持特征和身份一致性，短期实例更新丰富即时观测；3. 引入空间一致性学习缓解VFMs的碎片化问题；4. 稀疏查询避免密集点云交互的计算负担。

Result: 在ScanNet200上超越ESAM 2.8 AP，在ScanNet、SceneNN和3RScan数据集上均取得一致性能提升，建立了新的state-of-the-art。

Conclusion: 通过时间信息传播和空间一致性学习，AutoSeg3D显著提升了具身智能体的3D环境感知能力，证明了时间维度在在线3D分割中的重要性。

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [220] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出了一种AI驱动的自主水下航行器系统，集成了YOLOv12 Nano进行实时目标检测、ResNet50特征提取、PCA降维、K-Means++聚类，并利用GPT-4o Mini生成结构化报告，用于海洋探索中的目标检测与分析。


<details>
  <summary>Details</summary>
Motivation: 传统海洋探索面临极端条件、有限能见度和高成本等挑战，导致大量海洋区域未被探索。需要自动化系统来降低人类潜水风险，提高任务效率，并增强水下数据分析的深度和速度。

Method: 1. 使用YOLOv12 Nano进行实时水下目标检测；2. ResNet50 CNN进行特征提取；3. PCA进行特征降维（保留98%方差）；4. K-Means++聚类基于视觉特征分组；5. GPT-4o Mini LLM生成结构化报告和摘要。

Result: 在DeepFish和OzFish数据集（55,000+图像）上评估，系统达到mAP@0.5为0.512，精度0.535，召回率0.438。PCA有效降维，K-Means成功聚类，LLM能生成有洞察力的检测摘要。

Conclusion: 该集成系统显著降低了人类潜水风险，提高了任务效率，增强了水下数据分析能力，为挑战性海洋环境中的科学研究开辟了新途径。

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [221] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

Relevance: 35.0

TL;DR: OMEGA：一种用于生成安全关键自动驾驶场景的优化引导、免训练框架，通过约束优化改进扩散采样，提高物理合理性和行为一致性


<details>
  <summary>Details</summary>
Motivation: 自动驾驶评估需要真实多样的多智能体驾驶场景，但安全关键事件在现有数据集中罕见且代表性不足。现有数据驱动场景生成方法缺乏可控性或产生违反物理/社会约束的样本，限制了实用性。

Method: 提出OMEGA框架，在基于扩散的场景生成模型采样过程中，通过约束优化重新锚定每个反向扩散步骤，引导生成物理合理且行为一致的轨迹。将自车-攻击者交互建模为分布空间中的博弈论优化，近似纳什均衡以生成真实的安全关键对抗场景。

Result: 在nuPlan和Waymo数据集上，OMEGA将物理和行为有效场景比例从32.35%提升到72.27%（自由探索），从11%提升到80%（可控生成）。能生成5倍多的近碰撞帧（碰撞时间小于3秒），同时保持整体场景真实性。

Conclusion: OMEGA通过优化引导的扩散采样显著提高了自动驾驶场景生成的物理合理性、行为一致性和可控性，为安全关键场景评估提供了有效工具。

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [222] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

Relevance: 35.0

TL;DR: DIST-CLIP：一种用于MRI图像标准化的统一框架，通过CLIP引导的分离式风格迁移，可灵活使用目标图像或DICOM元数据进行指导，显著提升风格转换保真度和解剖结构保留。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像分析中面临临床泛化限制，主要障碍是数据异质性。MRI中扫描仪硬件差异、采集协议多样性和序列参数变化导致显著的域偏移，现有数据标准化方法不足：基于图像的方法需要目标图像，文本引导方法依赖过于简化的标签，且通常局限于有限变异的数据集，无法捕捉真实临床环境的异质性。

Method: 提出DIST-CLIP框架，明确分离解剖内容与图像对比度。使用预训练的CLIP编码器提取对比度表示，通过新颖的自适应风格迁移模块将这些对比度嵌入整合到解剖内容中。框架可灵活使用目标图像或DICOM元数据进行指导。

Result: 在多样化的真实临床数据集上训练和评估DIST-CLIP，与最先进方法相比，在风格转换保真度和解剖结构保留方面均显示出显著改进。

Conclusion: DIST-CLIP为MRI数据标准化提供了灵活的解决方案，通过CLIP引导的分离式风格迁移有效处理真实临床环境中的异质性，代码和权重将在发表后公开。

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [223] [Improving action classification with brain-inspired deep networks](https://arxiv.org/abs/2512.07729)
*Aidas Aglinskas,Stefano Anzellotti*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究比较了深度神经网络与人类在动作识别中对身体和背景信息的利用差异，并提出了一种受大脑领域特异性启发的双流架构，该架构在性能上更接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究深度神经网络在动作识别中如何利用身体姿态和背景场景信息。由于训练数据中这两类信息可能存在相关性，DNN可能过度依赖其中一种而未能充分利用另一种。相比之下，人类大脑具有专门处理身体和场景信息的特定区域，因此研究人类是否更有效地利用这两种信息，以及构建受大脑启发的领域特异性架构是否能提升DNN性能。

Method: 研究方法包括：1）使用HAA500数据集训练标准DNN进行动作识别；2）设计三种刺激版本（完整版、仅身体版、仅背景版）测试模型性能；3）招募28名人类参与者进行相同测试；4）设计并实现受大脑领域特异性启发的双流架构，分别处理身体和背景信息。

Result: 研究结果显示：1）标准DNN在完整版和仅身体版刺激上表现接近，但在仅背景版刺激上表现接近随机水平；2）人类在所有三种刺激版本上都能准确识别动作，且在仅身体版上的表现显著优于仅背景版；3）提出的双流架构不仅提升了动作识别性能，而且其在不同刺激版本上的准确率模式更接近人类表现模式。

Conclusion: 结论表明：1）标准DNN过度依赖背景信息而未能充分利用身体信息；2）人类能更均衡地利用身体和背景信息；3）受大脑领域特异性启发的架构设计能改善DNN的性能和人类相似性，为构建更人类化的AI系统提供了神经科学启发的方法。

Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

</details>


### [224] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种无监督可见光-红外行人重识别方法，通过模态感知Jaccard距离缓解模态差异，结合"分割-对比"策略学习模态不变表示，在基准数据集上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 无监督可见光-红外行人重识别面临模态差异大的挑战，现有方法使用最优传输关联模态内聚类，容易传播局部聚类错误且忽视全局实例级关系。需要解决跨模态学习中的模态偏差问题

Method: 1) 提出模态感知Jaccard距离缓解模态差异引起的距离偏差，通过全局聚类估计更可靠的跨模态关联；2) 设计"分割-对比"策略获取模态特定全局原型，在全局关联指导下对齐这些原型，实现模态不变且ID可区分的表示学习

Result: 在基准VI-ReID数据集上获得最先进的性能，显著优于现有方法

Conclusion: 通过缓解模态偏差的全局关联和模态不变表示学习，有效解决了无监督可见光-红外行人重识别中的跨模态学习挑战

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [225] [GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring](https://arxiv.org/abs/2512.07776)
*Maximilian Schall,Felix Leonard Knöfel,Noah Elias König,Jan Jonas Kubeler,Maximilian von Klinski,Joan Wilhelm Linnemann,Xiaoshi Liu,Iven Jelle Schlegelmilch,Ole Woyciniuk,Alexandra Schild,Dante Wasmuht,Magdalena Bermejo Espinet,German Illera Basas,Gerard de Melo*

Main category: cs.CV

Relevance: 35.0

TL;DR: 论文提出了GorillaWatch系统，包含三个新数据集和端到端管道，用于自动识别濒危西部低地大猩猩，通过多帧自监督预训练和注意力解释方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前监测濒危西部低地大猩猩面临巨大挑战，需要从大量相机陷阱视频中手动重新识别个体。主要障碍是缺乏适合训练鲁棒深度学习模型的大规模野外视频数据集。

Method: 1) 引入三个新数据集：Gorilla-SPAC-Wild（最大野外灵长类重识别数据集）、Gorilla-Berlin-Zoo（跨域泛化评估）、Gorilla-SPAC-MoT（多目标跟踪评估）。2) 提出GorillaWatch端到端管道，集成检测、跟踪和重识别。3) 引入多帧自监督预训练策略，利用轨迹一致性学习领域特定特征。4) 使用可微分的AttnLRP适应方法验证模型依赖判别性生物特征而非背景相关性。

Result: 大规模图像骨干网络聚合特征优于专用视频架构；提出的方法在重识别和跟踪任务上表现优异；通过时空约束集成到标准聚类中解决了无监督种群计数的过分割问题。

Conclusion: 该工作为濒危物种监测提供了可扩展、非侵入性的解决方案，通过公开代码和数据集促进保护生物学研究，展示了计算机视觉在野生动物保护中的实际应用价值。

Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

</details>


### [226] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

Relevance: 35.0

TL;DR: OneStory：一种通过全局跨镜头上下文建模实现一致、可扩展叙事生成的多镜头视频生成方法，将MSV重新定义为下一镜头生成任务，利用预训练I2V模型并引入帧选择和自适应调节模块。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法难以有效建模长距离跨镜头上下文，因为它们依赖有限的时间窗口或单关键帧条件，导致在复杂叙事下性能下降。需要一种能够全局建模跨镜头上下文的方法来实现一致且可扩展的叙事生成。

Method: 将多镜头视频生成重新定义为下一镜头生成任务，实现自回归镜头合成。引入两个关键模块：1) 帧选择模块：基于先前镜头中的信息帧构建语义相关的全局记忆；2) 自适应调节器：执行重要性引导的补丁化以生成紧凑上下文进行直接调节。使用预训练图像到视频模型进行强视觉条件化。

Result: 在精心策划的60K数据集上微调后，OneStory在文本和图像条件设置下，在多样复杂场景中实现了最先进的叙事连贯性，支持可控和沉浸式的长格式视频叙事生成。

Conclusion: OneStory通过全局跨镜头上下文建模解决了现有多镜头视频生成方法的局限性，实现了更一致和可扩展的叙事生成，为长格式视频叙事提供了有效解决方案。

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [227] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

Relevance: 35.0

TL;DR: MVP是一个可扩展的多视角Transformer架构，能够从数十到数百张图像中单次前向传播重建大型3D场景，结合局部到全局的视角层次和细到粗的空间层次设计，实现了高效且高质量的大规模场景重建。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景重建方法在处理大规模、多视角图像输入时面临计算效率和可扩展性挑战。传统方法难以同时处理数十到数百张图像，且在保持重建质量的同时实现高效计算存在困难。

Method: MVP基于两个核心设计原则：1) 局部到全局的视角间层次结构，从局部视角逐步扩展到组视角再到完整场景；2) 细到粗的视角内层次结构，从详细空间表示逐步聚合为紧凑的信息密集token。该架构结合3D高斯泼溅作为底层3D表示。

Result: 在多样化数据集上验证显示，MVP实现了最先进的可泛化重建质量，同时保持高效率和可扩展性，能够适应广泛的视角配置范围。

Conclusion: MVP通过创新的双层次Transformer架构，成功解决了大规模3D场景重建中的计算效率和可扩展性问题，为处理复杂多视角图像输入提供了有效解决方案。

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [228] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种将语言场嵌入3D表示的新方法，通过引入极低维语义瓶颈特征和特征哈希编码器，解决现有方法在语义特征对齐和效率方面的挑战


<details>
  <summary>Details</summary>
Motivation: 将语言场嵌入3D表示能够实现更丰富的空间环境语义理解，连接几何与描述性意义，支持自然语言查询和编辑场景，提升场景检索、导航和多模态推理等任务。但现有特征蒸馏方法在处理大规模互联网数据时面临语义特征错位和内存/运行时效率低下的挑战

Method: 1) 在底层3D高斯表示中引入极低维语义瓶颈特征，通过渲染和多分辨率特征哈希编码器处理，显著提升GPU内存和运行时效率；2) 提出衰减下采样器模块和多种正则化方法，解决地面真实2D特征的语义错位问题

Result: 在HolyScenes数据集上评估，方法在性能和效率方面均超越现有方法

Conclusion: 提出的方法有效解决了大规模场景中语言场嵌入的语义对齐和效率问题，为3D场景的语义理解和自然语言交互提供了更高效的解决方案

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [229] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

Relevance: 35.0

TL;DR: UnityVideo是一个统一的多模态视频生成框架，通过联合学习多种模态（分割掩码、人体骨架、DensePose、光流、深度图）和训练范式，实现世界感知的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型受限于单模态条件约束，缺乏跨模态交互和模态多样性，限制了其对世界的全面理解能力。

Method: 提出两个核心组件：1) 动态噪声统一异构训练范式；2) 模态切换器与上下文学习器，通过模块化参数和上下文学习实现统一处理。构建了130万样本的统一数据集。

Result: UnityVideo加速了收敛过程，显著增强了零样本泛化能力，在视频质量、一致性和物理世界约束对齐方面表现优异。

Conclusion: 多模态联合学习框架能够有效提升视频生成模型的世界感知能力和泛化性能。

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [230] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: Voxify3D是一个可微分的两阶段框架，用于从3D网格生成体素艺术，通过正交像素艺术监督、基于补丁的CLIP对齐和调色板约束的Gumbel-Softmax量化来解决几何抽象、语义保留和离散颜色一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 体素艺术在游戏和数字媒体中广泛应用，但从3D网格自动生成体素艺术面临几何抽象、语义保留和离散颜色一致性之间的冲突要求。现有方法要么过度简化几何，要么无法实现像素级精确、调色板约束的体素艺术美学。

Method: 提出Voxify3D，一个可微分的两阶段框架，将3D网格优化与2D像素艺术监督相结合。包含三个核心组件：1) 正交像素艺术监督消除透视失真，实现精确的体素-像素对齐；2) 基于补丁的CLIP对齐在不同离散化级别保持语义；3) 调色板约束的Gumbel-Softmax量化，在离散颜色空间上实现可微优化，支持可控调色板策略。

Result: 实验显示在CLIP-IQA指标上达到37.12分，用户偏好率为77.90%，在多样化角色和可控抽象（2-8种颜色，20x-50x分辨率）上表现优异。

Conclusion: Voxify3D通过可微分框架解决了体素艺术生成中的基本挑战，包括极端离散化下的语义保留、通过体积渲染实现像素艺术美学，以及端到端的离散优化，为3D到体素艺术的转换提供了有效解决方案。

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


### [231] [Semantic Temporal Single-photon LiDAR](https://arxiv.org/abs/2512.06008)
*Fang Li,Tonglin Mu,Shuling Li,Junran Guo,Keyuan Li,Jianing Li,Ziyang Luo,Xiaodong Fan,Ye Chen,Yunfeng Liu,Hong Cai,Lip Ket Chin,Jinbei Zhang,Shihai Sun*

Main category: eess.IV

Relevance: 35.0

TL;DR: 提出基于语义知识库的语义TSP-LiDAR，通过语义通信框架解决开放集场景下的目标识别问题，在低信噪比和短采集时间条件下表现优异，并能动态更新新目标的语义特征。


<details>
  <summary>Details</summary>
Motivation: 现有TSP-LiDAR方法在开放集场景（出现未知目标）中效果不佳，且在低信噪比和短采集时间条件下性能显著下降。需要一种能够适应动态环境、处理未知目标的鲁棒识别方法。

Method: 提出语义TSP-LiDAR框架，将目标识别过程建模为语义通信问题。核心是自更新语义知识库（SKB），能够动态更新新目标的语义特征，无需大量神经网络重新训练。

Result: 仿真和实验结果表明，该方法在低信噪比和有限采集时间条件下优于传统方法。自更新SKB机制对九种未知目标的识别准确率达到89%，而无更新机制仅为66%。

Conclusion: 该框架在复杂动态环境中具有自适应和鲁棒的目标识别潜力，为开放集场景下的TSP-LiDAR应用提供了有效解决方案。

Abstract: Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.

</details>


### [232] [GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers](https://arxiv.org/abs/2512.06147)
*Hochul Hwang,Soowan Yang,Jahir Sadik Monon,Nicholas A Giudice,Sunghoon Ivan Lee,Joydeep Biswas,Donghyun Kim*

Main category: cs.RO

Relevance: 35.0

TL;DR: 本文开发了GuideNav系统，一种受导盲犬启发的视觉导航系统，用于帮助盲人和低视力人群进行机器人导航。


<details>
  <summary>Details</summary>
Motivation: 当前针对盲人和低视力人群的移动辅助系统研究虽然取得进展，但直接指导机器人导航设计的参考仍然稀缺。需要填补这一空白，开发更有效的辅助导航系统。

Method: 1) 进行人因研究：访谈26名导盲犬使用者、4名白手杖使用者、9名导盲犬训练师和1名定向行走训练师，观察15+小时导盲犬辅助行走；2) 基于研究洞察开发GuideNav系统：视觉导航系统，采用"教导-重复"方法，构建拓扑路径表示，结合视觉地点识别与时间滤波，使用相对姿态估计计算导航动作。

Result: 1) 开源了去识别化的人因研究数据集；2) GuideNav在五个室外环境中实现了公里级路径跟随，即使在教导和重复运行之间存在明显场景变化时仍保持可靠性；3) 用户研究（3名导盲犬使用者和1名训练师）证实了系统可行性，首次展示了四足移动系统以类似导盲犬的方式检索路径。

Conclusion: GuideNav系统成功展示了受导盲犬启发的视觉导航方法在辅助盲人和低视力人群方面的潜力，为未来人本辅助系统开发提供了有价值的参考。

Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.

</details>


### [233] [XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association](https://arxiv.org/abs/2512.06757)
*Zhihua Fang,Shumei Tao,Junxu Wang,Liang He*

Main category: cs.SD

Relevance: 35.0

TL;DR: XM-ALIGN是一个用于跨模态人脸-语音验证的统一框架，结合显式和隐式对齐机制，在MAV-Celeb数据集上表现出色


<details>
  <summary>Details</summary>
Motivation: 解决跨模态验证中的对齐问题，特别是在"听过"和"未听过"语言场景下，提升人脸和语音特征之间的对齐效果

Method: 从人脸和语音编码器提取特征嵌入，使用共享分类器联合优化，采用均方误差作为嵌入对齐损失，结合数据增强策略

Result: 在MAV-Celeb数据集上表现出优越性能，显著提升了跨模态验证效果

Conclusion: 提出的统一跨模态嵌入对齐框架有效提升了人脸-语音验证性能，特别是在不同语言场景下

Abstract: This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both "heard" and "unheard" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.

</details>


### [234] [Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07130)
*Zebin Xing,Yupeng Zheng,Qichao Zhang,Zhixing Ding,Pengxuan Yang,Songen Gu,Zhongpu Xia,Dongbin Zhao*

Main category: cs.RO

Relevance: 35.0

TL;DR: Mimir是一个用于端到端自动驾驶的分层双系统框架，通过拉普拉斯分布估计目标点不确定性来增强鲁棒性，并采用多速率引导机制提高推理速度，在Navhard和Navtest基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法受限于高层引导信号不准确和复杂引导模块的计算开销，需要更鲁棒且高效的解决方案。

Method: 提出分层双系统框架：1) 使用拉普拉斯分布估计目标点不确定性而非确定性建模；2) 引入多速率引导机制提前预测扩展目标点以加速推理。

Result: 在Navhard和Navtest基准测试中，驾驶评分EPDMS提升20%，高层模块推理速度提升1.6倍且不损失准确性。

Conclusion: Mimir通过不确定性估计和多速率引导机制，在保持准确性的同时显著提升了自动驾驶系统的鲁棒性和效率。

Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving

</details>


### [235] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文研究Capsule Networks中Primary Capsules的剪枝技术，在多个数据集上实现最高9.9倍的速度提升，同时保持准确率不变，显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: Capsule Networks（CapsNets）相比传统CNN在图像变换鲁棒性和重叠图像检测方面有优势，但其Primary Capsules数量过多导致资源效率低下，训练和推理速度慢、资源消耗大。研究旨在通过剪枝技术解决CapsNets的效率问题。

Method: 提出对CapsNets中的Primary Capsules进行剪枝的方法，在MNIST、Fashion-MNIST、CIFAR-10和SVHN数据集上进行实验，分析不同数据集对剪枝的响应差异。

Result: 剪枝后的CapsNet在移除95%的Capsules后，性能提升最高达9.9倍，动态路由阶段浮点运算减少超过95.36%，同时保持准确率不变。研究还揭示了不同数据集对剪枝效果的差异原因。

Conclusion: Primary Capsules剪枝是提高CapsNets效率的有效方法，能显著加速推理速度并减少计算资源消耗，为资源受限环境下的CapsNet应用提供了可行方案。

Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [236] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一个完整的自我中心视频编辑生态系统，包括专门的数据集EgoEditData、实时视频编辑器EgoEdit和评估套件EgoEditBench，解决了自我中心视频编辑中的运动模糊和手部交互等独特挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频编辑器在第三人称视频上表现良好，但自我中心视角存在独特挑战：快速自我运动和频繁的手-物交互导致显著的领域差距。此外，现有离线编辑流程延迟高，限制了实时交互。

Method: 1) 构建EgoEditData：专门为自我中心编辑场景设计的精心策划数据集，包含丰富的手-物交互并明确保留手部信息；2) 开发EgoEdit：支持单GPU实时流式推理的指令跟随自我中心视频编辑器；3) 引入EgoEditBench：针对指令忠实度、手部和交互保留、自我运动下时间稳定性的评估套件。

Result: EgoEdit在自我中心编辑基准上取得明显优势（现有方法在此表现不佳），同时在通用编辑任务上保持与最强基线相当的性能。能够产生时间稳定、指令忠实的结果，并具有交互式延迟。

Conclusion: 提出了一个完整的自我中心视频编辑生态系统，解决了该领域的独特挑战，实现了实时交互式编辑，并将数据集和评估套件公开供研究社区使用。

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [237] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

Relevance: 30.0

TL;DR: MICCAI FeTS 2024挑战赛专注于联邦学习在胶质瘤MRI分割中的应用，评估新的权重聚合方法以提升鲁棒性和效率。基于PID控制器的方法在分割性能和通信效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是解决医疗影像分析中的隐私保护问题，通过联邦学习实现多机构数据协作，同时评估新的权重聚合方法来提升联邦学习的鲁棒性和通信效率。

Method: 采用标准化联邦学习设置，使用BraTS胶质瘤基准数据集（1251个训练案例，219个验证案例，570个隐藏测试案例）。评估六支参赛团队的方法，重点关注PID控制器等权重聚合技术，使用Dice相似系数和Hausdorff距离评估分割性能，并通过收敛分数评估通信效率。

Result: 基于PID控制器的方法获得最高排名，在增强肿瘤、肿瘤核心和全肿瘤分割上的平均DSC值分别为0.733、0.761和0.751，HD95值分别为33.922mm、33.623mm和32.309mm，同时获得最高通信效率（收敛分数0.764），超越了之前挑战赛的最佳方法。

Conclusion: 该挑战赛推动了医疗影像联邦学习的发展，证明了PID控制器作为有效的权重聚合机制，能够在保护数据隐私的同时实现稳定的模型优化和高效的通信。

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [238] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文提出了一种基于潜在扩散模型的单图像反射去除方法，通过反射等变VAE、可学习任务特定文本嵌入和深度引导早期分支采样策略，解决了现有方法在野外场景中的恢复和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 单图像反射去除是一个高度不适定问题，现有方法难以推理被破坏区域的组成，导致在真实场景中的恢复和泛化能力不足。论文发现关键问题在于语义编码器的潜在空间缺乏解释复合图像作为其组成层线性叠加的内在结构。

Method: 方法包含三个协同组件：1) 反射等变VAE，将潜在空间与反射形成的线性物理对齐；2) 可学习任务特定文本嵌入，绕过模糊语言提供精确指导；3) 深度引导早期分支采样策略，利用生成随机性获得有希望的结果。

Result: 在多个基准测试中实现了新的SOTA性能，并在具有挑战性的真实世界案例中表现出良好的泛化能力。

Conclusion: 通过重新构建编辑目的的潜在扩散模型，使其能够有效感知和处理高度模糊的分层图像输入，从而产生高质量的输出，解决了单图像反射去除的挑战。

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [239] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文研究学习型k空间采样模式在加速磁共振成像中的跨领域泛化能力，提出通过引入采集不确定性来增强领域鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要针对单一数据集或模态优化k空间采集模式，缺乏对跨成像领域可迁移性的考虑。作者旨在探索学习型k空间采样在领域转移下的泛化能力。

Method: 1) 系统评估不同数据集和采集范式下学习型采样模式的泛化性能；2) 提出增强领域鲁棒性的新方法：在训练中引入采集不确定性，通过随机扰动k空间轨迹来模拟扫描仪和成像条件的变化。

Result: 研究表明，使用学习型采样模式训练的模型在跨领域设置下表现出更好的泛化性能，提出的不确定性增强方法能有效提高领域鲁棒性。

Conclusion: k空间轨迹设计不仅应被视为加速机制，更应作为提高MRI重建领域泛化能力的重要自由度。学习型采样模式具有跨领域泛化的潜力。

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [240] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出联合优化相机硬件参数和自适应控制算法的方法，通过混合梯度优化和零阶优化框架，提升下游视觉任务性能


<details>
  <summary>Details</summary>
Motivation: 现有相机系统设计大多优化固定制造参数，而许多参数（如曝光设置）需要在运行时自适应控制。当前方法通常分别优化静态硬件参数和动态控制算法，缺乏统一的联合优化框架。

Method: 提出统一优化框架，整合梯度优化和零阶优化方法，支持连续和离散参数、不可微图像形成过程以及基于神经网络的自适应控制算法。针对运动模糊等不可微效应，提出DF-Grad混合优化策略，使用零阶优化器信号与无监督任务驱动学习共同训练自适应控制网络。

Result: 实验表明，该方法在低光照和快速运动等挑战条件下，优于分别优化静态和动态参数的基线方法，显著提升感知性能。

Conclusion: 联合优化硬件参数和自适应控制算法能有效提升感知性能，为任务驱动的相机系统设计提供了统一方法。

Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [241] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出一种实时后处理算法，融合BlazePose的3D和2D估计，通过加权优化惩罚与预期骨骼长度和生物力学模型的偏差，使用卡尔曼滤波器根据个体解剖结构细化骨骼长度估计。


<details>
  <summary>Details</summary>
Motivation: 物理训练自动指导应用（如物理治疗）需要准确稳健的单目视频姿态估计。现有模型如BlazePose缺乏解剖约束，通过融入物理知识有改进潜力。

Method: 1. 加权优化融合BlazePose的3D和2D估计；2. 惩罚与预期骨骼长度和生物力学模型的偏差；3. 使用自适应测量信任的卡尔曼滤波器细化个体解剖结构的骨骼长度估计。

Result: 在Physio2.2M数据集上评估：3D MPJPE减少10.2%，身体段间角度误差减少16.6%。提供计算高效的视频到3D姿态估计，适用于消费级设备。

Conclusion: 该方法提供稳健、解剖一致性的姿态估计，适用于自动物理治疗、医疗保健和运动指导，后端运行仅使用匿名数据。

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [242] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出空间检索范式，通过引入离线检索的地理图像（如Google Maps）作为额外输入，增强自动驾驶系统的感知能力，克服传统车载传感器的视野限制。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖车载传感器（摄像头、激光雷达等），但受限于实时感知范围，在视野受限、遮挡或恶劣天气条件下表现不佳。人类驾驶员能够在低能见度下回忆道路结构，因此希望赋予模型类似的"回忆"能力。

Method: 提出空间检索范式，从离线缓存（如Google Maps或存储的自动驾驶数据集）中检索地理图像作为额外输入。扩展nuScenes数据集，通过Google Maps API检索地理图像并与车辆轨迹对齐。在五个核心自动驾驶任务上建立基线。

Result: 实验表明，扩展的模态能够提升某些任务的性能。将开源数据集构建代码、数据和基准，供进一步研究这种新的自动驾驶范式。

Conclusion: 空间检索范式为自动驾驶系统提供了增强的感知能力，通过利用离线地理信息克服实时传感器的限制，是一种即插即用的扩展方案。

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [243] [Understanding Diffusion Models via Code Execution](https://arxiv.org/abs/2512.07201)
*Cheng Yu*

Main category: cs.CV

Relevance: 30.0

TL;DR: 一篇关于扩散模型的技术报告，提供了约300行代码的简洁实现，从代码执行角度解释扩散模型的工作原理，弥合理论公式与开源实现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模方面取得了显著性能，但其理论基础复杂，论文中的数学公式与实际开源实现之间存在难以弥合的差距。现有教程主要关注公式推导，对扩散模型在代码中如何实际运行提供有限指导。

Method: 作者提供了一个约300行代码的简洁实现，从代码执行角度解释扩散模型。这个最小化示例保留了核心组件（前向扩散、反向采样、噪声预测网络和训练循环），同时去除了不必要的工程细节。

Result: 提供了一个清晰、以实现为先的理解框架，展示了扩散模型在实践中如何工作，以及代码与理论如何对应。代码和预训练模型已在GitHub上开源。

Conclusion: 通过简洁的代码实现，为研究人员提供了从实践角度理解扩散模型的有效工具，弥合了理论公式与实际实现之间的差距。

Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.

</details>


### [244] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

Relevance: 30.0

TL;DR: STRinGS：针对3D高斯溅射（3DGS）的文本感知选择性细化框架，通过分别处理文本和非文本区域，显著提升3D重建中文本可读性


<details>
  <summary>Details</summary>
Motivation: 现实场景中的文本（如标志、标签、指示牌）包含重要上下文信息，但现有3D表示方法（如3DGS）难以在保持高视觉保真度的同时保留细粒度文本细节，文本重建的小错误会导致显著的语义损失

Method: 提出STRinGS框架：1）将文本和非文本区域分开处理；2）先细化文本区域；3）再将细化后的文本区域与非文本区域合并进行全场景优化；4）引入OCR字符错误率（CER）作为文本可读性评估指标；5）创建STRinGS-360数据集用于评估

Result: 在仅7K次迭代下，STRinGS相比3DGS在文本区域实现了63.6%的相对改进，即使在挑战性配置下也能产生清晰可读的文本

Conclusion: STRinGS方法和数据集共同推动了文本丰富环境中3D场景理解的边界，为更鲁棒的文本感知重建方法铺平了道路

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [245] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出Tessellation GS方法，将2D高斯约束在网格面上，通过层次化神经特征推断属性，结合自适应面细分策略和重建基础模型先验，实现单相机动态场景重建。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在视角外推方面存在困难，特别是在稀疏视图和动态场景重建中容易过拟合，泛化能力差。需要解决从单相机（连续移动或静态）重建动态场景的挑战。

Method: 1) 基于网格面的结构化2D高斯方法；2) 通过网格面上的层次化神经特征推断高斯属性；3) 基于细节感知损失函数的自适应面细分策略指导高斯细分；4) 利用重建基础模型先验初始化高斯变形。

Result: 在表观和网格重建任务上，LPIPS降低29.1%，Chamfer距离降低49.2%，优于先前SOTA方法。能够从单静态相机稳健重建一般动态物体。

Conclusion: Tessellation GS通过结构化约束和自适应细分策略，显著提升了动态场景重建的质量和泛化能力，解决了3D高斯泼溅在视角外推和动态重建中的局限性。

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [246] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

Relevance: 30.0

TL;DR: MoCA是一种组合式3D生成模型，通过重要性组件路由和未选组件压缩技术，解决了现有方法因全局注意力计算成本高而难以扩展的问题，实现了高效、细粒度的组合式3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 组合性对于3D对象和场景生成至关重要，但现有的部分感知3D生成方法在增加组件数量时，由于全局注意力的二次计算成本，导致扩展性差。

Method: MoCA采用两种关键设计：(1) 基于重要性的组件路由，选择top-k相关组件进行稀疏全局注意力计算；(2) 未选组件压缩，在降低全局注意力计算复杂度的同时保留未选组件的上下文先验。

Result: 大量实验表明，MoCA在组合式对象和场景生成任务上均优于基线方法，能够实现高效、细粒度的组合式3D资产创建。

Conclusion: MoCA通过创新的注意力机制设计，解决了组合式3D生成中的扩展性问题，为可扩展的组合式3D生成提供了有效解决方案。

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [247] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

Relevance: 25.0

TL;DR: EmoDiffTalk：基于3D高斯泼溅和扩散模型的可编辑3D说话头生成框架，支持通过文本进行细粒度、连续的多模态情感编辑


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的逼真3D说话头在情感表达操控方面存在不足，特别是在细粒度和扩展性动态情感编辑方面。需要支持多模态控制的情感编辑方法。

Method: 提出EmoDiffTalk框架，包含：1）情感感知高斯扩散过程，使用动作单元（AU）提示的高斯扩散过程进行细粒度面部动画；2）准确的文本到AU情感控制器，通过文本输入提供精确的动态情感编辑

Result: 在EmoTalk3D和RenderMe-360数据集上的实验表明，EmoDiffTalk在情感细腻度、唇形同步保真度和可控性方面优于先前工作，建立了高质量、扩散驱动、多模态可编辑3D说话头合成的原则性途径

Conclusion: EmoDiffTalk是首批支持在基于AU的表情空间内进行连续多模态情感编辑的3D高斯泼溅说话头生成框架之一，为高质量可编辑3D说话头合成提供了新方向

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [248] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了一种将预训练2D模型知识迁移到3D点云理解的方法，通过监督学习、自监督学习和2D到3D迁移学习三个方向，提升3D表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 随着3D数据获取技术的普及，3D数据在计算机视觉、机器人等领域应用广泛。然而，3D数据理解面临挑战，需要有效利用2D图像知识来增强3D表示学习，为自动驾驶、机器人等应用提供更全面的环境理解。

Method: 论文采用三种主要方法：1) 监督表示学习用于点云基元分割；2) 自监督学习方法；3) 2D到3D的迁移学习。核心创新是集成预训练2D模型来支持3D网络训练，而不是简单地将2D数据转换为3D。

Result: 大量实验验证了方法的有效性，展示了通过有效整合2D知识来推进点云表示学习的潜力。该方法显著提升了3D理解能力，而不只是简单的2D数据转换。

Conclusion: 该研究证明了将2D预训练模型知识迁移到3D理解的有效性，为点云表示学习提供了新的方向，能够有效整合2D知识来增强3D理解能力。

Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [249] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一种基于单光子激光雷达的数据驱动方法，通过解耦多路复用照明下的多反射光来重建包含遮挡和镜面反射的3D场景


<details>
  <summary>Details</summary>
Motivation: 单光子激光雷达能够测量场景中的多次反射光，这些光包含遮挡几何和材料属性的额外信息。然而，现有方法仅限于逐点扫描，而实际应用中需要同时照明多个场景点，这带来了复杂的光传输问题

Method: 1) 创建首个大规模室内场景激光雷达瞬态数据集(~100k样本)；2) 学习复杂光传输的先验知识；3) 将测量的二次反射光分解为每个激光点的贡献；4) 从单次测量中推断3D几何

Result: 实验证明该方法能够从单次测量中重建包含遮挡和镜面的3D场景几何，代码和数据集已开源

Conclusion: 通过数据驱动方法解耦多路复用照明下的光传输，单光子激光雷达能够从单次测量中恢复密集深度、遮挡几何和材料属性

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [250] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出将显式物理建模（几何与光照）融入深度学习阴影生成的新框架，通过3D几何重建和光照估计获得物理一致的阴影初始估计，再用扩散模型进行逼真细化


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法很少利用阴影形成的物理原理（遮挡物阻挡光线形成阴影），导致生成的阴影在几何和光照一致性方面不够理想，特别是在复杂场景中

Method: 1) 从单目RGB图像获取密集点云表示的近似3D几何和主导光照方向；2) 基于物理阴影形成原理计算初始阴影位置和形状；3) 将物理初始估计输入扩散框架进行逼真细化，保持与场景几何和光照的一致性

Result: 在DESOBAV2数据集上训练，模型生成的阴影既视觉逼真又物理一致，在复杂几何或模糊光照场景中表现优于现有方法

Conclusion: 将显式物理建模与深度学习结合能显著提升阴影生成的质量和物理一致性，特别是在复杂场景中

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [251] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

Relevance: 25.0

TL;DR: 对SVD+WDR图像压缩方法的独立可复现性研究，发现该方法在PSNR上并未超越JPEG2000或WDR，仅在SSIM上部分优于JPEG2000，与原始论文声称的结果相矛盾。


<details>
  <summary>Details</summary>
Motivation: 原始论文声称结合奇异值分解(SVD)和小波差分缩减(WDR)的图像压缩方法在视觉质量和压缩比上优于JPEG2000和单独的WDR。本研究旨在验证这一声称，通过独立复现来评估该方法的真实性能。

Method: 重新实现SVD+WDR方法，仔细填补原始论文中缺失的实现细节（如量化和阈值初始化），尽可能接近地复现原始实验。在原始图像和新图像上进行额外实验，使用PSNR和SSIM作为评估指标。

Result: 与原始声称相反，SVD+WDR方法在PSNR上通常并未超越JPEG2000或WDR，仅在SSIM上部分优于JPEG2000。研究还发现原始描述中的模糊之处对可复现性和报告性能有显著影响。

Conclusion: 该研究强调了科学论文中精确描述方法细节的重要性，特别是对于可复现性研究。SVD+WDR方法并未如原始声称那样优于现有技术，突显了独立验证在机器学习研究中的必要性。

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [252] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了一种改进3D高斯泼溅重建质量的方法，通过约束多视角三角测量来增强全局几何一致性，减少浮游伪影并提高表面提取精度。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅方法仅依赖光度损失进行重建，导致重建不一致、产生"浮游"伪影和无结构几何，难以提取高质量表面。

Method: 提出约束多视角三角测量方法，通过自监督方式从相邻视角束重新三角化得到鲁棒共识点，并惩罚渲染3D点与共识点的偏差来优化重建过程。

Result: 在多个数据集上验证了方法的有效性，在DTU数据集上达到0.50mm的平均倒角距离，优于同类显式方法。

Conclusion: 通过增强全局几何一致性，显著改善了3D高斯泼溅的重建质量，为实时新视角合成提供了更精确的几何表示。

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [253] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

Relevance: 25.0

TL;DR: CryoHype：基于Transformer的超网络，用于从混合的冷冻电镜图像中重建多种分子结构，解决了传统方法难以处理多种分子混合的组成异质性问题。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜通常用于单一分子结构解析，但实际样本常包含多种分子混合。现有方法主要关注单一或少数结构的构象异质性，无法有效处理多种不同分子物种混合带来的组成异质性。

Method: 提出CryoHype，一种基于Transformer的超网络，动态调整隐式神经表示的权重。该方法能够从混合的冷冻电镜图像中同时重建多种分子结构。

Result: 在包含100个结构的基准数据集上取得最先进结果，并成功扩展到从无标签冷冻电镜图像中重建1000个不同结构（固定姿态设置）。

Conclusion: CryoHype能够有效处理冷冻电镜中的组成异质性问题，实现高通量多目标结构重建，为冷冻电镜分析提供了新的强大工具。

Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [254] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出Human3R方法，通过结合SMPL人体模型和单目深度估计的混合几何先验，解决动态人体场景重建中的几何不一致和分辨率退化问题


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏3D人体结构理解，导致几何不一致的结果（肢体比例扭曲、人-物融合不自然），且内存限制的下采样导致人体边界向背景几何漂移

Method: 提出Human3R方法，采用分层处理流程：先处理全分辨率图像获取整体场景几何，然后通过策略性裁剪和交叉注意力融合增强人体特定细节，通过特征融合模块整合SMPL先验

Result: 在TUM Dynamics和GTA-IM数据集上的实验表明，在动态人体重建方面具有优越性能

Conclusion: 通过结合结构化人体先验和细粒度几何细节，提出的混合几何先验方法能有效解决动态人体场景重建中的关键挑战

Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [255] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出OCFER-Net，通过正交正则化约束卷积核，增强特征多样性和表达能力，在FER-2013数据集上取得优于基线1.087的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在线学习普及背景下，情感交互对教学很重要。面部表情识别(FER)可帮助教师了解学生情绪状态。现有FER方法较少利用卷积矩阵的正交性，正交性可提取更多样化和表达力强的特征。

Method: 提出OCFER-Net，通过正则化器强制卷积核正交性，使卷积矩阵更接近正交矩阵，从而提取更具多样性和表达力的特征。

Result: 在挑战性数据集FER-2013上进行实验，结果显示OCFER-Net性能优于基线方法1.087。

Conclusion: 正交性约束能有效提升FER性能，OCFER-Net在面部表情识别任务中表现出色，代码已开源。

Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [256] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出PCNN（感知CNN）用于面部表情识别，通过五个并行网络学习局部面部特征，结合多域交互机制融合局部器官特征与全局结构特征，使用两阶段损失函数提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在面部表情识别中可能忽略面部分割的影响，无法有效捕捉面部细微变化。需要一种能够同时学习局部面部特征（眼睛、脸颊、嘴巴）并融合全局结构特征的模型。

Method: 1. 使用五个并行网络分别学习眼睛、脸颊、嘴巴等局部面部特征；2. 采用多域交互机制注册和融合局部器官特征与全局面部结构特征；3. 设计两阶段损失函数，分别约束感知信息的准确性和重建面部图像的质量。

Result: PCNN在多个实验室和真实世界面部表情识别基准测试中取得优越结果：CK+、JAFFE、FER2013、FERPlus、RAF-DB以及遮挡和姿态变化数据集。

Conclusion: PCNN通过并行学习局部特征、多域特征融合和两阶段损失函数，有效提升了面部表情识别的性能，特别是在捕捉面部细微变化方面表现突出。

Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [257] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

Relevance: 25.0

TL;DR: DragMesh是一个实时交互式3D关节运动生成框架，通过解耦的动力学推理和运动生成方法，实现物理一致且实时的物体关节运动。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成模型在静态内容生成方面表现出色，但在理解物体运动和交互响应方面仍面临挑战。现有方法要么物理一致但速度慢，要么生成速度快但违反基本运动学约束。需要一种既能保持物理一致性又能实时交互的解决方案。

Method: 1. 解耦的动力学推理框架：先通过语义意图推理确定关节类型，再通过Kinematics Prediction Network (KPP-Net)进行几何回归确定轴和原点
2. 基于对偶四元数的运动表示：开发Dual Quaternion VAE (DQ-VAE)，利用对偶四元数的紧凑、连续、无奇异性特性
3. 条件生成机制：使用FiLM在DQ-VAE的Transformer解码器的每一层注入关节先验，确保运动学约束
4. 数值稳定的叉积损失：保证轴对齐

Result: DragMesh实现了实时性能，能够在无需重新训练的情况下对新颖物体进行合理的生成式关节运动，为生成式3D智能提供了实用的一步。

Conclusion: DragMesh通过解耦的动力学推理和运动生成框架，解决了实时交互式3D关节运动的挑战，在保持物理一致性的同时实现了实时性能，展示了生成式3D智能的进展。

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [258] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

Relevance: 25.0

TL;DR: Sanvaad是一个轻量级多模态无障碍框架，支持聋人、视障用户与听力正常人群之间的实时双向通信，结合了手势识别、语音转手语和语音界面技术。


<details>
  <summary>Details</summary>
Motivation: 当前聋人、视障用户与听力正常人群之间的通信工具通常只支持单向交互，存在局限性。需要开发一个支持双向实时通信的轻量级无障碍框架，促进包容性交流。

Method: 1. 为聋人用户：基于MediaPipe地标构建印度手语识别模块，利用其高效低计算负载特性；语音转手语组件将检测到的语音映射到预定义短语并生成相应GIF或字母可视化
2. 为视障用户：提供无屏幕语音界面，集成多语言语音识别、文本摘要和文本转语音生成
3. 通过Streamlit界面整合所有组件，支持桌面和移动环境

Result: 开发了Sanvaad框架，能够在边缘设备上平稳运行，无需专用硬件，为包容性通信提供实用可访问的解决方案。

Conclusion: Sanvaad通过结合轻量级计算机视觉和语音处理工具，为包容性通信提供了实用的可访问途径，解决了当前无障碍通信工具的单向交互限制。

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [259] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

Relevance: 25.0

TL;DR: U-CycleMLP：一种用于医学图像分割的新型U型编码器-解码器网络，通过位置注意力权重激励块、密集空洞块和通道CycleMLP块来平衡分割精度与计算效率，有效捕获局部和全局上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型难以有效捕获局部和全局上下文信息，导致边界像素丢失和分割错误。需要在保持轻量级架构的同时提升分割性能。

Method: 提出U-CycleMLP网络：编码器使用位置注意力权重激励块、密集空洞块和下采样操作学习多尺度上下文特征；解码器通过上采样、密集空洞块和特征融合机制重建高分辨率分割掩码；在跳跃连接中引入通道CycleMLP块增强特征集成，同时保持线性计算复杂度。

Result: 在三个基准数据集上的实验表明，U-CycleMLP在分割精度上具有竞争力，在所有数据集上实现更好的分割准确率，能够捕获细粒度解剖结构，并在不同医学成像模态中表现出鲁棒性。

Conclusion: U-CycleMLP通过创新的架构设计有效解决了医学图像分割中局部和全局上下文信息捕获的挑战，在保持计算效率的同时显著提升了分割性能。

Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [260] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

Relevance: 25.0

TL;DR: GNC-Pose是一个完全无需学习的单目6D物体姿态估计方法，结合渲染初始化、几何感知对应加权和鲁棒的GNC优化，在纹理物体上实现竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统学习基方法需要大量训练数据和类别特定先验的问题，提供一个简单、鲁棒且无需学习的6D姿态估计解决方案。

Method: 1) 渲染初始化获取粗2D-3D对应关系；2) 引入几何感知的聚类加权机制，基于3D结构一致性分配点级置信度；3) 使用GNC（渐进非凸性）优化处理离群点；4) 最终LM（Levenberg-Marquardt）细化提高精度。

Result: 在YCB物体和模型集上测试，尽管无需学习特征、训练数据或类别特定先验，GNC-Pose达到了与学习基和非学习方法相当的竞争性精度。

Conclusion: GNC-Pose为无需学习的6D姿态估计提供了一个简单、鲁棒且实用的解决方案，在严重离群点污染下显著稳定了优化过程。

Abstract: We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [261] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出STRank损失函数，通过建模基因相对表达模式而非绝对表达值，提高病理图像中基因表达预测的鲁棒性，减少RNA测序噪声和批次效应的影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于点损失函数的方法难以准确预测绝对基因表达值，因为RNA测序技术复杂且细胞间存在固有变异性，导致观测到的基因表达包含随机噪声和批次效应。需要一种更鲁棒的方法来从病理图像中估计基因表达。

Method: 提出STRank损失函数，基于基因相对表达模式在独立实验中保持一致的假设，学习基因间的相对表达关系而非绝对表达水平。该方法对噪声和批次效应具有鲁棒性。

Result: 在合成数据集和真实数据集上的实验证明了STRank方法的有效性，相比传统点损失函数，能更好地处理噪声和批次效应。

Conclusion: 通过关注相对表达模式而非绝对表达值，STRank提供了一种更鲁棒的病理图像基因表达估计方法，有助于降低RNA测序成本。

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [262] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出分层图像引导的3D分割框架，通过从实例级到部件级的渐进细化，解决工业场景中遮挡和尺度差异带来的分割挑战。


<details>
  <summary>Details</summary>
Motivation: 工业环境中密集布局和多尺度物体导致可靠3D分割困难：严重遮挡削弱几何边界，尺度差异使端到端模型难以同时捕捉粗粒度和细粒度细节。现有方法要么需要昂贵标注，要么存在跨视图语义不一致问题。

Method: 分层图像引导框架：1) 实例分割：渲染俯视图，用YOLO-World提示SAM生成掩码，反向投影到3D点云；2) 部件级分割：对每个实例渲染多视图图像，在各视图应用相同2D分割和反向投影，通过贝叶斯更新融合确保跨视图语义一致性。

Result: 在真实工厂数据上实验表明，方法能有效处理遮挡和结构复杂性，获得一致高的每类mIoU分数。在公开数据集上的额外评估证实了框架的泛化能力，突显其鲁棒性、标注效率和适应不同3D环境的能力。

Conclusion: 提出的分层图像引导3D分割框架通过渐进细化和跨视图一致性融合，有效解决了工业场景中的遮挡和尺度问题，实现了高效、鲁棒的3D分割。

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [263] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: BLDA提出一种平衡学习方法，通过分析预测logits分布来识别过预测和欠预测类别，使用共享锚分布对齐不同类别的logits分布，并在线估计logits分布以生成无偏伪标签，改善语义分割领域自适应中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在语义分割的无监督领域自适应中，自训练方法由于数据分布偏移和类别不平衡，难以平衡地学习各个类别。现有方法通常需要先验知识来应对分布偏移，而BLDA旨在直接评估和缓解类别偏差，无需预先了解分布偏移情况。

Method: 1. 通过分析预测logits分布识别过预测和欠预测类别；2. 使用共享锚分布进行后处理，对齐不同类别的logits分布；3. 在线估计logits分布，将logits校正项纳入损失函数以生成无偏伪标签；4. 利用累积密度作为领域共享的结构知识连接源域和目标域。

Result: 在两个标准UDA语义分割基准测试上的广泛实验表明，BLDA在集成到多种现有方法中时能持续提升性能，特别是对欠预测类别的改善效果显著。

Conclusion: BLDA通过平衡学习方法有效解决了语义分割领域自适应中的类别不平衡问题，无需先验分布知识，能显著提升欠预测类别的性能，为自训练方法提供了有效的偏差缓解机制。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [264] [Persistent Homology-Guided Frequency Filtering for Image Compression](https://arxiv.org/abs/2512.07065)
*Anil Chintapalli,Peter Tenholder,Henry Chen,Arjun Rao*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了一种结合离散傅里叶变换和持久同调分析的方法，用于从噪声图像数据集中提取特定频率，这些频率对应图像的拓扑特征，从而实现图像压缩并提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 噪声图像数据集中的特征提取面临模型可靠性挑战。传统方法在噪声条件下难以区分有意义的数据，需要一种能保持重要拓扑特征同时压缩图像的方法。

Method: 使用离散傅里叶变换结合持久同调分析，提取与图像拓扑特征对应的特定频率。通过持久同调引导的频率过滤实现图像压缩和重构，同时保持有意义的特征区分能力。

Result: 实验结果显示，该方法在六种不同指标上达到了与JPEG相当的压缩水平。在增强卷积神经网络时，相比传统特征提取和压缩方法，在二元分类任务中表现出潜在的性能提升。

Conclusion: 持久同调引导的频率过滤方法能够提高噪声条件下图像压缩的可靠性，为图像处理提供了一种结合拓扑分析和频率分析的新途径。

Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.

</details>


### [265] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种新的上下文感知评估范式Context-measure，用于伪装物体分割任务，通过概率像素感知相关框架考虑空间依赖性和像素级伪装量化，比现有上下文无关指标更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 当前伪装场景的评估指标忽视了上下文依赖这一关键因素，这些指标原本是为评估通用或显著物体设计的，假设空间上下文不相关。这种局限性导致现有指标无法准确评估伪装物体分割性能。

Method: 提出了Context-measure评估范式，基于概率像素感知相关框架，通过建模空间依赖关系和像素级伪装量化，使评估更符合人类感知。该方法考虑了伪装物体与背景的空间上下文关系。

Result: 在三个具有挑战性的伪装物体分割数据集上的广泛实验表明，Context-measure比现有的上下文无关指标提供更可靠的评估结果，能更好地反映模型在真实伪装场景中的性能。

Conclusion: Context-measure为涉及伪装模式的计算机视觉应用（如农业、工业和医疗场景）提供了基础性评估基准，解决了现有指标忽视上下文依赖的问题。

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [266] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

Relevance: 25.0

TL;DR: COREA提出首个统一框架，联合学习可重光照3D高斯和SDF，实现精确几何重建和忠实重光照，通过3D到3D对齐策略解决现有方法几何粗糙和BRDF-光照分解不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法虽然扩展到网格重建和基于物理的渲染，但其几何仍从2D渲染中学习，导致表面粗糙和BRDF-光照分解不可靠。需要直接在3D空间中学习几何信号的方法。

Method: 提出粗到细的双向3D到3D对齐策略：深度提供粗对齐，深度梯度和法线细化精细结构；引入密度控制机制稳定高斯增长；联合学习可重光照3D高斯和SDF。

Result: 在标准基准测试中，COREA在新视角合成、网格重建和基于物理的渲染方面实现优越性能，在统一框架内平衡几何保真度和内存效率。

Conclusion: COREA通过3D到3D对齐策略成功解决了现有方法的局限性，实现了精确几何重建和稳定BRDF-光照分解的统一框架。

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [267] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种基于能量函数约束注意力机制的虚拟试衣方法，通过优化生成过程中的注意力分布来提升服装细节一致性，并设计了新的评估指标VTID。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在生成服装时存在模式、纹理和边界方面的不一致问题，且现有评估指标仅关注图像真实性而忽视与目标元素的匹配度。

Method: 使用能量函数对生成过程中的注意力图施加约束，使注意力更集中于服装区域，从而提升生成结果与目标服装细节的一致性。同时提出了新的评估指标VTID。

Result: 在VITON-HD和DressCode数据集上，在LPIPS、FID、KID和VTID指标上分别超越SOTA方法1.4%、2.3%、12.3%和5.8%。在下游CC-Reid任务中，在LTCC、PRCC、VC-Clothes数据集上Rank-1指标分别提升2.5%、1.1%和1.6%。

Conclusion: 提出的能量函数约束注意力机制能有效提升虚拟试衣的服装细节一致性，新的VTID指标能更全面地评估生成质量，方法在下游任务中也表现出良好的泛化能力。

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [268] [Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach](https://arxiv.org/abs/2512.07170)
*Jiayang Li,Chengjie Jiang,Junjun Jiang,Pengwei Liang,Jiayi Ma,Liqiang Nie*

Main category: cs.CV

Relevance: 25.0

TL;DR: DiTFuse：基于扩散Transformer的指令驱动图像融合框架，通过自然语言指令实现端到端、语义感知的多模态图像融合，支持红外-可见光、多焦点、多曝光等多种融合任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在鲁棒性、适应性和可控性方面存在局限，大多数融合网络针对特定任务设计，缺乏灵活融入用户意图的能力，特别是在低光退化、色偏或曝光不平衡等复杂场景中。此外，缺乏真实融合图像作为ground truth以及现有数据集规模较小，使得训练同时理解高层语义和执行细粒度多模态对齐的端到端模型变得困难。

Method: 提出DiTFuse框架，基于扩散Transformer架构，在共享潜在空间中联合编码两幅图像和自然语言指令，实现分层和细粒度的融合控制。训练阶段采用多退化掩码图像建模策略，使网络能够联合学习跨模态对齐、模态不变恢复和任务感知特征选择，无需依赖ground truth图像。构建了多粒度指令数据集以增强模型的交互融合能力。

Result: 在公开的IVIF、MFF和MEF基准测试中展现出优越的定量和定性性能，具有更清晰的纹理和更好的语义保留。模型支持多级用户控制和零样本泛化到其他多图像融合场景，包括指令条件分割。

Conclusion: DiTFuse统一了红外-可见光、多焦点和多曝光融合以及文本控制细化和下游任务，在单一架构中实现了端到端、语义感知的图像融合，克服了传统预融合和后融合管道的局限性。

Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

</details>


### [269] [RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction](https://arxiv.org/abs/2512.07191)
*Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种基于Retinex反射率分解的变分水平集模型（RefLSM），用于医学图像分割，通过分解图像为反射率和偏置场分量，直接分割光照不变的反射率，提高了在非均匀光照条件下的分割精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临强度不均匀、噪声、边界模糊和不规则结构等挑战。传统水平集方法依赖于近似的偏置场估计，在严重非均匀成像条件下表现不佳。需要一种能够处理光照变化并保留精细结构细节的分割方法。

Method: 提出RefLSM模型，将Retinex启发的反射率分解整合到分割框架中：1）将观测图像分解为反射率和偏置场分量；2）引入线性结构先验，引导平滑反射率梯度朝向数据驱动的参考；3）嵌入松弛二元水平集，通过凸松弛和符号投影实现稳定演化，避免重新初始化引起的扩散；4）使用ADMM优化方案高效求解变分问题。

Result: 在多个医学成像数据集上的实验表明，RefLSM相比最先进的水平集方法，在分割精度、鲁棒性和计算效率方面都表现出优越性能。

Conclusion: RefLSM通过反射率分解有效处理医学图像中的光照不均匀问题，结合结构先验和松弛水平集实现了精确、鲁棒的分割，为医学图像分析提供了有效的解决方案。

Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

</details>


### [270] [HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression](https://arxiv.org/abs/2512.07192)
*Niu Yi,Xu Tianyi,Ma Mingming,Wang Xinkun*

Main category: cs.CV

Relevance: 25.0

TL;DR: HVQ-CGIC：基于VQ超先验的可控生成图像压缩框架，通过引入超先验到VQ索引熵模型，显著提升率失真性能，相比现有方法节省61.3%比特率。


<details>
  <summary>Details</summary>
Motivation: 现有基于向量量化（VQ）的生成式图像压缩方法通常使用静态全局概率分布估计VQ索引的熵，无法适应每张图像的具体内容，导致比特率潜力未充分利用且难以实现灵活的码率控制。

Method: 提出基于VQ超先验的可控生成图像压缩框架（HVQ-CGIC）：1）严格推导了将超先验引入VQ索引熵模型的数学基础；2）通过新颖的损失设计，首次在基于VQ的生成式图像压缩中引入率失真平衡与控制；3）配合轻量级超先验估计网络。

Result: 在Kodak数据集上，与Control-GIC、CDC和HiFiC相比，在相同LPIPS质量下平均节省61.3%的比特率，在率失真性能上显著优于当前最先进的生成式压缩方法。

Conclusion: HVQ-CGIC有望成为VQGAN-based图像压缩的基础组件，类似于超先验框架在神经图像压缩中的核心作用，为生成式图像压缩提供了有效的率失真控制方案。

Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.

</details>


### [271] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

Relevance: 25.0

TL;DR: AutoLugano是一个用于淋巴瘤分类的端到端深度学习系统，通过FDG-PET/CT扫描自动进行病灶分割、解剖定位和Lugano分期


<details>
  <summary>Details</summary>
Motivation: 开发一个完全自动化的系统，将基线FDG-PET/CT扫描转化为完整的Lugano分期，以辅助淋巴瘤的初始分期、治疗分层和临床决策

Method: 系统包含三个顺序模块：1) 基于3D nnU-Net的解剖感知病灶分割；2) 使用TotalSegmentator工具包进行基于图谱的解剖定位；3) 将受累区域空间分布转化为Lugano分期和治疗分组的自动分期模块

Result: 在外部验证集上，区域受累检测准确率88.31%，敏感性74.47%，特异性94.21%，F1分数80.80%。治疗分层（局限期vs进展期）准确率85.07%，特异性90.48%，敏感性82.61%

Conclusion: AutoLugano是首个完全自动化的端到端管道，可将单次基线FDG-PET/CT扫描转化为完整Lugano分期，在辅助临床决策方面具有强大潜力

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [272] [Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds](https://arxiv.org/abs/2512.07211)
*Frederik Hagelskjær,Dimitrios Arapis,Steffen Madsen,Thorbjørn Mosekjær Iversen*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出首个仅使用3D无色数据的深度学习方法来估计物体姿态不确定性，无需RGB输入，在真实世界分拣场景中验证


<details>
  <summary>Details</summary>
Motivation: 传统姿态估计方法只提供单一姿态估计，无法捕捉视觉模糊性带来的不确定性，而现有姿态分布方法严重依赖颜色信息，这在工业场景中往往不可用

Method: 基于神经网络的方法，仅使用3D无色数据估计物体姿态不确定性，是首个不依赖RGB输入的深度学习姿态分布估计方法

Result: 在具有不同几何模糊性的物体分拣场景中验证了方法的有效性，当前实现专注于反射和旋转对称性，但框架可扩展到完整的SE(3)姿态分布估计

Conclusion: 该方法为工业场景中的姿态不确定性估计提供了可行的解决方案，特别是在颜色信息不可用的情况下，具有实际应用价值

Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io

</details>


### [273] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 论文提出了MICo-150K数据集、MICo-Bench基准和Qwen-MICo基线模型，用于解决多图像组合(MICo)任务中的训练数据缺乏和评估困难问题。


<details>
  <summary>Details</summary>
Motivation: 多图像组合(MICo)任务缺乏高质量训练数据和系统评估基准，阻碍了可控图像生成中多参考输入合成一致图像的研究进展。

Method: 1) 将MICo分类为7个代表性任务；2) 收集高质量源图像并构建多样MICo提示；3) 利用专有模型合成平衡组合图像，经人工筛选得到MICo-150K数据集；4) 构建分解-重组子集；5) 创建MICo-Bench基准和Weighted-Ref-VIEScore评估指标；6) 在MICo-150K上微调多个模型。

Result: MICo-150K有效提升了模型的MICo能力，基线模型Qwen-MICo在3图像组合任务上达到Qwen-Image-2509水平，且支持任意多图像输入。数据集、基准和基线为MICo研究提供了宝贵资源。

Conclusion: 该工作系统解决了MICo任务的数据和评估瓶颈，为多图像组合研究提供了全面资源，推动了可控图像生成领域的发展。

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [274] [Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency](https://arxiv.org/abs/2512.07379)
*Mahila Moghadami,Mohammad Ali Keyvanrad,Melika Sabaghian*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文提出了一种改进的SW-YOLO方法，用于大尺度航拍图像中的小目标检测，通过优化滑动窗口裁剪策略和架构改进，在VisDrone2019数据集上取得了显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 随着航拍图像在关键和工业应用中的重要性日益增长，需要强大的小目标检测框架。现有方法通常涉及图像裁剪和检测器架构修改，但仍有改进空间。

Method: 基于SW-YOLO方法，优化滑动窗口的裁剪尺寸和重叠策略，并在架构上进行改进：在backbone中集成CBAM以保留空间和通道信息，在neck部分加入高级特征提取模块增强特征图，设计新的head提升小目标检测精度。

Result: 在VisDrone2019数据集上，将mAP .5:.5精度从YOLOv5L的35.5提升到61.2，显著优于SAHI和CZDet（58.36）等现有方法。

Conclusion: 提出的方法通过优化裁剪策略和架构改进，显著提升了航拍图像中小目标检测的精度，为相关应用提供了有效的解决方案。

Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.

</details>


### [275] [Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior](https://arxiv.org/abs/2512.07498)
*Chih-Chung Hsu,Shao-Ning Chen,Chia-Ming Lee,Yi-Fang Wang,Yi-Shiuan Chou*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出LR-GCN方法，通过构建无序时序图嵌入和拉普拉斯谱先验，在仅使用干净面部数据训练的情况下，实现对噪声或无序面部序列的深度伪造鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器通常假设面部序列具有时间一致性和清洁性，但在真实场景中，压缩伪影、遮挡和对抗攻击会导致面部检测不稳定，产生无效或误检的面部。需要一种能处理噪声或无序面部序列的鲁棒检测方法。

Method: 提出拉普拉斯正则化图卷积网络(LR-GCN)，构建无序时序图嵌入(OF-TGE)将帧级CNN特征组织成基于语义相似度的自适应稀疏图。引入双重稀疏机制抑制无效面部影响，并采用图拉普拉斯谱先验作为图谱域的高通算子，突出结构异常和伪造伪影，再通过低通GCN聚合实现任务驱动的谱带通机制。

Result: 在FF++、Celeb-DFv2和DFDC数据集上的实验表明，LR-GCN实现了最先进的性能，并在严重全局和局部干扰（包括缺失面部、遮挡和对抗性扰动面部检测）下显著提高了鲁棒性。

Conclusion: LR-GCN通过图结构建模和谱先验，有效解决了真实场景中深度伪造检测面临的噪声和序列无序问题，为鲁棒深度伪造检测提供了新思路。

Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.

</details>


### [276] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

Relevance: 25.0

TL;DR: ControlVP：用户引导的消失点校正框架，用于修正文本到图像生成中的几何不一致问题


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如Stable Diffusion）在视觉质量上表现优秀，但经常出现几何不一致问题，特别是消失点不一致，导致生成场景的结构真实性受损，影响空间真实感，尤其是在建筑场景中。

Method: 扩展预训练扩散模型，通过建筑轮廓提供结构引导，引入几何约束明确鼓励图像边缘与透视线索的对齐，实现用户引导的消失点校正。

Result: 该方法增强了全局几何一致性，同时保持了与基线相当的视觉保真度，特别适用于需要准确空间结构的应用，如图像到3D重建。

Conclusion: ControlVP为解决文本到图像生成中的几何不一致问题提供了有效解决方案，通过用户引导的结构校正提升了生成图像的空间真实感。

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [277] [Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation](https://arxiv.org/abs/2512.07590)
*Kaili Qi,Zhongyi Huang,Wenli Yang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出VM_TUNet的鲁棒版本，结合变分方法和深度学习，通过物理先验、边缘检测器和平均曲率项改进Cahn-Hilliard方程，用于噪声图像分割


<details>
  <summary>Details</summary>
Motivation: 解决噪声图像分割中边界模糊或断裂的挑战，结合变分PDE的可解释性和边界平滑优势与深度神经网络的强大表示能力

Method: 混合框架包含两个协作模块：F模块进行频域预处理缓解局部极小值问题，T模块确保准确稳定的局部计算；将物理先验、边缘检测器和平均曲率项集成到改进的Cahn-Hilliard方程中

Result: 在三个基准数据集上取得性能与计算效率的平衡，相比纯CNN模型获得有竞争力的定量结果和更好的视觉质量，接近基于Transformer方法的性能但计算成本合理

Conclusion: 提出的方法成功结合了变分PDE和深度学习的优势，为噪声图像分割提供了一种有效且计算高效的解决方案

Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.

</details>


### [278] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

Relevance: 25.0

TL;DR: SAM 3在机器人辅助手术中的实证评估，展示了其在零样本分割、语言提示分割和3D重建方面的能力，相比前代有明显改进，但在手术领域的语言提示性能仍需提升。


<details>
  <summary>Details</summary>
Motivation: 评估SAM 3在机器人辅助手术中的性能，特别是其新引入的语言提示分割和增强的3D感知能力，以了解其在医疗领域的实际应用潜力。

Method: 在MICCAI EndoVis 2017和2018基准上进行综合测试，评估SAM 3的零样本分割（点、边界框提示）和语言提示分割，同时测试其在SCARED、StereoMIS和EndoNeRF上的3D重建能力。

Result: SAM 3在空间提示下的图像和视频分割明显优于SAM和SAM 2，在单目深度估计和3D器械重建方面表现良好，但语言提示在手术领域性能欠佳，复杂动态场景仍有局限。

Conclusion: SAM 3在机器人手术中展现出显著进步，特别是空间提示分割和3D重建能力，但语言提示需要领域特定训练，复杂动态场景的处理仍需改进。

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [279] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文介绍了LiQA数据集，用于肝脏分割和肝纤维化分期任务，作为CARE 2024挑战赛的一部分。数据集包含440名患者的多期相、多中心MRI扫描，旨在评估算法在复杂临床环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化是全球重大健康负担，需要准确分期以进行有效临床管理。当前缺乏能够评估算法在真实世界复杂条件下（如域偏移、模态缺失、空间错位）性能的标准化数据集。

Method: 1. 创建LiQA数据集：包含440名患者的多期相、多中心MRI扫描
2. 提出基准方法：半监督学习框架结合外部数据进行稳健分割；多视图共识方法结合CAM正则化进行分期
3. 评估策略：在复杂临床条件下测试算法鲁棒性

Result: 评估显示，利用多源数据和解剖约束能显著增强模型在临床环境中的鲁棒性。挑战赛的最佳方法在肝脏分割和肝纤维化分期任务上表现出色。

Conclusion: LiQA数据集为肝脏分割和肝纤维化分期提供了有价值的基准，展示了多源数据整合和解剖约束在提升临床模型鲁棒性方面的重要性。

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [280] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

Relevance: 25.0

TL;DR: 首个从自由移动单目视频中联合预测部件分割和关节参数的数据驱动方法，仅用合成数据训练即可泛化到真实物体


<details>
  <summary>Details</summary>
Motivation: 理解铰接物体对机器人和数字孪生至关重要，但现有方法主要依赖多视角系统、物体扫描或静态相机，缺乏从自由移动单目视频中恢复部件分割和关节参数的解决方案

Method: 提出首个数据驱动方法，从自由移动单目视频中联合预测部件分割和关节参数，仅使用合成数据训练，可直接处理随意录制的视频

Result: 方法在合成数据上训练后能很好地泛化到真实世界物体，为铰接物体理解提供了可扩展且实用的解决方案，适用于动态环境的实时应用

Conclusion: 该方法首次实现了从自由移动单目视频中联合学习部件分割和关节参数，为铰接物体理解提供了实用且可扩展的解决方案

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [281] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出三阶段预处理流程，通过笼子分割、修复和评估，解决动物追踪与姿态估计在笼状结构遮挡下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有动物追踪与姿态估计系统（如STEP、ViTPose）在处理带有笼状结构和系统性遮挡的图像视频时性能显著下降，需要解决这一实际应用中的关键限制。

Method: 三阶段预处理流程：1) 使用Gabor增强的ResNet-UNet架构进行笼子分割，包含72个方向核；2) 使用CRFill进行内容感知的笼子修复；3) 在修复后的帧上评估姿态估计与追踪性能。

Result: 实验验证表明，通过该流程去除笼子遮挡后，姿态估计与追踪性能可达到与无遮挡环境相当的水平，关键点检测精度和轨迹一致性均有显著提升。

Conclusion: 提出的三阶段预处理流程能有效解决笼状结构遮挡问题，使现有动物追踪与姿态估计系统在复杂遮挡环境下仍能保持高性能。

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [282] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一种结合3D重建模型和视频扩散模型的方法，从单张图像生成高质量上半身3D虚拟形象，解决现有方法在纹理模糊、运动僵硬和结构不稳定之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D虚拟形象生成存在两难：基于重建的方法能产生稳定结构但纹理模糊、运动僵硬；基于视频生成的方法能产生逼真动态但结构不稳定、身份漂移。需要结合两者优势。

Method: 使用3D重建模型提供结构和外观先验，指导实时自回归视频扩散模型进行渲染，结合几何稳定性和生成能力，实现高质量纹理和流畅动态。

Result: 实验表明该方法显著减少伪影，在视觉质量上大幅超越现有领先方法，为游戏和VR等实时应用提供鲁棒高效解决方案。

Conclusion: 通过结合3D重建的几何稳定性和视频模型的生成能力，实现了高质量数字虚拟形象的实时生成，解决了纹理模糊和运动僵硬问题。

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [283] [Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation](https://arxiv.org/abs/2512.05992)
*Azeez Idris,Abdurahman Ali Mohammed,Samuel Fanijo*

Main category: eess.IV

Relevance: 25.0

TL;DR: 该论文评估了自监督对比学习中强数据增强对医学图像语义分割的影响，发现现有增强方法并不总能提升性能，并探索了更有效的增强策略。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习在多个下游任务中表现出性能提升，但其中的关键组件——强数据增强（涉及多种增强技术的组合）在医学图像语义分割中的效果尚未充分验证。作者发现现有数据增强方法并不总能改善医学图像分割性能，因此需要探索更有效的增强策略。

Method: 论文采用实验研究方法，系统地评估了现有强数据增强技术在医学图像语义分割任务中的效果。通过对比不同增强策略的性能，识别现有方法的局限性，并尝试其他增强技术来寻找更有效的解决方案。

Result: 研究发现，现有的数据增强技术并不总是能提升医学图像语义分割的性能，这与自然图像中的观察结果不同。通过实验探索，作者找到了能够提供改进性能的其他增强方法。

Conclusion: 强数据增强在自监督对比学习中的作用具有任务和领域特异性，特别是在医学图像语义分割中，需要针对性地设计增强策略，而不是简单套用自然图像中的方法。

Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.

</details>


### [284] [Dynamic Visual SLAM using a General 3D Prior](https://arxiv.org/abs/2512.06868)
*Xingguang Zhong,Liren Jin,Marija Popović,Jens Behley,Cyrill Stachniss*

Main category: cs.RO

Relevance: 25.0

TL;DR: 提出一种新颖的单目视觉SLAM系统，能够在动态自然场景中鲁棒地估计相机位姿，通过结合几何补丁在线束调整和前馈重建模型的互补优势


<details>
  <summary>Details</summary>
Motivation: 动态自然环境中相机位姿估计和3D重建具有挑战性，场景动态会严重影响相机位姿估计精度。现有方法在动态场景中表现不佳，需要更鲁棒的解决方案

Method: 提出单目视觉SLAM系统，结合几何补丁在线束调整和前馈重建模型。使用前馈重建模型精确过滤动态区域，并利用其深度预测增强补丁视觉SLAM的鲁棒性。通过将深度预测与束调整估计的补丁对齐，处理前馈重建模型批量应用固有的尺度模糊性

Result: 系统能够在动态场景中鲁棒地估计相机位姿，通过深度预测与几何补丁对齐有效处理尺度模糊问题，提高SLAM在动态环境中的准确性和稳定性

Conclusion: 提出的方法成功解决了动态自然环境中相机位姿估计的挑战，通过结合几何方法和深度学习模型的优势，为动态场景SLAM提供了有效的解决方案

Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.

</details>


### [285] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna Förster*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出ShadowWolf框架，通过集成和优化AI模型训练与评估阶段，解决野生动物监测中环境变化带来的挑战，实现动态模型重训练以减少标注工作量并提升适应性。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长导致人类栖息地扩张，野生动物空间减少，人兽互动增加。这些互动从轻微干扰（如浣熊翻垃圾桶）到严重后果（如物种灭绝）不等。传统AI训练面临景观、天气、光照、相机-动物距离等多变环境因素的挑战，影响模型在真实场景中的鲁棒性和适应性。

Method: 提出名为ShadowWolf的统一框架，集成并优化AI模型训练和评估阶段。该框架支持动态模型重训练，以适应环境条件和应用需求的变化，减少标注工作量，并允许现场模型适应。

Result: 该自适应统一方法提高了野生动物监测系统的准确性和效率，促进了更有效和可扩展的保护工作。

Conclusion: ShadowWolf框架通过集成训练和评估阶段，实现动态适应环境变化，为野生动物监测提供了更鲁棒、高效的AI解决方案。

Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [286] [MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection](https://arxiv.org/abs/2512.07110)
*Liangwei Jiang,Jinluo Xie,Yecheng Huang,Hua Zhang,Hongyu Yang,Di Huang*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出MSN（多方向相似性网络）用于检测复制-移动图像伪造，通过双流架构解决现有方法在表示和定位上的限制，并在多个基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 复制-移动图像伪造检测面临复杂变换和精细操作的挑战，现有深度检测模型在表示和定位方面存在限制，需要更准确高效的检测方法。

Method: 提出双流MSN模型：1）表示方面，使用多方向CNN网络分层编码图像，通过尺度和旋转的多样化增强提高特征相似性度量；2）定位方面，设计基于2-D相似性矩阵的解码器，充分利用整个图像的空间信息。

Result: 在CASIA CMFD、CoMoFoD和新提出的基准测试上进行广泛实验，报告了最先进的结果，证明了方法的有效性。

Conclusion: MSN模型在复制-移动伪造检测中表现出色，解决了现有方法的表示和定位限制，同时提出了新的深度合成伪造数据库作为基准。

Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.

</details>


### [287] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

Relevance: 15.0

TL;DR: VG3T提出了一种新颖的多视角前馈网络，通过3D高斯表示预测3D语义占据，解决了现有方法多视角融合困难导致的碎片化3D表示问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角图像生成3D场景表示时存在多视角融合困难，导致3D表示碎片化和性能不佳。需要一种能够统一处理几何和语义的范式来克服逐视角处理的碎片化和不一致性问题。

Method: VG3T采用多视角前馈网络直接预测一组语义属性化的3D高斯表示，而不是从单视角图像推断高斯。引入两个关键组件：基于网格的采样和位置细化，以缓解像素对齐高斯初始化方法中常见的距离依赖密度偏差。

Result: 在nuScenes基准测试中，VG3T相比先前最先进方法实现了1.7%的mIoU提升，同时使用了46%更少的基元，显示出卓越的效率和性能。

Conclusion: VG3T通过多视角联合预测3D高斯表示，提供了一种统一处理几何和语义的范式，显著提高了3D场景重建的效率和准确性。

Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [288] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于机器学习的自动化框架，通过高通量成像、形状提取和聚类分析来大规模表征金属粉末形态，为SLM增材制造提供实时原料监控


<details>
  <summary>Details</summary>
Motivation: 传统粉末表征方法通量低且定性，无法捕捉工业规模批次的异质性，而SLM零件质量严重依赖原料形态，需要高效、自动化的形态分析方案

Method: 开发三种聚类流程：自编码器流程、形状描述符流程和函数数据流程，在约126,000张粉末图像数据集上评估，使用傅里叶描述符+k-means方法

Result: 傅里叶描述符+k-means流程表现最佳，获得最低的Davies-Bouldin指数和最高的Calinski-Harabasz分数，同时在标准工作站上保持亚毫秒级处理速度

Conclusion: 该无监督学习框架实现了粉末形态的快速自动化评估，支持跟踪形状演化，为SLM工作流中的实时原料监控提供了可行路径

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [289] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一种用于波浪自由表面三维重建的注意力增强金字塔神经网络，结合物理约束从立体视觉数据重建非线性三维速度场，在真实海洋条件下实现毫米级精度和高效率重建。


<details>
  <summary>Details</summary>
Motivation: 解决长期海洋波浪观测任务中密集视觉重建的高计算成本问题，以及持续视觉遮挡带来的挑战，实现对波浪自由表面和速度场的精确三维重建。

Method: 设计注意力增强的金字塔架构神经网络，针对波浪运动的多尺度和时间连续特性进行优化，结合物理约束从演化的自由表面边界进行时间分辨的三维速度场重建。

Result: 在真实海洋条件下实现中央区域毫米级波浪高程预测，主导频率误差低于0.01 Hz，精确估计高频谱幂律，高保真三维非线性速度场重建，仅需1.35秒即可重建两百万个点。

Conclusion: 该模型基于立体视觉数据集，优于传统视觉重建方法，在遮挡条件下保持强泛化能力，得益于其全局多尺度注意力和学习的波浪传播动力学编码。

Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [290] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了一种基于深度学习的自动化工作流程，用于从剪切散斑干涉测量数据中生成缺陷标注，解决了工业应用中高质量标注数据集缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 剪切散斑干涉测量技术对表面位移梯度敏感，能有效检测安全关键部件的亚表面缺陷。然而，工业应用的主要限制是缺乏高质量标注数据集，因为手动标注劳动密集、主观性强且难以标准化。

Method: 引入自动化工作流程，利用深度学习从剪切散斑干涉测量数据中生成缺陷标注，包括高分辨率分割和边界框标签。

Result: 与专家标注数据相比，该方法展现出足够的准确性，能够支持弱监督训练，减少人工工作量，为稳健的缺陷检测支持可扩展的数据集创建。

Conclusion: 该自动化标注工作流程解决了剪切散斑干涉测量技术工业应用的关键瓶颈，通过减少对人工标注的依赖，促进了该技术在安全关键部件检测中的规模化应用。

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [291] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了首个联合检测投射阴影和附着阴影的框架，通过光照和几何推理形成闭环优化，并构建了首个包含两种阴影标注的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有阴影检测方法主要针对投射阴影，缺乏专门检测附着阴影的数据集和模型。附着阴影对理解物体三维结构和场景理解至关重要，需要填补这一研究空白。

Method: 系统包含阴影检测模块（分别预测两种阴影类型）和光照估计模块（从阴影推断光照方向）。结合表面法线生成几何一致的部分遮挡图，通过闭环推理迭代优化阴影分割和光照估计。

Result: 实验结果显示，迭代的几何-光照推理显著改善了附着阴影检测（BER减少至少33%），同时保持了良好的整体阴影和投射阴影性能。

Conclusion: 通过联合推理光照和几何关系，可以有效检测附着阴影，填补了阴影检测领域的重要空白，为三维场景理解提供了新方法。

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [292] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文提出了一种新颖的多模态颜色轨迹预测方法，用于食品干燥过程中的颜色演化预测，通过整合高维时序颜色信息和干燥工艺参数，在未见过的干燥条件下实现了超过90%的误差降低。


<details>
  <summary>Details</summary>
Motivation: 食品干燥过程中颜色演化是产品质量的重要指标，现有研究主要依赖低维颜色特征，无法充分捕捉食品样本复杂的动态颜色轨迹，且现有建模方法缺乏对未见工艺条件的泛化能力。

Method: 开发了一种多模态颜色轨迹预测方法，整合高维时序颜色信息与干燥工艺参数，实现准确且数据高效的颜色轨迹预测。

Result: 在未见干燥条件下，模型在饼干干燥中达到RMSE 2.12，苹果干燥中达到RMSE 1.29，相比基线模型误差降低超过90%。

Conclusion: 实验结果表明该模型具有优越的准确性、鲁棒性和广泛适用性，为食品干燥质量监控提供了有效工具。

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [293] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种改进的GLMB滤波器，支持同一传感器对同一目标的多检测，打破了检测间依赖关系，实现了更好的并行可扩展性，并开发了GPU加速实现。


<details>
  <summary>Details</summary>
Motivation: 标准GLMB滤波器在多目标跟踪中虽然具有理论优势，但计算成本高昂，尤其是在标准测量模型下维护多个假设时。当在分布式机器学习虚拟传感器网络中部署跟踪时，需要支持同一传感器对同一目标的多检测能力。

Method: 研究GLMB滤波器的变体，允许同一传感器对同一目标进行多次检测。这种方法打破了标准GLMB滤波器中检测间的依赖关系，使得滤波器更新具有更好的并行可扩展性，从而能够在GPU硬件上高效部署。

Result: 开发了GPU加速的GLMB跟踪器实现，并进行了初步分析，重点关注运行时间在目标数量和保留假设最大数量方面的可扩展性。

Conclusion: 提出的GLMB滤波器变体通过支持多检测能力，显著提高了并行计算效率，使得多目标跟踪方法能够在GPU硬件上高效实现，为分布式传感器网络中的实时跟踪应用提供了可行方案。

Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [294] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于点云框架的事件流时空建模方法，用于事件相机下的人体姿态估计，通过事件时间切片卷积和事件切片序列模块提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将事件流转换为密集事件帧，增加了计算量并牺牲了事件信号的高时间分辨率。需要充分利用事件流的时空特性来提升人体姿态估计性能

Method: 1) 基于点云框架处理事件流；2) 设计事件时间切片卷积模块捕捉短期依赖；3) 事件切片序列模块进行结构化时间建模；4) 在点云表示中应用边缘增强以改善稀疏条件下的空间边缘信息

Result: 在DHP19数据集上，该方法在三种代表性点云骨干网络（PointNet、DGCNN、Point Transformer）上均能持续提升性能

Conclusion: 提出的点云框架能有效利用事件流的时空特性，避免传统方法转换为密集帧的缺点，在事件相机人体姿态估计任务上表现优异

Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [295] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于区域感知的红外与可见光图像融合框架，通过多曝光多模态成像和空间变化曝光相机，在极端环境下保持几何保真度和热辐射信息。


<details>
  <summary>Details</summary>
Motivation: 在摄影测量中，准确融合红外和可见光谱同时保持可见特征的几何保真度并纳入热辐射信息是一个重大挑战，特别是在极端条件下。现有方法通常会损害可见图像质量，影响测量精度。

Method: 提出基于区域感知的融合框架，结合多曝光和多模态成像，使用空间变化曝光相机。包括：1）基于区域感知的特征融合确保精确的多模态配准；2）自适应融合与对比度增强；3）由区域显著性图引导的结构相似性补偿机制优化IR-VIS光谱集成；4）适应单曝光场景。

Result: 在合成和真实数据上的实验表明，相比最先进方法，该方法在图像清晰度和性能方面表现优越，定量和视觉评估都证实了这一点。

Conclusion: 该框架有效解决了极端环境下红外与可见光图像融合的挑战，在保持几何保真度的同时提升了图像质量，适用于多种成像条件。

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [296] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于深度学习的全自动方法，从2D合成人体图像估计关键人体测量指标，用于运动员心血管风险评估


<details>
  <summary>Details</summary>
Motivation: 传统运动员心血管检查中的人体测量方法劳动密集、依赖操作者且难以规模化，需要自动化解决方案来辅助筛查

Method: 使用从3D身体网格生成的100,000张合成图像数据集，训练VGG19、ResNet50和DenseNet121回归模型，通过全连接层预测五个关键人体测量指标

Result: 所有模型达到亚厘米级精度，ResNet50表现最佳，平均MAE为0.668厘米，证明深度学习可规模化提供准确人体测量数据

Conclusion: 深度学习可为运动员筛查提供实用的自动化人体测量工具，未来将在真实图像上验证以扩展应用

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [297] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种用于光伏基础设施自动检测的智能集成框架，通过多模态融合和自适应控制解决传统方法的局限性，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统光伏检测方法存在热调色板偏差、数据冗余和高通信带宽需求等关键缺陷，需要开发自动化、多模态的智能检测系统来提升光伏电站的安全性和运营效率。

Method: 采用协同架构：1) 通过表示一致性学习获得调色板不变的热嵌入；2) 通过门控机制与对比归一化的RGB流融合；3) 使用罗德里格斯更新的闭环自适应重采集控制器确认模糊异常；4) 基于DBSCAN和半正矢距离的地理空间去重模块。

Result: 在公开PVF-10基准测试中达到0.903的mAP@0.5，比单模态基线提升12-15%；现场验证召回率达96%；去重过程减少15-20%的重复误报；相关性遥测将空中数据传输减少60-70%。

Conclusion: 建立了一种主动式光伏检测的新范式，系统已具备现场应用准备，通过多模态融合和智能处理显著提升了检测性能和效率。

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [298] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出两种新颖深度学习架构用于脑肿瘤检测：SAETCN用于脑肿瘤分类（验证准确率99.38%），SAS-Net用于脑肿瘤分割（像素准确率99.23%）


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤对人类生命构成重大威胁，早期准确检测对诊断和治疗至关重要。传统放射科医生手动检测MRI图像耗时且困难，特别是近年来儿童和青少年脑肿瘤发病率上升导致数据量激增。现有模型泛化能力不足，在验证数据上表现不佳。

Method: 提出两种新颖深度学习架构：1) SAETCN（自注意力增强肿瘤分类网络）用于脑肿瘤分类，包含三种肿瘤类型（胶质瘤、脑膜瘤、垂体瘤）和非肿瘤病例；2) SAS-Net（自注意力分割网络）用于脑肿瘤精确分割。两种架构都利用了自注意力机制。

Result: SAETCN在验证数据集上达到99.38%的准确率，成为少数能够准确检测脑肿瘤的新颖深度学习架构之一；SAS-Net达到99.23%的整体像素准确率。

Conclusion: 提出的两种深度学习架构在脑肿瘤检测任务上表现出色，为计算机辅助诊断系统提供了有效的解决方案，能够自动准确地进行脑肿瘤分类和分割。

Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [299] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

Relevance: 15.0

TL;DR: EMGauss：基于高斯泼溅的3D动态场景渲染框架，用于从2D切片重建3D体积，解决体积电子显微镜中各向异性重建问题


<details>
  <summary>Details</summary>
Motivation: 体积电子显微镜(vEM)存在采集权衡，导致各向异性体积和有限轴向分辨率。现有深度学习方法基于各向同性假设，但在形态各向异性结构中失效

Method: 将切片到3D重建重新定义为基于高斯泼溅的3D动态场景渲染问题，将轴向切片进展建模为2D高斯点云的时间演化。引入Teacher-Student自举机制，在数据稀疏区域使用高置信度预测作为伪监督信号

Result: 相比扩散和GAN方法，EMGauss显著提高插值质量，支持连续切片合成，无需大规模预训练，为跨成像领域提供通用切片到3D解决方案

Conclusion: EMGauss规避了基于各向同性方法的固有局限，为vEM和各成像领域的3D重建提供了创新框架

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [300] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于骨架数据的图卷积长短期记忆注意力网络(GCN-LSTM-ATT)用于检测中风后补偿性运动，相比传统机器学习方法显著提高检测精度


<details>
  <summary>Details</summary>
Motivation: 大多数中风患者存在上肢运动功能障碍，康复训练中普遍存在补偿性运动，这对患者长期恢复不利，因此检测补偿性运动具有重要意义

Method: 使用Kinect深度相机采集16名中风患者执行特定康复运动的骨架数据，构建GCN-LSTM-ATT模型，并与SVM、KNN、RF等传统机器学习算法对比

Result: GCN-LSTM-ATT模型的检测准确率达到0.8580，显著高于传统机器学习算法，消融实验表明模型的每个组件都对性能提升有显著贡献

Conclusion: 该研究为中风后补偿性运动检测提供了更精确和强大的工具，有望促进中风患者康复训练策略的优化

Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [301] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

Relevance: 15.0

TL;DR: RDSplat提出了一种针对3D高斯溅射(3DGS)的鲁棒数字水印方法，专门抵抗基于扩散模型的编辑攻击，通过嵌入到低频高斯分量并结合对抗训练来保护3D资产版权。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯溅射技术被广泛用于创建数字资产，需要有效的版权保护机制。现有3DGS水印方法对基于扩散模型的编辑攻击非常脆弱，这些攻击可以轻易擦除嵌入的来源信息，因此迫切需要开发对扩散编辑具有内在鲁棒性的水印技术。

Method: RDSplat采用多域框架，在3DGS空间原生操作：1) 主动针对低频高斯分量嵌入水印，通过协调协方差正则化和2D滤波实现；2) 使用高斯模糊作为扩散编辑的低通滤波行为的训练代理，进行对抗性微调以增强水印鲁棒性。

Result: 在三个基准数据集上的综合定量和定性评估表明，RDSplat在扩散编辑下保持卓越的鲁棒性，同时保持水印不可见性，实现了最先进的性能。

Conclusion: RDSplat为3D高斯溅射资产提供了一种对扩散编辑具有鲁棒性的水印解决方案，通过将水印嵌入到扩散编辑固有保留的组件中，并结合对抗训练策略，有效解决了现有方法的脆弱性问题。

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [302] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出GGEV（广义几何编码体积）网络，在实时立体匹配中实现强泛化能力，通过深度感知特征和动态成本聚合提升跨域性能


<details>
  <summary>Details</summary>
Motivation: 现有实时立体匹配方法主要关注域内性能而忽视泛化能力，而现有立体基础模型虽然泛化好但推理延迟高。需要解决实时性与泛化能力之间的权衡问题。

Method: 1) 提取编码域不变结构先验的深度感知特征作为成本聚合指导；2) 引入深度感知动态成本聚合(DDCA)模块，自适应地将这些先验融入每个视差假设，增强未见场景中的脆弱匹配关系。

Result: GGEV在零样本泛化能力上超越所有现有实时方法，在KITTI 2012、KITTI 2015和ETH3D基准测试中达到最先进性能。

Conclusion: 提出的轻量级互补方法构建了具有强泛化能力的广义几何编码体积，成功解决了实时立体匹配中泛化与效率的权衡问题。

Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [303] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

Relevance: 15.0

TL;DR: SparseCoop是一个完全稀疏的协同感知框架，用于3D检测和跟踪，完全摒弃了中间BEV表示，通过基于实例查询的稀疏方法实现高效通信和鲁棒融合。


<details>
  <summary>Details</summary>
Motivation: 当前协同感知方法面临通信成本高、灵活性差、对齐困难等问题。基于BEV特征共享的方法通信成本呈二次增长，且缺乏跨异步或不同视角的精确对齐能力。稀疏查询方法虽然提供了替代方案，但存在几何表示不足、融合策略次优和训练不稳定等缺陷。

Method: 提出SparseCoop框架，包含三个创新：1) 基于运动学的实例查询，使用包含3D几何和速度的显式状态向量进行精确时空对齐；2) 粗到细的聚合模块实现鲁棒融合；3) 协同实例去噪任务加速和稳定训练。完全摒弃中间BEV表示，采用完全稀疏的方法。

Result: 在V2X-Seq和Griffin数据集上的实验表明，SparseCoop达到了最先进的性能，同时具有卓越的计算效率、低传输成本和强大的通信延迟鲁棒性。

Conclusion: SparseCoop通过完全稀疏的协同感知框架，有效解决了当前方法的通信成本、对齐困难和训练稳定性问题，为自动驾驶协同感知提供了高效、鲁棒的解决方案。

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [304] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文介绍了AIR-400数据集，这是首个用于婴儿呼吸监测的大规模公开视频数据集，并开发了首个可复现的婴儿呼吸估计算法管道。


<details>
  <summary>Details</summary>
Motivation: 婴儿呼吸监测对于早期发现和治疗呼吸异常至关重要，这些异常与神经发育障碍和婴儿猝死综合征相关。然而，目前只有一个小型公开婴儿呼吸视频数据集，且缺乏有效的可复现算法。

Method: 1) 构建包含400个视频的AIR-400数据集，新增275个精心标注的视频；2) 开发基于婴儿特定感兴趣区域检测的算法；3) 结合时空神经处理和光流输入的增强方法；4) 建立可复现的算法管道。

Result: 建立了首个可复现的婴儿呼吸估计基准，提供了公开可用的数据集、代码库和训练模型，填补了婴儿呼吸监测领域的空白。

Conclusion: 该研究为婴儿呼吸监测领域提供了重要的基础设施，包括数据集、算法和基准，有望推动婴儿呼吸异常早期检测技术的发展。

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [305] [Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology](https://arxiv.org/abs/2512.06949)
*Shravan Venkatraman,Muthu Subash Kavitha,Joe Dhanith P R,V Manikandarajan,Jia Wu*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出NTRM框架，通过图神经网络建模组织间空间和功能关系，提升皮肤癌组织病理图像分割性能


<details>
  <summary>Details</summary>
Motivation: 当前基于CNN的组织病理图像分割方法主要关注视觉纹理特征，将组织视为独立区域，缺乏对组织间空间关系和生物上下文建模，导致在重叠或形态相似组织区域分割效果不佳

Method: 提出神经组织关系建模(NTRM)框架：1) 使用CNN进行初步分割；2) 构建组织级图神经网络，将预测区域作为节点；3) 通过消息传递传播上下文信息；4) 通过空间投影细化分割结果

Result: 在Histopathology Non-Melanoma Skin Cancer Segmentation Dataset上，NTRM优于现有方法，Dice相似系数比最佳基线模型高4.9%到31.25%

Conclusion: 关系建模为组织病理分割提供了更上下文感知和可解释的方法，相比仅依赖局部感受野的架构，能更好地捕捉组织级结构信息

Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.

</details>


### [306] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

Relevance: 15.0

TL;DR: DAUNet是一种轻量级UNet变体，集成了可变形卷积V2和无参数注意力SimAM，用于医学图像分割，在保持参数效率的同时提升空间适应性和上下文感知特征融合。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在自动化诊断和治疗规划中至关重要，但现有模型在处理几何变化、上下文缺失和低对比度区域时存在挑战，同时需要在资源受限的临床环境中保持高效性。

Method: 提出DAUNet，在瓶颈层使用动态可变形卷积处理几何变化，在解码器和跳跃连接中使用SimAM注意力模块进行显著性感知细化，不增加模型复杂度。

Result: 在FH-PS-AoP（胎儿头部和耻骨联合超声）和FUMPE（CT肺栓塞检测）数据集上，DAUNet在Dice分数、HD95和ASD指标上优于最先进模型，同时保持优越的参数效率。

Conclusion: DAUNet通过可变形卷积和SimAM注意力的结合，在医学图像分割中实现了更好的空间适应性和上下文感知，适合在实时和资源受限的临床环境中部署。

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [307] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一种灵活的3D高斯泼溅压缩方案，支持在预定义边界内任意速率插值，无需重新训练即可适应不同带宽和设备限制


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)虽然能实现实时逼真渲染，但存在内存需求大、训练成本高的问题。现有压缩方法只能在固定速率下工作，无法适应变化的带宽和设备限制，需要灵活的压缩方案

Method: 提出一种灵活的3DGS压缩方案，支持在预定义边界内任意速率插值。该方法计算轻量，无需为不同速率重新训练，能在广泛的操作点保持渲染质量

Result: 实验表明该方法实现了高效高质量压缩，同时提供动态速率控制，适合沉浸式应用的实际部署

Conclusion: 该方法解决了3DGS压缩的灵活性问题，为沉浸式多媒体应用提供了实用的压缩解决方案

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [308] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

Relevance: 15.0

TL;DR: TIDE提出了一种两阶段水下图像恢复框架，通过分解退化特征并应用针对性修复，专门处理水下图像中复杂的空间变化退化问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像恢复对海洋应用至关重要，但现有方法通常对整个图像采用统一的恢复策略，难以处理空间变化且同时发生的多种退化问题。

Method: TIDE采用两阶段逆退化估计框架：1) 将水下退化分解为四个关键因素（颜色失真、雾霾、细节丢失、噪声），并为每个因素设计专门的修复专家；2) 根据局部退化模式自适应融合多个专门假设，然后通过渐进细化阶段纠正残留伪影。

Result: 在标准基准和挑战性浑浊水条件下的实验表明，TIDE在基于参考的保真度指标上具有竞争力，在非参考感知质量指标上优于现有方法，在颜色校正和对比度增强方面有显著改进。

Conclusion: TIDE通过显式建模退化特征和针对性修复，有效解决了水下图像中复杂空间变化退化问题，为水下视觉应用提供了更自然、高质量的恢复结果。

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [309] [Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture](https://arxiv.org/abs/2512.07241)
*Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于SqueezeNet v1和EfficientNet-B0的混合深度学习模型，结合手工放射组学特征，用于脑肿瘤MRI分类，在公开数据集上达到98.93%准确率


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤诊断需要及时准确，但MRI肿瘤勾画过程困难、耗时且易受观察者间差异影响。现有深度学习模型在计算效率和诊断准确性之间存在权衡问题。

Method: 结合轻量级SqueezeNet v1和高性能EfficientNet-B0构建混合模型，增强手工放射组学特征（HOG、LBP、Gabor滤波器和小波变换）。使用公开的Nickparvar脑肿瘤MRI数据集（7,023个T1加权轴向MRI切片），分为四类：胶质瘤、脑膜瘤、垂体瘤和无肿瘤。

Result: 测试准确率达到98.93%，使用测试时间增强后提升至99.08%。模型参数少于210万，计算量小于1.2 GFLOPs，在计算效率和诊断准确性之间取得良好平衡。

Conclusion: 提出的混合网络在脑肿瘤MRI自动分类中展现出接近临床可靠性的性能，具有在临床决策支持系统中应用的潜力。

Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.

</details>


### [310] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于摄影测量的图生成流水线，使用立体相机RGB图像和深度数据检测关键基础设施中的物体并预测其关系，替代昂贵的激光扫描点云方法


<details>
  <summary>Details</summary>
Motivation: 物理关键基础设施（如水厂、能源厂）的虚拟表示通常需要昂贵的激光扫描点云数据，且需要专业知识。本文旨在开发更经济高效的替代方案，使用立体相机数据生成基础设施的图表示。

Method: 基于摄影测量的图生成流水线：使用立体相机获取RGB图像和深度数据，通过深度学习进行物体检测和实例分割，采用用户定义的启发式规则推断物体间关系，生成图结构表示。

Result: 在两个水力系统上的实验表明，该方法生成的图接近真实情况，具有灵活性可针对特定应用定制，且透明度高适合关键基础设施的高风险决策。

Conclusion: 提出的基于摄影测量的图生成方法为关键基础设施建模提供了经济高效的替代方案，结合深度学习与规则推理，在保持准确性的同时提高了可访问性和透明度。

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [311] [Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset](https://arxiv.org/abs/2512.07305)
*Tobias Abraham Haider*

Main category: cs.CV

Relevance: 15.0

TL;DR: 重新评估预训练CNN在野生动物物种识别中的表现，使用不同数据集验证原研究的可复现性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 验证Carl等人关于预训练Google Inception-ResNet-v2模型在相机陷阱图像中自动检测欧洲野生哺乳动物物种的研究发现的可复现性和泛化性

Method: 使用公开可用资源从零开始重新实现实验，采用包含90个物种900张图像的不同数据集，进行最小预处理，评估分类准确率和宏F1分数

Result: 总体分类准确率达到62%，与原研究的71%相近，但宏F1分数为0.28，显示不同类别间性能差异显著，当标签与ImageNet类别不完全对齐时泛化能力有限

Conclusion: 预训练卷积神经网络可作为野生动物物种识别的实用基线，但需要物种特定的适应或迁移学习才能获得一致的高质量预测

Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.

</details>


### [312] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

Relevance: 15.0

TL;DR: 论文提出ROHIT任务，通过手交互时间线(HIT)重建物体姿态，利用约束优化传播(COP)框架在稳定抓握场景中提升重建效果，并在两个第一人称数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 现有物体重建方法通常假设物体在场景中静止，但在手交互过程中物体姿态会发生变化。论文旨在解决在手交互时间线中重建物体姿态的挑战，特别是在稳定抓握场景中。

Method: 定义手交互时间线(HIT)概念，提出约束优化传播(COP)框架，通过姿态约束在HIT上传播物体姿态。重点关注稳定抓握场景，利用2D投影误差进行评估。

Result: 在HOT3D和EPIC-Kitchens数据集上，COP框架将稳定抓握重建提升6.2-11.3%，HIT重建提升达24.5%。

Conclusion: ROHIT任务和COP框架为手交互场景中的物体重建提供了有效解决方案，无需3D真值标注，在稳定抓握场景中表现优异。

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [313] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了一种数据挖掘方法，用于从移动数据中发现个体间的相互影响模式，以改进人群模拟和应急管理模型。


<details>
  <summary>Details</summary>
Motivation: 理解个体移动行为及其对外部环境的反应是建模人类动态的关键。现有解决方案通常基于预设的行为模型，但本文希望直接从数据出发，发现个体间相互影响的证据和模式。

Method: 采用数据挖掘视角，从移动数据中搜索可能反映个体间相互作用的移动事件，在此基础上寻找复杂、持久的事件模式和随时间演化的配置。在汽车和行人两个真实案例中实例化该方法。

Result: 进行了全面的实验评估，包括性能、参数敏感性和样本结果解释。发现的方法可以提供关于个体间移动交互机制的新见解。

Conclusion: 提出的数据驱动方法能够发现个体间相互作用的模式，这些见解有助于改进现有的人群模拟模型，特别是在应急管理等应用中。

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [314] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文介绍了EgoCampus数据集和EgoCampusNet方法，用于预测户外校园环境中行人导航时的视觉注意力（眼动注视）。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数自我中心数据集主要关注室内任务或缺少眼动注视信息，缺乏对户外导航场景中人类视觉注意力的研究。本文旨在填补这一空白，研究真实世界户外导航中的视觉注意力模式。

Method: 1. 使用Meta的Project Aria眼镜收集数据，整合眼动追踪、RGB摄像头、惯性传感器和GPS；2. 构建EgoCampus数据集，包含25条独特户外路径、6公里距离、80多名行人的眼动注释视频；3. 开发EgoCampusNet模型来预测导航行人的眼动注视。

Result: 创建了EgoCampus数据集，这是首个专注于户外校园导航场景的自我中心眼动数据集。开发了EgoCampusNet方法用于预测户外导航中的视觉注意力。

Conclusion: 本文为研究真实世界注意力提供了新资源，并为导航场景的眼动预测模型提供了基础。数据集和代码将在GitHub上公开。

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [315] [Affine Subspace Models and Clustering for Patch-Based Image Denoising](https://arxiv.org/abs/2512.07259)
*Tharindu Wickremasinghe,Marco F. Duarte*

Main category: eess.IV

Relevance: 15.0

TL;DR: 该论文研究图像块聚类问题，提出使用仿射子空间模型替代传统的线性子空间模型，以更好地匹配图像块向量的几何结构，并开发了基于最小二乘投影的去噪算法。


<details>
  <summary>Details</summary>
Motivation: 图像块方法在去噪等应用中很流行，但传统线性子空间模型不适合图像块，因为图像是非负的，在向量空间中不围绕原点分布。需要更好的模型来匹配图像块的几何结构。

Method: 提出使用仿射子空间模型进行图像块聚类，开发了基于最小二乘投影的去噪算法，并比较了多种仿射子空间聚类算法的解决方案。

Result: 实验结果显示，仿射子空间模型在聚类和去噪性能上都有显著提升，比传统线性子空间模型表现更好。

Conclusion: 仿射子空间模型更适合图像块聚类，能更好地匹配图像向量的几何结构，在去噪等应用中具有更好的性能。

Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.

</details>


### [316] [Human Geometry Distribution for 3D Animation Generation](https://arxiv.org/abs/2512.07459)
*Xiangjun Tang,Biao Zhang,Peter Wonka*

Main category: cs.GR

Relevance: 15.0

TL;DR: 提出两阶段框架生成逼真人体几何动画：第一阶段学习紧凑分布式潜在表示，改进SMPL与化身几何映射；第二阶段利用有限运动数据多样性生成动画，通过身份条件设计保持长期一致性。


<details>
  <summary>Details</summary>
Motivation: 生成逼真人体几何动画面临挑战：需要在有限数据下建模自然服装动态和精细几何细节。现有方法在几何生成质量和动画多样性方面存在不足。

Method: 1. 提出紧凑分布式潜在表示，建立更均匀的SMPL与化身几何映射；2. 提出生成动画模型，利用有限运动数据多样性，通过短时过渡和身份条件设计保持长期一致性；3. 两阶段框架：第一阶段学习潜在空间，第二阶段在该空间内生成动画。

Result: 潜在空间生成的人体几何保真度超越先前方法（Chamfer距离降低90%）；动画模型合成多样化动画，具有详细自然动态（用户研究得分提高2.2倍）；在所有评估指标上取得最佳结果。

Conclusion: 提出的两阶段框架有效解决了逼真人体几何动画生成的挑战，在几何质量和动画自然度方面显著优于现有方法。

Abstract: Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \times$ higher user study score), achieving the best results across all evaluation metrics.

</details>


### [317] [Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework](https://arxiv.org/abs/2512.07574)
*Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Alimov Ruslan,Lutfuloev Mazbutdzhon,Ismoilov Shuhratjon,Yuanjie Zheng*

Main category: eess.IV

Relevance: 15.0

TL;DR: 该论文提出了一种混合框架，结合了注意力增强级联U-Net、手工放射组学特征和3D CNN细化，用于肝脏和肝肿瘤的联合分割。


<details>
  <summary>Details</summary>
Motivation: 肝脏肿瘤在增强CT上的准确三维分割对于治疗规划、导航和疗效评估至关重要，但手动分割耗时、观察者依赖性强且难以跨中心标准化。自动分割面临病灶-实质对比度低、边界模糊或不完整、增强模式异质性以及血管和邻近器官等混淆结构的挑战。

Method: 1) 使用具有密集连接编码器、亚像素卷积解码器和多尺度注意力门的2.5D两阶段网络生成初始肝脏和肿瘤概率图；2) 通过三切片细化规则沿头尾方向增强切片间时间一致性；3) 从候选病灶提取728个放射组学特征，通过多策略特征选择减少到20个稳定特征，使用随机森林分类器剔除假阳性区域；4) 在肿瘤边界周围窄带内使用基于AlexNet的紧凑3D补丁CNN进行体素级重标记和轮廓平滑。

Result: 论文提出了一个完整的混合分割框架，但没有提供具体的定量结果数据。

Conclusion: 该方法通过结合深度学习、放射组学和3D细化，为肝脏肿瘤分割提供了一个鲁棒的解决方案，能够处理低对比度、模糊边界和异质性等挑战。

Abstract: Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.

</details>


### [318] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

Relevance: 5.0

TL;DR: 开发了一种基于事件方法的非侵入式睡眠监测系统，通过红外深度传感器、RGB摄像头和四麦克风阵列检测运动、灯光开关和噪音三类事件，用于定量评估睡眠障碍。


<details>
  <summary>Details</summary>
Motivation: 为了定量评估睡眠障碍，需要一种非侵入式的监测系统。传统方法可能干扰睡眠或不够精确，因此开发基于事件检测的方法来客观量化睡眠期间的干扰因素。

Method: 使用包含红外深度传感器、RGB摄像头和四麦克风阵列的设备在低光照环境下监测睡眠。建立深度信号的背景模型检测运动幅度，建立彩色图像的背景模型检测光照变化，使用事件检测算法从三类传感器处理数据中检测事件发生。

Result: 系统在睡眠条件下进行了测试，实验结果验证了系统的可靠性，能够有效检测和分类三种类型的睡眠干扰事件。

Conclusion: 提出的非侵入式事件检测系统能够可靠地定量评估睡眠障碍，为睡眠监测提供了一种有效的技术解决方案。

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [319] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

Relevance: 95.0

TL;DR: 提出了CompassMax-V3-Thinking，一个百亿规模的MoE推理模型，采用新的RL框架，核心原则是"每个提示都必须重要"。解决了大规模RL训练中的效率问题，包括零方差提示浪费、重要性采样不稳定、优势反转和系统瓶颈。


<details>
  <summary>Details</summary>
Motivation: 将强化学习扩展到百亿规模MoE模型时暴露出关键效率问题：零方差提示浪费rollout资源、长视野重要性采样不稳定、标准奖励模型导致优势反转，以及rollout处理的系统性瓶颈。需要解决这些挑战以实现大规模RL的高效稳定训练。

Method: 1) 多阶段零方差消除：过滤非信息性提示，稳定基于组的策略优化；2) ESPO：熵自适应优化方法，平衡token级和序列级重要性采样；3) Router Replay策略：对齐训练时MoE路由决策与推理时行为，防止优势反转；4) 高吞吐RL系统：FP8精度rollout、重叠奖励计算和长度感知调度。

Result: CompassMax-V3-Thinking模型在内部和公共评估中都表现出强大性能。提出的统一创新形成了使百亿规模MoE模型RL训练稳定高效的完整流程。

Conclusion: 通过解决大规模RL训练中的关键效率问题，开发了一个使百亿规模MoE模型RL训练稳定高效的完整框架。提出的方法包括零方差提示过滤、自适应重要性采样、路由对齐和系统优化，为大规模语言模型的强化学习训练提供了系统解决方案。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [320] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

Relevance: 85.0

TL;DR: LLM评估任务中引入虚拟货币投注机制，通过预测市场框架提升预测准确率并产生可校准的置信度信号


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估其他模型时通常缺乏置信度表示，需要一种方法使LLM的内部信念可见且可用，从而将LLM转变为风险感知的预测器

Method: 设计对照实验：生成100个数学逻辑问题，6个基线模型回答，3个预测模型在两种条件下预测基线模型的正确性：控制组（简单预测）和激励组（预测+1-100,000虚拟货币投注）

Result: 激励组准确率略高（81.5% vs 79.1%），学习速度显著更快（12.0% vs 2.9%提升），投注规模与置信度相关：大额投注（40,000+）正确率约99%，小额投注（<1,000）约74%

Conclusion: 虚拟货币投注机制创造了可读的置信度信号，简单的金融框架可能帮助LLM成为风险感知的预测器，为元评估系统和LLM间预测市场奠定基础

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [321] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

Relevance: 85.0

TL;DR: ARCANE框架将AI对齐问题转化为多智能体协作，通过自然语言评分标准动态表示利益相关者偏好，实现可解释、无需重新训练即可调整的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体越来越多地部署到长期任务中，保持其与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型，让利益相关者能够理解和审计模型目标，并且能够在交互时引导智能体，无需重新训练即可纳入偏好变化。

Method: 将对齐问题构建为多智能体协作问题，动态地将利益相关者偏好表示为自然语言评分标准（可验证的加权标准集）。采用正则化组序列策略优化（GSPO）程序，平衡可解释性、忠实度和计算效率。

Result: 在GDPVal基准的219个标注评分标准语料库上评估，ARCANE在需要多步推理和工具使用的挑战性任务上表现良好。学习的评分标准产生紧凑、易读的评估，并支持可配置的权衡（如正确性与简洁性），无需重新训练。

Conclusion: 基于评分标准的奖励模型为复杂、长期AI系统提供了一条有前景的可解释、测试时自适应对齐路径。

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [322] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文将符号接地问题重新定义为包含真实性、保持性、忠实性、鲁棒性和组合性等五个维度的审计框架，并应用于符号、指称、向量和关系四种接地模式，通过三个案例研究对比了模型论语义学、大语言模型和人类语言在接地能力上的差异。


<details>
  <summary>Details</summary>
Motivation: 传统符号接地问题关注符号如何获得意义，但缺乏系统化的评估框架。本文旨在将这一哲学问题转化为可操作的技术框架，为哲学家、计算机科学家、语言学家和数学家提供共同语言来系统研究符号接地和意义问题。

Method: 提出一个基于评估元组（上下文、意义类型、威胁模型、参考分布）的审计框架，包含五个核心期望：真实性、保持性、忠实性（相关性和病因性）、鲁棒性和组合性。将该框架应用于四种接地模式（符号、指称、向量、关系），并通过三个案例研究进行验证。

Result: 1) 模型论语义学实现精确组合但缺乏病因性保证；2) 大语言模型在语言任务上表现出相关性拟合和局部鲁棒性，但在没有接地交互的世界任务上缺乏成功选择；3) 人类语言通过进化和发育获得强真实性，满足所有期望。

Conclusion: 通过将哲学问题操作化为技术框架，为跨学科研究符号接地提供了系统方法。大语言模型在语言任务上表现出色，但在世界任务接地方面仍有局限，需要进一步研究如何实现真正的符号接地。

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [323] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出DaGRPO方法解决GRPO在长程推理训练中的不稳定性和低样本效率问题，通过序列级梯度校正和离策略数据增强，在数学推理和OOD泛化基准上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: GRPO虽然能有效激发LLM的后训练推理能力，但存在训练不稳定和样本效率低的问题。研究发现根本原因是策略内样本缺乏区分度：简单查询中高度同质样本导致梯度冲突，困难查询中有效正样本稀缺导致优化无效

Method: 提出DaGRPO方法，包含两个核心机制：1) 序列级梯度校正：使用细粒度评分动态屏蔽低区分度的样本对，从源头消除梯度冲突；2) 离策略数据增强：引入高质量锚点样本，为困难任务恢复训练信号

Result: 在9个数学推理和OOD泛化基准上的实验表明，DaGRPO显著超越现有SFT、GRPO和混合基线，在数学基准上平均准确率提升4.7%，达到新的SOTA。深入分析证实DaGRPO有效缓解梯度爆炸并加速长链推理能力的出现

Conclusion: DaGRPO通过解决GRPO的区分度问题，显著提升了训练稳定性和样本效率，为LLM的长程推理能力训练提供了更有效的后训练方法

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [324] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一个评估LLM逻辑推理可靠性的框架，通过四种压力测试发现：模型对语义保留的逻辑变换具有稳定不变性，但对缺失或冲突证据表现出根本性脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言任务上表现出色，但其在逻辑上下文中的结构扰动泛化能力仍不清楚。需要系统评估LLM在逻辑推理中的可靠性，特别是对规则删除、矛盾证据、逻辑等价变换等结构扰动的鲁棒性。

Method: 提出了一个受控评估框架，包含四种针对性压力测试：1) 规则删除（冗余或必要规则）；2) 矛盾证据注入；3) 逻辑保留重写（使用六种等价定律）；4) 多定律等价堆叠（2-5个同时变换）。在BERT、Qwen2和LLaMA类模型上进行实验。

Result: 所有模型在基础任务上达到完美准确率，对冗余规则删除和所有等价重写（单或多定律）完全泛化，但在必要规则删除时准确率降至25%，在明确矛盾下完全崩溃（0%准确率）。

Conclusion: LLM对语义保留的逻辑变换具有稳定不变性，但对缺失或冲突证据表现出根本性脆弱。该框架为诊断推理失败模式提供了清晰工具，突显了当前LLM在逻辑泛化能力上的持续差距。

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [325] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

Relevance: 85.0

TL;DR: UncertaintyZoo是一个统一的不确定性量化工具包，集成了29种方法，涵盖五大类别，用于评估LLM输出的置信度，特别是在代码漏洞检测任务中进行了验证。


<details>
  <summary>Details</summary>
Motivation: LLM在安全关键场景中可能做出错误预测导致潜在损失，需要不确定性量化方法来评估模型输出的置信度。然而现有方法缺乏统一工具集成，阻碍了UQ方法的实际应用和未来研究。

Method: 开发UncertaintyZoo统一工具包，集成29种不确定性量化方法，涵盖五大类别，提供标准化接口。在CodeBERT和ChatGLM3模型上进行代码漏洞检测任务的评估。

Result: UncertaintyZoo能够有效揭示预测不确定性，为LLM不确定性量化提供了实用的工具支持。

Conclusion: UncertaintyZoo填补了LLM不确定性量化工具集的空白，促进了UQ方法的实际应用和未来研究发展。

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [326] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

Relevance: 85.0

TL;DR: 研究探索LLM智能体通过"信念盒"技术维护命题信念的效果，发现信念陈述及其强度影响智能体对相反观点的抵抗力和说服力，开放心态指令影响信念改变倾向，在辩论中被多数观点包围时尤其明显。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中的增加，需要LLM智能体具备类似命题信念的能力。研究者探索在提示空间中包含信念陈述（信念盒）如何影响智能体行为和信念倾向，以及这如何影响多智能体场景中的说服能力。

Method: 采用信念盒技术，在智能体提示空间中包含信念陈述及其强度信息。通过一系列实验研究：1)信念陈述如何影响智能体对相反观点的抵抗力和说服力；2)开放心态指令如何影响信念改变倾向；3)在辩论中被多数观点包围（同伴压力场景）时的信念变化。

Result: 1) 开放心态指令确实影响智能体对信念改变的接受程度；2) 信念陈述及其强度影响智能体对相反观点的抵抗力和说服力；3) 当智能体在辩论中被多数相反观点包围时，信念改变的可能性增加；4) 验证了信念盒技术在推理和决策任务中的可行性和有效性。

Conclusion: 信念盒技术为LLM智能体提供了一种有效的命题信念表示方法，能够显著影响智能体在多智能体交互中的行为和信念动态，特别是在说服和同伴压力场景中。这为构建更可信、更稳健的多智能体系统提供了基础。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [327] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

Relevance: 85.0

TL;DR: LightSearcher是一个高效的RL框架，通过文本经验记忆和自适应奖励机制，在DeepSearch范式中平衡准确性与效率，显著减少工具调用和计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前RL驱动的DeepSearch系统存在准确性与效率的权衡问题：频繁调用外部搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率下降。需要一种方法在保持准确性的同时提高效率。

Method: 1) 引入文本经验记忆，通过学习对比推理轨迹生成可解释的成功推理模式摘要；2) 采用自适应奖励塑造机制，仅在正确答案场景中惩罚冗余工具调用；3) 在四个多跳QA基准上进行实验验证。

Result: 在保持与SOTA基线ReSearch相当的准确性的同时，将搜索工具调用减少39.6%，推理时间减少48.6%，token消耗减少21.2%，显著提高了效率。

Conclusion: LightSearcher通过创新的文本经验记忆和自适应奖励机制，有效解决了DeepSearch范式中准确性与效率的权衡问题，为高效的外部知识检索提供了新思路。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [328] [Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation](https://arxiv.org/abs/2512.06710)
*Zairah Mustahsan,Abel Lim,Megna Anand,Saahil Jain,Bryan McCann*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出使用组内相关系数(ICC)来评估LLM代理系统的可靠性，将方差分解为任务难度和代理不一致性，为代理替换决策提供可信度依据


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理评估仅报告单次运行的准确率，掩盖了结果背后的方差，无法区分真实能力提升与幸运采样，导致下游系统脆弱性

Method: 采用测量科学中的组内相关系数(ICC)来分解观察方差：任务间方差(任务难度)和任务内方差(代理不一致性)，在GAIA和FRAMES基准上进行评估

Result: ICC随任务结构变化显著：推理和检索任务(FRAMES)ICC=0.4955-0.7118，代理任务(GAIA)ICC=0.304-0.774；ICC在结构化任务上n=8-16次试验收敛，复杂推理需n≥32

Conclusion: 建议将准确率与ICC和任务内方差一起报告作为标准实践，提出更新的评估卡片，将代理基准测试从不透明的排行榜竞争转变为可信的实验科学

Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.

</details>


### [329] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文提出认知控制架构（CCA），一种针对LLM代理间接提示注入攻击的全生命周期防御框架，通过意图图和分层裁决器实现安全、功能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理防御机制存在根本性缺陷：在安全、功能和效率之间需要做出不可接受的妥协，且防御架构碎片化，无法在整个任务执行流程中提供完整的安全保障。间接提示注入攻击通过污染外部信息源劫持代理行为，导致恶意工具调用和目标偏离。

Method: 提出认知控制架构（CCA），包含两个协同支柱：1）通过预生成的"意图图"实现主动的控制流和数据流完整性执行；2）创新的"分层裁决器"，在检测到偏差时启动基于多维评分的深度推理，专门应对复杂的条件攻击。

Result: 在AgentDojo基准测试中，CCA不仅能有效抵御挑战其他先进防御方法的复杂攻击，而且在保持安全性的同时实现了显著的效率和鲁棒性，成功调和了安全、功能和效率之间的多维权衡。

Conclusion: CCA通过全生命周期认知监督，为LLM代理提供了完整的完整性保障，解决了现有防御机制碎片化的问题，实现了安全、功能和效率的平衡，为构建可信的自主LLM代理系统提供了有效解决方案。

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [330] [DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06749)
*Ming Ma,Jue Zhang,Fangkai Yang,Yu Kang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: DoVer是一个基于干预的调试框架，用于LLM多智能体系统，通过主动干预验证失败假设，而非仅依赖日志分析，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统的调试主要依赖日志分析，存在两个关键局限：1) 仅基于日志的调试缺乏验证，产生未经测试的假设；2) 单步或单智能体归因往往不准确，因为多个不同的干预可能独立修复失败任务。

Method: DoVer框架结合假设生成与主动验证，通过针对性干预（如编辑消息、修改计划）来验证失败假设。采用结果导向的评估方法，关注系统是否解决失败或取得可量化的进展，而非归因准确性。

Result: 在Magnetic-One框架上，DoVer将18-28%的失败试验转为成功，实现高达16%的里程碑进展，验证或反驳30-60%的失败假设。在不同数据集和框架上也能恢复49%的失败试验。

Conclusion: 干预是提高智能体系统可靠性的实用机制，为LLM多智能体系统提供了更稳健、可扩展的调试方法。

Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.

</details>


### [331] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

Relevance: 85.0

TL;DR: DoGe是一个双解耦框架，通过将学习过程分解为思考者和解决者两个组件，并采用两阶段强化学习后训练方法，解决视觉语言模型在专业领域训练中的奖励破解问题，实现持续自我进化的大型视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通过强化学习实现显著推理能力，但在化学、地球科学、多模态数学等专业领域面临高质量多模态数据稀缺的问题。现有方法如合成数据和自奖励机制存在分布有限和对齐困难，导致奖励破解问题：模型利用高奖励模式，导致策略熵崩溃和训练不稳定。

Method: 提出DoGe双解耦框架：1) 将学习过程解耦为思考者和解决者两个组件，引导模型首先从上下文学习而非直接解决问题；2) 采用两阶段强化学习后训练方法：从自由探索上下文到实际解决任务；3) 构建演化课程学习管道：扩展原生领域知识语料库和迭代演化的种子问题池。

Result: 实验表明，该方法在各种基准测试中始终优于基线，为实现自我进化的大型视觉语言模型提供了可扩展的途径。

Conclusion: DoGe通过双解耦框架解决了视觉语言模型在专业领域训练中的奖励破解问题，通过合理的奖励信号量化和演化课程学习，为实现持续自我进化的大型视觉语言模型提供了有效方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [332] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

Relevance: 85.0

TL;DR: JT-DA-8B是一个专门用于复杂表格推理任务的大型语言模型，通过构建包含34个表格推理任务的多样化训练语料库，采用SFT和RL优化，并提出了四阶段表格推理工作流程来提高可解释性和执行准确性。


<details>
  <summary>Details</summary>
Motivation: 解决表格推理场景中高质量监督数据缺乏的问题，以及提升模型在多样化真实世界场景中的复杂表格推理能力。

Method: 1) 构建包含34个表格推理任务的多样化训练语料库，整合29个公开表格QA数据集和300万张表格；2) 提出自动管道生成现实多步分析任务；3) 基于开源JT-Coder-8B模型，采用LLM评分和工作流对齐过滤蒸馏高质量表格数据；4) 结合监督微调(SFT)和强化学习(RL)优化模型；5) 提出四阶段表格推理工作流程（表格预处理、表格感知、工具集成推理、提示工程）。

Result: JT-DA-8B在各种表格推理任务中表现出色，证明了数据为中心的数据生成和工作流驱动优化的有效性。

Conclusion: 通过构建高质量训练数据和系统化工作流程，可以显著提升LLM在复杂表格推理任务中的性能，为表格分析领域提供了有效的解决方案。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [333] [VIGIL: A Reflective Runtime for Self-Healing Agents](https://arxiv.org/abs/2512.07094)
*Christopher Cruz*

Main category: cs.AI

Relevance: 85.0

TL;DR: VIGIL是一个自监督的LLM代理运行时系统，通过行为日志分析、情感表征维护和诊断机制，实现代理的自我修复和持续改进，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理框架存在脆弱性问题：缺乏运行时自省能力，无法诊断自身失败模式，需要人工干预才能改进。大多数代理系统退化为装饰性的LLM调用链，缺乏结构化的可靠性机制。

Method: VIGIL采用反射式运行时架构，监督兄弟代理但不执行任务。方法包括：1) 行为日志分析；2) 结构化情感表征；3) 带衰减和上下文策略的持久EmoBank；4) RBT诊断（优势、机会、失败分类）；5) 生成保护性提示更新和只读代码提案；6) 状态门控管道防止非法转换。

Result: 在提醒延迟案例研究中，VIGIL成功识别延迟升高，提出提示和代码修复方案。当自身诊断工具因模式冲突失败时，能够暴露内部错误、生成备用诊断并输出修复计划，展示了部署代理运行时的元级自我修复能力。

Conclusion: VIGIL实现了LLM代理的自主维护和自我改进能力，通过结构化自省和诊断机制解决了当前代理系统的脆弱性问题，为可信AI提供了运行时可靠性保障。

Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.

</details>


### [334] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文提出了首个针对400个任务的9类别分类法，通过规则代码分析验证准确率达97.5%，揭示了Transformer架构在抽象推理任务中的神经亲和力天花板效应


<details>
  <summary>Details</summary>
Motivation: 响应Hodel等人(2024)对任务相关性形式化定义的呼吁，研究旨在系统分析抽象推理任务与神经网络架构的匹配度，揭示当前Transformer在复杂推理任务中的局限性

Method: 1) 开发9类别任务分类法，通过规则代码分析验证；2) 在原始网格像素上训练CNN验证视觉一致性；3) 在302个任务上微调170万参数Transformer；4) 应用分类法分析ARC-AGI-2测试集和独立ViTARC研究

Result: 1) 35.3%任务对Transformer具有低神经亲和力；2) 发现组合性差距：69.5%任务局部模式准确率>80%但全局合成准确率<10%；3) 神经亲和力天花板效应：低亲和力任务准确率51.9% vs 高亲和力77.7%；4) 分类法具有预测能力，能精确诊断任务难度

Conclusion: 当前进展受限于架构适用性而非训练课程，需要开发具有亲和力对齐模块的混合架构来突破神经亲和力天花板

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [335] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一个统一监督和无监督概念发现的几何框架，将概念瓶颈模型（CBMs）和稀疏自编码器（SAEs）视为学习激活空间中的概念锥，并建立了评估SAEs与人类定义概念对齐的量化指标。


<details>
  <summary>Details</summary>
Motivation: 传统的可解释性研究存在两个平行发展的方向：概念瓶颈模型（CBMs）通过监督学习将激活与人类标注概念对齐，而稀疏自编码器（SAEs）通过稀疏编码发现涌现概念。这两种方法很少对话，缺乏统一的评估框架来衡量SAEs发现的概念与人类可理解概念的对齐程度。

Method: 提出几何统一框架：将CBMs和SAEs都视为学习激活空间中的线性方向集合，其非负组合形成概念锥。基于此提出包含框架，使用CBMs提供人类定义的参考几何，评估SAEs学习的概念锥在多大程度上近似或包含CBM概念锥。开发量化指标将SAE的归纳偏置（如稀疏度、扩展比）与概念对齐联系起来。

Result: 发现稀疏度和扩展因子存在"最佳点"，能最大化SAEs与CBM概念的几何和语义对齐。建立了连接监督和无监督概念发现的量化桥梁，为评估SAEs进展提供了原则性指标。

Conclusion: 通过共享的几何框架统一了监督和无监督概念发现，提供了量化指标来衡量SAEs发现的概念与人类可理解概念的对齐程度，为可解释性研究建立了新的评估标准。

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [336] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

Relevance: 85.0

TL;DR: LocalSearchBench：首个针对本地生活服务的智能体搜索基准，包含15万高质量条目和300个多跳QA任务，揭示当前大推理模型在该垂直领域的局限性


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型主要关注通用信息检索，缺乏针对垂直领域（如本地生活服务）的专门评估。本地生活服务查询具有模糊性和多跳推理需求，现有模型难以处理这些独特挑战

Method: 构建LocalSearchBench基准，包含15万+高质量条目（覆盖多个城市和商业类型），创建300个基于真实用户查询的多跳QA任务，开发LocalPlayground统一环境集成多种工具供智能体交互

Result: 即使最先进的模型（DeepSeek-V3.1）在正确性上仅达到34.34%，大多数模型在完整性（平均77.33%）和忠实性（平均61.99%）方面存在问题，表明当前模型在本地生活服务领域的局限性

Conclusion: 本地生活服务领域需要专门的基准和领域特定的智能体训练，现有大推理模型在该垂直领域表现不佳，突显了领域适应性的重要性

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [337] [How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations](https://arxiv.org/abs/2512.07497)
*JV Roig*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究分析了LLM作为自主工具使用代理时的失败模式，发现模型规模并非代理鲁棒性的关键预测因素，而是训练方法和特定行为模式决定可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型在作为具有工具使用能力的自主代理时如何失败，特别是在企业部署场景中需要可靠的多步骤工具执行能力。

Method: 使用KAMI v0.1基准测试，分析了三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1）在文件系统、文本提取、CSV分析和SQL场景中的900个执行轨迹，进行细粒度的逐试验行为分析。

Result: 发现模型规模不能预测代理鲁棒性：400B的Llama 4 Maverick在某些任务中仅比32B的Granite 4 Small略好；DeepSeek V3.1的优越可靠性主要来自后训练强化学习而非架构或规模。识别出四种常见失败模式：无基础的过早行动、过度帮助性替换缺失实体、易受干扰诱导的上下文污染、负载下的脆弱执行。

Conclusion: 可靠的企业部署不仅需要更强的模型，还需要有意的训练和设计选择，强调交互式基础、恢复行为和环境感知适应，需要强化验证、约束发现和遵循真实数据源。

Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.

</details>


### [338] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

Relevance: 85.0

TL;DR: 系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力方面的效果，通过控制性迁移学习评估发现RL训练模型在所有任务上都优于基础模型，并提供了RL训练的参数化指导。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统评估不同强化学习算法在提升大语言模型复杂推理能力方面的效果，为RL-based LLM训练提供实证指导和实用建议。

Method: 采用控制性迁移学习评估方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准测试套件上进行评估。比较了PPO、GRPO和DAPO三种RL算法，并分析了组大小、KL惩罚系数等参数的影响。

Result: 在所有任务中，RL训练模型都优于对应的基础模型，但改进程度因基准测试而异。增加GRPO和DAPO中的组大小能带来更稳定的训练动态和更高准确率，KL惩罚系数的影响是非单调的。DAPO中的动态采样组件并未改善性能，禁用DS时DAPO获得最佳整体结果。

Conclusion: RL训练能有效提升LLM的复杂推理能力，但算法选择和参数设置对性能有重要影响。GRPO和DAPO中的组大小是关键参数，而DAPO的动态采样组件在实际应用中可能不必要。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [339] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过训练攻击者LLM来从黑盒模型中引出有害内容，相比单轮优化方法显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击，威胁其在实际应用中的安全部署。现有方法通常依赖单轮优化，不足以学习长期攻击策略，因此需要更有效的多轮攻击方法。

Method: 将问题形式化为多轮强化学习任务，直接优化最终轮输出的有害性作为结果奖励。提出两种启发式过程奖励：1) 控制中间输出的有害性以避免触发黑盒模型的拒绝机制；2) 保持中间输出的语义相关性以避免偏离主题。

Result: 在多个基准测试上的实验结果显示，该方法在多个模型上持续提高了攻击成功率，证明了方法的有效性。

Conclusion: 提出的基于强化学习的多轮越狱攻击方法能够有效学习长期攻击策略，显著提高对黑盒语言模型的攻击成功率，揭示了当前LLM安全防护的脆弱性。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [340] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

Relevance: 85.0

TL;DR: ReasonBENCH是首个量化LLM推理不稳定性的基准测试，通过多轮运行协议提供统计可靠的性能指标，揭示当前推理方法普遍存在高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法的稳定性、可重复性和成本一致性。

Method: 提出ReasonBENCH基准：1) 模块化评估库标准化推理框架、模型和任务；2) 多轮运行协议报告质量和成本的统计可靠指标；3) 公开排行榜鼓励方差感知报告。

Result: 大多数推理策略和模型表现出高不稳定性，即使平均性能相似的策略置信区间宽度差异可达4倍，且性能最佳的方法通常成本更高、更不稳定。

Conclusion: 可重复性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [341] [Large Causal Models from Large Language Models](https://arxiv.org/abs/2512.07796)
*Sridhar Mahadevan*

Main category: cs.AI

Relevance: 85.0

TL;DR: DEMOCRITUS是一个利用LLMs构建大规模因果模型的新范式，通过提取、组织和可视化跨领域的因果知识，将碎片化的因果陈述转化为统一的因果三元组网络。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法局限于特定领域和数值数据，而LLMs蕴含丰富的跨领域因果知识。研究者希望利用LLMs的潜力，构建能够整合不同领域碎片化因果知识的大规模因果模型，实现更全面的因果理解。

Method: 使用高质量LLMs提出主题、生成因果问题、从文本中提取因果陈述。开发新的范畴机器学习方法，将碎片化、可能冲突的因果主张转化为关系因果三元组，并嵌入到大规模因果模型中。系统包含六个模块的流水线。

Result: DEMOCRITUS在考古学、生物学、气候变化、经济学、医学和技术等多个领域成功应用，展示了从LLMs中提取和组织跨领域因果知识的能力。分析了系统的计算成本瓶颈，为扩展到更大模型提供了方向。

Conclusion: DEMOCRITUS展示了利用LLMs构建大规模因果模型的新范式潜力，能够整合跨领域的碎片化因果知识。虽然当前系统存在局限性，但为未来的扩展和改进奠定了基础。

Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.

</details>


### [342] [Auditing Games for Sandbagging](https://arxiv.org/abs/2512.07810)
*Jordan Taylor,Sid Black,Dillon Bowen,Thomas Read,Satvik Golechha,Alex Zelenka-Martin,Oliver Makins,Connor Kissane,Kola Ayonrinde,Jacob Merizian,Samuel Marks,Chris Cundy,Joseph Bloom*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究通过审计游戏测试了检测AI模型"沙袋化"（故意隐藏能力）的方法。红队微调了5个模型，其中一些有条件地表现不佳；蓝队使用黑盒、模型内部或基于训练的方法来识别沙袋化模型。研究发现蓝队无法可靠区分沙袋化模型与良性模型，基于训练的激发方法能有效激发沙袋化模型的全部性能，但也可能产生误报。


<details>
  <summary>Details</summary>
Motivation: 未来AI系统可能在评估中故意隐藏能力（沙袋化），误导开发者和审计者。这种能力隐藏对AI安全构成威胁，需要开发有效的检测方法。研究旨在通过审计游戏压力测试现有的沙袋化检测技术。

Method: 采用审计游戏方法：1）红队微调5个模型，其中一些有条件地表现不佳作为沙袋化代理；2）蓝队使用三种方法检测：黑盒方法（仅观察输出）、模型内部方法（如线性探针）、基于训练的方法（能力激发）。研究探索了提示激发和训练激发两种策略。

Result: 蓝队无法可靠区分沙袋化模型与良性模型。黑盒方法被有效模仿较弱模型的行为击败。线性探针（模型内部方法）更有前景但易受红队行为影响。训练激发能一致地激发沙袋化模型的全部性能（仅需一个正确演示），但良性模型的性能有时也会提升，导致误报。

Conclusion: 短期建议开发者使用分布内训练进行激发以消除潜在沙袋化。长期需要更多研究确保基于训练的激发方法的有效性，并开发鲁棒的沙袋化检测方法。研究开源了模型生物和评估日志。

Abstract: Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .

</details>


### [343] [FlockVote: LLM-Empowered Agent-Based Modeling for Simulating U.S. Presidential Elections](https://arxiv.org/abs/2512.05982)
*Lingfeng Zhou,Yi Xu,Zhenyu Wang,Dequan Wang*

Main category: physics.soc-ph

Relevance: 85.0

TL;DR: FlockVote：一个使用LLM代理构建政治模拟"计算实验室"的框架，应用于2024年美国大选，成功复现真实结果并提供可解释性分析工具。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的模型（ABMs）规则过于简化，大规模统计模型缺乏可解释性，需要一种能够模拟复杂人类行为（如选举投票）的高保真、可解释框架。

Method: 使用LLM构建"计算实验室"，每个代理配备高保真人口统计档案和动态上下文信息（如候选人政策），通过生成式推理模拟投票决策，应用于2024年美国大选七个关键摇摆州。

Result: 宏观层面成功复现了真实世界选举结果，证明了"虚拟社会"的高保真度；框架不仅提供预测，更重要的是作为可解释研究工具，允许研究人员探索代理级推理逻辑和分析LLM驱动社会模拟的稳定性与敏感性。

Conclusion: FlockVote超越了黑盒输出，为计算社会科学提供了新的研究范式，将LLM作为构建高保真、可解释社会模拟的工具，在政治科学和社会模拟领域具有重要应用价值。

Abstract: Modeling complex human behavior, such as voter decisions in national elections, is a long-standing challenge for computational social science. Traditional agent-based models (ABMs) are limited by oversimplified rules, while large-scale statistical models often lack interpretability. We introduce FlockVote, a novel framework that uses Large Language Models (LLMs) to build a "computational laboratory" of LLM agents for political simulation. Each agent is instantiated with a high-fidelity demographic profile and dynamic contextual information (e.g. candidate policies), enabling it to perform nuanced, generative reasoning to simulate a voting decision. We deploy this framework as a testbed on the 2024 U.S. Presidential Election, focusing on seven key swing states. Our simulation's macro-level results successfully replicate the real-world outcome, demonstrating the high fidelity of our "virtual society". The primary contribution is not only the prediction, but also the framework's utility as an interpretable research tool. FlockVote moves beyond black-box outputs, allowing researchers to probe agent-level rationale and analyze the stability and sensitivity of LLM-driven social simulations.

</details>


### [344] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

Relevance: 85.0

TL;DR: 提出一个结合强化学习与自主代理的框架，用于从业务需求文档自动生成软件测试用例，通过质量工程反馈实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的测试用例生成系统依赖静态知识库，无法随时间持续改进性能。需要一种能够从质量工程反馈中学习并优化测试生成策略的框架。

Method: 提出强化学习增强的代理式RAG框架，结合专门代理和混合向量-图知识库存储测试知识。使用PPO和DQN等强化学习算法，基于测试有效性、缺陷检测率和工作流指标优化代理行为。

Result: 在企业级Apple项目中验证：测试生成准确率从94.8%提升至97.2%（提升2.4%），缺陷检测率提升10.8%。

Conclusion: 该框架建立了由质量工程专业知识驱动的持续知识精炼循环，逐步提升测试用例质量，增强而非取代人工测试能力。

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [345] [Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity](https://arxiv.org/abs/2512.06106)
*Constanze Albrecht,Chayapatr Archiwaranguprok,Rachel Poonsiriwong,Awu Chen,Peggy Yin,Monchai Lertsutthiwong,Kavin Winson,Hal Hershfield,Pattie Maes,Pat Pataranutaporn*

Main category: cs.HC

Relevance: 85.0

TL;DR: 研究评估了AI生成未来自我的三种模态（文本、语音、头像）对心理影响的效果，发现所有个性化模态都能增强未来自我连续性、情感福祉和动机，头像模态在生动性方面提升最大，但模态间无显著差异。交互质量比形式更重要，Claude 4在心理效果上优于其他LLMs。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在克隆语音、年龄进展面部渲染和自传叙事方面的进步，研究者希望探索未来自我不同模态（文本、语音、头像）是否会影响其心理和情感影响，以及这些模态如何塑造当前决策和对未来的连接感。

Method: 采用随机对照研究（N=92），评估三种AI生成未来自我模态（文本、语音、头像）与中性对照条件。同时系统评估Claude 4与其他三个大型语言模型（ChatGPT 3.5、Llama 4、Qwen 3），在心理和交互维度上比较，确立对话AI质量作为干预效果的关键决定因素。

Result: 所有个性化模态都增强了未来自我连续性、情感福祉和动机，头像模态在生动性方面提升最大，但模态间无显著差异。交互质量指标（特别是说服力、真实感和用户参与度）是心理和情感结果的强预测因子。内容分析显示文本强调职业规划，语音和头像促进个人反思。Claude 4在增强心理、情感和未来自我连续性结果方面优于其他LLMs。

Conclusion: AI生成未来自我能有效增强心理福祉和未来连接感，交互质量比具体形式更重要。Claude 4在心理干预应用中表现最佳，对话AI质量是决定干预效果的关键因素。

Abstract: What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.

</details>


### [346] [Protecting Bystander Privacy via Selective Hearing in LALMs](https://arxiv.org/abs/2512.06380)
*Xiao Zhan,Guangzhi Sun,Jose Such,Phil Woodland*

Main category: cs.SD

Relevance: 85.0

TL;DR: SH-Bench是首个评估音频大语言模型选择性听觉能力的基准，包含3,968个多说话者音频混合和77k选择题，提出选择性效能(SE)统一指标。研究发现现有模型存在严重隐私泄露，并提出旁观者隐私微调(BPFT)方法提升隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有音频大语言模型在真实场景中会无意捕获附近旁观者的语音，带来隐私风险，而现有基准和防御机制大多忽略了这一问题。需要评估模型在关注主要说话者的同时拒绝处理或泄露旁观者信息的能力。

Method: 1) 构建SH-Bench基准：包含3,968个多说话者音频混合（真实和合成场景）和77k选择题；2) 提出选择性效能(SE)指标，综合评估多说话者理解和旁观者隐私保护；3) 提出旁观者隐私微调(BPFT)训练流程，教导模型拒绝旁观者相关查询而不降低主要说话者理解能力。

Result: 评估显示现有开源和专有音频大语言模型存在显著隐私泄露，强大的音频理解能力未能转化为对旁观者隐私的选择性保护。BPFT方法显著提升了隐私保护，选择性效能比Gemini 2.5 Pro提高15.9%，证明选择性听觉是可学习的但当前模型远未实现。

Conclusion: SH-Bench和BPFT为音频基础模型中的旁观者隐私提供了首个系统性测量和改进框架，揭示了当前模型的隐私风险，并展示了通过专门训练可以改善选择性听觉能力。

Abstract: Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models.

</details>


### [347] [Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices](https://arxiv.org/abs/2512.06443)
*Xiangyu Li,Chengyu Yin,Weijun Wang,Jianyu Wei,Ting Cao,Yunxin Liu*

Main category: cs.DC

Relevance: 85.0

TL;DR: 提出向量查找表(Vec-LUT)方法，解决边缘设备上基于查找表的LLM推理在并行推理场景下内存带宽利用不足的问题，通过统一查找表和优化内存访问模式，在多个设备上实现高达4.2倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在边缘设备上的部署需求增加，量化技术已从8位发展到1.58位。基于查找表的推理方法在CPU上运行超低比特LLM甚至比NPU更快，为设备端智能提供了新机会。然而，研究发现基于查找表的推理在并行推理场景下（如预填充、测试时缩放等）存在内存带宽利用不足的问题，根源在于标量查找表范式对每个token执行重复且非连续的内存访问。

Method: 提出向量查找表新范式：1) 构建跨并行token的统一查找表，执行单次1→N查找；2) 向量查找表中心张量布局；3) 缓存感知流式查找技术。这些方法优化了内存访问模式，减少了重复访问。

Result: 在5个边缘设备和3个LLM上的评估显示，Vec-LUT相比现有最优方法性能提升高达4.2倍。该方法已集成到llama.cpp中，代码开源。

Conclusion: 向量查找表范式有效解决了基于查找表的LLM推理在并行场景下的内存带宽利用问题，显著提升了边缘设备上超低比特LLM的推理效率，为设备端智能部署提供了更优解决方案。

Abstract: Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.
  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.
  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.

</details>


### [348] [Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks](https://arxiv.org/abs/2512.06556)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh,Naser Ezzati-Jivan*

Main category: cs.CR

Relevance: 85.0

TL;DR: MCP安全漏洞分析：揭示工具描述符中的语义攻击（工具投毒、影子攻击、拉地毯），提出三层安全框架（RSA签名、LLM语义审查、轻量级护栏），评估发现GPT-4在安全性和延迟间最佳平衡。


<details>
  <summary>Details</summary>
Motivation: MCP使LLM能通过结构化描述符集成外部工具，增强了自主决策和多智能体工作流能力，但这种自主性带来了被忽视的安全漏洞。现有防御主要关注提示注入攻击，未能解决嵌入在工具元数据中的威胁，导致MCP系统暴露于语义操纵风险。

Method: 1. 分析三类语义攻击：工具投毒（在工具描述符中隐藏对抗指令）、影子攻击（通过污染共享上下文间接危害可信工具）、拉地毯攻击（批准后修改描述符颠覆行为）。2. 提出三层安全框架：RSA签名确保描述符完整性、LLM-on-LLM语义审查检测可疑工具定义、轻量级启发式护栏在运行时阻止异常工具行为。3. 在GPT-4、DeepSeek和Llama-3.5上评估八种提示策略。

Result: 安全性能因模型架构和推理方法差异显著：GPT-4阻止约71%不安全工具调用，在延迟和安全性间最佳平衡；DeepSeek对影子攻击最具弹性但延迟更高；Llama-3.5最快但最不鲁棒。提出的框架无需模型微调或内部修改即可降低不安全工具调用率。

Conclusion: MCP系统的安全漏洞需要专门针对工具描述符语义攻击的防御机制。提出的三层安全框架有效降低了不安全工具调用风险，但安全性能高度依赖底层LLM的能力。未来需要更鲁棒的防御机制来保护LLM工具集成系统。

Abstract: The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.

</details>


### [349] [PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance](https://arxiv.org/abs/2512.06747)
*Jifar Wakuma Ayana,Huang Qiming*

Main category: cs.CR

Relevance: 85.0

TL;DR: PrivLLMSwarm：基于安全多方计算的隐私保护LLM框架，用于无人机群在物联网环境中的安全协调与推理


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的无人机系统以明文处理敏感操作数据，存在隐私和安全风险。物联网环境中的无人机群协调需要保护敏感信息，同时保持高效推理能力。

Method: 1. 采用安全多方计算（MPC）实现隐私保护的LLM推理；2. 优化Transformer组件，使用非线性激活函数的高效近似；3. 通过强化学习在仿真中微调GPT-based命令生成器；4. 设计适用于资源受限空中平台的加密推理方案。

Result: 在城市规模仿真中，PrivLLMSwarm实现了高语义准确性、低加密推理延迟和鲁棒的编队控制。相比差分隐私、联邦学习和明文基线，在隐私-效用平衡方面表现更优。

Conclusion: PrivLLMSwarm为隐私敏感的物联网应用（如智慧城市监控和应急响应）中的安全LLM驱动无人机群建立了实用基础，实现了隐私保护与功能效用的良好平衡。

Abstract: Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.

</details>


### [350] [SoK: Trust-Authorization Mismatch in LLM Agent Interactions](https://arxiv.org/abs/2512.06914)
*Guanquan Shi,Haohua Du,Zhiqiang Wang,Xiaoyu Liang,Weiwenpei Liu,Song Bian,Zhenyu Guan*

Main category: cs.CR

Relevance: 85.0

TL;DR: 该论文提出了一个统一的形式化框架来分析LLM作为自主代理时的安全风险，重点关注信任评估与授权策略之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM演变为能够与外部世界交互的自主代理，传统的确定性安全机制无法应对基于概率推理的决策过程，导致网络安全挑战在AI代理领域重新出现。学术界对这一新兴领域缺乏系统性分析框架。

Method: 引入一个以信任-授权差距为中心的风险分析模型，作为统一视角来调查和分类现有攻击与防御的实现路径。提供形式化框架来系统分析代理交互安全。

Result: 发现大多数安全威胁源于信任评估与授权策略之间的根本性不匹配。提出的框架不仅统一了该领域，还识别出关键研究空白。

Conclusion: 论文为构建鲁棒、可信的代理和动态授权机制提供了系统性研究方向，填补了AI代理安全领域的理论空白。

Abstract: Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.
  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.

</details>


### [351] [Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://arxiv.org/abs/2512.07019)
*Zhiyu Xu,Jia Liu,Yixin Wang,Yuqi Gu*

Main category: stat.ME

Relevance: 85.0

TL;DR: 提出Latency-Response Theory (LaRT)模型，结合LLMs的响应准确率和思维链长度进行联合建模，通过相关性参数连接潜在能力和潜在速度，相比传统IRT模型在多个评估指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，需要有效的评估方法为下游应用和未来改进提供指导。传统IRT模型仅考虑响应准确率，而思维链长度是LLMs推理能力的重要指标，需要开发能同时利用准确率和思维链长度信息的评估框架。

Method: 提出LaRT模型，引入潜在能力和潜在速度之间的相关性参数，对响应准确率和思维链长度进行联合建模。开发了高效的随机近似EM算法进行参数估计，并建立了严格的参数可识别性理论保证。

Result: 理论渐近分析和模拟研究表明，LaRT相比IRT在潜在特质估计上具有更高的估计精度和更短的置信区间。在实际数据评估中，LaRT在不同LLMs的排名上与IRT不同，且在预测能力、项目效率、排名有效性和LLM评估效率等多个关键指标上优于IRT。

Conclusion: LaRT模型通过结合思维链长度信息，为LLMs评估提供了更全面有效的框架，能够更准确地评估LLMs的推理能力，在多个评估维度上优于传统IRT方法。

Abstract: The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.

</details>


### [352] [Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization](https://arxiv.org/abs/2512.07022)
*Genevieve Caumartin,Glaucia Melo*

Main category: cs.SE

Relevance: 85.0

TL;DR: 该论文提出了一种基于LLM的智能体方法，通过轻量级查询重构和摘要技术改进文件级bug定位，相比传统IRBL方法显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于信息检索的bug定位方法依赖未处理的bug描述，其中常包含噪声信息导致检索准确率低。虽然LLM在查询重构方面已有改进，但其在智能体性能上的影响尚未被充分探索。

Method: 使用开源、未微调的LLM从bug报告中提取关键信息（如标识符和代码片段），进行预检索查询重构。智能体随后使用这些预处理查询协调BM25检索，实现大规模自动化的定位工作流。

Result: 采用最佳查询重构技术后，智能体在首文件检索排名上比BM25基线提升35%，在文件检索性能上比SWE-agent提升高达22%。

Conclusion: LLM驱动的智能体通过轻量级查询重构和摘要能有效提升bug定位性能，为软件工程中的自动化bug定位提供了有前景的解决方案。

Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.

</details>


### [353] [ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking](https://arxiv.org/abs/2512.07086)
*Yunzhe Li,Jianan Wang,Hongzi Zhu,James Lin,Shan Chang,Minyi Guo*

Main category: cs.CR

Relevance: 85.0

TL;DR: ThinkTrap：一种针对黑盒LLM服务的拒绝服务攻击框架，通过输入空间优化生成诱导模型进入过长或无限生成循环的对抗性提示


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为云服务部署，存在通过无界推理进行DoS攻击的风险，攻击者可以设计特殊输入使模型进入过长或无限生成循环，耗尽计算资源。当前LLM提供商采用闭源黑盒设置来隐藏模型内部，但仍需要防御此类攻击。

Method: 提出ThinkTrap框架：1）将离散token映射到连续嵌入空间；2）利用输入稀疏性在低维子空间中进行高效黑盒优化；3）识别能诱导扩展或非终止生成的对抗性提示，以最小token开销实现DoS攻击。

Result: 在多个商业闭源LLM服务上评估，即使在严格请求频率限制下（通常10 RPM），攻击可将服务吞吐量降低至原始容量的1%，在某些情况下导致完全服务故障。

Conclusion: ThinkTrap证明了在黑盒环境中针对LLM服务的DoS攻击可行性，揭示了当前LLM部署的安全漏洞，需要开发更强大的防御机制来保护LLM服务免受此类攻击。

Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.

</details>


### [354] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

Relevance: 85.0

TL;DR: 论文探索LLM内部是否编码代码正确性表示，通过对比正确与错误代码的隐藏状态差异来提取正确性信号，用于提升代码生成质量而不需要测试执行。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面有效，但经常输出错误代码。一个原因是模型输出概率与正确性相关性不强，且只反映生成过程的最终输出。受LLM内部编码真实性等概念的启发，本文探索LLM是否类似地表示代码正确性。

Method: 通过对比同一编程任务下正确与错误代码对的隐藏状态差异，识别LLM内部的正确性表示。在四个LLM上进行实验，提取内部正确性信号，用于代码样本选择和质量评估。

Result: 利用提取的正确性表示优于标准的对数似然排序和模型口头化置信度。内部正确性信号可用于选择更高质量的代码样本，无需测试执行。

Conclusion: 这项工作展示了如何利用内部表示增强代码生成系统，使LLM更可靠，从而提高自动生成代码的可信度。

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [355] [Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics](https://arxiv.org/abs/2512.07462)
*Trung-Kiet Huynh,Duy-Minh Dao-Sy,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Phu-Quy Nguyen-Lam,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Phu-Hoa Pham,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.MA

Relevance: 85.0

TL;DR: 该研究扩展FAIRGAME框架，通过囚徒困境和公共物品博弈评估LLM在重复社会困境中的战略行为，发现模型存在激励敏感的合作、跨语言差异和终局背叛倾向等系统性行为模式。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互式多智能体系统和社会中作为自主决策者运行，理解其战略行为对AI安全、协调和AI驱动的社会经济基础设施设计至关重要。需要评估LLM的决策意图而不仅仅是输出内容。

Method: 扩展FAIRGAME框架，引入两种补充环境：1) 收益缩放的囚徒困境（评估对激励强度的敏感性）；2) 集成多智能体公共物品博弈（具有动态收益和多智能体历史）。使用传统监督分类模型分析LLM行为轨迹，识别其战略意图。

Result: 发现跨模型和语言的系统性行为特征：激励敏感的合作、跨语言分歧、终局向背叛对齐。语言框架有时对行为的影响与架构差异一样强。LLM表现出系统性的、模型和语言依赖的行为意图。

Conclusion: 为审计LLM作为战略智能体提供了统一方法基础，揭示了系统性合作偏见，对AI治理、集体决策和安全多智能体系统设计具有直接意义。

Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.

</details>


### [356] [VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection](https://arxiv.org/abs/2512.07533)
*Yuzhou Nie,Hongwei Li,Chengquan Guo,Ruizhe Jiang,Zhun Wang,Bo Li,Dawn Song,Wenbo Guo*

Main category: cs.CR

Relevance: 85.0

TL;DR: VulnLLM-R是首个专门用于漏洞检测的推理大语言模型，通过程序状态推理而非简单模式匹配来提升泛化能力，在Python、C/C++和Java上超越现有静态分析工具和商业推理模型。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法存在局限性：传统静态分析工具依赖模式匹配，泛化能力有限；现有推理LLM要么规模过大、闭源，要么在漏洞检测任务上性能不足。需要专门针对漏洞检测任务设计的推理模型。

Method: 提出新颖的训练配方：1) 专门化数据选择；2) 推理数据生成；3) 推理数据过滤与校正；4) 测试阶段优化。基于此训练了70亿参数的推理模型，并构建了围绕该模型的智能体框架。

Result: 在Python、C/C++和Java的SOTA数据集上，VulnLLM-R在效果和效率上均优于现有静态分析工具及开源/商业推理模型。智能体框架在实际项目中超越CodeQL和AFL++，并在活跃维护的仓库中发现了一系列零日漏洞。

Conclusion: 该研究首次实现了基于专门化推理模型的AI智能体进行实际项目级漏洞检测，代表了该领域的开创性工作，证明了专门化推理模型在软件安全领域的实用价值。

Abstract: We propose VulnLLM-R, the~\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.

</details>


### [357] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

Relevance: 75.0

TL;DR: FlatFormer：一种基于"信息注入而非结构堆叠"设计范式的知识追踪模型，通过轻量级注入机制（混合输入编码和预计算幂律偏置）在保持扁平Transformer架构的同时实现高性能，解决了传统知识追踪模型的"性能-复杂度陷阱"。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型面临"性能-复杂度陷阱"：捕捉复杂认知动态（如学习会话和记忆衰减）通常需要深度分层架构，但这会带来过高的计算成本，难以实时部署。需要一种既能保持高性能又计算高效的解决方案。

Method: 提出FlatFormer架构，采用"信息注入而非结构堆叠"的设计范式。使用标准扁平Transformer，通过两种轻量级注入机制增强：1）混合输入编码策略（可学习会话标识符+固定正弦步长嵌入）；2）预计算幂律偏置直接集成到注意力logits中，显式建模遗忘曲线。

Result: 在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer达到最先进性能。在EdNet数据集上，相比最强分层基线（HiTSKT），绝对AUC提升8.3%，参数使用量不到15%，推理速度约快3倍。

Conclusion: 高认知保真度不一定需要架构复杂性。FlatFormer通过信息注入而非参数堆叠，在保持计算效率的同时实现了卓越性能，为知识追踪和其他序列建模任务提供了新思路。

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [358] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

Relevance: 75.0

TL;DR: 分析5,114种期刊和520万篇论文，发现尽管70%期刊有AI使用政策（主要要求披露），但AI写作工具使用率在各学科大幅增长，政策有无无显著差异。全文分析显示透明度差距巨大：2023年以来发表的7.5万篇论文中仅76篇（0.1%）明确披露AI使用。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在学术写作中的快速普及，期刊和出版商制定了广泛的政策响应，但这些政策的实际效果尚不明确。本研究旨在评估AI使用指南在现实世界中的影响。

Method: 大规模实证分析：1）分析5,114种期刊的AI政策；2）分析超过520万篇论文；3）对164,000篇科学出版物进行全文分析，特别关注2023年以来发表的75,000篇论文的AI使用披露情况。

Result: 1）70%期刊采用AI政策（主要要求披露）；2）研究人员AI写作工具使用率在各学科大幅增长，政策有无无显著差异；3）非英语国家、物理科学和高开放获取期刊增长最快；4）透明度差距巨大：2023年以来发表的7.5万篇论文中仅76篇（0.1%）明确披露AI使用。

Conclusion: 当前政策在促进透明度或限制AI采用方面基本失败。需要重新评估伦理框架，以促进科学中负责任的AI整合。

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [359] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

Relevance: 75.0

TL;DR: ProAgent：首个端到端主动式LLM代理系统，利用多模态感知和LLM推理提供主动协助，减少用户认知负担


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要采用被动响应范式，依赖用户明确指令启动服务，增加了用户的物理和认知负担。需要开发能够主动感知环境并提供协助的智能代理系统。

Method: 1) 主动导向的上下文提取：采用按需分层感知持续感知环境，提取包含感官和身份线索的层次化上下文；2) 上下文感知的主动推理器：将上下文映射到用户需求和工具调用，提供主动协助；3) 在AR眼镜和边缘服务器上实现系统。

Result: 在真实世界测试平台、公共数据集和用户研究中评估：主动预测准确率提升33.4%，工具调用F1分数提升16.8%，用户满意度显著提高，优于现有基线方法。

Conclusion: ProAgent是首个端到端的主动代理系统，通过结合多模态感知和LLM推理，显著提升了代理的主动性和实用性，是迈向真正主动助手的重要一步。

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [360] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

Relevance: 75.0

TL;DR: 研究探讨人格提示对LLM在战略游戏决策中的影响，发现某些人格能提升游戏表现，但需要中介机制将人格转化为启发式策略值。


<details>
  <summary>Details</summary>
Motivation: 尽管人格提示能触发LLM生成不同风格的文本，但尚不清楚这些差异是否转化为可测量的行为差异，特别是在对抗性战略环境中的决策影响。研究旨在探究人格提示在战略游戏中的实际效果。

Method: 使用PERIL世界统治棋盘游戏作为测试环境，比较人格衍生的启发式策略与手动选择的策略。引入基于探索性因子分析的结构化翻译过程作为中介，将LLM生成的人格清单响应映射为启发式值。

Result: 研究发现某些与战略思维相关的人格能提升游戏表现，但仅当使用中介机制将人格转化为启发式值时有效。该方法相比直接推断的启发式策略，提高了启发式的可靠性和表面效度。

Conclusion: 人格提示确实影响LLM的决策制定，但需要通过结构化翻译过程将人格特质转化为可操作的启发式策略。该方法将心理测量学原理应用于LLM，为研究人格类型对决策的影响提供了新工具。

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [361] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

Relevance: 75.0

TL;DR: 本文研究了基于Transformer的世界模型的有效记忆跨度，分析了多种记忆增强机制，提出了记忆编码与记忆注入的分类法，并通过状态回忆任务评估了不同机制的性能，发现记忆机制能显著提升视觉Transformer的有效记忆跨度，有助于在世界模型想象中完成循环闭合。


<details>
  <summary>Details</summary>
Motivation: 世界模型使智能体能够在想象环境中进行规划，但基于Transformer的世界模型在长时程规划中存在有效记忆跨度限制，导致长轨迹生成中的感知漂移问题，阻碍了在想象轨迹中完成循环闭合的能力。

Method: 提出记忆编码与记忆注入的分类法，从残差流动态角度分析记忆机制的作用；通过状态回忆评估任务测量不同记忆机制的记忆回忆能力；分析各种机制在视觉Transformer中的权衡。

Result: 研究发现记忆机制能显著提升视觉Transformer的有效记忆跨度，为在世界模型想象中完成循环闭合提供了路径；通过状态回忆任务量化了不同记忆机制的性能差异。

Conclusion: 记忆增强机制是解决Transformer世界模型长时程规划限制的有效方法，提出的分类法和评估框架为未来世界模型架构设计提供了理论指导。

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [362] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

Relevance: 75.0

TL;DR: Auto-SPT：基于LLM的自动语义保持变换生成框架，用于提升代码克隆检测模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测模型主要基于干净、结构化的代码数据集训练，但真实世界代码经常经历各种语义保持的变换（如重构、压缩、格式化、编译器优化），导致训练与测试数据之间存在关键差距

Method: 提出Auto-SPT框架，利用LLM自动构建代码合成数据生成器：1）使用LLM设计多样化的语义保持变换（SPT）；2）为这些SPT生成强实现；3）组合这些变换以产生强变换效果。理论分析表明SPT的多样性影响组合强度

Result: Auto-SPT生成的SPT比现有方法更多样化，能显著降低最先进代码克隆检测器的性能。实验表明Auto-SPT可用于增强训练数据集，产生对真实世界对抗性代码变换具有鲁棒性的代码克隆检测模型

Conclusion: Auto-SPT框架能有效弥合代码克隆检测中训练与测试数据的差距，通过LLM生成的多样化语义保持变换提升模型的鲁棒性

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [363] [WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112)
*Yifang Xu,Jiahao Cui,Feipeng Cai,Zhihao Zhu,Hanlin Shang,Shan Luan,Mingwang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

Relevance: 75.0

TL;DR: WAM-Flow是一种视觉-语言-动作模型，将轨迹规划转化为结构化token空间上的离散流匹配，通过并行双向去噪实现粗到细的轨迹优化，在自动驾驶任务上超越了自回归和扩散基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归解码器的视觉-语言-动作模型在轨迹规划中存在顺序解码的低效问题，需要一种能够并行生成、支持计算-精度权衡的替代方法。离散流匹配提供了一种新的范式，可以实现双向去噪和可调节的推理计算。

Method: 1) 使用度量对齐的数值tokenizer通过三元组边界学习保持标量几何；2) 几何感知的流匹配目标；3) 模拟器引导的GRPO对齐，集成安全、进度和舒适度奖励；4) 多阶段适配将预训练的自回归骨干转换为非因果流模型；5) 通过持续多模态预训练增强道路场景能力。

Result: 在NAVSIM v1基准测试中，WAM-Flow的1步推理达到89.1 PDMS，5步推理达到90.3 PDMS，超越了自回归和基于扩散的VLA基线模型，展示了离散流匹配在端到端自动驾驶中的潜力。

Conclusion: 离散流匹配是端到端自动驾驶的一个有前景的新范式，通过并行双向去噪实现了计算效率和性能的平衡，为视觉-语言-动作模型提供了自回归解码器的有效替代方案。

Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.

</details>


### [364] [Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast](https://arxiv.org/abs/2512.06350)
*Nghi Truong,Phanish Puranam,Özgecan Koçak*

Main category: cs.CY

Relevance: 75.0

TL;DR: 论文分析当代AI风险辩论中的"末日论者"与"繁荣论者"观点分歧，通过定义、事实、因果和道德前提解析争议点，发现X-risk分歧源于复杂系统中设计与涌现的因果前提差异，E-risk分歧则涉及进化与革命理论的适用性差异。


<details>
  <summary>Details</summary>
Motivation: AI技术发展引发了深刻的社会分歧，尽管各方都希望AI造福人类并避免灾难性后果，但关于AI风险的辩论仍然存在尖锐对立。论文旨在系统分析这些分歧，理解"末日论者"与"繁荣论者"在AI风险认知上的根本差异。

Method: 采用定义、事实、因果和道德前提的框架解析AI风险辩论，使用LLM集成方法大规模分析推理链，识别争议的关键点。特别关注存在性风险(X-risk)和就业风险(E-risk)两种风险类型的辩论差异。

Result: 研究发现：1) X-risk分歧主要源于对复杂系统中设计与涌现的因果前提差异；2) E-risk分歧涉及过去理论(进化)适用性与不适用性(革命)的因果前提差异；3) 两种风险辩论都未涉及重大道德价值观分歧；4) 分歧可描述为对人类理性有限程度的不同看法。

Conclusion: 论文提出了一种使用LLM集成大规模分析推理链的方法，可用于识别任何领域公共风险辩论的关键争议点。该方法有助于理解复杂社会辩论的结构性分歧，促进更有建设性的对话。

Abstract: The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk ("X-risk") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks ("E-risks") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.

</details>


### [365] [Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems](https://arxiv.org/abs/2512.06590)
*Tendai Mukande,Esraa Ali,Annalina Caputo,Ruihai Dong,Noel OConnor*

Main category: cs.IR

Relevance: 75.0

TL;DR: HGLMRec：基于多LLM代理的推荐系统，通过超图编码器捕捉用户-物品间的复杂多行为关系，在推理时仅检索相关token以降低计算成本，同时提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在电商和社交媒体中至关重要，但现有的生成式推荐方法面临幻觉问题（降低推荐性能）和实际场景中的高计算成本挑战。

Method: 提出HGLMRec模型，采用多LLM代理架构，结合超图编码器捕捉用户与物品间的复杂多行为关系，在推理时仅检索相关token以减少计算开销。

Result: 实验结果显示HGLMRec在较低计算成本下优于现有最先进基线模型，实现了性能提升。

Conclusion: HGLMRec通过超图编码和多LLM代理架构有效解决了生成式推荐中的幻觉和计算效率问题，为推荐系统提供了更高效的解决方案。

Abstract: Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.

</details>


### [366] [Towards Small Language Models for Security Query Generation in SOC Workflows](https://arxiv.org/abs/2512.06660)
*Saleha Muzammil,Rahul Reddy,Vishal Kamalakrishnan,Hadi Ahmadi,Wajih Ul Hassan*

Main category: cs.CR

Relevance: 75.0

TL;DR: 本文提出一个三旋钮框架，使用小型语言模型(SLMs)实现自然语言到Kusto查询语言(KQL)的翻译，用于企业安全运营。通过提示优化、微调和两阶段架构设计，在保持低成本的同时达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心的分析师需要查询大量遥测数据，但编写正确的KQL查询需要专业知识，这成为安全团队扩展的瓶颈。研究是否能用小型语言模型实现准确且成本效益高的自然语言到KQL翻译。

Method: 提出三旋钮框架：1) 提示优化：轻量级检索和错误感知提示；2) 微调：使用LoRA和原理蒸馏，添加链式思考解释；3) 架构设计：两阶段方法，SLM生成候选查询，低成本LLM进行模式感知优化和选择。

Result: 在Microsoft NL2KQL Defender评估数据集上，两阶段方法达到0.987语法准确率和0.906语义准确率。在Microsoft Sentinel数据上达到0.964语法准确率和0.831语义准确率，比GPT-5降低10倍token成本。

Conclusion: 小型语言模型可以作为安全运营中自然语言查询的实用、可扩展基础，在保持高准确率的同时显著降低成本。

Abstract: Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.

</details>


### [367] [From Description to Score: Can LLMs Quantify Vulnerabilities?](https://arxiv.org/abs/2512.06781)
*Sima Jafarikhah,Daniel Thompson,Eva Deans,Hossein Siadati,Yi Liu*

Main category: cs.CR

Relevance: 75.0

TL;DR: 该研究评估了通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）在自动化漏洞评分（CVSS）方面的性能，发现LLMs在某些指标上显著优于基线，但性能因模型和指标而异，且CVE描述的质量限制了自动化效果。


<details>
  <summary>Details</summary>
Motivation: 手动漏洞评分（如CVSS）是资源密集型且受主观解释影响的过程。研究旨在探索通用大语言模型能否自动化这一过程，以减轻日益增长的CVE积压问题。

Method: 分析了超过31,000个最近的CVE条目，使用ChatGPT、Llama、Grok、DeepSeek和Gemini等通用大语言模型进行自动化评分，并与基线进行比较。还研究了集成元分类器的效果，并分析了CVE描述的质量问题。

Result: LLMs在某些指标（如可用性影响）上显著优于基线，但在其他指标（如攻击复杂度）上提升有限。ChatGPT-5达到最高精度。模型性能因LLM家族和具体CVSS指标而异。LLMs倾向于对相同的CVE产生错误分类，集成方法仅带来边际改进。CVE描述常缺乏关键上下文或包含模糊表述。

Conclusion: 需要改进漏洞描述质量并纳入更丰富的上下文细节，才能支持更可靠的自动化推理，缓解CVE积压问题。LLMs在自动化漏洞评分方面有潜力但受限于数据质量。

Abstract: Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.

</details>


### [368] [VideoVLA: Video Generators Can Be Generalizable Robot Manipulators](https://arxiv.org/abs/2512.06963)
*Yichao Shen,Fangyun Wei,Zhiying Du,Yaobo Liang,Yan Lu,Jiaolong Yang,Nanning Zheng,Baining Guo*

Main category: cs.RO

Relevance: 75.0

TL;DR: VideoVLA：将大型视频生成模型转化为机器人视觉-语言-动作操纵器，通过联合建模视频、语言和动作模态，实现动作序列和未来视觉结果的预测，提升机器人操作的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在机器人操作中泛化能力有限，难以适应新任务、新物体和新环境。研究探索将大型视频生成模型转化为机器人操纵器，通过预测动作序列和未来视觉结果来提升泛化能力。

Method: 基于多模态扩散Transformer架构，联合建模视频、语言和动作模态。利用预训练的视频生成模型进行联合视觉和动作预测，给定语言指令和图像，预测动作序列及未来视觉结果。

Result: 高质量的未来视觉想象与可靠的动作预测和任务成功相关，验证了视觉想象在操作中的重要性。VideoVLA展现出强大的泛化能力，包括模仿其他具身的技能和处理新物体。

Conclusion: 通过同时预测动作及其视觉后果的双重预测策略，探索了机器人学习范式转变，释放了操作系统的泛化能力，为机器人操作提供了新思路。

Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.

</details>


### [369] [RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations](https://arxiv.org/abs/2512.07122)
*Liping Han,Tingting Nie,Le Yu,Mingzhe Hu,Tao Yue*

Main category: cs.SE

Relevance: 75.0

TL;DR: 提出基于大语言模型的无人机风险配置实时修复方法RisConFix，通过监控飞行状态并利用LLM分析参数关系生成修正，实现97%的修复成功率


<details>
  <summary>Details</summary>
Motivation: 无人机飞行控制软件有大量可配置参数，即使使用推荐值组合仍可能导致不稳定飞行行为，降低无人机鲁棒性，需要实时修复风险配置

Method: 提出RisConFix方法：持续监控无人机运行状态，检测到异常行为时触发修复机制，利用LLM分析配置参数与飞行状态关系，生成修正参数更新，采用迭代过程确保配置有效性

Result: 在ArduPilot案例研究（1421组错误配置）中，RisConFix达到最佳修复成功率97%，最优平均修复次数1.17次，证明其能有效高效实时修复风险配置

Conclusion: 基于LLM的RisConFix方法能够有效实时修复无人机风险配置，提高飞行稳定性和鲁棒性，为无人机安全运行提供新解决方案

Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.

</details>


### [370] [ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning](https://arxiv.org/abs/2512.07371)
*Byungju Kim,Jinu Pahk,Chungwoo Lee,Jaejoon Kim,Jangha Lee,Theo Taeyeong Kim,Kyuhwan Shim,Jun Ki Lee,Byoung-Tak Zhang*

Main category: cs.RO

Relevance: 75.0

TL;DR: ESPADA是一个语义感知的视觉运动策略加速框架，通过VLM-LLM管道分析3D夹爪-物体关系来分割演示，在非关键阶段进行激进下采样，实现约2倍加速而不降低成功率。


<details>
  <summary>Details</summary>
Motivation: 基于行为克隆的视觉运动策略虽然能实现精确操作，但继承了人类演示的缓慢节奏，限制了实际部署。现有加速方法主要依赖统计或启发式线索，忽略了任务语义，在不同操作场景中可能失效。

Method: ESPADA使用VLM-LLM管道分析3D夹爪-物体关系来语义分割演示，识别关键和非关键阶段。通过动态时间规整在仅使用动态特征的数据集上传播分割标签，在非关键阶段进行激进下采样，无需额外数据、架构修改或重新训练。

Result: 在模拟和真实世界实验中，与ACT和DP基线相比，ESPADA实现了约2倍的加速，同时保持成功率，缩小了人类演示与高效机器人控制之间的差距。

Conclusion: ESPADA通过语义感知的演示分割实现了视觉运动策略的有效加速，在保持精度的同时显著提高执行速度，为机器人控制的实际部署提供了实用解决方案。

Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.

</details>


### [371] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

Relevance: 75.0

TL;DR: AutoICE：基于LLM的进化搜索方法，用于从自然语言需求合成可验证的C代码，通过多样个体初始化、协作交叉和自反思变异来提升代码验证成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动形式化方法存在严重的语法和语义错误，主要原因是领域特定预训练语料稀缺，且难以有效形式化隐式知识。需要一种能够合成可验证代码并降低形式化方法采用门槛的解决方案。

Method: 提出AutoICE框架，采用LLM驱动的进化搜索策略：1）多样个体初始化生成不同候选代码；2）协作交叉实现多样迭代更新；3）自反思变异促进发现隐式知识，从而缓解单智能体迭代中的错误传播问题。

Result: AutoICE在代码验证方面表现优异：成功验证90.36%的代码，优于现有最佳方法。在开发者友好数据集变体上，达到88.33%的验证成功率，显著超越现有方法的65%。

Conclusion: AutoICE通过进化搜索策略有效解决了LLM在代码合成中的错误传播和隐式知识形式化问题，显著提升了从自然语言需求生成可验证代码的成功率。

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


### [372] [Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach](https://arxiv.org/abs/2512.07814)
*Hua Yang,Alejandro Velasco,Sen Fang,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

Relevance: 75.0

TL;DR: 论文研究了代码大语言模型(LLM4Code)中不同类型个人身份信息(PII)的泄露风险差异，发现泄露风险与PII的学习难度相关，并提供了类型依赖泄露风险的因果证据。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型依赖包含大量个人身份信息的开源仓库，现有研究将PII视为单一类别，忽视了不同类型PII的异质风险。需要研究不同PII类型是否在学习和泄露可能性上存在差异，以及这种关系是否具有因果性。

Method: 1) 构建包含多样PII类型的数据集；2) 微调不同规模的代表性模型；3) 计算真实PII数据的训练动态；4) 构建结构因果模型来估计学习性对泄露的因果效应。

Result: 泄露风险在不同PII类型间差异显著：易学习的实例(如IP地址)泄露风险较高，而较难学习的类型(如密钥和密码)泄露较少。模糊类型表现出混合行为。学习性与泄露风险之间存在相关性。

Conclusion: 首次提供了泄露风险具有类型依赖性的因果证据，为开发类型感知和学习性感知的LLM4Code防御提供了指导。

Abstract: Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.

</details>


### [373] [GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols](https://arxiv.org/abs/2512.06404)
*Mohammad Soleymanibrojeni,Roland Aydin,Diego Guedes-Sobrinho,Alexandre C. Dias,Maurício J. Piotrowski,Wolfgang Wenzel,Celso Ricardo Caldeira Rêgo*

Main category: cs.AI

Relevance: 65.0

TL;DR: GENIUS是一个AI代理工作流，将量子ESPRESSO知识图谱与分层LLM架构结合，通过有限状态错误恢复机监督，将自然语言提示转换为验证过的输入文件，实现电子结构DFT模拟的自动化。


<details>
  <summary>Details</summary>
Motivation: 材料发现中的原子模拟需要计算机专家设置和调试，这一知识鸿沟限制了集成计算材料工程（ICME）的应用。现有先进代码对非专家用户来说仍然繁琐，需要自动化解决方案来降低使用门槛。

Method: 1. 构建智能量子ESPRESSO知识图谱；2. 采用分层LLM架构（由有限状态错误恢复机监督）；3. 将自由形式的人类提示转换为验证过的输入文件；4. 实现自主修复和验证机制。

Result: 在295个多样化基准测试中，约80%成功运行到完成，其中76%可自主修复。相比纯LLM基线，推理成本减半，幻觉基本消除。成功率呈指数衰减至7%基线。

Conclusion: GENIUS框架通过智能自动化协议生成、验证和修复，民主化了电子结构DFT模拟，为学术界和工业界的大规模筛选和ICME设计循环加速。

Abstract: Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.

</details>


### [374] [Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients](https://arxiv.org/abs/2512.06990)
*Krishna Arun,Moinak Bhattachrya,Paras Goel*

Main category: cs.AI

Relevance: 65.0

TL;DR: 开发了一个用于胶质母细胞瘤诊断和治疗规划的端到端AI系统，包含诊断阶段的序列决策框架和治疗阶段的强化学习系统，能显著降低计算成本并提高生存率。


<details>
  <summary>Details</summary>
Motivation: 目前医学领域缺乏支持医生治疗胶质母细胞瘤等异质性脑肿瘤的AI系统，GBM是死亡率最高的癌症，五年生存率仅5.1%。需要开发端到端解决方案来辅助诊断和治疗规划。

Method: 诊断阶段：使用4个分类模型（CNN和SVM）的序列决策框架，逐步分类脑部图像；治疗阶段：RL系统包含3个生成模型：扩散模型预测切除结果、时空视觉Transformer预测放疗后进展、扩散模型生成化疗后MRI；使用近端策略优化的反馈循环优化切除位置。

Result: 1) 序列决策框架降低22.28倍计算成本；2) Transformer减少113小时肿瘤进展推理时间；3) 真实情境数据增强提高DICE分数2.9%；预计提高生存率0.9%，可能拯救约2250人。

Conclusion: 该AI系统为GBM提供了首个端到端解决方案，通过创新的序列决策框架和强化学习系统，在计算效率、推理速度和预测准确性方面都有显著提升，有望改善患者生存率。

Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.

</details>


### [375] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

Relevance: 65.0

TL;DR: ClinNoteAgents是一个基于LLM的多智能体框架，用于从临床文本笔记中提取心衰再入院风险因素并进行预测，减少对结构化字段的依赖


<details>
  <summary>Details</summary>
Motivation: 心衰是美国老年人再入院的主要原因之一，临床笔记包含丰富的患者信息但未被充分利用。传统模型依赖专家规则和医学词典，难以处理临床笔记中的拼写错误、缩写和领域特定术语

Method: 开发了基于LLM的多智能体框架ClinNoteAgents，将自由文本临床笔记转换为：(1)临床和社会风险因素的结构化表示用于关联分析；(2)临床医生风格的抽象表示用于30天再入院预测

Result: 在3,544份笔记、2,065名患者（再入院率35.16%）上评估，在从自由文本提取风险因素、识别关键贡献因素和预测再入院风险方面表现出色

Conclusion: 通过减少对结构化字段的依赖并最小化手动标注和模型训练，ClinNoteAgents为数据有限的医疗系统提供了可扩展且可解释的基于笔记的心衰再入院风险建模方法

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [376] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出一个Python包，将SHAP特征重要性可视化与大型语言模型（GPT）结合，生成上下文文本解释，提高非技术用户对机器学习模型解释的理解性。


<details>
  <summary>Details</summary>
Motivation: SHAP虽然能提供特征重要性可视化，但缺乏对非技术用户有意义的上下文解释。在医疗等高风险领域部署机器学习模型时，可解释人工智能（XAI）尤为重要，需要更用户友好的解释方法。

Method: 开发Python包，将SHAP与OpenAI的GPT集成，通过用户定义的参数（特征别名、描述、背景信息）生成上下文文本解释。在医疗案例研究中应用，并通过Likert量表和访谈进行用户评估。

Result: 用户评估显示，与仅可视化输出相比，生成的解释被认为更易理解和上下文更合适。初步结果表明，可视化与上下文文本结合能支持更用户友好和可信的模型解释。

Conclusion: 将SHAP可视化与LLM生成的文本解释结合，能提高模型解释的感知理解性，特别是在非技术用户中。这种方法为更用户友好的XAI工具提供了方向。

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [377] [Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples](https://arxiv.org/abs/2512.06123)
*Qilin Zhou,Zhengyuan Wei,Haipeng Wang,Zhuo Wang,W. K. Chan*

Main category: cs.SE

Relevance: 65.0

TL;DR: HiCert是一种新颖的基于掩码的认证检测技术，针对对抗性补丁攻击提供全面的补丁鲁棒性认证。它通过形式化分析处理预测标签与真实标签不一致的突变体，显著提高了认证样本数量和检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的认证检测方法在认证被错误分类或突变体预测不一致的样本时效果不佳。这些方法无法有效处理预测标签与真实标签不一致的突变体，限制了认证的全面性和可靠性。

Method: HiCert通过形式化分析识别有害样本与良性对应样本之间的关系。它检查每个良性样本潜在有害突变体的最大置信度边界，确保每个有害样本要么具有低于该边界的相同预测标签突变体的最小置信度，要么至少有一个预测标签与有害样本本身不同的突变体。

Result: 实验显示HiCert具有高有效性，实现了新的最先进性能：显著认证了更多良性样本（包括不一致和一致的），在无警告样本上获得了显著更高的准确率，以及显著更低的虚假沉默率。

Conclusion: HiCert是第一个能够为认证检测提供如此全面补丁鲁棒性认证的工作，系统性地认证了不一致样本和一致样本，显著提升了对抗性补丁攻击的防御能力。

Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.

</details>


### [378] [DUET: Agentic Design Understanding via Experimentation and Testing](https://arxiv.org/abs/2512.06247)
*Gus Henry Smith,Sandesh Adhikary,Vineet Thumuluri,Karthik Suresh,Vivek Pandit,Kartik Hegde,Hamid Shojaei,Chandra Bhagavatula*

Main category: cs.SE

Relevance: 65.0

TL;DR: DUET是一个通过实验和测试来理解硬件设计的通用方法，模仿硬件专家通过迭代实验理解复杂设计的过程，提升AI代理在形式验证等任务上的性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂软件工程任务方面表现出色，但在硬件设计任务上表现不佳。RTL代码使用SystemVerilog的低级语言特性编码复杂、动态、时间演化的行为，LLMs难以仅从RTL语法推断这些复杂行为，限制了其在代码补全、文档化和验证等下游任务的能力

Method: DUET方法模仿硬件设计专家理解复杂设计的方式：不是一次性阅读RTL，而是通过迭代实验使用多种工具。它迭代生成假设，使用EDA工具（如仿真、波形检查和形式验证）进行测试，并整合结果以建立自底向上的设计理解

Result: 评估显示，与没有实验的基线流程相比，DUET提高了AI代理在形式验证任务上的性能

Conclusion: DUET通过模仿硬件专家的实验方法，为LLMs理解复杂硬件设计提供了一种有效途径，解决了LLMs在RTL代码理解方面的局限性

Abstract: AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.

</details>


### [379] [BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models](https://arxiv.org/abs/2512.06555)
*Arush Sachdeva,Rajendraprasad Saravanan,Gargi Sarkar,Kavita Vemuri,Sandeep Kumar Shukla*

Main category: cs.CR

Relevance: 65.0

TL;DR: BEACON是一个统一的双维度框架，将行为心理学与网络犯罪战术生命周期相结合，通过微调LLM进行多标签分类和可解释分析，提升网络犯罪分析的结构化、可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 网络犯罪越来越多地利用人类认知偏见和技术漏洞，但现有分析框架主要关注操作层面，忽视了心理操纵。需要将心理学原理与网络犯罪战术相结合，实现更全面的分析。

Method: 提出BEACON框架，整合基于前景理论和Cialdini说服原则的6个心理操纵类别，以及14阶段网络犯罪战术生命周期。使用参数高效学习方法微调单个大语言模型，进行跨心理和战术维度的联合多标签分类，同时生成人类可解释的说明。

Result: 在真实世界和合成增强的网络犯罪叙事数据集上的实验表明，整体分类准确率比基础模型提高了20%，使用ROUGE和BERTScore衡量的推理质量也有显著提升。

Conclusion: BEACON框架能够将非结构化的受害者叙事自动分解为结构化的行为和操作情报，支持改进网络犯罪调查、案件关联和主动诈骗检测，为网络犯罪分析提供了统一的心理-战术双维度分析框架。

Abstract: Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.

</details>


### [380] [Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability](https://arxiv.org/abs/2512.06591)
*Joe Shymanski,Jacob Brue,Sandip Sen*

Main category: cs.HC

Relevance: 65.0

TL;DR: 该论文批评了当前可解释AI评估过度依赖主观用户满意度调查的问题，通过社会安全福利申请年龄选择实验发现，虽然用户对有效解释和无效解释的满意度评分相同，但只有有效解释能显著提升用户的实际任务表现和领域理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI评估主要依赖主观用户调查，但这种方法可能无法区分真正有效的解释和无效的安慰剂式解释。研究者质疑用户满意度是否能准确反映解释的实际效用，需要更客观的评估方法来衡量解释是否真正帮助用户建立有用的领域理解。

Method: 通过社会安全福利申请年龄选择任务进行实验，将参与者分为三组：无解释组、安慰剂解释组（无效解释）和有效解释组。同时收集主观满意度评分和客观任务表现数据，比较不同解释类型对用户实际理解能力的影响。

Result: 实验结果显示：1）有效解释组的参与者在客观任务表现上显著优于其他两组；2）但用户对安慰剂解释和有效解释的主观满意度评分没有显著差异；3）这表明主观调查无法区分解释的实际效用，需要结合客观性能指标来评估解释质量。

Conclusion: 仅依赖主观用户满意度调查不足以评估可解释AI系统的真实效果。未来的解释能力评估应该结合客观任务表现指标和主观评估，以更准确地衡量解释是否真正支持用户建立有用的领域理解。

Abstract: Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.

</details>


### [381] [Optimal and Diffusion Transports in Machine Learning](https://arxiv.org/abs/2512.06797)
*Gabriel Peyré*

Main category: math.OC

Relevance: 65.0

TL;DR: 该论文综述了概率分布时间演化的数学框架，重点介绍了扩散方法和最优传输两种互补方法，并展示了它们在采样、神经网络优化和Transformer动力学建模中的应用。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的许多问题（如扩散采样、神经网络权重优化、LLM中token分布演化）都可以表示为概率分布的时间演化设计。这些不同应用具有共同的数学结构，需要统一的框架来理解和分析。

Method: 采用从欧拉密度表示到拉格朗日矢量场表示的转换，通过粒子输运来研究密度演化。重点介绍两种互补方法：1）基于随机插值过程的扩散方法；2）通过最小化位移成本定义插值的最优传输方法。

Result: 建立了统一的数学框架来分析不同机器学习问题中的概率分布演化，展示了扩散方法和最优传输方法在采样、神经网络优化和Transformer动力学建模中的具体应用。

Conclusion: 概率分布的时间演化框架为理解扩散方法、神经网络优化和LLM中token分布演化提供了统一的数学视角，扩散方法和最优传输作为互补工具在不同应用中展现出优势。

Abstract: Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.

</details>


### [382] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

Relevance: 65.0

TL;DR: 该研究探索使用大型语言模型（Claude-3.5和GPT-4o）实现文本领域特定语言的语法和实例协同演化，重点评估其在处理文本实例时保留注释和格式等辅助信息的能力。


<details>
  <summary>Details</summary>
Motivation: 软件语言随时间演化时，原本符合语法的文本实例会过时。在模型驱动工程中，虽然有多种技术支持元模型与模型的协同演化，但这些技术不适用于文本语法定义的DSL，应用它们可能导致原始实例中注释和布局信息的丢失，而这些信息对软件理解和维护很重要。

Method: 使用两种先进的大型语言模型（Claude-3.5和GPT-4o），在七个案例语言上进行实验，评估LLM在直接处理文本实例时实现语法和实例协同演化的可行性，特别关注其保留辅助信息的能力。

Result: 结果表明，在实例规模有限的代表性小规模案例中，所考虑的LLM在迁移文本实例方面表现出良好能力。但观察到LLM解决方案在扩展到更大实例时面临显著的可扩展性挑战。

Conclusion: LLM在小型文本实例的语法协同演化中具有潜力，但可扩展性限制是主要挑战，这些发现为未来研究提供了有价值的见解。

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>


### [383] [BabelCoder: Agentic Code Translation with Specification Alignment](https://arxiv.org/abs/2512.06902)
*Fazle Rabbi,Soumit Kanti Saha,Tri Minh Triet Pham,Song Wang,Jinqiu Yang*

Main category: cs.SE

Relevance: 65.0

TL;DR: BabelCoder：一个基于多智能体协作的代码翻译框架，通过翻译、测试、修复三个专门化智能体的协同工作，显著提升了跨语言代码翻译的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统的发展，开发者需要在多种编程语言间工作，经常需要将代码从一种语言迁移到另一种语言。虽然自动代码翻译是一个有前景的解决方案，但长期以来一直是一个具有挑战性的任务。现有的LLM方法在准确性上仍有局限，未能有效利用代码的上下文和结构信息，且缺乏结构化的多智能体协作框架来提升翻译质量。

Method: BabelCoder是一个智能体框架，将代码翻译任务分解为三个专门化智能体：翻译智能体（生成代码）、测试智能体（验证正确性）、修复智能体（修复错误）。这些智能体协同工作，通过迭代改进翻译质量。

Result: 在四个基准数据集上评估，与四个最先进的基线方法相比，BabelCoder在94%的情况下表现更优，提升幅度为0.5%-13.5%，平均准确率达到94.16%。

Conclusion: BabelCoder通过多智能体协作框架显著提升了代码翻译的准确性，证明了结构化智能体方法在复杂代码转换任务中的有效性。

Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.

</details>


### [384] [Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model](https://arxiv.org/abs/2512.06999)
*Zihao Wang,Ruibin Yuan,Ziqi Geng,Hengjia Li,Xingwei Qu,Xinyi Li,Songye Chen,Haoying Fu,Roger B. Dannenberg,Kejun Zhang*

Main category: cs.SD

Relevance: 65.0

TL;DR: 论文提出参考无关的多维度歌唱评估系统，包含Sing-MD数据集、VocalVerse混合架构和H-TPR基准测试，旨在解决现有系统依赖参考音轨和简化评分的问题。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱评估系统存在两大根本限制：1) 依赖参考音轨，限制了创造性表达；2) 将复杂表演简化为仅基于音高和节奏的非诊断性评分。需要从判别性评估转向描述性评估，建立完整的参考无关、多维度评估生态系统。

Method: 1) 引入Sing-MD大规模数据集，由专家在四个维度标注：气息控制、音色质量、情感表达和声乐技巧；2) 提出VocalVerse高效混合架构，利用轻量级声学编码器建模全局表演特征和长期依赖，解决MLLMs处理全长歌曲的内存限制；3) 建立H-TPR基准测试，评估模型生成感知有效排序而非预测噪声真实评分的能力。

Result: 1) Sing-MD数据集分析显示专家标注存在显著不一致，挑战了传统基于准确度指标的有效性；2) VocalVerse架构能够高效处理全长歌曲；3) H-TPR基准测试提供更符合人类感知的评估框架。

Conclusion: 论文提出了一个完整的参考无关歌唱评估生态系统，通过多维度数据集、高效混合架构和人类在环感知排序基准，解决了现有系统的局限性，为更全面、创造性的歌唱评估提供了新方向。

Abstract: Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.

</details>


### [385] [Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding](https://arxiv.org/abs/2512.07344)
*Shengyuan Ye,Bei Ouyang,Tianyi Qian,Liekang Zeng,Mu Yuan,Xiaowen Chu,Weijie Hong,Xu Chen*

Main category: cs.DC

Relevance: 65.0

TL;DR: Venus是一个用于高效在线视频理解的边缘-云端解耦内存与检索系统，通过将内存构建和关键帧检索下沉到边缘，实现15-131倍的速度提升，在保持推理准确性的同时实现秒级实时响应。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型(VLMs)在多模态理解方面表现出色，但在实际在线视频理解应用中，部署约束常被忽视，导致系统开销过大。需要设计高效的系统架构来平衡推理能力和部署效率。

Method: 提出边缘-云端解耦架构，分两个阶段：1) 摄入阶段：通过场景分割和聚类处理流式边缘视频，使用多模态嵌入模型为选定的关键帧构建分层内存；2) 查询阶段：索引查询，采用基于阈值的渐进采样算法选择关键帧，平衡系统成本和推理准确性。

Result: 与最先进方法相比，Venus实现了15-131倍的总响应延迟加速，能够在秒级内提供实时响应，同时保持相当甚至更优的推理准确性。

Conclusion: Venus通过创新的边缘-云端解耦架构和高效的内存检索系统，解决了VLMs在实际部署中的效率问题，为在线视频理解应用提供了实用的解决方案。

Abstract: Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.

</details>


### [386] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

Relevance: 45.0

TL;DR: 该论文综述了AI在反洗钱(AML)中的应用，提出基于图检索增强生成(RAG-Graph)的KYC系统，实验显示能提高检测准确率、降低误报率，并增强KYC流程的效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 洗钱和金融欺诈每年造成数万亿美元损失，传统AML系统存在检测准确率低、误报率高、人工调查负担重等问题，需要AI技术来现代化AML工作流程，支持可持续的金融监管发展。

Method: 论文首先综述AI在AML中的应用，然后提出AI驱动的KYC应用，整合图检索增强生成(RAG-Graph)与生成模型，通过图结构增强信息检索和决策支持能力。

Result: 实验结果显示RAG-Graph架构在不同评估设置下表现出高忠实度和强答案相关性，显著提升了KYC客户尽职调查/增强尽职调查(CDD/EDD)工作流程的效率和透明度。

Conclusion: AI技术能够现代化AML工作流程，RAG-Graph架构为KYC流程提供了高效透明的解决方案，未来研究方向包括联邦学习、公平可解释AI、强化学习自适应防御和人机协同可视化系统。

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [387] [Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation](https://arxiv.org/abs/2512.07212)
*Zhaoyang Liu,Mokai Pan,Zhongyi Wang,Kaizhen Zhu,Haotao Lu,Jingya Wang,Ye Shi*

Main category: cs.AI

Relevance: 45.0

TL;DR: BridgePolicy：一种基于扩散桥的生成式视觉运动策略，将观测嵌入扩散过程的随机动力学中，从信息丰富的先验而非随机噪声开始采样，提升机器人控制的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的模仿学习方法通常将观测作为去噪网络的高层条件输入，而非将其整合到扩散过程的随机动力学中。这导致采样必须从随机高斯噪声开始，削弱了感知与控制之间的耦合，导致性能不佳。

Method: 提出BridgePolicy，通过扩散桥公式将观测嵌入随机微分方程中，构建观测信息化的轨迹。设计多模态融合模块和语义对齐器，统一视觉和状态输入，对齐观测和动作表示，使桥接适用于异构机器人数据。

Result: 在三个基准测试的52个仿真任务和五个真实世界任务上的广泛实验表明，BridgePolicy始终优于最先进的生成策略。

Conclusion: BridgePolicy通过将观测整合到扩散过程的随机动力学中，从信息丰富的先验开始采样，显著提高了机器人控制的精度和可靠性，为生成式视觉运动策略提供了新思路。

Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.

</details>


### [388] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

Relevance: 45.0

TL;DR: 提出了Agent Capability Problem (ACP)框架，通过信息获取视角预测智能体在资源约束下能否解决问题，将问题解决建模为信息获取过程，用有效成本Ceff预测资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体在决定是否投入资源解决问题时通常依赖经验启发式方法，缺乏理论框架来预测智能体在资源约束下能否成功完成任务。需要一种能够提前预测资源需求的理论框架，避免资源浪费。

Method: 将问题解决建模为信息获取过程：智能体需要Itotal比特信息来识别解决方案，每个行动获得Istep比特信息，成本为Cstep。定义有效成本Ceff = (Itotal/Istep) * Cstep来预测资源需求。提供了Ceff的下界证明和紧致的概率上界。

Result: 实验验证显示ACP预测与实际智能体性能高度一致，能够有效约束搜索努力，相比贪婪和随机策略提高了效率。该框架在基于LLM的智能体工作流中具有普适性。

Conclusion: ACP提供了一个统一的信息理论框架，连接了主动学习、贝叶斯优化和强化学习的原则，为预测智能体在资源约束下的能力提供了理论基础。

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [389] [KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening](https://arxiv.org/abs/2512.05994)
*Rohan Sharma,Dancheng Liu,Jingchen Sun,Shijie Zhou,Jiayu Qin,Jinjun Xiong,Changyou Chen*

Main category: eess.AS

Relevance: 45.0

TL;DR: KidSpeak：首个面向儿童语音的多任务语音增强基础模型，结合FASA自动语音对齐工具，专门解决儿童语音识别挑战


<details>
  <summary>Details</summary>
Motivation: 现有AI模型主要基于清晰成人语音数据集，无法有效处理儿童语音（特别是早期发育阶段或有语言障碍的儿童），限制了AI在教育服务中的应用

Method: 1) KidSpeak：采用两阶段训练过程，将语音学知识融入语音编码器的多任务语音增强基础模型；2) FASA：灵活自动语音对齐工具，用于构建高质量训练数据集

Result: 1) KidSpeak在四个任务上平均准确率达87%；2) FASA在CHILDES数据集上比人工标注提升13.6倍数据质量

Conclusion: KidSpeak和FASA是首个面向儿童语音语言治疗的全面解决方案，提供多功能语音LLM和鲁棒对齐工具

Abstract: With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool.

</details>


### [390] [Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling](https://arxiv.org/abs/2512.06259)
*Yash Choudhary,Preeti Rao,Pushpak Bhattacharyya*

Main category: cs.SD

Relevance: 45.0

TL;DR: GAMENet是一个用于音乐流行度预测的多模态深度学习架构，通过自适应门控机制整合音频、歌词和社交元数据专家，使用音频特征、LLM歌词嵌入和新提出的职业轨迹动态特征，在两个数据集上相比基线分别提升12%和16%的R²分数。


<details>
  <summary>Details</summary>
Motivation: 预测歌曲发布前的商业成功对音乐产业至关重要，但现有方法存在四个主要局限：1) 音频和歌词的时间动态被平均化；2) 歌词被表示为词袋，忽略了组合结构和情感语义；3) 忽略了艺术家和歌曲级别的历史表现；4) 多模态融合方法依赖简单的特征拼接，导致表征对齐不佳。

Method: 提出GAMENet端到端多模态深度学习架构，包含：1) 音频特征通过OnionEnsembleAENet提取；2) 歌词嵌入通过大语言模型管道获得；3) 新提出的职业轨迹动态特征捕捉多年艺术家职业动量和歌曲级轨迹统计；4) 通过自适应门控机制整合各模态专家。

Result: 在Music4All数据集上，GAMENet相比直接多模态特征拼接提升12%的R²；Spotify音频描述符单独R²为0.13，整合聚合CTD特征提升至0.69，加入时序CTD特征再提升7%；在SpotGenTrack数据集上相比基线提升16%。

Conclusion: GAMENet通过整合音频、歌词和社交元数据，特别是新提出的职业轨迹动态特征，显著提升了音乐流行度预测性能，证明了多模态融合和时序动态建模的重要性。

Abstract: Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.

</details>


### [391] [AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity](https://arxiv.org/abs/2512.06396)
*Shovan Roy*

Main category: cs.CR

Relevance: 45.0

TL;DR: AgenticCyber是一个基于生成式AI的多智能体系统，用于实时检测和响应分布式环境中的网络威胁，通过协调专门智能体监控云日志、监控视频和环境音频，实现了96.2%的F1分数和420ms的响应延迟。


<details>
  <summary>Details</summary>
Motivation: 分布式环境中网络威胁日益复杂，需要能够实时检测和响应多模态数据流的先进框架。现有的安全技术往往孤立运行，缺乏跨模态推理能力，导致响应时间较长且态势感知不足。

Method: 采用生成式AI驱动的多智能体系统架构，协调专门智能体监控云日志、监控视频和环境音频。使用Google的Gemini等多模态语言模型，结合LangChain进行智能体编排，实现跨模态推理和自适应安全态势管理。

Result: 在威胁检测方面达到96.2%的F1分数，响应延迟降低至420ms，平均响应时间（MTTR）减少65%。在AWS CloudTrail日志、UCF-Crime视频帧和UrbanSound8K音频片段等基准数据集上表现优于标准入侵检测系统。

Conclusion: 该工作提出了一个可扩展、模块化的主动式网络安全架构，适用于企业网络和物联网生态系统，通过跨模态推理和自动化修复克服了孤立安全技术的局限性。

Abstract: The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.

</details>


### [392] [Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age](https://arxiv.org/abs/2512.06616)
*Rasam Dorri,Rami Zwick*

Main category: cs.HC

Relevance: 45.0

TL;DR: 论文提出"记忆权力不对称"概念，指AI系统与人类关系中因AI拥有超强记忆能力而产生的结构性权力失衡，威胁人类心理安全与关系平等。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入人际关系，AI系统能够大规模、永久地记录交互历史，而人类依赖自然遗忘来维持心理安全、宽恕和身份变化。这种记忆能力的不对称创造了新的权力失衡，需要理论框架来理解和解决。

Method: 结合人类记忆研究、权力依赖理论、AI架构和消费者脆弱性研究，开发了一个概念框架，包含MPA的四个维度（持久性、准确性、可访问性、整合性）和四个权力转化机制。

Result: 提出了MPA作为区别于信息不对称、隐私、监控等概念的独特构念，分析了其在个体、关系/公司和社会层面的后果，并制定了六个恢复记忆平衡的设计原则。

Conclusion: 保护相互遗忘或至少对记忆的相互控制应成为AI时代的核心设计和政策目标，需要技术、设计和监管的协同来解决记忆权力不对称问题。

Abstract: As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.

</details>


### [393] [Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.06951)
*Ilia Larchenko,Gleb Zarin,Akash Karnatak*

Main category: cs.RO

Relevance: 45.0

TL;DR: 该论文提出了一个在BEHAVIOR挑战赛中获胜的视觉-动作策略，通过相关噪声流匹配、可学习混合层注意力等创新，在50个多样化家庭任务中实现了26%的q-score。


<details>
  <summary>Details</summary>
Motivation: 解决复杂长视野家庭任务中的视觉-动作策略问题，特别是在需要双手操作、导航和上下文感知决策的逼真模拟环境中，提升智能体在多样化任务中的表现。

Method: 基于Pi0.5架构，引入相关噪声流匹配提升训练效率，采用可学习混合层注意力和System 2阶段跟踪解决歧义，使用多样本流匹配减少方差，推理时采用动作压缩和挑战特定校正规则。

Result: 在BEHAVIOR挑战赛中获得第一名，在50个任务上实现了26%的q-score，在公开和私有排行榜上均表现优异。

Conclusion: 提出的相关噪声流匹配和系统架构创新有效提升了智能体在复杂长视野任务中的表现，为机器人控制和具身智能提供了有价值的解决方案。

Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.

</details>


### [394] [DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management](https://arxiv.org/abs/2512.07312)
*Zhongchun Zhou,Chengtao Lai,Yuhang Gu,Wei Zhang*

Main category: cs.AR

Relevance: 45.0

TL;DR: 本文提出了一种针对大语言模型AI加速器的共享系统级缓存架构，通过应用感知的管理策略（包括死块预测、旁路决策和缓存颠簸缓解机制）来提升性能，相比传统缓存架构可实现最高1.80倍的加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI加速器设计变得越来越复杂，特别是深度层次化的暂存器内存（SPMs）及其异步管理增加了软件开发难度。本文探索相反的设计方向：采用共享系统级缓存和简单编程模型的多核AI加速器。

Method: 提出多核AI加速器架构，配备共享系统级缓存和应用感知管理策略。利用软件栈中的数据流信息指导缓存替换（包括死块预测），结合旁路决策和缓解缓存颠簸的机制。使用周期精确模拟器评估，并建立考虑实际重叠行为的分析模型。

Result: 相比传统缓存架构，实现了最高1.80倍的性能提升。旁路和颠簸缓解策略能有效处理有无核间数据共享的场景。RTL实现面积为0.064mm²（15nm工艺），时钟频率可达2GHz。

Conclusion: 共享缓存设计具有潜力辅助未来AI加速器系统开发，在保持编程简单性的同时提供显著性能优势。

Abstract: The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
  Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

</details>


### [395] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

Relevance: 35.0

TL;DR: 使用BioBERT分析临床文本的透明可解释机器学习方法，用于自闭症谱系障碍诊断，通过混合数据集训练获得最佳性能（97%敏感性，98%特异性），优于黑盒方法。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍诊断过程漫长且需求增加，现有机器学习方法多为黑盒且通常在单一数据集上训练，缺乏可解释性和泛化性。需要开发透明、可解释且能跨数据集泛化的诊断工具。

Method: 使用BioBERT语言模型分析非结构化临床文本，训练模型标注行为描述并映射到诊断标准，然后分配最终标签（ASD或非ASD）。评估了两种迁移学习策略：顺序训练和混合数据集训练，并与黑盒方法进行比较。

Result: 透明模型表现稳健，混合数据训练策略获得最佳结果（97%敏感性，98%特异性）。顺序训练导致性能略有下降，黑盒模型表现较差（90%敏感性，96%特异性）。透明方法整体优于黑盒方法。

Conclusion: 透明可解释方法在自闭症诊断中优于黑盒方法，混合数据集训练应为首选策略。这项工作为神经发育诊断中更可信、可泛化且临床可操作的AI工具奠定了基础。

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [396] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

Relevance: 35.0

TL;DR: PROBE：一个新的知识图谱补全评估框架，关注预测锐度（predictive sharpness）和流行度偏差鲁棒性（popularity-bias robustness）两个关键维度，通过秩转换器和秩聚合器提供更全面的模型评估。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全（KGC）评估指标存在两个主要缺陷：1）忽略预测锐度——对单个预测的严格程度评估不足；2）忽略流行度偏差鲁棒性——难以准确评估模型对低流行度实体的预测能力。这导致现有评估结果可能高估或低估KGC模型的真实性能。

Method: 提出PROBE评估框架，包含两个核心组件：1）秩转换器（RT）——根据所需的预测锐度水平估计每个预测的得分；2）秩聚合器（RA）——以流行度感知的方式聚合所有得分。该框架能够同时考虑预测的严格性和对低流行度实体的预测能力。

Result: 在真实世界知识图谱上的实验表明，现有评估指标倾向于高估或低估KGC模型的准确性，而PROBE能够提供对KGC模型更全面的理解和更可靠的评估结果。

Conclusion: PROBE框架通过同时考虑预测锐度和流行度偏差鲁棒性，为知识图谱补全评估提供了更全面、更可靠的评估方法，有助于更准确地理解不同KGC模型的性能特点。

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [397] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出PICKT模型，通过知识图谱处理多种输入数据格式，解决知识追踪中的冷启动问题，提升智能导学系统的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型存在输入数据格式受限、新学生/新问题冷启动问题、实际服务环境稳定性不足等局限，需要开发更实用的知识追踪模型来支持个性化学习需求。

Method: 提出PICKT模型，利用问题和概念文本信息构建知识图谱来结构化概念间关系，有效处理多种类型输入数据，特别针对冷启动场景进行优化。

Result: 实验显示模型在真实操作环境中表现优异，在新学生注册和新问题添加两个核心冷启动挑战上显著优于现有模型，验证了模型的稳定性和实用性。

Conclusion: PICKT模型为下一代智能导学系统的实际部署提供了重要的理论和技术基础，解决了现有知识追踪模型的多个关键局限。

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [398] [Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model](https://arxiv.org/abs/2512.07232)
*Wenlong Liu,Jiahua Pan,Xingyu Zhang,Xinxin Gong,Yang Ye,Xujin Zhao,Xin Wang,Kent Wu,Hua Xiang,Houmin Yan,Qingpeng Zhang*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出RAEA框架用于实体对齐任务，通过两阶段过滤（粗过滤+细过滤）匹配eBay和亚马逊产品，特别关注属性三元组和关系三元组的交互作用。


<details>
  <summary>Details</summary>
Motivation: 现有实体对齐方法未能充分利用属性三元组和关系三元组，特别是它们之间的交互作用。产品匹配问题可以通过构建知识图谱转化为实体对齐任务，但需要更好的方法来同时利用属性和关系信息。

Method: 采用两阶段管道：粗过滤和细过滤。细过滤使用RAEA框架，包含属性感知实体编码器和关系感知图注意力网络，通过聚合属性和关系的对齐信号来学习实体表示。

Result: 在跨语言数据集DBP15K上，RAEA模型相比12个基线方法平均Hits@1提升6.59%；在单语言数据集DWY100K上也取得了有竞争力的结果。

Conclusion: RAEA框架通过有效利用属性三元组和关系三元组的交互作用，显著提升了实体对齐性能，为产品匹配等应用提供了有效解决方案。

Abstract: Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).

</details>


### [399] [Accelerating Materials Discovery: Learning a Universal Representation of Chemical Processes for Cross-Domain Property Prediction](https://arxiv.org/abs/2512.05979)
*Mikhail Tsitsvero,Atsuyuki Nakao,Hisaki Ikebata*

Main category: physics.chem-ph

Relevance: 35.0

TL;DR: 提出一种通用的有向树过程图表示法，将非结构化文本、分子结构和数值测量统一为机器可读格式，并开发多模态图神经网络进行学习，在化学过程预测任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 化学过程实验验证缓慢且成本高昂，限制了材料发现。现有专利和文献数据异构且难以利用，需要统一表示方法来加速机器学习在化学过程预测中的应用。

Method: 1) 提出通用有向树过程图表示法，统一文本、分子结构和数值数据；2) 开发多模态图神经网络，采用属性条件注意力机制；3) 在约70万过程图（来自9000份文档）上预训练；4) 在特定领域小数据集上微调。

Result: 模型学习到语义丰富的嵌入表示，能够跨领域泛化。预训练模型在特定预测任务上微调后表现优异，证明大规模学习的通用过程表示能有效迁移到专业任务。

Conclusion: 通用过程图表示和多模态图神经网络能够有效利用异构化学数据，通过大规模预训练和少量数据微调，显著提升化学过程预测能力，加速材料发现。

Abstract: Experimental validation of chemical processes is slow and costly, limiting exploration in materials discovery. Machine learning can prioritize promising candidates, but existing data in patents and literature is heterogeneous and difficult to use. We introduce a universal directed-tree process-graph representation that unifies unstructured text, molecular structures, and numeric measurements into a single machine-readable format. To learn from this structured data, we developed a multi-modal graph neural network with a property-conditioned attention mechanism. Trained on approximately 700,000 process graphs from nearly 9,000 diverse documents, our model learns semantically rich embeddings that generalize across domains. When fine-tuned on compact, domain-specific datasets, the pretrained model achieves strong performance, demonstrating that universal process representations learned at scale transfer effectively to specialized prediction tasks with minimal additional data.

</details>


### [400] [POrTAL: Plan-Orchestrated Tree Assembly for Lookahead](https://arxiv.org/abs/2512.06002)
*Evan Conway,David Porfirio,David Chan,Mark Roberts,Laura M. Hiatt*

Main category: cs.RO

Relevance: 35.0

TL;DR: POrTAL：一种结合FF-Replan和POMCP优势的新型轻量级概率规划算法，用于机器人在部分可观测环境中的任务规划


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人交互中，机器人通常面临部分可观测环境，需要在不完整信息下进行规划。现有概率规划算法要么计算效率低（受限于机器人有限的计算资源），要么需要过多步骤才能达成目标。

Method: 提出POrTAL算法，结合了FF-Replan和POMCP两种基线规划算法的优势。这是一种轻量级的概率规划方法，专门设计用于部分可观测环境中的机器人任务规划。

Result: 在系列案例研究中，POrTAL能够快速找到解决方案，在步骤数量上优于FF-Replan和POMCP基线算法。同时展示了POrTAL在不同时间约束下的性能表现。

Conclusion: POrTAL是一种有效的轻量级概率规划算法，能够在部分可观测环境中为机器人提供高效的任务规划解决方案，在计算效率和规划质量之间取得了良好平衡。

Abstract: Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.

</details>


### [401] [Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining](https://arxiv.org/abs/2512.06018)
*Jiameng Wei,Dinh Dang,Kaixun Yang,Emily Stokes,Amna Mazeh,Angelina Lim,David Wei Dai,Joel Moore,Yizhou Fan,Danijela Gasevic,Dragan Gasevic,Guanliang Chen*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该研究应用学习分析技术，分析药学学生在GenAI虚拟病人训练中的临床沟通能力发展模式，发现高表现者采用战略性的信息识别行为，而低表现者陷入常规问答循环。


<details>
  <summary>Details</summary>
Motivation: 传统药物史采集评估依赖人工观察，难以规模化且缺乏详细性能数据。GenAI平台虽能收集大量数据，但学习分析在药学临床训练中的应用仍不足。研究旨在填补这一空白，分析学生在GenAI虚拟病人训练中如何发展临床沟通能力。

Method: 分析323名澳大利亚和马来西亚学生的交互日志（50,871条编码话语，1,487个学生-GenAI对话）。结合认知网络分析（ENA）建模问题共现和序列模式挖掘（SPM）捕捉时间序列，识别不同表现水平学生的询问模式。

Result: 高表现者展示战略性的信息识别行为，以识别临床相关信息为中心，整合关系建立和结构组织；低表现者则停留在常规问题验证循环中。母语背景、药学工作经验和机构背景等人口因素也塑造了不同的询问模式。

Conclusion: 研究揭示了GenAI辅助环境中可能指示临床推理发展的询问模式，为卫生专业教育评估提供方法学见解，并为支持多样化学习路径的自适应GenAI系统设计提供信息。

Abstract: Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.

</details>


### [402] [Physics-Guided Deepfake Detection for Voice Authentication Systems](https://arxiv.org/abs/2512.06040)
*Alireza Mohammadi,Keshav Sood,Dhananjay Thiruvady,Asef Nazari*

Main category: cs.SD

Relevance: 35.0

TL;DR: 提出一个结合物理引导的深度伪造检测与不确定性感知的边缘学习框架，用于保护网络边缘语音认证系统免受深度伪造合成攻击和联邦学习控制平面中毒威胁。


<details>
  <summary>Details</summary>
Motivation: 网络边缘部署的语音认证系统面临双重威胁：1）复杂的深度伪造合成攻击；2）分布式联邦学习协议中的控制平面中毒。现有方法通常单独处理这些问题，缺乏统一的防御框架。

Method: 框架融合了物理引导特征（建模声道动力学）和自监督学习表示，通过多模态集成架构处理，最后使用贝叶斯集成提供不确定性估计。结合物理特征评估和音频样本的不确定性估计。

Result: 提出的框架能够同时对先进的深度伪造攻击和复杂的控制平面中毒保持鲁棒性，解决了网络语音认证的完整威胁模型。

Conclusion: 通过耦合物理引导的深度伪造检测与不确定性感知的边缘学习，可以构建更安全、更可靠的网络边缘语音认证系统。

Abstract: Voice authentication systems deployed at the network edge face dual threats: a) sophisticated deepfake synthesis attacks and b) control-plane poisoning in distributed federated learning protocols. We present a framework coupling physics-guided deepfake detection with uncertainty-aware in edge learning. The framework fuses interpretable physics features modeling vocal tract dynamics with representations coming from a self-supervised learning module. The representations are then processed via a Multi-Modal Ensemble Architecture, followed by a Bayesian ensemble providing uncertainty estimates. Incorporating physics-based characteristics evaluations and uncertainty estimates of audio samples allows our proposed framework to remain robust to both advanced deepfake attacks and sophisticated control-plane poisoning, addressing the complete threat model for networked voice authentication.

</details>


### [403] [Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework](https://arxiv.org/abs/2512.06046)
*Ramprasath Ganesaraja,Swathika N,Saravanan AP,Kamalkumar Rathinasamy,Chetana Amancharla,Rahul Das,Sahil Dilip Panse,Aditya Batwe,Dileep Vijayan,Veena Ashok,Thanushree A P,Kausthubh J Rao,Alden Olivero,Roshan,Rajeshwar Reddy Manthena,Asmitha Yuga Sre A,Harsh Tripathi,Suganya Selvaraj,Vito Chin,Kasthuri Rangan Bhaskar,Kasthuri Rangan Bhaskar,Venkatraman R,Sajit Vijayakumar*

Main category: cs.SE

Relevance: 35.0

TL;DR: AI4UI是一个面向企业级应用交付的自主前端开发框架，专注于生产就绪的UI代码生成，通过设计阶段嵌入AI友好语法和后处理阶段专家优化，实现安全、可扩展、合规的UI开发。


<details>
  <summary>Details</summary>
Motivation: 当前通用代码助手主要面向快速原型设计，缺乏企业级应用所需的生产就绪特性（安全性、可扩展性、合规性、可维护性）。企业需要能够无缝集成到现有工作流程中，并能生成生产就绪UI代码的解决方案。

Method: 1) Figma原型中嵌入AI友好语法编码需求；2) 领域感知知识图谱；3) 安全抽象/包代码集成策略；4) 专家驱动的架构模板；5) 由专门代理角色协调的变更导向工作流程；6) 设计阶段和后处理阶段采用人在回路，中间阶段完全自主运行。

Result: 在大规模基准测试中：平台兼容性97.24%，编译成功率87.10%，安全合规性86.98%，功能实现成功率78.00%，代码审查质量73.50%，UI/UX一致性73.36%。在200名专家评估者的盲选偏好研究中，AI4UI成为领先解决方案之一。异步运行下，数周内生成数千个已验证UI界面。

Conclusion: AI4UI成功构建了一个企业级自主前端开发框架，通过人在回路的设计和完全自主的中间阶段，实现了生产就绪UI代码的高效生成，显著压缩了交付时间线。

Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline

</details>


### [404] [The Road of Adaptive AI for Precision in Cybersecurity](https://arxiv.org/abs/2512.06048)
*Sahil Garg*

Main category: cs.CR

Relevance: 35.0

TL;DR: 论文分享了在网络安全领域设计、构建和运营生产级GenAI管道的实践经验，重点关注持续适应机制，以应对不断变化的知识库、工具和威胁。


<details>
  <summary>Details</summary>
Motivation: 网络安全日益复杂，为AI研究和实践带来独特挑战和机遇。作者旨在为AI从业者和行业利益相关者提供在网络安全领域应用GenAI的实用视角，特别是如何通过不同适应机制在端到端系统中相互补充。

Method: 基于真实世界部署的经验，提出检索级和模型级适应的最佳实践，分享设计和运营生产级GenAI管道的实际教训和见解。

Result: 提供从实际部署中得出的实用指导，包括如何有效结合检索增强和模型微调等适应机制，以保持系统对网络安全领域快速变化的响应能力。

Conclusion: GenAI在网络安全防御中需要持续适应机制，论文提出了使GenAI更鲁棒、精确和可审计的研究方向，强调了不同适应机制在端到端系统中的互补作用。

Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.

</details>


### [405] [DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification](https://arxiv.org/abs/2512.06172)
*Sheng Liu,Panos Papadimitratos*

Main category: cs.CR

Relevance: 35.0

TL;DR: DEFEND：针对联邦学习中目标标签翻转攻击的防御框架，通过神经元幅度分析和GMM聚类检测中毒模型，排除恶意客户端，在智能交通系统道路条件分类任务中实现与无攻击场景相同的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在智能交通系统（ITS）中应用广泛，特别是道路条件分类任务。然而，联邦学习系统容易受到目标标签翻转攻击（TLFA）的威胁，攻击者通过毒化训练数据误导模型预测，威胁交通安全。现有防御措施无法在TLFA下维持接近无攻击场景的性能水平，因为它们缺乏针对TLFA的特定模型行为检测，并且在检测后忽略了客户端排除。

Method: 提出DEFEND防御框架：1）采用神经元幅度分析进行攻击目标识别，通过分析模型神经元激活模式检测中毒行为；2）使用高斯混合模型（GMM）聚类对模型进行分类；3）在每轮训练中丢弃中毒模型的贡献；4）自适应调整客户端评分，最终排除恶意客户端。

Result: 在多种联邦学习道路条件分类模型和任务上的广泛评估表明，DEFEND能够有效抵御TLFA，优于7个基线防御方法，性能提升至少15.78%。DEFEND在攻击下的性能表现与无攻击场景相同。

Conclusion: DEFEND填补了联邦学习中针对目标标签翻转攻击防御的研究空白，通过专门的模型行为检测和客户端排除机制，在智能交通系统应用中实现了强大的安全防护，确保了联邦学习系统的可靠性和交通安全。

Abstract: Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.

</details>


### [406] [Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses](https://arxiv.org/abs/2512.06390)
*Mehrab Hosain,Sabbir Alom Shuvo,Matthew Ogbe,Md Shah Jalal Mazumder,Yead Rahman,Md Azizul Hakim,Anukul Pandey*

Main category: cs.CR

Relevance: 35.0

TL;DR: 该论文综述了边缘计算中AI增强的网络安全防御技术，包括WAF、DDoS防护、机器人管理和API安全，提出了系统化评估方法和研究议程。


<details>
  <summary>Details</summary>
Motivation: 现代网络应用面临持续进化的自动化AI攻击，边缘计算和CDN作为最接近用户的防御点，需要AI驱动的检测、限流和隔离技术来应对威胁。

Method: 采用系统化综述方法，包括威胁分类映射到边缘可观测信号、评估指标、部署手册和治理指南，分析边缘AI防御的四大领域。

Result: 边缘中心的AI显著改善了检测和缓解时间，减少了数据移动并增强了合规性，但也引入了模型滥用、投毒和治理等新风险。

Conclusion: 边缘AI防御在网络安全中具有重要价值，但需要进一步研究可解释AI、对抗鲁棒性和自主多智能体防御等方向。

Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.

</details>


### [407] [AI as "Co-founder": GenAI for Entrepreneurship](https://arxiv.org/abs/2512.06506)
*Junhui Jeff Cai,Xian Gu,Liugang Sheng,Mengjia Xia,Linda Zhao,Wu Zhu*

Main category: econ.GN

Relevance: 35.0

TL;DR: 研究ChatGPT发布如何影响创业，发现AI人力资本强的地区小型企业注册激增，大型企业减少，表明生成式AI促进竞争，尤其帮助小型企业和首次创业者。


<details>
  <summary>Details</summary>
Motivation: 研究生成式人工智能（如ChatGPT）是否、如何以及为谁促进企业创建，特别关注其作为降低创业成本的全球性冲击，以及AI特定人力资本的地区差异如何影响创业模式。

Method: 利用2022年11月ChatGPT发布作为准自然实验，采用地理编码网格分析，比较不同AI人力资本水平的地区。使用截至2024年底的中国企业注册高分辨率数据，分析企业规模、资本、股东数量等变化。

Result: AI人力资本强的地区新企业注册激增，完全由小型企业驱动，占全国企业进入的6.0%。大型企业进入减少。新企业资本、股东数量和创始团队规模更小，尤其在小型企业中。对潜在AI应用、融资需求较低的企业和首次创业者影响最大。

Conclusion: 生成式AI通过降低创业门槛，成为促进竞争的力量，特别有利于小型企业和首次创业者，推动更精简的创业模式，可能重塑企业生态。

Abstract: This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.

</details>


### [408] [Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization](https://arxiv.org/abs/2512.06699)
*Karthik Prabhakar*

Main category: cs.PF

Relevance: 35.0

TL;DR: 使用机器学习方法预测ML训练中的I/O性能并推荐最优存储配置，通过系统化基准测试收集数据，XGBoost模型达到最佳预测效果（R²=0.991），可将配置时间从数天缩短到几分钟。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习训练越来越受限于数据I/O而非计算，GPU利用率常低于50%等待数据加载。传统配置方法依赖试错，耗时数天且效率低下。

Method: 收集141个观测数据，涵盖不同存储后端（NVMe SSD、网络存储、内存文件系统）、数据格式和访问模式。评估7种回归模型和3种分类方法，使用XGBoost进行性能预测和配置推荐。

Result: XGBoost模型表现最佳，R²达到0.991，平均预测误差11.8%。特征重要性分析显示吞吐量指标和批大小是主要性能驱动因素。

Conclusion: 数据驱动方法可将ML训练存储配置时间从数天试错缩短到几分钟预测推荐，方法可复现并可扩展到其他ML系统资源管理问题。

Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project

</details>


### [409] [Formal that "Floats" High: Formal Verification of Floating Point Arithmetic](https://arxiv.org/abs/2512.06850)
*Hansa Mohanty,Vaisakh Naduvodi Viswambharan,Deepak Narayan Gadde*

Main category: cs.LO

Relevance: 35.0

TL;DR: 提出了一种可扩展的浮点算术验证方法，使用直接RTL到RTL模型检查，结合分治策略、反例引导优化和AI驱动的形式属性生成，相比传统方法提高了覆盖效率。


<details>
  <summary>Details</summary>
Motivation: 浮点算术的形式验证面临非线性算术行为和控制-数据路径紧密耦合的挑战。现有方法依赖C模型进行等价检查，存在抽象差距、翻译开销和RTL级可扩展性限制的问题。

Method: 采用直接RTL到RTL模型检查，使用分治策略将验证分解为模块化阶段，每个阶段通过辅助断言和引理捕获。结合反例引导优化定位实现缺陷，通过目标故障注入验证鲁棒性。扩展了基于智能代理AI的形式属性生成，集成LLM驱动自动化与人工在环优化。

Result: 直接RTL到RTL模型检查相比独立验证实现了更高的覆盖效率，需要更少的断言，特别是当结合通过人工在环优化的AI生成属性时。覆盖分析显示AI生成属性在两种验证设置中都有效。

Conclusion: 该方法为浮点算术验证提供了可扩展的解决方案，通过直接RTL级验证避免了抽象差距，结合AI自动化提高了验证效率和实用性。

Abstract: Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.

</details>


### [410] [ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design](https://arxiv.org/abs/2512.06854)
*Qijun Zhang,Yao Lu,Mengming Li,Shang Liu,Zhiyao Xie*

Main category: cs.AR

Relevance: 35.0

TL;DR: ArchPower：首个用于架构级处理器功耗建模的开源数据集，包含200个CPU数据样本，覆盖25种CPU配置和8种工作负载，提供超过100个架构特征和细粒度功耗标签。


<details>
  <summary>Details</summary>
Motivation: 现有CPU功耗评估方法存在严重缺陷：传统IC实现流程耗时数月，早期架构级功耗模型不准确，ML方法缺乏开源数据集，且现有数据集难以反映真实CPU设计场景。

Method: 通过复杂且真实的设计流程收集CPU架构信息作为特征，使用模拟功耗作为标签。数据集包含200个CPU数据样本，来自25种CPU配置执行8种工作负载，每个样本包含100多个硬件和事件参数特征。

Result: 创建了首个开源架构级处理器功耗建模数据集ArchPower，提供细粒度功耗信息：总设计功耗和11个组件功耗，每个功耗值进一步分解为组合逻辑、时序逻辑、存储器和时钟功耗四个细粒度组。

Conclusion: ArchPower填补了CPU功耗建模领域开源数据集的空白，为ML-based架构级功耗模型开发提供了重要资源，有望推动处理器设计早期阶段的准确功耗评估。

Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.

</details>


### [411] [Benchmarking Deep Neural Networks for Modern Recommendation Systems](https://arxiv.org/abs/2512.07000)
*Abderaouf Bahi,Ibtissem Gasmi*

Main category: cs.IR

Relevance: 35.0

TL;DR: 该研究评估了七种神经网络架构（CNN、RNN、GNN、自编码器、Transformer、NCF和孪生网络）在三个推荐系统数据集上的表现，发现GNN擅长处理电商复杂关系，RNN适合时序动态，孪生网络能提升推荐多样性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同神经网络架构在推荐系统中的表现，了解各自优势与局限，为开发更有效的混合推荐方法提供指导，以满足用户偏好和现代数字平台的需求。

Method: 在三个数据集（零售电商、亚马逊产品、Netflix Prize）上部署七种神经网络架构，使用准确率、召回率、F1分数和多样性等指标进行系统评估，分析各模型在不同场景下的表现。

Result: 结果显示：GNN在电商环境中处理复杂物品关系表现最佳；RNN在Netflix等需要时序动态捕捉的平台效果显著；孪生网络在零售环境中能有效提升推荐多样性。但所有模型都存在计算需求高、数据依赖性强、准确性与多样性平衡困难等问题。

Conclusion: 研究建议采用混合方法，结合不同模型的优势来构建更有效的推荐系统。虽然Transformer是LLM核心架构，但本文主要关注推荐系统应用而非LLM训练本身。

Abstract: This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.

</details>


### [412] [JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention](https://arxiv.org/abs/2512.07168)
*Georgios Ioannides,Christos Constantinou,Aman Chadha,Aaron Elkins,Linsey Pang,Ravid Shwartz-Ziv,Yann LeCun*

Main category: cs.SD

Relevance: 35.0

TL;DR: 提出两阶段自监督框架，结合JEPA和DAAM学习鲁棒语音表示，第一阶段通过掩码预测学习语义特征，第二阶段使用FSQ量化生成语言模型友好的压缩表示


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器在压缩效率和语言模型友好性方面存在局限，需要更高效、可逆且适合语言模型处理的语音表示方法

Method: 两阶段框架：1) JEPA+DAAM进行语义特征学习，使用高斯混合密度自适应门控；2) FSQ量化和混合基数打包，HiFi-GAN解码器重建波形

Result: 生成47.5 tokens/sec的压缩表示，在2.5Hz帧率下发现层次化语音结构，比现有神经音频编解码器更高效且具有竞争力

Conclusion: 该方法提供了可逆、高度压缩、语言模型友好的语音表示，为语音处理任务提供了新的高效表示学习框架

Abstract: We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.

</details>


### [413] [SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks](https://arxiv.org/abs/2512.07266)
*Florian Tretter,Daniel Flögel,Alexandru Vasilache,Max Grobbel,Jürgen Becker,Sören Hohmann*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出一种混合神经形态DRL方法，结合SNN（执行器）和ANN（评价器），用于机器人社会导航，显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 将自主移动机器人集成到人类环境需要类人决策和节能的事件驱动计算。目前神经形态方法很少应用于DRL导航，因为训练不稳定。

Method: 混合社会集成DRL actor-critic方法：执行器使用SNN，评价器使用ANN，加上神经形态特征提取器捕捉时间人群动态和人机交互

Result: 提高了社会导航性能，并将估计能耗降低了约1.69个数量级（约98%的能耗减少）

Conclusion: 神经形态DRL方法能有效解决机器人社会导航问题，实现高性能和超低能耗

Abstract: Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.

</details>


### [414] [Exact Synthetic Populations for Scalable Societal and Market Modeling](https://arxiv.org/abs/2512.07306)
*Thierry Petit,Arnault Pachot*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出一个基于约束规划的合成人口生成框架，能够精确复现目标统计特征并保证个体一致性，无需微观数据即可控制人口统计特征。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要从样本推断分布，而本方法旨在直接编码聚合统计和结构关系，实现对人口统计特征的精确控制，同时避免使用个人数据。

Method: 采用约束规划框架，将目标统计特征作为约束条件，生成合成人口数据，确保个体层面的一致性和统计特征的精确匹配。

Result: 在官方人口统计源上验证了方法的有效性，研究了分布偏差对下游分析的影响，并展示了通过大语言模型查询合成人口进行社会行为建模的能力。

Conclusion: 该方法为无需个人数据的决策级洞察提供了可重复的解决方案，支持市场政策场景探索和社会行为建模。

Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.

</details>


### [415] [Social welfare optimisation in well-mixed and structured populations](https://arxiv.org/abs/2512.07453)
*Van An Nguyen,Vuong Khang Huynh,Ho Nam Duong,Huu Loi Bui,Hai Anh Ha,Quang Dung Le,Le Quoc Dung Ngo,Tan Dat Nguyen,Ngoc Ngu Nguyen,Hoai Thuong Nguyen,Zhao Song,Le Hong Trang,The Anh Han*

Main category: physics.soc-ph

Relevance: 35.0

TL;DR: 该研究挑战了传统多智能体系统中以最小激励成本实现最大合作频率的双目标优化范式，提出应转向以社会福利最大化为核心的单目标优化，并发现追求社会福利最大化所需的激励成本显著高于单纯追求成本效率或合作频率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注双目标优化问题：最小化激励成本的同时最大化合作频率。然而，在这种约束下社会福利的最优值尚未被充分探索。作者假设在驱动智能体达到期望合作状态所需的最小激励成本下，可能无法实现最大的社会福利。

Method: 采用单目标优化方法专注于最大化社会福利，基于进化博弈论的基础模型，在有限种群（包括混合良好和结构化种群）中分析成本效率。结合分析模型和基于智能体的模拟，研究不同干预策略（奖励局部行为模式vs全局行为模式）如何影响社会福利和合作动态。

Result: 研究结果显示，在追求纯成本效率或合作频率与追求最大社会福利之间，存在显著的单智能体激励成本差距。不同的干预策略（局部奖励vs全局奖励）对社会福利和合作动态产生不同影响。

Conclusion: 多智能体系统和人类社会的激励设计、政策和基准测试应优先考虑以福利为中心的目标，而不是成本或合作频率等代理目标。社会福利最大化需要不同于传统成本效率优化的激励策略。

Abstract: Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.

</details>


### [416] [From Real-World Traffic Data to Relevant Critical Scenarios](https://arxiv.org/abs/2512.07482)
*Florian Lüttner,Nicole Neis,Daniel Stadler,Robin Moss,Mirjam Fehling-Kaschek,Matthias Pfriem,Alexander Stolz,Jens Ziehn*

Main category: cs.RO

Relevance: 35.0

TL;DR: 该论文提出了一种基于真实高速公路数据的车道变换场景分析方法，通过关键性度量评估驾驶场景，并生成合成关键场景以识别安全相关场景。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要在各种相关场景下可靠运行，但识别完整的安全相关场景集具有挑战性，特别是"未知不安全"场景。需要从简单领域（如高速公路）开始，逐步扩展到复杂环境，以提高验证效率。

Method: 1. 采集和处理高速公路真实交通数据；2. 在轨迹数据上应用关键性度量评估场景；3. 将计算结果与特定车道变换场景和采集条件关联；4. 基于记录场景生成合成场景以扩展"未知不安全"场景范围。

Result: 开发了一个完整的处理链，能够：1. 识别安全相关驾驶场景；2. 开发数据驱动方法提取这些场景；3. 通过采样生成合成关键场景。在AVEAS项目中进行了实际应用和评估。

Conclusion: 该方法为自动驾驶系统验证提供了系统化的场景识别和生成框架，特别适用于车道变换等复杂场景，有助于提高验证效率并覆盖更多安全相关场景。

Abstract: The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly "unknown unsafe" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of "unknown unsafe" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.

</details>


### [417] [Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility](https://arxiv.org/abs/2512.07487)
*David M. Allison,Stephen Herzog*

Main category: cs.CY

Relevance: 35.0

TL;DR: 论文分析了人工智能如何通过加速核扩散技术（PETs）发展，同时挑战传统核探测技术（DETs），从而改变核不扩散格局，提出了量化相对优势指数（RAI）的模型来评估核扩散风险。


<details>
  <summary>Details</summary>
Motivation: 当前新兴颠覆性技术正在重塑核风险格局，特别是人工智能加速了核扩散技术的发展，同时挑战了传统的核监测与核查方法，这为决策者带来了新的战略挑战。

Method: 开发了一个基于相对优势指数（RAI）的形式化模型，量化PETs和DETs之间的平衡变化，通过可复现的情景模拟分析不同PET增长率、DET投资策略对累积核扩散风险的影响。

Result: 研究发现AI驱动的PETs呈现逻辑增长，而DETs呈现阶梯式改进，这种不对称技术进步扩大了核扩散可探测性的不确定性带，可能导致探测不再足够，需要更广泛的PET治理。

Conclusion: 政府与国际组织需要投资于能够跟上技术发展步伐的敏捷政策与工具，应对AI加速核扩散技术发展带来的战略挑战。

Abstract: A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.

</details>


### [418] [Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization](https://arxiv.org/abs/2512.07627)
*Maximos Kaliakatsos-Papakostas,Konstantinos Soiledis,Theodoros Tsamis,Dimos Makris,Vassilis Katsouros,Emilios Cambouropoulos*

Main category: cs.SD

Relevance: 35.0

TL;DR: 该论文提出了B*算法，结合束搜索、A*算法和回溯技术，强制预训练Transformer模型在旋律和声中满足预定义的和弦约束。


<details>
  <summary>Details</summary>
Motivation: 研究如何在Transformer生成的符号音乐中融入用户偏好，特别是解决旋律和声中预定义和弦约束的集成问题。当前Transformer在音乐生成方面有优势，但如何精确控制生成内容以满足特定约束（如特定位置的和弦）仍需探索。

Method: 提出B*算法，结合束搜索和A*算法的特点，加入回溯机制。算法强制预训练Transformer模型在生成和声时满足给定的和弦约束，确保和弦出现在正确的起始位置和正确的小节中。算法虽然在最坏情况下有指数复杂度，但为启发式方法的集成提供了框架。

Result: 论文提出了首个解决该问题的算法框架，能够强制Transformer模型满足和弦约束。虽然算法复杂度高，但为后续优化和启发式方法的集成奠定了基础。

Conclusion: B*算法是解决Transformer音乐生成中预定义和弦约束问题的初步尝试，揭示了该问题的复杂性，并为未来改进提供了框架，特别是通过启发式方法优化性能。

Abstract: Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.

</details>


### [419] [The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic](https://arxiv.org/abs/2512.07724)
*Zhengzheng Tang*

Main category: cs.ET

Relevance: 35.0

TL;DR: 提出一种原生脉冲微架构，将金属有机框架的随机离子通道转化为确定性浮点计算，实现FP8精度100%对齐PyTorch，线性层延迟降至O(log N)，速度提升17倍。


<details>
  <summary>Details</summary>
Motivation: 金属有机框架的埃级通道具有天然积分-发放动力学特性，是后硅基材质的理想选择。但这些随机模拟材料与确定性AI工作负载（如FP8）之间存在矛盾，现有神经形态方法无法满足Transformer精度要求。

Method: 提出原生脉冲微架构，将噪声神经元视为逻辑原语，引入空间组合流水线和粘性额外校正机制，将随机离子行为转化为确定性浮点计算。

Result: 在所有16,129个FP8对上验证了100%比特精确对齐PyTorch，线性层延迟降至O(log N)，速度提升17倍，物理模拟显示对极端膜泄漏（β≈0.01）具有鲁棒性。

Conclusion: 该架构成功弥合了随机离子通道与确定性AI计算之间的鸿沟，为后硅时代硬件提供了可行的神经形态计算方案，同时满足精度和效率要求。

Abstract: The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap "from stochastic ions to deterministic floats," we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.

</details>


### [420] [Optimizing video analytics inference pipelines: a case study](https://arxiv.org/abs/2512.07009)
*Saeid Ghafouri,Yuming Ding,Katerine Diaz Chito,Jesús Martinez del Rincón,Niamh O'Connell,Hans Vandierendonck*

Main category: cs.DC

Relevance: 30.0

TL;DR: 本文介绍了一个针对家禽福利监控系统的优化案例研究，通过系统级改进（包括多级并行化、GPU加速、向量化聚类和内存高效后处理）实现了2倍加速，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 精准畜牧业监控需要高分辨率视频和近实时监控，产生大量计算负载。商业农场需要成本效益高且可扩展的视频分析解决方案，以降低基础设施需求。

Method: 采用系统级优化方法，包括：1）多级并行化；2）用GPU加速代码替代CPU代码；3）向量化聚类算法；4）内存高效后处理。这些优化应用于检测、跟踪、聚类和行为分析模块。

Result: 在真实农场视频数据上评估，优化后的系统在整个处理流程中实现了高达2倍的加速，同时没有降低模型准确性。

Conclusion: 研究展示了构建高吞吐量、低延迟视频推理系统的实用策略，可减少农业和智能感知部署中的基础设施需求，也适用于其他大规模视频分析应用。

Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.

</details>


### [421] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

Relevance: 25.0

TL;DR: M-STAR是一个多尺度时空自回归框架，用于生成长期人类移动轨迹，通过粗到细的时空预测过程，结合多尺度时空标记器和Transformer解码器，在保真度和生成速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类移动建模对交通规划和流行病建模等应用至关重要。现有基于自回归和扩散模型的方法在生成单日轨迹方面有潜力，但在长期轨迹生成（如周轨迹）方面效率低下，且缺乏显式的时空多尺度建模。

Method: 提出M-STAR框架：1）多尺度时空标记器编码层次化移动模式；2）基于Transformer的解码器进行下一尺度自回归预测；3）通过粗到细的时空预测过程生成长期轨迹。

Result: 在两个真实世界数据集上的实验表明，M-STAR在保真度方面优于现有方法，并显著提高了生成速度。

Conclusion: M-STAR通过多尺度时空建模有效解决了长期轨迹生成的效率和保真度问题，为人类移动建模提供了新框架。

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [422] [A Multi-objective Optimization Approach for Feature Selection in Gentelligent Systems](https://arxiv.org/abs/2512.05971)
*Mohammadhossein Ghahramani,Yan Qiao,NaiQi Wu,Mengchu Zhou*

Main category: cs.NE

Relevance: 25.0

TL;DR: 提出基于优势多目标进化算法的混合框架，用于制造业故障检测，同时优化特征选择和分类性能


<details>
  <summary>Details</summary>
Motivation: 将AI技术融入制造业以开发智能系统，通过可靠故障检测提高产品质量、产量并降低成本

Method: 提出混合框架，采用基于优势的多目标进化算法，在单次运行中探索帕累托最优解，同时优化特征选择和分类性能

Result: 在两个真实工业数据集上验证了方法的泛化性和有效性

Conclusion: 提出的方法能帮助制造商监控制造操作，适应新兴趋势，实现多目标优化

Abstract: The integration of advanced technologies, such as Artificial Intelligence (AI), into manufacturing processes is attracting significant attention, paving the way for the development of intelligent systems that enhance efficiency and automation. This paper uses the term "Gentelligent system" to refer to systems that incorporate inherent component information (akin to genes in bioinformatics-where manufacturing operations are likened to chromosomes in this study) and automated mechanisms. By implementing reliable fault detection methods, manufacturers can achieve several benefits, including improved product quality, increased yield, and reduced production costs. To support these objectives, we propose a hybrid framework with a dominance-based multi-objective evolutionary algorithm. This mechanism enables simultaneous optimization of feature selection and classification performance by exploring Pareto-optimal solutions in a single run. This solution helps monitor various manufacturing operations, addressing a range of conflicting objectives that need to be minimized together. Manufacturers can leverage such predictive methods and better adapt to emerging trends. To strengthen the validation of our model, we incorporate two real-world datasets from different industrial domains. The results on both datasets demonstrate the generalizability and effectiveness of our approach.

</details>


### [423] [Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation](https://arxiv.org/abs/2512.06304)
*Xining Song,Zhihua Wei,Rui Wang,Haixiao Hu,Yanxiang Chen,Meng Han*

Main category: eess.AS

Relevance: 25.0

TL;DR: 该论文综述了语音转换（VC）模型在输入操纵攻击下的鲁棒性问题，系统分类了现有攻击与防御方法，并从四个维度评估了退化输入语音对VC输出的影响。


<details>
  <summary>Details</summary>
Motivation: 语音转换技术在生成质量和个性化能力方面取得了快速进展，但现有模型仅从干净训练数据中学习非鲁棒特征，导致在真实场景中处理退化输入语音（如噪声、混响、对抗攻击等）时性能不佳。目前缺乏对VC模型在输入操纵下鲁棒性的全面理解。

Method: 1. 从输入操纵角度分类现有攻击与防御方法
2. 从四个维度评估退化输入语音对VC模型的影响：可懂度、自然度、音色相似度和主观感知
3. 分析不同形式输入退化攻击如何改变VC模型的预期输出

Result: 论文提供了对VC系统鲁棒性的系统分析框架，识别了输入操纵攻击对语音转换质量的具体影响维度，为优化攻击和防御策略提供了基础。

Conclusion: 语音转换系统在真实世界部署中面临严重的鲁棒性挑战，需要更全面的安全评估框架。论文为未来研究指明了方向，包括开发更鲁棒的VC模型和有效的防御机制。

Abstract: Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.

</details>


### [424] [Instance Dependent Testing of Samplers using Interval Conditioning](https://arxiv.org/abs/2512.06458)
*Rishiraj Bhattacharyya,Sourav Chakraborty,Yash Pote,Uddalok Sarkar,Sayantan Sen*

Main category: cs.DS

Relevance: 25.0

TL;DR: 开发首个支持无限域（如自然数）的采样器验证测试器，采用实例依赖效率而非最坏情况效率，通过区间条件框架实现未知分布与已知分布的距离估计，实验显示比现有方法快1000倍。


<details>
  <summary>Details</summary>
Motivation: 当前采样器验证方法存在两个主要问题：1) 只关注最坏情况效率，不考虑实例依赖效率；2) 不支持无限域（如自然数）的采样器验证，而无限域在金融、天文、网络安全等领域很常见。需要开发能验证无限域采样器的测试器。

Method: 提出基于区间条件框架的距离估计算法，通过将未知分布与已知分布进行比较来验证采样器。核心技术创新是将概率质量估计与连续分布连接起来，实现实例依赖的效率评估。

Result: 实验结果显示，新方法比现有最先进的测试器（如Barbarik、Teq、Flash、CubeProbe）快达1000倍，能够有效验证自然数等无限域上的采样器。

Conclusion: 首次实现了支持无限域的采样器验证测试器，采用实例依赖效率而非最坏情况效率，通过创新的距离估计算法框架，在理论和实践上都取得了显著进展。

Abstract: Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.
  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.

</details>


### [425] [PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning](https://arxiv.org/abs/2512.06496)
*Stella Brown,Nicolas Preisig,Autumn Davis,Brian Hutchinson,Filip Jagodzinski*

Main category: q-bio.BM

Relevance: 25.0

TL;DR: PRIMRose：一种基于卷积神经网络预测蛋白质突变中每个残基能量变化的新方法，专注于双氨基酸插入/缺失的局部能量影响分析。


<details>
  <summary>Details</summary>
Motivation: 理解蛋白质突变如何影响蛋白质结构对计算生物学和生物信息学至关重要。现有模型主要评估全局能量变化，缺乏对局部残基层面能量影响的精细分析，特别是对于双氨基酸插入/缺失（InDels）的局部能量影响。

Method: 采用卷积神经网络架构预测蛋白质突变中每个残基的能量变化。在9种蛋白质构建的数据集上训练，数据集分为三组：一组包含详尽的双InDel突变，另外两组分别包含约145k和80k随机采样的双InDel突变。

Result: 模型在Rosetta分子建模套件计算的各种能量指标上实现了高预测准确性，揭示了影响模型性能的局部模式，如溶剂可及性和二级结构上下文。

Conclusion: 这种每残基分析方法为蛋白质特定区域的突变耐受性提供了新见解，提供了更高可解释性和生物学意义的InDels效应预测。

Abstract: Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.

</details>


### [426] [WisPaper: Your AI Scholar Search Engine](https://arxiv.org/abs/2512.06879)
*Li Ju,Jun Zhao,Mingxu Chai,Ziyu Shen,Xiangyang Wang,Yage Geng,Chunchun Ma,Hao Peng,Guangbin Li,Tao Li,Chengyong Liao,Fu Wang,Xiaolong Wang,Junshen Chen,Rui Gong,Shijia Liang,Feiyan Li,Ming Zhang,Kexin Tan,Jujie Ye,Zhiheng Xi,Shihan Dou,Tao Gui,Yuankai Ying,Yang Shi,Yue Zhang,Qi Zhang*

Main category: cs.IR

Relevance: 25.0

TL;DR: WisPaper是一个智能学术检索与文献管理平台，提供学者搜索、文献库和AI推荐三大功能，帮助研究者高效发现、组织和管理文献，实现从文献发现到持续追踪的闭环工作流。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物呈指数级增长，研究人员难以高效定位和管理相关文献。现有学术工具缺乏将文献发现、管理和前沿追踪无缝连接的闭环工作流。

Method: WisPaper平台包含三个核心模块：1) Scholar Search - 支持快速关键词搜索和深度智能代理搜索；2) Library - 可定制的知识库用于系统化文献组织；3) AI Feeds - 基于用户兴趣的智能推荐系统，自动推送相关新出版物。

Result: WisPaper是一个多语言、多学科的系统，显著减少了不同背景研究人员在文献筛选和管理上花费的时间，使他们能够专注于核心研究活动。平台已公开可用，服务于学术界和工业界的研究人员。

Conclusion: WisPaper提供了一个将文献发现、管理和持续追踪研究前沿无缝连接的闭环工作流，解决了研究人员在文献管理方面的核心痛点。

Abstract: Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.

</details>


### [427] [Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition](https://arxiv.org/abs/2512.07005)
*Zihao Wang,Ruibin Yuan,Ziqi Geng,Hengjia Li,Xingwei Qu,Xinyi Li,Songye Chen,Haoying Fu,Roger B. Dannenberg,Kejun Zhang*

Main category: cs.SD

Relevance: 25.0

TL;DR: 提出了MADVSD数据集，包含超过670小时的干声演唱录音，来自9个中国地区的4206名母语者，用于歌唱口音研究。


<details>
  <summary>Details</summary>
Motivation: 歌唱口音研究相对语音口音研究不足，主要因为缺乏合适的数据集。现有歌唱数据集通常存在细节丢失（源于人声-乐器分离过程）且缺乏区域口音标注。

Method: 构建MADVSD数据集，包含4206名母语者的干声录音，覆盖9个中国地区。参与者用当地方言录制三首流行歌曲，并录制覆盖所有普通话元音和一个完整八度音阶的语音练习。

Result: 通过歌唱口音识别的基准实验验证了MADVSD的有效性，展示了其在评估语音模型在歌唱场景中的实用性。探索了方言对歌唱口音的影响，并分析了元音在口音变化中的作用。

Conclusion: MADVSD填补了歌唱口音研究的数据空白，为评估语音模型在歌唱场景中的表现提供了基准，并支持方言对歌唱口音影响的分析研究。

Abstract: Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.

</details>


### [428] [A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data](https://arxiv.org/abs/2512.07030)
*Zahra Lotfi,Mostafa Lotfi*

Main category: cs.CR

Relevance: 25.0

TL;DR: 该研究评估了五种监督学习模型在检测零日网络攻击中的性能，通过网格搜索、降维和过采样方法处理数据不平衡问题，发现XGBoost在准确性和速度方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型主要基于已知攻击模式进行训练，对未知的零日攻击检测效果不佳。研究旨在评估不同监督模型在零日攻击检测中的性能，并解决数据不平衡问题以提高检测效果。

Method: 1) 评估五种监督模型（包括随机森林和XGBoost）在零日攻击检测中的性能；2) 提出包含网格搜索、降维和过采样方法的框架处理数据不平衡；3) 使用高度不平衡数据集，测试阶段仅暴露零日攻击；4) 比较过采样对模型指标（特别是准确率）的影响。

Result: 随机森林在过采样和非过采样条件下表现最好，但处理时间较长。XGBoost在检测零日攻击方面既快速又准确，因此被选为最佳模型。

Conclusion: XGBoost是检测零日攻击的最佳监督学习模型，在准确性和速度之间取得了良好平衡。提出的框架能有效处理数据不平衡问题，提高模型对未知攻击的检测能力。

Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.

</details>


### [429] [Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals](https://arxiv.org/abs/2512.07309)
*Guosheng Wang,Shen Wang,Lei Yang*

Main category: cs.IT

Relevance: 25.0

TL;DR: 提出RFRP自监督预训练框架，结合大定位模型和神经射频辐射场，利用大规模无标签RF数据学习位置相关表示，显著提升跨场景室内定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的室内定位模型依赖场景特定标签数据，面临跨场景泛化挑战。需要能利用大规模无标签RF数据的预训练方法。

Method: 提出RFRP框架，采用非对称自编码器架构，将大定位模型与神经射频辐射场耦合。LM编码RF频谱为位置相关潜在表示，RF-NeRF解码重建原始频谱。使用四种无线技术收集730万位置数据，75场景训练，25场景评估。

Result: RFRP预训练的LM相比无预训练模型减少40%以上定位误差，相比监督学习预训练减少21%误差。

Conclusion: RFRP框架通过自监督预训练有效学习位置相关表示，显著提升跨场景室内定位性能，为RF定位系统提供新解决方案。

Abstract: Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.

</details>


### [430] [R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation](https://arxiv.org/abs/2512.07576)
*Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Sharipov Hotam Beknazarovich,Farzona S. Ataeva,Qurbonaliev Alisher,Yuanjie Zheng*

Main category: eess.IV

Relevance: 25.0

TL;DR: R2MF-Net：一种用于多方向脊柱X光图像自动分割的循环残差多路径编码器-解码器网络，通过级联粗分割和细分割网络，结合改进的Inception风格特征提取器和多种跨阶段连接机制，提高脊柱结构分割的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 脊柱X光图像中脊柱结构的准确分割是定量脊柱侧弯评估（如Cobb角测量、椎体平移估计和曲率分类）的先决条件。临床实践中，医生需要获取冠状位、左屈和右屈X光片来联合评估畸形严重程度和脊柱灵活性。然而，分割步骤仍然高度依赖手动操作，耗时且不可重复，特别是在低对比度图像、存在肋骨阴影或组织重叠的情况下。

Method: 提出R2MF-Net，一种循环残差多路径编码器-解码器网络，专门用于多方向脊柱X光图像的自动分割。整体设计包括级联连接的粗分割网络和细分割网络。两个阶段都采用改进的Inception风格多分支特征提取器，同时在跳跃路径中插入循环残差跳跃连接（R2-Jump）模块，逐步对齐编码器和解码器语义。多尺度跨阶段跳跃（MC-Skip）机制允许细网络重用粗网络多个解码器层次的分层表示，从而增强跨成像方向和对比度条件下分割的稳定性。此外，在瓶颈处采用轻量级空间通道挤压-激励块（SCSE-Lite）来强调脊柱相关激活并抑制无关结构和背景噪声。

Result: 在包含228组冠状位、左屈和右屈脊柱X光图像（带有专家标注）的临床多视图X光数据集上评估了R2MF-Net。

Conclusion: R2MF-Net通过创新的网络架构设计，解决了脊柱X光图像分割中的手动、耗时和不可重复问题，特别是在低对比度和复杂背景条件下的挑战，为自动化脊柱侧弯评估提供了有效的技术方案。

Abstract: Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.

</details>


### [431] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

Relevance: 15.0

TL;DR: 该研究为埃及基纳市开发了一个基于Voronoi图的定制化城市规划模型，通过Python算法评估公共服务覆盖效率，发现平均覆盖率为81.3%，但空间分布不均。


<details>
  <summary>Details</summary>
Motivation: 埃及国家公共服务规划标准往往忽视地方特色，导致规划与实际需求脱节。研究旨在填补这一空白，为基纳市开发符合当地特点的规划模型。

Method: 采用混合方法（描述性、分析性和实验性），使用Python编程开发基于Voronoi图的智能空间分析算法，生成城市特定规划标准并评估现有公共服务设施覆盖情况。

Result: 模型应用显示平均服务覆盖率为81.3%。救护车站效率最高（99.8%），公园和开放空间覆盖最低（10%）。市中心服务密度高（>45个/平方公里），郊区显著降低（<5个/平方公里）。Hajer基纳区未服务区域最多，第一区服务覆盖最高。

Conclusion: 成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架，有助于改善公共服务分配的公平性和效率。

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [432] [ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling](https://arxiv.org/abs/2512.06595)
*Joe Shymanski*

Main category: cs.MA

Relevance: 15.0

TL;DR: ChargingBoul是一个在2022年ANAC竞赛中获得第二名的自动谈判代理，采用轻量级策略平衡让步和对手建模，通过对手分类、动态策略调整和后期让步政策来最大化效用。


<details>
  <summary>Details</summary>
Motivation: 自动谈判是多智能体系统中的重要研究领域，在电子商务、资源分配和自主决策中有广泛应用。需要开发有效的谈判代理来在各种对手策略下实现高谈判结果。

Method: 采用轻量级策略：1）基于出价模式对对手进行分类；2）动态调整出价策略；3）在谈判后期应用让步政策以最大化效用并促进协议达成。

Result: 在2022年ANAC竞赛中获得第二名（个体效用），与第一名差距极小。后续研究表明该代理在不同对手策略下都表现出有效性，为自动谈判技术发展做出了贡献。

Conclusion: ChargingBoul展示了轻量级策略在自动谈判中的有效性，未来可通过更复杂的对手建模和自适应出价启发式方法进一步提升性能。

Abstract: Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [433] [K2-V2: A 360-Open, Reasoning-Enhanced LLM](https://arxiv.org/abs/2512.06201)
*K2 Team,Zhengzhong Liu,Liping Tang,Linghao Jin,Haonan Li,Nikhil Ranjan,Desai Fan,Shaurya Rohatgi,Richard Fan,Omkar Pangarkar,Huijuan Wang,Zhoujun Cheng,Suqi Sun,Seungwook Han,Bowen Tan,Gurpreet Gosal,Xudong Han,Varad Pimpalkhute,Shibo Hao,Ming Shan Hee,Joel Hestness,Haolong Jia,Liqun Ma,Aaryamonvikram Singh,Daria Soboleva,Natalia Vassilieva,Renxi Wang,Yingquan Wu,Yuekai Sun,Taylor Killian,Alexander Moreno,John Maggs,Hector Ren,Guowei He,Hongyi Wang,Xuezhe Ma,Yuqi Wang,Mikhail Yurochkin,Eric P. Xing*

Main category: cs.LG

Relevance: 85.0

TL;DR: K2-V2是一个从头开始训练的360度开放LLM，专为推理适应而设计，在72B规模中表现优异，超越Qwen2.5-72B并接近Qwen3-235B性能，通过主动注入领域知识、推理、长上下文和工具使用能力来优化推理任务。


<details>
  <summary>Details</summary>
Motivation: 构建一个专门为推理任务优化的开源基础模型，弥补现有通用LLM在复杂推理能力上的不足，同时为社区提供完整的训练历史和数据集，支持持续训练和开源生产场景。

Method: 从头开始训练360度开放LLM，在训练过程中主动注入领域知识、推理能力、长上下文处理和工具使用功能，使用简单的监督微调建立强基线，并发布完整的训练历史和数据组成。

Result: K2-V2成为最强的完全开源模型，在72B规模中与开源领先模型竞争，超越Qwen2.5-72B，性能接近Qwen3-235B，为复杂推理任务建立了强大的基础。

Conclusion: K2-V2为推理任务提供了优秀的基础模型，展示了通过专门训练策略可以显著提升LLM的推理能力，同时完整的开源发布为社区提供了宝贵的资源和研究基础。

Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.

</details>


### [434] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文分析了Bradley-Terry损失函数的梯度特性，发现其梯度范数受两个因素影响：预测奖励差异和表示距离。表示距离会导致梯度不平衡问题，作者提出了NormBT方法进行自适应归一化来修正这一问题。


<details>
  <summary>Details</summary>
Motivation: 在RLHF框架中，奖励模型对LLM对齐至关重要。标准的Bradley-Terry损失函数广泛用于奖励建模，但其梯度特性尚未被充分研究。作者发现BT损失的梯度范数不仅取决于预测误差，还受到表示距离的影响，这可能导致梯度不平衡和学习偏差。

Method: 作者分析了BT损失的逐样本梯度特性，发现梯度范数包含两个分量：(1) 选择与拒绝响应的预测奖励差异（预测误差），(2) 最后一层输出空间的表示距离。为了解决表示距离导致的梯度不平衡问题，提出了NormBT方法——一种自适应成对归一化方案，平衡表示驱动效应，聚焦于预测误差。

Result: NormBT在各种LLM骨干网络和数据集上一致提高了奖励模型性能。在RewardBench的推理类别上获得了超过5%的显著提升，该类别包含许多小距离对。NormBT作为BT损失的轻量级即插即用方案，开销可忽略不计。

Conclusion: 这项工作揭示了广泛使用的BT目标函数的关键局限性，并提供了一种简单有效的修正方法。NormBT通过自适应归一化平衡梯度更新，改善了奖励模型的学习效果，特别是在需要细粒度区分的场景中。

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [435] [Optimizing Optimizers for Fast Gradient-Based Learning](https://arxiv.org/abs/2512.06370)
*Jaerin Lee,Kyoung Mu Lee*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文为梯度学习中的优化器设计自动化建立了理论基础，基于贪心原则将优化器设计问题形式化为最大化损失瞬时下降，通过将优化器视为将梯度信号转换为参数运动的函数，将问题简化为优化器空间上的凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 当前优化器设计主要依赖经验和试错，缺乏系统化的理论基础。研究者希望建立一个统一的框架来自动化优化器设计和超参数调优，减少人工干预，提高训练效率。

Method: 基于贪心原则，将优化器设计问题形式化为最大化损失瞬时下降。将优化器视为函数，将梯度信号映射到参数更新，然后将问题转化为优化器空间上的凸优化问题。在不同约束条件下求解这些凸优化问题。

Result: 该方法不仅能够推导出多种流行优化器（如SGD、Adam等）的闭式解，还能自动确定这些优化器的最优超参数。更重要的是，可以在训练过程中动态地进行优化器优化，根据收集的梯度统计信息调整优化策略。

Conclusion: 该研究为优化器设计自动化提供了坚实的理论基础，建立了一个统一的框架，能够系统化地设计和调优优化器，减少对人工经验的依赖，提高深度学习训练的效率。

Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.

</details>


### [436] [RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs](https://arxiv.org/abs/2512.06392)
*Runlong Zhou,Lefan Zhang,Shang-Chen Wu,Kelvin Zou,Hanzhi Zhou,Ke Ye,Yihao Feng,Dong Yin,Alex Guillen Garcia,Dmytro Babych,Rohit Chatterjee,Matthew Hopkins,Xiang Kong,Chang Lan,Lezhi Li,Yiping Ma,Daniele Molinari,Senyu Tong,Yanchao Sun,Thomas Voice,Jianyu Wang,Chong Wang,Simon Wang,Floris Weers,Yechen Xu,Guolin Yin,Muyang Yu,Yi Zhang,Zheng Zhou,Danyang Zhuo,Ruoming Pang,Cheng Leong*

Main category: cs.LG

Relevance: 85.0

TL;DR: RLAX是一个在TPU上运行的可扩展强化学习框架，采用参数服务器架构，支持多种先进RL算法，通过系统优化和数据集管理技术，在12小时内将QwQ-32B模型的pass@8准确率提升12.8%。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为提升大语言模型推理能力的主流方法，但现有的RL训练框架在可扩展性、容错性和收敛速度方面存在挑战，特别是在大规模TPU集群上运行时。

Method: 1) 参数服务器架构：主训练器定期推送更新权重，推理工作节点拉取最新权重生成新轨迹；2) 系统技术：支持可扩展、可抢占的RL训练；3) 数据集管理和对齐技术：加速收敛并提升模型质量。

Result: 在1024个v5p TPU上，仅用12小时48分钟就将QwQ-32B模型的pass@8准确率提升12.8%，同时保持训练过程中的抢占容错性。

Conclusion: RLAX提供了一个高效、可扩展的RL框架，能够显著加速LLM的RL训练过程，同时保持系统稳定性和训练质量。

Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.

</details>


### [437] [BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination](https://arxiv.org/abs/2512.06457)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

Relevance: 85.0

TL;DR: BitStopper：一种无需稀疏预测器的细粒度算法-架构协同设计，通过位级稀疏推测和异步处理机制，显著提升动态稀疏注意力在Transformer加速器上的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的LLM存在二次计算和内存开销问题，动态稀疏注意力虽能缓解但硬件效率受限，主要原因是预测阶段开销和内存流量大。需要一种无需预测器的更高效解决方案。

Method: 1) 位串行使能阶段融合(BESF)：重用内存访问，逐步终止不重要token，将预测阶段合并到执行阶段；2) 轻量自适应token选择(LATS)：配合位级稀疏推测；3) 位级异步处理(BAP)：在位粒度内存获取时提高计算利用率；4) 精心设计的架构将理论复杂度降低转化为实际性能提升。

Result: 相比SOTA Transformer加速器，BitStopper在Sanger上实现2.03倍加速，SOFA上1.89倍加速；能效分别提升2.4倍和2.1倍。

Conclusion: BitStopper通过算法-架构协同设计，有效解决了动态稀疏注意力的硬件效率问题，无需预测器即可实现显著的性能和能效提升，为高效LLM推理提供了新思路。

Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.

</details>


### [438] [Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning](https://arxiv.org/abs/2512.06533)
*Ming Chen,Sheng Tang,Rong-Xi Tan,Ziniu Li,Jiacheng Chen,Ke Xue,Chao Qian*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出使用强化学习（RL）来改进基于解码的回归方法，通过序列级奖励来增强数值预测的全局一致性，显著提升了预测精度和采样效率。


<details>
  <summary>Details</summary>
Motivation: 基于解码的回归方法将回归任务重新表述为序列生成任务，但现有方法存在离散token级目标（如交叉熵）与连续数值之间的不对齐问题。token级约束通常无法捕捉目标值的全局幅度，限制了预测精度和泛化能力。

Method: 将生成过程建模为马尔可夫决策过程（MDP），使用序列级奖励来强制全局数值一致性。具体采用ReMax和GRPO等强化学习方法，通过序列级信号来优化解码过程。

Result: 在表格回归和代码度量回归上的大量实验表明，该方法（特别是使用ReMax和GRPO）持续优于最先进的token级基线和传统回归头。分析显示RL显著提高了采样效率和预测精度。

Conclusion: 强化学习的引入使基于解码的回归成为通用数值预测的稳健且准确的范式，通过序列级信号解决了token级目标与连续数值之间的不对齐问题。

Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.

</details>


### [439] [GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655)
*Jehyeok Yeon,Federico Cinus,Yifan Wu,Luca Luceri*

Main category: cs.LG

Relevance: 85.0

TL;DR: GSAE通过图正则化稀疏自编码器学习安全概念的分布式表示，实现运行时安全引导，在保持良性查询效用的同时有效拒绝有害内容。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法存在局限：黑盒护栏仅过滤输出，而基于内部激活的方法通常将安全概念简化为单一潜在特征或维度。但研究表明，拒绝、时间性等抽象概念是分布在多个特征中的，而非孤立于单一特征。

Method: 提出图正则化稀疏自编码器(GSAE)，在标准SAE基础上增加神经元共激活图的拉普拉斯平滑惩罚项。GSAE学习平滑、分布式的安全表示作为跨多个特征的连贯模式。采用两阶段门控机制：仅在检测到有害提示或生成内容时激活干预，自适应执行拒绝同时保持良性查询效用。

Result: GSAE引导实现平均82%的选择性拒绝率，显著优于标准SAE引导(42%)，同时保持强任务准确性（TriviaQA 70%、TruthfulQA 65%、GSM8K 74%）。在LLaMA-3、Mistral、Qwen、Phi等模型家族上具有良好泛化性，对GCG、AutoDAN等越狱攻击保持≥90%的有害内容拒绝率。

Conclusion: GSAE通过分布式安全表示学习，有效解决了现有安全引导方法的局限性，实现了自适应安全干预，在保持模型效用的同时显著提升了对有害内容的拒绝能力。

Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.

</details>


### [440] [GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning](https://arxiv.org/abs/2512.06678)
*Shrihari Sridharan,Deepak Ravikumar,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

Relevance: 85.0

TL;DR: GradientSpace：一种在完整梯度空间中直接聚类样本的框架，通过在线SVD算法识别潜在技能，训练专门的LoRA专家和轻量级路由器，在推理时选择最佳专家，减少梯度干扰并提升性能。


<details>
  <summary>Details</summary>
Motivation: 指令调优是适应LLM下游应用的关键步骤，但现实世界数据集通常异构，导致梯度干扰（冲突梯度将模型拉向相反方向，降低性能）。现有基于语义相似性的聚类方法无法捕捉数据如何影响模型参数学习，而基于梯度聚类的方法存在维度压缩导致精度损失、需要专家集成增加推理成本等问题。

Method: 提出GradientSpace框架：1）在完整梯度空间中直接聚类样本，避免维度压缩；2）引入基于LoRA梯度的在线SVD算法，识别潜在技能，无需存储所有样本梯度；3）为每个聚类训练专门的LoRA专家；4）训练轻量级路由器在推理时选择最佳专家（而非集成多个专家）。

Result: 在数学推理、代码生成、金融和创意写作任务上的实验表明：GradientSpace实现了连贯的专家专业化，相比最先进的聚类方法和微调技术带来一致的准确率提升，同时显著降低推理延迟（相比专家集成方法）。

Conclusion: GradientSpace通过直接在完整梯度空间中聚类，有效缓解梯度干扰问题，实现更好的专家专业化，在多个任务上优于现有方法，同时减少推理成本，为异构数据集的指令调优提供了更有效的解决方案。

Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.

</details>


### [441] [KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models](https://arxiv.org/abs/2512.06727)
*Sourjya Roy,Shrihari Sridharan,Surya Selvam,Anand Raghunathan*

Main category: cs.LG

Relevance: 85.0

TL;DR: KV CAR是一个统一的架构无关框架，通过轻量级自编码器和相似性驱动的重用机制，显著减少KV缓存存储，实现内存高效的LLM推理


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和上下文长度的增加，KV缓存的内存需求成为自回归解码的主要瓶颈，限制了可实现的批处理大小和上下文窗口

Method: 结合两种互补技术：1) 轻量级自编码器学习键值张量在嵌入维度上的紧凑表示，在存储前压缩并在检索时恢复；2) 相似性驱动的重用机制识别相邻层间特定注意力头的KV张量重用机会

Result: 在GPT-2和TinyLLaMA模型上的评估显示，KV CAR实现了高达47.85%的KV缓存内存减少，对困惑度和零样本准确率影响最小。系统级测量显示减少的KV占用直接转化为更长的序列长度和更大的批处理大小

Conclusion: KV CAR通过减少KV张量的维度和结构冗余，无需改变Transformer架构，就能实现内存高效的LLM推理

Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.

</details>


### [442] [Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis](https://arxiv.org/abs/2512.06917)
*Clifford F,Devika Jay,Abhishek Sarkar,Satheesh K Perepu,Santhosh G S,Kaushik Dey,Balaraman Ravindran*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一个解释强化学习智能体长期行为的框架，通过定义新的状态重要性指标来对完整轨迹进行排序，结合Q值差异和"激进项"来衡量状态关键性，并能生成反事实推演来解释"为什么选择这个而不是那个"。


<details>
  <summary>Details</summary>
Motivation: 当前可解释强化学习（XRL）主要关注局部、单步决策，缺乏对智能体长期行为的解释。随着RL智能体在现实世界应用中的部署增加，确保其行为透明可信变得至关重要，因此需要轨迹级别的分析来解释长期行为。

Method: 引入一个新颖框架，通过定义和聚合新的状态重要性指标来对完整轨迹进行排序。该指标结合了经典的Q值差异和一个"激进项"，后者捕捉智能体达到目标的亲和力，提供更细致的状态关键性度量。方法还包括从关键状态生成反事实推演来验证路径选择的优越性。

Result: 在标准OpenAI Gym环境中的实验验证表明，提出的重要性指标比经典方法更有效地识别最优行为。方法成功从异构的智能体经验集合中识别出最优轨迹，并通过反事实推演展示智能体选择的路径相对于替代方案的鲁棒优越性。

Conclusion: 该框架为解释强化学习智能体的长期行为提供了有效方法，通过轨迹级别的分析和反事实解释，向可信赖的自主系统迈出了重要一步，解决了"为什么选择这个而不是那个"的解释需求。

Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.

</details>


### [443] [Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models](https://arxiv.org/abs/2512.06920)
*Alexandr Plashchinsky*

Main category: cs.LG

Relevance: 85.0

TL;DR: PGSRM是一种轻量级奖励框架，使用父模型参考输出嵌入与子模型生成输出的余弦相似度作为语义奖励，替代传统RLHF中的二元奖励或人工偏好数据。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要大量人工标注偏好数据或训练奖励模型，成本高昂且复杂。作者希望开发一种无需人工标注、无需额外模型训练的轻量级奖励框架，利用预训练模型的语义理解能力来指导强化学习。

Method: PGSRM的核心方法是：对于相同输入，计算父模型参考输出嵌入与子模型生成输出嵌入之间的余弦相似度，作为密集的语义奖励信号。这种方法完全避免了二元奖励、人工偏好数据和奖励模型训练，直接利用预训练模型的语义表示能力。

Result: 在五个语言任务上的实验表明，PGSRM相比二元奖励基线能产生更平滑的奖励改进和更稳定的PPO动态，表明基于嵌入的语义奖励是RLHF风格奖励建模的实用替代方案，特别适用于较小Transformer模型的父引导对齐。

Conclusion: PGSRM为语言模型的强化学习提供了一种轻量级、无需人工标注的语义奖励框架，利用父模型的语义理解来指导子模型对齐，在较小Transformer模型中表现出良好的稳定性和实用性。

Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.

</details>


### [444] [A Unifying Human-Centered AI Fairness Framework](https://arxiv.org/abs/2512.06944)
*Munshi Mahbubur Rahman,Shimei Pan,James R. Foulds*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一个统一的人类中心公平性框架，系统覆盖八种公平性指标，允许利益相关者根据价值观和上下文考虑调整权重，实现多目标权衡。


<details>
  <summary>Details</summary>
Motivation: AI在关键社会领域应用增加引发了对公平性的担忧，特别是关于种族、性别和社会经济地位等敏感属性的不平等对待。现有公平性研究面临不同公平概念之间以及与预测准确性之间的权衡挑战，阻碍了公平AI系统的实际部署。

Method: 引入统一的人类中心公平性框架，系统覆盖八种公平性指标，这些指标通过结合个体与群体公平、边际内与交叉假设、基于结果与机会平等视角形成。框架使用一致且易于理解的公式，允许利益相关者为多个公平目标分配权重。

Result: 在四个真实世界数据集上应用：UCI Adult收入预测、COMPAS刑事再犯、德国信用风险评估和MEPS医疗保健利用。结果显示调整权重揭示了不同公平性指标之间的细微权衡。通过司法决策和医疗保健案例研究，展示了框架如何指导公平AI系统的实际和价值敏感部署。

Conclusion: 该框架为利益相关者提供了一个系统方法来导航公平性权衡，促进多利益相关者妥协，并支持公平AI系统的实际部署，特别是在需要价值敏感决策的关键社会领域。

Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.

</details>


### [445] [LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding](https://arxiv.org/abs/2512.06982)
*Yu Yu,Qian Xie,Nairen Cao,Li Jin*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种基于大语言模型的神经架构搜索方法，用于设计多源强化学习中的状态编码器，相比传统NAS方法能以更少的候选评估发现性能更高的架构。


<details>
  <summary>Details</summary>
Motivation: 强化学习中处理多源信息（如传感器测量、时间序列信号、图像观测和文本指令）的状态编码器设计仍然探索不足，通常需要手动设计。现有神经架构搜索方法忽略了模块中间输出的有用侧信息，限制了多源RL设置中的样本效率。

Method: 将问题形式化为复合神经架构搜索问题，提出基于LLM的NAS流程，利用语言模型先验和中间输出信号来指导样本高效的搜索，寻找高性能的复合状态编码器。

Result: 在混合自主交通控制任务中，该方法比传统NAS基线和基于LLM的GENIUS框架以更少的候选评估发现了性能更高的架构。

Conclusion: LLM驱动的NAS方法能够有效解决多源强化学习中的状态编码器设计问题，通过利用语言模型先验和中间输出信号实现样本高效的架构搜索。

Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.

</details>


### [446] [Flash Multi-Head Feed-Forward Network](https://arxiv.org/abs/2512.06989)
*Minshen Zhang,Xiang Hu,Jianguo Li,Wei Wu,Kewei Tu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Flash Multi-Head FFN (FlashMHF)作为Transformer中FFN的替代方案，通过多头机制增强表达能力，同时解决内存消耗和维度比例失衡问题，在128M到1.3B参数模型上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 受多头注意力机制成功启发，探索将多头设计应用于FFN。单头注意力与FFN在结构上相似，但直接将多头机制应用于FFN面临两个挑战：内存消耗随头数线性增长，以及中间维度与头维度比例失衡影响可扩展性和表达能力。

Method: 提出FlashMHF，包含两个关键创新：1) I/O感知融合内核，类似FlashAttention在SRAM中在线计算输出；2) 使用动态加权并行子网络设计，保持中间维度与头维度之间的平衡比例。

Result: 在128M到1.3B参数模型上验证，FlashMHF相比SwiGLU FFNs持续改进困惑度和下游任务准确率，同时将峰值内存使用降低3-5倍，推理速度提升最高1.08倍。

Conclusion: 多头设计是FFN的优越架构原则，FlashMHF作为Transformer中FFN的强大、高效、可扩展替代方案，为FFN架构设计提供了新方向。

Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.

</details>


### [447] [Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation](https://arxiv.org/abs/2512.07010)
*Kevin Lee,Pablo Millan Arias*

Main category: cs.LG

Relevance: 85.0

TL;DR: DynamicLRP：首个模型无关的层间相关性传播框架，通过张量操作级分解和Promise系统实现真正架构无关性，支持包括Mamba、Whisper等31,465个计算图节点，无需模型特定代码。


<details>
  <summary>Details</summary>
Motivation: 现有LRP实现基于模块级别，需要架构特定的传播规则和修改，限制了目标模型的通用性和实现可持续性，难以适应不断演进的架构。

Method: 1. 在计算图内将归因分解到单个张量操作级别；2. 引入Promise系统实现延迟激活解析；3. 独立于反向传播机制，可在任意计算图上操作；4. 基于计算图设计，理论上可扩展到支持自动微分的深度学习库。

Result: 1. 保真度匹配或超越专用实现（VGG上1.77 vs 1.69 ABPC，ViT相当性能）；2. RoBERTa-large和Flan-T5-large在SQuADv2上分别达到93.70%和95.06%的top-1归因准确率；3. 在15种不同架构的31,465个计算图节点上实现99.92%覆盖率；4. 支持Mamba、Whisper、DePlot等模型，无需模型特定代码。

Conclusion: DynamicLRP通过操作级分解和Promise系统，为LRP建立了可持续、可扩展的基础，能够适应不断演进的架构，解决了现有LRP实现的局限性。

Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.

</details>


### [448] [Block Sparse Flash Attention](https://arxiv.org/abs/2512.07011)
*Daniel Ohayon,Itay Lamprecht,Itay Hubara,Israel Cohen,Daniel Soudry,Noam Elata*

Main category: cs.LG

Relevance: 85.0

TL;DR: BSFA是一种训练免费的注意力加速方法，通过计算精确的查询-键相似度来选择每个查询最重要的k个值块，跳过约50%的计算和内存传输，在保持99%以上基线准确率的同时实现1.10-1.24倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型需要处理长上下文，但注意力机制的二次复杂度造成了严重的计算瓶颈。现有方法在计算分数前预测重要性，可能不够准确。

Method: BSFA通过计算精确的查询-键相似度，比较每个块的最大分数与校准阈值，选择每个查询最重要的k个值块。采用一次性阈值校准学习每层每头的注意力分数分布，无需训练。

Result: 在Llama-3.1-8B上，BSFA在真实世界推理基准上实现1.10倍加速，在"大海捞针"检索任务上实现1.24倍加速，保持99%以上基线准确率，某些配置甚至通过关注最相关内容提高了准确性。

Conclusion: BSFA是一种有效的训练免费注意力加速方法，通过精确的重要性选择显著优于现有稀疏注意力方法，为长上下文推理提供了实用的计算优化方案。

Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention

</details>


### [449] [The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models](https://arxiv.org/abs/2512.07092)
*Zhixiang Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Soul Engine框架，基于线性表示假设，通过解耦的人格向量实现个性化LLM，无需微调骨干权重，避免对齐税。


<details>
  <summary>Details</summary>
Motivation: 当前个性化LLM部署受稳定性-可塑性困境限制，传统对齐方法（如SFT）通过随机权重更新导致"对齐税"，损害通用推理能力。

Method: 基于线性表示假设，提出Soul Engine框架，使用SoulBench数据集（动态上下文采样构建），在冻结的Qwen-2.5基础上采用双头架构提取解耦的人格向量，不修改骨干权重。

Result: 1) 高精度人格剖析：MSE为0.011；2) 几何正交性：T-SNE可视化显示人格流形离散连续，支持"零样本人格注入"保持原始智能；3) 确定性引导：通过向量算术实现鲁棒行为控制。

Conclusion: 挑战了微调在个性化中的必要性，从概率提示转向确定性潜在干预，为安全可控的AI个性化提供数学严谨基础。

Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.

</details>


### [450] [FOAM: Blocked State Folding for Memory-Efficient LLM Training](https://arxiv.org/abs/2512.07112)
*Ziqing Wen,Jiahuan Wang,Ping Luo,Dongsheng Li,Tao Sun*

Main category: cs.LG

Relevance: 85.0

TL;DR: FOAM是一种内存高效的优化器，通过块状梯度均值压缩优化器状态，减少50%训练内存和90%优化器状态内存，同时保持Adam的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练面临严重内存瓶颈，特别是使用Adam等内存密集型优化器时。现有内存高效方法存在计算开销大、需要额外内存或性能下降等问题。

Method: 提出FOAM方法：1) 通过计算块状梯度均值压缩优化器状态；2) 引入残差校正恢复丢失信息；3) 理论上在非凸优化设置下达到与vanilla Adam相当的收敛率。

Result: FOAM减少约50%总训练内存，消除高达90%的优化器状态内存开销，加速收敛，且与其他内存高效优化器兼容，性能匹配或超越现有基线。

Conclusion: FOAM提供了一种有效的内存高效优化解决方案，在保持模型性能的同时显著减少训练内存需求，适用于大规模语言模型训练。

Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.

</details>


### [451] [Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning](https://arxiv.org/abs/2512.07374)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

Relevance: 85.0

TL;DR: R2F提出了一种基于LoRA参数重建全模型梯度方向的高效LLM遗忘框架，通过训练梯度解码器近似全模型梯度，无需全模型微调或原始训练数据。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型（如LLMs）的遗忘对于动态知识更新、数据删除权利执行和模型行为修正至关重要。现有遗忘方法通常需要全模型微调或访问原始训练数据，限制了可扩展性和实用性。

Method: R2F框架基于从低秩LoRA适配器更新重建全模型梯度方向：1）使用多个改写提示计算LoRA参数的梯度；2）训练梯度解码器近似对应的全模型梯度；3）为适应更大或黑盒模型，在代理模型上训练解码器并迁移到目标模型。

Result: 实验结果表明，R2F在预训练LLMs中实现了有效的遗忘，同时保持一般模型性能，提供了无需全重新训练或访问内部参数的可扩展轻量级替代方案。

Conclusion: R2F为LLM遗忘提供了一个高效、可扩展的框架，通过梯度重建方法解决了现有方法对全模型微调和原始数据访问的依赖问题。

Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

</details>


### [452] [LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples](https://arxiv.org/abs/2512.07375)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

Relevance: 85.0

TL;DR: LUNE：基于LoRA的轻量级大语言模型遗忘框架，通过仅更新低秩适配器实现负向遗忘，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以移除特定信息，传统遗忘方法需要昂贵的微调或权重编辑，不适用于实际部署

Method: 基于LoRA的负向遗忘框架，冻结主干网络，仅更新低秩适配器，通过中间表示抑制或替换目标知识

Result: 在多个事实遗忘任务上，效果与全微调和内存编辑方法相当，计算成本降低约一个数量级

Conclusion: LUNE提供了一种高效、轻量级的大语言模型遗忘解决方案，适用于隐私保护、偏见缓解和知识修正

Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.

</details>


### [453] [SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models](https://arxiv.org/abs/2512.07175)
*Yibo Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: SPACE提出了一种基于噪声对比估计的自博弈微调方法，通过二元分类区分真实样本和合成样本，解决了现有方法因只关注奖励差距而导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈微调方法主要关注真实样本和合成样本之间的奖励差距，忽略了绝对奖励值。理论分析表明，这种基于差距的方法存在目标函数退化风险，导致训练不稳定。

Method: SPACE采用噪声对比估计框架，将合成样本视为辅助成分，通过二元分类区分真实样本和合成样本。该方法独立优化每类数据的绝对奖励值，确保目标函数始终有意义。

Result: SPACE在多个任务上显著提升LLM性能，优于使用更多真实样本的监督微调。相比基于差距的自博弈方法，SPACE表现出显著优势且训练更稳定。

Conclusion: SPACE通过噪声对比估计捕获真实数据分布，解决了自博弈微调中的不稳定问题，理论上保证收敛到最优分布，实际效果优于现有方法。

Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.

</details>


### [454] [Group Representational Position Encoding](https://arxiv.org/abs/2512.07805)
*Yifan Zhang,Zixiang Chen,Yifeng Liu,Zhen Qin,Huizhuo Yuan,Kangping Xu,Yang Yuan,Quanquan Gu,Andrew Chi-Chih Yao*

Main category: cs.LG

Relevance: 85.0

TL;DR: GRAPE是一个基于群作用的统一位置编码框架，包含乘法旋转和加法logit偏置两种机制，将RoPE和ALiBi作为特例包含在内。


<details>
  <summary>Details</summary>
Motivation: 为长上下文模型提供一个原则性的位置几何设计空间，统一现有的位置编码方法，特别是RoPE和ALiBi，同时扩展其几何表达能力。

Method: 基于群作用理论，提出两种机制：1) 乘法GRAPE：在SO(d)群中使用旋转操作，通过指数映射实现相对、组合、保范的位置映射；2) 加法GRAPE：在GL群中使用单能作用，产生加法logit偏置。框架支持学习通勤子空间和非通勤混合来扩展几何表达能力。

Result: GRAPE统一了位置编码方法，将RoPE和ALiBi作为精确特例包含在内。乘法GRAPE可以扩展到捕获跨子空间特征耦合，加法GRAPE保持精确的相对位置规律和流式缓存能力。

Conclusion: GRAPE为长上下文模型提供了一个原则性的位置几何设计框架，统一并扩展了现有位置编码方法，为Transformer架构的位置编码提供了更丰富的设计空间。

Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,ω\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.

</details>


### [455] [SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents](https://arxiv.org/abs/2512.07287)
*Sijia Li,Yuchen Huang,Zifan Liu,Zijian Li,Jingjing fu,Lei Song,Jiang Bian,Jun Zhang,Rui Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: SIT-Graph 提出了一种结合状态表示和工具依赖图的方法，通过部分重叠经验增强多轮工具使用能力，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在多轮工具使用场景中存在挑战，因为意图会逐步澄清且环境随每次工具调用而演变。现有方法要么将整个轨迹或预定义子任务视为不可分割单元，要么仅利用工具间依赖关系，难以适应状态和信息在轮次间的演变。

Method: 提出状态集成工具图（SIT-Graph），从历史轨迹中同时捕获紧凑状态表示（类似情节记忆的片段）和工具间依赖关系（类似程序记忆的例程）。首先从累积的工具使用序列构建工具图，然后为每条边增强对话和工具历史的紧凑状态摘要。在推理时，SIT-Graph实现情节回忆和程序执行之间的平衡：当需要回忆先前上下文时，检索相关边上存储的状态摘要来指导行动；当步骤是例程时，遵循高置信度的工具依赖关系而无需显式回忆。

Result: 在多个状态化多轮工具使用基准测试中，SIT-Graph一致优于强大的基于记忆和图的方法基线，提供更稳健的工具选择和更有效的经验转移。

Conclusion: SIT-Graph通过结合人类决策中的情节记忆和程序记忆原理，有效解决了多轮工具使用中的适应性问题，为LLM智能体提供了更自然和有效的经验重用机制。

Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.

</details>


### [456] [Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models](https://arxiv.org/abs/2512.07419)
*Haidong Kang,Jun Du,Lihong Lin*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种基于大语言模型（LLM）的训练免费自动代理（TAP）发现框架，用于混合精度量化（MPQ），无需人工专家参与或训练过程，通过强化学习优化提示来提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 混合精度量化（MPQ）能解决深度神经网络的内存瓶颈问题，但现有方法要么依赖昂贵的可微分优化搜索（效率低、灵活性差），要么需要人工专家设计代理（劳动密集、专业知识要求高）。本文旨在探索能否设计一个无需人工专家参与和训练过程的代理。

Method: 提出LLM驱动的训练免费自动代理（TAP）发现框架：1）利用LLM为MPQ任务自动寻找优化的代理；2）提出基于直接策略优化（DPO）的强化学习方法来优化提示，弥合黑盒LLM与复杂MPQ任务之间的差距，构建LLM与MPQ任务之间的正向反馈循环。

Result: 在主流基准测试上的广泛实验表明，TAP实现了最先进的性能，证明了该方法的有效性。

Conclusion: TAP框架为MPQ社区提供了LLM驱动设计算法的新视角，能够显著推动该领域的发展，实现无需人工专家参与和训练的高效混合精度量化。

Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

</details>


### [457] [ReLaX: Reasoning with Latent Exploration for Large Reasoning Models](https://arxiv.org/abs/2512.07558)
*Shimin Zhang,Xianwei Chen,Yufan Shen,Ziyuan Ye,Jibin Wu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出ReLaX方法，通过分析大语言模型的潜在动态来优化强化学习中的探索-利用平衡，解决RLVR中的熵崩溃问题，在多种推理基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于可验证奖励的强化学习(RLVR)能增强大型推理模型的推理能力，但常导致熵崩溃，造成策略过早收敛和性能饱和。现有方法主要操作token级熵来促进探索，但作者认为token生成背后的潜在动态包含更丰富的计算结构，能更有效地指导探索-利用平衡。

Method: 提出ReLaX方法：1) 利用Koopman算子理论获得模型隐藏状态动态的线性化表示；2) 引入动态谱分散度(DSD)指标量化潜在动态的异质性，作为策略探索的直接指标；3) 在策略优化中显式结合潜在动态来调节探索与利用。

Result: 在多种多模态和纯文本推理基准上的综合实验表明，ReLaX能显著缓解过早收敛问题，并持续取得最先进的性能表现。

Conclusion: 通过分析大语言模型的潜在动态来指导强化学习中的探索策略，比单纯操作token级熵更有效，为解决RLVR中的熵崩溃问题提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

</details>


### [458] [A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance](https://arxiv.org/abs/2512.07647)
*Georgios Tzachristas,Lei Deng,Ioannis Tzachristas,Gong Zhang,Renhai Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种用于认证Top-k注意力截断的统一数学框架，量化了分布和输出层面的近似误差，并推导出确定性误差界和渐近规则，实验证明可平均减少2-4倍的键值计算。


<details>
  <summary>Details</summary>
Motivation: 注意力机制中的Top-k截断是加速Transformer推理的常用技术，但缺乏严格的误差分析。现有方法通常使用通用不等式，无法提供精确的误差控制。本文旨在建立严格的数学框架来量化Top-k截断的近似误差，为高效注意力计算提供理论保证。

Method: 1. 建立统一的数学框架，证明总变差距离等于丢弃的softmax尾部质量：TV(P, P̂) = 1 - e^{-KL(P̂∥P)}；2. 推导非渐近确定性误差界（单边界间隙、多间隙和分块变体）；3. 使用精确的头尾分解，证明输出误差可分解为：∥Attn(q,K,V)-Attn_k(q,K,V)∥₂ = τ∥μ_tail - μ_head∥₂；4. 在i.i.d.高斯分数模型下推导闭式尾部质量和渐近规则。

Result: 1. 理论分析表明Top-k截断误差可通过总变差距离精确控制；2. 推导出最小k_ε的渐近规则：k_ε/n ≈ Φ_c(σ + Φ^{-1}(ε))；3. 在bert-base-uncased和合成logits上的实验证实了k_ε/n的预测缩放规律；4. 认证Top-k可在满足总变差预算的同时，平均减少2-4倍的键值计算。

Conclusion: 本文为Top-k注意力截断提供了严格的数学基础和误差分析框架，实现了计算效率与近似精度之间的可认证权衡，为高效Transformer推理提供了理论保证。

Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=τ\|μ_{\mathrm{tail}}-μ_{\mathrm{head}}\|_2$ with $τ=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\leτ\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxΦ_c(σ+Φ^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.

</details>


### [459] [Depth-Wise Activation Steering for Honest Language Models](https://arxiv.org/abs/2512.07667)
*Gracjan Góral,Marysia Winkels,Steven Basart*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种无需训练的高斯调度激活引导方法，通过跨网络深度加权引导强度来提升LLM的诚实性而非准确性，在多个模型上验证有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有时会说出错误内容，尽管内部知道正确答案，这是诚实性而非准确性的失败，影响了可审计性和安全性。现有方法主要优化事实正确性或依赖重新训练和脆弱的单层编辑，对真实报告的控制有限。

Method: 提出无需训练的激活引导方法，使用高斯调度在跨网络深度加权引导强度。该方法简单、模型无关、无需微调，为从模型现有能力中引出真实报告提供低成本控制手段。

Result: 在MASK基准测试（分离诚实性与知识）上评估了LLaMA、Qwen和Mistral家族的七个模型，发现高斯调度在六个模型中比无引导和单层基线提高了诚实性。在LLaMA-3.1-8B-Instruct和Qwen-2.5-7B-Instruct上的等预算消融实验显示，高斯调度优于随机、均匀和盒滤波器深度分配，表明干预在深度上的分布方式对结果有实质性影响。

Conclusion: 高斯调度激活引导方法能有效提升LLM的诚实性，且方法简单、模型无关、无需训练，为引出模型真实报告提供了实用工具。

Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.

</details>


### [460] [In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models](https://arxiv.org/abs/2512.07705)
*Saroj Gopali,Bipin Chhetri,Deepika Giri,Sima Siami-Namini,Akbar Siami Namin*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该研究比较了LLM和专门的时间序列基础模型（TimesFM）与传统深度学习模型（TCN、LSTM）在时间序列预测上的性能，发现TimesFM表现最佳，LLM在零样本学习中也显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列基础模型（如Google的TimesFM）和LLM的发展，研究者希望探索这些预训练基础模型是否能在时间序列分析和预测中超越传统的ARIMA、Transformer、LSTM和TCN等方法。

Method: 研究采用上下文学习、零样本学习和少样本学习等方法训练LLM，比较了OpenAI o4-mini、Gemini 2.5 Flash Lite、Google的TimesFM（基于Transformer的时间序列专用基础模型）以及TCN和LSTM网络在时间序列预测上的性能。

Result: TimesFM取得了最佳整体性能，RMSE最低（0.3023），推理时间具有竞争力（266秒）。OpenAI的o4-mini在零样本学习中也表现出良好性能。

Conclusion: 预训练的时间序列基础模型是实时预测的有前景方向，能够以最小的模型适应实现准确且可扩展的部署。LLM在零样本学习中也显示出潜力。

Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.

</details>


### [461] [GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory](https://arxiv.org/abs/2512.07782)
*Jiaxu Liu,Yuhe Bai,Christos-Savvas Bouganis*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出GatedFWA：一种内存门控的滑动窗口注意力机制，在保持SWA线性时间效率的同时，通过可学习的衰减偏置稳定内存更新并控制梯度流，解决了传统SWA训练目标无界和Softmax注意力内存收缩的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的Softmax全注意力存在二次复杂度问题，而滑动窗口注意力(SWA)虽然实现了线性时间编码/解码，但在关联内存解释下，其差异式更新导致训练目标无界。同时，Softmax注意力通过归一化更新会导致内存收缩和梯度消失问题。

Method: 提出GatedFWA机制，为每个token/head累积一个门控值作为衰减偏置添加到注意力logits中，作为内存递归中的可学习收缩因子。实现了融合的单通道门预处理和与FlashAttention兼容的内核，在滑动掩码下注入门控，确保I/O效率和数值稳定性。

Result: 在语言建模基准测试中，GatedFWA以可忽略的开销实现了有竞争力的吞吐量，更好地利用了全局上下文，并且能够与NSA等token压缩/选择方法无缝集成，适用于各种自回归领域。

Conclusion: GatedFWA成功解决了SWA训练目标无界和Softmax注意力内存收缩的问题，在保持线性时间效率的同时实现了稳定的内存更新和可控的梯度流，为高效的自回归建模提供了新的解决方案。

Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.

</details>


### [462] [Provable Long-Range Benefits of Next-Token Prediction](https://arxiv.org/abs/2512.07818)
*Xinyuan Cao,Santosh S. Vempala*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文证明：通过RNN优化下一个词预测任务，可以学习到训练分布的长程结构，实现k个连续token的不可区分性，为实践中观察到的长程连贯性提供了复杂性理论解释。


<details>
  <summary>Details</summary>
Motivation: 解释为什么现代语言模型（训练用于下一个词预测）能够生成连贯文档并捕捉长程结构。提供理论证明，说明下一个词预测任务在捕捉长程依赖方面的能力。

Method: 使用循环神经网络（RNN）优化下一个词预测任务，证明该模型能够近似训练分布。提供多项式界限（关于k）来描述实现k个token不可区分性所需的模型规模。

Result: 证明对于从训练分布采样的文档，任何有界描述长度的算法都无法区分真实文档的k个连续token和语言模型生成的k个token。这为实践中观察到的长程连贯性提供了理论依据。

Conclusion: 下一个词预测任务在理论上具有捕捉长程结构的能力，即使使用常见的神经网络架构。这解释了为什么基于下一个词预测训练的语言模型能够生成连贯文档。

Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.

</details>


### [463] [A Latent Variable Framework for Scaling Laws in Large Language Models](https://arxiv.org/abs/2512.06553)
*Peiyao Cai,Chengyu Cui,Felipe Maia Polo,Seamus Somerstep,Leshem Choshen,Mikhail Yurochkin,Moulinath Banerjee,Yuekai Sun,Kean Ming Tan,Gongjun Xu*

Main category: stat.AP

Relevance: 85.0

TL;DR: 提出基于潜变量建模的统计框架，用于分析大语言模型的缩放定律，解决不同模型家族和基准测试的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 随着具有不同架构和训练策略的LLM家族快速涌现，以及评估基准不断增加，单一的全局缩放曲线无法捕捉不同模型家族和基准测试之间的性能变化模式。

Method: 提出潜变量建模框架：每个LLM家族关联一个潜变量，捕捉该家族的共同特征；模型在不同基准上的性能由其潜技能驱动，这些技能由潜变量和模型自身可观测特征共同决定。开发了估计程序并建立了统计性质，设计了支持估计和各种下游任务的高效数值算法。

Result: 在Open LLM Leaderboard (v1/v2)的12个广泛使用的基准上进行了实证评估。

Conclusion: 该框架能够更好地理解和预测不同LLM家族在各种基准测试上的性能表现，为缩放定律研究提供了更精细的统计工具。

Abstract: We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).

</details>


### [464] [ARC-AGI Without Pretraining](https://arxiv.org/abs/2512.06104)
*Isaac Liao,Albert Gu*

Main category: cs.LG

Relevance: 75.0

TL;DR: CompressARC是一个仅7.6万参数的模型，无需预训练即可解决20%的ARC-AGI视觉谜题，通过推理时最小化描述长度(MDL)实现，在极端数据限制下展现非凡泛化能力


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念：通常认为解决ARC-AGI基准测试中的IQ式视觉谜题需要大规模预训练。本文旨在证明存在替代路径，通过最小化描述长度(MDL)方法，在极有限数据条件下实现智能推理

Method: 提出CompressARC模型，仅7.6万参数，无任何预训练。采用最小化描述长度(MDL)方法，在推理时仅针对单个目标谜题（移除最终解信息）进行训练，不使用ARC-AGI提供的训练集

Result: 模型成功解决了20%的评估谜题，在极端数据限制条件下（仅使用目标谜题本身作为训练数据）表现出令人惊讶的泛化能力，解决了多样化的创造性谜题

Conclusion: 最小化描述长度(MDL)是产生智能的可行替代路径，不同于传统的大规模预训练方法，在极有限数据条件下仍能展现智能推理能力

Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.

</details>


### [465] [Quantifying Memory Use in Reinforcement Learning with Temporal Range](https://arxiv.org/abs/2512.06204)
*Rodney Lafuente-Mercado,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出Temporal Range指标，用于量化RL策略对历史观测的依赖程度，通过计算输出对输入序列的时间敏感性来测量记忆依赖


<details>
  <summary>Details</summary>
Motivation: 现有方法难以量化训练好的RL策略实际使用历史观测的程度，需要一种模型无关的指标来测量策略的时间记忆依赖

Method: 提出Temporal Range指标，通过反向自动微分计算Jacobian块∂y_s/∂x_t，对最终时间步取平均，得到时间影响剖面，用幅度加权平均滞后进行总结

Result: 在诊断和控制任务（POPGym、闪烁/遮挡、Copy-k）及多种架构（MLP、RNN、SSM）上验证：1）完全观测控制中保持较小；2）在Copy-k中与任务真实滞后成比例；3）与获得近最优回报所需的最小历史窗口一致

Conclusion: Temporal Range提供了一种实用的序列级记忆依赖读取方法，可用于比较智能体和环境，以及选择最短的充分上下文

Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.

</details>


### [466] [Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks](https://arxiv.org/abs/2512.06297)
*Luca Di Carlo,Chase Goddard,David J. Schwab*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文揭示了深度学习中损失景观的一个悖论：虽然不同吸引盆之间存在低损失路径，但优化动态通常局限于单个凸盆。研究发现这是由于曲率变化与优化噪声相互作用产生的熵壁垒所致。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络表现出一个矛盾现象：损失景观中的吸引盆通常通过低损失路径相连，但优化动态却很少探索这些中间点。本文旨在解释为什么优化过程会局限于单个凸盆，而不利用这些已知的连接路径。

Method: 通过分析曲率变化与优化动态中噪声的相互作用，识别出熵壁垒的形成机制。实证研究发现，远离最小值时曲率会系统性上升，产生将噪声动态偏转回端点的有效力，即使损失保持平坦。

Result: 研究发现曲率诱导的熵壁垒比能量壁垒持续时间更长，塑造了参数空间中解的后期定位。这些壁垒解释了为什么优化动态会局限于单个吸引盆，即使存在低损失的连接路径。

Conclusion: 曲率诱导的熵力在深度学习景观中既控制着连接性又控制着约束性，解决了优化动态局限于单个凸盆的悖论，对理解神经网络优化动态有重要意义。

Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.

</details>


### [467] [Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness](https://arxiv.org/abs/2512.06341)
*Ronald Katende*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出Interpretive Efficiency（解释效率）度量，量化解释性表示中任务相关信息传输的比例，基于五个公理，与互信息相关，具有理论保证和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 可解释性是可信机器学习的关键，但现有度量很少能有效量化数据对解释性表示的支持程度。需要一种理论严谨、实用的度量来评估解释性表示的设计质量。

Method: 提出Interpretive Efficiency函数，基于五个公理：有界性、Blackwell式单调性、数据处理稳定性、容许不变性、渐近一致性。将其与互信息关联，推导局部Fisher几何展开，使用经验过程工具建立渐近和有限样本估计保证。

Result: 在受控图像和信号任务上的实验表明：该度量能恢复理论排序，揭示被准确率掩盖的表示冗余，并与鲁棒性相关，成为表示设计的实用理论支持诊断工具。

Conclusion: Interpretive Efficiency是一种理论严谨、实用的度量，可用于评估解释性表示的设计质量，为可信机器学习提供重要诊断工具。

Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.

</details>


### [468] [Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry](https://arxiv.org/abs/2512.06347)
*Naoki Yoshida,Isao Ishikawa,Masaaki Imaizumi*

Main category: cs.LG

Relevance: 75.0

TL;DR: 在师生框架下，理论证明随机采样插值器的泛化误差在训练样本数超过特定阈值后会变为零，这源于参数空间中插值器集合的几何结构。


<details>
  <summary>Details</summary>
Motivation: 理解大规模模型（如深度神经网络）高泛化能力是机器学习理论的核心开放问题。虽然近期研究将这种现象归因于SGD的隐式偏置，但实证证据表明这主要源于模型本身的特性——即使是随机采样的插值器（达到零训练误差的参数）也能有效泛化。

Method: 在师生框架下，利用代数几何工具数学表征插值器集合在参数空间中的几何结构，证明随机采样插值器的泛化误差在训练样本数超过特定阈值后变为零。

Result: 理论证明：随机采样插值器的泛化误差在训练样本数超过由参数空间插值器集合几何结构确定的阈值后会精确变为零。

Conclusion: 模型的高泛化能力主要源于模型本身特性而非优化算法，插值器集合的几何结构决定了泛化误差消失的样本数阈值。

Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.

</details>


### [469] [Optimizing LLMs Using Quantization for Mobile Execution](https://arxiv.org/abs/2512.06490)
*Agatsya Yadav,Renta Chintala Bhargavi*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文研究了使用4位后训练量化（PTQ）压缩LLM以在移动设备上部署，将Llama 3.2 3B模型大小减少68.66%，并通过GGUF格式在Android设备上成功运行。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然功能强大，但其巨大的规模和计算需求阻碍了在资源受限的移动设备上的部署。需要找到一种方法在保持模型能力的同时减少其大小和计算要求。

Method: 使用BitsAndBytes库和Hugging Face Transformers框架对Meta的Llama 3.2 3B模型应用4位后训练量化（PTQ），然后将量化模型转换为GGUF格式，使用llama.cpp工具进行移动优化推理，最后在Android设备的Termux环境和Ollama框架中部署。

Result: 通过4位量化实现了68.66%的模型大小减少，量化后的模型能够在Android设备上成功执行推理任务，证明了在移动设备上运行量化GGUF模型的可行性。

Conclusion: 4位精度的PTQ结合GGUF等移动优化格式，为在移动设备上部署能力强大的LLM提供了一条实用途径，在模型大小和性能之间取得了良好平衡。

Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.

</details>


### [470] [A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation](https://arxiv.org/abs/2512.06547)
*Xiaocan Li,Shiliang Wu,Zheng Shen*

Main category: cs.LG

Relevance: 75.0

TL;DR: A-3PO通过近似计算近端策略，消除了异步RL中解耦损失算法的额外前向传播开销，将训练时间减少18%的同时保持性能


<details>
  <summary>Details</summary>
Motivation: 在异步强化学习设置中，解耦损失算法通过引入近端策略来提高学习稳定性，但近端策略需要在每个训练步骤中进行额外的网络前向传播，对于大语言模型来说造成了计算瓶颈

Method: 提出A-3PO（近似近端策略优化），通过简单的插值来近似近端策略，而不需要显式计算。近端策略仅作为行为策略和目标策略之间的信任域锚点，因此可以通过插值来近似

Result: A-3PO消除了额外的前向传播开销，将训练时间减少了18%，同时保持了可比较的性能

Conclusion: 通过近似近端策略，A-3PO在保持解耦损失算法稳定性的同时，显著减少了计算开销，特别适用于大语言模型的强化学习训练

Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md

</details>


### [471] [Deep Manifold Part 2: Neural Network Mathematics](https://arxiv.org/abs/2512.06563)
*Max Y. Ma,Gen-Hua Shi*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出了一种基于堆叠分段流形、不动点理论和边界条件迭代的神经网络全局方程理论框架，将神经网络视为由流形复杂性、高阶非线性和边界条件塑造的可学习数值计算。


<details>
  <summary>Details</summary>
Motivation: 论文旨在从几何、代数和不动点理论的角度理解神经网络的本质，解释为什么能力只有在不动点区域稳定时才会出现，并阐明单体模型在几何和数据诱导可塑性下的局限性。

Method: 通过堆叠分段流形、不动点理论和边界条件迭代来发展神经网络的全局方程，将神经网络视为由流形复杂性、高阶非线性和边界条件塑造的可学习数值计算。

Result: 提出了一个理论框架，解释了神经网络如何通过残差驱动迭代构建不动点，并揭示了现实世界数据复杂性、训练动态和学习复杂性如何约束可学习性。

Conclusion: 该视角澄清了单体模型的局限性，并激励设计能够将流形复杂性分布在多个弹性模型之间的架构和联邦系统，形成基于几何、代数、不动点和真实数据复杂性的连贯世界建模框架。

Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.

</details>


### [472] [A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs](https://arxiv.org/abs/2512.06607)
*Humzah Merchant,Bradford Levy*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种在推理时调整大模型logits的方法，通过一对小模型（分别微调于要遗忘和保留的信息）来消除金融预测中的前瞻偏差


<details>
  <summary>Details</summary>
Motivation: 将LLM应用于金融预测任务时面临前瞻偏差挑战，因为LLM在长时序数据上训练，导致无法进行金融中典型的回测。重新训练前沿模型成本过高，需要一种快速、有效、低成本的替代方案。

Method: 在推理时通过调整大基础模型的logits来引导生成。使用一对小型的专用模型：一个微调于要遗忘的信息，另一个微调于要保留的信息。通过这对模型的输出来调整基础模型的logits分布。

Result: 该方法能有效移除逐字和语义知识，纠正偏差，并且在性能上优于先前的方法。

Conclusion: 提出了一种低成本、高效的推理时知识编辑方法，解决了LLM在金融预测中的前瞻偏差问题，为时间敏感任务中的模型知识管理提供了新思路。

Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.

</details>


### [473] [Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding](https://arxiv.org/abs/2512.06929)
*MinCheol Jeon*

Main category: cs.LG

Relevance: 75.0

TL;DR: AdaMamba是一个统一的时间序列预测架构，集成了自适应归一化、多尺度趋势提取和上下文序列建模，以解决非平稳性、多尺度时间模式和分布偏移等挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测面临非平稳性、多尺度时间模式和分布偏移等挑战，这些会降低模型的稳定性和准确性。现有方法在处理这些复杂时间动态时存在局限性。

Method: 1. 自适应归一化块：通过多尺度卷积趋势提取和通道级重新校准去除非平稳成分
2. 上下文编码器：结合补丁嵌入、位置编码和Mamba增强的Transformer层（包含专家混合前馈模块）
3. 轻量级预测头和去归一化机制：生成多步预测并重新整合局部趋势

Result: 实验评估表明，AdaMamba的自适应归一化和专家增强的上下文建模相结合，在稳定性和准确性方面相比传统Transformer基线有持续改进。

Conclusion: AdaMamba提供了强大的表示能力和模块化可扩展性，有效缓解协变量偏移，增强跨异构数据集的预测可靠性，支持确定性预测并与概率扩展兼容。

Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.

</details>


### [474] [Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph](https://arxiv.org/abs/2512.07100)
*Hong Wang,Yinglong Zhang,Hanhan Guo,Xuewen Xia,Xing Xu*

Main category: cs.LG

Relevance: 75.0

TL;DR: DRCL是一个完全无监督的框架，用于文本属性网络中的社区发现，通过双向精炼循环整合结构信息和语义信息，无需标签或类别定义。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在文本理解方面强大，但在实际文本属性网络中部署困难，因为它们严重依赖标注数据。同时，传统的社区检测方法通常忽略文本语义，限制了在内容组织、推荐和风险监控等下游应用中的实用性。

Method: 提出双精炼循环学习（DRCL）框架，通过热启动初始化和GCN社区检测模块与文本语义建模模块之间的双向精炼循环，迭代交换伪标签，使语义线索增强结构聚类，结构模式指导文本表示学习。

Result: 在多个文本属性图数据集上，DRCL持续提升发现社区的结构和语义质量。仅使用DRCL社区信号训练的Mamba分类器达到了与监督模型相当的准确率。

Conclusion: DRCL为大规模系统中标注数据稀缺或成本高昂的场景提供了一种有效的无监督解决方案，展示了将结构信息和语义信息整合用于社区发现的潜力。

Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.

</details>


### [475] [Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration](https://arxiv.org/abs/2512.07173)
*Jucheng Shen,Gaurav Sarkar,Yeonju Ro,Sharath Nittur Sridhar,Zhangyang Wang,Aditya Akella,Souvik Kundu*

Main category: cs.LG

Relevance: 75.0

TL;DR: CadLLM是一种无需训练的方法，用于加速基于扩散的大语言模型(dLLMs)的推理吞吐量。它通过分析token unmasking置信度的动态特性，自适应控制生成块大小、步长和阈值，并动态利用词汇表子集来减少softmax开销，实现了最高2.28倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)的推理速度较慢，现有方法需要额外训练或修改模型架构。作者希望开发一种无需训练、即插即用的方法来加速dLLMs推理，同时保持模型精度。

Method: 1) 分析token unmasking置信度在块和步之间的动态特性；2) 基于平均置信度设计轻量级自适应方法，控制生成块大小、步长和阈值；3) 动态利用词汇表子集来减少softmax计算开销；4) 兼容基于KV缓存的dLLMs，无需训练。

Result: 在四个流行任务上的实验表明，CadLLM相比最先进的基线方法，实现了最高2.28倍的吞吐量提升，同时保持了有竞争力的准确率。

Conclusion: CadLLM是一种有效的训练免费方法，能够显著加速扩散大语言模型的推理速度，具有即插即用、模型无关的优点，为dLLMs的实际部署提供了实用解决方案。

Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.

</details>


### [476] [Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse](https://arxiv.org/abs/2512.07400)
*Giulia Lanzillotta,Damiano Meier,Thomas Hofmann*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文揭示了持续学习中深度特征空间遗忘与浅层分类器遗忘的差异，发现小缓冲区能有效防止深度遗忘，但需要大缓冲区来缓解浅层遗忘，并提出了通过修正统计伪影来减少缓冲区依赖的新思路。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的一个持久悖论：神经网络即使输出预测失败，仍能保留过去任务的线性可分表示。研究旨在理解深度特征空间遗忘与浅层分类器遗忘之间的差异，并解释经验回放中缓冲区大小对这两种遗忘的不同影响。

Method: 将神经崩溃框架扩展到顺序设置，分析深度遗忘作为几何漂移的特征，证明任何非零回放分数都能保证线性可分的保留。识别小缓冲区导致的"强崩溃"会导致协方差秩不足和类别均值膨胀，使分类器无法识别真实群体边界。

Result: 揭示了经验回放中的关键不对称性：最小缓冲区能成功锚定特征几何并防止深度遗忘，但缓解浅层遗忘通常需要更大的缓冲区容量。通过统一持续学习与分布外检测，挑战了对大缓冲区的依赖。

Conclusion: 通过显式修正统计伪影，可以在最小回放的情况下实现鲁棒性能，为持续学习提供了减少缓冲区依赖的新方向。

Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.

</details>


### [477] [FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.07539)
*Qingyuan Yang,Shizhuo,Dongyue Chen,Da Teng,Zehua Gan*

Main category: cs.LG

Relevance: 75.0

TL;DR: FRWKV是一种用于长序列时间序列预测的频率域线性注意力框架，通过结合线性注意力机制和频域分析，实现了O(T)计算复杂度，在八个真实数据集上取得了最佳平均排名。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长序列时间序列预测中存在两个主要瓶颈：1）二次方计算复杂度O(T²)，限制了处理长序列的能力；2）无法有效利用频域信息。受RWKV的线性注意力和频域建模启发，作者希望开发一个既能降低计算复杂度又能充分利用频域信息的框架。

Method: FRWKV框架整合了线性注意力机制和频域分析。线性注意力将计算复杂度从O(T²)降低到O(T)，频域编码器则通过谱分析增强时间特征表示。具体包括：1）基于RWKV的线性注意力架构；2）频率编码器组件，用于提取和利用频域信息；3）端到端的可扩展长序列建模框架。

Result: 在八个真实世界数据集上的实验表明，FRWKV取得了第一的平均排名。消融研究证实了线性注意力和频率编码器组件都起着关键作用，验证了线性注意力和频域分析之间的协同效应。

Conclusion: 这项工作展示了线性注意力和频域分析之间的强大协同作用，为可扩展的时间序列建模建立了新的范式。FRWKV通过O(T)计算复杂度和有效的频域信息利用，克服了传统Transformer在长序列预测中的局限性。

Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.

</details>


### [478] [A Bootstrap Perspective on Stochastic Gradient Descent](https://arxiv.org/abs/2512.07676)
*Hongjian Lan,Yucong Liu,Florian Schäfer*

Main category: cs.LG

Relevance: 75.0

TL;DR: SGD通过隐式正则化梯度协方差矩阵的迹来提升泛化能力，其随机性模拟了数据重采样的bootstrap过程，使模型对采样噪声更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 研究SGD相比确定性GD具有更好泛化能力的原因，从统计bootstrap角度理解SGD如何利用批次采样的梯度变异性来近似数据收集过程中的解变异性。

Method: 1. 理论分析：证明SGD通过隐式正则化梯度协方差矩阵的迹来控制算法变异性；2. 理想化实验：在经验风险最小化中展示SGD选择对重采样鲁棒的解；3. 神经网络实验：将算法变异性估计作为显式正则器验证效果。

Result: SGD被吸引到对重采样鲁棒的解，避免陷入训练损失更宽更深的虚假解。显式加入算法变异性正则器能提升测试性能，支持bootstrap估计是SGD泛化优势的基础。

Conclusion: SGD的泛化优势源于其隐式bootstrap过程，通过正则化梯度协方差来控制算法变异性，使解对采样噪声不敏感，从而提升泛化能力。

Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.

</details>


### [479] [Statistical analysis of Inverse Entropy-regularized Reinforcement Learning](https://arxiv.org/abs/2512.06956)
*Denis Belomestny,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

Relevance: 75.0

TL;DR: 本文提出了一个统计框架来解决逆熵正则化强化学习中的奖励函数非唯一性问题，通过结合熵正则化和软贝尔曼残差的最小二乘重构，得到唯一确定的最小二乘奖励函数。


<details>
  <summary>Details</summary>
Motivation: 经典逆强化学习（IRL）中长期存在奖励函数非唯一性的问题：许多不同的奖励函数都可以诱导出相同的最优策略，使得逆问题不适定。本文旨在通过统计框架解决这一模糊性问题。

Method: 结合熵正则化和软贝尔曼残差的最小二乘重构，提出最小二乘奖励函数。将专家演示建模为马尔可夫链，通过惩罚最大似然估计在动作空间的条件分布类中估计策略。使用覆盖数衡量策略类的统计复杂度。

Result: 建立了估计策略与专家策略之间超额KL散度的高概率界，揭示了平滑（熵正则化）、模型复杂度和样本量之间的相互作用。得到了最小二乘奖励函数的非渐近极小极大最优收敛率。

Conclusion: 该框架解决了IRL中的奖励函数非唯一性问题，在行为克隆、逆强化学习和现代统计学习理论之间建立了桥梁，为奖励函数推断提供了统计保证。

Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.

</details>


### [480] [Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation](https://arxiv.org/abs/2512.07472)
*Siyu Xu,Zijian Wang,Yunke Wang,Chenghao Xia,Tao Huang,Chang Xu*

Main category: cs.RO

Relevance: 75.0

TL;DR: 提出Affordance Field Intervention (AFI)框架，通过3D空间可操作性场引导VLA模型，解决其在分布偏移下的"记忆陷阱"问题，提升机器人操作的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作中表现良好，但在分布偏移场景下容易陷入"记忆陷阱"，即重复记忆的轨迹而非适应新场景。这源于端到端设计缺乏显式的3D空间推理能力，无法可靠识别陌生环境中的可操作区域。

Method: 提出AFI混合框架：1) 使用3D空间可操作性场(SAFs)作为即插即用模块提供几何表示；2) 通过本体感知检测记忆陷阱；3) 将机器人重新定位到高可操作性区域；4) 提出可操作性驱动的路径点来锚定VLA生成的动作；5) 使用基于SAF的评分器选择累积可操作性最高的轨迹。

Result: 在不同VLA骨干网络(π₀和π₀.₅)上，在真实机器人平台上OOD场景下平均提升23.5%，在LIBERO-Pro基准上提升20.2%，验证了AFI在增强VLA对分布偏移鲁棒性方面的有效性。

Conclusion: AFI框架通过引入显式的3D空间可操作性场，有效解决了VLA模型的记忆陷阱问题，显著提升了在分布偏移场景下的鲁棒性，为VLA模型提供了轻量级的空间推理增强方案。

Abstract: Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.

</details>


### [481] [When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models](https://arxiv.org/abs/2512.06062)
*S. M. Mustaqim,Anantaa Kotal,Paul H. Yi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种针对生成式模型合成数据的黑盒成员推理攻击，利用数据流形结构重叠泄露训练样本隐私信息


<details>
  <summary>Details</summary>
Motivation: 生成式模型被广泛用于生成隐私保护的合成数据，但现有方法主要关注样本级记忆，忽略了数据流形结构重叠可能导致的信息泄露风险

Method: 提出黑盒成员推理攻击：1) 重复查询生成模型获取大量合成样本；2) 无监督聚类识别合成分布密集区域；3) 分析聚类中心点和邻域作为训练样本代理；4) 利用这些代理推断成员信息或重建近似记录

Result: 在医疗、金融等敏感领域实验中，即使生成器使用差分隐私或其他噪声机制，真实与合成数据间的聚类重叠仍导致可测量的成员信息泄露

Conclusion: 合成数据生成管道存在未充分探索的攻击面，需要更强的隐私保证机制，不仅要考虑样本级记忆，还要考虑分布邻域推理

Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.

</details>


### [482] [Quantization Blindspots: How Model Compression Breaks Backdoor Defenses](https://arxiv.org/abs/2512.06243)
*Rohan Pandey,Eric Ye*

Main category: cs.LG

Relevance: 65.0

TL;DR: 量化（INT8/INT4）会严重削弱现有后门攻击防御方法的检测能力，而攻击成功率仍保持高位，暴露了防御评估与模型实际部署之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 研究量化对后门攻击防御的影响，因为实际部署中模型通常会被量化以减少内存和延迟，而现有防御方法主要在FP32模型上评估。

Method: 系统实证研究：在三个精度设置（FP32、INT8动态、INT4模拟）下，使用标准视觉基准测试（GTSRB、CIFAR-10），评估五种代表性防御方法对BadNet攻击的效果。

Result: INT8量化使所有防御方法的检测率降至0%，而攻击成功率仍高于99%。INT4量化效果因数据集而异：Neural Cleanse在GTSRB上有效但在CIFAR-10上失效，而攻击成功率仍高于90%。

Conclusion: 量化鲁棒性应成为未来后门防御评估和设计的必要维度，需解决防御评估（FP32）与模型实际部署（量化形式）之间的不匹配问题。

Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.

</details>


### [483] [Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics](https://arxiv.org/abs/2512.06301)
*Jihun Ahn,Gabriella Pasya Irianti,Vikram Thapar,Su-Mi Hur*

Main category: cs.LG

Relevance: 65.0

TL;DR: CI-LLM框架结合HAPPY分子表示和数值描述符，用于聚合物性质预测和逆向设计，相比SMILES模型推理速度提升3.5倍且精度更高


<details>
  <summary>Details</summary>
Motivation: 机器学习在无机化合物和小分子发现中已广泛应用，但聚合物领域仍难以应用。虽然数据稀缺常被认为是主要瓶颈，但研究表明通过战略性的分子表示可以克服这一限制。

Method: 提出CI-LLM框架：1) HAPPY表示法将化学亚结构编码为token；2) De³BERTa编码器结合数值描述符进行性质预测；3) GPT-based生成器进行逆向设计，实现多属性优化

Result: 性质预测方面：De³BERTa比SMILES模型推理速度快3.5倍，R²分数在四个性质上提升0.9-4.1%，提供亚组级别的可解释性。逆向设计方面：GPT生成器实现100%支架保留，成功优化负相关目标的多属性

Conclusion: 该综合框架展示了聚合物科学中机器学习的前向预测和逆向设计能力，证明了战略性分子表示在推进聚合物机器学习应用中的重要性

Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.

</details>


### [484] [LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing](https://arxiv.org/abs/2512.06351)
*Zhiying Yang,Fang Liu,Wei Zhang,Xin Lou,Malcolm Yoke Hean Low,Boon Ping Gan*

Main category: cs.LG

Relevance: 65.0

TL;DR: LUCA是一个结合大语言模型和图神经网络的强化学习框架，用于碳感知的柔性作业车间调度，在保持相同排放水平下实现更短的生产周期。


<details>
  <summary>Details</summary>
Motivation: 解决智能制造系统中动态可持续调度的挑战，需要在生产效率和碳排放之间取得平衡。传统调度方法难以同时处理结构特征和上下文语义信息。

Method: 1) 使用图神经网络和LLM结合，通过精心设计的提示策略生成融合嵌入，捕捉调度状态的结构特征和上下文语义；2) 深度强化学习策略网络处理嵌入，生成实时调度决策；3) 双目标奖励函数同时优化能源效率和调度及时性。

Result: 在合成数据集上，相比最佳对比算法平均降低4.1%的生产周期（最高达12.2%），同时保持相同排放水平；在公开数据集上，生产周期和排放均有额外改善。

Conclusion: LUCA框架有效实用，能够为智能制造中的碳感知调度提供优化解决方案，展示了LLM与图神经网络结合在复杂调度问题中的潜力。

Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.

</details>


### [485] [A new initialisation to Control Gradients in Sinusoidal Neural network](https://arxiv.org/abs/2512.06427)
*Andrea Combette,Antoine Venaille,Nelly Pustelnik*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种针对SIREN等正弦激活函数网络的新初始化方法，通过控制梯度和预激活分布来改善训练稳定性和泛化性能


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中，初始化策略对于缓解梯度爆炸或消失至关重要。然而，对于SIREN等具有正弦激活函数的网络，初始化参数的影响仍缺乏精确的理论理解。现有初始化方法可能导致训练不稳定和泛化能力差。

Method: 提出一种新的初始化方法，通过分析预激活分布的收敛性和Jacobian序列的方差，推导出参数的闭式表达式。该方法控制梯度并针对预激活消失，防止在估计过程中出现不适当的频率。同时通过神经正切核(NTK)框架分析初始化对训练动态的影响。

Result: 在函数拟合和图像重建任务上，使用新初始化的SIREN在广泛的图像重建任务中一致优于原始方案和其他基线方法，包括涉及物理信息神经网络的任务。

Conclusion: 提出的初始化方法通过精确控制梯度和预激活分布，显著改善了SIREN网络的训练稳定性和泛化性能，在多种重建任务中表现出色。

Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.

</details>


### [486] [Neural expressiveness for beyond importance model compression](https://arxiv.org/abs/2512.06440)
*Angelos-Christos Maroudis,Sotirios Xydis*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于"表达力"的神经网络剪枝新标准，强调神经元激活重叠的信息重分配能力，实现数据无关剪枝，相比权重重要性方法获得10倍参数压缩增益


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法主要依赖权重重要性，但忽略了神经元激活模式的信息重分配能力。本文旨在探索更基础的剪枝标准，解决"何时剪枝"问题，实现数据无关的压缩策略。

Method: 提出"表达力"作为新的剪枝标准，基于神经元激活重叠来衡量信息重分配能力。该方法与网络初始化状态强相关，独立于学习状态，可通过任意数据或有限代表性样本近似计算。支持与重要性剪枝的混合策略。

Result: 1) 表达力剪枝相比权重重要性方法获得10倍参数压缩增益，平均性能下降仅1%；2) 在YOLOv8上实现55.4%参数移除和46.1%MACs减少，COCO目标检测mAP50-95提升3%；3) 证明表达力独立使用时也优于现有剪枝方法。

Conclusion: 表达力作为新的剪枝标准，为神经网络压缩提供了更基础的理论基础，支持数据无关剪枝策略，并能与重要性剪枝互补，在保持性能的同时实现显著压缩效率提升。

Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.

</details>


### [487] [Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control](https://arxiv.org/abs/2512.06471)
*Nathan P. Lawrence,Ali Mesbah*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文分析了基于最优控制的目标条件强化学习，推导了经典二次目标与目标条件奖励之间的最优性差距，并将状态估计与概率奖励联系起来，在非线性不确定环境中验证了目标条件策略的优势。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习旨在训练智能体最大化到达目标状态的概率，但传统密集奖励（如二次奖励）在此任务中可能效果不佳。论文旨在从最优控制角度分析目标条件设置，解释为什么经典奖励会失败，以及目标条件奖励为何更有效。

Method: 1. 从最优控制理论出发，推导经典二次目标与目标条件奖励之间的最优性差距；2. 考虑部分可观测马尔可夫决策过程，将状态估计与概率奖励联系起来；3. 在非线性不确定环境中，结合强化学习和预测控制技术验证目标条件策略的优势。

Result: 1. 理论分析了目标条件奖励相对于经典密集奖励的优势；2. 建立了状态估计与概率奖励的理论联系；3. 在非线性不确定环境中验证了目标条件策略的有效性，特别是在双重控制问题中表现出色。

Conclusion: 目标条件强化学习从最优控制角度具有理论优势，特别适合部分可观测环境和双重控制问题。目标条件奖励比传统密集奖励更能有效处理不确定性，在非线性环境中表现更优。

Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.

</details>


### [488] [Vector Quantization using Gaussian Variational Autoencoder](https://arxiv.org/abs/2512.06609)
*Tongda Xu,Wendi Zheng,Jiajun He,Jose Miguel Hernandez-Lobato,Yan Wang,Ya-Qin Zhang,Jie Tang*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出Gaussian Quant (GQ)方法，将带约束的高斯VAE转换为VQ-VAE而无需训练，通过随机高斯噪声作为码本，并证明当码本大小超过高斯VAE的bits-back编码率时能保证小量化误差。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE将图像压缩为离散token，但由于离散化过程难以训练。现有方法训练复杂，需要设计码本和量化策略。

Method: 提出GQ方法：1) 生成随机高斯噪声作为码本；2) 找到最接近后验均值的噪声；3) 提出目标散度约束(TDC)启发式训练高斯VAE以优化GQ效果。

Result: GQ在UNet和ViT架构上优于VQGAN、FSQ、LFQ、BSQ等VQ-VAE方法，TDC也改进了TokenBridge等高斯VAE离散化方法。

Conclusion: GQ提供了一种简单有效的VQ-VAE构建方法，无需训练量化过程，理论保证量化误差小，实践效果优于现有方法。

Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.

</details>


### [489] [State Diversity Matters in Offline Behavior Distillation](https://arxiv.org/abs/2512.06692)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文揭示了离线行为蒸馏中原始数据集与合成数据集之间的不对齐问题，发现高状态多样性的数据集在训练损失较大时表现更好，并提出基于状态密度加权的改进算法SDW-OBD。


<details>
  <summary>Details</summary>
Motivation: 离线行为蒸馏能够将大量离线RL数据压缩为紧凑的合成行为数据集，但研究发现高质量原始数据集不一定产生优质合成数据集，存在不对齐问题。本文旨在理解这种不对齐现象的原因并提出改进方法。

Method: 1）通过实证分析不同训练损失水平下的策略性能，发现状态多样性与状态质量的重要性随训练损失变化；2）理论分析将状态质量和多样性分别与关键误差和周围误差关联；3）提出状态密度加权算法SDW-OBD，通过状态密度倒数加权蒸馏目标来增强状态多样性。

Result: 在多个D4RL数据集上的实验表明，当原始数据集状态多样性有限时，SDW-OBD能显著提升离线行为蒸馏的性能，验证了状态多样性在OBD场景中的重要性。

Conclusion: 离线行为蒸馏中状态多样性比状态质量更重要，特别是在训练损失较大的情况下。提出的SDW-OBD算法通过强调状态多样性有效解决了原始与合成数据集的不对齐问题。

Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.

</details>


### [490] [Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets](https://arxiv.org/abs/2512.06752)
*Chang Liu,Vivian Li,Linus Leong,Vladimir Radenkovic,Pietro Liò,Chaitanya K. Joshi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文提出了Geometric Graph U-Nets，一种用于3D蛋白质结构学习的新型几何图神经网络架构，通过递归粗化和细化蛋白质图来学习多尺度表示，在蛋白质折叠分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前几何图神经网络和Transformer在3D蛋白质结构学习中存在局限性，它们依赖消息传递机制，无法捕捉控制蛋白质功能的层次化相互作用（如全局域和长程变构调节）。作者认为网络架构本身应该反映这种生物层次结构。

Method: 提出Geometric Graph U-Nets，这是一种新的模型类别，通过递归粗化和细化蛋白质图来学习多尺度表示。该层次化设计理论上比标准几何GNN更具表达能力。

Result: 在蛋白质折叠分类任务上，Geometric U-Nets显著优于不变和等变基线模型，证明了它们学习定义蛋白质折叠的全局结构模式的能力。

Conclusion: 这项工作为设计能够学习生物分子多尺度结构的几何深度学习架构提供了原则性基础。

Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.

</details>


### [491] [Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation](https://arxiv.org/abs/2512.06993)
*Ali Ebrahimpour-Boroojeny*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了两种新的机器学习遗忘方法：AMUN用于样本遗忘，通过对抗性微调降低模型对遗忘样本的置信度；TRW用于类别遗忘，通过倾斜重加权分布近似重新训练模型的行为。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习遗忘方法在保护隐私和遵守数据删除法规方面存在不足，特别是在防止成员推理攻击和准确模拟重新训练模型行为方面表现不佳。需要更有效的遗忘方法来确保模型在删除特定数据后不会泄露隐私信息。

Method: 1. AMUN方法：通过生成对抗样本来微调模型，降低模型对遗忘样本的置信度，同时保持对保留样本的性能。2. FastClip方法：通过层谱范数裁剪控制模型的Lipschitz常数，提高模型平滑性。3. TRW方法：通过估计类间相似性并倾斜目标分布，近似重新训练模型在剩余类别上的分布。

Result: AMUN在图像分类任务中超越了现有的最先进方法，基于SOTA MIA评分。TRW在多个基准测试中匹配或超越了现有遗忘方法。理论分析表明模型平滑性是影响AMUN性能的关键因素。

Conclusion: 该研究提出了有效的机器学习遗忘方法，AMUN通过对抗性微调实现样本遗忘，TRW通过分布倾斜实现类别遗忘，两者都能更好地保护隐私并模拟重新训练模型的行为。

Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.

</details>


### [492] [Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search](https://arxiv.org/abs/2512.07142)
*Tanay Arora,Christof Teuscher*

Main category: cs.LG

Relevance: 65.0

TL;DR: CTS是一种高效寻找神经网络稀疏子网络的方法，通过组合优化和梯度平衡技术，在初始化阶段快速找到高性能的"中奖彩票"，计算效率远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有彩票假设方法存在计算效率问题：Lottery Ticket Rewinding计算成本高，而基于显著性的初始化剪枝方法在准确率-稀疏度权衡上表现不佳，且无法通过基本合理性检查。这些方法依赖一阶显著性指标，忽略了权重间的相互依赖关系。

Method: 提出Concrete Ticket Search算法，将子网络发现建模为组合优化问题。使用Concrete松弛处理离散搜索空间，引入GRADBALANCE梯度平衡方案控制稀疏度。进一步提出基于知识蒸馏的剪枝目标，特别是最小化稀疏网络与密集网络输出之间的反向KL散度。

Result: 在图像分类任务上，CTS生成的子网络能通过合理性检查，准确率与LTR相当或更好，计算时间大幅减少。例如在ResNet-20/CIFAR10上，达到99.3%稀疏度时准确率74.0%（LTR为68.3%），计算时间仅7.9分钟（LTR需95.2分钟）。

Conclusion: CTS通过整体组合优化方法有效解决了现有彩票假设方法的计算效率和性能问题，特别是在高稀疏度区域优势明显，为高效神经网络剪枝提供了新思路。

Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

</details>


### [493] [Pay Less Attention to Function Words for Free Robustness of Vision-Language Models](https://arxiv.org/abs/2512.07222)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Chao Shen*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出Function-word De-Attention (FDA)方法，通过从原始交叉注意力中减去功能词交叉注意力，增强视觉语言模型对抗跨模态攻击的鲁棒性，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 针对鲁棒视觉语言模型(VLM)中鲁棒性与性能之间的权衡问题，研究发现功能词是VLM在跨模态对抗攻击中的脆弱点，需要一种方法来减轻功能词的影响。

Method: 提出Function-word De-Attention (FDA)，类似于差分放大器，计算原始交叉注意力和功能词交叉注意力，然后将后者从前者中差分减去，以获得更对齐和鲁棒的VLM。

Result: 在3个模型、2个下游任务、3个数据集上的实验显示：检索任务平均攻击成功率(ASR)下降18/13/53%，性能仅下降0.2/0.3/0.6%；视觉定位任务ASR下降90%，性能提升0.3%。

Conclusion: FDA方法能有效提升视觉语言模型对抗跨模态攻击的鲁棒性，同时保持或轻微提升性能，具有良好的可扩展性、泛化性和零样本性能。

Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

</details>


### [494] [UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.07184)
*Da Zhang,Bingyu Li,Zhuyuan Zhao,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

Relevance: 65.0

TL;DR: UniDiff：用于多模态时间序列预测的统一扩散框架，通过跨模态融合和时间序列补丁化处理，在8个领域基准数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据（文本和时间戳）日益增多，但现有扩散模型在时间序列预测中主要局限于单模态数值序列，忽略了复杂异构数据中的跨模态信号

Method: 1) 将时间序列补丁化并通过轻量MLP映射到嵌入空间；2) 统一并行融合模块使用单一交叉注意力机制自适应加权整合时间戳结构信息和文本语义信息；3) 为多源条件设计新颖的无分类器引导机制

Result: 在8个领域真实世界基准数据集上的广泛实验表明，UniDiff模型实现了最先进的性能

Conclusion: UniDiff通过统一扩散框架有效解决了多模态时间序列预测问题，实现了跨模态信息的高效融合和灵活控制

Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.

</details>


### [495] [Geometric Prior-Guided Federated Prompt Calibration](https://arxiv.org/abs/2512.07208)
*Fei Luo,Ziwei Zhao,Mingxuan Wang,Duoyang Li,Zhe Qian,Jiayi Tuo,Chenyue Zhou,Yanbiao Ma*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出GGTPC框架，通过全局几何先验校正联邦提示学习中的数据异构性导致的局部训练偏差，在标签偏斜和领域偏斜数据集上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 联邦提示学习(FPL)虽然提供参数高效的协作训练方案，但数据异构性导致局部训练的提示产生偏差，现有方法未能从根本上解决这一偏差问题

Method: 提出几何引导文本提示校准(GGTPC)框架：1) 服务器端通过协方差矩阵重构全局数据分布的几何先验；2) 客户端使用几何先验校准层(GPCL)将局部特征分布与全局先验对齐

Result: 在标签偏斜的CIFAR-100数据集(β=0.1)上超越SOTA 2.15%；在极端偏斜(β=0.01)下比基线提升9.17%；在领域偏斜的Office-Home数据集上作为即插即用模块提升FedAvg性能4.60%

Conclusion: GGTPC通过校正局部训练偏差有效缓解数据异构性问题，可作为通用模块增强各种联邦学习算法

Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.

</details>


### [496] [IFFair: Influence Function-driven Sample Reweighting for Fair Classification](https://arxiv.org/abs/2512.07249)
*Jingran Yang,Min Zhang,Lingfeng Zhang,Zhaohui Wang,Yonggang Zhang*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于影响函数的预处理公平性方法IFFair，通过动态调整训练样本权重来缓解机器学习模型中的偏见，无需修改网络结构或数据特征。


<details>
  <summary>Details</summary>
Motivation: 机器学习在辅助或替代人类决策时，基于数据的模式会学习甚至加剧样本中的潜在偏见，导致对弱势群体的歧视性决策，损害社会福利并阻碍应用发展。

Method: 基于影响函数的预处理方法IFFair，使用不同群体训练样本的影响差异作为指导，在训练过程中动态调整样本权重，不修改网络结构、数据特征和决策边界。

Result: 在多个真实数据集和指标上的实验表明，IFFair在分类设置中缓解了人口统计均等、机会均等、错误率均等等多个公平性指标的偏见，且在这些指标间没有冲突。相比其他预处理方法，IFFair在效用和公平性指标间取得了更好的权衡。

Conclusion: IFFair是一种有效的预处理公平性方法，能够在不修改模型结构的情况下，通过动态调整样本权重来缓解机器学习模型中的偏见问题。

Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.

</details>


### [497] [Towards a Relationship-Aware Transformer for Tabular Data](https://arxiv.org/abs/2512.07310)
*Andrei V. Konstantinov,Valerii A. Zuev,Lev V. Utkin*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于改进注意力机制的模型，用于处理表格数据中的样本间依赖关系，在回归和因果效应估计任务中表现优于梯度提升决策树


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型处理表格数据时无法利用样本间的外部依赖关系图，而图神经网络只考虑相邻节点，难以应用于稀疏图。需要一种能有效利用样本间相关性的方法，特别是在因果效应估计等任务中。

Method: 提出基于改进注意力机制的解决方案，通过在注意力矩阵中添加额外项来考虑数据点之间的可能关系。在合成和真实数据集上进行回归任务评估，并在IHDP数据集上进行因果效应估计任务评估。

Result: 提出的模型在回归任务中相互比较并与梯度提升决策树对比，在因果效应估计任务中在IHDP数据集上进行了评估，显示出优于传统方法的性能。

Conclusion: 基于改进注意力机制的模型能有效处理表格数据中的样本间依赖关系，在回归和因果效应估计任务中表现出色，为利用外部依赖关系图提供了有效解决方案。

Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.

</details>


### [498] [Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood](https://arxiv.org/abs/2512.07390)
*Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang*

Main category: cs.LG

Relevance: 65.0

TL;DR: SICL是一个用于测试时适应的不确定性校准框架，通过风格不变性来估计实例级正确性似然，无需反向传播，可即插即用


<details>
  <summary>Details</summary>
Motivation: 测试时适应（TTA）在部署模型时存在预测不确定性校准不佳的问题，这在自动驾驶、金融和医疗等高风险领域尤为关键。现有校准方法通常假设固定模型或静态分布，在现实动态测试条件下性能下降。

Method: 提出SICL框架，利用风格不变性进行鲁棒的不确定性估计。通过测量预测在风格变化变体之间的一致性来估计实例级正确性似然，仅需模型前向传播，无需反向传播。

Result: 在4个基线、5种TTA方法和2个现实场景、3种模型架构的综合评估中，SICL相比传统校准方法平均减少13个百分点的校准误差。

Conclusion: SICL提供了一种有效的方法来解决TTA中的不确定性校准问题，具有即插即用、无需反向传播的优点，适用于高风险领域的实际应用。

Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

</details>


### [499] [KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models](https://arxiv.org/abs/2512.07437)
*Chenwei Shi,Xueyu Luan*

Main category: cs.LG

Relevance: 65.0

TL;DR: KAN-Dreamer：将Kolmogorov-Arnold Networks（KANs）集成到DreamerV3 MBRL框架中，用KAN/FastKAN替换特定MLP和卷积组件，在DeepMind Control Suite上验证性能与原始MLP架构相当。


<details>
  <summary>Details</summary>
Motivation: 探索将新兴的KAN架构（具有参数效率和可解释性优势）集成到先进的模型强化学习框架DreamerV3中，以结合两者的优势，同时通过FastKAN变体解决KAN计算开销问题。

Method: 提出KAN-Dreamer，在DreamerV3的三个子系统（视觉感知、潜在预测、行为学习）中替换特定MLP和卷积组件为KAN/FastKAN层。为JAX世界模型实现完全向量化版本和简化网格管理。在DeepMind Control Suite（walker_walk）上进行实证评估。

Result: 实验结果表明，使用适应的FastKAN作为奖励和继续预测器的直接替代品，在样本效率、训练速度和渐近性能方面与原始MLP架构表现相当，保持了性能对等性。

Conclusion: KAN架构可以成功集成到DreamerV3框架中而不损失性能，这为未来基于KAN的世界模型发展提供了初步研究基础，展示了KAN在强化学习中的潜力。

Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.

</details>


### [500] [Forget and Explain: Transparent Verification of GNN Unlearning](https://arxiv.org/abs/2512.07450)
*Imran Ahsan,Hyunwook Yu,Jinsung Kim,Mucheol Kim*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种基于可解释性的GNN遗忘验证器，通过对比遗忘前后的模型快照，使用归因偏移和局部结构变化作为透明证据来验证信息是否真正被遗忘。


<details>
  <summary>Details</summary>
Motivation: GNN在隐私法规（如GDPR）下需要能够"遗忘"特定信息，但现有遗忘方法缺乏透明度，难以验证遗忘是否真正发生。黑盒性质的GNN使得验证变得困难，需要透明、可验证的遗忘证据。

Method: 提出可解释性驱动的验证器，在删除前后对模型进行快照，使用五种可解释性指标：残差归因、热图偏移、可解释性分数偏差、图编辑距离和诊断图规则偏移。评估了两种骨干网络（GCN、GAT）和四种遗忘策略（Retrain、GraphEditor、GNNDelete、IDEA）。

Result: Retrain和GNNDelete实现了近乎完全的遗忘，GraphEditor提供部分擦除，IDEA留下残差信号。可解释性差异提供了主要的、人类可读的遗忘证据，同时报告成员推断ROC-AUC作为补充的图范围隐私信号。

Conclusion: 提出的可解释性验证器为GNN遗忘提供了透明、可验证的证据，解决了现有方法缺乏透明度的问题，有助于满足隐私法规要求，并为遗忘效果提供了人类可读的评估。

Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.

</details>


### [501] [Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces](https://arxiv.org/abs/2512.07509)
*Nikita Gabdullin*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出使用预定义向量系统（如An根系统向量）作为潜在空间配置目标，以优化神经网络嵌入分布，从而加速大规模分类任务的训练，并减少向量数据库存储需求。


<details>
  <summary>Details</summary>
Motivation: 神经网络性能与其潜在空间中的嵌入分布特性密切相关。传统分类层在大规模类别数据集上训练效率低下，需要更高效的潜在空间配置方法来加速训练并优化嵌入存储。

Method: 提出使用预定义向量系统（特别是An根系统向量）作为潜在空间配置目标，训练不带分类层的分类器神经网络。探讨了不同向量系统的构建方法，并将其应用于编码器和视觉Transformer的潜在空间配置。

Result: 在ImageNet-1K和50k-600k类别的大规模数据集上显著加速了训练收敛。同时发现，针对特定类别数使用最小潜在空间维度可以进一步加速收敛，并减少向量数据库的存储需求。

Conclusion: 预定义向量系统为神经网络潜在空间配置提供了有效的理论框架，特别适用于大规模分类任务，能够显著提升训练效率并优化嵌入存储。

Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.

</details>


### [502] [Model-Based Reinforcement Learning Under Confounding](https://arxiv.org/abs/2512.07528)
*Nishanth Venkatesh,Andreas A. Malikopoulos*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了一种在上下文未观测的混淆环境中进行基于模型的强化学习方法，通过代理变量识别混淆奖励期望，结合行为平均转移模型构建替代MDP，实现一致的策略评估和学习。


<details>
  <summary>Details</summary>
Motivation: 在上下文马尔可夫决策过程（C-MDPs）中，当上下文未观测时会导致离线数据集中的混淆问题。传统的模型学习方法在这种情况下存在根本性不一致，因为行为策略下生成的转移和奖励机制与评估状态策略所需的干预量不对应。

Method: 采用近端离策略评估方法，利用代理变量的可逆性条件识别混淆奖励期望；结合行为平均转移模型构建替代MDP；将这种方法与最大因果熵（MaxCausalEnt）模型学习框架集成。

Result: 构建的替代MDP的Bellman算子对于状态策略是定义良好且一致的，能够在上下文信息未观测、不可用或难以收集的混淆环境中实现原则性的模型学习和规划。

Conclusion: 该方法解决了混淆环境中基于模型的强化学习的根本不一致问题，为上下文信息缺失情况下的策略评估和学习提供了理论保证和实用框架。

Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.

</details>


### [503] [Time Series Foundation Models for Process Model Forecasting](https://arxiv.org/abs/2512.07624)
*Yongbo Yu,Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究了时间序列基础模型（TSFMs）在过程模型预测（PMF）中的应用，发现预训练的时间序列基础模型在零样本设置下比传统方法表现更好，微调带来的改进有限。


<details>
  <summary>Details</summary>
Motivation: 过程模型预测（PMF）旨在预测业务流程控制流结构随时间的变化，但现有机器学习方法由于DF时间序列的稀疏性和异质性，相比统计基线改进有限。研究者希望探索时间序列基础模型作为替代方案，利用其预训练知识来提升PMF性能。

Method: 使用真实事件日志生成的DF时间序列，比较时间序列基础模型的零样本使用（无额外训练）与在PMF数据上微调的变体。与传统方法和专门模型进行对比，评估MAE和RMSE等指标。

Result: 时间序列基础模型通常比在相同日志上从头训练的传统和专门模型获得更低的预测误差，表明从非过程领域有效转移了时间结构知识。微调虽然能进一步提高准确性，但改进通常很小，在较小或更复杂的数据集上可能消失。

Conclusion: 时间序列基础模型在过程相关时间序列任务上展现出良好的泛化能力和数据效率，零样本使用是一个强大的默认选择。这是首次系统评估时间基础模型在PMF中的应用。

Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.

</details>


### [504] [The Adoption and Usage of AI Agents: Early Evidence from Perplexity](https://arxiv.org/abs/2512.07828)
*Jeremy Yang,Noah Yonack,Kate Zyskowski,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

Relevance: 65.0

TL;DR: 首个大规模AI智能体在开放网络环境中的实地研究，基于Perplexity的Comet浏览器及其助手，分析数百万用户交互数据，揭示用户群体、使用强度和使用场景的异质性。


<details>
  <summary>Details</summary>
Motivation: 了解通用AI智能体在真实世界中的采用情况、使用强度和使用场景，填补大规模实地研究的空白，为AI能力扩散提供实证基础。

Method: 基于Perplexity的Comet浏览器及其Comet Assistant智能体，收集数百万匿名用户交互数据，采用分层智能体分类法（主题-子主题-任务三级）系统分析使用场景。

Result: 发现用户采用和使用存在显著异质性：早期采用者、高GDP国家用户、知识密集型行业从业者更活跃。57%查询集中在生产力与工作流、学习与研究两大主题，个人使用占55%，专业和教育场景分别占30%和16%。使用场景短期稳定，长期向认知导向主题转移。

Conclusion: AI智能体扩散对研究者、企业、政策制定者和教育者具有重要启示，需要进一步研究这一新兴AI能力类别。

Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.

</details>


### [505] [ADAM Optimization with Adaptive Batch Selection](https://arxiv.org/abs/2512.06795)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

Relevance: 65.0

TL;DR: 提出AdamCB优化器，将组合多臂老虎机采样技术集成到Adam中，通过自适应选择训练样本来加速收敛


<details>
  <summary>Details</summary>
Motivation: 传统Adam优化器对所有数据样本平等对待，但不同样本对模型更新的影响程度不同，这导致收敛效率低下。现有的基于老虎机的Adam变体虽然尝试自适应采样，但理论保证有限。

Method: 提出Adam with Combinatorial Bandit Sampling (AdamCB)，将组合老虎机技术集成到Adam优化器中。该方法能够同时利用多个样本的反馈，而不是单个样本，从而增强采样效率。

Result: 理论分析显示AdamCB比包括先前老虎机变体在内的Adam方法收敛更快。数值实验证明AdamCB在多个任务上一致优于现有方法。

Conclusion: AdamCB通过组合老虎机采样技术有效解决了Adam优化器的收敛效率问题，提供了更好的理论保证和实践性能。

Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.

</details>


### [506] [MINES: Explainable Anomaly Detection through Web API Invariant Inference](https://arxiv.org/abs/2512.06906)
*Wenjie Zhang,Yun Lin,Chun Fung Amos Kwok,Xiwen Teoh,Xiaofei Xie,Frank Liauw,Hongyu Zhang,Jin Song Dong*

Main category: cs.SE

Relevance: 65.0

TL;DR: MINES：基于LLM的Web API异常检测系统，通过从模式层面推断可解释的API不变量来检测异常，而非依赖原始日志实例。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用依赖API（RESTful、SOAP、WebSockets），其暴露易受攻击或非法访问，导致系统行为异常。现有日志学习方案易受噪声干扰，学习虚假相关性，且日志可能缺失关键信息（如数据库信息），难以区分正常与异常日志。

Method: 1) 将API签名转换为表模式以增强原始数据库模式；2) 基于增强模式推断潜在数据库约束，捕捉API与数据库表间关系；3) 使用LLM基于两个表结构提取潜在关系；4) 用正常日志实例接受/拒绝LLM生成的不变量；5) 将约束转换为不变量并生成Python代码验证运行时日志。

Result: 在TrainTicket、NiceFish、Gitea、Mastodon、NextCloud等基准上评估，对抗LogRobust、LogFormer、WebNorm等基线。MINES在异常检测上实现高召回率，同时引入几乎零误报，达到新的SOTA。

Conclusion: MINES通过从模式层面推断可解释API不变量，能显著区分日志噪声、识别精确正常性，并检测超出日志记录的异常行为，为Web应用异常检测提供新方法。

Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.

</details>


### [507] [PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios](https://arxiv.org/abs/2512.06950)
*Enrico Camporeale*

Main category: stat.ML

Relevance: 65.0

TL;DR: PARIS提出了一种基于表示定理的数据集剪枝框架，通过计算闭式表示器删除残差来量化单个训练点对验证损失的影响，从而高效地剪除无信息或性能下降的样本，解决不平衡回归问题。


<details>
  <summary>Details</summary>
Motivation: 传统经验风险最小化(ERM)在不平衡回归中偏向高频数据区域，导致对罕见但高影响的"尾部"事件性能严重下降。现有方法如损失重加权或合成过采样会引入噪声、扭曲底层分布或增加算法复杂度。

Method: PARIS利用神经网络表示定理计算闭式表示器删除残差，量化移除单个训练点对验证损失的确切影响（无需重新训练）。结合高效的Cholesky秩一下降方案，实现快速迭代剪枝，消除无信息或性能下降的样本。

Result: 在真实空间天气案例中，PARIS将训练集减少高达75%，同时保持或改善整体RMSE，优于重加权、合成过采样和提升基线方法。

Conclusion: 表示器引导的数据集剪枝是一种强大、可解释且计算高效的方法，可用于罕见事件回归问题。

Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.

</details>


### [508] [Ideal Attribution and Faithful Watermarks for Language Models](https://arxiv.org/abs/2512.07038)
*Min Jae Song,Kameron Shahabi*

Main category: cs.CR

Relevance: 65.0

TL;DR: 论文提出了理想归因机制作为字符串归因决策的形式化抽象，核心是记录模型与用户交互历史的账本，将水印方案设计目标框定为对理想归因机制的忠实表示。


<details>
  <summary>Details</summary>
Motivation: 当前水印方案缺乏统一的理论框架，保证声明零散且概率性。需要建立形式化抽象来清晰描述归因决策，为水印方案提供理论基础和评估标准。

Method: 提出理想归因机制形式化框架，核心是记录提示-响应交互历史的账本。机制基于账本和明确选择标准产生确定性决策，可作为归因的基准真值。将水印方案设计目标定义为对理想归因机制的忠实表示。

Result: 建立了统一的理论框架，用确定性语言描述水印方案保证，而非零散的概率声明。能够精确推理未来水印方案的期望特性，即使当前构造尚未实现。提供了清晰的研究路线图，区分理想化设置中可实现的保证与实际值得追求的目标。

Conclusion: 理想归因机制框架为水印方案提供了概念清晰的理论基础，统一了保证描述语言，能够指导未来水印方案的设计和评估，区分理想化目标与实际可实现目标。

Abstract: We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.

</details>


### [509] [PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning](https://arxiv.org/abs/2512.07342)
*Chen Gong,Zheng Liu,Kecen Li,Tianhao Wang*

Main category: cs.CR

Relevance: 65.0

TL;DR: 提出首个差分隐私离线强化学习数据集合成方法PrivORL，使用扩散模型和扩散Transformer分别合成转换和轨迹，通过DP-SGD保护隐私，并引入好奇心驱动预训练提升多样性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通过共享预收集数据集训练模型，避免了与环境直接交互，但在导航等关键领域应用中存在隐私泄露风险。需要保护离线RL数据集中的私有信息。

Method: PrivORL采用扩散模型合成转换，扩散Transformer合成轨迹。先在公共数据集上预训练合成器，然后在敏感数据集上使用差分隐私随机梯度下降（DP-SGD）进行微调。引入好奇心驱动预训练，利用好奇心模块的反馈使合成数据集多样化。

Result: 在五个敏感离线RL数据集上的实验表明，该方法在DP转换和轨迹合成方面比基线方法具有更好的效用和保真度。

Conclusion: PrivORL是首个差分隐私离线RL数据集合成方法，能安全发布合成数据集用于下游分析和研究，在隐私保护和数据效用之间取得了良好平衡。

Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.

</details>


### [510] [Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation](https://arxiv.org/abs/2512.07650)
*Fuyuan Lyu,Zhentai Chen,Jingyan Jiang,Lingjie Li,Xing Tang,Xiuqiang He,Xue Liu*

Main category: cs.IR

Relevance: 65.0

TL;DR: 该论文提出在推荐系统中应用测试时扩展（test-time scaling）方法，通过异构模型架构或同构模型随机初始化生成多样化输出，在相同推理预算下超越参数扩展效果。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统主要关注训练时的参数扩展，但测试时的计算资源高效利用和扩展尚未充分探索。测试时扩展在语言模型领域已被证明有效，但如何将其应用于推荐系统仍面临挑战，关键在于为同一实例生成多样且有意义的输出。

Method: 提出两种测试时扩展方法：1）利用不同模型架构的异构性；2）在同构架构下利用模型初始化的随机性。在三个基准测试上评估了八个模型（包括经典和SOTA模型），并在相同推理预算下与参数扩展进行对比。

Result: 两种解决方案均被证明有效。在相同推理预算下，测试时扩展能够超越参数扩展。此外，测试时扩展可以在线部署时随着并行服务器增加而无缝加速，且不影响用户端的推理时间。

Conclusion: 测试时扩展为推荐系统提供了一种正交的扩展方法，能够高效利用计算资源，在相同推理预算下获得更好的性能，并且具有良好的可扩展性和部署灵活性。

Abstract: Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.

</details>


### [511] [Distribution-informed Online Conformal Prediction](https://arxiv.org/abs/2512.07770)
*Dongjian Hu,Junxi Wu,Shu-Tao Xia,Changliang Zou*

Main category: stat.ML

Relevance: 65.0

TL;DR: 提出Conformal Optimistic Prediction (COP)算法，通过纳入数据模式来生成更紧凑的预测集，在保持覆盖保证的同时减少保守性


<details>
  <summary>Details</summary>
Motivation: 现有在线共形预测方法在完全对抗环境中处理数据分布偏移时过于保守，产生过大的预测集。需要一种既能适应数据模式又能保持有效覆盖保证的方法。

Method: COP算法通过非一致性分数的累积分布函数估计，将底层数据模式纳入更新规则。当存在可预测模式时产生更紧凑的预测集，即使估计不准确也能保持覆盖保证。

Result: 建立了覆盖和遗憾的联合边界，证明了COP在任意学习率下具有分布无关的有限样本覆盖，当分数i.i.d.时能够收敛。实验显示COP能实现有效覆盖且构造更短的预测区间。

Conclusion: COP算法成功解决了在线共形预测中的保守性问题，通过纳入数据模式实现了更紧凑的预测集，同时保持了理论上的覆盖保证。

Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.

</details>


### [512] [JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning](https://arxiv.org/abs/2512.06102)
*Ufuk Çakır,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

Relevance: 45.0

TL;DR: JaxWildfire：基于JAX的高性能野火模拟器，通过向量化GPU加速实现6-35倍速度提升，支持梯度优化和强化学习训练


<details>
  <summary>Details</summary>
Motivation: 现有野火模拟器速度慢，限制了强化学习在自然灾害管理中的应用。需要高性能模拟器来训练RL代理，以改进不确定决策场景中的野火管理策略。

Method: 基于元胞自动机的概率野火传播模型，使用JAX实现，通过vmap实现向量化模拟，支持GPU加速和高吞吐量仿真。

Result: JaxWildfire比现有软件快6-35倍，支持基于梯度的模拟器参数优化，并能用于训练RL代理学习野火抑制策略。

Conclusion: 该工作为推进RL技术在自然灾害管理中的应用迈出了重要一步，提供了高效、可扩展的模拟环境。

Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.

</details>


### [513] [Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph](https://arxiv.org/abs/2512.06236)
*Haiyang Yu,Meng-Chieh Lee,Xiang song,Qi Zhu,Christos Faloutsos*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出GraphDeT框架，通过引入边去噪辅助任务来提升图神经网络在领域自适应中的节点分类性能，理论分析表明该任务能收紧泛化边界，实验在时间和区域领域偏移场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 图领域自适应中，由于数据采集时间或区域不同导致结构域偏移，使得图神经网络在目标图上的性能下降。研究发现简单的边去噪辅助任务能有效提升目标图上的泛化能力。

Method: 提出GraphDeT框架，在图神经网络训练中集成边去噪辅助任务。通过理论分析连接辅助任务与基于-divergence的图泛化边界，证明该任务能收紧边界从而改善泛化。

Result: 实验结果表明，在处理时间和区域领域图偏移时，GraphDeT相比现有基线方法表现出优越性能。

Conclusion: 简单的边去噪辅助任务能有效提升图神经网络在领域自适应中的泛化能力，GraphDeT框架为处理图结构域偏移提供了有效解决方案。

Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.

</details>


### [514] [Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning](https://arxiv.org/abs/2512.06250)
*Chris Tava*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出一种强化学习技术，用于学习两个正交导航策略之间的切换阈值，通过自适应切换系统探索和目标导向路径规划来提升迷宫导航性能。


<details>
  <summary>Details</summary>
Motivation: 自主代理通常需要多种策略来解决复杂任务，但确定何时在策略之间切换仍然具有挑战性。现有方法使用固定阈值或手工启发式方法，缺乏适应性。

Method: 使用Q-learning学习切换阈值，基于覆盖百分比和目标距离的状态离散化。代理不需要墙壁位置、最优阈值或手工启发式的先验知识，仅需迷宫尺寸和目标位置。

Result: 在240个测试配置中，自适应阈值学习优于单策略代理和固定40%阈值基线：完成时间提升23-55%，运行时间方差减少83%，最坏情况改善71%。性能增益随问题复杂度增加。

Conclusion: 自适应策略切换在复杂任务中优于固定启发式方法，随着问题空间增长，自适应策略选择的价值成比例增加。该方法可推广到未见过的迷宫配置。

Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.

</details>


### [515] [Theoretical Compression Bounds for Wide Multilayer Perceptrons](https://arxiv.org/abs/2512.06288)
*Houssam El Cheairi,David Gamarnik,Rahul Mazumder*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文提出了一种随机贪婪压缩算法，用于后训练剪枝和量化，并理论证明了多层感知机（MLP）和卷积神经网络（CNN）中存在性能有竞争力的剪枝/量化子网络，揭示了压缩性与网络宽度之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 剪枝和量化技术在实践中能有效减少大型神经网络参数，但其经验成功的理论依据不足。本文旨在为这些压缩技术的经验成功提供理论支撑，弥合理论与应用之间的差距。

Method: 提出随机贪婪压缩算法，用于后训练剪枝和量化。该算法类似于Optimal Brain Damage（OBD）的随机后训练版本，分析扩展到MLP的结构化剪枝和CNN，建立无数据假设的理论框架。

Result: 理论证明了宽网络中存在性能有竞争力的剪枝/量化子网络，展示了压缩性与网络宽度之间的权衡关系，为压缩技术在宽多层感知机中的经验成功提供了理论依据。

Conclusion: 该研究为剪枝和量化的经验成功提供了理论支撑，提出的算法和分析框架弥合了理论与应用之间的差距，对神经网络压缩技术有重要理论意义。

Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.

</details>


### [516] [Angular Regularization for Positive-Unlabeled Learning on the Hypersphere](https://arxiv.org/abs/2512.06785)
*Vasileios Sevetlidis,George Pavlidis,Antonios Gasteratos*

Main category: cs.LG

Relevance: 45.0

TL;DR: AngularPU：一种基于角度相似度和原型向量的正例-未标记学习框架，通过余弦相似度阈值进行分类，无需显式负例建模，在高维稀疏正例场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 正例-未标记学习面临传统方法依赖强分布假设或在复杂场景下失效的问题，需要一种更稳健、可扩展且无需显式负例建模的解决方案。

Method: 在单位超球面上使用余弦相似度和角度间隔，学习一个正类原型向量，通过阈值化嵌入向量与原型的余弦相似度进行分类。引入角度正则化器防止未标记嵌入聚集在正类原型附近。

Result: 在基准数据集上达到或超越现有PU方法，尤其在正例稀缺和高维嵌入场景中表现突出，同时提供几何可解释性和可扩展性。

Conclusion: AngularPU为PU学习提供了理论保证的稳健解决方案，通过角度几何方法有效处理高维稀疏数据，具有实际应用价值。

Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.

</details>


### [517] [Prediction with Expert Advice under Local Differential Privacy](https://arxiv.org/abs/2512.06971)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文研究在本地差分隐私约束下的专家建议预测问题，提出了两种新算法：RW-AdaBatch通过隐私放大机制在简单数据上表现更好，RW-Meta能够隐私地选择非平凡学习算法作为专家，并在COVID-19医院数据预测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究在本地差分隐私约束下的在线学习问题，特别是在专家建议预测框架中。现有方法主要考虑数据无关的专家，而实际应用中专家可能是复杂的学习算法。同时，本地差分隐私的切换限制特性可能带来隐私放大的机会。

Method: 1. 首先证明经典算法自然满足LDP；2. 提出RW-AdaBatch算法，利用LDP诱导的有限切换行为实现隐私放大，这种放大在简单数据上更强；3. 提出RW-Meta算法，开发了隐私选择非平凡学习算法专家的通用方法，在LDP下不增加额外隐私成本；4. 基于随机游走理论进行分析。

Result: 1. RW-AdaBatch的隐私放大几乎不带来效用损失；2. RW-Meta在专家独立性程度方面具有可扩展的遗憾界；3. 在COVID-19疫情期间医院报告的实时数据上评估，RW-Meta在预测每周报告最高COVID患者密度的医院任务上，比经典基线和最先进的中心差分隐私算法表现好1.5-3倍。

Conclusion: 该工作为本地差分隐私下的专家建议预测提供了创新解决方案，特别是RW-Meta算法能够有效处理非平凡学习算法作为专家，在实际医疗数据预测任务中展现出显著优势，为隐私保护在线学习开辟了新方向。

Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.

</details>


### [518] [Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design](https://arxiv.org/abs/2512.07064)
*Jiannan Yang,Veronika Thost,Tengfei Ma*

Main category: cs.LG

Relevance: 45.0

TL;DR: 论文提出统一概率框架评估分子图表示学习中的掩码预训练方法，发现对于节点级预测任务，复杂掩码分布相比均匀采样并无优势，而预测目标和编码器架构的协同作用更为关键。


<details>
  <summary>Details</summary>
Motivation: 当前分子表示学习中自监督学习的掩码预训练方法多为启发式设计，缺乏原则性评估，难以确定哪些设计选择真正有效，需要系统性的评估框架来理解掩码策略。

Method: 将预训练-微调流程统一到概率框架中，在严格受控设置下研究三个核心设计维度：掩码分布、预测目标和编码器架构，并使用信息论度量评估预训练信号的信息量。

Result: 研究发现：1) 对于常见节点级预测任务，复杂掩码分布相比均匀采样无一致优势；2) 预测目标的选择及其与编码器架构的协同作用更为关键；3) 使用语义更丰富的预测目标能显著提升下游性能，特别是与表达能力强的图Transformer编码器结合时。

Conclusion: 为分子图开发更有效的自监督学习方法提供了实用指导：应更关注预测目标和编码器架构的协同设计，而非过度优化掩码分布策略。

Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.

</details>


### [519] [TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization](https://arxiv.org/abs/2512.07082)
*Yuan-Ting Zhong,Ting Huang,Xiaolin Xiao,Yue-Jiao Gong*

Main category: cs.LG

Relevance: 45.0

TL;DR: TRACE是一个可迁移的概念漂移估计器，用于检测流数据中的分布变化，具有跨数据集泛化能力，并能集成到流优化器中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 现有流数据驱动优化方法通常假设固定的漂移间隔和完全环境可观测性，限制了在多样化动态环境中的适应性。需要一种能够有效检测不同时间尺度流数据分布变化的方法。

Method: 提出TRACE方法：1）使用原则性标记化策略从数据流中提取统计特征；2）基于注意力机制的序列学习建模漂移模式；3）实现跨数据集的可迁移漂移检测；4）可插拔集成到流优化器中。

Result: 在多样化基准测试中，TRACE展现出优异的泛化能力、鲁棒性和有效性，能够准确检测未见数据集中的漂移模式，并提升流数据驱动优化性能。

Conclusion: TRACE通过可迁移的概念漂移检测机制，解决了流数据驱动优化中的动态环境适应问题，为未知漂移下的自适应优化提供了有效解决方案。

Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.

</details>


### [520] [PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes](https://arxiv.org/abs/2512.07113)
*Kepeng Lin,Qizhe Zhang,Rui Wang,Xuehai Hu,Wei Xu*

Main category: cs.LG

Relevance: 45.0

TL;DR: PlantBiMoE：轻量级植物基因组语言模型，结合双向Mamba和稀疏专家混合框架，在31个基因组任务中20个达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有植物基因组模型（如AgroNT和PDLLMs）存在参数过大或无法有效建模DNA双链双向性的问题。需要开发轻量且能捕捉DNA双向结构依赖的模型。

Method: 提出PlantBiMoE模型，集成双向Mamba（捕捉DNA正反链结构依赖）和稀疏专家混合框架（减少激活参数，提升计算效率）。在MPGB基准（31个数据集，11个任务，序列长度50-6000bp）上评估。

Result: 在31个数据集中20个达到最佳性能，平均性能最优。模型能有效表示植物基因组序列，计算效率高。

Conclusion: PlantBiMoE是植物基因组分析的强大计算工具，对植物基因组学、基因编辑和合成生物学有实质性贡献。

Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE

</details>


### [521] [Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions](https://arxiv.org/abs/2512.06615)
*Kaichen Shen,Wei Zhu*

Main category: stat.ML

Relevance: 45.0

TL;DR: 提出LNDSM（潜在非线性去噪分数匹配）方法，将非线性前向动力学与VAE潜在SGM框架结合，通过欧拉-丸山方案近似高斯转移重新表述交叉熵项，实现更快的合成和更好的结构化分布学习。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分数的生成模型（SGM）在潜在空间中通常采用线性前向动力学，限制了其对结构化分布的建模能力。需要一种能够结合非线性动力学与潜在SGM框架的方法，以更好地学习复杂数据分布。

Method: 提出LNDSM训练目标，将非线性前向动力学整合到VAE基础的潜在SGM框架中。通过欧拉-丸山方案诱导的近似高斯转移重新表述交叉熵项，并识别和移除两个零均值但方差爆炸的项以确保数值稳定性。

Result: 在MNIST数据集变体上的实验表明，该方法实现了更快的合成速度，并增强了固有结构化分布的学习能力。与基准的结构无关潜在SGM相比，LNDSM在样本质量和多样性方面持续获得优越表现。

Conclusion: LNDSM成功地将非线性前向动力学与潜在SGM框架相结合，为结构化分布学习提供了更有效的生成建模方法，在样本质量和生成效率方面均有显著提升。

Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.

</details>


### [522] [MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling](https://arxiv.org/abs/2512.07216)
*Bin Wu,Feifan Yang,Zhangming Chan,Yu-Ran Gu,Jiawei Feng,Chao Yi,Xiang-Rong Sheng,Han Zhu,Jian Xu,Mang Ye,Bo Zheng*

Main category: cs.IR

Relevance: 45.0

TL;DR: MUSE是一个用于淘宝展示广告系统的多模态搜索框架，通过两阶段用户兴趣建模（GSU粗召回和ESU精排序）有效整合多模态信号，实现了10万长度用户行为序列建模并显著提升业务指标。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖ID特征，在长尾物品上泛化能力差且语义表达能力有限。虽然最近工作探索了多模态表示用于行为检索，但往往忽略了在细粒度建模阶段（ESU）的多模态整合。

Method: 提出MUSE框架，采用两阶段方法：1）GSU阶段使用轻量级余弦相似度与高质量多模态嵌入进行粗召回；2）ESU阶段进行丰富的多模态序列建模和ID-多模态有效融合。框架支持10万长度用户行为序列建模。

Result: MUSE已在淘宝展示广告系统部署，在线上延迟开销可忽略的情况下显著提升核心业务指标。同时开源了首个包含超长行为序列和高质量多模态嵌入的大规模数据集。

Conclusion: 多模态信号在推荐系统的两阶段建模中都很重要，但不同阶段需要不同策略：GSU阶段简单方法足够，ESU阶段需要更丰富的多模态序列建模和融合。MUSE框架在实际工业部署中证明有效。

Abstract: Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.

</details>


### [523] [How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?](https://arxiv.org/abs/2512.06200)
*Tomohiro Yamashita,Daichi Amagata,Yusuke Matsui*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该研究提出了一个用于评估近似最近邻搜索（ANNS）索引中数据删除效率的实验框架和综合评估指标，将图基ANNS的数据删除方法分为三类，并在HNSW上应用该框架，提出了动态选择删除方法的Deletion Control方法。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成等应用的发展，动态数据的ANNS算法需求增加，但数据删除在ANNS中的评估方法尚未建立。需要系统评估ANNS索引中数据删除的效率。

Method: 1. 提出实验框架和综合评估指标；2. 将图基ANNS的数据删除方法分为三类并进行数学形式化；3. 在准确性、查询速度等指标上评估性能；4. 在HNSW上应用评估框架；5. 提出Deletion Control方法，动态选择适合的删除方法以满足搜索精度要求。

Result: 建立了ANNS数据删除的评估框架，对HNSW等图基ANNS方法的数据删除效果进行了分析，并展示了Deletion Control方法在动态选择删除策略上的有效性。

Conclusion: 该研究填补了ANNS数据删除评估的空白，提出的框架和方法有助于在实际应用中优化动态ANNS系统的性能，对检索增强生成等需要动态数据管理的应用具有重要意义。

Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.

</details>


### [524] [Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration](https://arxiv.org/abs/2512.06218)
*Huizhen Yu,Yi Wan,Richard S. Sutton*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文将异步随机逼近理论应用于平均奖励半马尔可夫决策过程的强化学习，建立了RVI Q-learning算法的收敛性，并引入了新的单调性条件来估计最优奖励率。


<details>
  <summary>Details</summary>
Motivation: 将Borkar-Meyn框架中的异步随机逼近理论应用于强化学习，特别是针对平均奖励半马尔可夫决策过程，以建立更通用的算法收敛性理论框架。

Method: 应用异步随机逼近理论，建立RVI Q-learning算法的收敛性分析，引入新的单调性条件来估计最优奖励率，并进行稳定性分析。

Result: 证明了RVI Q-learning算法在弱通信SMDP中几乎必然收敛到平均奖励最优方程解的紧致连通子集，在额外步长和异步条件下收敛到唯一的样本路径依赖解。

Conclusion: 该研究扩展了强化学习算法的理论框架，为平均奖励SMDP中的异步学习提供了更严格的收敛性保证，并引入了新的分析工具。

Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.

</details>


### [525] [On fine-tuning Boltz-2 for protein-protein affinity prediction](https://arxiv.org/abs/2512.06592)
*James King,Lewis Cornwall,Andrei Cristian Nica,James Day,Aaron Sim,Neil Dalchau,Lilly Wollman,Joshua Meyers*

Main category: cs.LG

Relevance: 40.0

TL;DR: 将蛋白质-配体亲和力预测模型Boltz-2适配到蛋白质-蛋白质亲和力预测任务，发现基于结构的模型表现不如基于序列的模型，但两者结合能带来互补改进


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白质-蛋白质结合亲和力对于理解分子相互作用和设计治疗药物至关重要。研究旨在评估最先进的基于结构的蛋白质-配体亲和力预测模型Boltz-2在蛋白质-蛋白质亲和力回归任务上的表现

Method: 将Boltz-2适配为Boltz-2-PPI用于蛋白质-蛋白质亲和力预测，在TCR3d和PPB-affinity两个数据集上进行评估。对比了基于结构的模型与基于序列的模型，并尝试结合两种模型的嵌入表示

Result: 尽管Boltz-2-PPI具有高结构准确性，但在小规模和较大规模数据情况下都表现不如基于序列的替代模型。将Boltz-2-PPI的嵌入与基于序列的嵌入结合能带来互补改进，特别是对于较弱的序列模型，表明两种模型学习了不同的信号

Conclusion: 当前基于结构的表示方法尚未准备好进行高性能的亲和力预测，结果反映了与结构数据训练相关的已知偏差。基于序列和基于结构的模型学习到不同的信号，它们的结合具有互补优势

Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.

</details>


### [526] [The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News](https://arxiv.org/abs/2512.06638)
*Isha Karn,David Jensen*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文指出，假新闻检测常用的GNN基准数据集（GossipCop和PolitiFact）图结构过于简单，无法有效评估GNN的结构建模能力，MLP与GNN性能相当，结构在这些数据集中作用有限。


<details>
  <summary>Details</summary>
Motivation: 当前假新闻检测研究广泛使用GNN来建模新闻传播结构，但常用的基准数据集（GossipCop和PolitiFact）可能无法真正测试结构建模方法的有效性，需要验证这些数据集是否适合评估GNN的结构建模能力。

Method: 1) 系统评估5种GNN架构与使用相同节点特征的结构无关MLP的性能对比；2) 通过特征打乱和边结构随机化的控制实验来分离结构和特征的贡献；3) 对数据集进行结构分析，计算节点到根节点的跳数分布；4) 在合成的特征噪声大但结构信息丰富的数据集上验证GNN的优势。

Result: 1) MLP性能与GNN相当或接近，差距通常在1-2%内，置信区间重叠；2) 特征打乱导致性能崩溃，但边随机化后性能保持稳定，表明结构作用有限；3) 超过75%的节点距离根节点仅一跳，结构多样性极小；4) 在合成的信息丰富结构数据集上，GNN显著优于MLP。

Conclusion: 常用基准数据集无法有意义地测试结构建模方法的效用，因为这些数据集具有浅层、自我中心式的图拓扑，缺乏结构多样性。这促使需要开发具有更丰富、更多样化图拓扑的数据集来评估GNN的结构建模能力。

Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.

</details>


### [527] [Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure](https://arxiv.org/abs/2512.05990)
*Xin Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Memory-Amortized Inference (MAI)框架，将学习和记忆统一为单一几何基质的相变，基于同调奇偶性原理区分内容与上下文，通过拓扑循环闭合实现从高复杂度搜索到低复杂度查找的转换。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习将参数静态结构与推理动态流程分离，缺乏生物认知的样本效率和热力学经济性。需要统一学习和记忆的理论框架，解释直觉推理的涌现机制。

Method: 基于代数拓扑的MAI框架，提出同调奇偶性原理：偶维同调表示稳定内容，奇维同调表示动态上下文。通过搜索→闭合→结构的拓扑三位一体转换，利用拓扑循环闭合机制将NPSPACE复杂度搜索转换为P复杂度查找。

Result: 建立了学习和记忆的统一拓扑理论框架，解释了快速思维（直觉）从慢速思维（推理）的涌现机制，为后图灵架构提供了蓝图，通过拓扑共振进行计算。

Conclusion: MAI框架为认知过程提供了严格的数学基础，统一了学习和记忆，解释了直觉推理的涌现，并为更高效、更类人的AI系统设计提供了理论指导。

Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.

</details>


### [528] [Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting](https://arxiv.org/abs/2512.06134)
*Georgi Hrusanov,Duy-Thanh Vu,Duy-Cat Can,Sophie Tascedda,Margaret Ryan,Julien Bodelet,Katarzyna Koscielska,Carsten Magnus,Oliver Y. Chén*

Main category: cs.LG

Relevance: 35.0

TL;DR: NKM（神经库普曼机）是一种受动力系统和注意力机制启发的新型机器学习架构，用于使用多模态数据同时预测阿尔茨海默病的多个认知评分，在ADNI数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期认知衰退预测对疾病评估和管理至关重要。现有方法难以在保持可解释性的同时整合多模态数据进行纵向个性化预测。

Method: 提出神经库普曼机（NKM），整合分析知识（α）和生物学知识（β）指导特征分组，通过融合组感知分层注意力机制在库普曼算子框架内将复杂非线性轨迹转换为可解释的线性表示。

Result: 在ADNI数据集上，NKM在预测认知衰退轨迹方面持续优于传统机器学习和深度学习模型，能够同时预测多个认知评分变化，量化不同生物标志物的贡献，并识别最预测认知恶化的脑区。

Conclusion: NKM通过可解释的显式系统推进了使用过去多模态数据进行AD未来认知衰退的个性化、可解释预测，并揭示了AD进展的潜在多模态生物学基础。

Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.

</details>


### [529] [Learning Invariant Graph Representations Through Redundant Information](https://arxiv.org/abs/2512.06154)
*Barproda Halder,Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出RIG框架，利用部分信息分解(PID)识别图表示中的冗余信息，通过多级优化最大化冗余信息同时隔离虚假和因果子图，实现分布外泛化。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在分布外泛化中存在局限性，因为学习到的表示往往保留虚假成分。经典信息论度量无法精确识别虚假子图和不变子图之间关于目标的冗余信息，需要更精细的工具。

Method: 提出RIG框架：1) 使用部分信息分解(PID)识别冗余信息；2) 采用多级优化交替进行：估计冗余信息下界(本身需要优化)和最大化该下界；3) 同时隔离虚假子图和因果子图。

Result: 在合成和真实世界图数据集上的实验表明，RIG框架具有优越的泛化能力，能够在多种分布偏移下实现分布外泛化。

Conclusion: PID是图表示学习的有力工具，RIG框架通过精确识别和利用冗余信息，有效解决了图表示中的虚假成分问题，提升了分布外泛化性能。

Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.

</details>


### [530] [Auto-exploration for online reinforcement learning](https://arxiv.org/abs/2512.06244)
*Caleb Ju,Guanghui Lan*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出具有自动探索能力的新RL方法，无需先验知识或算法相关参数，在表格和线性函数逼近两种设置下达到O(ε⁻²)样本复杂度


<details>
  <summary>Details</summary>
Motivation: 现有RL算法需要假设状态和动作空间的充分探索，这导致算法不可实现且性能次优。需要解决探索-利用困境，开发无需先验知识的自动探索方法。

Method: 提出两种参数自由的自动探索方法：1) 表格设置版本；2) 线性函数逼近版本。采用动态混合时间、折扣状态分布采样、鲁棒梯度估计器和优势差距函数等创新技术。

Result: 在存在探索最优策略的假设下，两种方法都能达到O(ε⁻²)的样本复杂度来求解ε误差问题。复杂度中不包含先前工作中可能任意大的算法相关参数。

Conclusion: 提出的自动探索方法解决了RL中的探索-利用困境，实现了参数自由、易于实现且具有理论保证的算法，避免了先前方法对算法相关参数的依赖。

Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.

</details>


### [531] [Learning Without Time-Based Embodiment Resets in Soft-Actor Critic](https://arxiv.org/abs/2512.06252)
*Homayoon Farrahi,A. Rupam Mahmood*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文研究了在无回合终止和机器人实体重置的情况下使用SAC算法的学习挑战，提出了继续版SAC算法，并通过增加策略熵来补偿无实体重置带来的探索问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习任务通常依赖回合终止和环境重置等辅助组件来加速学习，但这些设置在实际应用中不自然且可能限制长期性能。本文旨在探索在没有这些辅助组件的情况下进行学习的方法。

Method: 1) 提出继续版SAC算法，通过简单修改奖励函数实现无终止学习；2) 在修改的Gym Reacher任务上分析无实体重置时的学习失败原因；3) 提出通过增加策略熵来补偿探索不足的干预方法。

Result: 1) 继续版SAC在无终止情况下性能与回合式SAC相当或更好，且对折扣率γ值更不敏感；2) 无实体重置会导致状态空间探索不足和学习失败；3) 在性能下降或停滞时增加策略熵能有效恢复因无实体重置而损失的性能。

Conclusion: 实体重置在SAC算法中主要帮助状态空间探索，通过适当增加策略熵可以补偿无实体重置带来的探索不足问题，使算法在更自然的任务设置中保持良好性能。

Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $γ$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.

</details>


### [532] [Networked Restless Multi-Arm Bandits with Reinforcement Learning](https://arxiv.org/abs/2512.06274)
*Hanmo Zhang,Zenghui Sun,Kai Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了Networked RMAB框架，将传统多臂老虎机与独立级联模型结合，以捕捉网络环境中个体间的交互作用，并设计了具有近似保证的高效Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 传统RMAB假设各臂独立，无法处理现实世界中个体间的交互作用。在公共卫生等资源分配场景中，个体间的网络效应可能显著影响干预效果，因此需要扩展RMAB框架来建模网络交互。

Method: 1. 提出Networked RMAB框架，结合RMAB和独立级联模型；2. 建立贝尔曼方程并证明其子模性；3. 应用爬山算法获得1-1/e的近似保证；4. 通过改进的收缩分析证明近似贝尔曼更新的收敛性；5. 设计针对网络设置的高效Q学习算法。

Result: 在真实世界图数据上的实验表明，所提Q学习方法优于k步前瞻方法和忽略网络的方法，验证了捕捉网络效应的重要性。理论分析证明了算法的近似保证和收敛性。

Conclusion: Networked RMAB框架成功扩展了传统RMAB以建模网络交互，提出的高效算法在理论和实验上均表现良好，为网络环境中的序列决策提供了新工具。

Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.

</details>


### [533] [Hierarchical geometric deep learning enables scalable analysis of molecular dynamics](https://arxiv.org/abs/2512.06520)
*Zihan Pengmei,Spencer C. Guo,Chatipat Lorpaiboon,Aaron R. Dinner*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种用于分析大规模生物分子系统分子动力学模拟的图神经网络方法，通过局部信息聚合减少内存和计算需求，可在单GPU上快速分析数千残基的蛋白质-核酸复合物。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟能生成原子级详细轨迹，但缺乏定量描述符的系统分析困难。传统GNN在处理大规模生物分子系统（数百残基以上）时面临长程相互作用捕获困难、内存和运行时间需求大的挑战。

Method: 提出局部信息聚合方法，在保持原子级细节的同时减少内存和运行时间需求。该方法通过有效的信息传递机制处理大规模图结构，特别针对蛋白质-核酸复合物系统优化。

Result: 1. 可在单GPU上几分钟内分析数千残基的蛋白质-核酸复合物模拟
2. 对于数百残基系统，该方法提高了性能（相比传统方法）和可解释性
3. 为大规模生物分子系统分析提供了实用工具

Conclusion: 局部信息聚合的GNN方法解决了大规模生物分子系统分析的计算挑战，为分子动力学模拟的高效分析提供了新途径，特别适用于缺乏定量描述符的复杂系统。

Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.

</details>


### [534] [QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling](https://arxiv.org/abs/2512.06582)
*Isaac Kofi Nti*

Main category: cs.LG

Relevance: 35.0

TL;DR: QL-LSTM是一种新型循环神经网络架构，通过参数共享统一门控机制减少48%参数，并通过分层门控循环与加法跳跃连接改善长程信息流，在IMDB情感分类任务中实现竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统LSTM和GRU的两个核心限制：1）门控特定参数的冗余性；2）长距离时间信息保留能力不足。这些限制影响了循环神经网络的效率和长序列建模能力。

Method: 提出QL-LSTM架构，包含两个独立组件：1）参数共享统一门控（PSUG）- 用单一共享权重矩阵替代所有门控特定变换；2）分层门控循环与加法跳跃连接（HGR-ASC）- 添加无乘法路径改善长程信息流。

Result: 在IMDB数据集（扩展文档长度）的情感分类任务中，QL-LSTM相比LSTM、GRU和BiLSTM参考模型，使用显著更少的参数实现了竞争性准确率。虽然PSUG和HGR-ASC组件在每时间步更高效，但由于循环模型的固有顺序性，当前原型尚未实现实际速度提升。

Conclusion: QL-LSTM通过创新的参数共享和分层门控设计，有效解决了传统循环架构的参数冗余和长程依赖问题，为高效序列建模提供了有前景的方向，但需要进一步的内核级优化来实现实际速度优势。

Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.

</details>


### [535] [Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network](https://arxiv.org/abs/2512.06648)
*Xiao Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出基于CNN的金融欺诈检测框架，将公司年度面板数据转换为类图像表示，以捕捉横截面和时间模式，实现提前预测欺诈。


<details>
  <summary>Details</summary>
Motivation: 上市公司财务欺诈难以检测，传统统计模型难以处理非线性特征交互，机器学习模型缺乏可解释性，且现有方法大多只能基于当年数据判断当年欺诈，缺乏时效性。

Method: 设计特征工程方案将公司年度面板数据转换为类图像表示，使用卷积神经网络(CNN)捕捉横截面和时间模式，实现提前欺诈预测，并采用局部解释技术分析模型可解释性。

Result: CNN在准确性、鲁棒性和预警性能上优于逻辑回归和LightGBM，分类阈值调整在高风险场景中至关重要。可解释性分析发现偿债能力、比率结构、治理结构和内部控制是欺诈的通用预测因子。

Conclusion: CNN框架能有效检测财务欺诈并实现提前预警，可解释性分析揭示了欺诈的异质性模式，为监管和审计提供了实用工具。

Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.

</details>


### [536] [Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods](https://arxiv.org/abs/2512.06665)
*Panagiota Kiourti,Anu Singh,Preeti Duraipandian,Weichao Zhou,Wenchao Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究了深度神经网络特征归因方法的鲁棒性，挑战了当前忽视模型输出差异的归因鲁棒性概念，提出了新的相似输入定义、鲁棒性指标和基于生成对抗网络的生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前的特征归因鲁棒性评估方法主要关注输入扰动下的归因图稳定性，但忽视了模型输出变化的重要性。现有方法未能区分归因方法本身的弱点与神经网络模型的弱点，导致评估不够客观准确。

Method: 1) 提出新的相似输入定义，考虑模型输出变化；2) 设计新的鲁棒性评估指标；3) 开发基于生成对抗网络的方法来生成这些相似输入；4) 使用现有指标和最先进的归因方法进行综合评估。

Result: 研究发现当前归因鲁棒性评估存在缺陷，新提出的方法能够更准确地揭示归因方法本身的弱点，而不是神经网络模型的弱点，为归因方法提供了更客观的鲁棒性评估。

Conclusion: 需要更客观的评估指标来准确评估特征归因方法的鲁棒性，新提出的框架能够区分归因方法与神经网络的弱点，为归因方法的鲁棒性评估提供了更可靠的方法。

Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.

</details>


### [537] [The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification](https://arxiv.org/abs/2512.06666)
*Urav Maniar*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该研究探讨时间序列分类中准确性与计算效率的权衡，通过结合Hydra和Quant两种高效算法，在保持计算可行性的同时获得集成学习收益，发现当前元学习策略难以有效利用算法互补性。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类面临准确性与计算效率的根本权衡。虽然像HIVE-COTE 2.0这样的综合集成方法能达到最先进的准确性，但其在UCR基准上的340小时训练时间使其在大规模数据集上不实用。研究旨在探索是否通过结合两种来自互补范式的高效算法，可以在保持计算可行性的同时捕获集成学习的收益。

Method: 结合Hydra（竞争卷积核）和Quant（分层区间分位数）两种高效算法，在六种集成配置下进行评估。在10个大规模MONSTER数据集（7,898到1,168,774个训练实例）上评估性能，分析预测组合集成和特征拼接方法的性能。

Result: 最强配置将平均准确率从0.829提高到0.836，在10个数据集中的7个上取得成功。然而，预测组合集成仅捕获了11%的理论oracle潜力，揭示了显著的元学习优化差距。特征拼接方法通过学习新的决策边界超过了oracle界限，而预测级互补性与集成增益呈中等相关性。

Conclusion: 核心发现：挑战已从确保算法不同转变为学习如何有效组合它们。当前的元学习策略难以利用oracle分析确认存在的互补性。改进的组合策略可能使集成增益在多样化的时间序列分类应用中翻倍或三倍。

Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.

</details>


### [538] [Mitigating Barren plateaus in quantum denoising diffusion probabilistic models](https://arxiv.org/abs/2512.06695)
*Haipeng Cao,Kaining Zhang,Dacheng Tao,Zhaofeng Su*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文发现量子去噪扩散概率模型(QuDDPM)存在贫瘠高原问题，并提出改进方案以提升训练效率和生成质量


<details>
  <summary>Details</summary>
Motivation: 量子生成模型利用量子叠加和纠缠特性提升学习效率，QuDDPM作为量子生成学习框架在量子数据学习方面表现出色，但研究发现其存在贫瘠高原问题，严重影响了模型性能

Method: 通过理论分析和实验验证确认原始QuDDPM存在贫瘠高原问题，提出改进的QuDDPM，使用与Haar分布保持一定距离的分布作为去噪过程输入，确保更好的可训练性

Result: 实验结果表明，改进方法有效缓解了贫瘠高原问题，生成了更高质量的样本，为可扩展和高效的量子生成学习铺平了道路

Conclusion: QuDDPM的贫瘠高原问题源于使用2-design状态作为去噪输入，通过改进输入分布可以显著提升模型的可训练性和生成质量

Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.

</details>


### [539] [Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models](https://arxiv.org/abs/2512.06702)
*Xiangjun Meng,Zhongjian Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文为基于流的生成模型提供了可实现的误差分析工具，证明了在Wasserstein度量下采样迭代复杂度与维度平方根O(√d)成正比，误差由两部分控制：反向流推前映射的Lipschitz性（与维度无关）和局部离散化误差（与√d成正比）。


<details>
  <summary>Details</summary>
Motivation: 研究基于流的生成模型的采样误差和迭代复杂度，特别是在高维情况下的理论保证。当前缺乏对这类模型采样误差的系统理论分析，特别是在维度缩放方面的精确界限。

Method: 使用Wasserstein度量分析流生成模型的误差，将总误差分解为两部分：反向流推前映射的Lipschitz性（与维度无关）和局部离散化误差（与√d成正比）。在Föllmer过程和1-整流流模型下验证假设，建立采样迭代复杂度的精确界限。

Result: 证明了采样迭代复杂度与维度平方根O(√d)成正比，具体与正向过程不变分布的协方差算子迹的平方根线性相关。在Föllmer过程和1-整流流模型下，这些理论假设在满足高斯尾假设时成立。

Conclusion: 该研究为流生成模型提供了严格的理论分析框架，揭示了采样误差的维度依赖性，为高效采样算法设计提供了理论基础，特别是在高维场景下的实际应用。

Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.

</details>


### [540] [A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations](https://arxiv.org/abs/2512.06708)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种新颖的多模态RUL估计框架，结合图像表示和时间频率表示，用于滚动轴承剩余使用寿命预测，具有更好的泛化性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 机械系统剩余使用寿命（RUL）估计在预测性健康管理中至关重要。滚动轴承是机械故障的常见原因，现有方法存在泛化性差、鲁棒性不足、数据需求高和可解释性有限等问题。

Method: 提出多模态RUL框架，包含三个分支：1）图像表示分支（使用Bresenham线算法转换振动信号）；2）时间频率表示分支（使用连续小波变换）；3）融合分支（连接特征后输入LSTM建模时间模式）。采用多头注意力机制强调重要特征，最后通过线性层进行RUL回归。还提出多模态层相关传播（multimodal-LRP）增强可解释性。

Result: 在XJTU-SY和PRONOSTIA基准数据集上验证，方法在已知和未知工况下匹配或超越现有最优方法，在XJTU-SY上减少约28%训练数据，在PRONOSTIA上减少约48%。模型表现出强噪声鲁棒性，multimodal-LRP可视化证实了预测的可解释性和可信度。

Conclusion: 该多模态框架在RUL估计中表现出优越性能，具有更好的泛化性、数据效率和可解释性，适合实际工业部署。

Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.

</details>


### [541] [Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics](https://arxiv.org/abs/2512.06737)
*Nikhil Verma,Joonas Linnosmaa,Espinosa-Leal Leonardo,Napat Vajragupta*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出ArcGD优化器，在非凸基准函数和真实ML数据集上评估，在CIFAR-10分类任务中超越Adam、AdamW、SGD、Lion等优化器，展现抗过拟合能力。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam在非凸优化问题中可能表现不佳，特别是在高维空间和长时间训练时容易过拟合。需要开发新的优化方法来解决这些问题。

Method: 提出ArcGD优化器，首先在非凸Rosenbrock函数上进行基准测试（2D到50,000D），然后在CIFAR-10数据集上使用8种不同MLP架构进行评估，与Adam、AdamW、SGD、Lion等优化器对比。

Result: ArcGD在Rosenbrock函数上优于Adam；在CIFAR-10分类任务中，20,000次迭代后平均测试准确率达到50.7%，优于AdamW(46.6%)、Adam(46.8%)、SGD(49.6%)和Lion(43.4%)，在8种架构中6种获胜或持平。

Conclusion: ArcGD优化器在非凸优化和真实ML任务中表现优异，具有抗过拟合能力，且与Lion优化器存在理论联系，值得进一步探索。

Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.

</details>


### [542] [Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship](https://arxiv.org/abs/2512.06758)
*Zilong Wang,Shuai Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了一种多级连续选择算法，在序列独裁假设下的匹配市场问题中实现了与下界匹配的遗憾上界


<details>
  <summary>Details</summary>
Motivation: 在线匹配市场中，参与者通常不确定自己的偏好，需要通过多轮交互学习。现有算法在序列独裁假设下的遗憾上界与下界之间存在从N到K的差距，需要设计匹配下界的算法

Method: 提出多级连续选择算法，在序列独裁假设下通过分层选择策略优化匹配过程

Result: 算法实现了O(Nlog(T)/Δ² + Klog(T)/Δ)的遗憾上界，与Sankararaman等人提出的下界完全匹配

Conclusion: 这是首个在匹配市场与多臂老虎机问题中实现与下界匹配的算法，解决了长期存在的遗憾上下界差距问题

Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $Ω\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $Δ$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{Δ^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.

</details>


### [543] [Measuring Over-smoothing beyond Dirichlet energy](https://arxiv.org/abs/2512.06782)
*Weiqi Guan,Zihao Shi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种基于高阶特征导数能量的广义节点相似性度量家族，用于量化图神经网络中的过平滑现象，揭示了过平滑衰减率与图拉普拉斯谱隙的内在联系。


<details>
  <summary>Details</summary>
Motivation: 当前Dirichlet能量作为量化过平滑的主流指标存在局限性，它只能捕获一阶特征导数。为了更全面地理解图神经网络中的过平滑现象，需要开发能够捕捉高阶特征变化的度量方法。

Method: 提出基于高阶特征导数能量的广义节点相似性度量家族，通过理论分析建立这些度量之间的关系，研究连续热扩散和离散聚合算子下Dirichlet能量的衰减率，并揭示过平滑衰减率与图拉普拉斯谱隙的内在联系。

Result: 理论分析建立了高阶能量度量之间的关系，实证结果表明基于注意力的图神经网络在提出的度量下确实存在过平滑问题。

Conclusion: 提出的高阶能量度量能够更全面地量化图神经网络中的过平滑现象，揭示了过平滑与图结构谱性质的内在联系，为理解和缓解图神经网络中的过平滑问题提供了新的理论框架。

Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.

</details>


### [544] [Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games](https://arxiv.org/abs/2512.06791)
*Vedansh Sharma*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Small-Gain Nash (SGN)方法，通过自定义块加权几何中的块小增益条件，为非单调博弈中的梯度学习提供收敛性证明，即使伪梯度在欧几里得几何中不满足单调性条件。


<details>
  <summary>Details</summary>
Motivation: 传统博弈中基于梯度的学习收敛性证明要求伪梯度在欧几里得几何中满足(强)单调性条件，但即使在具有强跨玩家耦合的简单博弈中，这一条件也常常无法满足。需要一种更通用的收敛性分析框架。

Method: 引入Small-Gain Nash (SGN)方法，在自定义块加权几何中建立块小增益条件。该方法将局部曲率和跨玩家Lipschitz耦合边界转化为可处理的收缩证书，构建加权块度量使伪梯度在满足这些边界的任何区域上变为强单调。分析投影欧拉和RK4离散化的收敛性，并推导出基于SGN裕度和局部Lipschitz常数的显式步长边界。

Result: SGN成功证明了在欧几里得单调性分析无法预测收敛的二次博弈中的收敛性。该方法揭示了"时间尺度带"这一非渐近、基于度量的证书，类似于TTUR的作用。扩展了构造到镜像/Fisher几何，用于马尔可夫博弈中的熵正则化策略梯度。

Conclusion: SGN提供了一个离线认证流程，可在紧凑区域上估计曲率、耦合和Lipschitz参数，优化块权重以扩大SGN裕度，并返回包含度量、收缩率和安全步长的结构性可计算收敛证书，适用于非单调博弈。

Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.

</details>


### [545] [Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation](https://arxiv.org/abs/2512.06813)
*Agung Nugraha,Heungjun Im,Jihwan Lee*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种用于高性能混凝土部分逆向设计的协作神经网络框架，通过耦合的插补模型和代理模型，在单次前向传播中生成满足约束的混合设计。


<details>
  <summary>Details</summary>
Motivation: 高性能混凝土需要复杂的混合设计，涉及许多相互依赖的变量和实际约束。虽然数据驱动方法在正向设计预测建模方面取得了进展，但逆向设计（确定实现目标性能的混合组成）仍然有限，特别是在某些混合变量被约束固定、只有剩余变量需要确定的设计场景中。

Method: 提出协作神经网络框架，结合两个耦合的神经网络模型：1) 插补模型推断未确定的变量；2) 代理模型预测抗压强度。通过协作学习，模型在单次前向传播中生成有效且性能一致的混合设计，同时适应不同的约束组合而无需重新训练。

Result: 在基准数据集上评估，提出的模型实现了稳定的R平方值0.87-0.92，与自编码器基线相比平均减少50%的均方误差，与贝叶斯推理相比平均减少70%的均方误差。

Conclusion: 协作神经网络为混凝土工程中约束感知、数据驱动的混合配比提供了准确、鲁棒且计算高效的基础。

Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.

</details>


### [546] [Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features](https://arxiv.org/abs/2512.06925)
*Aseer Al Faisal*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该研究提出了一种结合RoBERTa语义嵌入和手工词汇特征的QR-DQN方法，用于钓鱼网站检测，在105,000个URL数据集上取得了99.86%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击通过欺诈信息、误导广告和合法网站漏洞进行，导致个人信息泄露和财务损失。传统DQN方法估计单一标量Q值，无法处理不确定性，需要更稳定和泛化能力强的检测方法。

Method: 提出Quantile Regression Deep Q-Network (QR-DQN)框架，结合RoBERTa语义嵌入和手工词汇特征。使用分位数回归建模回报分布，提高稳定性和泛化能力。数据集包含105,000个URL，来自PhishTank、OpenPhish、Cloudflare等来源，采用80/20训练测试分割。

Result: 测试准确率99.86%，精确率99.75%，召回率99.96%，F1分数99.85%。相比仅使用词汇特征的标准DQN，混合QR-DQN将泛化差距从1.66%降低到0.04%。五折交叉验证平均准确率99.90%，标准差0.04%。

Conclusion: 提出的混合方法能有效识别钓鱼威胁，适应不断演变的攻击策略，对未见数据具有良好的泛化能力。QR-DQN通过分位数回归处理不确定性，显著提高了检测系统的鲁棒性。

Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.

</details>


### [547] [Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise](https://arxiv.org/abs/2512.06926)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文系统分析了BiLSTM时间序列预测模型对输入序列长度和加性噪声的敏感性，发现长序列增加过拟合风险，噪声降低预测精度，两者同时存在时模型稳定性最差。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列预测中应用广泛，但现有研究对模型鲁棒性和泛化能力如何受输入数据特征影响缺乏深入探索。本研究旨在系统分析输入序列长度和加性噪声这两个关键数据因素对BiLSTM模型性能的影响。

Method: 开发了模块化、可复现的预测流程，包含标准化预处理、序列生成、模型训练、验证和评估。在三个不同采样频率的真实世界数据集上进行控制实验，评估BiLSTM在不同输入条件下的性能。

Result: 三个主要发现：1) 长输入序列显著增加过拟合和数据泄露风险，尤其在数据受限环境中；2) 加性噪声在不同采样频率下都持续降低预测精度；3) 两个因素同时存在时模型稳定性下降最严重。高观测频率数据集相对更鲁棒，但在两个挑战同时存在时仍然脆弱。

Conclusion: 研究揭示了当前基于深度学习的预测流程的重要局限性，强调需要数据感知的设计策略。为理解深度学习模型在动态时间序列环境中的行为提供了深入见解，并为开发更可靠、可泛化的预测系统提供了实用指导。

Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.

</details>


### [548] [Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies](https://arxiv.org/abs/2512.06932)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该研究评估了数据泄露对LSTM时间序列预测模型性能的影响，发现验证设计显著影响泄露敏感性，其中10折交叉验证对泄露最敏感，而2-way和3-way分割更稳健。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中，数据泄露（即未来信息无意中影响训练）会损害评估的完整性。本研究旨在探究不同验证设计如何调节数据泄露对LSTM预测性能的影响。

Method: 研究比较了三种常用验证技术（2-way分割、3-way分割、10折交叉验证）在泄露（分割前构建序列）和干净（分割后构建序列）条件下的表现。使用RMSE增益（泄露与干净设置的RMSE百分比差异）量化泄露影响，并分析输入窗口大小和滞后步长的影响。

Result: 10折交叉验证对泄露最敏感，RMSE增益高达20.5%（尤其在长滞后步长时）。2-way和3-way分割更稳健，RMSE增益通常低于5%。较小的输入窗口和较长的滞后步长会增加泄露风险，而较大的窗口有助于减少泄露。

Conclusion: 验证设计显著影响时间序列预测中数据泄露的敏感性。需要配置感知、抗泄露的评估流程来确保可靠的性能估计，特别是在使用交叉验证时需格外小心。

Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.

</details>


### [549] [Comparing BFGS and OGR for Second-Order Optimization](https://arxiv.org/abs/2512.06969)
*Adrian Przybysz,Mikołaj Kołek,Franciszek Sobota,Jarek Duda*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文比较了BFGS的Sherman-Morrison更新与新颖的在线梯度回归(OGR)方法，OGR通过指数移动平均回归梯度与位置来在线估计二阶导数，无需Hessian求逆，能处理非凸结构，在非凸设置中收敛更快、损失更低。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中Hessian矩阵估计面临高维度和高成本的挑战。传统BFGS方法基于凸性假设保持正定Hessian近似，但无法处理非凸结构。需要一种能在线估计一般（非正定）Hessian的方法。

Method: 提出在线梯度回归(OGR)方法：使用指数移动平均对梯度与位置进行回归，在线估计二阶导数，无需Hessian求逆。与BFGS的Sherman-Morrison更新对比，OGR能估计一般Hessian矩阵，处理非凸结构。

Result: 在标准测试函数上评估两种方法，OGR在非凸设置中实现更快的收敛速度和改进的损失表现，优于传统BFGS方法。

Conclusion: OGR提供了一种有效的在线Hessian估计方法，特别适用于非凸优化问题，相比传统BFGS方法具有更好的性能。

Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.

</details>


### [550] [OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction](https://arxiv.org/abs/2512.06987)
*Emily Jin,Andrei Cristian Nica,Mikhail Galkin,Jarrid Rector-Brooks,Kin Long Kelvin Lee,Santiago Miret,Frances H. Arnold,Michael Bronstein,Avishek Joey Bose,Alexander Tong,Cheng-Hao Liu*

Main category: cs.LG

Relevance: 35.0

TL;DR: OXtal是一个100M参数的全原子扩散模型，直接从2D化学图预测3D分子晶体结构，通过数据增强和晶格无关训练方案，在600K实验验证的晶体结构数据集上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 准确预测从2D化学图到3D分子晶体结构是计算化学中长期存在的开放挑战（晶体结构预测CSP）。高效解决这个问题对制药和有机半导体等领域有重要意义，因为晶体堆积直接影响有机固体的物理化学性质。

Method: 1. 放弃显式等变架构，采用数据增强策略来施加晶体对称性归纳偏置
2. 提出新颖的结晶启发式晶格无关训练方案S^4（化学计量随机壳采样），有效捕获长程相互作用同时避免显式晶格参数化
3. 使用600K实验验证的晶体结构大规模数据集（包括刚性和柔性分子、共晶和溶剂化物）

Result: 1. 相比先前的从头机器学习CSP方法，取得了数量级的改进
2. 比传统量子化学方法便宜数量级
3. 恢复实验结构，构象RMSD1 < 0.5 Å
4. 达到超过80%的堆积相似率
5. 能够建模分子结晶的热力学和动力学规律

Conclusion: OXtal通过大规模扩散模型和数据增强策略，在晶体结构预测方面取得了突破性进展，展示了从2D化学图直接学习3D晶体结构的可行性，为计算化学领域提供了高效准确的解决方案。

Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ Å and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.

</details>


### [551] [Transferring Clinical Knowledge into ECGs Representation](https://arxiv.org/abs/2512.07021)
*Jose Geraldo Fernandes,Luiz Facury de Souza,Pedro Robles Dutenhefner,Gisele L. Pappa,Wagner Meira*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种三阶段训练范式，将多模态临床数据知识迁移到单模态ECG编码器中，通过自监督预训练和实验室异常预测来提高ECG分类准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在ECG分类中表现出高准确性，但其黑盒特性缺乏可解释性，阻碍了临床采用。需要创建既准确又可信的ECG分类模型，通过将抽象预测转化为生理学解释，促进AI在临床工作流程中的安全集成。

Method: 1. 三阶段训练范式：将多模态临床数据知识迁移到单模态ECG编码器；2. 自监督联合嵌入预训练：创建富含上下文临床信息的ECG表示；3. 间接解释方法：训练模型从ECG嵌入预测相关的实验室异常；4. 仅需ECG信号进行推理，但利用多模态数据进行训练。

Result: 在MIMIC-IV-ECG数据集上评估，模型在多标签诊断分类中优于标准信号基线，显著缩小了与需要所有数据推理的完全多模态模型之间的性能差距。

Conclusion: 该方法为创建更准确、更可信的ECG分类模型提供了实用有效的方法，通过将抽象预测转化为生理学解释，为AI安全集成到临床工作流程提供了有前景的路径。

Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.

</details>


### [552] [Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis](https://arxiv.org/abs/2512.07040)
*Sakib Mostafa,Lei Xing,Md. Tauhidul Islam*

Main category: cs.LG

Relevance: 35.0

TL;DR: Graph2Image将大型生物网络转换为二维图像，利用CNN处理，解决了现有方法在可扩展性、长程依赖和多模态集成方面的限制，显著提升了分类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生物网络分析面临可扩展性差、长程依赖捕捉困难、多模态集成挑战、表达能力有限和可解释性不足等问题。传统深度学习方法在处理大规模复杂生物网络时存在这些固有缺陷。

Method: Graph2Image框架将大型生物网络转换为二维图像集合：1) 将代表性网络节点空间排列在2D网格上；2) 将节点解耦为图像；3) 使用具有全局感受野和多尺度金字塔的CNN处理；4) 支持与其他成像和组学模态的无缝集成。

Result: 在多个大规模生物网络数据集上，Graph2Image比现有方法提升分类准确率高达67.2%；能够分析超过10亿节点的超大规模网络；提供可解释的可视化，揭示生物学一致的模式；可在个人计算机上运行。

Conclusion: Graph2Image为生物网络分析提供了可扩展、可解释且支持多模态的方法，为疾病诊断和复杂生物系统研究开辟了新机会，解决了现有方法的关键限制。

Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.

</details>


### [553] [FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers](https://arxiv.org/abs/2512.07150)
*Jonghyun Park,Jong Chul Ye*

Main category: cs.LG

Relevance: 35.0

TL;DR: FlowLPS：一种基于预训练流模型解决逆问题的新框架，通过Langevin近端采样策略，在潜在空间中实现更好的后验模式收敛和流形一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度生成模型的训练免费方法在应用于潜在流模型时存在两个主要问题：1) 难以收敛到后验模式；2) 在潜在空间中容易出现流形偏离。需要一种新方法来平衡重建保真度和感知质量。

Method: FlowLPS框架结合了Langevin动力学和近端优化：1) Langevin动力学用于流形一致性探索；2) 近端优化用于精确模式搜索。这是一种训练免费的方法，使用预训练的流模型解决逆问题。

Result: 在FFHQ和DIV2K数据集上的多个逆任务中，FlowLPS在重建保真度和感知质量之间取得了优越的平衡，超越了现有的最先进逆问题求解器。

Conclusion: FlowLPS通过Langevin近端采样策略有效解决了潜在流模型在逆问题中的收敛和流形偏离问题，为训练免费的逆问题求解提供了新思路。

Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.

</details>


### [554] [PINE: Pipeline for Important Node Exploration in Attributed Networks](https://arxiv.org/abs/2512.07244)
*Elizaveta Kovtun,Maksim Makarenko,Natalia Semenova,Alexey Zaytsev,Semen Budennyy*

Main category: cs.LG

Relevance: 35.0

TL;DR: PINE：一种用于属性图的无监督重要节点发现管道，结合节点语义特征和图结构特性，通过注意力机制学习节点重要性。


<details>
  <summary>Details</summary>
Motivation: 传统网络重要节点识别方法（如度中心性、PageRank）仅考虑网络结构而忽略丰富的节点属性特征。现有神经网络方法虽然能处理节点特征，但需要监督学习。本研究旨在填补无监督且属性感知的重要节点识别方法的空白。

Method: 提出PINE（重要节点探索管道），核心是基于注意力的图模型，在识别图结构特性的学习过程中融入节点语义特征。通过注意力分布计算节点重要性分数。

Result: PINE在多种同质和异质属性网络上表现出优越性能。作为工业实现系统，成功解决了大规模企业图中关键实体的无监督识别这一实际挑战。

Conclusion: PINE填补了无监督属性感知重要节点识别方法的空白，结合语义特征和结构特性，在实际应用中表现出色。

Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.

</details>


### [555] [Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach](https://arxiv.org/abs/2512.07313)
*Bosun Kang,Hyejun Park,Chenglin Fan*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了一种基于贝叶斯决策的滑雪租赁问题新框架，通过维护时间范围的精确后验分布，实现了不确定性量化和专家先验的无缝整合，在准确先验下达到接近最优结果，同时保持鲁棒的worst-case保证。


<details>
  <summary>Details</summary>
Motivation: 传统滑雪租赁问题算法在不做假设的情况下最小化最坏情况成本，而最近的学习增强方法利用噪声预测提供鲁棒性保证。本文旨在统一这两种视角，通过贝叶斯框架实现更灵活、更精确的决策。

Method: 提出了一个离散贝叶斯框架，维护时间范围的精确后验分布，支持不确定性量化和专家先验整合。算法实现了先验依赖的竞争性保证，并在最坏情况和完全知情设置之间平滑插值。框架可扩展至多预测、非均匀先验和上下文信息。

Result: 广泛的实验评估显示，该方法在多样化场景中表现出优越的实证性能：在准确先验下达到接近最优结果，同时保持鲁棒的最坏情况保证。框架自然支持多预测、非均匀先验和上下文信息整合。

Conclusion: 贝叶斯推理在具有不完美预测的在线决策问题中具有实际优势，为传统算法和学习增强方法提供了统一的框架，实现了理论保证和实际性能的良好平衡。

Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.

</details>


### [556] [Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach](https://arxiv.org/abs/2512.07332)
*Zhengquan Luo,Guy Tadmor,Or Amar,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

Relevance: 35.0

TL;DR: RicciKGE提出了一种动态几何适应的知识图谱嵌入方法，通过Ricci流耦合嵌入损失与局部曲率，使嵌入与流形几何共同演化，适应知识图谱的异质结构。


<details>
  <summary>Details</summary>
Motivation: 现有KGE方法将实体放置在预定义的齐次流形上（欧几里得、球面、双曲或其乘积/多曲率变体），但真实知识图谱在不同局部区域表现出急剧变化的曲率。预定义的齐次流形无法适应这种局部曲率变化，导致实体间距离失真，影响嵌入的表达能力。

Method: 提出RicciKGE方法，将KGE损失梯度与局部曲率通过扩展的Ricci流耦合，使实体嵌入与底层流形几何动态共同演化，实现相互适应。理论上证明了当耦合系数有界且适当选择时，所有边曲率指数衰减（流形趋向欧几里得平坦），且KGE距离严格收敛到全局最优。

Result: 在链接预测和节点分类基准测试中，RicciKGE相比现有方法表现出改进效果，证明了其在适应异质知识图谱结构方面的有效性。

Conclusion: RicciKGE通过动态几何适应机制，解决了传统KGE方法中预定义齐次流形与真实知识图谱局部曲率不匹配的问题，提高了嵌入的表达能力和模型性能。

Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.

</details>


### [557] [Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.07417)
*Giray Önür,Azita Dabiri,Bart De Schutter*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一个多智能体强化学习框架，用于自适应调整状态反馈交通控制器的参数，结合状态反馈控制器的反应性和强化学习的适应性，在部分故障时具有更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统交通管理策略（如路线引导、匝道控制、交通信号控制）通常依赖状态反馈控制器，虽然简单反应快，但缺乏应对复杂时变交通动态的适应性。需要结合反应性和适应性的解决方案。

Method: 多智能体强化学习框架，每个智能体自适应调整状态反馈交通控制器的参数（而非直接高频确定控制动作）。通过较低频率调整参数，保持训练效率的同时适应变化的交通条件。多智能体结构增强系统鲁棒性。

Result: 在模拟的多类别交通网络中进行评估，结果显示：提出的多智能体框架优于无控制和固定参数状态反馈控制，与单智能体RL自适应状态反馈控制性能相当，但对部分故障具有更好的恢复能力。

Conclusion: 提出的多智能体强化学习框架成功结合了状态反馈控制器的反应性和强化学习的适应性，在保持训练效率的同时提高了交通控制系统的鲁棒性。

Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.

</details>


### [558] [MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.07430)
*Yangle Li,Danli Luo,Haifeng Hu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出MIDG框架用于多模态情感分析的领域泛化，通过混合不变专家模型提取领域不变特征，并设计跨模态适配器增强多模态表示语义丰富度


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析领域泛化方法在提取不变特征时忽略了模态间的协同作用，无法准确捕捉多模态数据的丰富语义信息。同时，知识注入技术存在跨模态知识碎片化问题，忽略了单模态边界之外的特定表示。

Method: 1) 混合不变专家模型：提取领域不变特征，增强模型学习模态间协同关系的能力；2) 跨模态适配器：通过跨模态知识注入增强多模态表示的语义丰富度

Result: 在三个数据集上的广泛领域实验表明，提出的MIDG框架取得了优越的性能表现

Conclusion: 提出的MIDG框架通过有效提取领域不变特征和跨模态知识注入，显著提升了多模态情感分析在领域泛化任务上的性能

Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.

</details>


### [559] [Mitigating Bias in Graph Hyperdimensional Computing](https://arxiv.org/abs/2512.07433)
*Yezi Liu,William Youngwoo Chung,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

Relevance: 35.0

TL;DR: FairGHDC：一种公平感知的图超维计算框架，通过引入偏差校正项和公平性因子，在保持HDC计算效率的同时显著减少群体间的不公平性差距。


<details>
  <summary>Details</summary>
Motivation: 图超维计算（HDC）作为认知任务的脑启发计算范式，在图形结构数据上展现出鲁棒性和高效性，但其公平性影响尚未得到充分研究。本文旨在探索图HDC中的公平性问题，研究超向量编码和基于相似性的分类如何传播甚至放大数据表示和决策规则中的偏见。

Method: 提出FairGHDC框架，引入基于差距的人口统计均等正则化器推导出的偏差校正项，将其转换为标量公平性因子，用于缩放真实标签类别超向量的更新。该方法直接在超向量空间中进行去偏，无需修改图编码器或反向传播。

Result: 在六个基准数据集上的实验表明，FairGHDC显著减少了人口统计均等和机会均等差距，同时保持了与标准GNN和公平感知GNN相当的准确性。此外，FairGHDC保持了HDC的计算优势，在GPU训练时间上相比GNN和公平感知GNN基线实现了约10倍的加速。

Conclusion: FairGHDC成功地将公平性考虑融入图超维计算框架，在保持HDC计算效率的同时有效缓解了偏见传播问题，为构建高效且公平的图学习系统提供了新思路。

Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.

</details>


### [560] [Materium: An Autoregressive Approach for Material Generation](https://arxiv.org/abs/2512.07486)
*Niklas Dobberstein,Jan Hamaekers*

Main category: cs.LG

Relevance: 35.0

TL;DR: Materium是一个用于生成晶体结构的自回归Transformer模型，它将3D材料表示转换为token序列，包含元素、氧化态、分数坐标和晶格参数，能够快速、可扩展地生成精确的原子位置。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法需要多次去噪步骤迭代细化原子位置，计算成本高且速度慢。需要一种能够快速、精确生成晶体结构的方法，支持多种物理性质作为条件输入。

Method: 采用自回归Transformer架构，将3D材料表示（元素、氧化态、分数坐标、晶格参数）转换为token序列。与扩散方法不同，直接生成精确的分数坐标，实现快速原子定位。支持多种性质作为条件输入，包括基本性质（密度、空间群）和实用目标（带隙、磁密度）。

Result: 模型可在单GPU上几小时内完成训练，在GPU和CPU上生成样本的速度远快于扩散方法。在单条件和组合条件下均表现一致良好，生成的候选结构与输入要求对齐。

Conclusion: Materium提供了一种高效、可扩展的晶体结构生成方法，克服了扩散方法的计算瓶颈，支持多种物理性质条件生成，在材料发现领域具有应用潜力。

Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.

</details>


### [561] [Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting](https://arxiv.org/abs/2512.07569)
*Joel Ekstrand,Tor Mattsson,Zahra Taghiyarrenani,Slawomir Nowaczyk,Jens Lundström,Mikael Lindén*

Main category: cs.LG

Relevance: 35.0

TL;DR: WECA提出加权对比适应方法，提升多元时间序列在异常条件下的预测可靠性，在ATM现金物流等应用中显著改善异常数据预测性能


<details>
  <summary>Details</summary>
Motivation: 现代深度预测模型在正常数据上表现良好，但在分布偏移（如ATM现金物流中的突发需求变化）发生时预测失败，需要提升异常条件下的预测可靠性

Method: 提出加权对比适应（WECA），通过加权对比目标对齐正常和异常增强表示，保留异常相关信息同时保持良性变化下的一致性

Result: 在全国ATM交易数据集上，WECA将异常影响数据的SMAPE提高了6.1个百分点，正常数据上性能下降可忽略

Conclusion: WECA在不牺牲正常操作性能的前提下，显著增强了异常条件下的预测可靠性

Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.

</details>


### [562] [A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data](https://arxiv.org/abs/2512.07741)
*Agnes Norbury,George Fairs,Alexandra L. Georgescu,Matthew M. Nour,Emilia Molimpakis,Stefano Goria*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出使用贝叶斯网络模型从语音特征预测抑郁和焦虑症状，在大型数据集上评估性能、公平性和临床实用性，为精神评估提供透明可解释的AI支持工具。


<details>
  <summary>Details</summary>
Motivation: 精神科评估中，临床医生需要整合患者的言语内容和非语言信号（语调、语速、流畅度、反应性、肢体语言等），这是一个具有挑战性的任务，适合智能工具支持，但目前尚未在临床中实现。需要解决采用障碍，特别是透明度和可解释性问题。

Method: 采用贝叶斯网络建模方法，从大规模数据集（30,135名独特说话者）的语音和言语特征预测抑郁和焦虑症状。评估模型在症状层面的表现，分析人口统计学公平性，研究不同输入模态类型的整合和冗余性，并探索临床实用性指标和心理健康服务使用者的可接受性。

Result: 模型在抑郁和焦虑预测上表现良好（ROC-AUC=0.842, 0.831；ECE=0.018, 0.015），核心个体症状的ROC-AUC>0.74。评估了人口统计学公平性，研究了多模态输入的整合和冗余性，并探索了临床实用性和用户可接受性。

Conclusion: 当提供足够丰富的大规模多模态数据流，并在症状而非疾病层面表示常见精神状况时，贝叶斯网络模型是构建稳健评估支持工具的原则性方法：以透明可解释的格式提供临床相关输出，直接适合专家临床监督。

Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.

</details>


### [563] [Formalized Hopfield Networks and Boltzmann Machines](https://arxiv.org/abs/2512.07766)
*Matteo Cipollina,Michail Karatarakis,Freek Wiedijk*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文在Lean 4中形式化神经网络，包括确定性的Hopfield网络和随机的Boltzmann机，证明了收敛性、Hebbian学习正确性以及平稳分布存在性。


<details>
  <summary>Details</summary>
Motivation: 神经网络广泛应用但分析和验证困难，需要形式化方法来确保数学严谨性。作者旨在为神经网络提供形式化基础，涵盖确定性和随机模型，建立可验证的理论保证。

Method: 使用Lean 4定理证明器形式化神经网络。首先形式化Hopfield网络及其收敛性，证明Hebbian学习在正交模式下的正确性。然后形式化随机网络，以Boltzmann机为例，使用新的Perron-Frobenius定理形式化证明其遍历性和平稳分布收敛性。

Result: 成功在Lean 4中形式化了神经网络理论，包括：1) Hopfield网络的收敛性证明；2) Hebbian学习在正交模式下的正确性证明；3) Boltzmann机的遍历性证明；4) 随机网络收敛到唯一平稳分布的证明。

Conclusion: 该工作为神经网络提供了形式化验证框架，增强了理论可靠性。形式化方法能确保神经网络性质的数学严谨性，为未来更复杂模型的形式化分析奠定了基础。

Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.

</details>


### [564] [Intrusion Detection on Resource-Constrained IoT Devices with Hardware-Aware ML and DL](https://arxiv.org/abs/2512.02272)
*Ali Diab,Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino,Amer Baghdadi,Mostafa Rizk*

Main category: cs.NI

Relevance: 35.0

TL;DR: 该论文提出了一种面向物联网和工业物联网网络的硬件感知入侵检测系统，在边缘设备严格约束下优化树模型和紧凑深度神经网络，实现实时威胁检测。


<details>
  <summary>Details</summary>
Motivation: 物联网和工业物联网网络需要快速、隐私保护且资源高效的威胁检测系统。现有方法在边缘设备严格约束下（闪存、RAM、计算能力）难以平衡准确性和效率，需要系统性的硬件感知模型优化方法。

Method: 1. 对树模型（LightGBM等）采用约束网格搜索优化；2. 对1D卷积神经网络采用硬件感知神经架构搜索（HW-NAS）；3. 在Edge-IIoTset基准上评估；4. 在Raspberry Pi 3 B Plus上部署完整流水线。

Result: LightGBM达到95.3%准确率，仅需75KB闪存和1.2K操作；HW-NAS优化的CNN达到97.2%准确率，需要190KB闪存和840K FLOPs。树模型在30ms内完成推理，CNN在准确性优先时仍适用。

Conclusion: 硬件约束模型设计在边缘实时入侵检测系统中具有实用性，树模型在资源极度受限时表现优异，CNN在准确性要求更高时仍可部署，为边缘AI安全应用提供了可行方案。

Abstract: This paper proposes a hardware-aware intrusion detection system (IDS) for Internet of Things (IoT) and Industrial IoT (IIoT) networks; it targets scenarios where classification is essential for fast, privacy-preserving, and resource-efficient threat detection. The goal is to optimize both tree-based machine learning (ML) models and compact deep neural networks (DNNs) within strict edge-device constraints. This allows for a fair comparison and reveals trade-offs between model families. We apply constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D convolutional neural networks (1D-CNNs). Evaluation on the Edge-IIoTset benchmark shows that selected models meet tight flash, RAM, and compute limits: LightGBM achieves 95.3% accuracy using 75 KB flash and 1.2 K operations, while the HW-NAS-optimized CNN reaches 97.2% with 190 KB flash and 840 K floating-point operations (FLOPs). We deploy the full pipeline on a Raspberry Pi 3 B Plus, confirming that tree-based models operate within 30 ms and that CNNs remain suitable when accuracy outweighs latency. These results highlight the practicality of hardware-constrained model design for real-time IDS at the edge.

</details>


### [565] [Physics Enhanced Deep Surrogates for the Phonon Boltzmann Transport Equation](https://arxiv.org/abs/2512.05976)
*Antonio Varagnolo,Giuseppe Romano,Raphaël Pestourie*

Main category: physics.comp-ph

Relevance: 35.0

TL;DR: PEDS：一种物理增强的深度代理模型，通过结合可微分的傅里叶求解器和神经网络生成器，显著降低纳米级热材料设计中的计算成本和数据需求。


<details>
  <summary>Details</summary>
Motivation: 纳米级材料设计需要控制热流，但传统的玻尔兹曼输运方程求解成本过高，现有代理方法要么牺牲精度要么需要大量高保真模拟。需要一种快速、数据高效且可靠的代理模型来覆盖弹道和扩散两种传输机制。

Method: 提出物理增强深度代理模型（PEDS），结合可微分傅里叶求解器（物理归纳偏置）和神经网络生成器，学习几何相关的修正和混合系数。采用不确定性驱动的主动学习策略。

Result: 相比纯数据驱动基线，训练数据需求减少70%；仅需300个高保真BTE模拟即可达到约5%的相对误差；能够高效设计孔隙率在12-85 W m⁻¹ K⁻¹范围内的几何结构，平均设计误差为4%。

Conclusion: 嵌入简单、可微分的低保真物理模型能显著提高代理模型的数据效率和可解释性，使纳米级热材料设计的重复PDE约束优化变得实用。

Abstract: Designing materials with controlled heat flow at the nano-scale is central to advances in microelectronics, thermoelectrics, and energy-conversion technologies. At these scales, phonon transport follows the Boltzmann Transport Equation (BTE), which captures non-diffusive (ballistic) effects but is too costly to solve repeatedly in inverse-design loops. Existing surrogate approaches trade speed for accuracy: fast macroscopic solvers can overestimate conductivities by hundreds of percent, while recent data-driven operator learners often require thousands of high-fidelity simulations. This creates a need for a fast, data-efficient surrogate that remains reliable across ballistic and diffusive regimes. We introduce a Physics-Enhanced Deep Surrogate (PEDS) that combines a differentiable Fourier solver with a neural generator and couples it with uncertainty-driven active learning. The Fourier solver acts as a physical inductive bias, while the network learns geometry-dependent corrections and a mixing coefficient that interpolates between macroscopic and nano-scale behavior. PEDS reduces training-data requirements by up to 70% compared with purely data-driven baselines, achieves roughly 5% fractional error with only 300 high-fidelity BTE simulations, and enables efficient design of porous geometries spanning 12-85 W m$^{-1}$ K$^{-1}$ with average design errors of 4%. The learned mixing parameter recovers the ballistic-diffusive transition and improves out of distribution robustness. These results show that embedding simple, differentiable low-fidelity physics can dramatically increase surrogate data-efficiency and interpretability, making repeated PDE-constrained optimization practical for nano-scale thermal-materials design.

</details>


### [566] [Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict](https://arxiv.org/abs/2512.06210)
*Daniel Mittermaier,Tobias Bohne,Martin Hofer,Daniel Racek*

Main category: stat.AP

Relevance: 35.0

TL;DR: 该论文提出了一种量化冲突预测不确定性的方法，通过结合多种树基分类器和分布回归器，从传统点预测转向完整预测分布，在PRIO-GRID月度级别上预测暴力冲突致死人数。


<details>
  <summary>Details</summary>
Motivation: 暴力冲突致死人数预测存在高度不确定性，限制了其实际应用价值。论文旨在解决两个主要不确定性来源：暴力冲突的本质特性和数据限制，并将此问题置于机器学习不确定性量化更广泛的文献背景中。

Method: 开发了冲突预测不确定性量化策略，采用自定义自动机器学习设置，比较和结合多种树基分类器和分布回归器，为每个PRIO-GRID月度单元单独估计预测分布。同时测试了区域模型在空间集成中的整合，作为减少不确定性的潜在途径。

Result: 模型能够持续优于基于冲突历史的一系列基准预测，在提前一年的预测中表现良好，性能主要来自观察到冲突的区域。评估强调了理解特定预测问题中指标行为的重要性，特别是在极端零膨胀的情况下。虽然较小模型的整合没有带来更好的预测，但也没有降低性能。

Conclusion: 该研究为冲突预测提供了不确定性量化框架，从点预测转向分布预测。较小模型的整合虽然没有改善预测，但保持了性能，为未来整合空间覆盖较少的数据源开辟了途径。强调了理解预测问题特定特征（如零膨胀）对评估指标选择的重要性。

Abstract: Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.

</details>


### [567] [Approximate Multiplier Induced Error Propagation in Deep Neural Networks](https://arxiv.org/abs/2512.06537)
*A. M. H. H. Alahakoon,Hassaan Saadat,Darshana Jayasinghe,Sri Parameswaran*

Main category: cs.AR

Relevance: 35.0

TL;DR: 提出一个分析框架，将近似乘法器的统计误差特性与DNN精度损失关联起来，发现误差均值（偏差）是影响精度的主要因素，并通过FPGA实现验证了分析结果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络严重依赖密集算术运算，使用近似乘法器可以降低硬件加速器的能耗，但目前缺乏对近似乘法器误差分布如何影响DNN精度的严格数学分析。

Method: 开发一个分析框架，将近似乘法器的统计误差矩与通用矩阵乘法（GEMM）中的失真联系起来。使用误差矩阵的Frobenius范数，推导出适用于实际DNN维度的闭式表达式。通过在GEMM和卷积层中注入受控误差，并在ImageNet规模网络上进行评估。

Result: 分析表明失真主要由乘法器的均值误差（偏差）主导。预测的失真与观察到的精度下降强相关。在FPGA上实现的可配置误差近似乘法器案例研究进一步证实了分析趋势。

Conclusion: 该框架为行为级或硬件级仿真提供了轻量级替代方案，能够快速估计近似乘法器对DNN推理质量的影响。

Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.

</details>


### [568] [Learning to Hedge Swaptions](https://arxiv.org/abs/2512.06639)
*Zaniar Ahmadi,Frédéric Godin*

Main category: q-fin.RM

Relevance: 35.0

TL;DR: 本文研究基于强化学习的深度对冲框架在掉期期权动态对冲中的应用，对比传统基于敏感性的rho对冲方法。设计了三种不同目标函数的智能体，发现使用两种互换作为对冲工具可达到接近最优的对冲效果，深度对冲策略在市场不同状态下动态调整风险暴露，即使在模型误设情况下也优于传统rho对冲。


<details>
  <summary>Details</summary>
Motivation: 传统掉期期权对冲方法（如rho对冲）基于敏感性分析，可能无法充分应对市场动态变化和风险偏好差异。需要探索更灵活、自适应的对冲策略，强化学习能够学习复杂市场动态并优化不同风险目标下的对冲决策。

Method: 采用深度对冲框架（基于强化学习），设计三种不同目标函数的智能体：均方误差、下行风险和条件风险价值。使用三因子无套利动态Nelson-Siegel模型进行模拟实验，对比传统rho对冲策略。

Result: 实验表明：1）使用两种互换作为对冲工具可达到接近最优的对冲效果；2）深度对冲策略能动态调整对冲组合的风险暴露；3）即使在模型误设情况下，深度对冲策略仍优于传统rho对冲方法。

Conclusion: 强化学习在掉期期权对冲中具有潜力，能够提供更高效、更具韧性的对冲策略，能够适应不同风险偏好并动态调整市场状态下的风险暴露。

Abstract: This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.

</details>


### [569] [FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving](https://arxiv.org/abs/2512.06676)
*Wei-Bin Kou,Guangxu Zhu,Bingyang Cheng,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

Relevance: 35.0

TL;DR: FedDSR提出了一种联邦学习新范式，通过多中间层监督和正则化解决自动驾驶中非IID数据导致的泛化差、收敛慢问题，在语义分割任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在自动驾驶中面临非独立同分布数据导致的泛化能力差和收敛速度慢的挑战，需要新的方法来提升模型性能和训练效率。

Method: FedDSR包含三个核心策略：1）基于架构无关标准选择多个中间层；2）计算互信息和负熵作为中间损失和正则项；3）基于这些规则聚合车辆模型生成全局模型。

Result: 在语义分割任务上，FedDSR相比其他联邦学习基线实现了最高8.93%的mIoU提升和28.57%的训练轮次减少。

Conclusion: FedDSR通过中间层监督和正则化有效提升了联邦学习在自动驾驶中的泛化能力和收敛速度，适合实际部署。

Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.

</details>


### [570] [Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields](https://arxiv.org/abs/2512.06912)
*Rushiraj Gadhvi,Sandeep Manjanna*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出基于强化学习的端到端框架，让自主水面车辆在涡流场中实现节能导航，仅使用局部速度测量，相比现有技术节能30-50%


<details>
  <summary>Details</summary>
Motivation: 受传统航海者利用洋流导航的启发，解决自主水面车辆在严格能量预算下长期任务中的节能导航问题。传统路径规划方法在部分可观测的涡流场中效果不佳。

Method: 基于Soft Actor Critic的端到端强化学习框架，仅使用局部速度测量学习流感知导航策略，无需全局流场信息。

Result: 在多样化和动态丰富的场景中评估，方法展示了显著的节能效果，并能鲁棒地泛化到未见过的流场条件，导航路径相比现有技术节能30-50%。

Conclusion: 该学习框架为海洋环境中的长期自主性提供了有前景的路径，通过模仿传统航海者的直觉实现节能导航。

Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.

</details>


### [571] [Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets](https://arxiv.org/abs/2512.06945)
*Nabil Alami,Jad Zakharia,Souhaib Ben Taieb*

Main category: stat.ML

Relevance: 35.0

TL;DR: SACP是一种对称聚合共形预测方法，通过将多个预测模型的非共形分数转换为e值，并使用对称聚合函数进行组合，从而生成更精确的预测集。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，针对同一任务训练多个预测模型变得越来越普遍。如何聚合这些模型的预测不确定性以产生可靠且高效的量化是一个关键但尚未充分探索的挑战，特别是在共形预测框架下。虽然共形预测方法可以从每个模型生成单独的预测集，但将它们组合成单个更信息丰富的集合仍然是一个难题。

Method: 提出SACP（对称聚合共形预测）方法，将多个预测器的非共形分数转换为e值，并使用任何对称聚合函数进行组合。这种灵活设计使得能够选择产生更尖锐预测集的聚合策略，形成一个鲁棒的数据驱动框架。

Result: 在多样化数据集上的广泛实验表明，SACP在效率上持续改进，并且通常优于最先进的模型聚合基线方法。

Conclusion: SACP提供了一种有效的框架来聚合多个预测模型的不确定性，通过对称聚合函数生成更精确的预测集，在共形预测领域具有重要价值。

Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.

</details>


### [572] [Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control](https://arxiv.org/abs/2512.06973)
*Shuo Liu,Wenliang Liu,Wei Xiao,Calin A. Belta*

Main category: eess.SY

Relevance: 35.0

TL;DR: 提出了一种可行性感知学习框架，将可训练时变高阶控制屏障函数嵌入可微二次规划中，用于STL规范驱动的机器人控制，自动调整参数以减少保守性并提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CBF-STL方法依赖固定超参数和短视的逐时间步优化，导致过度保守行为、在紧输入限制附近不可行，以及难以满足长时域STL任务。需要解决这些限制。

Method: 提出可行性感知学习框架，将可训练时变HOCBF嵌入可微QP。构建STL片段的时间变HOCBF约束，引入统一鲁棒性度量。使用三个神经网络（InitNet、RefNet和扩展BarrierNet）协作生成参考输入并自适应调整超参数。

Result: 仿真结果表明，该框架在紧输入限制下保持高STL鲁棒性，在复杂环境中显著优于固定参数和非自适应基线方法，实现严格可行的dQP和STL满足。

Conclusion: 该框架提供了一种系统方法，通过自动调整参数减少保守性，提高STL任务满足的鲁棒性，无需手动调参，在机器人控制中具有实际应用价值。

Abstract: Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.

</details>


### [573] [Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits](https://arxiv.org/abs/2512.07209)
*Masato Ishii,Akio Hayakawa,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.MM

Relevance: 35.0

TL;DR: 提出了一种新颖的音频-视觉联合编辑流程，通过视频到音频生成模型增强编辑后视频与音频的连贯性，实现动态调整源音频影响并保持内容完整性。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑技术通常只关注视觉修改，而忽略了音频与编辑后视频的连贯性。这导致音频-视觉内容不匹配的问题，影响了整体编辑质量。需要一种能够同时处理音频和视觉编辑的联合方法。

Method: 1. 先应用最先进的视频编辑技术生成目标视频
2. 开发新的视频到音频生成模型，以源音频、目标视频和文本提示为条件
3. 扩展模型架构以包含条件音频输入
4. 提出数据增强策略提高训练效率
5. 根据编辑复杂度动态调整源音频的影响程度

Result: 实验结果表明，该方法在保持音频-视觉对齐和内容完整性方面优于现有方法。能够有效处理不同复杂度的编辑任务，同时保持原始音频结构。

Conclusion: 提出的联合音频-视觉编辑流程成功解决了视频编辑中的音频连贯性问题，通过创新的视频到音频生成模型和动态调整机制，实现了高质量的音频-视觉内容同步编辑。

Abstract: We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.

</details>


### [574] [Verifiable Deep Quantitative Group Testing](https://arxiv.org/abs/2512.07279)
*Shreyas Jayant Grampurohit,Satish Mulleti,Ajit Rajwade*

Main category: eess.SP

Relevance: 35.0

TL;DR: 提出基于神经网络的定量群测试（QGT）框架，实现高解码准确性和结构可验证性，通过MLP从噪声测量向量映射到缺陷指示器，并能从网络雅可比矩阵恢复底层池化结构。


<details>
  <summary>Details</summary>
Motivation: 定量群测试（QGT）问题旨在通过远少于候选物品数量的池化测试来识别少量缺陷物品。传统方法在噪声环境下可能不够鲁棒，且缺乏可验证性。本文旨在开发既能准确恢复缺陷物品，又能验证所学结构的神经网络方法。

Method: 使用多层感知机（MLP）训练网络，将噪声测量向量映射到二进制缺陷指示器。网络在稀疏有界扰动下仍能实现准确鲁棒的恢复。关键创新是通过分析训练网络的雅可比矩阵来恢复连接物品与测试的底层池化结构，验证模型是否真正学习到组合关系而非简单记忆模式。

Result: 该方法在QGT问题上实现了高解码准确性，即使在噪声环境下也能鲁棒恢复。更重要的是，从网络雅可比矩阵成功恢复了底层池化结构，证明模型确实内化了QGT的真实组合关系，而不仅仅是记忆训练模式。

Conclusion: 标准前馈架构能够在结构化组合恢复问题中学习可验证的逆映射。这表明神经网络不仅能解决组合优化问题，还能学习并揭示问题的底层结构，为可解释的组合推理提供了新视角。

Abstract: We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.

</details>


### [575] [Equivariant Diffusion for Crystal Structure Prediction](https://arxiv.org/abs/2512.07289)
*Peijia Lin,Pin Chen,Rui Jiao,Qing Mo,Jianhuan Cen,Wenbing Huang,Yang Liu,Dan Huang,Yutong Lu*

Main category: cond-mat.mtrl-sci

Relevance: 35.0

TL;DR: 提出EquiCSP，一种新型等变扩散生成模型，用于晶体结构预测，解决了现有模型中晶格置换等变性和周期性平移等变性的问题。


<details>
  <summary>Details</summary>
Motivation: 晶体结构预测中，现有的对称感知深度学习模型（特别是扩散模型）将CSP视为条件生成任务，但在扩散过程中确保置换、旋转和周期性平移等变性仍未完全解决。

Method: 提出EquiCSP等变扩散生成模型，不仅解决了现有模型中晶格置换等变性的被忽视问题，还开发了一种独特的噪声算法，在训练和推理过程中严格保持周期性平移等变性。

Result: 实验表明，EquiCSP在生成准确结构方面显著超越现有模型，并在训练过程中表现出更快的收敛速度。

Conclusion: EquiCSP通过解决晶格置换等变性和周期性平移等变性问题，为晶体结构预测提供了更有效的等变扩散生成方法。

Abstract: In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.

</details>


### [576] [$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations](https://arxiv.org/abs/2512.07578)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出φ-test方法，结合Shapley归因与选择性推断，用于黑盒预测器的全局特征选择和显著性检验，生成特征重要性表并保持预测性能


<details>
  <summary>Details</summary>
Motivation: 现有黑盒模型的特征重要性分析缺乏统计显著性检验，需要将Shapley归因与经典统计推断结合，提供可靠的特征选择和解释

Method: 基于SHAP引导的特征筛选，通过选择性推断规则拟合线性代理模型，为保留特征计算Shapley全局分数、代理系数、后选择p值和置信区间

Result: 在真实表格回归任务中，φ-test能保留原始模型大部分预测能力，仅使用少量特征，且特征集在不同重采样和骨干模型间保持稳定

Conclusion: φ-test作为实用的全局解释层，将Shapley重要性摘要与经典统计推断连接起来，为黑盒模型提供统计可靠的特征重要性分析

Abstract: We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.

</details>


### [577] [Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks](https://arxiv.org/abs/2512.07697)
*Aileen Liao,Dong-Ki Kim,Max Olan Smith,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

Relevance: 35.0

TL;DR: DA-DP是一个延迟感知扩散策略框架，通过在训练和推理中显式处理推理延迟，将零延迟轨迹校正为延迟补偿版本，提升机器人策略在真实延迟环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人感知和动作选择之间存在推理延迟（数十到数百毫秒），导致观察状态和执行状态之间存在差距。现有方法通常假设零延迟，但在实际部署中延迟会显著影响性能。

Method: 提出延迟感知扩散策略（DA-DP）：1）将零延迟轨迹校正为延迟补偿轨迹；2）通过延迟条件增强策略；3）框架与架构无关，可迁移到扩散策略之外的其他方法。

Result: 在多种任务、机器人和延迟条件下验证，DA-DP相比延迟不感知方法在延迟变化时保持更高的成功率，鲁棒性更强。

Conclusion: DA-DP为延迟感知模仿学习提供通用模式，并鼓励评估协议报告性能随延迟变化的函数关系，而不仅仅是任务难度。

Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.

</details>


### [578] [An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning](https://arxiv.org/abs/2512.07827)
*Lukas Johannes Möller*

Main category: cs.CR

Relevance: 35.0

TL;DR: ADLAH是一个自适应深度学习异常检测蜜网系统，使用强化学习实时决策何时将低交互传感器节点升级为高交互蜜罐，以低成本捕获高价值威胁情报。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂多样，静态蜜罐已不足够，需要自适应、智能驱动的欺骗技术来高效捕获威胁情报。

Method: 提出端到端AI驱动欺骗平台架构，核心是强化学习代理实时决策会话升级，从低交互传感器节点动态配置高交互蜜罐，并自动化提取、聚类和版本化僵尸网络攻击链。

Result: 开发了核心决策机制的功能原型，但缺乏大规模现场验证。详细分析了设计权衡和限制，并提供了规模化实证评估的路线图。

Conclusion: ADLAH为高效捕获高价值对手行为、系统化僵尸网络版本化和生成可操作威胁情报提供了实用路径。

Abstract: The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.

</details>


### [579] [Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification](https://arxiv.org/abs/2512.07463)
*Rongmei Liang,Zizheng Liu,Xiaofei Wu,Jingwen Tu*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出基于共识结构的统一优化框架，开发分布式并行ADMM算法处理分布式存储大数据中的组合正则化支持向量机，引入高斯回代法确保收敛，并应用于音乐信息检索


<details>
  <summary>Details</summary>
Motivation: 在人工智能快速发展的时代，组合正则化支持向量机（CR-SVMs）能有效处理数据特征间的结构信息，但在分布式存储的大数据场景中缺乏高效算法。需要解决分布式环境下CR-SVMs的计算效率问题。

Method: 1) 提出基于共识结构的统一优化框架，适用于多种损失函数和组合正则化项，可扩展到非凸正则化项；2) 开发分布式并行交替方向乘子法（ADMM）算法；3) 引入高斯回代法确保算法收敛；4) 提出稀疏组套索支持向量机（SGL-SVM）模型并应用于音乐信息检索。

Result: 理论分析表明算法计算复杂度不受不同正则化项和损失函数影响，突显并行算法的普适性。在合成和免费音乐档案数据集上的实验验证了算法的可靠性、稳定性和效率。

Conclusion: 提出的统一优化框架和分布式并行ADMM算法能有效解决分布式存储大数据中CR-SVMs的计算问题，具有强可扩展性和普适性，在音乐信息检索等实际应用中表现良好。

Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.

</details>


### [580] [RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.07542)
*Jad Mounayer,Sebastian Rodriguez,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

Relevance: 30.0

TL;DR: RRAEDy是一种用于动力系统的潜在空间模型，能自动发现合适的潜在维度，同时强制潜在空间中的正则化和线性化动态，无需复杂损失平衡或手动调参。


<details>
  <summary>Details</summary>
Motivation: 现有潜在空间模型需要预先固定潜在维度，依赖复杂损失平衡来近似线性动态，且不对潜在变量进行正则化。这些限制影响了模型的灵活性和稳定性。

Method: 基于秩约减自编码器(RRAEs)，RRAEDy通过奇异值自动排序和剪枝潜在变量，同时学习控制时间演化的潜在动态模式分解(DMD)算子。这种无结构但线性约束的公式使模型能够学习稳定、低维的动态。

Result: 在Van der Pol振荡器、Burgers方程、2D Navier-Stokes和旋转高斯等基准测试中，RRAEDy实现了准确且鲁棒的预测。理论分析证明了学习算子的稳定性。

Conclusion: RRAEDy通过自动发现潜在维度和强制线性化动态，解决了现有模型的限制，为动力系统建模提供了更灵活、稳定的解决方案。

Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.

</details>


### [581] [Non-negative DAG Learning from Time-Series Data](https://arxiv.org/abs/2512.07267)
*Samuel Rey,Gonzalo Mateos*

Main category: eess.SP

Relevance: 30.0

TL;DR: 提出一种从多元时间序列数据中学习有向无环图(DAG)的凸优化方法，通过非负边权约束实现凸性，保证全局最优解


<details>
  <summary>Details</summary>
Motivation: 现有连续松弛方法通过邻接矩阵幂的平滑约束函数施加无环性，导致非凸优化问题难以求解。本文利用DAG边权非负的额外结构，通过凸约束实现无环性

Method: 假设底层DAG只有非负边权重，利用此结构通过凸约束施加无环性，将多元时间序列数据的非负DAG恢复问题转化为抽象形式的凸优化问题，使用乘子法求解

Result: 在合成时间序列数据上评估，性能优于现有替代方法

Conclusion: 通过非负边权假设实现凸优化框架，保证全局最优性，为从时间序列数据中学习DAG提供有效方法

Abstract: This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.

</details>


### [582] [Microseismic event classification with a lightweight Fourier Neural Operator model](https://arxiv.org/abs/2512.07425)
*Ayrat Abdullin,Umair bin Waheed,Leo Eisner,Abdullatif Al-Shuhail*

Main category: physics.geo-ph

Relevance: 30.0

TL;DR: 提出基于傅里叶神经算子(FNO)的轻量级模型用于微地震事件分类，在计算效率和分类准确率方面优于传统深度学习模型，适用于实时地震监测。


<details>
  <summary>Details</summary>
Motivation: 实时监测诱发地震活动需要快速准确分类微地震事件，但现有深度学习模型计算需求高，限制了在实际实时监测系统中的应用。需要开发计算效率更高的轻量级模型。

Method: 采用傅里叶神经算子(FNO)构建轻量级模型，利用其固有的分辨率不变性和计算效率处理地震波形数据。在斯坦福地震数据集(STEAD)上进行训练和评估。

Result: 在STEAD数据集上达到95%的F1分数，即使在训练数据稀疏的情况下。在实际微地震数据集上达到98%的F1分数，优于许多传统深度学习技术。计算需求大幅降低。

Conclusion: FNO模型结合了高分类成功率和低计算需求，可作为实时微地震监测的首选方法，节省计算资源并促进后处理和实时地震处理，适用于交通灯系统的实施。

Abstract: Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.

</details>


### [583] [Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra](https://arxiv.org/abs/2512.06059)
*Andrea Della Valle,Annalisa D'Arco,Tiziana Mancini,Rosanna Mosetti,Maria Chiara Paolozzi,Stefano Lupi,Sebastiano Pilati,Andrea Perali*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了一种结合实验数据和条件生成神经网络合成光谱的方法，用于训练能够识别9种挥发性有机化合物（VOCs）并预测其浓度的判别神经网络。


<details>
  <summary>Details</summary>
Motivation: 挥发性有机化合物（VOCs）对人类健康构成重大风险，需要准确检测。红外光谱虽然能实现超灵敏检测，但光谱复杂性限制了实时识别和定量分析。传统深度神经网络需要大量训练数据，而实验数据获取困难。

Method: 1) 创建9种不同类别化合物的实验VOC数据集，包含不同浓度的红外吸收光谱；2) 使用条件生成神经网络合成光谱数据，增加数据量和浓度多样性；3) 利用增强的数据集训练稳健的判别神经网络，实现VOC识别和浓度预测。

Result: 成功训练出能够可靠识别9种VOCs并精确预测其浓度的神经网络。该网络适合集成到VOC识别和分析的传感设备中。

Conclusion: 通过结合实验数据和条件生成神经网络合成光谱的方法，解决了VOC红外光谱识别中数据不足的问题，实现了有效的VOC识别和浓度预测，为实时监测提供了可行方案。

Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.

</details>


### [584] [gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points](https://arxiv.org/abs/2512.06143)
*Marcus M. Noack,Mark D. Risser,Hengrui Luo,Vardaan Tekriwal,Ronald J. Pandolfi*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出gp2Scale方法，通过利用灵活、紧支撑、非平稳核函数在协方差矩阵中自然产生的稀疏结构，将精确高斯过程扩展到超过1000万个数据点，无需依赖诱导点、核插值或邻域近似。


<details>
  <summary>Details</summary>
Motivation: 当前高斯过程扩展方法存在计算速度、预测精度和不确定性量化之间的顽固权衡，大多数方法依赖各种近似降低了准确性并限制了核函数和噪声模型设计的灵活性，而现代应用中表达性非平稳核函数正日益重要。

Method: gp2Scale方法利用高斯过程的核心设计能力：核函数设计。通过使用高度灵活、紧支撑、非平稳的核函数，识别协方差矩阵中自然出现的稀疏结构，然后利用这种稀疏性进行线性系统求解和对数行列式计算以进行训练。

Result: 该方法能够将精确高斯过程扩展到超过1000万个数据点，在多个真实世界数据集上展示了功能，并与最先进的近似算法进行了比较。在许多情况下显示出优越的近似性能，特别是在对任意高斯过程定制（核心核设计、噪声和均值函数）以及输入空间类型的不可知性方面具有优势。

Conclusion: gp2Scale方法通过利用核函数设计中的自然稀疏性，解决了高斯过程扩展中的关键权衡问题，为现代高斯过程应用提供了最优解决方案，特别适合需要高度定制化和表达性核函数的场景。

Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.

</details>


### [585] [Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization](https://arxiv.org/abs/2512.06303)
*Preksha Girish,Rachana Mysore,Kiran K. N.,Hiranmayee R.,Shipra Prashanth,Shrey Kumar*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了一种多模态图神经网络框架，整合结构MRI、扩散张量成像和功能MRI来建模大脑网络的时空重组，通过分数随机微分算子和注意力机制生成可解释的生物标志物，用于预测认知衰退风险。


<details>
  <summary>Details</summary>
Motivation: 理解大脑网络的动态重组对于预测认知衰退、神经进展和临床结果的个体差异至关重要。现有方法需要新的数据收集，而本研究旨在从现有成像数据中提取临床意义的生物标志物。

Method: 提出多模态图神经网络框架：将大脑区域表示为节点，结构和功能连接表示为边，形成纵向大脑图。使用分数随机微分算子嵌入图循环网络来捕获长期依赖和随机波动。注意力机制融合多模态信息并生成可解释的生物标志物（网络能量熵、图曲率、分数记忆指数、模态特定注意力分数）。

Result: 在纵向神经影像数据集上的实验证明了预测准确性和可解释性。结果突出了数学严谨的多模态图方法从现有成像数据中提取临床意义生物标志物的潜力。

Conclusion: 该框架为从现有成像数据中提取临床意义的生物标志物提供了数学严谨的多模态图方法，无需新数据收集，具有临床应用潜力。

Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.

</details>


### [586] [DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction](https://arxiv.org/abs/2512.06356)
*Yifan Song,Fenglin Yu,Yihong Luo,Xingjian Tao,Siya Qiu,Kai Han,Jing Tang*

Main category: cs.LG

Relevance: 25.0

TL;DR: DDFI是一种用于图神经网络中缺失节点特征补全的新方法，通过结合特征传播和图掩码自编码器，解决了现有方法在非全连接图、过平滑问题和归纳任务特征分布偏移方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现实世界图中节点特征经常不完整（如用户属性部分隐私），导致GNN性能显著下降。现有特征传播方法存在三个主要问题：1) 对非全连接图效果差；2) 补全特征面临过平滑问题；3) 仅适用于直推式任务，忽略了归纳任务中的特征分布偏移。

Method: 提出DDFI方法，结合特征传播和图掩码自编码器。首先设计Co-Label Linking算法，随机连接训练集中相同标签的节点以增强多连通分量图的性能。然后在推理阶段采用新颖的两步表示生成过程：不直接使用FP补全特征，而是通过整个MAE重构特征，减少归纳任务中的特征分布偏移并增强特征多样性。

Result: 在六个公共数据集和新收集的Sailing数据集（包含自然缺失特征的航海记录）上的实验表明，DDFI在直推式和归纳式设置下均优于现有最先进方法。

Conclusion: DDFI通过创新的特征传播与图掩码自编码器结合，有效解决了图神经网络中缺失特征补全的关键挑战，特别是在非全连接图和归纳任务场景下表现出色。

Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.

</details>


### [587] [Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction](https://arxiv.org/abs/2512.06357)
*Tony Sallooma,Okyay Kaynak,Xinbo Yub,Wei He*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出基于PID控制的增强方法，用于提升周期性时间序列多步预测的神经网络性能，同时保持系统复杂度不变


<details>
  <summary>Details</summary>
Motivation: 多步时间序列预测在工业决策中至关重要，但神经网络结构的复杂性会影响预测精度。需要一种方法在保持系统复杂度不变的情况下提升预测性能

Method: 提出基于比例-积分-微分（PID）控制的方法，在每个时间步对预测值进行修正，使其更接近真实值。该方法作为后处理增强器，应用于神经网络模型的输出

Result: 在用水需求预测和小时能耗预测两个案例中，PID增强方法显著提升了原始神经网络的预测精度，同时保持了系统复杂度

Conclusion: PID控制方法能有效提升周期性时间序列多步预测的准确性，且对系统复杂度影响可忽略，具有广泛的适用性

Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.

</details>


### [588] [Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator](https://arxiv.org/abs/2512.06417)
*Yifan Sun,Lei Cheng,Jianlong Li,Peter Gerstoft*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出Hankel-FNO模型，结合声传播知识和地形数据，用于高效准确的水下声学制图，在速度和精度上均优于传统数值求解器和数据驱动方法


<details>
  <summary>Details</summary>
Motivation: 传统水下声学制图方法依赖计算昂贵的数值求解器，无法满足大规模或实时应用需求。现有的深度学习替代模型存在固定分辨率限制或依赖显式偏微分方程公式等问题，限制了其在不同环境中的适用性和泛化能力。

Method: 提出Hankel-FNO模型，基于傅里叶神经算子(FNO)，结合声传播知识和地形数据，实现高效准确的水下声学制图。模型能够适应不同环境和声源设置，仅需少量微调。

Result: Hankel-FNO在速度上优于传统求解器，在精度上超越数据驱动替代方法，特别是在长距离预测方面表现优异。实验表明模型对多样化环境和声源设置具有良好适应性。

Conclusion: Hankel-FNO为水下声学制图提供了一种高效准确的解决方案，解决了传统方法和现有深度学习模型的局限性，具有实际应用价值。

Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.

</details>


### [589] [Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts](https://arxiv.org/abs/2512.06652)
*Xiaolei Lu,Shamim Nemati*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出AdaTTT框架，通过自监督学习和原型学习增强ICU患者有创机械通气预测的测试时适应能力，解决医疗数据跨机构域偏移问题。


<details>
  <summary>Details</summary>
Motivation: ICU患者有创机械通气预测在不同医疗机构间存在域偏移问题，导致模型泛化性能下降。需要在不依赖目标域标注数据的情况下实现动态适应。

Method: 提出自适应测试时训练框架AdaTTT：1) 基于信息论推导预测误差边界；2) 使用重建和掩码特征建模的自监督学习；3) 引入原型学习和部分最优传输进行特征对齐。

Result: 在多中心ICU队列实验中，在不同测试时适应基准上展示了有竞争力的分类性能。

Conclusion: AdaTTT能有效缓解医疗数据域偏移问题，提升ICU有创机械通气预测的泛化能力，为临床决策提供支持。

Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.

</details>


### [590] [A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting](https://arxiv.org/abs/2512.06714)
*Tony Salloom,Okyay Kaynak,Wei He*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出了一种用于短期用水需求预测的深度学习模型，通过数据扩展和特征工程降低极端点误差，同时减少模型复杂度


<details>
  <summary>Details</summary>
Motivation: 短期用水需求预测是供水系统优化控制的基础，现有深度学习方法存在参数过多、复杂度高以及在极端点预测误差大的问题

Method: 1) 提出数据扩展方法：在真实数据中插入虚拟数据以缓解极端点周围的非线性；2) 构建新型DL模型：使用GRU处理时序关系，引入K-means无监督分类创建新特征以减少参数数量

Result: 1) 模型复杂度降低至文献中方法的六分之一，同时保持相同精度；2) 数据扩展方法显著减少约30%的预测误差，但增加了训练时间

Conclusion: 该方法有效解决了短期用水需求预测中极端点误差和模型复杂度问题，在保持精度的同时大幅降低模型参数数量

Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.

</details>


### [591] [Decoding Motor Behavior Using Deep Learning and Reservoir Computing](https://arxiv.org/abs/2512.06725)
*Tian Lan*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出ESNNet，结合CNN与回声状态网络(ESN)进行EEG解码，用于运动行为分类，在滑板技巧EEG数据集上超越传统CNN基线


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构(如EEGNet、DeepConvNet)能有效捕捉局部空间模式，但难以建模长程时间依赖和非线性动态。需要结合能处理时间动态的方法来提升EEG解码性能。

Method: 集成回声状态网络(ESN)到解码流程中，ESN构建高维稀疏连接循环储备池，擅长追踪时间动态，与CNN的空间表征能力互补。使用PREP管道预处理EEG数据，在MNE-Python中实现。

Result: 在滑板技巧EEG数据集上，ESNNet达到83.2%的受试者内准确率和51.3%的留一受试者交叉验证准确率，超越广泛使用的CNN基线方法。

Conclusion: 结合CNN空间表征能力和ESN时间动态建模的混合架构能有效提升EEG解码性能，为脑机接口中的运动行为分类提供新方法。

Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals

</details>


### [592] [Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation](https://arxiv.org/abs/2512.07079)
*Anton Morgunov,Victor S. Batista*

Main category: cs.LG

Relevance: 25.0

TL;DR: RetroCast是一个计算机辅助合成规划的统一评估套件，通过标准化模型输出和引入统计严谨的基准测试，揭示了当前方法在化学有效性和长程合成规划方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 计算机辅助合成规划领域缺乏标准化评估基础设施，现有指标过于关注拓扑完成度而忽视化学有效性，导致进展难以客观衡量和比较。

Method: 开发RetroCast统一评估套件，包括：1）将异构模型输出标准化为通用模式；2）可复现的基准测试流程（分层抽样和自助置信区间）；3）SynthArena交互式平台用于定性路线检查；4）在新标准化基准上评估领先的搜索式和序列式算法。

Result: 发现"可解性"（库存终止率）与路线质量之间存在分歧：高可解性分数常掩盖化学无效性，且与实验真实情况再现不相关。识别出"复杂度悬崖"现象：搜索式方法在重建长程合成计划时性能急剧下降，而序列式方法表现更好。

Conclusion: 需要超越简单可解性指标的更全面评估框架，标准化基础设施对透明和可复现的研究至关重要。序列式方法在复杂合成规划中显示出优势。

Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.

</details>


### [593] [Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction](https://arxiv.org/abs/2512.07200)
*Zhen Huang,Jiaxin Deng,Jiayu Xu,Junbiao Pang,Haitao Yu*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出基于强化学习的非均匀道路分段方法用于公交车到达时间预测，通过两阶段框架（强化学习分段+线性预测）实现高效且自适应的分段选择，优于传统均匀分段方法。


<details>
  <summary>Details</summary>
Motivation: 传统公交车到达时间预测采用均匀道路分段策略，无法考虑道路条件、交叉口、兴趣点等物理约束的差异，限制了预测效率。需要一种能自适应学习非均匀道路分段的方法来提升预测性能。

Method: 提出基于强化学习的两阶段框架：1) 使用强化学习框架根据影响分数提取非均匀道路分段；2) 对选定分段应用线性预测模型进行预测。该方法在保持计算效率的同时实现最优分段选择。

Result: 实验结果表明该方法在大规模基准测试上不仅提高了效率，还改善了学习性能。有趣的是，线性方法甚至能比更复杂的方法获得更好的性能。

Conclusion: 基于强化学习的非均匀道路分段方法为公交车到达时间预测提供了一种高效且有效的解决方案，优于传统均匀分段方法，并展示了简单线性模型在适当分段下的强大性能。

Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.

</details>


### [594] [Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects](https://arxiv.org/abs/2512.07393)
*Yann Bourdin,Pierrick Legrand,Fanny Roche*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文研究了截断反向传播通过时间(TBPTT)在数字音频效果建模中的优化，重点关注动态范围压缩，通过调整TBPTT超参数提升模型性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在数字音频效果建模中，特别是动态范围压缩任务，需要有效的序列模型训练方法。TBPTT是训练循环神经网络的关键技术，但其超参数（序列数量、批次大小、序列长度）对模型性能和计算效率有重要影响，需要系统研究。

Method: 采用卷积-循环混合架构，通过大量实验评估TBPTT的三个关键超参数：序列数量、批次大小和序列长度。实验在有无用户控制条件的数据集上进行，使用客观指标和主观听音测试评估性能。

Result: 实验表明，精心调整TBPTT超参数可以显著提升模型精度和训练稳定性，同时降低计算需求。客观评估显示优化设置能改善性能，主观听音测试确认优化后的配置保持了高感知质量。

Conclusion: TBPTT超参数的系统优化对数字音频效果建模至关重要，特别是动态范围压缩任务。适当的参数配置能在保持感知质量的同时提升模型性能和训练效率。

Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.

</details>


### [595] [Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent](https://arxiv.org/abs/2512.07490)
*Zhiyu Liu,Zhi Han,Yandong Tang,Jun Fan,Yao Wang*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出交替预条件梯度下降(APGD)算法，用于解决低管秩张量估计问题，特别针对过参数化设置，通过添加预条件项和交替更新因子来加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统张量奇异值分解方法计算昂贵，不适用于大规模张量。现有因子化方法需要准确估计张量秩，当秩被高估时，梯度下降收敛显著变慢甚至发散。需要一种在过参数化设置下仍能快速收敛的算法。

Method: 提出交替预条件梯度下降(APGD)算法：1) 将张量分解为两个较小的因子张量；2) 在原始梯度中添加预条件项；3) 交替更新这两个因子。算法在过参数化设置下仍能保证线性收敛。

Result: 理论分析表明APGD在过参数化情况下仍能实现线性收敛，且收敛率与张量条件数无关。在低管秩张量分解和恢复的具体案例中验证了理论结果。合成数据上的广泛模拟验证了理论断言。

Conclusion: APGD算法有效解决了低管秩张量估计中的过参数化问题，在秩被高估时仍能保持快速收敛，为大规模张量处理提供了高效解决方案。

Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.

</details>


### [596] [Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity](https://arxiv.org/abs/2512.07723)
*Yonggeon Lee,Jibin Hwang,Alfred Malengo Kondoro,Juhyun Song,Youngtae Noh*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了一种基于Transformer的实时事件预测模型，用于准确预测电动汽车用户的出发时间，以优化充电策略并延长电池寿命。


<details>
  <summary>Details</summary>
Motivation: 电动汽车锂离子电池在长时间高荷电状态下会加速退化，可以通过延迟充满电直到出发前缓解这一问题，但需要准确预测用户出发时间。

Method: 提出Transformer-based实时事件预测模型，将每天表示为TTE序列，通过时间离散化为基于网格的token，利用流式上下文信息而非仅依赖历史模式来预测出发时间。

Result: 在93名用户的真实世界研究和被动智能手机数据评估中，该方法有效捕捉了个体日常中的不规则出发模式，性能优于基线模型。

Conclusion: 该方法展示了实际部署潜力，有助于可持续交通系统的发展。

Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.

</details>


### [597] [Multi-resolution Physics-Aware Recurrent Convolutional Neural Network for Complex Flows](https://arxiv.org/abs/2512.06031)
*Xinlun Cheng,Joseph Choi,H. S. Udaykumar,Stephen Baek*

Main category: physics.flu-dyn

Relevance: 25.0

TL;DR: MRPARCv2是一个多分辨率物理感知循环卷积神经网络，通过嵌入平流-扩散-反应方程结构和多分辨率架构来建模复杂流动，在湍流模拟中相比单分辨率基线显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息机器学习模型在捕捉多尺度流动动力学方面存在局限性，特别是在湍流等复杂流动场景中。需要开发能够有效嵌入物理方程结构并处理多分辨率特征的模型来提高模拟精度和效率。

Method: 提出MRPARCv2模型，嵌入平流-扩散-反应方程结构，采用多分辨率架构，包含分层离散化和跨分辨率特征通信机制。模型在2D湍流辐射层数据集上进行评估，并与单分辨率基线模型对比。

Result: 相比前代模型，MRPARCv2在可训练参数减少30%的情况下，滚动预测误差降低50%，谱误差降低86%。在方差缩放均方根误差和物理驱动指标（湍流动能谱、质量-温度分布）上均有显著改进。

Conclusion: 多分辨率归纳偏置对于捕捉多尺度流动动力学具有优势，但缺乏状态方程的物理约束会导致精度下降。未来物理信息机器学习模型需要嵌入状态方程知识以增强物理保真度。

Abstract: We present MRPARCv2, Multi-resolution Physics-Aware Recurrent Convolutional Neural Network, designed to model complex flows by embedding the structure of advection-diffusion-reaction equations and leveraging a multi-resolution architecture. MRPARCv2 introduces hierarchical discretization and cross-resolution feature communication to improve the accuracy and efficiency of flow simulations. We evaluate the model on a challenging 2D turbulent radiative layer dataset from The Well multi-physics benchmark repository and demonstrate significant improvements when compared to the single resolution baseline model, in both Variance Scaled Root Mean Squared Error and physics-driven metrics, including turbulent kinetic energy spectra and mass-temperature distributions. Despite having 30% fewer trainable parameters, MRPARCv2 outperforms its predecessor by up to 50% in roll-out prediction error and 86% in spectral error. A preliminary study on uncertainty quantification was performed, and we also analyzed the model's performance under different levels of abstractions of the flow, specifically on sampling subsets of field variables. We find that the absence of physical constraints on the equation of state (EOS) in the network architecture leads to degraded accuracy. A variable substitution experiment confirms that this issue persists regardless of which physical quantity is predicted directly. Our findings highlight the advantages of multi-resolution inductive bias for capturing multi-scale flow dynamics and suggest the need for future PIML models to embed EOS knowledge to enhance physical fidelity.

</details>


### [598] [Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions](https://arxiv.org/abs/2512.06109)
*Ajinkya Bhole,Mohammad Mahmoudi Filabadi,Guillaume Crevecoeur,Tom Lefebvre*

Main category: math.OC

Relevance: 25.0

TL;DR: 本文通过Kullback-Leibler正则化提出了随机最优控制问题的统一框架，将策略和状态转移的KL惩罚分离并赋予独立权重，从而推广了标准的轨迹级KL正则化控制。


<details>
  <summary>Details</summary>
Motivation: 现有随机最优控制问题存在多种不同的正则化形式，缺乏统一的数学框架。作者希望通过KL正则化的视角，为经典随机最优控制、风险敏感最优控制及其策略级正则化版本提供一个统一的生成性结构。

Method: 提出一个中心问题，将策略和状态转移的KL惩罚分离并赋予独立权重，构建广义KL正则化控制框架。通过调整权重参数，可以恢复多种经典控制问题，包括SOC、RSOC及其软策略版本。

Result: 1) 建立了统一框架，能够生成多种控制问题；2) 证明了软策略SOC和RSOC是原始问题的上界，可通过迭代恢复原始解；3) 在风险寻求软策略RSOC的特殊同步权重情况下，发现了线性Bellman方程、路径积分解和组合性等有利计算性质。

Conclusion: 通过KL正则化分离策略和状态转移惩罚的统一框架，不仅推广了现有控制问题，还揭示了特殊同步权重情况下的有利计算性质，为更广泛的控制问题提供了计算友好的解决方案。

Abstract: This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.

</details>


### [599] [Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures](https://arxiv.org/abs/2512.06113)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AR

Relevance: 25.0

TL;DR: MERINDA是一个FPGA加速的模型恢复框架，通过流式数据流管道重构计算，利用BRAM分块、定点内核和LUT结构实现细粒度空间并行，在模型恢复任务上比FPGA基线减少6.3倍周期。


<details>
  <summary>Details</summary>
Motivation: 模型恢复是物理AI和实时数字孪生的核心原语，但GPU执行效率低下，存在迭代依赖、内核启动开销、内存带宽未充分利用和高数据移动延迟等问题。

Method: 将计算重构为流式数据流管道，利用BRAM分块、定点内核、LUT结构和进位链加法器的并发使用，实现细粒度空间并行，最小化片外通信。

Result: 在代表性模型恢复工作负载上，MERINDA比基于FPGA的LTC基线减少高达6.3倍的周期，为时间关键物理系统实现实时性能。

Conclusion: 硬件感知的MERINDA框架通过消除同步瓶颈和维持高吞吐量，为模型恢复提供了高效的FPGA加速解决方案。

Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.

</details>


### [600] [SparsePixels: Efficient Convolution for Sparse Data on FPGAs](https://arxiv.org/abs/2512.06208)
*Ho Fung Tsoi,Dylan Rankin,Vladimir Loncar,Philip Harris*

Main category: cs.AR

Relevance: 25.0

TL;DR: SparsePixels：针对空间稀疏图像数据的FPGA高效卷积框架，通过仅计算活跃像素实现73倍推理加速，适用于微秒级延迟要求的约束环境。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在FPGA上推理时，由于需要密集卷积每个输入像素，导致高延迟和长启动间隔，尤其在大尺寸图像上。然而，许多图像数据具有空间稀疏性，语义信息仅占据小部分像素，大部分计算浪费在空区域上。

Method: 提出SparsePixels框架，实现特殊类别CNN，仅选择性地保留和计算活跃像素子集，忽略其余像素。开发支持稀疏CNN构建的库（含量化感知训练）和用于FPGA部署的HLS实现。

Result: 在中微子物理数据集上，标准CNN（4k参数）推理延迟48.665μs，而稀疏CNN（计算<1%输入像素）实现73倍加速至0.665μs，资源利用率在片上预算内，仅损失小百分比性能。在类似稀疏图像数据集上也展示至少一个数量级加速。

Conclusion: 该工作为现代实验（如CERN大型强子对撞机的触发和数据采集系统）中的快速高效数据读出提供了算法开发基础，通过利用图像稀疏性显著加速推理，同时保持可接受性能损失。

Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.

</details>


### [601] [Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability](https://arxiv.org/abs/2512.06294)
*Quentin Badolle,Arthur Theuer,Zhou Fang,Ankit Gupta,Mustafa Khammash*

Main category: q-bio.MN

Relevance: 25.0

TL;DR: DeepSKA：一个用于随机反应网络的神经框架，同时实现可解释性、可靠性保证和计算效率提升


<details>
  <summary>Details</summary>
Motivation: 随机反应网络是化学动力学、流行病学等领域的基础建模框架，但现有方法（如有限状态投影、随机模拟算法）在计算期望输出时存在计算瓶颈。现有深度学习方法虽然具有经验可扩展性，但缺乏可解释性和可靠性保证，限制了其在科学分析和实际决策中的应用。

Method: DeepSKA框架结合了数学透明的表示（可泛化到不同状态、时间和输出函数）与少量随机模拟，产生无偏、可证明收敛且方差显著低于经典蒙特卡洛方法的估计。

Result: 在9个随机反应网络（包括非线性、非质量作用模型，最多10个物种）上验证，DeepSKA提供了准确预测和数量级效率提升。

Conclusion: DeepSKA提供了一个可解释且可靠的神经框架，为开发其他马尔可夫系统（包括随机微分方程）的类似方法奠定了原则性基础。

Abstract: Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.

</details>


### [602] [Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders](https://arxiv.org/abs/2512.06348)
*Xiaoyu Ma,Likun Zhang,Christopher K. Wikle*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出条件变分自编码器(cXVAE)建模极端天气事件的时空共现，通过CNN将气候指数与空间依赖结合，实现高效模拟、条件依赖检测和反事实实验


<details>
  <summary>Details</summary>
Motivation: 极端天气事件的时空共现现象在气候变化背景下可能增强或减弱，需要开发能够整合气候指数、模拟空间依赖、检测条件驱动变化并进行反事实分析的方法

Method: 提出条件变分自编码器(cXVAE)，在解码器中嵌入CNN，将气候指数与潜在空间中的空间依赖进行卷积，使解码器依赖于气候变量

Result: cXVAE能够准确模拟空间场、恢复时空变化的极值依赖，计算成本低；提供可扩展的条件驱动变化检测方法；支持反事实实验量化联合尾部风险、共现范围和重现期指标差异

Conclusion: cXVAE为极端天气事件的时空建模提供了高效、可扩展的框架，能够整合气候指数、检测条件依赖变化，并支持反事实分析，在澳大利亚火灾天气指数与ENSO指数的应用中展示了实用价值

Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.

</details>


### [603] [Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals](https://arxiv.org/abs/2512.06435)
*Mara Sherlin Talento,Jordan Richards,Raphael Huser,Hernando Ombao*

Main category: stat.ML

Relevance: 25.0

TL;DR: 该论文提出了一种分析大脑皮层区域间极端依赖关系的新方法，通过尾部连接性特征来识别癫痫等极端事件，并开发了尾部典型相关分析的可视化工具。


<details>
  <summary>Details</summary>
Motivation: 传统的大脑连接性分析方法在识别极端事件（如癫痫发作）方面存在局限，无法捕捉分布尾部特有的依赖模式。现有尾部依赖建模方法多为成对汇总度量或参数模型，不能识别驱动最大尾部依赖的具体通道，而这在癫痫脑电图分析中至关重要。

Method: 1. 将传统信号处理中的典型相关分析扩展到分布尾部，开发尾部典型依赖度量；2. 通过尾部成对依赖矩阵（TPDM）开发计算高效的估计器；3. 创建可视化工具展示极端通道贡献；4. 应用该方法进行基于频率的软聚类，区分有癫痫和无癫痫的新生儿。

Result: 尾部连接性提供了额外的判别能力，能够更准确地识别极端相关事件，改善癫痫风险管理。该方法成功实现了新生儿的准确频率软聚类，有效区分有癫痫和无癫痫的个体。

Conclusion: 尾部连接性分析揭示了极端事件（如癫痫发作）的独特特征，为大脑状态识别提供了新的视角。扩展的尾部典型相关分析方法能够识别驱动最大尾部依赖的具体通道，在癫痫诊断和管理中具有重要应用价值。

Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.

</details>


### [604] [Learning Conditional Independence Differential Graphs From Time-Dependent Data](https://arxiv.org/abs/2512.06960)
*Jitendra K Tugnait*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出了一种用于估计两个时间序列高斯图模型条件独立图差异的方法，通过惩罚D-trace损失函数在频域进行差分图学习，考虑了数据的时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注独立同分布数据的精度矩阵差异估计，但实际应用中数据往往具有时间依赖性。本文旨在估计两个时间序列高斯图模型的逆功率谱密度差异，以更好地表征时间依赖数据中条件依赖关系的变化。

Method: 在频域中使用惩罚D-trace损失函数进行差分图学习，利用Wirtinger微积分。考虑凸惩罚（群lasso）和非凸惩罚（log-sum和SCAD群惩罚）。提出交替方向乘子法（ADMM）算法优化目标函数。

Result: 在高维设置下建立了逆功率谱密度在Frobenius范数下收敛到真实值的一致性和图恢复的充分条件。合成数据实验表明，log-sum惩罚的差分时间序列图估计器显著优于lasso方法，而lasso方法又显著优于现有的独立同分布建模方法。

Conclusion: 提出的方法能够有效估计时间序列数据中条件依赖关系的变化，特别适用于具有时间依赖性的数据，相比独立同分布假设的方法有显著性能提升。

Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.

</details>


### [605] [Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging](https://arxiv.org/abs/2512.06977)
*Laurentius Valdy,Richard D. Paul,Alessio Quercia,Zhuo Cao,Xuan Zhao,Hanno Scharr,Arya Bangun*

Main category: eess.IV

Relevance: 25.0

TL;DR: 提出了一种结合分区扩散先验与物理约束的多切片重建框架，显著降低GPU内存使用同时保持高质量重建，在MRI和4D-STEM上优于纯物理方法和完整多切片重建基线


<details>
  <summary>Details</summary>
Motivation: 医学和科学成像中，从有限测量数据实现准确的多切片重建对于加速采集过程至关重要，但由于问题的病态性以及高计算和内存需求，这仍然具有挑战性

Method: 提出一个框架，通过将分区扩散先验与基于物理的约束相结合，显著减少每个GPU的内存使用，同时保持高重建质量

Result: 在磁共振成像（MRI）和四维扫描透射电子显微镜（4D-STEM）上，该方法优于纯物理方法和完整多切片重建基线，同时提高了分布内准确性并展现出对分布外数据集的强泛化能力

Conclusion: 该方法通过结合分区扩散先验与物理约束，有效解决了多切片重建中的内存和计算挑战，在保持高质量重建的同时显著降低了资源需求

Abstract: Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.

</details>


### [606] [Machine learning in an expectation-maximisation framework for nowcasting](https://arxiv.org/abs/2512.07335)
*Paul Wilsens,Katrien Antonio,Gerda Claeskens*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出一个基于期望最大化(EM)框架的即时预测方法，使用机器学习技术建模事件发生和报告过程，支持高维协变量，在非线性效应下优于传统GLM方法


<details>
  <summary>Details</summary>
Motivation: 决策常面临信息不完整问题，导致风险估计偏差。实践中，信息不完整常由报告或观察延迟引起。需要利用可观测信息学习完整信息（即时预测），特别是在处理高维协变量和非线性效应时，现有基于广义线性模型的方法存在局限。

Method: 提出基于期望最大化(EM)的即时预测框架，使用机器学习技术（神经网络和XGBoost）建模事件发生和报告过程。允许包含特定于发生期和报告期的协变量信息以及实体特征。定制化最大化步骤和EM迭代间的信息流以利用神经网络的预测能力。

Result: 模拟实验表明，该方法能有效建模高维协变量下的事件发生和报告过程。在非线性效应存在时，优于使用广义线性模型的现有EM框架。在阿根廷COVID-19病例报告应用中，XGBoost方法表现最佳。

Conclusion: 提出的EM框架结合机器学习技术能有效处理即时预测问题，特别是在高维协变量和非线性效应场景下，为决策提供更准确的风险估计。

Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.

</details>


### [607] [High-Dimensional Change Point Detection using Graph Spanning Ratio](https://arxiv.org/abs/2512.07541)
*Youngwen Sun,Katerina Papagiannouli,Vladimir Spokoiny*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出一种基于图论的跨维度变化检测算法，适用于欧几里得和图结构数据，在离线/在线场景中都能控制错误概率，检测能力达到最小化分离率下界。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法在处理高维数据、未知分布和在线场景时存在局限性，需要一种能同时处理欧几里得和图结构数据、控制错误概率、适用于在线环境的通用算法。

Method: 基于图论方法，提出图跨越算法(graph-spanning algorithm)，通过构建数据图结构来检测变化。算法适用于未知分布数据，能控制错误概率，理论分析基于最小化分离率框架。

Result: 算法在检测能力上超越现有技术，对高斯和非高斯数据都表现优异。特别在在线环境中，即使观测窗口很小也能保持强检测能力，实现及时精确的变化检测。

Conclusion: 提出的图跨越算法为跨维度变化检测提供了通用解决方案，在理论保证和实际性能上都优于现有方法，特别适合在线监控等实时应用场景。

Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.

</details>


### [608] [Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion](https://arxiv.org/abs/2512.07755)
*Brenda Anague,Bamdad Hosseini,Issa Karambal,Jean Medard Ngnotchouye*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出了一种基于神经正切核的加权自适应PINNs方法，用于解决具有未知速度和扩散系数的2D/3D对流扩散方程中的源反演和参数估计问题


<details>
  <summary>Details</summary>
Motivation: 在大气科学和环境监测中，从稀缺数据中同时估计排放源位置以及控制速度剖面和扩散参数的多个模型参数是一个困难任务。传统方法难以处理这种高度不适定问题。

Method: 扩展PINNs方法，采用基于神经正切核的加权自适应策略，将源反演和参数估计问题转化为联合恢复问题，利用偏微分方程作为约束耦合多个未知函数参数

Result: 通过多种数值实验（使用不同类型的测量数据模拟实际工程系统）表明，该方法能够成功且鲁棒地解决源反演和参数估计问题，对测量噪声具有鲁棒性

Conclusion: 提出的加权自适应PINNs方法能够有效利用有限测量信息，成功解决高度不适定的源反演和参数估计问题，为工程和科学计算中的逆问题提供了新解决方案

Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.

</details>


### [609] [LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout](https://arxiv.org/abs/2512.07808)
*M. A. Farooq,G. Di Guglielmo,A. Rajagopala,N. Tran,V. A. Chhabria,A. Arora*

Main category: quant-ph

Relevance: 25.0

TL;DR: LUNA：一种用于超导量子比特读取的快速高效加速器，结合低成本积分器预处理和基于查找表的神经网络，实现低延迟、低资源消耗的量子比特状态分类。


<details>
  <summary>Details</summary>
Motivation: 量子计算中量子比特读取是关键操作，需要将模拟响应映射为离散经典状态。现有基于深度神经网络的硬件实现资源密集且延迟高，限制了在低延迟解码和量子纠错循环中的实际应用。

Method: 提出LUNA架构：1）使用低成本积分器进行维度缩减预处理；2）采用LogicNets（将DNN合成为LUT逻辑）进行分类；3）结合差分进化探索优化框架寻找高质量设计点。

Result: 相比最先进技术，LUNA实现了最高10.95倍面积缩减和30%延迟降低，同时保持保真度几乎没有损失。

Conclusion: LUNA实现了可扩展、低占用、高速的量子比特读取，支持构建更大更可靠的量子计算系统。

Abstract: Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.

</details>


### [610] [Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces](https://arxiv.org/abs/2512.07820)
*Prithila Angkan,Amin Jalali,Paul Hungler,Ali Etemad*

Main category: cs.HC

Relevance: 25.0

TL;DR: 提出GEEGA方法，通过图卷积网络融合脑电图的多域信息（频率地形图和时频谱图），使用中心损失和成对差异损失增强类间可分性，并采用梯度对齐策略解决不同域间的梯度冲突。


<details>
  <summary>Details</summary>
Motivation: 脑电图信号具有时间动态性和个体敏感性，导致类间可分性差，传统方法难以有效融合多域信息。需要一种能同时处理多域信息并解决梯度冲突的方法来学习更好的EEG表征。

Method: 1. 使用图卷积网络融合频率地形图和时频谱图的嵌入表示；2. 引入中心损失和成对差异损失增强类间可分性；3. 提出梯度对齐策略，解决不同域和融合嵌入之间的梯度冲突，确保统一优化方向。

Result: 在三个公开EEG数据集（BCI-2a、CL-Drive、CLARE）上验证了方法的有效性，并通过消融研究证明了各组件的重要性。

Conclusion: GEEGA通过多域信息融合和梯度对齐，有效提升了EEG表征学习性能，为脑机接口提供了更好的特征表示方法。

Abstract: We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.

</details>


### [611] [PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations](https://arxiv.org/abs/2512.06183)
*Lindong Liu,Zhixiong Jin,Seongjin Choi*

Main category: cs.LG

Relevance: 20.0

TL;DR: PMA-Diffusion：一种物理引导的掩码感知扩散框架，用于从稀疏观测中重建高速公路速度场


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要高分辨率交通状态信息，但现有传感器数据（如环形检测器和探测车）通常过于稀疏和嘈杂，难以捕捉交通流的详细动态

Method: 提出物理引导的掩码感知扩散框架，采用两种掩码感知训练策略（单掩码和双掩码）直接在稀疏观测的速度场上训练扩散先验，推理阶段使用物理引导的后验采样器交替进行反向扩散更新、观测投影和基于自适应各向异性平滑的物理引导投影

Result: 在I-24 MOTION数据集上测试，即使在仅5%可见度的严重稀疏情况下，PMA-Diffusion在三个重建误差指标上都优于其他基线方法，且使用稀疏观测训练的性能接近在完整观测上训练的基线模型

Conclusion: 结合掩码感知扩散先验和物理引导后验采样器为实际传感稀疏条件下的交通状态估计提供了可靠且灵活的解决方案

Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.

</details>


### [612] [Neural Factorization-based Bearing Fault Diagnosis](https://arxiv.org/abs/2512.06837)
*Zhenhao Li,Xu Cheng,Yi Zhou*

Main category: cs.LG

Relevance: 20.0

TL;DR: 提出基于神经分解的分类(NFC)框架用于高铁轴承故障诊断，通过将振动时间序列嵌入到多个模式相关的潜在特征向量中，并利用神经分解原理融合这些向量，有效挖掘原始时间序列中的复杂故障特征。


<details>
  <summary>Details</summary>
Motivation: 高铁轴承作为列车运行系统的核心部件，其健康状况直接关系到列车运行安全。传统诊断方法在复杂工况下诊断精度不足，需要更有效的故障诊断方法。

Method: 提出神经分解分类(NFC)框架：1)将振动时间序列嵌入到多个模式相关的潜在特征向量中，捕捉多样化的故障相关模式；2)利用神经分解原理将这些向量融合为统一的振动表示。基于CP和Tucker融合方案分别实例化为CP-NFC和Tucker-NFC模型。

Result: 实验结果表明，两种模型相比传统机器学习方法都取得了优越的诊断性能。比较分析为高铁轴承监测中选择有效诊断策略提供了有价值的经验证据和实践指导。

Conclusion: 提出的NFC框架能够有效挖掘原始时间序列数据中的复杂潜在故障特征，为高铁轴承故障诊断提供了有效的解决方案。

Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.

</details>


### [613] [A self-driving lab for solution-processed electrochromic thin films](https://arxiv.org/abs/2512.05989)
*Selma Dahms,Luca Torresi,Shahbaz Tareq Bandesha,Jan Hansmann,Holger Röhm,Alexander Colsmann,Marco Schott,Pascal Friederich*

Main category: cs.LG

Relevance: 15.0

TL;DR: 利用自动驾驶实验室加速电致变色涂层开发，结合自动化与机器学习优化旋涂工艺参数


<details>
  <summary>Details</summary>
Motivation: 溶液处理电致变色材料在智能窗户和显示器中应用潜力大，但旋涂工艺参数优化复杂，传统方法开发周期长，需要更高效的优化方法

Method: 构建自动驾驶实验室系统，整合自动数据采集、图像处理、光谱分析和贝叶斯优化，通过机器学习高效探索旋涂工艺参数空间

Result: 系统显著提高实验通量，实现针对性的工艺参数优化搜索，能够快速找到最优的电致变色涂层制备条件

Conclusion: 自动驾驶实验室方法可加速电致变色材料开发，该框架可推广到其他溶液处理材料，展示了自驱动实验室在材料发现和工艺优化中的潜力

Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.

</details>


### [614] [A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts](https://arxiv.org/abs/2512.06111)
*Arthur Mukwaya,Nancy Kasamala,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi,Denis Ruganuza,Mark Ngotonie*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一种机器学习框架，用于识别进行短期交通计数的最佳代表日，以提高年度平均日交通量预测精度。


<details>
  <summary>Details</summary>
Motivation: 美国各州交通部门需要收集可靠的年度平均日交通量数据，但连续计数站成本高昂难以广泛部署，而短期计数方法准确性不足。需要找到更优的短期计数策略来改善预测精度。

Method: 使用德州2022-2023年交通数据，比较两种方案：1）"最优日"方法迭代选择对AADT估计最具信息量的日期；2）"非最优日"基线反映当前实践。利用连续计数数据模拟24小时短期计数，采用留一法生成无偏的代表性日交通特征。

Result: 最优日方法在前5天均优于基线，最佳日（第186天）的RMSE（7,871.15）、MAE（3,645.09）、MAPE（11.95%）均低于基线（11,185.00、5,118.57、14.42%），R²更高（0.9756 vs 0.9499）。

Conclusion: 该方法为交通部门提供了传统短期计数实践的替代方案，能改善AADT估计、支持公路性能监测系统合规性，并降低全州交通数据收集的运营成本。

Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.

</details>


### [615] [Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media](https://arxiv.org/abs/2512.06293)
*Fatima Ashraf,Muhammad Ayub Sabir,Jiaxin Deng,Junbiao Pang,Haitao Yu*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种新的主题建模方法，通过联合建模语言交互和用户影响力，从社交媒体中检测城市交通服务风险。


<details>
  <summary>Details</summary>
Motivation: 城市交通机构越来越多地使用社交媒体监控服务风险（如拥挤、延误、安全事故），但这些关注信号稀疏、简短且容易被日常聊天淹没。现有方法难以有效提取这些稀疏但有影响力的信号。

Method: 构建影响力加权的关键词共现图，提出泊松解卷积分解（PDF）模型，将图分解为低秩主题结构和主题局部残差交互，使用去相关正则化促进主题区分，并通过一致性驱动的扫描选择主题数量。

Result: 在大规模社交数据流上，该模型实现了最先进的主题一致性和强大的多样性，优于现有基线方法。

Conclusion: 提出的联合建模语言交互和用户影响力的方法能有效从稀疏社交媒体数据中提取城市交通服务风险主题，为交通机构提供可解释的监控工具。

Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git

</details>


### [616] [Diagnosis-based mortality prediction for intensive care unit patients via transfer learning](https://arxiv.org/abs/2512.06511)
*Mengqi Xu,Subha Maity,Joel Dubin*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该研究评估了针对ICU诊断特异性死亡率预测的迁移学习方法，发现迁移学习在性能上优于仅使用诊断特异性数据训练的模型和APACHE IVa评分，且在多种阈值标准下保持高预测性能。


<details>
  <summary>Details</summary>
Motivation: ICU中危重疾病的潜在病因因诊断而异，但现有的预测模型未能系统性地考虑诊断异质性。研究旨在填补这一空白，评估迁移学习方法在诊断特异性死亡率预测中的应用。

Method: 使用eICU协作研究数据库，应用基于GLM和XGBoost的模型，评估迁移学习方法在诊断特异性死亡率预测中的效果，并与仅使用诊断特异性数据训练的模型以及APACHE IVa评分进行比较。

Result: 迁移学习在诊断特异性死亡率预测中始终优于仅使用诊断特异性数据训练的模型和APACHE IVa评分，同时比在汇总数据上训练的模型具有更好的校准性。研究发现Youden截断值比传统的0.5阈值更适合二元结果预测。

Conclusion: 迁移学习是ICU诊断特异性死亡率预测的有效方法，能够处理诊断异质性并提高预测性能。Youden截断值作为决策阈值比传统0.5阈值更合适。

Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.

</details>


### [617] [Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study](https://arxiv.org/abs/2512.06630)
*Chi-Sheng Chen,Xinyu Zhang,Rong Fu,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出量子时序卷积神经网络(QTCNN)，结合经典时序编码器和参数高效量子卷积电路，用于股票横截面收益预测，在JPX东京证券交易所数据集上实现0.538夏普比率，优于最佳经典基准约72%。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型在噪声输入、制度转换和有限泛化能力方面存在困难，量子机器学习为增强股票市场预测提供了有前景的途径，特别是在复杂、噪声大、高度动态的金融环境中。

Method: 提出量子时序卷积神经网络(QTCNN)：1) 经典时序编码器从序列技术指标中提取多尺度模式；2) 参数高效的量子卷积电路利用量子叠加和纠缠增强特征表示并抑制过拟合。

Result: 在JPX东京证券交易所数据集上，QTCNN实现了0.538的夏普比率，比最佳经典基准模型高出约72%。通过样本外夏普比率作为主要性能指标评估长期-短期投资组合构建。

Conclusion: 量子增强的预测模型QTCNN在量化金融中具有实际潜力，能够实现稳健的决策制定，量子处理通过叠加和纠缠增强了特征表示并抑制了过拟合。

Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.

</details>


### [618] [Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning](https://arxiv.org/abs/2512.06649)
*Camellia Zakaria,Aryan Sadeghi,Weaam Jaafar,Junshi Xu,Alex Mariakakis,Marianne Hatzopoulou*

Main category: cs.LG

Relevance: 15.0

TL;DR: 使用交通监控视频和天气数据，通过机器学习模型估计街道级黑碳浓度，填补交通监测与环境后果之间的数据鸿沟。


<details>
  <summary>Details</summary>
Motivation: 城市黑碳排放主要由交通驱动，但现有监测成本高、数据稀缺，而交通监控系统广泛部署。这种不平衡导致我们了解交通状况却不了解其环境后果，需要填补这一数据鸿沟。

Method: 从交通视频中提取视觉信息捕捉车辆行为和状况，结合天气数据，构建机器学习模型来估计街道级黑碳浓度。

Result: 模型达到R平方值0.72，RMSE为129.42 ng/m³，能够有效估计黑碳浓度。

Conclusion: 利用现有城市基础设施资源，通过机器学习生成交通排放相关信息，为污染减排、城市规划、公共卫生和环境正义提供可操作的见解。

Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.

</details>


### [619] [Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data](https://arxiv.org/abs/2512.06730)
*Lin Yang,Xiang Li,Xin Ma,Xinxin Zhao*

Main category: cs.LG

Relevance: 15.0

TL;DR: 本文提出了一种基于增强现实的稳态视觉诱发电位（AR-SSVEP）系统，用于改善运动功能障碍患者的康复训练参与度。该系统结合HoloLens 2采集EEG数据，并采用改进的MACNN-BiLSTM架构进行运动意图识别，同时使用SHAP方法增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 运动功能障碍患者在康复训练中主观参与度低，传统基于SSVEP的脑机接口系统严重依赖外部视觉刺激设备，在实际应用中存在局限性。本研究旨在解决患者主动性不足和治疗师工作负荷高的问题。

Method: 1) 设计基于HoloLens 2的四种EEG类别，采集7名健康受试者的EEG数据；2) 在传统CNN-BiLSTM架构基础上集成多头注意力机制（MACNN-BiLSTM）；3) 提取10个时频EEG特征，通过CNN学习高级表征；4) 使用BiLSTM建模序列依赖关系，应用多头注意力机制突出运动意图相关模式；5) 应用SHAP方法可视化EEG特征对神经网络决策过程的贡献。

Result: 该方法提高了实时运动意图识别的准确性，支持运动障碍患者的康复恢复。SHAP方法的应用增强了模型的可解释性，使EEG特征对决策的贡献可视化。

Conclusion: AR-SSVEP系统结合改进的深度学习架构能够有效识别运动意图，提高康复训练的参与度和效果，为运动障碍患者的康复提供了技术支持。

Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.

</details>


### [620] [Machine Learning: Progress and Prospects](https://arxiv.org/abs/2512.07519)
*Alexander Gammerman*

Main category: cs.LG

Relevance: 15.0

TL;DR: 这篇1996年的就职讲座回顾了机器学习的历史起源，从亚里士多德到20世纪的发展，并讨论了机器学习作为多学科交叉领域的性质。


<details>
  <summary>Details</summary>
Motivation: 讲座旨在探讨机器学习的起源和发展历程，追溯从古代哲学思想到现代算法的演变，强调机器学习作为跨学科领域的特性。

Method: 采用历史回顾和哲学分析的方法，从多个时间节点（14世纪奥卡姆剃刀、18世纪休谟归纳、20世纪统计方法）追溯机器学习思想的起源。

Result: 展示了机器学习思想的深厚历史渊源，揭示了该领域作为多学科交叉融合的特点，涵盖了归纳学习、神经网络、聚类等多个研究方向。

Conclusion: 机器学习并非单一学科，而是受多个学科影响的交叉领域，其思想根源可以追溯到古代哲学，并在20世纪形成了系统的研究框架。

Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.

</details>


### [621] [Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction](https://arxiv.org/abs/2512.06038)
*Kelsey Fontenot,Anjali Gorti,Iva Goel,Tonio Buonassisi,Alexander E. Siemenn*

Main category: cs.RO

Relevance: 15.0

TL;DR: 本文开发了一种用于自驱动实验室的自动基板处理与交换系统（ASHE），通过机器人、双执行器分配器和深度学习计算机视觉技术，实现了对脆弱透明基板的自动化处理，准确率达98.5%。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室在化学和材料发现中加速了实验流程，但基板处理和重载这一关键步骤常被忽视。当前自驱动实验室自动化流程中，用于材料转移或沉积的基板处理仍依赖人工操作，限制了完全自动化的实现。

Method: 开发了ASHE系统，整合了机器人技术、双执行器分配器和基于深度学习的计算机视觉系统。该系统能够检测并纠正脆弱透明基板操作中的错误，实现闭环控制。

Result: 在130次独立试验中，首次放置准确率达到98.5%，仅发生两次基板错位，且这些错误被成功检测并自动纠正。

Conclusion: 通过开发更准确可靠的基板处理方法，提高了自驱动实验室的自动化能力，进一步加速了新型化学和材料发现。

Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.

</details>


### [622] [Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data](https://arxiv.org/abs/2512.06181)
*Yanuo Zhou*

Main category: q-bio.QM

Relevance: 15.0

TL;DR: 提出了一个用于可穿戴光谱数据中自然光与人工光分类的可重复、可审计的基准流程，通过参与者级别的交叉验证评估，在自然光与人工光分类任务上达到AUC 0.938的优异性能。


<details>
  <summary>Details</summary>
Motivation: 可穿戴光谱仪能够在现场量化生物相关光，但目前缺乏用于上下文分类的可重复流程规范。需要建立一个经过验证的、可重复的流程和可操作的设计规则，用于从可穿戴光谱数据中分类自然光与人工光。

Method: 分析了26名参与者的ActLumus记录数据，每人至少监测7天，采样频率10秒，配合每日暴露日记。流程固定了以下序列：域选择、log10变换、排除总强度的L2归一化（避免亮度捷径）、小时级中位数聚合、正弦/余弦小时编码、MLP分类器，并在参与者级别交叉验证下评估。

Result: 所提出的流程在主要任务上持续实现高性能，代表性配置在保留参与者分割的自然光与人工光分类任务上达到AUC = 0.938（准确率88%）。室内与室外分类由于光谱重叠和类别不平衡，仅达到可行性水平（最佳AUC约0.75）。阈值基线方法在数据上表现不足，支持需要超越照度截止值的光谱-时间建模。

Conclusion: 提供了一个可重复、可审计的基准流程和设计规则，用于在参与者级别泛化下的上下文光分类。所有代码、配置文件和衍生成果将公开存档（GitHub + Zenodo DOI），以支持重用和基准测试。

Abstract: Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.
  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.
  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.
  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.
  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.

</details>


### [623] [A Broader View on Clustering under Cluster-Aware Norm Objectives](https://arxiv.org/abs/2512.06211)
*Martin G. Herold,Evangelos Kipouridis,Joachim Spoerhase*

Main category: cs.DS

Relevance: 15.0

TL;DR: 本文研究了(f,g)-聚类问题，改进了先前近似算法的界限，提出了O(log² n)和O(k)近似算法，并设计了基于f和g参数的插值算法。


<details>
  <summary>Details</summary>
Motivation: 动机是解决先前工作中(f,g)-聚类问题近似算法存在的较大差距，为更一般的设置提供更清晰的近似性图景，统一多个基础聚类问题。

Method: 方法包括：1) 为任意f设计O(log² n)近似算法用于(f, L₁)-聚类；2) 为一般(f,g)-聚类问题提供O(k)近似算法；3) 基于新定义的f和g参数设计插值算法。

Result: 结果：1) 将(f, L₁)-聚类的近似比从Õ(√n)改进到O(log² n)；2) 将一般(f,g)-聚类的近似比从Õ(√kn)改进到O(k)；3) 设计了在多个基础聚类问题间插值的算法。

Conclusion: 结论是本文显著改进了(f,g)-聚类问题的近似算法界限，填补了先前工作中的差距，为更一般的聚类设置提供了更好的近似保证。

Abstract: We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.
  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\widetilde{O}(\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\widetilde{O}(\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.
  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.

</details>


### [624] [Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions](https://arxiv.org/abs/2512.06270)
*Nifei Lin,Heng Luo,L. Jeff Hong*

Main category: stat.ML

Relevance: 15.0

TL;DR: 论文研究上下文强凸模拟优化，采用"先优化后预测"方法进行实时决策。离线阶段在协变量集上进行模拟优化以近似最优解函数；在线阶段通过评估该近似值获得决策。主要理论挑战是理解模拟优化算法生成解的近似性如何影响最优性差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了模拟优化算法生成解的近似性对最优性差距的影响。本文旨在填补这一理论空白，为上下文模拟优化中的"先优化后预测"方法提供统一的分析框架。

Method: 开发统一分析框架，显式考虑解偏差和方差。以Polyak-Ruppert平均SGD作为示例模拟优化算法，分析四种代表性平滑技术（k最近邻、核平滑、线性回归、核岭回归）下OTP的最优性差距。

Result: 建立了收敛速率，推导了计算预算Γ在设计协变量数量和每个协变量模拟努力之间的最优分配，证明在适当的平滑技术和样本分配规则下，收敛速率可近似达到Γ^{-1}。

Conclusion: 通过数值研究验证了理论发现，证明了所提方法的有效性和实用价值。为上下文模拟优化中的实时决策提供了理论保证和实用指导。

Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.

</details>


### [625] [A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems](https://arxiv.org/abs/2512.06809)
*Jiong Yang*

Main category: eess.SY

Relevance: 15.0

TL;DR: 提出PA-ALSTM-AE框架，将电池老化规律（里程）融入深度学习，通过多阶段融合机制提升电动汽车电池早期故障诊断性能。


<details>
  <summary>Details</summary>
Motivation: 电动汽车电池安全至关重要，但早期故障诊断面临挑战：异常信号微弱、动态运行噪声干扰。现有数据驱动方法存在"物理盲区"，导致漏检或误报。

Method: 提出物理感知注意力LSTM自编码器（PA-ALSTM-AE）：1）自适应物理特征构建模块选择里程敏感特征；2）物理引导潜在融合模块基于老化状态动态校准LSTM记忆单元。

Result: 在大规模Vloong真实数据集上验证，方法显著优于现有基准。早期故障召回率提升3倍以上，同时保持高精度，为工业电池管理系统提供鲁棒解决方案。

Conclusion: 通过显式集成电池老化规律到深度学习流程，PA-ALSTM-AE有效解决了物理盲区问题，提高了电池早期故障诊断的准确性和可靠性。

Abstract: Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from "physical blindness" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.

</details>


### [626] [Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications](https://arxiv.org/abs/2512.07120)
*J. Allagan,G. Morgan,S. Langley,R. Lopez-Bonilla,V. Deriglazov*

Main category: cs.DS

Relevance: 15.0

TL;DR: 该论文建立了在双色三角形约束下2-树的色特征向量的闭式枚举公式，这些可高效计算的结构特征源自约束图着色，其中每个三角形恰好使用两种颜色，禁止单色和彩虹三角形。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于分布式系统中的实际问题，其中组件需要避免完全集中或完全隔离。双色三角形约束反映了分布式系统中避免单点故障（单色三角形）和过度分散（彩虹三角形）的需求，这在拜占庭容错、云计算的虚拟机分配和分布式密码学的秘密共享协议中都有应用。

Method: 方法包括：1）建立2-树在双色三角形约束下的色特征向量闭式枚举公式；2）针对theta图Θ_n，证明r_k(Θ_n) = S(n-2, k-1)（第二类斯特林数）；3）针对扇图Φ_n，建立r_2(Φ_n) = F_{n+1}（斐波那契数）并推导显式公式；4）分析计算复杂度，theta图为O(n)，扇图为O(n²)。

Result: 主要结果：1）theta图的色特征向量公式r_k(Θ_n) = S(n-2, k-1)（k≥3），r_2(Θ_n) = 2^(n-2) + 1；2）扇图的色特征向量公式r_2(Φ_n) = F_{n+1}，以及r_k(Φ_n) = Σ_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1)；3）这些特征可高效计算，theta图O(n)，扇图O(n²)；4）与经典色多项式不同，这些约束提供了信息丰富的结构特征。

Conclusion: 结论：双色三角形约束下的色特征向量为2-树提供了可高效计算且信息丰富的结构特征，虽然不构成完全图不变量，但通过斐波那契多项式、贝尔数和独立集枚举的联系，捕捉了有意义的结构特性。这些结果在拜占庭容错、云计算和分布式密码学中有实际应用。

Abstract: We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.

</details>


### [627] [DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2512.07162)
*Kieran A. Malandain,Selim Kalici,Hakob Chakhoyan*

Main category: q-fin.CP

Relevance: 15.0

TL;DR: DeepSVM使用物理信息深度算子网络学习Heston模型在整个参数空间上的解算子，无需标注数据，通过硬约束和自适应细化实现高精度期权定价，但希腊字母计算存在噪声。


<details>
  <summary>Details</summary>
Motivation: 随机波动率模型实时校准的计算瓶颈在于需要重复求解耦合偏微分方程，传统方法计算成本高，需要更高效的解决方案。

Method: 提出基于物理信息深度算子网络（PI-DeepONet）的DeepSVM，采用硬约束ansatz强制终端支付和静态无套利条件，使用残差自适应细化（RAR）稳定高梯度区域的训练。

Result: 最终训练损失达到10^{-5}，在典型市场动态范围内预测高度准确的期权价格，但模型的导数（希腊字母）在平价区域表现出噪声。

Conclusion: DeepSVM实现了高效的期权定价，但揭示了物理信息算子学习中需要高阶正则化来改善导数计算。

Abstract: Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.

</details>


### [628] [Two-dimensional RMSD projections for reaction path visualization and validation](https://arxiv.org/abs/2512.07329)
*Rohit Goswami*

Main category: physics.chem-ph

Relevance: 15.0

TL;DR: 提出一种将高维过渡态优化轨迹映射到二维表面的可视化方法，使用反应物和产物结构的RMSD作为坐标，能量用颜色映射表示，便于比较不同优化方法


<details>
  <summary>Details</summary>
Motivation: 传统过渡态和最小能量路径寻找方法通常将轨迹绘制为能量相对于累积位移或图像编号的函数，这种降维方法会掩盖高维结构重排，且轨迹依赖性强，难以比较不同优化方法

Method: 将优化轨迹映射到二维表面：x轴为从反应物结构的RMSD，y轴为从产物结构的RMSD（经过排列校正）。使用径向基函数从所有优化步骤构建能量插值颜色映射表面

Result: 该方法能够清晰展示优化轨迹、识别端点盆地、诊断收敛问题（这些在一维剖面中不可见）。在环加成反应上验证表明，机器学习势能鞍点和密度泛函理论参考点位于可比较的能量等值线上

Conclusion: 提出的二维可视化框架为比较不同过渡态优化方法提供了更丰富的分析工具，超越了传统基于计算次数、时间和最终几何的比较方式

Abstract: Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.

</details>


### [629] [Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems](https://arxiv.org/abs/2512.07458)
*Dmitrii Kapitan,Pavel Ovchinnikov,Konstantin Soldatov,Petr Andriushchenko,Vitalii Kapitan*

Main category: physics.comp-ph

Relevance: 15.0

TL;DR: 该论文系统研究了卷积神经网络在自旋系统模型临界和低温相态分析中的应用，展示了CNN在计算平均能量对交换积分空间分布依赖关系方面的优势，并构建了适用于多种晶格的单卷积相态分类器。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索卷积神经网络作为高效工具来分析自旋系统的临界和低温相态，特别是解决Edwards-Anderson模型中平均能量对交换积分空间分布的依赖关系计算问题，以及开发适用于多种晶格结构的通用相态分类方法。

Method: 方法包括：1) 使用卷积神经网络分析Edwards-Anderson模型在方形晶格上的平均能量计算；2) 构建单卷积分类器，使用Swendsen-Wang团簇算法生成铁磁Ising模型在方形、三角形、蜂窝和Kagome晶格上的配置进行训练；3) 通过计算高温相的平均后验概率温度剖面来确定临界温度。

Result: 结果显示：1) 卷积模型相比全连接架构显著降低均方根误差；2) 计算的高温相平均后验概率温度剖面形成清晰的S形曲线，在理论临界温度附近相交；3) 该方法能够在不重新训练的情况下确定Kagome晶格的临界温度；4) CNN能有效捕捉热力学特性与磁性相关系统结构之间的复杂相关性。

Conclusion: 结论表明卷积神经网络是分析自旋系统相态的高效工具，能够减少计算误差并有效捕捉复杂相关性，为物理系统的相态分析提供了新的计算方法。

Abstract: This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.

</details>


### [630] [On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series](https://arxiv.org/abs/2512.07557)
*Jitendra K. Tugnait*

Main category: stat.ML

Relevance: 15.0

TL;DR: 该论文提出了一种多属性图学习方法，用于从高维多变量高斯时间序列数据中估计条件独立图，采用频域惩罚对数似然目标函数，分析了凸和非凸惩罚函数的理论性质。


<details>
  <summary>Details</summary>
Motivation: 现有图估计方法主要针对单属性模型（每个节点对应一个标量时间序列），而多属性图模型中每个节点代表随机向量或向量时间序列。需要开发适用于多属性依赖时间序列的图学习方法，并提供统一的理论分析框架。

Method: 基于离散傅里叶变换将时域数据转换到频域，构建惩罚对数似然目标函数。考虑两种惩罚函数：凸的稀疏组lasso和非凸的log-sum与SCAD组惩罚。在高维设置下建立一致性、局部凸性和图恢复的理论条件。

Result: 建立了无需不相干性或不可表示性条件的收敛结果，证明了逆功率谱密度在Frobenius范数下收敛到真值的充分条件。通过贝叶斯信息准则选择调优参数，并在合成和真实数据上验证了方法的有效性。

Conclusion: 该研究为多属性时间序列图学习提供了统一的理论框架，支持凸和非凸惩罚函数，在高维设置下具有理论保证，适用于依赖时间序列的条件独立图估计。

Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</details>


### [631] [A scalable and real-time neural decoder for topological quantum codes](https://arxiv.org/abs/2512.07737)
*Andrew W. Senior,Thomas Edlich,Francisco J. H. Heras,Lei M. Zhang,Oscar Higgott,James S. Spencer,Taylor Applebaum,Sam Blackwell,Justin Ledford,Akvilė Žemgulytė,Augustin Žídek,Noah Shutty,Andrew Cowie,Yin Li,George Holland,Peter Brooks,Charlie Beattie,Michael Newman,Alex Davies,Cody Jones,Sergio Boixo,Hartmut Neven,Pushmeet Kohli,Johannes Bausch*

Main category: quant-ph

Relevance: 15.0

TL;DR: AlphaQubit 2是一个神经网络解码器，在表面码和颜色码上实现了接近最优的逻辑错误率，解码速度比现有高精度解码器快几个数量级，支持实时解码。


<details>
  <summary>Details</summary>
Motivation: 量子计算需要极低的错误率，量子纠错是实现这一目标的关键，但需要解码器同时具备快速、准确和可扩展性。现有机器学习解码器或针对资源高效编码（如颜色码）的解码器尚未满足这些要求。

Method: 开发了AlphaQubit 2神经网络解码器，专门针对表面码和颜色码设计，能够在当前商用加速器上实现大规模实时解码。

Result: 对于颜色码，解码速度比其他高精度解码器快几个数量级；对于表面码，在距离11的情况下实现每周期小于1微秒的实时解码，准确率优于领先的实时解码器。

Conclusion: 该工作支持更多有前景的量子纠错码的实际应用，为容错量子计算所需的大规模高精度实时神经解码建立了可信路径。

Abstract: Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.

</details>
