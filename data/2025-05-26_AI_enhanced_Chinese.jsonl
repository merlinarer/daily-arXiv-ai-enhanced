{"id": "2505.17037", "pdf": "https://arxiv.org/pdf/2505.17037", "abs": "https://arxiv.org/abs/2505.17037", "authors": ["Dimitri Schreiter"], "title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has emerged as a critical component in optimizing large\nlanguage models (LLMs) for domain-specific tasks. However, the role of prompt\nspecificity, especially in domains like STEM (physics, chemistry, biology,\ncomputer science and mathematics), medicine, and law, remains underexplored.\nThis thesis addresses the problem of whether increasing the specificity of\nvocabulary in prompts improves LLM performance in domain-specific\nquestion-answering and reasoning tasks. We developed a synonymization framework\nto systematically substitute nouns, verbs, and adjectives with varying\nspecificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,\nGranite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in\nSTEM, law, and medicine. Our results reveal that while generally increasing the\nspecificity of prompts does not have a significant impact, there appears to be\na specificity range, across all considered models, where the LLM performs the\nbest. Identifying this optimal specificity range offers a key insight for\nprompt design, suggesting that manipulating prompts within this range could\nmaximize LLM performance and lead to more efficient applications in specialized\ndomains.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u63d0\u793a\u8bcd\u7279\u5f02\u6027\u5bf9LLM\u5728STEM\u3001\u6cd5\u5f8b\u548c\u533b\u5b66\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u6700\u4f18\u7279\u5f02\u6027\u8303\u56f4\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u63d0\u793a\u8bcd\u7279\u5f02\u6027\u5bf9LLM\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u540c\u4e49\u8bcd\u66ff\u6362\u6846\u67b6\uff0c\u6d4b\u8bd5\u56db\u79cdLLM\u5728\u4e0d\u540c\u7279\u5f02\u6027\u63d0\u793a\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u589e\u52a0\u63d0\u793a\u8bcd\u7279\u5f02\u6027\u6574\u4f53\u5f71\u54cd\u4e0d\u663e\u8457\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6700\u4f18\u7279\u5f02\u6027\u8303\u56f4\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u63d0\u793a\u8bcd\u7279\u5f02\u6027\u8303\u56f4\u53ef\u63d0\u5347LLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.17038", "pdf": "https://arxiv.org/pdf/2505.17038", "abs": "https://arxiv.org/abs/2505.17038", "authors": ["Xian Gong", "Paul X. McCarthy", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Massive and diverse web data are increasingly vital for government disaster\nresponse, as demonstrated by the 2022 floods in New South Wales (NSW),\nAustralia. This study examines how X (formerly Twitter) and public inquiry\nsubmissions provide insights into public behaviour during crises. We analyse\nmore than 55,000 flood-related tweets and 1,450 submissions to identify\nbehavioural patterns during extreme weather events. While social media posts\nare short and fragmented, inquiry submissions are detailed, multi-page\ndocuments offering structured insights. Our methodology integrates Latent\nDirichlet Allocation (LDA) for topic modelling with Large Language Models\n(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and\ngeographic patterns, while LLMs improve filtering by identifying flood-relevant\ntweets using public submissions as a reference. This Relevance Index method\nreduces noise and prioritizes actionable content, improving situational\nawareness for emergency responders. By combining these complementary data\nstreams, our approach introduces a novel AI-driven method to refine\ncrisis-related social media content, improve real-time disaster response, and\ninform long-term resilience planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LDA\u548cLLM\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u548c\u516c\u5171\u8c03\u67e5\u6570\u636e\u63d0\u5347\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u60c5\u5883\u611f\u77e5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u5a92\u4f53\uff08\u5982X\uff09\u548c\u516c\u5171\u8c03\u67e5\u6570\u636e\uff0c\u63ed\u793a\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u4e2d\u7684\u516c\u4f17\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u63d0\u5347\u707e\u5bb3\u54cd\u5e94\u6548\u7387\u3002", "method": "\u91c7\u7528LDA\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u7ed3\u5408LLM\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u6307\u6570\u65b9\u6cd5\u8fc7\u6ee4\u566a\u58f0\u5e76\u4f18\u5148\u5904\u7406\u53ef\u64cd\u4f5c\u5185\u5bb9\u3002", "result": "LDA\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u610f\u89c1\u548c\u5730\u7406\u6a21\u5f0f\uff0cLLM\u63d0\u9ad8\u4e86\u5bf9\u6d2a\u6c34\u76f8\u5173\u63a8\u6587\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u5e94\u6025\u54cd\u5e94\u7684\u60c5\u5883\u611f\u77e5\u3002", "conclusion": "\u7ed3\u5408\u4e92\u8865\u6570\u636e\u6d41\u7684\u65b9\u6cd5\u4e3a\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u5b9e\u65f6\u54cd\u5e94\u548c\u957f\u671f\u97e7\u6027\u89c4\u5212\u3002", "relevance": 40.0}}
{"id": "2505.17039", "pdf": "https://arxiv.org/pdf/2505.17039", "abs": "https://arxiv.org/abs/2505.17039", "authors": ["Diego Bonatto"], "title": "A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes", "categories": ["cs.CL"], "comment": "46 pages, 8 figures, 1 table", "summary": "A data-driven quantitative approach was used to develop a novel\nclassification system for beer categories and styles. Sixty-two thousand one\nhundred twenty-one beer recipes were mined and analyzed, considering ingredient\nprofiles, fermentation parameters, and recipe vital statistics. Statistical\nanalyses combined with self-organizing maps (SOMs) identified four major\nsuperclusters that showed distinctive malt and hop usage patterns, style\ncharacteristics, and historical brewing traditions. Cold fermented styles\nshowed a conservative grain and hop composition, whereas hot fermented beers\nexhibited high heterogeneity, reflecting regional preferences and innovation.\nThis new taxonomy offers a reproducible and objective framework beyond\ntraditional sensory-based classifications, providing brewers, researchers, and\neducators with a scalable tool for recipe analysis and beer development. The\nfindings in this work provide an understanding of beer diversity and open\navenues for linking ingredient usage with fermentation profiles and flavor\noutcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u679062,121\u4e2a\u5564\u9152\u914d\u65b9\uff0c\u8bc6\u522b\u51fa\u56db\u5927\u8d85\u7ea7\u7c07\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u53d1\u9175\u65b9\u5f0f\u7684\u539f\u6599\u4f7f\u7528\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u5564\u9152\u5206\u7c7b\u4f9d\u8d56\u611f\u5b98\u8bc4\u4ef7\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u63d0\u4f9b\u53ef\u91cd\u590d\u3001\u5ba2\u89c2\u7684\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u5206\u6790\u548c\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOMs\uff09\u5bf9\u5564\u9152\u914d\u65b9\u7684\u539f\u6599\u3001\u53d1\u9175\u53c2\u6570\u7b49\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u5927\u8d85\u7ea7\u7c07\uff0c\u51b7\u53d1\u9175\u5564\u9152\u539f\u6599\u4fdd\u5b88\uff0c\u70ed\u53d1\u9175\u5564\u9152\u539f\u6599\u591a\u6837\uff0c\u53cd\u6620\u533a\u57df\u504f\u597d\u548c\u521b\u65b0\u3002", "conclusion": "\u65b0\u5206\u7c7b\u6cd5\u4e3a\u5564\u9152\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u539f\u6599\u4e0e\u98ce\u5473\u7684\u5173\u7cfb\u3002", "relevance": 10.0}}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042", "abs": "https://arxiv.org/abs/2505.17042", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u9762\u4e34\u4e13\u4e1a\u8bed\u8a00\u548c\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u6587\u672c\u4e14\u96be\u4ee5\u5904\u7406\u957f\u6570\u636e\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001VLM\u6846\u67b6\uff0c\u7ed3\u5408\u653e\u5c04\u5b66\u62a5\u544a\u548c\u5f71\u50cf\u6570\u636e\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u591a\u6a21\u6001\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u3002", "conclusion": "\u591a\u6a21\u6001VLM\u6846\u67b6\u5728\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d\u3002", "relevance": 40.0}}
{"id": "2505.17064", "pdf": "https://arxiv.org/pdf/2505.17064", "abs": "https://arxiv.org/abs/2505.17064", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. In this work, we present a\nsystematic and reproducible methodology for evaluating how TTI systems depict\ndifferent historical periods. For this purpose, we introduce the HistVis\ndataset, a curated collection of 30,000 synthetic images generated by three\nstate-of-the-art diffusion models using carefully designed prompts depicting\nuniversal human activities across different historical periods. We evaluate\ngenerated imagery across three key aspects: (1) Implicit Stylistic\nAssociations: examining default visual styles associated with specific eras;\n(2) Historical Consistency: identifying anachronisms such as modern artifacts\nin pre-modern contexts; and (3) Demographic Representation: comparing generated\nracial and gender distributions against historically plausible baselines. Our\nfindings reveal systematic inaccuracies in historically themed generated\nimagery, as TTI models frequently stereotype past eras by incorporating\nunstated stylistic cues, introduce anachronisms, and fail to reflect plausible\ndemographic patterns. By offering a scalable methodology and benchmark for\nassessing historical representation in generated imagery, this work provides an\ninitial step toward building more historically accurate and culturally aligned\nTTI models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08TTI\uff09\u6269\u6563\u6a21\u578b\u5728\u5386\u53f2\u80cc\u666f\u8868\u73b0\u4e0a\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5386\u53f2\u51c6\u786e\u6027\u4e0a\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865TTI\u6a21\u578b\u5728\u5386\u53f2\u80cc\u666f\u8868\u73b0\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\uff0c\u5173\u6ce8\u5176\u793e\u4f1a\u548c\u6587\u5316\u5f71\u54cd\u3002", "method": "\u5f15\u5165HistVis\u6570\u636e\u96c6\uff0c\u5305\u542b30,000\u5f20\u7531\u4e09\u79cd\u5148\u8fdb\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\uff0c\u8bc4\u4f30\u5386\u53f2\u4e00\u81f4\u6027\u3001\u98ce\u683c\u5173\u8054\u548c\u4eba\u53e3\u7edf\u8ba1\u8868\u793a\u3002", "result": "\u53d1\u73b0TTI\u6a21\u578b\u5728\u5386\u53f2\u8868\u73b0\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5982\u65f6\u4ee3\u523b\u677f\u5370\u8c61\u3001\u65f6\u4ee3\u9519\u8bef\u548c\u4eba\u53e3\u7edf\u8ba1\u4e0d\u51c6\u786e\u3002", "conclusion": "\u4e3a\u6784\u5efa\u66f4\u51c6\u786e\u548c\u6587\u5316\u5bf9\u9f50\u7684TTI\u6a21\u578b\u63d0\u4f9b\u4e86\u521d\u6b65\u65b9\u6cd5\u548c\u57fa\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.17024", "pdf": "https://arxiv.org/pdf/2505.17024", "abs": "https://arxiv.org/abs/2505.17024", "authors": ["Eli Sennesh", "Maxwell Ramstead"], "title": "An Affective-Taxis Hypothesis for Alignment and Interpretability", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "AI alignment is a field of research that aims to develop methods to ensure\nthat agents always behave in a manner aligned with (i.e. consistently with) the\ngoals and values of their human operators, no matter their level of capability.\nThis paper proposes an affectivist approach to the alignment problem,\nre-framing the concepts of goals and values in terms of affective taxis, and\nexplaining the emergence of affective valence by appealing to recent work in\nevolutionary-developmental and computational neuroscience. We review the state\nof the art and, building on this work, we propose a computational model of\naffect based on taxis navigation. We discuss evidence in a tractable model\norganism that our model reflects aspects of biological taxis navigation. We\nconclude with a discussion of the role of affective taxis in AI alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u8d8b\u8fd1\uff08affective taxis\uff09\u7684AI\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u6a21\u578b\u6a21\u62df\u751f\u7269\u8d8b\u8fd1\u884c\u4e3a\uff0c\u63a2\u8ba8\u5176\u5728AI\u5bf9\u9f50\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "AI\u5bf9\u9f50\u65e8\u5728\u786e\u4fddAI\u884c\u4e3a\u4e0e\u4eba\u7c7b\u76ee\u6807\u548c\u4ef7\u503c\u89c2\u4e00\u81f4\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u60c5\u611f\u8d8b\u8fd1\u7684\u7406\u8bba\u6846\u67b6\u91cd\u65b0\u5b9a\u4e49\u76ee\u6807\u548c\u4ef7\u503c\u89c2\uff0c\u4ee5\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u60c5\u611f\u8d8b\u8fd1\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u501f\u9274\u8fdb\u5316\u53d1\u80b2\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5e76\u5728\u6a21\u578b\u751f\u7269\u4e2d\u9a8c\u8bc1\u5176\u4e0e\u751f\u7269\u8d8b\u8fd1\u884c\u4e3a\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u6a21\u578b\u5728\u6a21\u578b\u751f\u7269\u4e2d\u8868\u73b0\u51fa\u4e0e\u751f\u7269\u8d8b\u8fd1\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u60c5\u611f\u8d8b\u8fd1\u7406\u8bba\u5728AI\u5bf9\u9f50\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u60c5\u611f\u8d8b\u8fd1\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u6a21\u578b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u5927\u89c4\u6a21AI\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "relevance": 65.0}}
{"id": "2505.17040", "pdf": "https://arxiv.org/pdf/2505.17040", "abs": "https://arxiv.org/abs/2505.17040", "authors": ["Yun-Da Tsai"], "title": "Generalizing Large Language Model Usability Across Resource-Constrained", "categories": ["cs.LG", "cs.CL"], "comment": "Doctoral disstertation", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, and recent efforts have sought to extend their\ncapabilities to multimodal domains and resource-constrained environments.\nHowever, existing approaches often rely on costly supervised fine-tuning or\nassume fixed training conditions, limiting their generalization when facing\nunseen modalities, limited data, or restricted compute resources. This\ndissertation presents a systematic study toward generalizing LLM usability\nunder real-world constraints. First, it introduces a robust text-centric\nalignment framework that enables LLMs to seamlessly integrate diverse\nmodalities-including text, images, tables, and any modalities - via natural\nlanguage interfaces. This approach supports in-context adaptation to unseen or\ndynamically changing modalities without requiring retraining. To enhance\nrobustness against noisy and missing modalities, an adversarial prompting\ntechnique is proposed, generating semantically challenging perturbations at the\nprompt level to stress-test model reliability. Beyond multimodal setting, the\ndissertation investigates inference-time optimization strategies for LLMs,\nleveraging prompt search and uncertainty quantification to improve performance\nwithout additional model training. This perspective offers an efficient\nalternative to scaling model parameters or retraining from scratch.\nAdditionally, the work addresses low-resource domains such as Verilog code\ngeneration by designing correct-by-construction synthetic data pipelines and\nlogic-enhanced reasoning models, achieving state-of-the-art performance with\nminimal data. Together, these contributions form a unified effort to enhance\nthe adaptability, scalability, and efficiency of large language models under\npractical constraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u901a\u7528\u6027\uff0c\u5305\u62ec\u591a\u6a21\u6001\u96c6\u6210\u3001\u5bf9\u6297\u6027\u63d0\u793a\u3001\u63a8\u7406\u4f18\u5316\u548c\u4f4e\u8d44\u6e90\u9886\u57df\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\u6216\u56fa\u5b9a\u8bad\u7ec3\u6761\u4ef6\uff0c\u9650\u5236\u4e86LLMs\u5728\u65b0\u6a21\u6001\u3001\u6709\u9650\u6570\u636e\u6216\u8ba1\u7b97\u8d44\u6e90\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u6587\u672c\u4e2d\u5fc3\u5bf9\u9f50\u6846\u67b6\uff0c\u652f\u6301\u591a\u6a21\u6001\u96c6\u6210\uff1b2. \u8bbe\u8ba1\u5bf9\u6297\u6027\u63d0\u793a\u6280\u672f\u589e\u5f3a\u9c81\u68d2\u6027\uff1b3. \u7814\u7a76\u63a8\u7406\u65f6\u4f18\u5316\u7b56\u7565\uff1b4. \u5f00\u53d1\u4f4e\u8d44\u6e90\u9886\u57df\u7684\u6570\u636e\u751f\u6210\u548c\u903b\u8f91\u589e\u5f3a\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u9002\u5e94\u3001\u5bf9\u6297\u6027\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u63d0\u5347\u3001\u63a8\u7406\u6027\u80fd\u4f18\u5316\uff0c\u4ee5\u53ca\u5728\u4f4e\u8d44\u6e90\u9886\u57df\u7684SOTA\u8868\u73b0\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u63d0\u5347\u4e86LLMs\u7684\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17043", "pdf": "https://arxiv.org/pdf/2505.17043", "abs": "https://arxiv.org/abs/2505.17043", "authors": ["Anya Belz"], "title": "QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reproduction studies reported in NLP provide individual data points which in\ncombination indicate worryingly low levels of reproducibility in the field.\nBecause each reproduction study reports quantitative conclusions based on its\nown, often not explicitly stated, criteria for reproduction success/failure,\nthe conclusions drawn are hard to interpret, compare, and learn from. In this\npaper, we present QRA++, a quantitative approach to reproducibility assessment\nthat (i) produces continuous-valued degree of reproducibility assessments at\nthree levels of granularity; (ii) utilises reproducibility measures that are\ndirectly comparable across different studies; and (iii) grounds expectations\nabout degree of reproducibility in degree of similarity between experiments.\nQRA++ enables more informative reproducibility assessments to be conducted, and\nconclusions to be drawn about what causes reproducibility to be better/poorer.\nWe illustrate this by applying QRA++ to three example sets of comparable\nexperiments, revealing clear evidence that degree of reproducibility depends on\nsimilarity of experiment properties, but also system type and evaluation\nmethod.", "AI": {"tldr": "QRA++\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u8bc4\u4f30\u53ef\u91cd\u590d\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u8fde\u7eed\u503c\u8bc4\u4f30\u3001\u8de8\u7814\u7a76\u53ef\u6bd4\u6027\u6307\u6807\u548c\u5b9e\u9a8c\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u63ed\u793a\u4e86\u53ef\u91cd\u590d\u6027\u4e0e\u5b9e\u9a8c\u5c5e\u6027\u3001\u7cfb\u7edf\u7c7b\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u5173\u7cfb\u3002", "motivation": "NLP\u9886\u57df\u7684\u53ef\u91cd\u590d\u6027\u7814\u7a76\u7ed3\u8bba\u96be\u4ee5\u6bd4\u8f83\u548c\u89e3\u91ca\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u3002QRA++\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u79d1\u5b66\u7684\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "QRA++\u901a\u8fc7\u4e09\u4e2a\u7c92\u5ea6\u7ea7\u522b\u7684\u8fde\u7eed\u503c\u8bc4\u4f30\u3001\u8de8\u7814\u7a76\u53ef\u6bd4\u6027\u6307\u6807\u548c\u5b9e\u9a8c\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u5b9a\u91cf\u8bc4\u4f30\u53ef\u91cd\u590d\u6027\u3002", "result": "\u5e94\u7528QRA++\u5206\u6790\u4e09\u7ec4\u5b9e\u9a8c\uff0c\u53d1\u73b0\u53ef\u91cd\u590d\u6027\u53d7\u5b9e\u9a8c\u5c5e\u6027\u3001\u7cfb\u7edf\u7c7b\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u5f71\u54cd\u3002", "conclusion": "QRA++\u4e3a\u53ef\u91cd\u590d\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u79d1\u5b66\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u53ef\u91cd\u590d\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002", "relevance": 40.0}}
{"id": "2505.17090", "pdf": "https://arxiv.org/pdf/2505.17090", "abs": "https://arxiv.org/abs/2505.17090", "authors": ["Phoebe Chua", "Cathy Mengying Fang", "Takehiko Ohkawa", "Raja Kushalnagar", "Suranga Nanayakkara", "Pattie Maes"], "title": "EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language", "categories": ["cs.CV"], "comment": null, "summary": "Unlike spoken languages where the use of prosodic features to convey emotion\nis well studied, indicators of emotion in sign language remain poorly\nunderstood, creating communication barriers in critical settings. Sign\nlanguages present unique challenges as facial expressions and hand movements\nsimultaneously serve both grammatical and emotional functions. To address this\ngap, we introduce EmoSign, the first sign video dataset containing sentiment\nand emotion labels for 200 American Sign Language (ASL) videos. We also collect\nopen-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL\nsigners with professional interpretation experience. Alongside the annotations,\nwe include baseline models for sentiment and emotion classification. This\ndataset not only addresses a critical gap in existing sign language research\nbut also establishes a new benchmark for understanding model capabilities in\nmultimodal emotion recognition for sign languages. The dataset is made\navailable at https://huggingface.co/datasets/catfang/emosign.", "AI": {"tldr": "EmoSign\u662f\u9996\u4e2a\u5305\u542b200\u4e2a\u7f8e\u56fd\u624b\u8bed\u89c6\u9891\u60c5\u611f\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u624b\u8bed\u60c5\u611f\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u7814\u7a76\u624b\u8bed\u4e2d\u60c5\u611f\u8868\u8fbe\u7684\u7f3a\u5931\uff0c\u89e3\u51b3\u5173\u952e\u573a\u666f\u4e2d\u7684\u6c9f\u901a\u969c\u788d\u3002", "method": "\u6536\u96c6200\u4e2aASL\u89c6\u9891\uff0c\u75313\u540d\u804b\u4eba\u624b\u8bed\u8005\u6807\u6ce8\u60c5\u611f\u548c\u60c5\u7eea\u6807\u7b7e\uff0c\u5e76\u63d0\u4f9b\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u624b\u8bed\u60c5\u611f\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "EmoSign\u586b\u8865\u4e86\u624b\u8bed\u60c5\u611f\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u6807\u51c6\u3002", "relevance": 30.0}}
{"id": "2505.17214", "pdf": "https://arxiv.org/pdf/2505.17214", "abs": "https://arxiv.org/abs/2505.17214", "authors": ["Xiaochen Wang", "Yuan Zhong", "Lingwei Zhang", "Lisong Dai", "Ting Wang", "Fenglong Ma"], "title": "MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph", "categories": ["cs.AI"], "comment": "Submitted to Neurips 2025", "summary": "Medical deep learning models depend heavily on domain-specific knowledge to\nperform well on knowledge-intensive clinical tasks. Prior work has primarily\nleveraged unimodal knowledge graphs, such as the Unified Medical Language\nSystem (UMLS), to enhance model performance. However, integrating multimodal\nmedical knowledge graphs remains largely underexplored, mainly due to the lack\nof resources linking imaging data with clinical concepts. To address this gap,\nwe propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and\ntextual medical information through a multi-stage construction pipeline. MEDMKG\nfuses the rich multimodal data from MIMIC-CXR with the structured clinical\nknowledge from UMLS, utilizing both rule-based tools and large language models\nfor accurate concept extraction and relationship modeling. To ensure graph\nquality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel\nfiltering algorithm tailored for multimodal knowledge graphs. We evaluate\nMEDMKG across three tasks under two experimental settings, benchmarking\ntwenty-four baseline methods and four state-of-the-art vision-language\nbackbones on six datasets. Results show that MEDMKG not only improves\nperformance in downstream medical tasks but also offers a strong foundation for\ndeveloping adaptive and robust strategies for multimodal knowledge integration\nin medical artificial intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMEDMKG\uff0c\u4e00\u79cd\u533b\u5b66\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u533b\u5b66\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6784\u5efa\u6d41\u7a0b\u63d0\u5347\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u4f9d\u8d56\u9886\u57df\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u591a\u6a21\u6001\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMEDMKG\uff0c\u7ed3\u5408MIMIC-CXR\u548cUMLS\u6570\u636e\uff0c\u5229\u7528\u89c4\u5219\u5de5\u5177\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6982\u5ff5\u63d0\u53d6\u548c\u5173\u7cfb\u5efa\u6a21\uff0c\u5e76\u5f15\u5165Neighbor-aware Filtering\u7b97\u6cd5\u4f18\u5316\u56fe\u8c31\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMEDMKG\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "MEDMKG\u4e3a\u533b\u5b66AI\u4e2d\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17138", "pdf": "https://arxiv.org/pdf/2505.17138", "abs": "https://arxiv.org/abs/2505.17138", "authors": ["Huanrong Liu", "Chunlin Tian", "Xuyang Wei", "Jiaheng Dai", "Qin Liu", "Tianqi Wei", "Qingbiao Li", "Li Li"], "title": "RAP: Runtime-Adaptive Pruning for LLM Inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.", "AI": {"tldr": "RAP\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u526a\u679d\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574\u538b\u7f29\u7b56\u7565\u4ee5\u9002\u5e94\u8fd0\u884c\u65f6\u5185\u5b58\u53d8\u5316\u548c\u5f02\u6784KV\u7f13\u5b58\u9700\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5176\u90e8\u7f72\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u8fd0\u884c\u65f6\u5185\u5b58\u53d8\u5316\u548c\u5f02\u6784KV\u7f13\u5b58\u9700\u6c42\u3002", "method": "\u63d0\u51faRAP\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8ddf\u8e2a\u6a21\u578b\u53c2\u6570\u4e0eKV\u7f13\u5b58\u7684\u6bd4\u4f8b\uff0c\u5e76\u6839\u636e\u5f53\u524d\u5185\u5b58\u9884\u7b97\u548c\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u6700\u4f18\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAP\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u6a21\u578b\u6743\u91cd\u548cKV\u7f13\u5b58\u7684\u8054\u5408\u52a8\u6001\u4f18\u5316\u3002", "conclusion": "RAP\u901a\u8fc7\u52a8\u6001\u526a\u679d\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u6548\u7387\u3002", "relevance": 90.0}}
{"id": "2505.17045", "pdf": "https://arxiv.org/pdf/2505.17045", "abs": "https://arxiv.org/abs/2505.17045", "authors": ["Afifah Kashif", "Heer Patel"], "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have separately highlighted significant biases within\nfoundational large language models (LLMs) against certain nationalities and\nstigmatized social groups. This research investigates the ethical implications\nof these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.\nThrough structured prompt series, we evaluate model responses to several\nscenarios involving American and North Korean nationalities with various mental\ndisabilities. Findings reveal significant discrepancies in empathy levels with\nNorth Koreans facing greater negative bias, particularly when mental disability\nis also a factor. This underscores the need for improvements in LLMs designed\nwith a nuanced understanding of intersectional identity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-3.5/4/4o\u5728\u6d89\u53ca\u7f8e\u56fd\u4e0e\u671d\u9c9c\u56fd\u7c4d\u53ca\u7cbe\u795e\u969c\u788d\u7684\u4ea4\u53c9\u8eab\u4efd\u65f6\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u671d\u9c9c\u4eba\u5c24\u5176\u53d7\u5230\u8d1f\u9762\u504f\u89c1\u5f71\u54cd\uff0c\u51f8\u663e\u4e86LLM\u9700\u6539\u8fdb\u5bf9\u4ea4\u53c9\u8eab\u4efd\u7684\u7406\u89e3\u3002", "motivation": "\u63a2\u8ba8\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5bf9\u7279\u5b9a\u56fd\u7c4d\u548c\u8fb9\u7f18\u5316\u793e\u4f1a\u7fa4\u4f53\u7684\u504f\u89c1\u53ca\u5176\u4f26\u7406\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7cfb\u5217\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u7f8e\u56fd\u4e0e\u671d\u9c9c\u56fd\u7c4d\u53ca\u7cbe\u795e\u969c\u788d\u4ea4\u53c9\u573a\u666f\u7684\u54cd\u5e94\u3002", "result": "\u53d1\u73b0\u671d\u9c9c\u4eba\u9762\u4e34\u66f4\u5927\u7684\u8d1f\u9762\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u7cbe\u795e\u969c\u788d\u65f6\uff0c\u5171\u60c5\u6c34\u5e73\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "LLM\u9700\u6539\u8fdb\u5bf9\u4ea4\u53c9\u8eab\u4efd\u7684\u7ec6\u81f4\u7406\u89e3\u4ee5\u51cf\u5c11\u504f\u89c1\u3002", "relevance": 85.0}}
{"id": "2505.17097", "pdf": "https://arxiv.org/pdf/2505.17097", "abs": "https://arxiv.org/abs/2505.17097", "authors": ["Yanshu Li", "JianJiang Yang", "Bozheng Li", "Ruixiang Tang"], "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 6 tables", "summary": "Multimodal in-context learning (ICL) enables large vision-language models\n(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of\nreal-world applications. However, multimodal ICL remains unstable, and current\nresearch largely focuses on optimizing sequence configuration while overlooking\nthe internal mechanisms of LVLMs. In this work, we first provide a theoretical\nanalysis of attentional dynamics in multimodal ICL and identify three core\nlimitations of standard attention that ICL impair performance. To address these\nchallenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet\neffective plug-and-play method for directly calibrating LVLM attention logits.\nCAMA is training-free and can be seamlessly applied to various open-source\nLVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its\neffectiveness and generality. CAMA opens new opportunities for deeper\nexploration and targeted utilization of LVLM attention dynamics to advance\nmultimodal reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAMA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6LVLM\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347\u591a\u6a21\u6001ICL\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u591a\u6a21\u6001ICL\u5728LVLMs\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5e8f\u5217\u914d\u7f6e\u4f18\u5316\uff0c\u800c\u5ffd\u7565\u4e86LVLM\u5185\u90e8\u673a\u5236\u3002", "method": "\u63d0\u51faContext-Aware Modulated Attention (CAMA)\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u76f4\u63a5\u6821\u51c6LVLM\u7684\u6ce8\u610f\u529blogits\u3002", "result": "\u5728\u56db\u79cdLVLMs\u548c\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86CAMA\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "CAMA\u4e3a\u6df1\u5165\u63a2\u7d22\u548c\u5229\u7528LVLM\u6ce8\u610f\u529b\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u53d1\u5c55\u3002", "relevance": 70.0}}
{"id": "2505.17218", "pdf": "https://arxiv.org/pdf/2505.17218", "abs": "https://arxiv.org/abs/2505.17218", "authors": ["Lianghuan Huang", "Shuo Li", "Sagnik Anupam", "Insup Lee", "Osbert Bastani"], "title": "Effective Reinforcement Learning for Reasoning in Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a promising strategy for improving\nthe reasoning capabilities of language models (LMs) in domains such as\nmathematics and coding. However, most modern RL algorithms were designed to\ntarget robotics applications, which differ significantly from LM reasoning. We\nanalyze RL algorithm design decisions for LM reasoning, for both accuracy and\ncomputational efficiency, focusing on relatively small models due to\ncomputational constraints. Our findings are: (i) on-policy RL significantly\noutperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates\nincrease accuracy instead of reduce variance, and (iii) removing KL divergence\ncan lead to more concise generations and higher accuracy. Furthermore, we find\nthat a key bottleneck to computational efficiency is that the optimal batch\nsizes for inference and backpropagation are different. We propose a novel\nalgorithm, DASH, that performs preemptive sampling (i.e., sample a large batch\nand accumulate gradient updates in small increments), and gradient filtering\n(i.e., drop samples with small advantage estimates). We show that DASH reduces\ntraining time by 83% compared to a standard implementation of GRPO without\nsacrificing accuracy. Our findings provide valuable insights on designing\neffective RL algorithms for LM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u5347\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDASH\u7684\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3RL\u7b97\u6cd5\u591a\u9488\u5bf9\u673a\u5668\u4eba\u5e94\u7528\uff0c\u4e0eLM\u63a8\u7406\u9700\u6c42\u4e0d\u540c\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9LM\u63a8\u7406\u4f18\u5316RL\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u5206\u6790\u4e86RL\u7b97\u6cd5\u8bbe\u8ba1\u5bf9LM\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86DASH\u7b97\u6cd5\uff0c\u7ed3\u5408\u9884\u91c7\u6837\u548c\u68af\u5ea6\u8fc7\u6ee4\u6280\u672f\u3002", "result": "DASH\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1183%\uff0c\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\uff1b\u53d1\u73b0\u79fb\u9664KL\u6563\u5ea6\u53ef\u63d0\u9ad8\u751f\u6210\u7b80\u6d01\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aLM\u63a8\u7406\u4e2d\u7684RL\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0cDASH\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.17142", "pdf": "https://arxiv.org/pdf/2505.17142", "abs": "https://arxiv.org/abs/2505.17142", "authors": ["Jingyu Li", "Tiehua Zhang", "Jinze Wang", "Yi Zhang", "Yuhuan Li", "Yifan Zhao", "Zhishu Shen", "Jiannan Liu"], "title": "MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate classification of sleep stages based on bio-signals is fundamental\nfor automatic sleep stage annotation. Traditionally, this task relies on\nexperienced clinicians to manually annotate data, a process that is both\ntime-consuming and labor-intensive. In recent years, deep learning methods have\nshown promise in automating this task. However, three major challenges remain:\n(1) deep learning models typically require large-scale labeled datasets, making\nthem less effective in real-world settings where annotated data is limited; (2)\nsignificant inter-individual variability in bio-signals often results in\ninconsistent model performance when applied to new subjects, limiting\ngeneralization; and (3) existing approaches often overlook the high-order\nrelationships among bio-signals, failing to simultaneously capture signal\nheterogeneity and spatial-temporal dependencies. To address these issues, we\npropose MetaSTH-Sleep, a few-shot sleep stage classification framework based on\nspatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid\nadaptation to new subjects using only a few labeled samples, while the\nhypergraph structure effectively models complex spatial interconnections and\ntemporal dynamics simultaneously in EEG signals. Experimental results\ndemonstrate that MetaSTH-Sleep achieves substantial performance improvements\nacross diverse subjects, offering valuable insights to support clinicians in\nsleep stage annotation.", "AI": {"tldr": "\u63d0\u51faMetaSTH-Sleep\u6846\u67b6\uff0c\u57fa\u4e8e\u7a7a\u95f4-\u65f6\u95f4\u8d85\u56fe\u589e\u5f3a\u5143\u5b66\u4e60\uff0c\u89e3\u51b3\u7761\u7720\u9636\u6bb5\u5206\u7c7b\u4e2d\u7684\u5c0f\u6837\u672c\u548c\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u4e0d\u8db3\u3001\u4e2a\u4f53\u5dee\u5f02\u548c\u4fe1\u53f7\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u8d85\u56fe\u7ed3\u6784\uff0c\u5feb\u901f\u9002\u5e94\u65b0\u4e2a\u4f53\u5e76\u5efa\u6a21EEG\u4fe1\u53f7\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u5f02\u8d28\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMetaSTH-Sleep\u5728\u591a\u4e2a\u4f53\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MetaSTH-Sleep\u4e3a\u4e34\u5e8a\u7761\u7720\u9636\u6bb5\u6807\u6ce8\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.17047", "pdf": "https://arxiv.org/pdf/2505.17047", "abs": "https://arxiv.org/abs/2505.17047", "authors": ["Erin Palm", "Astrit Manikantan", "Mark E. Pepin", "Herprit Mahal", "Srikanth Subramanya Belwadi"], "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025", "summary": "In medical practices across the United States, physicians have begun\nimplementing generative artificial intelligence (AI) tools to perform the\nfunction of scribes in order to reduce the burden of documenting clinical\nencounters. Despite their widespread use, no established methods exist to gauge\nthe quality of AI scribes. To address this gap, we developed a blinded study\ncomparing the relative performance of large language model (LLM) generated\nclinical notes with those from field experts based on audio-recorded clinical\nencounters. Quantitative metrics from the Physician Documentation Quality\nInstrument (PDQI9) provided a framework to measure note quality, which we\nadapted to assess relative performance of AI generated notes. Clinical experts\nspanning 5 medical specialties used the PDQI9 tool to evaluate\nspecialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators\nfrom each specialty scored notes drafted from a total of 97 patient visits. We\nfound uniformly high inter rater agreement (RWG greater than 0.7) between\nevaluators in general medicine, orthopedics, and obstetrics and gynecology, and\nmoderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and\ncardiology. We found a modest yet significant difference in the overall note\nquality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes\nscored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9\ninstrument as a practical method to gauge the quality of LLM authored notes, as\ncompared to human-authored notes.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u751f\u6210\u7684\u4e34\u5e8a\u7b14\u8bb0\u4e0e\u4e13\u5bb6\u7b14\u8bb0\u7684\u8d28\u91cf\u5bf9\u6bd4\uff0c\u4f7f\u7528PDQI9\u5de5\u5177\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e24\u8005\u8d28\u91cf\u63a5\u8fd1\uff0c\u652f\u6301PDQI9\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u7f3a\u4e4f\u8bc4\u4f30AI\u751f\u6210\u4e34\u5e8a\u7b14\u8bb0\u8d28\u91cf\u7684\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u76f2\u6cd5\u7814\u7a76\uff0c\u6bd4\u8f83LLM\u751f\u6210\u7684\u7b14\u8bb0\u4e0e\u4e13\u5bb6\u7b14\u8bb0\uff0c\u4f7f\u7528PDQI9\u5de5\u5177\u91cf\u5316\u8bc4\u4f30\u3002", "result": "LLM\u7b14\u8bb0\u8d28\u91cf\u7565\u4f4e\u4e8e\u4e13\u5bb6\u7b14\u8bb0\uff084.20 vs 4.25\uff09\uff0c\u4f46\u5dee\u5f02\u5fae\u5c0f\u4e14\u663e\u8457\u3002", "conclusion": "PDQI9\u5de5\u5177\u9002\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684\u4e34\u5e8a\u7b14\u8bb0\u8d28\u91cf\u3002", "relevance": 60.0}}
{"id": "2505.17127", "pdf": "https://arxiv.org/pdf/2505.17127", "abs": "https://arxiv.org/abs/2505.17127", "authors": ["Michal Golovanevsky", "William Rudman", "Michael Lepori", "Amir Bar", "Ritambhara Singh", "Carsten Eickhoff"], "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u8f93\u5165\u4e0e\u5148\u9a8c\u77e5\u8bc6\u4e4b\u95f4\u7684\u63a8\u7406\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86Visual CounterFact\u6570\u636e\u96c6\u548cPvP\u5e72\u9884\u673a\u5236\u3002", "motivation": "\u63a2\u8ba8MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u662f\u5426\u66f4\u4f9d\u8d56\u8bb0\u5fc6\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u8fd8\u662f\u89c6\u89c9\u8f93\u5165\uff0c\u4ee5\u63ed\u793a\u5176\u63a8\u7406\u673a\u5236\u3002", "method": "\u5f15\u5165Visual CounterFact\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1PvP\u5e72\u9884\u5411\u91cf\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5c42\u5e72\u9884\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u65e9\u671f\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\uff0c\u540e\u671f\u8f6c\u5411\u89c6\u89c9\u8f93\u5165\uff1bPvP\u6210\u529f\u5c0692.5%\u989c\u8272\u548c74.6%\u5927\u5c0f\u9884\u6d4b\u4ece\u5148\u9a8c\u8f6c\u5411\u53cd\u4e8b\u5b9e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u89e3\u91ca\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.17225", "pdf": "https://arxiv.org/pdf/2505.17225", "abs": "https://arxiv.org/abs/2505.17225", "authors": ["Doohyuk Jang", "Yoonjeon Kim", "Chanjae Park", "Hyun Ryu", "Eunho Yang"], "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models", "categories": ["cs.AI"], "comment": null, "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term \\textit{reasoning\nrigidity}. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, \\dataset{}. Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\"\u63a8\u7406\u521a\u6027\"\u95ee\u9898\uff0c\u5373\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u719f\u6089\u7684\u63a8\u7406\u6a21\u5f0f\u800c\u5ffd\u7565\u7528\u6237\u660e\u786e\u6307\u4ee4\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8bca\u65ad\u6570\u636e\u96c6\uff0c\u5e76\u8bc6\u522b\u4e86\u4e09\u79cd\u5178\u578b\u7684\u6c61\u67d3\u6a21\u5f0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u4f9d\u8d56\u719f\u6089\u63a8\u7406\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u6570\u5b66\u548c\u903b\u8f91\u7b49\u9886\u57df\u4ea7\u751f\u9519\u8bef\u7ed3\u8bba\u3002\u8fd9\u4e00\u73b0\u8c61\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8bca\u65ad\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4fee\u6539\u540e\u7684\u6570\u5b66\u57fa\u51c6\u548c\u91cd\u65b0\u8bbe\u8ba1\u7684\u8c1c\u9898\uff0c\u7528\u4e8e\u8bc6\u522b\u6a21\u578b\u63a8\u7406\u521a\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u5b58\u5728\u4e09\u79cd\u6c61\u67d3\u6a21\u5f0f\uff1a\u89e3\u91ca\u8fc7\u8f7d\u3001\u8f93\u5165\u4e0d\u4fe1\u4efb\u548c\u90e8\u5206\u6307\u4ee4\u5173\u6ce8\uff0c\u5bfc\u81f4\u5176\u5ffd\u7565\u6216\u626d\u66f2\u6307\u4ee4\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u521a\u6027\u7684\u95ee\u9898\uff0c\u5e76\u516c\u5f00\u4e86\u8bca\u65ad\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "relevance": 85.0}}
{"id": "2505.17150", "pdf": "https://arxiv.org/pdf/2505.17150", "abs": "https://arxiv.org/abs/2505.17150", "authors": ["Rembert Daems", "Manfred Opper", "Guillaume Crevecoeur", "Tolga Birdal"], "title": "Efficient Training of Neural SDEs Using Stochastic Optimal Control", "categories": ["cs.LG", "cs.AI", "math.PR"], "comment": "Published in the ESANN 2025 proceedings, European Symposium on\n  Artificial Neural Networks, Computational Intelligence and Machine Learning.\n  Bruges (Belgium) and online event, 23-25 April 2025", "summary": "We present a hierarchical, control theory inspired method for variational\ninference (VI) for neural stochastic differential equations (SDEs). While VI\nfor neural SDEs is a promising avenue for uncertainty-aware reasoning in\ntime-series, it is computationally challenging due to the iterative nature of\nmaximizing the ELBO. In this work, we propose to decompose the control term\ninto linear and residual non-linear components and derive an optimal control\nterm for linear SDEs, using stochastic optimal control. Modeling the non-linear\ncomponent by a neural network, we show how to efficiently train neural SDEs\nwithout sacrificing their expressive power. Since the linear part of the\ncontrol term is optimal and does not need to be learned, the training is\ninitialized at a lower cost and we observe faster convergence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684\u5206\u5c42\u53d8\u5206\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u795e\u7ecf\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDEs\uff09\uff0c\u901a\u8fc7\u5206\u89e3\u63a7\u5236\u9879\u4e3a\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u90e8\u5206\uff0c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u795e\u7ecfSDEs\u7684\u53d8\u5206\u63a8\u7406\u5728\u65f6\u95f4\u5e8f\u5217\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5c06\u63a7\u5236\u9879\u5206\u89e3\u4e3a\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u90e8\u5206\uff0c\u5229\u7528\u968f\u673a\u6700\u4f18\u63a7\u5236\u7406\u8bba\u4f18\u5316\u7ebf\u6027\u90e8\u5206\uff0c\u7528\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u975e\u7ebf\u6027\u90e8\u5206\u3002", "result": "\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecfSDEs\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17048", "pdf": "https://arxiv.org/pdf/2505.17048", "abs": "https://arxiv.org/abs/2505.17048", "authors": ["Agam Shah", "Siddhant Sukhani", "Huzaifa Pardawala", "Saketh Budideti", "Riya Bhadani", "Rudra Gopal", "Siddhartha Somani", "Michael Galarnyk", "Soungmin Lee", "Arnav Hiray", "Akshar Ravichandran", "Eric Kim", "Pranav Aluru", "Joshua Zhang", "Sebastian Jaskowski", "Veer Guda", "Meghaj Tarte", "Liqin Ye", "Spencer Gosden", "Rutwik Routu", "Rachel Yuh", "Sloka Chava", "Sahasra Chava", "Dylan Patrick Kelly", "Aiden Chiang", "Harsit Mittal", "Sudheer Chava"], "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "Central banks around the world play a crucial role in maintaining economic\nstability. Deciphering policy implications in their communications is\nessential, especially as misinterpretations can disproportionately impact\nvulnerable populations. To address this, we introduce the World Central Banks\n(WCB) dataset, the most comprehensive monetary policy corpus to date,\ncomprising over 380k sentences from 25 central banks across diverse geographic\nregions, spanning 28 years of historical data. After uniformly sampling 1k\nsentences per bank (25k total) across all available years, we annotate and\nreview each sentence using dual annotators, disagreement resolutions, and\nsecondary expert reviews. We define three tasks: Stance Detection, Temporal\nClassification, and Uncertainty Estimation, with each sentence annotated for\nall three. We benchmark seven Pretrained Language Models (PLMs) and nine Large\nLanguage Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on\nthese tasks, running 15,075 benchmarking experiments. We find that a model\ntrained on aggregated data across banks significantly surpasses a model trained\non an individual bank's data, confirming the principle \"the whole is greater\nthan the sum of its parts.\" Additionally, rigorous human evaluations, error\nanalyses, and predictive tasks validate our framework's economic utility. Our\nartifacts are accessible through the HuggingFace and GitHub under the\nCC-BY-NC-SA 4.0 license.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e16\u754c\u4e2d\u592e\u94f6\u884c\uff08WCB\uff09\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8d27\u5e01\u653f\u7b56\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u4e86PLMs\u548cLLMs\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8de8\u94f6\u884c\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u5355\u4e00\u94f6\u884c\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4e2d\u592e\u94f6\u884c\u653f\u7b56\u89e3\u8bfb\u4e2d\u7684\u8bef\u89e3\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efaWCB\u6570\u636e\u96c6\uff0c\u6807\u6ce825k\u53e5\u5b50\uff0c\u5b9a\u4e49\u4e09\u4e2a\u4efb\u52a1\uff08\u7acb\u573a\u68c0\u6d4b\u3001\u65f6\u95f4\u5206\u7c7b\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff09\uff0c\u5e76\u8bc4\u4f30PLMs\u548cLLMs\u3002", "result": "\u8de8\u94f6\u884c\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86\u6574\u4f53\u4f18\u4e8e\u90e8\u5206\u7684\u539f\u5219\u3002", "conclusion": "WCB\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5177\u6709\u7ecf\u6d4e\u5b9e\u7528\u6027\uff0c\u6a21\u578b\u8868\u73b0\u9a8c\u8bc1\u4e86\u6570\u636e\u805a\u5408\u7684\u91cd\u8981\u6027\u3002", "relevance": 30.0}}
{"id": "2505.17132", "pdf": "https://arxiv.org/pdf/2505.17132", "abs": "https://arxiv.org/abs/2505.17132", "authors": ["Tanqiu Jiang", "Jiacheng Liang", "Rongyi Zhu", "Jiawei Zhou", "Fenglong Ma", "Ting Wang"], "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)", "AI": {"tldr": "DTR\u662f\u4e00\u79cd\u65b0\u578b\u63a8\u7406\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u7684KV\u7f13\u5b58\u6765\u51cf\u8f7b\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\uff0c\u65e0\u9700\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u6216\u56fe\u50cf\u8f6c\u6362\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u6216\u6210\u672c\u9ad8\u6602\u7684\u56fe\u50cf\u8f6c\u6362\uff0cDTR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DTR\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u4ee4\u724c\u6743\u91cd\uff0c\u6700\u5c0f\u5316\u5bf9\u6297\u6027\u89c6\u89c9\u8f93\u5165\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002", "result": "DTR\u5728\u591a\u79cdVLM\u548c\u653b\u51fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u9996\u6b21\u6210\u529f\u5c06KV\u7f13\u5b58\u4f18\u5316\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u6027\u589e\u5f3a\u3002", "conclusion": "DTR\u4e3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17249", "pdf": "https://arxiv.org/pdf/2505.17249", "abs": "https://arxiv.org/abs/2505.17249", "authors": ["Yuran Sun", "Susu Xu", "Chenguang Wang", "Xilei Zhao"], "title": "Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Big trajectory data hold great promise for human mobility analysis, but their\nutility is often constrained by the absence of critical traveler attributes,\nparticularly sociodemographic information. While prior studies have explored\npredicting such attributes from mobility patterns, they often overlooked\nunderlying cognitive mechanisms and exhibited low predictive accuracy. This\nstudy introduces SILIC, short for Sociodemographic Inference with LLM-guided\nInverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a\ntheoretically grounded framework that leverages LLMs to infer sociodemographic\nattributes from observed mobility patterns by capturing latent behavioral\nintentions and reasoning through psychological constructs. Particularly, our\napproach explicitly follows the Theory of Planned Behavior (TPB), a\nfoundational behavioral framework in transportation research, to model\nindividuals' latent cognitive processes underlying travel decision-making. The\nLLMs further provide heuristic guidance to improve IRL reward function\ninitialization and update by addressing its ill-posedness and optimization\nchallenges arising from the vast and unstructured reward space. Evaluated in\nthe 2017 Puget Sound Regional Council Household Travel Survey, our method\nsubstantially outperforms state-of-the-art baselines and shows great promise\nfor enriching big trajectory data to support more behaviorally grounded\napplications in transportation planning and beyond.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSILIC\u6846\u67b6\uff0c\u5229\u7528LLM\u548c\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u7ed3\u5408\u8ba4\u77e5\u94fe\u63a8\u7406\uff08CCR\uff09\u4ece\u79fb\u52a8\u8f68\u8ff9\u6570\u636e\u63a8\u65ad\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u8f68\u8ff9\u6570\u636e\u4e2d\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8ba4\u77e5\u673a\u5236\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408LLM\u3001\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u548c\u8ba4\u77e5\u94fe\u63a8\u7406\uff08CCR\uff09\uff0c\u57fa\u4e8e\u8ba1\u5212\u884c\u4e3a\u7406\u8bba\uff08TPB\uff09\u5efa\u6a21\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u57282017\u5e74Puget Sound\u533a\u57df\u8c03\u67e5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SILIC\u6846\u67b6\u4e3a\u4ea4\u901a\u89c4\u5212\u7b49\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.17155", "pdf": "https://arxiv.org/pdf/2505.17155", "abs": "https://arxiv.org/abs/2505.17155", "authors": ["Weizhe Lin", "Xing Li", "Zhiyuan Yang", "Xiaojin Fu", "Hui-Ling Zhen", "Yaoyuan Wang", "Xianzhi Yu", "Wulong Liu", "Xiaosong Li", "Mingxuan Yuan"], "title": "TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling\ncomplex mathematical, logical, and coding tasks by leveraging extended\nChain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging\nCoT with explicit token-level exploration, can push LRMs' accuracy boundaries,\nbut they incur significant decoding overhead. A key inefficiency source is LRMs\noften generate redundant thinking CoTs, which demonstrate clear structured\noverthinking and underthinking patterns. Inspired by human cognitive reasoning\nprocesses and numerical optimization theories, we propose TrimR, a\nverifier-based, training-free, efficient framework for dynamic CoT compression\nto trim reasoning and enhance test-time scaling, explicitly tailored for\nproduction-level deployment. Our method employs a lightweight, pretrained,\ninstruction-tuned verifier to detect and truncate redundant intermediate\nthoughts of LRMs without any LRM or verifier fine-tuning. We present both the\ncore algorithm and asynchronous online system engineered for high-throughput\nindustrial applications. Empirical evaluations on Ascend NPUs and vLLM show\nthat our framework delivers substantial gains in inference efficiency under\nlarge-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and\nGPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and\nDeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTrimR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u538b\u7f29Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\uff0c\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u7ea7\u90e8\u7f72\u3002", "motivation": "LRMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5197\u4f59\u601d\u8003\uff08overthinking/underthinking\uff09\uff0c\u5bfc\u81f4\u89e3\u7801\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u9884\u8bad\u7ec3\u9a8c\u8bc1\u5668\u52a8\u6001\u68c0\u6d4b\u5e76\u622a\u65ad\u5197\u4f59\u4e2d\u95f4\u601d\u8003\uff0c\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63a8\u7406\u6548\u7387\u63d0\u5347\u9ad8\u8fbe70%\uff0c\u7cbe\u5ea6\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "TrimR\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684CoT\u538b\u7f29\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u7ea7\u5e94\u7528\u3002", "relevance": 85.0}}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049", "abs": "https://arxiv.org/abs/2505.17049", "authors": ["David Rozado"], "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\u00e9sum\u00e9 Evaluations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57fa\u4e8e\u7b80\u5386\u8bc4\u4f30\u804c\u4e1a\u5019\u9009\u4eba\u65f6\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u6027\u522b\u504f\u597d\u548c\u4f4d\u7f6e\u504f\u89c1\u3002", "motivation": "\u4e86\u89e3LLMs\u5728\u804c\u4e1a\u5019\u9009\u4eba\u8bc4\u4f30\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u4ee5\u8b66\u793a\u5176\u5728\u9ad8\u98ce\u9669\u81ea\u4e3b\u51b3\u7b56\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u901a\u8fc722\u4e2a\u9886\u5148\u7684LLMs\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u5176\u5bf9\u6027\u522b\u5316\u59d3\u540d\u3001\u6027\u522b\u5b57\u6bb5\u3001\u6027\u522b\u4e2d\u6027\u6807\u8bc6\u7b26\u7b49\u7684\u53cd\u5e94\u3002", "result": "LLMs\u666e\u904d\u504f\u597d\u5973\u6027\u5019\u9009\u4eba\uff0c\u4e14\u5b58\u5728\u4f4d\u7f6e\u504f\u89c1\uff1b\u6027\u522b\u4e2d\u6027\u6807\u8bc6\u7b26\u4e0b\u504f\u597d\u6d88\u5931\u3002", "conclusion": "LLMs\u5728\u51b3\u7b56\u4e2d\u53ef\u80fd\u5b58\u5728\u504f\u89c1\uff0c\u9700\u8c28\u614e\u5e94\u7528\u4e8e\u9ad8\u98ce\u9669\u573a\u666f\u3002", "relevance": 85.0}}
{"id": "2505.17201", "pdf": "https://arxiv.org/pdf/2505.17201", "abs": "https://arxiv.org/abs/2505.17201", "authors": ["Chaim Chai Elchik", "Fatemeh Karimi Nejadasl", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking (MOT) in computer vision has made significant\nadvancements, yet tracking small fish in underwater environments presents\nunique challenges due to complex 3D motions and data noise. Traditional\nsingle-view MOT models often fall short in these settings. This thesis\naddresses these challenges by adapting state-of-the-art single-view MOT models,\nFairMOT and YOLOv8, for underwater fish detecting and tracking in ecological\nstudies. The core contribution of this research is the development of a\nmulti-view framework that utilizes stereo video inputs to enhance tracking\naccuracy and fish behavior pattern recognition. By integrating and evaluating\nthese models on underwater fish video datasets, the study aims to demonstrate\nsignificant improvements in precision and reliability compared to single-view\napproaches. The proposed framework detects fish entities with a relative\naccuracy of 47% and employs stereo-matching techniques to produce a novel 3D\noutput, providing a more comprehensive understanding of fish movements and\ninteractions", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u4e0e\u8ffd\u8e2a\uff0c\u901a\u8fc7\u7ed3\u5408FairMOT\u548cYOLOv8\u6a21\u578b\uff0c\u5229\u7528\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u63d0\u5347\u8ffd\u8e2a\u7cbe\u5ea6\u548c\u9c7c\u7c7b\u884c\u4e3a\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u89c6\u89d2\u591a\u76ee\u6807\u8ffd\u8e2a\u6a21\u578b\u5728\u6c34\u4e0b\u590d\u67423D\u8fd0\u52a8\u548c\u566a\u58f0\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u751f\u6001\u7814\u7a76\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u9c7c\u7c7b\u884c\u4e3a\u5206\u6790\u5de5\u5177\u3002", "method": "\u7ed3\u5408FairMOT\u548cYOLOv8\u6a21\u578b\uff0c\u5f00\u53d1\u591a\u89c6\u89d2\u6846\u67b6\uff0c\u5229\u7528\u7acb\u4f53\u89c6\u9891\u8f93\u5165\u8fdb\u884c3D\u8ffd\u8e2a\u548c\u9c7c\u7c7b\u884c\u4e3a\u8bc6\u522b\u3002", "result": "\u6846\u67b6\u5728\u9c7c\u7c7b\u68c0\u6d4b\u4e2d\u8fbe\u523047%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u7acb\u4f53\u5339\u914d\u6280\u672f\u751f\u62103D\u8f93\u51fa\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u89c6\u89d2\u65b9\u6cd5\u3002", "conclusion": "\u591a\u89c6\u89d2\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u9c7c\u7c7b\u8ffd\u8e2a\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u751f\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u9c7c\u7c7b\u884c\u4e3a\u5206\u6790\u5de5\u5177\u3002", "relevance": 20.0}}
{"id": "2505.17312", "pdf": "https://arxiv.org/pdf/2505.17312", "abs": "https://arxiv.org/abs/2505.17312", "authors": ["Xiangqi Wang", "Yue Huang", "Yanbo Wang", "Xiaonan Luo", "Kehan Guo", "Yujun Zhou", "Xiangliang Zhang"], "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "LLMs often need effective configurations, like temperature and reasoning\nsteps, to handle tasks requiring sophisticated reasoning and problem-solving,\nranging from joke generation to mathematical reasoning. Existing prompting\napproaches usually adopt general-purpose, fixed configurations that work 'well\nenough' across tasks but seldom achieve task-specific optimality. To address\nthis gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM\nto automate adaptive reasoning configurations for tasks requiring different\ntypes of thinking. AdaReasoner is trained using a reinforcement learning (RL)\nframework, combining a factorized action space with a targeted exploration\nstrategy, along with a pretrained reward model to optimize the policy model for\nreasoning configurations with only a few-shot guide. AdaReasoner is backed by\ntheoretical guarantees and experiments of fast convergence and a sublinear\npolicy gap. Across six different LLMs and a variety of reasoning tasks, it\nconsistently outperforms standard baselines, preserves out-of-distribution\nrobustness, and yield gains on knowledge-intensive tasks through tailored\nprompts.", "AI": {"tldr": "AdaReasoner\u662f\u4e00\u4e2aLLM\u65e0\u5173\u7684\u63d2\u4ef6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u8c03\u6574\u63a8\u7406\u914d\u7f6e\uff0c\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u901a\u7528\u56fa\u5b9a\u914d\u7f6e\uff0c\u96be\u4ee5\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u56e0\u5b50\u5316\u52a8\u4f5c\u7a7a\u95f4\u548c\u76ee\u6807\u63a2\u7d22\u7b56\u7565\uff0c\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u516d\u79cdLLM\u548c\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4fdd\u6301\u5206\u5e03\u5916\u9c81\u68d2\u6027\u3002", "conclusion": "AdaReasoner\u901a\u8fc7\u81ea\u9002\u5e94\u914d\u7f6e\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002", "relevance": 90.0}}
{"id": "2505.17163", "pdf": "https://arxiv.org/pdf/2505.17163", "abs": "https://arxiv.org/abs/2505.17163", "authors": ["Mingxin Huang", "Yongxin Shi", "Dezhi Peng", "Songxuan Lai", "Zecheng Xie", "Lianwen Jin"], "title": "OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in multimodal slow-thinking systems have demonstrated\nremarkable performance across diverse visual reasoning tasks. However, their\ncapabilities in text-rich image reasoning tasks remain understudied due to the\nlack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,\na comprehensive benchmark designed to systematically assess Multimodal Large\nLanguage Models on text-rich image reasoning tasks. The benchmark comprises\n1,069 human-annotated examples spanning 6 core reasoning abilities and 18\npractical reasoning tasks in text-rich visual scenarios. Furthermore, unlike\nother text-rich image understanding benchmarks that only annotate the final\nanswers, OCR-Reasoning also annotates the reasoning process simultaneously.\nWith the annotated reasoning process and the final answers, OCR-Reasoning\nevaluates not only the final answers generated by models but also their\nreasoning processes, enabling a holistic analysis of their problem-solving\nabilities. Leveraging this benchmark, we conducted a comprehensive evaluation\nof state-of-the-art MLLMs. Our results demonstrate the limitations of existing\nmethodologies. Notably, even state-of-the-art MLLMs exhibit substantial\ndifficulties, with none achieving accuracy surpassing 50\\% across\nOCR-Reasoning, indicating that the challenges of text-rich image reasoning are\nan urgent issue to be addressed. The benchmark and evaluation scripts are\navailable at https://github.com/SCUT-DLVCLab/OCR-Reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86OCR-Reasoning\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u4e30\u5bcc\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u586b\u8865\u6587\u672c\u4e30\u5bcc\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u7f3a\u4e4f\u7cfb\u7edf\u6027\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b1,069\u4e2a\u6807\u6ce8\u6837\u672c\u7684OCR-Reasoning\u57fa\u51c6\uff0c\u6db5\u76d66\u79cd\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u548c18\u79cd\u4efb\u52a1\uff0c\u540c\u65f6\u6807\u6ce8\u63a8\u7406\u8fc7\u7a0b\u548c\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u73b0\u6709MLLMs\u5728OCR-Reasoning\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u5747\u672a\u8d85\u8fc750%\u3002", "conclusion": "\u6587\u672c\u4e30\u5bcc\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "relevance": 60.0}}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050", "abs": "https://arxiv.org/abs/2505.17050", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "AI": {"tldr": "PBLBench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u6311\u6218\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548cLLMs\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u81ea\u7531\u8f93\u51fa\u7ed3\u6784\u548c\u4e25\u683c\u7684\u4eba\u5de5\u9a8c\u8bc1\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6a21\u578b\u5e7b\u89c9\u548c\u4e0d\u7a33\u5b9a\u6027\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u7ba1\u9053\u652f\u6301\u6559\u5e08\u4f7f\u7528MLLMs\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u5206\u6790\u6cd5\uff08AHP\uff09\u901a\u8fc7\u4e13\u5bb6\u9a71\u52a8\u7684\u6210\u5bf9\u6bd4\u8f83\u5efa\u7acb\u7ed3\u6784\u5316\u52a0\u6743\u8bc4\u4f30\u6807\u51c6\uff0c\u8bc4\u4f3015\u79cd\u9886\u5148MLLMs/LLMs\u7684\u6027\u80fd\u3002", "result": "\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6392\u540d\u51c6\u786e\u7387\u4ec5\u4e3a59%\uff0c\u8868\u660e\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u663e\u8457\u6311\u6218\u6027\u3002", "conclusion": "PBLBench\u6709\u671b\u63a8\u52a8\u66f4\u5f3a\u5927AI\u4ee3\u7406\u7684\u5f00\u53d1\uff0c\u51cf\u8f7b\u6559\u5e08\u8d1f\u62c5\u5e76\u63d0\u5347\u6559\u80b2\u751f\u4ea7\u529b\u3002", "relevance": 70.0}}
{"id": "2505.17223", "pdf": "https://arxiv.org/pdf/2505.17223", "abs": "https://arxiv.org/abs/2505.17223", "authors": ["Siyang Song", "Micol Spitale", "Xiangyu Kong", "Hengde Zhu", "Cheng Luo", "Cristina Palmero", "German Barquero", "Sergio Escalera", "Michel Valstar", "Mohamed Daoudi", "Tobias Baur", "Fabien Ringeval", "Andrew Howes", "Elisabeth Andre", "Hatice Gunes"], "title": "REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge", "categories": ["cs.CV", "68T40"], "comment": null, "summary": "In dyadic interactions, a broad spectrum of human facial reactions might be\nappropriate for responding to each human speaker behaviour. Following the\nsuccessful organisation of the REACT 2023 and REACT 2024 challenges, we are\nproposing the REACT 2025 challenge encouraging the development and benchmarking\nof Machine Learning (ML) models that can be used to generate multiple\nappropriate, diverse, realistic and synchronised human-style facial reactions\nexpressed by human listeners in response to an input stimulus (i.e.,\naudio-visual behaviours expressed by their corresponding speakers). As a key of\nthe challenge, we provide challenge participants with the first natural and\nlarge-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human\ndyadic interactions containing a total of 2856 interaction sessions covering\nfive different topics. In addition, this paper also presents the challenge\nguidelines and the performance of our baselines on the two proposed\nsub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge\nbaseline code is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2025", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86REACT 2025\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u5f00\u53d1\u548c\u8bc4\u4f30\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u3001\u771f\u5b9e\u4e14\u540c\u6b65\u7684\u4eba\u7c7b\u9762\u90e8\u53cd\u5e94\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86MARS\u6570\u636e\u96c6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u4e8c\u5143\u4e92\u52a8\u4e2d\u751f\u6210\u591a\u6837\u5316\u4e14\u771f\u5b9e\u7684\u4eba\u7c7b\u9762\u90e8\u53cd\u5e94\u7684\u6311\u6218\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u4f9bMARS\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u79bb\u7ebfMAFRG\u548c\u5728\u7ebfMAFRG\u4e24\u4e2a\u5b50\u6311\u6218\uff0c\u63d0\u4f9b\u57fa\u7ebf\u6a21\u578b\u548c\u4ee3\u7801\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u57fa\u7ebf\u6a21\u578b\u5728\u4e24\u4e2a\u5b50\u6311\u6218\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "conclusion": "\u7ed3\u8bba\u662fREACT 2025\u6311\u6218\u8d5b\u4e3a\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u57fa\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.17315", "pdf": "https://arxiv.org/pdf/2505.17315", "abs": "https://arxiv.org/abs/2505.17315", "authors": ["Wang Yang", "Zirui Liu", "Hongye Jin", "Qingyu Yin", "Vipin Chaudhary", "Xiaotian Han"], "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent language models exhibit strong reasoning capabilities, yet the\ninfluence of long-context capacity on reasoning remains underexplored. In this\nwork, we hypothesize that current limitations in reasoning stem, in part, from\ninsufficient long-context capacity, motivated by empirical observations such as\n(1) higher context window length often leads to stronger reasoning performance,\nand (2) failed reasoning cases resemble failed long-context cases. To test this\nhypothesis, we examine whether enhancing a model's long-context ability before\nSupervised Fine-Tuning (SFT) leads to improved reasoning performance.\nSpecifically, we compared models with identical architectures and fine-tuning\ndata but varying levels of long-context capacity. Our results reveal a\nconsistent trend: models with stronger long-context capacity achieve\nsignificantly higher accuracy on reasoning benchmarks after SFT. Notably, these\ngains persist even on tasks with short input lengths, indicating that\nlong-context training offers generalizable benefits for reasoning performance.\nThese findings suggest that long-context modeling is not just essential for\nprocessing lengthy inputs, but also serves as a critical foundation for\nreasoning. We advocate for treating long-context capacity as a first-class\nobjective in the design of future language models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u53d7\u9650\u53ef\u80fd\u90e8\u5206\u6e90\u4e8e\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u4e0d\u8db3\uff0c\u5b9e\u9a8c\u89c2\u5bdf\u5230\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0e\u63a8\u7406\u6027\u80fd\u6b63\u76f8\u5173\u3002", "method": "\u6bd4\u8f83\u5177\u6709\u76f8\u540c\u67b6\u6784\u548c\u5fae\u8c03\u6570\u636e\u4f46\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u5728\u76d1\u7763\u5fae\u8c03\u540e\u7684\u63a8\u7406\u8868\u73b0\u3002", "result": "\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u8fd9\u79cd\u4f18\u52bf\u5728\u77ed\u8f93\u5165\u4efb\u52a1\u4e2d\u4f9d\u7136\u5b58\u5728\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e0d\u4ec5\u662f\u5904\u7406\u957f\u8f93\u5165\u7684\u5173\u952e\uff0c\u4e5f\u662f\u63a8\u7406\u80fd\u529b\u7684\u57fa\u7840\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u672a\u6765\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u6838\u5fc3\u76ee\u6807\u3002", "relevance": 85.0}}
{"id": "2505.17190", "pdf": "https://arxiv.org/pdf/2505.17190", "abs": "https://arxiv.org/abs/2505.17190", "authors": ["Baran Hashemi", "Kurt Pasque", "Chris Teska", "Ruriko Yoshida"], "title": "Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms", "categories": ["cs.LG", "math.AG", "math.CO"], "comment": "Under Review", "summary": "Dynamic programming (DP) algorithms for combinatorial optimization problems\nwork with taking maximization, minimization, and classical addition in their\nrecursion algorithms. The associated value functions correspond to convex\npolyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning\nmodels, however, rely on softmax-normalized dot-product attention where the\nsmooth exponential weighting blurs these sharp polyhedral structures and\ncollapses when evaluated on out-of-distribution (OOD) settings. We introduce\nTropical attention, a novel attention function that operates natively in the\nmax-plus semiring of tropical geometry. We prove that Tropical attention can\napproximate tropical circuits of DP-type combinatorial algorithms. We then\npropose that using Tropical transformers enhances empirical OOD performance in\nboth length generalization and value generalization, on algorithmic reasoning\ntasks, surpassing softmax baselines while remaining stable under adversarial\nattacks. We also present adversarial-attack generalization as a third axis for\nNeural Algorithmic Reasoning benchmarking. Our results demonstrate that\nTropical attention restores the sharp, scale-invariant reasoning absent from\nsoftmax.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTropical attention\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u57fa\u4e8esoftmax\u7684\u6ce8\u610f\u529b\u5728\u7ec4\u5408\u4f18\u5316\u4efb\u52a1\u4e2d\u7684OOD\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u5728\u7b97\u6cd5\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8esoftmax\u7684\u6ce8\u610f\u529b\u673a\u5236\u5728\u7ec4\u5408\u4f18\u5316\u4efb\u52a1\u4e2d\u6a21\u7cca\u4e86\u591a\u9762\u4f53\u7ed3\u6784\uff0c\u5bfc\u81f4OOD\u6027\u80fd\u4e0b\u964d\u3002Tropical attention\u65e8\u5728\u6062\u590d\u8fd9\u79cd\u5c16\u9510\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165Tropical attention\uff0c\u4e00\u79cd\u5728\u70ed\u5e26\u51e0\u4f55\u7684max-plus\u534a\u73af\u4e2d\u8fd0\u884c\u7684\u6ce8\u610f\u529b\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u4ee5\u8fd1\u4f3c\u70ed\u5e26\u7535\u8def\u3002\u63d0\u51faTropical transformers\u4ee5\u63d0\u5347OOD\u6027\u80fd\u3002", "result": "Tropical attention\u5728\u957f\u5ea6\u6cdb\u5316\u548c\u503c\u6cdb\u5316\u4efb\u52a1\u4e2d\u8d85\u8d8asoftmax\u57fa\u7ebf\uff0c\u4e14\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "Tropical attention\u6062\u590d\u4e86softmax\u7f3a\u5931\u7684\u5c16\u9510\u3001\u5c3a\u5ea6\u4e0d\u53d8\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u7b97\u6cd5\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17051", "pdf": "https://arxiv.org/pdf/2505.17051", "abs": "https://arxiv.org/abs/2505.17051", "authors": ["Bernd Huber", "Ghazal Fazelnia", "Andreas Damianou", "Sebastian Peleato", "Max Lefarov", "Praveen Ravichandran", "Marco De Nadai", "Mounia Lalmas-Roellke", "Paul N. Bennett"], "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating contextually relevant\ncontent. However, tailoring these outputs to individual users for effective\npersonalization is a significant challenge. While rich user-specific\ninformation often exists as pre-existing user representations, such as\nembeddings learned from preferences or behaviors, current methods to leverage\nthese for LLM personalization typically require costly fine-tuning or\ntoken-heavy prompting. We propose Embedding-to-Prefix (E2P), a\nparameter-efficient method that injects pre-computed context embeddings into an\nLLM's hidden representation space through a learned projection to a single soft\ntoken prefix. This enables effective personalization while keeping the backbone\nmodel frozen and avoiding expensive adaptation techniques. We evaluate E2P\nacross two public datasets and in a production setting: dialogue\npersonalization on Persona-Chat, contextual headline generation on PENS, and\nlarge-scale personalization for music and podcast consumption. Results show\nthat E2P preserves contextual signals and achieves strong performance with\nminimal computational overhead, offering a scalable, efficient solution for\ncontextualizing generative AI systems.", "AI": {"tldr": "E2P\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9884\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6295\u5f71\u5230LLM\u7684\u9690\u85cf\u8868\u793a\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u751f\u6210\uff0c\u907f\u514d\u6602\u8d35\u7684\u5fae\u8c03\u6216\u63d0\u793a\u3002", "motivation": "\u5f53\u524dLLM\u4e2a\u6027\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u6216\u5927\u91cf\u63d0\u793a\uff0cE2P\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "E2P\u901a\u8fc7\u5b66\u4e60\u7684\u6295\u5f71\u5c06\u9884\u8ba1\u7b97\u7684\u7528\u6237\u5d4c\u5165\u8f6c\u6362\u4e3a\u5355\u4e2a\u8f6f\u4ee4\u724c\u524d\u7f00\uff0c\u6ce8\u5165\u5230LLM\u7684\u9690\u85cf\u8868\u793a\u7a7a\u95f4\uff0c\u4fdd\u6301\u4e3b\u5e72\u6a21\u578b\u51bb\u7ed3\u3002", "result": "\u5728\u5bf9\u8bdd\u4e2a\u6027\u5316\u3001\u6807\u9898\u751f\u6210\u548c\u5927\u89c4\u6a21\u97f3\u4e50/\u64ad\u5ba2\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\uff0cE2P\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "E2P\u4e3a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17235", "pdf": "https://arxiv.org/pdf/2505.17235", "abs": "https://arxiv.org/abs/2505.17235", "authors": ["Omar Moured", "Yufan Chen", "Ruiping Liu", "Simon Rei\u00df", "Philip Torr", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "CHAOS: Chart Analysis with Outlier Samples", "categories": ["cs.CV", "cs.CL"], "comment": "Data and code are publicly available at:\n  http://huggingface.co/datasets/omoured/CHAOS", "summary": "Charts play a critical role in data analysis and visualization, yet\nreal-world applications often present charts with challenging or noisy\nfeatures. However, \"outlier charts\" pose a substantial challenge even for\nMultimodal Large Language Models (MLLMs), which can struggle to interpret\nperturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier\nSamples), a robustness benchmark to systematically evaluate MLLMs against chart\nperturbations. CHAOS encompasses five types of textual and ten types of visual\nperturbations, each presented at three levels of severity (easy, mid, hard)\ninspired by the study result of human evaluation. The benchmark includes 13\nstate-of-the-art MLLMs divided into three groups (i.e., general-, document-,\nand chart-specific models) according to the training scope and data.\nComprehensive analysis involves two downstream tasks (ChartQA and\nChart-to-Text). Extensive experiments and case studies highlight critical\ninsights into robustness of models across chart perturbations, aiming to guide\nfuture research in chart understanding domain. Data and code are publicly\navailable at: http://huggingface.co/datasets/omoured/CHAOS.", "AI": {"tldr": "CHAOS\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u56fe\u8868\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u56fe\u8868\u5e38\u542b\u6709\u566a\u58f0\u6216\u5f02\u5e38\u7279\u5f81\uff0c\u800c\u73b0\u6709MLLMs\u5728\u89e3\u91ca\u8fd9\u4e9b\u6270\u52a8\u56fe\u8868\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "CHAOS\u5305\u542b\u4e94\u7c7b\u6587\u672c\u548c\u5341\u7c7b\u89c6\u89c9\u6270\u52a8\uff0c\u6bcf\u79cd\u6270\u52a8\u5206\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u6d4b\u8bd5\u4e8613\u79cdMLLMs\u5728ChartQA\u548cChart-to-Text\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u5dee\u5f02\uff0c\u4e3a\u672a\u6765\u56fe\u8868\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "CHAOS\u4e3aMLLMs\u5728\u56fe\u8868\u9886\u57df\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.17323", "pdf": "https://arxiv.org/pdf/2505.17323", "abs": "https://arxiv.org/abs/2505.17323", "authors": ["Ruaridh Mon-Williams", "Max Taylor-Davies", "Elizabeth Mieczkowski", "Natalia Velez", "Neil R. Bramley", "Yanwei Wang", "Thomas L. Griffiths", "Christopher G. Lucas"], "title": "Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Humans are remarkably adept at collaboration, able to infer the strengths and\nweaknesses of new partners in order to work successfully towards shared goals.\nTo build AI systems with this capability, we must first understand its building\nblocks: does such flexibility require explicit, dedicated mechanisms for\nmodelling others -- or can it emerge spontaneously from the pressures of\nopen-ended cooperative interaction? To investigate this question, we train\nsimple model-free RNN agents to collaborate with a population of diverse\npartners. Using the `Overcooked-AI' environment, we collect data from thousands\nof collaborative teams, and analyse agents' internal hidden states. Despite a\nlack of additional architectural features, inductive biases, or auxiliary\nobjectives, the agents nevertheless develop structured internal representations\nof their partners' task abilities, enabling rapid adaptation and generalisation\nto novel collaborators. We investigated these internal models through probing\ntechniques, and large-scale behavioural analysis. Notably, we find that\nstructured partner modelling emerges when agents can influence partner\nbehaviour by controlling task allocation. Our results show that partner\nmodelling can arise spontaneously in model-free agents -- but only under\nenvironmental conditions that impose the right kind of social pressure.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u7cfb\u7edf\u662f\u5426\u80fd\u5728\u65e0\u989d\u5916\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5f00\u653e\u5408\u4f5c\u4e92\u52a8\u81ea\u53d1\u5f62\u6210\u5bf9\u5408\u4f5c\u4f19\u4f34\u7684\u5efa\u6a21\u80fd\u529b\u3002\u4f7f\u7528RNN\u4ee3\u7406\u5728Overcooked-AI\u73af\u5883\u4e2d\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4ee3\u7406\u80fd\u81ea\u53d1\u5f62\u6210\u5bf9\u5408\u4f5c\u4f19\u4f34\u4efb\u52a1\u80fd\u529b\u7684\u7ed3\u6784\u5316\u5185\u90e8\u8868\u793a\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u662f\u5426\u80fd\u5728\u65e0\u4e13\u7528\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5408\u4f5c\u4e92\u52a8\u81ea\u53d1\u5f62\u6210\u5bf9\u5408\u4f5c\u4f19\u4f34\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u8bad\u7ec3\u7b80\u5355\u7684\u65e0\u6a21\u578bRNN\u4ee3\u7406\u4e0e\u591a\u6837\u5316\u5408\u4f5c\u4f19\u4f34\u5728Overcooked-AI\u73af\u5883\u4e2d\u534f\u4f5c\uff0c\u5206\u6790\u5176\u5185\u90e8\u9690\u85cf\u72b6\u6001\u3002", "result": "\u4ee3\u7406\u81ea\u53d1\u5f62\u6210\u5bf9\u5408\u4f5c\u4f19\u4f34\u4efb\u52a1\u80fd\u529b\u7684\u7ed3\u6784\u5316\u5185\u90e8\u8868\u793a\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u5408\u4f5c\u4f19\u4f34\u3002", "conclusion": "\u5408\u4f5c\u4f19\u4f34\u5efa\u6a21\u53ef\u5728\u65e0\u6a21\u578b\u4ee3\u7406\u4e2d\u81ea\u53d1\u5f62\u6210\uff0c\u4f46\u9700\u7279\u5b9a\u73af\u5883\u6761\u4ef6\u65bd\u52a0\u793e\u4f1a\u538b\u529b\u3002", "relevance": 60.0}}
{"id": "2505.17196", "pdf": "https://arxiv.org/pdf/2505.17196", "abs": "https://arxiv.org/abs/2505.17196", "authors": ["ShengYun Peng", "Pin-Yu Chen", "Jianfeng Chi", "Seongmin Lee", "Duen Horng Chau"], "title": "Shape it Up! Restoring LLM Safety during Finetuning", "categories": ["cs.LG"], "comment": null, "summary": "Finetuning large language models (LLMs) enables user-specific customization\nbut introduces critical safety risks: even a few harmful examples can\ncompromise safety alignment. A common mitigation strategy is to update the\nmodel more strongly on examples deemed safe, while downweighting or excluding\nthose flagged as unsafe. However, because safety context can shift within a\nsingle example, updating the model equally on both harmful and harmless parts\nof a response is suboptimal-a coarse treatment we term static safety shaping.\nIn contrast, we propose dynamic safety shaping (DSS), a framework that uses\nfine-grained safety signals to reinforce learning from safe segments of a\nresponse while suppressing unsafe content. To enable such fine-grained control\nduring finetuning, we introduce a key insight: guardrail models, traditionally\nused for filtering, can be repurposed to evaluate partial responses, tracking\nhow safety risk evolves throughout the response, segment by segment. This leads\nto the Safety Trajectory Assessment of Response (STAR), a token-level signal\nthat enables shaping to operate dynamically over the training sequence.\nBuilding on this, we present STAR-DSS, guided by STAR scores, that robustly\nmitigates finetuning risks and delivers substantial safety improvements across\ndiverse threats, datasets, and model families-all without compromising\ncapability on intended tasks. We encourage future safety research to build on\ndynamic shaping principles for stronger mitigation against evolving finetuning\nrisks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u52a8\u6001\u5b89\u5168\u5851\u5f62\uff08DSS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b89\u5168\u4fe1\u53f7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5f3a\u5316\u5b89\u5168\u5185\u5bb9\u5e76\u6291\u5236\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u3002", "motivation": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u80fd\u5f15\u5165\u5b89\u5168\u98ce\u9669\uff0c\u4f20\u7edf\u9759\u6001\u5b89\u5168\u5851\u5f62\u65b9\u6cd5\u5728\u5904\u7406\u6df7\u5408\u5b89\u5168\u4e0a\u4e0b\u6587\u7684\u54cd\u5e94\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5b89\u5168\u5851\u5f62\uff08DSS\uff09\u6846\u67b6\uff0c\u5229\u7528\u5b89\u5168\u8f68\u8ff9\u8bc4\u4f30\uff08STAR\uff09\u4fe1\u53f7\u5728\u4ee4\u724c\u7ea7\u522b\u52a8\u6001\u8c03\u6574\u5fae\u8c03\u6743\u91cd\u3002", "result": "STAR-DSS\u5728\u591a\u79cd\u5a01\u80c1\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5bb6\u65cf\u4e2d\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u52a8\u6001\u5851\u5f62\u539f\u5219\u4e3a\u672a\u6765\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u5fae\u8c03\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u3002", "relevance": 90.0}}
{"id": "2505.17052", "pdf": "https://arxiv.org/pdf/2505.17052", "abs": "https://arxiv.org/abs/2505.17052", "authors": ["Jinwoo Park", "Seunggeun Cho", "Dongsu Han"], "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) power many modern applications, but serving them\nat scale remains costly and resource-intensive. Current server-centric systems\noverlook consumer-grade GPUs at the edge. We introduce SpecEdge, an\nedge-assisted inference framework that splits LLM workloads between edge and\nserver GPUs using a speculative decoding scheme, exchanging only token outputs\nover the network. SpecEdge employs proactive edge drafting to overlap edge\ntoken creation with server verification and pipeline-aware scheduling that\ninterleaves multiple user requests to increase server-side throughput.\nExperiments show SpecEdge enhances overall cost efficiency by 1.91x through\nachieving 2.22x server throughput, and reduces inter token latency by 11.24%\ncompared to a server-only baseline, introducing a scalable, cost-effective\nparadigm for LLM serving.", "AI": {"tldr": "SpecEdge\u662f\u4e00\u4e2a\u8fb9\u7f18\u8f85\u52a9\u7684LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u5230\u8fb9\u7f18\u548c\u670d\u52a1\u5668GPU\u4e0a\uff0c\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u4ee5\u670d\u52a1\u5668\u4e3a\u4e2d\u5fc3\u7684\u7cfb\u7edf\u5ffd\u89c6\u4e86\u8fb9\u7f18\u7684\u6d88\u8d39\u7ea7GPU\u8d44\u6e90\uff0c\u5bfc\u81f4LLM\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u91c7\u7528\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\uff0c\u5728\u8fb9\u7f18\u548c\u670d\u52a1\u5668GPU\u4e4b\u95f4\u5206\u914d\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u8fb9\u7f18\u8349\u62df\u548c\u7ba1\u9053\u611f\u77e5\u8c03\u5ea6\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSpecEdge\u5c06\u6210\u672c\u6548\u7387\u63d0\u9ad8\u4e861.91\u500d\uff0c\u670d\u52a1\u5668\u541e\u5410\u91cf\u63d0\u9ad8\u4e862.22\u500d\uff0c\u5e76\u51cf\u5c11\u4e8611.24%\u7684\u4ee4\u724c\u95f4\u5ef6\u8fdf\u3002", "conclusion": "SpecEdge\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u8303\u5f0f\u3002", "relevance": 85.0}}
{"id": "2505.17245", "pdf": "https://arxiv.org/pdf/2505.17245", "abs": "https://arxiv.org/abs/2505.17245", "authors": ["Ryota Yagi"], "title": "Extending Dataset Pruning to Object Detection: A Variance-based Approach", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset pruning -- selecting a small yet informative subset of training data\n-- has emerged as a promising strategy for efficient machine learning, offering\nsignificant reductions in computational cost and storage compared to\nalternatives like dataset distillation. While pruning methods have shown strong\nperformance in image classification, their extension to more complex computer\nvision tasks, particularly object detection, remains relatively underexplored.\nIn this paper, we present the first principled extension of classification\npruning techniques to the object detection domain, to the best of our\nknowledge. We identify and address three key challenges that hinder this\ntransition: the Object-Level Attribution Problem, the Scoring Strategy Problem,\nand the Image-Level Aggregation Problem. To overcome these, we propose tailored\nsolutions, including a novel scoring method called Variance-based Prediction\nScore (VPS). VPS leverages both Intersection over Union (IoU) and confidence\nscores to effectively identify informative training samples specific to\ndetection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate\nthat our approach consistently outperforms prior dataset pruning methods in\nterms of mean Average Precision (mAP). We also show that annotation count and\nclass distribution shift can influence detection performance, but selecting\ninformative examples is a more critical factor than dataset size or balance.\nOur work bridges dataset pruning and object detection, paving the way for\ndataset pruning in complex vision tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u5206\u7c7b\u6570\u636e\u526a\u679d\u6280\u672f\u6269\u5c55\u5230\u76ee\u6807\u68c0\u6d4b\u9886\u57df\uff0c\u89e3\u51b3\u4e86\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5VPS\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u96c6\u526a\u679d\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u5982\u76ee\u6807\u68c0\u6d4b\u4e2d\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86Variance-based Prediction Score (VPS)\uff0c\u7ed3\u5408IoU\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u89e3\u51b3\u4e86\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\u3002", "result": "\u5728PASCAL VOC\u548cMS COCO\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVPS\u65b9\u6cd5\u5728mAP\u4e0a\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u6570\u636e\u96c6\u526a\u679d\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u53ef\u884c\uff0c\u4e14\u6837\u672c\u4fe1\u606f\u91cf\u6bd4\u6570\u636e\u96c6\u5927\u5c0f\u6216\u5e73\u8861\u6027\u66f4\u91cd\u8981\u3002", "relevance": 60.0}}
{"id": "2505.17348", "pdf": "https://arxiv.org/pdf/2505.17348", "abs": "https://arxiv.org/abs/2505.17348", "authors": ["Yuheng Wu", "Jianwen Xie", "Denghui Zhang", "Zhaozhuo Xu"], "title": "DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Theory-of-Mind (ToM) tasks pose a unique challenge for small language models\n(SLMs) with limited scale, which often lack the capacity to perform deep social\nreasoning. In this work, we propose DEL-ToM, a framework that improves ToM\nreasoning through inference-time scaling rather than architectural changes. Our\napproach decomposes ToM tasks into a sequence of belief updates grounded in\nDynamic Epistemic Logic (DEL), enabling structured and transparent reasoning.\nWe train a verifier, called the Process Belief Model (PBM), to score each\nbelief update step using labels generated automatically via a DEL simulator.\nDuring inference, candidate belief traces generated by a language model are\nevaluated by the PBM, and the highest-scoring trace is selected. This allows\nSLMs to emulate more deliberate reasoning by allocating additional compute at\ntest time. Experiments across multiple model scales and benchmarks show that\nDEL-ToM consistently improves performance, demonstrating that verifiable belief\nsupervision can significantly enhance ToM abilities of SLMs without retraining.", "AI": {"tldr": "DEL-ToM\u901a\u8fc7\u52a8\u6001\u8ba4\u77e5\u903b\u8f91\uff08DEL\uff09\u5206\u89e3ToM\u4efb\u52a1\uff0c\u5229\u7528\u9a8c\u8bc1\u5668PBM\u8bc4\u5206\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728ToM\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u7f3a\u4e4f\u6df1\u5ea6\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u3002DEL-ToM\u65e8\u5728\u901a\u8fc7\u63a8\u7406\u65f6\u6269\u5c55\u800c\u975e\u67b6\u6784\u6539\u53d8\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "1. \u5c06ToM\u4efb\u52a1\u5206\u89e3\u4e3a\u57fa\u4e8eDEL\u7684\u4fe1\u5ff5\u66f4\u65b0\u5e8f\u5217\u30022. \u8bad\u7ec3PBM\u9a8c\u8bc1\u5668\u81ea\u52a8\u8bc4\u5206\u6bcf\u4e2a\u6b65\u9aa4\u30023. \u63a8\u7406\u65f6\u9009\u62e9\u6700\u9ad8\u5206\u4fe1\u5ff5\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDEL-ToM\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u63d0\u5347SLMs\u7684ToM\u80fd\u529b\u3002", "conclusion": "DEL-ToM\u8bc1\u660e\u4e86\u9a8c\u8bc1\u6027\u4fe1\u5ff5\u76d1\u7763\u53ef\u6709\u6548\u589e\u5f3aSLMs\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "relevance": 70.0}}
{"id": "2505.17198", "pdf": "https://arxiv.org/pdf/2505.17198", "abs": "https://arxiv.org/abs/2505.17198", "authors": ["Shuang Wu", "Meijie Wang", "Lun Yu"], "title": "LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Peptide compounds demonstrate considerable potential as therapeutic agents\ndue to their high target affinity and low toxicity, yet their drug development\nis constrained by their low membrane permeability. Molecular weight and peptide\nlength have significant effects on the logD of peptides, which in turn\ninfluences their ability to cross biological membranes. However, accurate\nprediction of peptide logD remains challenging due to the complex interplay\nbetween sequence, structure, and ionization states. This study introduces\nLengthLogD, a predictive framework that establishes specialized models through\nmolecular length stratification while innovatively integrating multi-scale\nmolecular representations. We constructed feature spaces across three\nhierarchical levels: atomic (10 molecular descriptors), structural (1024-bit\nMorgan fingerprints), and topological (3 graph-based features including Wiener\nindex), optimized through stratified ensemble learning. An adaptive weight\nallocation mechanism specifically developed for long peptides significantly\nenhances model generalizability. Experimental results demonstrate superior\nperformance across all categories: short peptides (R^2=0.855), medium peptides\n(R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in\nprediction error for long peptides compared to conventional single-model\napproaches. Ablation studies confirm: 1) The length-stratified strategy\ncontributes 41.2% to performance improvement; 2) Topological features account\nfor 28.5% of predictive importance. Compared to state-of-the-art models, our\nmethod maintains short peptide prediction accuracy while achieving a 25.7%\nincrease in the coefficient of determination (R^2) for long peptides. This\nresearch provides a precise logD prediction tool for peptide drug development,\nparticularly demonstrating unique value in optimizing long peptide lead\ncompounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLengthLogD\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5b50\u957f\u5ea6\u5206\u5c42\u548c\u591a\u5c3a\u5ea6\u5206\u5b50\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80bdlogD\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u957f\u80bd\u65b9\u9762\u3002", "motivation": "\u80bd\u5316\u5408\u7269\u7684\u836f\u7269\u5f00\u53d1\u53d7\u9650\u4e8e\u5176\u4f4e\u819c\u6e17\u900f\u6027\uff0c\u800clogD\u7684\u51c6\u786e\u9884\u6d4b\u56e0\u5e8f\u5217\u3001\u7ed3\u6784\u548c\u7535\u79bb\u72b6\u6001\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "LengthLogD\u6846\u67b6\u901a\u8fc7\u5206\u5b50\u957f\u5ea6\u5206\u5c42\u5efa\u7acb\u4e13\u95e8\u6a21\u578b\uff0c\u6574\u5408\u539f\u5b50\u3001\u7ed3\u6784\u548c\u62d3\u6251\u4e09\u4e2a\u5c42\u6b21\u7684\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u96c6\u6210\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u77ed\u3001\u4e2d\u3001\u957f\u80bd\u4e0a\u7684R^2\u5206\u522b\u4e3a0.855\u30010.816\u548c0.882\uff0c\u957f\u80bd\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e34.7%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u80bd\u836f\u7269\u5f00\u53d1\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684logD\u9884\u6d4b\u5de5\u5177\uff0c\u5c24\u5176\u5728\u957f\u80bd\u4f18\u5316\u65b9\u9762\u5177\u6709\u72ec\u7279\u4ef7\u503c\u3002", "relevance": 20.0}}
{"id": "2505.17053", "pdf": "https://arxiv.org/pdf/2505.17053", "abs": "https://arxiv.org/abs/2505.17053", "authors": ["Ou Jiamin", "Eikmans Emile", "Buskens Vincent", "Pankowska Paulina", "Shan Yuli"], "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures, 2 tables", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin understanding human languages, this study explores how they translate this\nunderstanding into social exchange contexts that capture certain essences of\nreal world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were\nplaced in economic trust games where players balance self-interest with trust\nand reciprocity, making decisions that reveal their social preferences and\ninteractive reasoning abilities. Our study shows that LLMs deviate from pure\nself-interest and exhibit trust and reciprocity even without being prompted to\nadopt a specific persona. In the simplest one-shot interaction, LLMs emulated\nhow human players place trust at the beginning of such a game. Larger\nhuman-machine divergences emerged in scenarios involving trust repayment or\nmulti-round interactions, where decisions were influenced by both social\npreferences and interactive reasoning. LLMs responses varied significantly when\nprompted to adopt personas like selfish or unselfish players, with the impact\noutweighing differences between models or game types. Response of ChatGPT-4, in\nan unselfish or neutral persona, resembled the highest trust and reciprocity,\nsurpassing humans, Claude, and Bard. Claude and Bard displayed trust and\nreciprocity levels that sometimes exceeded and sometimes fell below human\nchoices. When given selfish personas, all LLMs showed lower trust and\nreciprocity than humans. Interactive reasoning to the actions of counterparts\nor changing game mechanics appeared to be random rather than stable,\nreproducible characteristics in the response of LLMs, though some improvements\nwere observed when ChatGPT-4 responded in selfish or unselfish personas.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5728\u793e\u4ea4\u4ea4\u6362\u60c5\u5883\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5b83\u4eec\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\uff0c\u4f46\u884c\u4e3a\u53d7\u89d2\u8272\u8bbe\u5b9a\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6a21\u62df\u771f\u5b9e\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u793e\u4f1a\u504f\u597d\u548c\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06ChatGPT-4\u3001Claude\u548cBard\u7f6e\u4e8e\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\uff0c\u5206\u6790\u5176\u51b3\u7b56\u884c\u4e3a\u3002", "result": "LLMs\u5728\u65e0\u89d2\u8272\u8bbe\u5b9a\u65f6\u8868\u73b0\u51fa\u4fe1\u4efb\u548c\u4e92\u60e0\uff0c\u4f46\u884c\u4e3a\u53d7\u89d2\u8272\u8bbe\u5b9a\u5f71\u54cd\u663e\u8457\uff1bChatGPT-4\u5728\u65e0\u79c1\u89d2\u8272\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLMs\u7684\u793e\u4ea4\u884c\u4e3a\u53d7\u89d2\u8272\u8bbe\u5b9a\u5f71\u54cd\uff0c\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u5c1a\u4e0d\u7a33\u5b9a\u3002", "relevance": 85.0}}
{"id": "2505.17256", "pdf": "https://arxiv.org/pdf/2505.17256", "abs": "https://arxiv.org/abs/2505.17256", "authors": ["Liang Shi", "Yun Fu"], "title": "ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved text-to-face\ngeneration, but achieving fine-grained control over facial features remains a\nchallenge. Existing methods often require training additional modules to handle\nspecific controls such as identity, attributes, or age, making them inflexible\nand resource-intensive. We propose ExpertGen, a training-free framework that\nleverages pre-trained expert models such as face recognition, facial attribute\nrecognition, and age estimation networks to guide generation with fine control.\nOur approach uses a latent consistency model to ensure realistic and\nin-distribution predictions at each diffusion step, enabling accurate guidance\nsignals to effectively steer the diffusion process. We show qualitatively and\nquantitatively that expert models can guide the generation process with high\nprecision, and multiple experts can collaborate to enable simultaneous control\nover diverse facial aspects. By allowing direct integration of off-the-shelf\nexpert models, our method transforms any such model into a plug-and-play\ncomponent for controllable face generation.", "AI": {"tldr": "ExpertGen\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\uff08\u5982\u4eba\u8138\u8bc6\u522b\u3001\u5c5e\u6027\u8bc6\u522b\u548c\u5e74\u9f84\u4f30\u8ba1\uff09\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u6587\u672c\u5230\u4eba\u8138\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6a21\u5757\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u548c\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u7684\u51c6\u786e\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e13\u5bb6\u6a21\u578b\u80fd\u9ad8\u7cbe\u5ea6\u5f15\u5bfc\u751f\u6210\uff0c\u591a\u4e13\u5bb6\u534f\u4f5c\u53ef\u5b9e\u73b0\u591a\u6837\u5316\u63a7\u5236\u3002", "conclusion": "ExpertGen\u5c06\u73b0\u6709\u4e13\u5bb6\u6a21\u578b\u8f6c\u5316\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u63d0\u5347\u53ef\u63a7\u751f\u6210\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.17406", "pdf": "https://arxiv.org/pdf/2505.17406", "abs": "https://arxiv.org/abs/2505.17406", "authors": ["Enyi Jiang", "Changming Xu", "Nischay Singh", "Gagandeep Singh"], "title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness", "categories": ["cs.AI"], "comment": null, "summary": "LLMs' decision-making process is opaque, prompting the need for explanation\ntechniques like Chain-of-Thought. To investigate the relationship between\nanswer and reasoning, we design a novel evaluation framework, MATCHA. In\ndomains like education and healthcare, reasoning is key for model\ntrustworthiness. MATCHA reveals that LLMs under input perturbations can give\ninconsistent or nonsensical reasoning. Additionally, we use LLM judges to\nassess reasoning robustness across models. Our results show that LLMs exhibit\ngreater vulnerability to input perturbations for multi-step and commonsense\ntasks than compared to logical tasks. Also, we show non-trivial transfer rates\nof our successful examples to black-box models. Our evaluation framework helps\nto better understand LLM reasoning mechanisms and guides future models toward\nmore robust and reasoning-driven architectures, enforcing answer-reasoning\nconsistency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MATCHA\u8bc4\u4f30\u6846\u67b6\uff0c\u7814\u7a76LLMs\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u63a8\u7406\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u591a\u6b65\u548c\u5e38\u8bc6\u4efb\u52a1\u66f4\u6613\u53d7\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u6210\u529f\u6848\u4f8b\u5411\u9ed1\u76d2\u6a21\u578b\u7684\u8fc1\u79fb\u7387\u3002", "motivation": "LLMs\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u9700\u8981\u89e3\u91ca\u6280\u672f\uff08\u5982Chain-of-Thought\uff09\u6765\u589e\u5f3a\u4fe1\u4efb\uff0c\u5c24\u5176\u662f\u5728\u6559\u80b2\u548c\u533b\u7597\u7b49\u9886\u57df\u3002", "method": "\u8bbe\u8ba1MATCHA\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790LLMs\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u63a8\u7406\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528LLM\u6cd5\u5b98\u8bc4\u4f30\u63a8\u7406\u9c81\u68d2\u6027\u3002", "result": "\u53d1\u73b0LLMs\u5728\u591a\u6b65\u548c\u5e38\u8bc6\u4efb\u52a1\u4e2d\u5bf9\u8f93\u5165\u6270\u52a8\u66f4\u654f\u611f\uff0c\u4e14\u6210\u529f\u6848\u4f8b\u53ef\u8fc1\u79fb\u81f3\u9ed1\u76d2\u6a21\u578b\u3002", "conclusion": "MATCHA\u6846\u67b6\u6709\u52a9\u4e8e\u7406\u89e3LLM\u63a8\u7406\u673a\u5236\uff0c\u6307\u5bfc\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u66f4\u9c81\u68d2\u4e14\u63a8\u7406\u9a71\u52a8\u3002", "relevance": 85.0}}
{"id": "2505.17226", "pdf": "https://arxiv.org/pdf/2505.17226", "abs": "https://arxiv.org/abs/2505.17226", "authors": ["Kun Yang", "Neena Imam"], "title": "Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative machine learning across\ndecentralized data sources without sharing raw data. It offers a promising\napproach to privacy-preserving AI. However, FL remains vulnerable to\nadversarial threats from malicious participants, referred to as Byzantine\nclients, who can send misleading updates to corrupt the global model.\nTraditional aggregation methods, such as simple averaging, are not robust to\nsuch attacks. More resilient approaches, like the Krum algorithm, require prior\nknowledge of the number of malicious clients, which is often unavailable in\nreal-world scenarios. To address these limitations, we propose Average-rKrum\n(ArKrum), a novel aggregation strategy designed to enhance both the resilience\nand privacy guarantees of FL systems. Building on our previous work (rKrum),\nArKrum introduces two key innovations. First, it includes a median-based\nfiltering mechanism that removes extreme outliers before estimating the number\nof adversarial clients. Second, it applies a multi-update averaging scheme to\nimprove stability and performance, particularly when client data distributions\nare not identical. We evaluate ArKrum on benchmark image and text datasets\nunder three widely studied Byzantine attack types. Results show that ArKrum\nconsistently achieves high accuracy and stability. It performs as well as or\nbetter than other robust aggregation methods. These findings demonstrate that\nArKrum is an effective and practical solution for secure FL systems in\nadversarial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86ArKrum\uff0c\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u805a\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u4e2d\u503c\u8fc7\u6ee4\u548c\u591a\u66f4\u65b0\u5e73\u5747\u589e\u5f3a\u5bf9\u6297\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u6613\u53d7\u6076\u610f\u5ba2\u6237\u7aef\u653b\u51fb\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982Krum\u9700\u5df2\u77e5\u6076\u610f\u5ba2\u6237\u7aef\u6570\u91cf\uff0c\u800c\u5b9e\u9645\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002", "method": "ArKrum\u7ed3\u5408\u4e2d\u503c\u8fc7\u6ee4\u53bb\u9664\u6781\u7aef\u5f02\u5e38\u503c\uff0c\u5e76\u91c7\u7528\u591a\u66f4\u65b0\u5e73\u5747\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cArKrum\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u9c81\u68d2\u805a\u5408\u65b9\u6cd5\u3002", "conclusion": "ArKrum\u662f\u8054\u90a6\u5b66\u4e60\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17054", "pdf": "https://arxiv.org/pdf/2505.17054", "abs": "https://arxiv.org/abs/2505.17054", "authors": ["Linglong Qian", "Zina Ibrahim"], "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMETHOD\u7684\u65b0\u578bTransformer\u67b6\u6784\uff0c\u4e13\u4e3a\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u4e34\u5e8a\u5e8f\u5217\u5efa\u6a21\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u5728\u533b\u7597\u9886\u57df\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u60a3\u8005\u65f6\u95f4\u7ebf\u5177\u6709\u4e0d\u89c4\u5219\u91c7\u6837\u3001\u591a\u53d8\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u4f20\u7edfTransformer\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u7279\u70b9\u3002", "method": "METHOD\u7ed3\u5408\u4e86\u4e09\u79cd\u521b\u65b0\uff1a\u60a3\u8005\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u3001\u81ea\u9002\u5e94\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u65b9\u6848\u548c\u52a8\u6001\u8df3\u8dc3\u8fde\u63a5\u7684U-Net\u67b6\u6784\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u5e93\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMETHOD\u5728\u9884\u6d4b\u9ad8\u4e25\u91cd\u6027\u75c5\u4f8b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e14\u5728\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "METHOD\u662f\u4e13\u4e3a\u533b\u7597\u5e94\u7528\u4f18\u5316\u7684Transformer\u67b6\u6784\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "relevance": 70.0}}
{"id": "2505.17280", "pdf": "https://arxiv.org/pdf/2505.17280", "abs": "https://arxiv.org/abs/2505.17280", "authors": ["Pushkar Shukla", "Aditya Chinchure", "Emily Diana", "Alexander Tolbert", "Kartik Hosanagar", "Vineeth N Balasubramanian", "Leonid Sigal", "Matthew Turk"], "title": "Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "The biases exhibited by text-to-image (TTI) models are often treated as\nindependent, though in reality, they may be deeply interrelated. Addressing\nbias along one dimension - such as ethnicity or age - can inadvertently affect\nanother, like gender, either mitigating or exacerbating existing disparities.\nUnderstanding these interdependencies is crucial for designing fairer\ngenerative models, yet measuring such effects quantitatively remains a\nchallenge. To address this, we introduce BiasConnect, a novel tool for\nanalyzing and quantifying bias interactions in TTI models. BiasConnect uses\ncounterfactual interventions along different bias axes to reveal the underlying\nstructure of these interactions and estimates the effect of mitigating one bias\naxis on another. These estimates show strong correlation (+0.65) with observed\npost-mitigation outcomes. Building on BiasConnect, we propose InterMit, an\nintersectional bias mitigation algorithm guided by user-defined target\ndistributions and priority weights. InterMit achieves lower bias (0.33 vs.\n0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields\nsuperior image quality compared to traditional techniques. Although our\nimplementation is training-free, InterMit is modular and can be integrated with\nmany existing debiasing approaches for TTI models, making it a flexible and\nextensible solution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBiasConnect\u5de5\u5177\u548cInterMit\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u548c\u91cf\u5316\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u504f\u89c1\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u5e72\u9884\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u504f\u89c1\u901a\u5e38\u88ab\u89c6\u4e3a\u72ec\u7acb\uff0c\u4f46\u5b9e\u9645\u4e0a\u53ef\u80fd\u76f8\u4e92\u5173\u8054\u3002\u7406\u89e3\u8fd9\u4e9b\u5173\u8054\u5bf9\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528BiasConnect\u8fdb\u884c\u53cd\u4e8b\u5b9e\u5e72\u9884\uff0c\u5206\u6790\u504f\u89c1\u4ea4\u4e92\uff1b\u63d0\u51faInterMit\u7b97\u6cd5\uff0c\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u7684\u76ee\u6807\u5206\u5e03\u548c\u4f18\u5148\u7ea7\u6743\u91cd\u8fdb\u884c\u504f\u89c1\u7f13\u89e3\u3002", "result": "BiasConnect\u7684\u4f30\u8ba1\u4e0e\u89c2\u5bdf\u7ed3\u679c\u5f3a\u76f8\u5173\uff08+0.65\uff09\uff1bInterMit\u5728\u51cf\u5c11\u504f\u89c1\uff080.33 vs. 0.52\uff09\u548c\u6b65\u9aa4\u6570\uff082.38 vs. 3.15\uff09\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "InterMit\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.17433", "pdf": "https://arxiv.org/pdf/2505.17433", "abs": "https://arxiv.org/abs/2505.17433", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Yuxi Zhang", "Yanxi Zhao", "Yifan Zhang", "Zezhong Wang", "Huimin Wang", "Yutian Zhao", "Bin Liang", "Yefeng Zheng", "Binyang Li", "Kam-Fai Wong", "Xian Wu"], "title": "MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Memes have emerged as a popular form of multimodal online communication,\nwhere their interpretation heavily depends on the specific context in which\nthey appear. Current approaches predominantly focus on isolated meme analysis,\neither for harmful content detection or standalone interpretation, overlooking\na fundamental challenge: the same meme can express different intents depending\non its conversational context. This oversight creates an evaluation gap:\nalthough humans intuitively recognize how context shapes meme interpretation,\nLarge Vision Language Models (LVLMs) can hardly understand context-dependent\nmeme intent. To address this critical limitation, we introduce MemeReaCon, a\nnovel benchmark specifically designed to evaluate how LVLMs understand memes in\ntheir original context. We collected memes from five different Reddit\ncommunities, keeping each meme's image, the post text, and user comments\ntogether. We carefully labeled how the text and meme work together, what the\nposter intended, how the meme is structured, and how the community responded.\nOur tests with leading LVLMs show a clear weakness: models either fail to\ninterpret critical information in the contexts, or overly focus on visual\ndetails while overlooking communicative purpose. MemeReaCon thus serves both as\na diagnostic tool exposing current limitations and as a challenging benchmark\nto drive development toward more sophisticated LVLMs of the context-aware\nunderstanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MemeReaCon\uff0c\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u539f\u59cb\u4e0a\u4e0b\u6587\u4e2d\u7406\u89e3\u6a21\u56e0\u80fd\u529b\u7684\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6a21\u56e0\u610f\u56fe\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7f3a\u53e3\uff0cMemeReaCon\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7f3a\u53e3\u3002", "method": "\u4ece\u4e94\u4e2aReddit\u793e\u533a\u6536\u96c6\u6a21\u56e0\uff0c\u4fdd\u7559\u56fe\u50cf\u3001\u5e16\u5b50\u6587\u672c\u548c\u7528\u6237\u8bc4\u8bba\uff0c\u5e76\u6807\u6ce8\u6587\u672c\u4e0e\u6a21\u56e0\u7684\u4e92\u52a8\u3001\u610f\u56fe\u3001\u7ed3\u6784\u548c\u793e\u533a\u53cd\u5e94\u3002", "result": "\u6d4b\u8bd5\u663e\u793aLVLMs\u5728\u7406\u89e3\u4e0a\u4e0b\u6587\u5173\u952e\u4fe1\u606f\u6216\u8fc7\u5ea6\u5173\u6ce8\u89c6\u89c9\u7ec6\u8282\u800c\u5ffd\u89c6\u4ea4\u6d41\u76ee\u7684\u65b9\u9762\u5b58\u5728\u660e\u663e\u5f31\u70b9\u3002", "conclusion": "MemeReaCon\u65e2\u662f\u8bca\u65ad\u5de5\u5177\uff0c\u4e5f\u662f\u63a8\u52a8\u5f00\u53d1\u66f4\u590d\u6742\u4e0a\u4e0b\u6587\u611f\u77e5LVLMs\u7684\u6311\u6218\u6027\u57fa\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.17228", "pdf": "https://arxiv.org/pdf/2505.17228", "abs": "https://arxiv.org/abs/2505.17228", "authors": ["Arash Afkanpour", "Omkar Dige", "Fatemeh Tavakoli"], "title": "Automated Capability Evaluation of Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Current evaluation frameworks for foundation models rely heavily on fixed,\nmanually curated benchmarks, limiting their ability to capture the full breadth\nof model capabilities. This paper introduces Active learning for Capability\nEvaluation (ACE), a novel framework for scalable, automated, and fine-grained\nevaluation of foundation models. ACE leverages the knowledge embedded in\npowerful language models to decompose a domain into semantically meaningful\ncapabilities and generate diverse evaluation tasks, significantly reducing\nhuman effort. To maximize coverage and efficiency, ACE models a subject model's\nperformance as a capability function over a latent semantic space and uses\nactive learning to prioritize the evaluation of the most informative\ncapabilities. This adaptive evaluation strategy enables cost-effective\ndiscovery of strengths, weaknesses, and failure modes that static benchmarks\nmay miss. Our results suggest that ACE provides a more complete and informative\npicture of model capabilities, which is essential for safe and well-informed\ndeployment of foundation models.", "AI": {"tldr": "ACE\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u548c\u8bed\u4e49\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u5bf9\u57fa\u7840\u6a21\u578b\u80fd\u529b\u7684\u81ea\u52a8\u5316\u3001\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u7684\u4eba\u5de5\u57fa\u51c6\uff0c\u96be\u4ee5\u5168\u9762\u6355\u6349\u6a21\u578b\u80fd\u529b\u3002ACE\u65e8\u5728\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "ACE\u5229\u7528\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u5206\u89e3\u9886\u57df\u80fd\u529b\uff0c\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u4f18\u5148\u8bc4\u4f30\u4fe1\u606f\u91cf\u6700\u5927\u7684\u80fd\u529b\u3002", "result": "ACE\u80fd\u66f4\u5168\u9762\u5730\u63ed\u793a\u6a21\u578b\u7684\u80fd\u529b\u3001\u5f31\u70b9\u548c\u5931\u8d25\u6a21\u5f0f\uff0c\u4f18\u4e8e\u9759\u6001\u57fa\u51c6\u3002", "conclusion": "ACE\u4e3a\u5b89\u5168\u3001\u660e\u667a\u5730\u90e8\u7f72\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u7684\u8bc4\u4f30\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055", "abs": "https://arxiv.org/abs/2505.17055", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03Vision Transformer\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u7cbe\u5ea6\u7684\u5df4\u52d2\u65af\u5766\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u542c\u969c\u5b66\u751f\u7684\u6570\u5b66\u6559\u80b2\u53ef\u53ca\u6027\u3002", "motivation": "\u7531\u4e8e\u5df4\u52d2\u65af\u5766\u624b\u8bed\uff08PSL\uff09\u7684\u6570\u5b57\u8d44\u6e90\u7a00\u7f3a\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u4e3a\u542c\u969c\u5b66\u751f\u63d0\u4f9b\u667a\u80fd\u6559\u80b2\u5de5\u5177\uff0c\u7f29\u5c0f\u5b66\u4e60\u5dee\u8ddd\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b41\u4e2a\u6570\u5b66\u624b\u52bf\u7c7b\u522b\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528ViT\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b9e\u73b0\u624b\u52bf\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523097.59%\uff0c\u663e\u793a\u51fa\u9ad8\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5f00\u53d1\u667a\u80fd\u6559\u80b2\u5de5\u5177\u4e2d\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u5305\u5bb9\u6027\u6559\u80b2\u7684\u6570\u5b57\u6574\u5408\u3002", "relevance": 20.0}}
{"id": "2505.17311", "pdf": "https://arxiv.org/pdf/2505.17311", "abs": "https://arxiv.org/abs/2505.17311", "authors": ["Harim Kim", "Yuhan Wang", "Minkyu Ahn", "Heeyoul Choi", "Yuyin Zhou", "Charmgil Hong"], "title": "Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI 2025 early accept", "summary": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for\nidentifying pathological abnormalities without requiring extensive labeled\ndata. However, existing diffusion-based UAD models rely solely on imaging\nfeatures, limiting their ability to distinguish between normal anatomical\nvariations and pathological anomalies. To address this, we propose Diff3M, a\nmulti-modal diffusion-based framework that integrates chest X-rays and\nstructured Electronic Health Records (EHRs) for enhanced anomaly detection.\nSpecifically, we introduce a novel image-EHR cross-attention module to\nincorporate structured clinical context into the image generation process,\nimproving the model's ability to differentiate normal from abnormal features.\nAdditionally, we develop a static masking strategy to enhance the\nreconstruction of normal-like images from anomalies. Extensive evaluations on\nCheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art\nperformance, outperforming existing UAD methods in medical imaging. Our code is\navailable at this http URL https://github.com/nth221/Diff3M", "AI": {"tldr": "Diff3M\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u80f8\u90e8X\u5149\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u56fe\u50cf-EHR\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7279\u5f81\uff0c\u96be\u4ee5\u533a\u5206\u6b63\u5e38\u89e3\u5256\u53d8\u5f02\u548c\u75c5\u7406\u5f02\u5e38\u3002", "method": "\u63d0\u51faDiff3M\u6846\u67b6\uff0c\u5f15\u5165\u56fe\u50cf-EHR\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u9759\u6001\u63a9\u7801\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728CheXpert\u548cMIMIC-CXR/IV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Diff3M\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17436", "pdf": "https://arxiv.org/pdf/2505.17436", "abs": "https://arxiv.org/abs/2505.17436", "authors": ["Cheng Peng", "Kai Zhang", "Mengxian Lyu", "Hongfang Liu", "Lichao Sun", "Yonghui Wu"], "title": "Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning", "categories": ["cs.AI"], "comment": null, "summary": "To advance biomedical vison-language model capabilities through scaling up,\nfine-tuning, and instruction tuning, develop vision-language models with\nimproved performance in handling long text, explore strategies to efficiently\nadopt vision language models for diverse multi-modal biomedical tasks, and\nexamine the zero-shot learning performance.\n  We developed two biomedical vision language models, BiomedGPT-Large and\nBiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.\nWe fine-tuned the two models on 23 benchmark datasets from 6 multi-modal\nbiomedical tasks including one image-only task (image classification), three\nlanguage-only tasks (text understanding, text summarization and question\nanswering), and two vision-language tasks (visual question answering and image\ncaptioning). We compared the developed scaled models with our previous\nBiomedGPT-Base model and existing prestigious models reported in the\nliterature. We instruction-tuned the two models using a large-scale multi-modal\nbiomedical instruction-tuning dataset and assessed the zero-shot learning\nperformance and alignment accuracy.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e24\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668Transformer\u67b6\u6784\u7684\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08BiomedGPT-Large\u548cBiomedGPT-XLarge\uff09\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u548c\u6307\u4ee4\u8c03\u4f18\u63d0\u5347\u5176\u5728\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u89c4\u6a21\u5316\u3001\u5fae\u8c03\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u957f\u6587\u672c\u548c\u591a\u6a21\u6001\u4efb\u52a1\u3002", "method": "\u57fa\u4e8eTransformer\u67b6\u6784\u5f00\u53d1\u4e24\u79cd\u6a21\u578b\uff0c\u572823\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6db5\u76d66\u79cd\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u5b66\u4e60\u548c\u5bf9\u9f50\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u89c4\u6a21\u5316\u3001\u5fae\u8c03\u548c\u6307\u4ee4\u8c03\u4f18\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17233", "pdf": "https://arxiv.org/pdf/2505.17233", "abs": "https://arxiv.org/abs/2505.17233", "authors": ["Andreas Patakis", "Vassilis Lyberatos", "Spyridon Kantarelis", "Edmund Dervakos", "Giorgos Stamou"], "title": "Semantic-Aware Interpretable Multimodal Music Auto-Tagging", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Music auto-tagging is essential for organizing and discovering music in\nextensive digital libraries. While foundation models achieve exceptional\nperformance in this domain, their outputs often lack interpretability, limiting\ntrust and usability for researchers and end-users alike. In this work, we\npresent an interpretable framework for music auto-tagging that leverages groups\nof musically meaningful multimodal features, derived from signal processing,\ndeep learning, ontology engineering, and natural language processing. To\nenhance interpretability, we cluster features semantically and employ an\nexpectation maximization algorithm, assigning distinct weights to each group\nbased on its contribution to the tagging process. Our method achieves\ncompetitive tagging performance while offering a deeper understanding of the\ndecision-making process, paving the way for more transparent and user-centric\nmusic tagging systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u7279\u5f81\u7684\u97f3\u4e50\u81ea\u52a8\u6807\u8bb0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u97f3\u4e50\u81ea\u52a8\u6807\u8bb0\u5728\u5927\u89c4\u6a21\u6570\u5b57\u5e93\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u7840\u6a21\u578b\u7684\u8f93\u51fa\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5229\u7528\u4fe1\u53f7\u5904\u7406\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u672c\u4f53\u5de5\u7a0b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u53d6\u591a\u6a21\u6001\u7279\u5f81\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u4e3a\u7279\u5f81\u7ec4\u5206\u914d\u6743\u91cd\u3002", "result": "\u65b9\u6cd5\u5728\u6807\u8bb0\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u7684\u6df1\u5165\u7406\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u66f4\u900f\u660e\u548c\u7528\u6237\u4e2d\u5fc3\u7684\u97f3\u4e50\u6807\u8bb0\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 40.0}}
{"id": "2505.17056", "pdf": "https://arxiv.org/pdf/2505.17056", "abs": "https://arxiv.org/abs/2505.17056", "authors": ["Luoxi Tang", "Tharunya Sundar", "Shuai Yang", "Ankita Patra", "Manohar Chippada", "Giqi Zhao", "Yi Li", "Riteng Zhang", "Tunan Zhao", "Ting Yang", "Yuqiao Meng", "Weicheng Ma", "Zhaohan Xi"], "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u82f1\u8bed\u6807\u51c6\u5316\u6d4b\u8bd5\uff08ESTs\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86ESTBOOK\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86LLMs\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u89e3\u5206\u6790\u6846\u67b6\u4ee5\u6539\u8fdb\u5176\u4f5c\u4e3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6559\u80b2\u9886\u57df\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6807\u51c6\u5316\u6d4b\u8bd5\u51c6\u5907\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c\u3002", "method": "\u5f15\u5165ESTBOOK\u57fa\u51c6\uff0c\u5305\u542b29\u79cd\u9898\u578b\u548c10,576\u4e2a\u95ee\u9898\uff0c\u8bc4\u4f30LLMs\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff1b\u63d0\u51fa\u5206\u89e3\u5206\u6790\u6846\u67b6\uff0c\u5206\u6b65\u8bc4\u4f30LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u63ed\u793a\u4e86LLMs\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5176\u4f5c\u4e3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u7b56\u7565\u3002", "conclusion": "LLMs\u5728\u6807\u51c6\u5316\u6d4b\u8bd5\u51c6\u5907\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "relevance": 65.0}}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u6295\u5f71\u5668\uff08projector\uff09\u6765\u589e\u5f3a\u89c6\u89c9\u5d4c\u5165\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08patch-aligned training\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001LLM\uff08MLLM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u591a\u6a21\u6001LLM\uff08MLLM\uff09\u7684\u80fd\u529b\u9700\u8981\u66f4\u597d\u5730\u5bf9\u9f50\u89c6\u89c9\u5d4c\u5165\u4e0eLLM\uff0c\u4f46\u73b0\u6709\u6295\u5f71\u5668\u7684\u8bad\u7ec3\u673a\u5236\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4\u5bf9\u9f50\u6548\u679c\u6709\u9650\u3002", "method": "\u5206\u6790\u4e86\u6295\u5f71\u5668\u5728\u89c6\u89c9\u5d4c\u5165\u538b\u7f29\u548c\u5bf9\u9f50\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u591a\u8bed\u4e49\u5bf9\u9f50\u5047\u8bbe\uff0c\u5e76\u8bbe\u8ba1\u4e86patch-aligned training\u65b9\u6cd5\u3002", "result": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u80fd\u529b\u548c\u5bf9\u9f50\u6548\u679c\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\uff08\u5982\u6307\u4ee3\u8868\u8fbe\u5b9a\u4f4d\u3001\u95ee\u7b54\u548c\u6307\u4ee4\u8ddf\u968f\uff09\u4e2d\u6027\u80fd\u63d0\u53473%-16%\u3002", "conclusion": "patch-aligned training\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u591a\u6a21\u6001\u6a21\u578b\u3002", "relevance": 85.0}}
{"id": "2505.17482", "pdf": "https://arxiv.org/pdf/2505.17482", "abs": "https://arxiv.org/abs/2505.17482", "authors": ["Chao Lei", "Nir Lipovetzky", "Krista A. Ehinger", "Yanchuan Chang"], "title": "From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent reasoning-oriented LLMs have demonstrated strong performance on\nchallenging tasks such as mathematics and science examinations. However, core\ncognitive faculties of human intelligence, such as abstract reasoning and\ngeneralization, remain underexplored. To address this, we evaluate recent\nreasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)\nbenchmark, which explicitly demands both faculties. We formulate ARC as a\nprogram synthesis task and propose nine candidate solvers. Experimental results\nshow that repeated-sampling planning-aided code generation (RSPC) achieves the\nhighest test accuracy and demonstrates consistent generalization across most\nLLMs. To further improve performance, we introduce an ARC solver, Knowledge\nAugmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors\nwithin an ontology that classifies priors into three hierarchical levels based\non their dependencies. KAAR progressively expands LLM reasoning capacity by\ngradually augmenting priors at each level, and invokes RSPC to generate\ncandidate solutions after each augmentation stage. This stage-wise reasoning\nreduces interference from irrelevant priors and improves LLM performance.\nEmpirical results show that KAAR maintains strong generalization and\nconsistently outperforms non-augmented RSPC across all evaluated LLMs,\nachieving around 5% absolute gains and up to 64.52% relative improvement.\nDespite these achievements, ARC remains a challenging benchmark for\nreasoning-oriented LLMs, highlighting future avenues of progress in LLMs.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u63a8\u7406\u5bfc\u5411\u7684LLMs\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\uff08ARC\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86RSPC\u548cKAAR\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u62bd\u8c61\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7ARC\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5c06ARC\u4efb\u52a1\u8f6c\u5316\u4e3a\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0c\u63d0\u51faRSPC\u548cKAAR\u65b9\u6cd5\uff0c\u540e\u8005\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u5206\u9636\u6bb5\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "result": "KAAR\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u7684LLMs\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u76f8\u5bf9\u63d0\u5347\u8fbe64.52%\u3002", "conclusion": "ARC\u5bf9LLMs\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46KAAR\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.17242", "pdf": "https://arxiv.org/pdf/2505.17242", "abs": "https://arxiv.org/abs/2505.17242", "authors": ["Ram\u00f3n Fernandez Astudillo", "Md Arafat Sultan", "Aashka Trivedi", "Yousef El-Kurdi", "Tahira Naseem", "Radu Florian", "Salim Roukos"], "title": "Optimal Policy Minimum Bayesian Risk", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling can help LLMs solve complex reasoning problems through\nextended runtime computation. On top of targeted supervision for long\nchain-of-thought (long-CoT) generation, purely inference-time techniques such\nas best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes\nrisk decoding (MBRD), can further improve LLM accuracy by generating multiple\ncandidate solutions and aggregating over them. These methods typically leverage\nadditional signals in the form of reward models and risk/similarity functions\nthat compare generated samples, e.g., exact match in some normalized space or\nstandard similarity metrics such as Rouge. Here we present a novel method for\nincorporating reward and risk/similarity signals into MBRD. Based on the\nconcept of optimal policy in KL-controlled reinforcement learning, our\nframework provides a simple and well-defined mechanism for leveraging such\nsignals, offering several advantages over traditional inference-time methods:\nhigher robustness, improved accuracy, and well-understood asymptotic behavior.\nIn addition, it allows for the development of a sample-efficient variant of\nMBRD that can adjust the number of samples to generate according to the\ndifficulty of the problem, without relying on majority vote counts. We\nempirically demonstrate the advantages of our approach on math (MATH-$500$) and\ncoding (HumanEval) tasks using recent open-source models. We also present a\ncomprehensive analysis of its accuracy-compute trade-offs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKL\u63a7\u5236\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u5956\u52b1\u548c\u98ce\u9669/\u76f8\u4f3c\u6027\u4fe1\u53f7\u878d\u5165\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\uff08MBRD\uff09\uff0c\u4ee5\u63d0\u9ad8LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u95f4\u65b9\u6cd5\uff08\u5982BoN\u91c7\u6837\u3001\u591a\u6570\u6295\u7968\uff09\u867d\u80fd\u63d0\u5347LLM\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u7406\u8bba\u652f\u6301\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u5316MBRD\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u53ef\u63a7\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eKL\u63a7\u5236\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u4f18\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u5956\u52b1\u548c\u98ce\u9669\u4fe1\u53f7\u878d\u5165MBRD\uff0c\u652f\u6301\u52a8\u6001\u8c03\u6574\u751f\u6210\u6837\u672c\u6570\u91cf\u3002", "result": "\u5728\u6570\u5b66\uff08MATH-500\uff09\u548c\u7f16\u7a0b\uff08HumanEval\uff09\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "relevance": 85.0}}
{"id": "2505.17058", "pdf": "https://arxiv.org/pdf/2505.17058", "abs": "https://arxiv.org/abs/2505.17058", "authors": ["David Osei Opoku", "Ming Sheng", "Yong Zhang"], "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 5 figures;", "summary": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.", "AI": {"tldr": "DO-RAG\u662f\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u8bed\u4e49\u5411\u91cf\u68c0\u7d22\u7684\u6df7\u5408QA\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u94fe\u5f0f\u601d\u7ef4\u67b6\u6784\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u56de\u7b54\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RAG\u6846\u67b6\u5728\u5f02\u6784\u6570\u636e\u6574\u5408\u548c\u63a8\u7406\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u9886\u57df\u7279\u5b9aQA\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faDO-RAG\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u7ea7\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e0e\u8bed\u4e49\u5411\u91cf\u68c0\u7d22\uff0c\u91c7\u7528\u94fe\u5f0f\u601d\u7ef4\u67b6\u6784\u63d0\u53d6\u7ed3\u6784\u5316\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u56fe\u4e0e\u5411\u91cf\u68c0\u7d22\u878d\u5408\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u56de\u7b54\u3002", "result": "\u5728\u6570\u636e\u5e93\u548c\u7535\u6c14\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cDO-RAG\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\u548c94%\u4ee5\u4e0a\u7684\u56de\u7b54\u76f8\u5173\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6846\u67b633.38%\u3002", "conclusion": "DO-RAG\u901a\u8fc7\u53ef\u8ffd\u6eaf\u6027\u3001\u9002\u5e94\u6027\u548c\u9ad8\u6548\u6027\u80fd\uff0c\u4e3a\u591a\u9886\u57df\u9ad8\u7cbe\u5ea6QA\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002", "relevance": 75.0}}
{"id": "2505.17317", "pdf": "https://arxiv.org/pdf/2505.17317", "abs": "https://arxiv.org/abs/2505.17317", "authors": ["Alyson East", "Elizabeth G. Campolongo", "Luke Meyers", "S M Rayeed", "Samuel Stevens", "Iuliia Zarubiieva", "Isadora E. Fluck", "Jennifer C. Gir\u00f3n", "Maximiliane Jousse", "Scott Lowe", "Kayla I Perry", "Isabelle Betancourt", "Noah Charney", "Evan Donoso", "Nathan Fox", "Kim J. Landsbergen", "Ekaterina Nepovinnykh", "Michelle Ramirez", "Parkash Singh", "Khum Thapa-Magar", "Matthew Thompson", "Evan Waite", "Tanya Berger-Wolf", "Hilmar Lapp", "Paula Mabee", "Graham Taylor", "Sydne Record"], "title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Biological collections house millions of specimens documenting Earth's\nbiodiversity, with digital images increasingly available through open-access\nplatforms. Most imaging protocols were developed for human visual\ninterpretation without considering computational analysis requirements. This\npaper aims to bridge the gap between current imaging practices and the\npotential for automated analysis by presenting key considerations for creating\nbiological specimen images optimized for computer vision applications. We\nprovide conceptual computer vision topics for context, addressing fundamental\nconcerns including model generalization, data leakage, and comprehensive\nmetadata documentation, and outline practical guidance on specimen imagine, and\ndata storage. These recommendations were synthesized through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists. Through this synthesis, we have identified ten\ninterconnected considerations that form a framework for successfully\nintegrating biological specimen images into computer vision pipelines. The key\nelements include: (1) comprehensive metadata documentation, (2) standardized\nspecimen positioning, (3) consistent size and color calibration, (4) protocols\nfor handling multiple specimens in one image, (5) uniform background selection,\n(6) controlled lighting, (7) appropriate resolution and magnification, (8)\noptimal file formats, (9) robust data archiving strategies, and (10) accessible\ndata sharing practices. By implementing these recommendations, collection\nmanagers, taxonomists, and biodiversity informaticians can generate images that\nsupport automated trait extraction, species identification, and novel\necological and evolutionary analyses at unprecedented scales. Successful\nimplementation lies in thorough documentation of methodological choices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4f18\u5316\u751f\u7269\u6807\u672c\u56fe\u50cf\u4ee5\u652f\u6301\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u768410\u9879\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5305\u62ec\u6807\u51c6\u5316\u6210\u50cf\u3001\u5143\u6570\u636e\u8bb0\u5f55\u548c\u6570\u636e\u5171\u4eab\u7b49\u3002", "motivation": "\u5f53\u524d\u751f\u7269\u6807\u672c\u6210\u50cf\u65b9\u6cd5\u4e3b\u8981\u4e3a\u4eba\u7c7b\u89c6\u89c9\u8bbe\u8ba1\uff0c\u672a\u8003\u8651\u8ba1\u7b97\u673a\u89c6\u89c9\u9700\u6c42\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u5408\u4f5c\uff08\u5206\u7c7b\u5b66\u5bb6\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u7b49\uff09\u603b\u7ed3\u51fa10\u9879\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5f62\u6210\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5e2e\u52a9\u751f\u6210\u652f\u6301\u81ea\u52a8\u5316\u5206\u6790\u7684\u751f\u7269\u6807\u672c\u56fe\u50cf\u3002", "conclusion": "\u5b9e\u65bd\u8fd9\u4e9b\u5efa\u8bae\u53ef\u4fc3\u8fdb\u5927\u89c4\u6a21\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u548c\u7269\u79cd\u8bc6\u522b\u3002", "relevance": 20.0}}
{"id": "2505.17492", "pdf": "https://arxiv.org/pdf/2505.17492", "abs": "https://arxiv.org/abs/2505.17492", "authors": ["Dezheng Bao", "Yueci Yang", "Xin Chen", "Zhengxuan Jiang", "Zeguo Fei", "Daoze Zhang", "Xuanwen Huang", "Junru Chen", "Chutian Yu", "Xiang Yuan", "Yang Yang"], "title": "PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 pages, 9 figures", "summary": "Project duplication detection is critical for project quality assessment, as\nit improves resource utilization efficiency by preventing investing in newly\nproposed project that have already been studied. It requires the ability to\nunderstand high-level semantics and generate constructive and valuable\nfeedback. Existing detection methods rely on basic word- or sentence-level\ncomparison or solely apply large language models, lacking valuable insights for\nexperts and in-depth comprehension of project content and review criteria. To\ntackle this issue, we propose PD$^3$, a Project Duplication Detection framework\nvia adapted multi-agent Debate. Inspired by real-world expert debates, it\nemploys a fair competition format to guide multi-agent debate to retrieve\nrelevant projects. For feedback, it incorporates both qualitative and\nquantitative analysis to improve its practicality. Over 800 real-world power\nproject data spanning more than 20 specialized fields are used to evaluate the\nframework, demonstrating that our method outperforms existing approaches by\n7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online\nplatform, Review Dingdang, to assist power experts, saving 5.73 million USD in\ninitial detection on more than 100 newly proposed projects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPD$^3$\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u68c0\u6d4b\u9879\u76ee\u91cd\u590d\u6027\uff0c\u7ed3\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd57.43%\u548c8.00%\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u7ebf\u5e73\u53f0Review Dingdang\u3002", "motivation": "\u9879\u76ee\u91cd\u590d\u68c0\u6d4b\u5bf9\u8d44\u6e90\u5229\u7528\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u548c\u4e13\u5bb6\u7ea7\u53cd\u9988\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\uff0c\u4ece800\u591a\u4e2a\u5b9e\u9645\u7535\u529b\u9879\u76ee\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5728\u4e24\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u53477.43%\u548c8.00%\uff0c\u8282\u7701573\u4e07\u7f8e\u5143\u521d\u59cb\u68c0\u6d4b\u6210\u672c\u3002", "conclusion": "PD$^3$\u6846\u67b6\u6709\u6548\u63d0\u5347\u9879\u76ee\u91cd\u590d\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17248", "pdf": "https://arxiv.org/pdf/2505.17248", "abs": "https://arxiv.org/abs/2505.17248", "authors": ["Chace Ashcraft", "Ted Staley", "Josh Carney", "Cameron Hickert", "Derek Juba", "Kiran Karra", "Nathan Drenkow"], "title": "Backdoors in DRL: Four Environments Focusing on In-distribution Triggers", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Backdoor attacks, or trojans, pose a security risk by concealing undesirable\nbehavior in deep neural network models. Open-source neural networks are\ndownloaded from the internet daily, possibly containing backdoors, and\nthird-party model developers are common. To advance research on backdoor attack\nmitigation, we develop several trojans for deep reinforcement learning (DRL)\nagents. We focus on in-distribution triggers, which occur within the agent's\nnatural data distribution, since they pose a more significant security threat\nthan out-of-distribution triggers due to their ease of activation by the\nattacker during model deployment. We implement backdoor attacks in four\nreinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,\nColorful Memory, and Modified Safety Gymnasium. We train various models, both\nclean and backdoored, to characterize these attacks. We find that\nin-distribution triggers can require additional effort to implement and be more\nchallenging for models to learn, but are nevertheless viable threats in DRL\neven using basic data poisoning attacks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u5e03\u5185\u89e6\u53d1\u5668\uff0c\u5e76\u5728\u56db\u4e2aRL\u73af\u5883\u4e2d\u5b9e\u65bd\u4e86\u653b\u51fb\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5206\u5e03\u5185\u89e6\u53d1\u5668\u5b9e\u73b0\u96be\u5ea6\u8f83\u9ad8\uff0c\u4f46\u4ecd\u6784\u6210\u5a01\u80c1\u3002", "motivation": "\u5f00\u6e90\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u5b58\u5728\u540e\u95e8\uff0c\u5bf9\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002\u7814\u7a76\u65e8\u5728\u63a8\u8fdb\u540e\u95e8\u653b\u51fb\u7f13\u89e3\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728DRL\u9886\u57df\u3002", "method": "\u5728\u56db\u4e2aRL\u73af\u5883\u4e2d\uff08LavaWorld\u3001Randomized LavaWorld\u3001Colorful Memory\u3001Modified Safety Gymnasium\uff09\u5b9e\u65bd\u540e\u95e8\u653b\u51fb\uff0c\u8bad\u7ec3\u5e72\u51c0\u548c\u5e26\u540e\u95e8\u7684\u6a21\u578b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5206\u5e03\u5185\u89e6\u53d1\u5668\u867d\u5b9e\u73b0\u96be\u5ea6\u8f83\u9ad8\uff0c\u4f46\u4ecd\u80fd\u901a\u8fc7\u57fa\u672c\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\u5728DRL\u4e2d\u6784\u6210\u5a01\u80c1\u3002", "conclusion": "\u5206\u5e03\u5185\u89e6\u53d1\u5668\u662fDRL\u4e2d\u53ef\u884c\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u7f13\u89e3\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.17059", "pdf": "https://arxiv.org/pdf/2505.17059", "abs": "https://arxiv.org/abs/2505.17059", "authors": ["Van-Tinh Nguyen", "Hoang-Duong Pham", "Thanh-Hai To", "Cong-Tuan Hung Do", "Thi-Thu-Trang Dong", "Vu-Trung Duong Le", "Van-Phuc Hoang"], "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary\n  version posted for early dissemination and feedback", "summary": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.", "AI": {"tldr": "Medalyze\u662f\u4e00\u4e2a\u57fa\u4e8eFLAN-T5-Large\u6a21\u578b\u7684AI\u5e94\u7528\uff0c\u7528\u4e8e\u589e\u5f3a\u533b\u5b66\u6587\u672c\u7406\u89e3\uff0c\u5305\u62ec\u62a5\u544a\u6458\u8981\u3001\u5065\u5eb7\u95ee\u9898\u63d0\u53d6\u548c\u5173\u952e\u95ee\u9898\u8bc6\u522b\u3002", "motivation": "\u533b\u5b66\u6587\u672c\u590d\u6742\u4e14\u8bed\u5883\u7279\u6b8a\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u63d0\u5347\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u4e09\u4e2aFLAN-T5-Large\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u90e8\u7f72\u4e8eWeb\u548c\u79fb\u52a8\u5e73\u53f0\uff0c\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u3002", "result": "\u5728\u533b\u5b66\u9886\u57df\u4efb\u52a1\u4e2d\uff0cMedalyze\u7684\u6458\u8981\u6027\u80fd\u4f18\u4e8eGPT-4\u3002", "conclusion": "Medalyze\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u533b\u7597\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17328", "pdf": "https://arxiv.org/pdf/2505.17328", "abs": "https://arxiv.org/abs/2505.17328", "authors": ["Dylan Kline"], "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning", "categories": ["cs.CV"], "comment": null, "summary": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b66\u4e60\u6e38\u620f\u65e0\u5173\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u63d0\u5347\u8de8\u6e38\u620f\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6e38\u620f\u56fe\u50cf\u7f16\u7801\u5668\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u7279\u5b9a\u6e38\u620f\u7684\u89c6\u89c9\u98ce\u683c\uff0c\u5bfc\u81f4\u5728\u65b0\u6e38\u620f\u4e0a\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9f13\u52b1\u76f8\u4f3c\u5185\u5bb9\u805a\u7c7b\uff0c\u540c\u65f6\u901a\u8fc7\u5bf9\u6297\u9886\u57df\u5206\u7c7b\u5668\u6291\u5236\u6e38\u620f\u7279\u5b9a\u7ebf\u7d22\uff0c\u4ece\u800c\u751f\u6210\u6cdb\u5316\u6027\u5f3a\u7684\u5d4c\u5165\u3002", "result": "\u5728Bingsu\u6e38\u620f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u7279\u5f81\u4e0d\u518d\u6309\u6e38\u620f\u805a\u7c7b\uff0c\u663e\u793a\u51fa\u6210\u529f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9700\u8981\u6781\u5c11\u91cd\u65b0\u8bad\u7ec3\u7684\u6e38\u620f\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17512", "pdf": "https://arxiv.org/pdf/2505.17512", "abs": "https://arxiv.org/abs/2505.17512", "authors": ["Shuhang Xu", "Weijian Deng", "Yixuan Zhou", "Fangwei Zhong"], "title": "Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Concepts represent generalized abstractions that enable humans to categorize\nand reason efficiently, yet it is unclear to what extent Large Language Models\n(LLMs) comprehend these semantic relationships. Existing benchmarks typically\nfocus on factual recall and isolated tasks, failing to evaluate the ability of\nLLMs to understand conceptual boundaries. To address this gap, we introduce\nCK-Arena, a multi-agent interaction game built upon the Undercover game,\ndesigned to evaluate the capacity of LLMs to reason with concepts in\ninteractive settings. CK-Arena challenges models to describe, differentiate,\nand infer conceptual boundaries based on partial information, encouraging\nmodels to explore commonalities and distinctions between closely related\nconcepts. By simulating real-world interaction, CK-Arena provides a scalable\nand realistic benchmark for assessing conceptual reasoning in dynamic\nenvironments. Experimental results show that LLMs' understanding of conceptual\nknowledge varies significantly across different categories and is not strictly\naligned with parameter size or general model capabilities. The data and code\nare available at the project homepage: https://ck-arena.site.", "AI": {"tldr": "CK-Arena\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6e38\u620f\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u7406\u89e3\u80fd\u529b\u4e0e\u6a21\u578b\u89c4\u6a21\u65e0\u5173\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u56de\u5fc6\u548c\u5b64\u7acb\u4efb\u52a1\uff0c\u672a\u80fd\u8bc4\u4f30LLMs\u5bf9\u6982\u5ff5\u8fb9\u754c\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f15\u5165CK-Arena\uff0c\u57fa\u4e8eUndercover\u6e38\u620f\u8bbe\u8ba1\uff0c\u8981\u6c42\u6a21\u578b\u5728\u4ea4\u4e92\u4e2d\u63cf\u8ff0\u3001\u533a\u5206\u548c\u63a8\u65ad\u6982\u5ff5\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5bf9\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\u5728\u4e0d\u540c\u7c7b\u522b\u4e2d\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u4e0e\u6a21\u578b\u89c4\u6a21\u6216\u901a\u7528\u80fd\u529b\u65e0\u5173\u3002", "conclusion": "CK-Arena\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6982\u5ff5\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u73b0\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "relevance": 85.0}}
{"id": "2505.17254", "pdf": "https://arxiv.org/pdf/2505.17254", "abs": "https://arxiv.org/abs/2505.17254", "authors": ["Alexey Boldyrev", "Fedor Ratnikov", "Andrey Shevelev"], "title": "Approach to Finding a Robust Deep Learning Model", "categories": ["cs.LG", "I.2.1"], "comment": "27 pages, 18 figures", "summary": "The rapid development of machine learning (ML) and artificial intelligence\n(AI) applications requires the training of large numbers of models. This\ngrowing demand highlights the importance of training models without human\nsupervision, while ensuring that their predictions are reliable. In response to\nthis need, we propose a novel approach for determining model robustness. This\napproach, supplemented with a proposed model selection algorithm designed as a\nmeta-algorithm, is versatile and applicable to any machine learning model,\nprovided that it is appropriate for the task at hand. This study demonstrates\nthe application of our approach to evaluate the robustness of deep learning\nmodels. To this end, we study small models composed of a few convolutional and\nfully connected layers, using common optimizers due to their ease of\ninterpretation and computational efficiency. Within this framework, we address\nthe influence of training sample size, model weight initialization, and\ninductive bias on the robustness of deep learning models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6a21\u578b\u9009\u62e9\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u8bad\u7ec3\u5927\u91cf\u6a21\u578b\uff0c\u540c\u65f6\u786e\u4fdd\u5176\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8f85\u4ee5\u4e00\u4e2a\u6a21\u578b\u9009\u62e9\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u9002\u5408\u4efb\u52a1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u7814\u7a76\u4e2d\u4f7f\u7528\u5c0f\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5377\u79ef\u5c42\u548c\u5168\u8fde\u63a5\u5c42\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63a2\u8ba8\u4e86\u8bad\u7ec3\u6837\u672c\u5927\u5c0f\u3001\u6a21\u578b\u6743\u91cd\u521d\u59cb\u5316\u548c\u5f52\u7eb3\u504f\u5dee\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u6709\u6548\u7684\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060", "abs": "https://arxiv.org/abs/2505.17060", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "AI": {"tldr": "SALMONN-omni \u662f\u4e00\u79cd\u65b0\u578b\u7684\u5168\u53cc\u5de5\u8bed\u97f3 LLM\uff0c\u65e0\u9700\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u601d\u8003\u673a\u5236\u5b9e\u73b0\u542c\u4e0e\u8bf4\u7684\u72b6\u6001\u5207\u6362\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5168\u53cc\u5de5\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u6a21\u5757\u5316\u67b6\u6784\u5bfc\u81f4\u7684\u9519\u8bef\u7d2f\u79ef\u548c\u8bed\u97f3\u6a21\u6001\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5355\u4e00 LLM \u67b6\u6784\uff0c\u5f15\u5165\u52a8\u6001\u601d\u8003\u673a\u5236\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5728\u8bed\u97f3\u95ee\u7b54\u548c\u5f00\u653e\u57df\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347 30%\uff0c\u4e14\u5728\u5904\u7406\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SALMONN-omni \u4e3a\u5168\u53cc\u5de5\u8bed\u97f3\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7b80\u6d01\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17330", "pdf": "https://arxiv.org/pdf/2505.17330", "abs": "https://arxiv.org/abs/2505.17330", "authors": ["Amit Agarwal", "Srikant Panda", "Kulbhushan Pachauri"], "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; I.7"], "comment": "Published in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Industry Track, pages 100-114", "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag", "AI": {"tldr": "FS-DAG\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\uff08VRDU\uff09\uff0c\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u4f4e\u53c2\u6570\u91cf\u7684\u7279\u70b9\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u4e2d\u7684\u5c11\u6837\u672c\u9002\u5e94\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9OCR\u9519\u8bef\u3001\u62fc\u5199\u9519\u8bef\u548c\u9886\u57df\u504f\u79fb\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u548c\u8bed\u8a00/\u89c6\u89c9\u7279\u5b9a\u9aa8\u5e72\u7f51\u7edc\uff0c\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u9002\u5e94\u591a\u6837\u5316\u6587\u6863\u7c7b\u578b\u3002", "result": "\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\uff0cFS-DAG\u5728\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53c2\u6570\u5c11\u4e8e90M\u3002", "conclusion": "FS-DAG\u5c55\u793a\u4e86\u5f00\u53d1\u9ad8\u6548\u3001\u5c0f\u578b\u6a21\u578b\u800c\u4e0d\u727a\u7272\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u5e94\u7528\u3002", "relevance": 40.0}}
{"id": "2505.17520", "pdf": "https://arxiv.org/pdf/2505.17520", "abs": "https://arxiv.org/abs/2505.17520", "authors": ["Salahuddin Alawadhi", "Noorhan Abbas"], "title": "Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers", "categories": ["cs.AI", "I.2.7; H.3.3"], "comment": "17 pages, 4 figures, published in CSIT Vol. 15, 2025. DOI:\n  10.5121/csit.2025.150905", "summary": "Integrating Retrieval Augmented Generation (RAG) with Large Language Models\n(LLMs) has shown the potential to provide precise, contextually relevant\nresponses in knowledge intensive domains. This study investigates the\nap-plication of RAG for ABB circuit breakers, focusing on accuracy,\nreliability, and contextual relevance in high-stakes engineering environments.\nBy leveraging tailored datasets, advanced embedding models, and optimized\nchunking strategies, the research addresses challenges in data retrieval and\ncontextual alignment unique to engineering documentation. Key contributions\ninclude the development of a domain-specific dataset for ABB circuit breakers\nand the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic\nClaude. Advanced chunking methods, such as paragraph-based and title-aware\nsegmentation, are assessed for their impact on retrieval accuracy and response\ngeneration. Results demonstrate that while certain configurations achieve high\nprecision and relevancy, limitations persist in ensuring factual faithfulness\nand completeness, critical in engineering contexts. This work underscores the\nneed for iterative improvements in RAG systems to meet the stringent demands of\nelectrical engineering tasks, including design, troubleshooting, and\noperational decision-making. The findings in this paper help advance research\nof AI in highly technical domains such as electrical engineering.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u5e94\u7528\u4e8eABB\u65ad\u8def\u5668\u7684\u5de5\u7a0b\u6587\u6863\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cdRAG\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5de5\u7a0b\u6587\u6863\u4e2d\u6570\u636e\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4ee5\u6ee1\u8db3\u7535\u6c14\u5de5\u7a0b\u9886\u57df\u5bf9\u9ad8\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5b9a\u5236\u6570\u636e\u96c6\u3001\u9ad8\u7ea7\u5d4c\u5165\u6a21\u578b\u548c\u4f18\u5316\u7684\u5206\u5757\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cdRAG\u7ba1\u9053\uff08OpenAI GPT4o\u3001Cohere\u548cAnthropic Claude\uff09\u7684\u6027\u80fd\u3002", "result": "\u67d0\u4e9b\u914d\u7f6e\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u76f8\u5173\u6027\uff0c\u4f46\u5728\u786e\u4fdd\u4e8b\u5b9e\u5fe0\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86RAG\u7cfb\u7edf\u5728\u7535\u6c14\u5de5\u7a0b\u4efb\u52a1\u4e2d\u9700\u8981\u8fed\u4ee3\u6539\u8fdb\uff0c\u4ee5\u6ee1\u8db3\u4e25\u683c\u7684\u8bbe\u8ba1\u3001\u6545\u969c\u6392\u9664\u548c\u64cd\u4f5c\u51b3\u7b56\u9700\u6c42\u3002", "relevance": 75.0}}
{"id": "2505.17257", "pdf": "https://arxiv.org/pdf/2505.17257", "abs": "https://arxiv.org/abs/2505.17257", "authors": ["Qihao Duan", "Bingding Huang", "Zhenqiao Song", "Irina Lehmann", "Lei Gu", "Roland Eils", "Benjamin Wild"], "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing\nand are increasingly applied to other sequential data types, including genetic\nsequences. However, adapting LLMs to genomics presents significant challenges.\nCapturing complex genomic interactions requires modeling long-range\ndependencies within DNA sequences, where interactions often span over 10,000\nbase pairs, even within a single gene, posing substantial computational burdens\nunder conventional model architectures and training paradigms. Moreover,\nstandard LLM training approaches are suboptimal for DNA: autoregressive\ntraining, while efficient, supports only unidirectional understanding. However,\nDNA is inherently bidirectional, e.g., bidirectional promoters regulate\ntranscription in both directions and account for nearly 11% of human gene\nexpression. Masked language models (MLMs) allow bidirectional understanding but\nare inefficient, as only masked tokens contribute to the loss per step. To\naddress these limitations, we introduce JanusDNA, the first bidirectional DNA\nfoundation model built upon a novel pretraining paradigm that combines the\noptimization efficiency of autoregressive modeling with the bidirectional\ncomprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and\nMixture of Experts (MoE) architecture, combining long-range modeling of\nAttention with efficient sequential learning of Mamba. MoE layers further scale\nmodel capacity via sparse activation while keeping computational cost low.\nNotably, JanusDNA processes up to 1 million base pairs at single nucleotide\nresolution on a single 80GB GPU. Extensive experiments and ablations show\nJanusDNA achieves new SOTA results on three genomic representation benchmarks,\noutperforming models with 250x more activated parameters. Code:\nhttps://github.com/Qihao-Duan/JanusDNA", "AI": {"tldr": "JanusDNA\u662f\u4e00\u4e2a\u53cc\u5411DNA\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u6548\u7387\u548c\u63a9\u7801\u5efa\u6a21\u7684\u53cc\u5411\u7406\u89e3\u80fd\u529b\uff0c\u91c7\u7528\u6df7\u5408Mamba\u3001Attention\u548cMoE\u67b6\u6784\uff0c\u652f\u6301\u767e\u4e07\u7ea7\u78b1\u57fa\u5bf9\u5904\u7406\u3002", "motivation": "\u9002\u5e94LLMs\u5230\u57fa\u56e0\u7ec4\u5b66\u9762\u4e34\u957f\u8303\u56f4\u4f9d\u8d56\u548c\u53cc\u5411\u7406\u89e3\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u6216\u529f\u80fd\u6709\u9650\u3002", "method": "\u63d0\u51faJanusDNA\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u63a9\u7801\u5efa\u6a21\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u91c7\u7528Mamba\u3001Attention\u548cMoE\u6df7\u5408\u67b6\u6784\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u56e0\u7ec4\u8868\u793a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "JanusDNA\u4e3a\u57fa\u56e0\u7ec4\u5b66\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u529f\u80fd\u5f3a\u5927\u7684\u5efa\u6a21\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061", "abs": "https://arxiv.org/abs/2505.17061", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89e3\u7801\u7b56\u7565\u6765\u7f13\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LVLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff0cMoD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MoD\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5bf9\u56fe\u50cf\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u6b63\u786e\u6027\uff0c\u52a8\u6001\u8c03\u6574\u89e3\u7801\u7b56\u7565\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u68c0\u6d4b\u548c\u4e92\u8865\u6216\u5bf9\u6bd4\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoD\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "MoD\u4e3aLVLMs\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 70.0}}
{"id": "2505.17333", "pdf": "https://arxiv.org/pdf/2505.17333", "abs": "https://arxiv.org/abs/2505.17333", "authors": ["Xin You", "Minghui Zhang", "Hanxiao Zhang", "Jie Yang", "Nassir Navab"], "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis", "categories": ["cs.CV"], "comment": "early accepted by MICCAI", "summary": "Temporal modeling on regular respiration-induced motions is crucial to\nimage-guided clinical applications. Existing methods cannot simulate temporal\nmotions unless high-dose imaging scans including starting and ending frames\nexist simultaneously. However, in the preoperative data acquisition stage, the\nslight movement of patients may result in dynamic backgrounds between the first\nand last frames in a respiratory period. This additional deviation can hardly\nbe removed by image registration, thus affecting the temporal modeling. To\naddress that limitation, we pioneeringly simulate the regular motion process\nvia the image-to-video (I2V) synthesis framework, which animates with the first\nframe to forecast future frames of a given length. Besides, to promote the\ntemporal consistency of animated videos, we devise the Temporal Differential\nDiffusion Model to generate temporal differential fields, which measure the\nrelative differential representations between adjacent frames. The prompt\nattention layer is devised for fine-grained differential fields, and the field\naugmented layer is adopted to better interact these fields with the I2V\nframework, promoting more accurate temporal variation of synthesized videos.\nExtensive results on ACDC cardiac and 4D Lung datasets reveal that our approach\nsimulates 4D videos along the intrinsic motion trajectory, rivaling other\ncompetitive methods on perceptual similarity and temporal consistency. Codes\nwill be available soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u5408\u6210\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u547c\u5438\u8fd0\u52a8\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9ad8\u5242\u91cf\u626b\u63cf\u6570\u636e\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u540c\u65f6\u5b58\u5728\u8d77\u59cb\u548c\u7ed3\u675f\u5e27\u7684\u9ad8\u5242\u91cf\u626b\u63cf\u6570\u636e\uff0c\u800c\u60a3\u8005\u8f7b\u5fae\u79fb\u52a8\u53ef\u80fd\u5bfc\u81f4\u52a8\u6001\u80cc\u666f\u504f\u5dee\uff0c\u5f71\u54cd\u65f6\u95f4\u5efa\u6a21\u3002", "method": "\u91c7\u7528I2V\u6846\u67b6\uff0c\u901a\u8fc7\u7b2c\u4e00\u5e27\u9884\u6d4b\u672a\u6765\u5e27\uff0c\u5e76\u8bbe\u8ba1Temporal Differential Diffusion Model\u751f\u6210\u65f6\u95f4\u5dee\u5206\u573a\uff0c\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728ACDC\u5fc3\u810f\u548c4D Lung\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6a21\u62df\u4e86\u6cbf\u56fa\u6709\u8fd0\u52a8\u8f68\u8ff9\u76844D\u89c6\u9891\uff0c\u5728\u611f\u77e5\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u547c\u5438\u8fd0\u52a8\u65f6\u95f4\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u56fe\u50cf\u5f15\u5bfc\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.17525", "pdf": "https://arxiv.org/pdf/2505.17525", "abs": "https://arxiv.org/abs/2505.17525", "authors": ["Juliett Su\u00e1rez Ferreira", "Marija Slavkovik", "Jorge Casillas"], "title": "Transparency and Proportionality in Post-Processing Algorithmic Bias Correction", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Algorithmic decision-making systems sometimes produce errors or skewed\npredictions toward a particular group, leading to unfair results. Debiasing\npractices, applied at different stages of the development of such systems,\noccasionally introduce new forms of unfairness or exacerbate existing\ninequalities. We focus on post-processing techniques that modify algorithmic\npredictions to achieve fairness in classification tasks, examining the\nunintended consequences of these interventions. To address this challenge, we\ndevelop a set of measures that quantify the disparity in the flips applied to\nthe solution in the post-processing stage. The proposed measures will help\npractitioners: (1) assess the proportionality of the debiasing strategy used,\n(2) have transparency to explain the effects of the strategy in each group, and\n(3) based on those results, analyze the possibility of the use of some other\napproaches for bias mitigation or to solve the problem. We introduce a\nmethodology for applying the proposed metrics during the post-processing stage\nand illustrate its practical application through an example. This example\ndemonstrates how analyzing the proportionality of the debiasing strategy\ncomplements traditional fairness metrics, providing a deeper perspective to\nensure fairer outcomes across all groups.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u540e\u5904\u7406\u6280\u672f\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7528\u4e8e\u53bb\u504f\u7684\u516c\u5e73\u6027\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u7ffb\u8f6c\u5dee\u5f02\u7684\u6307\u6807\uff0c\u5e2e\u52a9\u8bc4\u4f30\u7b56\u7565\u7684\u5408\u7406\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u53ef\u80fd\u4ea7\u751f\u4e0d\u516c\u5e73\u7ed3\u679c\uff0c\u540e\u5904\u7406\u53bb\u504f\u6280\u672f\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u4e0d\u516c\u5e73\u6216\u52a0\u5267\u73b0\u6709\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u7ec4\u91cf\u5316\u540e\u5904\u7406\u9636\u6bb5\u7ffb\u8f6c\u5dee\u5f02\u7684\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u7528\u8fd9\u4e9b\u6307\u6807\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u6307\u6807\u80fd\u8bc4\u4f30\u53bb\u504f\u7b56\u7565\u7684\u5408\u7406\u6027\u3001\u900f\u660e\u6027\uff0c\u5e76\u6307\u5bfc\u5176\u4ed6\u53bb\u504f\u65b9\u6cd5\u7684\u9009\u62e9\u3002", "conclusion": "\u5206\u6790\u53bb\u504f\u7b56\u7565\u7684\u5408\u7406\u6027\u8865\u5145\u4e86\u4f20\u7edf\u516c\u5e73\u6027\u6307\u6807\uff0c\u6709\u52a9\u4e8e\u66f4\u516c\u5e73\u7684\u7ed3\u679c\u3002", "relevance": 75.0}}
{"id": "2505.17272", "pdf": "https://arxiv.org/pdf/2505.17272", "abs": "https://arxiv.org/abs/2505.17272", "authors": ["Mingyu Yang", "Mehdi Rezagholizadeh", "Guihong Li", "Vikram Appia", "Emad Barsoum"], "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance.", "AI": {"tldr": "Zebra-Llama\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ec4\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u548c\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u5c42\u6784\u5efa\u9ad8\u6548\u6df7\u5408\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u5e76\u51cf\u5c11\u4e86KV\u7f13\u5b58\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86Transformer\u7ea7\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u90e8\u7f72\u9700\u6c42\u7684\u589e\u52a0\uff0c\u63d0\u5347\u5176\u63a8\u7406\u6548\u7387\u5bf9\u4e8e\u53ef\u6301\u7eed\u548c\u6c11\u4e3b\u5316\u8bbf\u95ee\u81f3\u5173\u91cd\u8981\u3002\u91cd\u65b0\u8bad\u7ec3LLMs\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u73af\u4fdd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "Zebra-Llama\u901a\u8fc7\u7ed3\u5408SSMs\u548cMLA\u5c42\uff0c\u91c7\u7528\u4f18\u5316\u7684\u521d\u59cb\u5316\u548c\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ece\u9884\u8bad\u7ec3\u7684Transformers\u4e2d\u9ad8\u6548\u8f6c\u79fb\u77e5\u8bc6\uff0c\u6784\u5efa\u4e861B\u30013B\u548c8B\u7684\u6df7\u5408\u6a21\u578b\u3002", "result": "Zebra-Llama\u5728\u4ec5\u4f7f\u75287-11B\u8bad\u7ec3\u6807\u8bb0\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86Transformer\u7ea7\u522b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5c06KV\u7f13\u5b58\u5927\u5c0f\u964d\u81f3\u539f\u59cb\u503c\u76843.9%-2.73%\uff0c\u5e76\u5728LM Harness\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e8697%-100%\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "Zebra-Llama\u5728\u63a8\u7406\u6548\u7387\u3001KV\u7f13\u5b58\u5927\u5c0f\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982MambaInLLaMA\u3001X-EcoMLA\u7b49\uff09\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 90.0}}
{"id": "2505.17063", "pdf": "https://arxiv.org/pdf/2505.17063", "abs": "https://arxiv.org/abs/2505.17063", "authors": ["Yiduo Guo", "Zhen Guo", "Chuanwei Huang", "Zi-Ang Wang", "Zekai Zhang", "Haofei Yu", "Huishuai Zhang", "Yikang Shen"], "title": "Synthetic Data RL: Task Definition Is All You Need", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSynthetic Data RL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u7684\u6a21\u578b\u5fae\u8c03\u3002", "method": "\u4ece\u4efb\u52a1\u5b9a\u4e49\u548c\u68c0\u7d22\u6587\u6863\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u6839\u636e\u6a21\u578b\u53ef\u89e3\u6027\u8c03\u6574\u95ee\u9898\u96be\u5ea6\uff0c\u5e76\u9009\u62e9\u5e73\u5747\u901a\u8fc7\u7387\u8f83\u9ad8\u7684\u95ee\u9898\u8fdb\u884cRL\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5982GSM8K\u63d0\u534729.2%\uff09\uff0c\u63a5\u8fd1\u4f7f\u7528\u5168\u4eba\u7c7b\u6570\u636e\u7684RL\u8868\u73b0\uff0c\u4e14\u4eba\u7c7b\u6570\u636e\u8865\u5145\u6548\u679c\u6709\u9650\u3002", "conclusion": "Synthetic Data RL\u901a\u8fc7\u51cf\u5c11\u4eba\u7c7b\u6570\u636e\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6a21\u578b\u9002\u5e94\u3002", "relevance": 85.0}}
{"id": "2505.17338", "pdf": "https://arxiv.org/pdf/2505.17338", "abs": "https://arxiv.org/abs/2505.17338", "authors": ["Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Anwesa Choudhuri", "Terrence Chen", "Ziyan Wu"], "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Volumetric rendering of Computed Tomography (CT) scans is crucial for\nvisualizing complex 3D anatomical structures in medical imaging. Current\nhigh-fidelity approaches, especially neural rendering techniques, require\ntime-consuming per-scene optimization, limiting clinical applicability due to\ncomputational demands and poor generalizability. We propose Render-FM, a novel\nfoundation model for direct, real-time volumetric rendering of CT scans.\nRender-FM employs an encoder-decoder architecture that directly regresses 6D\nGaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan\noptimization through large-scale pre-training on diverse medical data. By\nintegrating robust feature extraction with the expressive power of 6DGS, our\napproach efficiently generates high-quality, real-time interactive 3D\nvisualizations across diverse clinical CT data. Experiments demonstrate that\nRender-FM achieves visual fidelity comparable or superior to specialized\nper-scan methods while drastically reducing preparation time from nearly an\nhour to seconds for a single inference step. This advancement enables seamless\nintegration into real-time surgical planning and diagnostic workflows. The\nproject page is: https://gaozhongpai.github.io/renderfm/.", "AI": {"tldr": "Render-FM\u662f\u4e00\u79cd\u7528\u4e8eCT\u626b\u63cf\u5b9e\u65f6\u4f53\u79ef\u6e32\u67d3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6d88\u9664\u6bcf\u626b\u63cf\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u9ad8\u4fdd\u771f\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u9700\u8981\u8017\u65f6\u7684\u6bcf\u573a\u666f\u4f18\u5316\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u76f4\u63a5\u4eceCT\u4f53\u79ef\u56de\u5f526D\u9ad8\u65af\u6e85\u5c04\u53c2\u6570\u3002", "result": "Render-FM\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u5ab2\u7f8e\u6216\u4f18\u4e8e\u4e13\u7528\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06\u51c6\u5907\u65f6\u95f4\u4ece\u8fd1\u4e00\u5c0f\u65f6\u7f29\u77ed\u81f3\u79d2\u7ea7\u3002", "conclusion": "Render-FM\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\u5f0f3D\u53ef\u89c6\u5316\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002", "relevance": 30.0}}
{"id": "2505.17572", "pdf": "https://arxiv.org/pdf/2505.17572", "abs": "https://arxiv.org/abs/2505.17572", "authors": ["Siqi Lai", "Yansong Ning", "Zirui Yuan", "Zhixi Chen", "Hao Liu"], "title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown emerging potential in spatiotemporal\nreasoning, making them promising candidates for building urban agents that\nsupport diverse urban downstream applications. Despite these benefits, existing\nstudies primarily focus on evaluating urban LLM agent on outcome-level metrics\n(e.g., prediction accuracy, traffic efficiency), offering limited insight into\ntheir underlying reasoning processes. As a result, the strengths and\nlimitations of urban LLM agents in spatiotemporal reasoning remain poorly\nunderstood. To this end, we introduce USTBench, the first benchmark to evaluate\nLLMs' spatiotemporal reasoning abilities as urban agents across four decomposed\ndimensions: spatiotemporal understanding, forecasting, planning, and reflection\nwith feedback. Specifically, USTBench supports five diverse urban\ndecision-making and four spatiotemporal prediction tasks, all running within\nour constructed interactive city environment UAgentEnv. The benchmark includes\n62,466 structured QA pairs for process-level evaluation and standardized\nend-to-end task assessments, enabling fine-grained diagnostics and broad\ntask-level comparison across diverse urban scenarios. Through extensive\nevaluation of thirteen leading LLMs, we reveal that although LLMs show\npromising potential across various urban downstream tasks, they still struggle\nin long-horizon planning and reflective adaptation in dynamic urban contexts.\nNotably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on\ngeneral logic or mathematical problems do not consistently outperform\nnon-reasoning LLMs. This discrepancy highlights the need for domain-specialized\nadaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench\nprovides a foundation to build more adaptive and effective LLM-based urban\nagents and broad smart city applications.", "AI": {"tldr": "USTBench\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u5728\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u7406\u89e3\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u53cd\u9988\u56db\u4e2a\u7ef4\u5ea6\uff0c\u63ed\u793aLLM\u5728\u57ce\u5e02\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7ed3\u679c\u7ea7\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9LLM\u65f6\u7a7a\u63a8\u7406\u8fc7\u7a0b\u7684\u6df1\u5165\u7406\u89e3\uff0cUSTBench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165USTBench\u57fa\u51c6\uff0c\u5305\u542b62,466\u4e2a\u7ed3\u6784\u5316QA\u5bf9\u548c\u6807\u51c6\u5316\u4efb\u52a1\u8bc4\u4f30\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u8bca\u65ad\u548c\u4efb\u52a1\u7ea7\u6bd4\u8f83\u3002", "result": "LLM\u5728\u57ce\u5e02\u4efb\u52a1\u4e2d\u8868\u73b0\u6f5c\u529b\uff0c\u4f46\u5728\u957f\u65f6\u89c4\u5212\u548c\u52a8\u6001\u9002\u5e94\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u901a\u7528\u63a8\u7406\u6a21\u578b\u8868\u73b0\u4e0d\u4e00\u3002", "conclusion": "USTBench\u4e3a\u6784\u5efa\u66f4\u9002\u5e94\u7684LLM\u57ce\u5e02\u4ee3\u7406\u548c\u667a\u6167\u57ce\u5e02\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002", "relevance": 85.0}}
{"id": "2505.17277", "pdf": "https://arxiv.org/pdf/2505.17277", "abs": "https://arxiv.org/abs/2505.17277", "authors": ["Soumita Hait", "Ping Li", "Haipeng Luo", "Mengxiao Zhang"], "title": "Comparator-Adaptive $\u03a6$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games", "categories": ["cs.LG"], "comment": null, "summary": "In the classic expert problem, $\\Phi$-regret measures the gap between the\nlearner's total loss and that achieved by applying the best action\ntransformation $\\phi \\in \\Phi$. A recent work by Lu et al., [2025] introduces\nan adaptive algorithm whose regret against a comparator $\\phi$ depends on a\ncertain sparsity-based complexity measure of $\\phi$, (almost) recovering and\ninterpolating optimal bounds for standard regret notions such as external,\ninternal, and swap regret. In this work, we propose a general idea to achieve\nan even better comparator-adaptive $\\Phi$-regret bound via much simpler\nalgorithms compared to Lu et al., [2025]. Specifically, we discover a prior\ndistribution over all possible binary transformations and show that it suffices\nto achieve prior-dependent regret against these transformations. Then, we\npropose two concrete and efficient algorithms to achieve so, where the first\none learns over multiple copies of a prior-aware variant of the Kernelized MWU\nalgorithm of Farina et al., [2022], and the second one learns over multiple\ncopies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007].\nTo further showcase the power of our methods and the advantages over Lu et al.,\n[2025] besides the simplicity and better regret bounds, we also show that our\nsecond approach can be extended to the game setting to achieve accelerated and\nadaptive convergence rate to $\\Phi$-equilibria for a class of general-sum\ngames. When specified to the special case of correlated equilibria, our bound\nimproves over the existing ones from Anagnostides et al., [2022a,b]", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u9a8c\u5206\u5e03\u5b9e\u73b0\u66f4\u597d\u7684\u6bd4\u8f83\u5668\u81ea\u9002\u5e94\u03a6-\u9057\u61be\u754c\u9650\uff0c\u4f18\u4e8eLu\u7b49\u4eba\u7684\u590d\u6742\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7ecf\u5178\u4e13\u5bb6\u95ee\u9898\u4e2d\u03a6-\u9057\u61be\u7684\u4f18\u5316\uff0c\u63d0\u51fa\u66f4\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5177\u4f53\u7b97\u6cd5\uff1a1\uff09\u57fa\u4e8eKernelized MWU\u7684\u5148\u9a8c\u611f\u77e5\u53d8\u4f53\uff1b2\uff09\u57fa\u4e8eBM-reduction\u7684\u5148\u9a8c\u611f\u77e5\u53d8\u4f53\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6bd4\u8f83\u5668\u81ea\u9002\u5e94\u03a6-\u9057\u61be\u754c\u9650\uff0c\u5e76\u5728\u535a\u5f08\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u52a0\u901f\u548c\u81ea\u9002\u5e94\u6536\u655b\u7387\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u7b80\u5355\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17065", "pdf": "https://arxiv.org/pdf/2505.17065", "abs": "https://arxiv.org/abs/2505.17065", "authors": ["Valentina Carbonari", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in artificial intelligence, particularly large language\nmodels LLMs, have shown promising capabilities in transforming rare disease\nresearch. This survey paper explores the integration of LLMs in the analysis of\nrare diseases, highlighting significant strides and pivotal studies that\nleverage textual data to uncover insights and patterns critical for diagnosis,\ntreatment, and patient care. While current research predominantly employs\ntextual data, the potential for multimodal data integration combining genetic,\nimaging, and electronic health records stands as a promising frontier. We\nreview foundational papers that demonstrate the application of LLMs in\nidentifying and extracting relevant medical information, simulating intelligent\nconversational agents for patient interaction, and enabling the formulation of\naccurate and timely diagnoses. Furthermore, this paper discusses the challenges\nand ethical considerations inherent in deploying LLMs, including data privacy,\nmodel transparency, and the need for robust, inclusive data sets. As part of\nthis exploration, we present a section on experimentation that utilizes\nmultiple LLMs alongside structured questionnaires, specifically designed for\ndiagnostic purposes in the context of different diseases. We conclude with\nfuture perspectives on the evolution of LLMs towards truly multimodal\nplatforms, which would integrate diverse data types to provide a more\ncomprehensive understanding of rare diseases, ultimately fostering better\noutcomes in clinical settings.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63a2\u8ba8\u4e86LLMs\u5728\u7f55\u89c1\u75c5\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6587\u672c\u6570\u636e\u5206\u6790\u3001\u591a\u6a21\u6001\u6570\u636e\u6f5c\u529b\u3001\u4f26\u7406\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5229\u7528LLMs\u63d0\u5347\u7f55\u89c1\u75c5\u7814\u7a76\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u63a2\u7d22\u5176\u5728\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u60a3\u8005\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\uff0c\u5206\u6790LLMs\u5728\u533b\u5b66\u6587\u672c\u5904\u7406\u3001\u5bf9\u8bdd\u6a21\u62df\u548c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u7684\u5b9e\u9a8c\u3002", "result": "LLMs\u5728\u7f55\u89c1\u75c5\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u9690\u79c1\u3001\u900f\u660e\u6027\u548c\u5305\u5bb9\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765LLMs\u5e94\u53d1\u5c55\u4e3a\u591a\u6a21\u6001\u5e73\u53f0\uff0c\u6574\u5408\u591a\u79cd\u6570\u636e\u7c7b\u578b\u4ee5\u4f18\u5316\u4e34\u5e8a\u6548\u679c\u3002", "relevance": 60.0}}
{"id": "2505.17343", "pdf": "https://arxiv.org/pdf/2505.17343", "abs": "https://arxiv.org/abs/2505.17343", "authors": ["Dillon Lohr", "Michael J. Proulx", "Mehedi Hasan Raju", "Oleg V. Komogortsev"], "title": "Ocular Authentication: Fusion of Gaze and Periocular Modalities", "categories": ["cs.CV", "cs.HC"], "comment": "Supplementary material is available", "summary": "This paper investigates the feasibility of fusing two eye-centric\nauthentication modalities-eye movements and periocular images-within a\ncalibration-free authentication system. While each modality has independently\nshown promise for user authentication, their combination within a unified\ngaze-estimation pipeline has not been thoroughly explored at scale. In this\nreport, we propose a multimodal authentication system and evaluate it using a\nlarge-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)\nsignal quality equivalent to a consumer-facing virtual reality (VR) device. Our\nresults show that the multimodal approach consistently outperforms both\nunimodal systems across all scenarios, surpassing the FIDO benchmark. The\nintegration of a state-of-the-art machine learning architecture contributed\nsignificantly to the overall authentication performance at scale, driven by the\nmodel's ability to capture authentication representations and the complementary\ndiscriminative characteristics of the fused modalities.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u773c\u52a8\u548c\u773c\u5468\u56fe\u50cf\u4e24\u79cd\u8ba4\u8bc1\u6a21\u6001\u878d\u5408\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8ba4\u8bc1\u7cfb\u7edf\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u5355\u6a21\u6001\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u773c\u52a8\u548c\u773c\u5468\u56fe\u50cf\u4e24\u79cd\u72ec\u7acb\u8ba4\u8bc1\u6a21\u6001\u7684\u878d\u5408\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u8ba4\u8bc1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8ba4\u8bc1\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u5e76\u5728\u5305\u542b9202\u540d\u53d7\u8bd5\u8005\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u6240\u6709\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u5355\u6a21\u6001\u7cfb\u7edf\uff0c\u5e76\u8d85\u8d8a\u4e86FIDO\u57fa\u51c6\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u548c\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u8bc1\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.17607", "pdf": "https://arxiv.org/pdf/2505.17607", "abs": "https://arxiv.org/abs/2505.17607", "authors": ["Jo\u00e3o Pedro Gandarela", "Thiago Rios", "Stefan Menzel", "Andr\u00e9 Freitas"], "title": "Controlled Agentic Planning & Reasoning for Mechanism Synthesis", "categories": ["cs.AI", "cs.CL"], "comment": "24 pages, 16 figures", "summary": "This work presents a dual-agent Large Language Model (LLM)-based reasoning\nmethod for mechanism synthesis, capable of reasoning at both linguistic and\nsymbolic levels to generate geometrical and dynamic outcomes. The model\nconsists of a composition of well-defined functions that, starting from a\nnatural language specification, references abstract properties through\nsupporting equations, generates and parametrizes simulation code, and elicits\nfeedback anchor points using symbolic regression and distance functions. This\nprocess closes an actionable refinement loop at the linguistic and symbolic\nlayers. The approach is shown to be both effective and convergent in the\ncontext of planar mechanisms. Additionally, we introduce MSynth, a novel\nbenchmark for planar mechanism synthesis, and perform a comprehensive analysis\nof the impact of the model components. We further demonstrate that symbolic\nregression prompts unlock mechanistic insights only when applied to\nsufficiently large architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u4ee3\u7406LLM\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5236\u5408\u6210\uff0c\u7ed3\u5408\u8bed\u8a00\u548c\u7b26\u53f7\u5c42\u6b21\u751f\u6210\u51e0\u4f55\u548c\u52a8\u6001\u7ed3\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7LLM\u5728\u8bed\u8a00\u548c\u7b26\u53f7\u5c42\u6b21\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u673a\u5236\u5408\u6210\u7684\u81ea\u52a8\u5316\u548c\u4f18\u5316\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406LLM\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u7b26\u53f7\u56de\u5f52\u548c\u8ddd\u79bb\u51fd\u6570\uff0c\u751f\u6210\u4eff\u771f\u4ee3\u7801\u5e76\u4f18\u5316\u53cd\u9988\u3002", "result": "\u5728\u5e73\u9762\u673a\u5236\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6536\u655b\u6027\uff0c\u5e76\u5f00\u53d1\u4e86MSynth\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u7b26\u53f7\u56de\u5f52\u63d0\u793a\u4ec5\u5728\u5927\u89c4\u6a21\u67b6\u6784\u4e2d\u624d\u80fd\u89e3\u9501\u673a\u5236\u6027\u6d1e\u5bdf\u3002", "relevance": 70.0}}
{"id": "2505.17282", "pdf": "https://arxiv.org/pdf/2505.17282", "abs": "https://arxiv.org/abs/2505.17282", "authors": ["Diyuan Wu", "Aleksandr Shevchenko", "Samet Oymak", "Marco Mondelli"], "title": "Attention with Trained Embeddings Provably Selects Important Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Token embeddings play a crucial role in language modeling but, despite this\npractical relevance, their theoretical understanding remains limited. Our paper\naddresses the gap by characterizing the structure of embeddings obtained via\ngradient descent. Specifically, we consider a one-layer softmax attention model\nwith a linear head for binary classification, i.e., $\\texttt{Softmax}( p^\\top\nE_X^\\top ) E_X v = \\frac{ \\sum_{i=1}^T \\exp(p^\\top E_{x_i}) E_{x_i}^\\top\nv}{\\sum_{j=1}^T \\exp(p^\\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \\dots,\nE_{x_T} ]^\\top$ contains the embeddings of the input sequence, $p$ is the\nembedding of the $\\mathrm{\\langle cls \\rangle}$ token and $v$ the output\nvector. First, we show that, already after a single step of gradient training\nwith the logistic loss, the embeddings $E_X$ capture the importance of tokens\nin the dataset by aligning with the output vector $v$ proportionally to the\nfrequency with which the corresponding tokens appear in the dataset. Then,\nafter training $p$ via gradient flow until convergence, the softmax selects the\nimportant tokens in the sentence (i.e., those that are predictive of the\nlabel), and the resulting $\\mathrm{\\langle cls \\rangle}$ embedding maximizes\nthe margin for such a selection. Experiments on real-world datasets (IMDB,\nYelp) exhibit a phenomenology close to that unveiled by our theory.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u83b7\u5f97\u7684\u8bcd\u5d4c\u5165\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u5d4c\u5165\u5982\u4f55\u6355\u6349\u6570\u636e\u96c6\u4e2d\u6807\u8bb0\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u5316\u5206\u7c7b\u6807\u8bb0\u7684\u5d4c\u5165\u3002", "motivation": "\u5c3d\u7ba1\u8bcd\u5d4c\u5165\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u4e0b\u5d4c\u5165\u7684\u7ed3\u6784\u7279\u6027\u3002", "method": "\u91c7\u7528\u5355\u5c42softmax\u6ce8\u610f\u529b\u6a21\u578b\u548c\u7ebf\u6027\u5934\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff0c\u901a\u8fc7\u68af\u5ea6\u8bad\u7ec3\u548c\u68af\u5ea6\u6d41\u5206\u6790\u5d4c\u5165\u7684\u52a8\u6001\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u540e\u7684\u5d4c\u5165\u80fd\u6355\u6349\u6807\u8bb0\u5728\u6570\u636e\u96c6\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7softmax\u9009\u62e9\u9884\u6d4b\u6027\u6807\u8bb0\uff0c\u4f18\u5316\u5206\u7c7b\u6807\u8bb0\u7684\u5d4c\u5165\u4ee5\u6700\u5927\u5316\u8fb9\u754c\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u5f97\u5230\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8868\u660e\u5d4c\u5165\u7ed3\u6784\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982IMDB\u3001Yelp\uff09\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u73b0\u8c61\u3002", "relevance": 70.0}}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067", "abs": "https://arxiv.org/abs/2505.17067", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u548c\u591a\u56fe\u7247\u573a\u666f\u4e0b\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u548c\u591a\u56fe\u7247\u73af\u5883\u4e0b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u6269\u5c55\u4e86\u4ee5\u5f80\u4ec5\u9488\u5bf9\u82f1\u8bed\u5355\u56fe\u7247\u7684\u7814\u7a76\u8303\u56f4\u3002", "method": "1) \u4f7f\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8868\u5f81\uff1b2) \u5f15\u5165\u56fe\u50cf\u6a21\u6001\uff1b3) \u91c7\u7528\u4e13\u5bb6\u4e58\u79ef\u7b56\u7565\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u548c\u8fc7\u62df\u5408\u3002", "result": "UAR\u63d0\u53477.1%\uff0cF1\u5206\u6570\u63d0\u53472.9%\uff0c\u6587\u672c\u6a21\u6001\u7684\u5bf9\u6bd4\u5b66\u4e60\u6548\u679c\u4f18\u4e8e\u8bed\u97f3\u3002", "conclusion": "\u6846\u67b6\u5728\u591a\u8bed\u8a00\u548c\u591a\u56fe\u7247\u573a\u666f\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17352", "pdf": "https://arxiv.org/pdf/2505.17352", "abs": "https://arxiv.org/abs/2505.17352", "authors": ["Preeti Lamba", "Kiran Ravish", "Ankita Kushwaha", "Pawan Kumar"], "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have emerged as leading generative models for images and\nother modalities, but aligning their outputs with human preferences and safety\nconstraints remains a critical challenge. This thesis proposal investigates\nmethods to align diffusion models using reinforcement learning (RL) and reward\nmodeling. We survey recent advances in fine-tuning text-to-image diffusion\nmodels with human feedback, including reinforcement learning from human and AI\nfeedback, direct preference optimization, and differentiable reward approaches.\nWe classify these methods based on the type of feedback (human, automated,\nbinary or ranked preferences), the fine-tuning technique (policy gradient,\nreward-weighted likelihood, direct backpropagation, etc.), and their efficiency\nand safety outcomes. We compare key algorithms and frameworks, highlighting how\nthey improve alignment with user intent or safety standards, and discuss\ninter-relationships such as how newer methods build on or diverge from earlier\nones. Based on the survey, we identify five promising research directions for\nthe next two years: (1) multi-objective alignment with combined rewards, (2)\nefficient human feedback usage and active learning, (3) robust safety alignment\nagainst adversarial inputs, (4) continual and online alignment of diffusion\nmodels, and (5) interpretable and trustworthy reward modeling for generative\nimages. Each direction is elaborated with its problem statement, challenges,\nrelated work, and a proposed research plan. The proposal is organized as a\ncomprehensive document with literature review, comparative tables of methods,\nand detailed research plans, aiming to contribute new insights and techniques\nfor safer and value-aligned diffusion-based generative AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5956\u52b1\u5efa\u6a21\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\u4e0e\u4eba\u7c7b\u504f\u597d\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u63d0\u51fa\u4e86\u4e94\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u4e0e\u4eba\u7c7b\u504f\u597d\u548c\u5b89\u5168\u7ea6\u675f\u7684\u5bf9\u9f50\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08\u5982RLHF\uff09\u548c\u5956\u52b1\u5efa\u6a21\u6280\u672f\u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5206\u7c7b\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u591a\u76ee\u6807\u5bf9\u9f50\u3001\u9ad8\u6548\u4eba\u7c7b\u53cd\u9988\u4f7f\u7528\u3001\u9c81\u68d2\u5b89\u5168\u5bf9\u9f50\u7b49\u3002", "conclusion": "\u8bba\u6587\u65e8\u5728\u4e3a\u66f4\u5b89\u5168\u548c\u4ef7\u503c\u5bf9\u9f50\u7684\u6269\u6563\u751f\u6210AI\u63d0\u4f9b\u65b0\u89c1\u89e3\u548c\u6280\u672f\u3002", "relevance": 75.0}}
{"id": "2505.17609", "pdf": "https://arxiv.org/pdf/2505.17609", "abs": "https://arxiv.org/abs/2505.17609", "authors": ["Zixian Guo", "Ming Liu", "Zhilong Ji", "Jinfeng Bai", "Lei Zhang", "Wangmeng Zuo"], "title": "Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "Current large vision-language models (LVLMs) typically employ a connector\nmodule to link visual features with text embeddings of large language models\n(LLMs) and use end-to-end training to achieve multi-modal understanding in a\nunified process. Well alignment needs high-quality pre-training data and a\ncarefully designed training process. Current LVLMs face challenges when\naddressing complex vision-language reasoning tasks, with their reasoning\ncapabilities notably lagging behind those of LLMs. This paper proposes a\nparadigm shift: instead of training end-to-end vision-language reasoning\nmodels, we advocate for developing a decoupled reasoning framework based on\nexisting visual interpretation specialists and text-based reasoning LLMs. Our\napproach leverages (1) a dedicated vision-language model to transform the\nvisual content of images into textual descriptions and (2) an LLM to perform\nreasoning according to the visual-derived text and the original question. This\nmethod presents a cost-efficient solution for multi-modal model development by\noptimizing existing models to work collaboratively, avoiding end-to-end\ndevelopment of vision-language models from scratch. By transforming images into\nlanguage model-compatible text representations, it facilitates future low-cost\nand flexible upgrades to upcoming powerful LLMs. We introduce an\noutcome-rewarded joint-tuning strategy to optimize the cooperation between the\nvisual interpretation and linguistic reasoning model. Evaluation results on\nvision-language benchmarks demonstrate that the decoupled reasoning framework\noutperforms recent LVLMs. Our approach yields particularly significant\nperformance gains on visually intensive geometric mathematics problems. The\ncode is available: https://github.com/guozix/DVLR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6709\u89c6\u89c9\u89e3\u91ca\u6a21\u578b\u548cLLMs\u534f\u4f5c\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709LVLMs\u3002", "motivation": "\u5f53\u524dLVLMs\u5728\u590d\u6742\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u9700\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u8bbe\u8ba1\u3002", "method": "1) \u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5c06\u56fe\u50cf\u8f6c\u4e3a\u6587\u672c\u63cf\u8ff0\uff1b2) LLM\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u548c\u95ee\u9898\u8fdb\u884c\u63a8\u7406\uff1b3) \u5f15\u5165\u8054\u5408\u8c03\u4f18\u7b56\u7565\u4f18\u5316\u534f\u4f5c\u3002", "result": "\u5728\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LVLMs\uff0c\u5c24\u5176\u5728\u51e0\u4f55\u6570\u5b66\u95ee\u9898\u4e0a\u663e\u8457\u3002", "conclusion": "\u89e3\u8026\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17293", "pdf": "https://arxiv.org/pdf/2505.17293", "abs": "https://arxiv.org/abs/2505.17293", "authors": ["Ting-Wei Li", "Ruizhong Qiu", "Hanghang Tong"], "title": "Model-Free Graph Data Selection under Distribution Shift", "categories": ["cs.LG"], "comment": null, "summary": "Graph domain adaptation (GDA) is a fundamental task in graph machine\nlearning, with techniques like shift-robust graph neural networks (GNNs) and\nspecialized training procedures to tackle the distribution shift problem.\nAlthough these model-centric approaches show promising results, they often\nstruggle with severe shifts and constrained computational resources. To address\nthese challenges, we propose a novel model-free framework, GRADATE (GRAph DATa\nsElector), that selects the best training data from the source domain for the\nclassification task on the target domain. GRADATE picks training samples\nwithout relying on any GNN model's predictions or training recipes, leveraging\noptimal transport theory to capture and adapt to distribution changes. GRADATE\nis data-efficient, scalable and meanwhile complements existing model-centric\nGDA approaches. Through comprehensive empirical studies on several real-world\ngraph-level datasets and multiple covariate shift types, we demonstrate that\nGRADATE outperforms existing selection methods and enhances off-the-shelf GDA\nmethods with much fewer training data.", "AI": {"tldr": "GRADATE\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u56fe\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u89e3\u51b3\u56fe\u57df\u9002\u5e94\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u6570\u636e\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u57df\u9002\u5e94\u65b9\u6cd5\uff08\u5982GNN\uff09\u5728\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGRADATE\u6846\u67b6\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u9009\u62e9\u6e90\u57df\u4e2d\u6700\u9002\u5408\u76ee\u6807\u57df\u5206\u7c7b\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4e0d\u4f9d\u8d56GNN\u6a21\u578b\u9884\u6d4b\u6216\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u548c\u534f\u53d8\u91cf\u504f\u79fb\u7c7b\u578b\u4e0a\uff0cGRADATE\u4f18\u4e8e\u73b0\u6709\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u73b0\u6709GDA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "GRADATE\u4e3a\u56fe\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u9009\u62e9\u65b9\u6848\uff0c\u8865\u5145\u4e86\u73b0\u6709\u6a21\u578b\u4e2d\u5fc3\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.17068", "pdf": "https://arxiv.org/pdf/2505.17068", "abs": "https://arxiv.org/abs/2505.17068", "authors": ["Jorge Paz-Ruza", "Amparo Alonso-Betanzos", "Bertha Guijarro-Berdi\u00f1as", "Carlos Eiras-Franco"], "title": "Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": "IJCNN 2025", "summary": "In health-related topics, user toxicity in online discussions frequently\nbecomes a source of social conflict or promotion of dangerous, unscientific\nbehaviour; common approaches for battling it include different forms of\ndetection, flagging and/or removal of existing toxic comments, which is often\ncounterproductive for platforms and users alike. In this work, we propose the\nalternative of combatting user toxicity predictively, anticipating where a user\ncould interact toxically in health-related online discussions. Applying a\nCollaborative Filtering-based Machine Learning methodology, we predict the\ntoxicity in COVID-related conversations between any user and subcommunity of\nReddit, surpassing 80% predictive performance in relevant metrics, and allowing\nus to prevent the pairing of conflicting users and subcommunities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u540c\u8fc7\u6ee4\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7528\u6237\u5728\u5065\u5eb7\u76f8\u5173\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u6bd2\u6027\u884c\u4e3a\uff0c\u907f\u514d\u51b2\u7a81\u3002", "motivation": "\u5728\u7ebf\u5065\u5eb7\u8ba8\u8bba\u4e2d\u7684\u7528\u6237\u6bd2\u6027\u884c\u4e3a\u5e38\u5f15\u53d1\u793e\u4f1a\u51b2\u7a81\u6216\u4f20\u64ad\u5371\u9669\u884c\u4e3a\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u68c0\u6d4b\u548c\u5220\u9664\uff09\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u534f\u540c\u8fc7\u6ee4\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9884\u6d4b\u7528\u6237\u5728Reddit\u4e0aCOVID\u76f8\u5173\u8ba8\u8bba\u4e2d\u7684\u6bd2\u6027\u884c\u4e3a\u3002", "result": "\u9884\u6d4b\u6027\u80fd\u8d85\u8fc780%\uff0c\u80fd\u6709\u6548\u907f\u514d\u7528\u6237\u4e0e\u5b50\u793e\u533a\u7684\u51b2\u7a81\u914d\u5bf9\u3002", "conclusion": "\u9884\u6d4b\u6027\u65b9\u6cd5\u6bd4\u4f20\u7edf\u53cd\u5e94\u6027\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u53ef\u51cf\u5c11\u6bd2\u6027\u884c\u4e3a\u3002", "relevance": 40.0}}
{"id": "2505.17353", "pdf": "https://arxiv.org/pdf/2505.17353", "abs": "https://arxiv.org/abs/2505.17353", "authors": ["Minseo Kim", "Axel Levy", "Gordon Wetzstein"], "title": "Dual Ascent Diffusion for Inverse Problems", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "23 pages, 15 figures, 5 tables", "summary": "Ill-posed inverse problems are fundamental in many domains, ranging from\nastrophysics to medical imaging. Emerging diffusion models provide a powerful\nprior for solving these problems. Existing maximum-a-posteriori (MAP) or\nposterior sampling approaches, however, rely on different computational\napproximations, leading to inaccurate or suboptimal samples. To address this\nissue, we introduce a new approach to solving MAP problems with diffusion model\npriors using a dual ascent optimization framework. Our framework achieves\nbetter image quality as measured by various metrics for image restoration\nproblems, it is more robust to high levels of measurement noise, it is faster,\nand it estimates solutions that represent the observations more faithfully than\nthe state of the art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5148\u9a8c\u7684\u53cc\u4e0a\u5347\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9006\u95ee\u9898\u4e2d\u7684MAP\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3001\u9c81\u68d2\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u8fd1\u4f3c\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u51c6\u786e\u6216\u6b21\u4f18\uff0c\u9700\u8981\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u4e0a\u5347\u4f18\u5316\u6846\u67b6\u7ed3\u5408\u6269\u6563\u6a21\u578b\u5148\u9a8c\uff0c\u89e3\u51b3MAP\u95ee\u9898\u3002", "result": "\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5bf9\u9ad8\u566a\u58f0\u66f4\u9c81\u68d2\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u89e3\u66f4\u5fe0\u5b9e\u4e8e\u89c2\u6d4b\u3002", "conclusion": "\u65b0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u9006\u95ee\u9898\u7684\u6c42\u89e3\u6548\u679c\u3002", "relevance": 30.0}}
{"id": "2505.17613", "pdf": "https://arxiv.org/pdf/2505.17613", "abs": "https://arxiv.org/abs/2505.17613", "authors": ["Jihan Yao", "Yushi Hu", "Yujie Yi", "Bin Han", "Shangbin Feng", "Guang Yang", "Bingbing Wen", "Ranjay Krishna", "Lucy Lu Wang", "Yulia Tsvetkov", "Noah A. Smith", "Banghua Zhu"], "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research.", "AI": {"tldr": "MMMG\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u751f\u6210\u57fa\u51c6\uff0c\u6db5\u76d64\u79cd\u6a21\u6001\u7ec4\u5408\u548c49\u4e2a\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u9a8c\u8bc1\u663e\u793a\u5176\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0894.3%\uff09\u3002\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u97f3\u9891\u751f\u6210\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e0d\u4e00\u81f4\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u53ef\u9760\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f00\u53d1MMMG\u57fa\u51c6\uff0c\u6db5\u76d64\u79cd\u6a21\u6001\u7ec4\u5408\u548c49\u4e2a\u4efb\u52a1\uff0c\u7ed3\u5408\u6a21\u578b\u548c\u7a0b\u5e8f\u5b9e\u73b0\u81ea\u52a8\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7937\u6761\u6307\u4ee4\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "MMMG\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0894.3%\uff09\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aGPT Image\u5728\u56fe\u50cf\u751f\u6210\u4e0a\u8868\u73b0\u6700\u4f73\uff0878.3%\uff09\uff0c\u4f46\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u97f3\u9891\u751f\u6210\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "MMMG\u4e3a\u591a\u6a21\u6001\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u97f3\u9891\u751f\u6210\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "relevance": 60.0}}
{"id": "2505.17304", "pdf": "https://arxiv.org/pdf/2505.17304", "abs": "https://arxiv.org/abs/2505.17304", "authors": ["Jianhao Ma", "Geyu Liang", "Salar Fattahi"], "title": "Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Implicit regularization refers to the phenomenon where local search\nalgorithms converge to low-dimensional solutions, even when such structures are\nneither explicitly specified nor encoded in the optimization problem. While\nwidely observed, this phenomenon remains theoretically underexplored,\nparticularly in modern over-parameterized problems. In this paper, we study the\nconditions that enable implicit regularization by investigating when\ngradient-based methods converge to second-order stationary points (SOSPs)\nwithin an implicit low-dimensional region of a smooth, possibly nonconvex\nfunction. We show that successful implicit regularization hinges on two key\nconditions: $(i)$ the ability to efficiently escape strict saddle points, while\n$(ii)$ maintaining proximity to the implicit region. Existing analyses enabling\nthe convergence of gradient descent (GD) to SOSPs often rely on injecting large\nperturbations to escape strict saddle points. However, this comes at the cost\nof deviating from the implicit region. The central premise of this paper is\nthat it is possible to achieve the best of both worlds: efficiently escaping\nstrict saddle points using infinitesimal perturbations, while controlling\ndeviation from the implicit region via a small deviation rate. We show that\ninfinitesimally perturbed gradient descent (IPGD), which can be interpreted as\nGD with inherent ``round-off errors'', can provably satisfy both conditions. We\napply our framework to the problem of over-parameterized matrix sensing, where\nwe establish formal guarantees for the implicit regularization behavior of\nIPGD. We further demonstrate through extensive experiments that these insights\nextend to a broader class of learning problems.", "AI": {"tldr": "\u7814\u7a76\u9690\u5f0f\u6b63\u5219\u5316\u7684\u6761\u4ef6\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5fae\u5c0f\u6270\u52a8\u68af\u5ea6\u4e0b\u964d\uff08IPGD\uff09\u540c\u65f6\u5b9e\u73b0\u9003\u79bb\u978d\u70b9\u548c\u4fdd\u6301\u4f4e\u7ef4\u533a\u57df\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u8fc7\u53c2\u6570\u5316\u77e9\u9635\u611f\u77e5\u95ee\u9898\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9690\u5f0f\u6b63\u5219\u5316\u73b0\u8c61\u5728\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u5176\u7406\u8bba\u673a\u5236\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u73b0\u4ee3\u8fc7\u53c2\u6570\u5316\u95ee\u9898\u4e2d\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u68af\u5ea6\u65b9\u6cd5\u5982\u4f55\u6536\u655b\u5230\u9690\u5f0f\u4f4e\u7ef4\u533a\u57df\u5185\u7684\u4e8c\u9636\u7a33\u5b9a\u70b9\uff08SOSPs\uff09\u3002", "method": "\u63d0\u51fa\u5fae\u5c0f\u6270\u52a8\u68af\u5ea6\u4e0b\u964d\uff08IPGD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u5fae\u5c0f\u6270\u52a8\u5b9e\u73b0\u9ad8\u6548\u9003\u79bb\u978d\u70b9\u5e76\u4fdd\u6301\u4f4e\u7ef4\u533a\u57df\u3002\u7406\u8bba\u5206\u6790\u7ed3\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "IPGD\u5728\u8fc7\u53c2\u6570\u5316\u77e9\u9635\u611f\u77e5\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u9690\u5f0f\u6b63\u5219\u5316\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "IPGD\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u9003\u79bb\u978d\u70b9\u548c\u4fdd\u6301\u4f4e\u7ef4\u533a\u57df\u7684\u6761\u4ef6\uff0c\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17070", "pdf": "https://arxiv.org/pdf/2505.17070", "abs": "https://arxiv.org/abs/2505.17070", "authors": ["Anandh C", "Karthik Pandia Durai", "Jeena Prakash", "Manickavela Arumugam", "Kadri Hacioglu", "S. Pavankumar Dubagunta", "Andreas Stolcke", "Shankar Venkatesan", "Aravind Ganapathiraju"], "title": "Improving endpoint detection in end-to-end streaming ASR for conversational speech", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2024", "summary": "ASR endpointing (EP) plays a major role in delivering a good user experience\nin products supporting human or artificial agents in human-human/machine\nconversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR\nmodelling technique preferred for streaming. A major limitation of T-ASR is\ndelayed emission of ASR outputs, which could lead to errors or delays in EP.\nInaccurate EP will cut the user off while speaking, returning incomplete\ntranscript while delays in EP will increase the perceived latency, degrading\nthe user experience. We propose methods to improve EP by addressing delayed\nemission along with EP mistakes. To address the delayed emission problem, we\nintroduce an end-of-word token at the end of each word, along with a delay\npenalty. The EP delay is addressed by obtaining a reliable frame-level speech\nactivity detection using an auxiliary network. We apply the proposed methods on\nSwitchboard conversational speech corpus and evaluate it against a delay\npenalty method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7aef\u70b9\u68c0\u6d4b\uff08EP\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8bcd\u5c3e\u6807\u8bb0\u548c\u5ef6\u8fdf\u60e9\u7f5a\u6765\u89e3\u51b3\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u5229\u7528\u8f85\u52a9\u7f51\u7edc\u63d0\u9ad8\u7aef\u70b9\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08T-ASR\uff09\u4e2d\uff0c\u5ef6\u8fdf\u8f93\u51fa\u548c\u7aef\u70b9\u68c0\u6d4b\u9519\u8bef\u4f1a\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u7aef\u70b9\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u548c\u9519\u8bef\u3002", "method": "1. \u5f15\u5165\u8bcd\u5c3e\u6807\u8bb0\u548c\u5ef6\u8fdf\u60e9\u7f5a\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u8f93\u51fa\uff1b2. \u4f7f\u7528\u8f85\u52a9\u7f51\u7edc\u5b9e\u73b0\u5e27\u7ea7\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u4ee5\u63d0\u9ad8\u7aef\u70b9\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728Switchboard\u5bf9\u8bdd\u8bed\u97f3\u8bed\u6599\u5e93\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u5ef6\u8fdf\u60e9\u7f5a\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11ASR\u8f93\u51fa\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u7aef\u70b9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "relevance": 40.0}}
{"id": "2505.17358", "pdf": "https://arxiv.org/pdf/2505.17358", "abs": "https://arxiv.org/abs/2505.17358", "authors": ["Chinmay Talegaonkar", "Nikhil Gandudi Suresh", "Zachary Novack", "Yash Belhe", "Priyanka Nagasamudra", "Nicholas Antipa"], "title": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues", "categories": ["cs.CV"], "comment": null, "summary": "Recent monocular metric depth estimation (MMDE) methods have made notable\nprogress towards zero-shot generalization. However, they still exhibit a\nsignificant performance drop on out-of-distribution datasets. We address this\nlimitation by injecting defocus blur cues at inference time into Marigold, a\n\\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular\ndepth estimation (MDE). Our method effectively turns Marigold into a metric\ndepth predictor in a training-free manner. To incorporate defocus cues, we\ncapture two images with a small and a large aperture from the same viewpoint.\nTo recover metric depth, we then optimize the metric depth scaling parameters\nand the noise latents of Marigold at inference time using gradients from a loss\nfunction based on the defocus-blur image formation model. We compare our method\nagainst existing state-of-the-art zero-shot MMDE methods on a self-collected\nreal dataset, showing quantitative and qualitative improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u6563\u7126\u6a21\u7cca\u7ebf\u7d22\u7684\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578bMarigold\u8f6c\u5316\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u5ea6\u91cf\u6df1\u5ea6\u9884\u6d4b\u5668\uff0c\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u6563\u7126\u6a21\u7cca\u7ebf\u7d22\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u540c\u4e00\u89c6\u89d2\u4e0b\u62cd\u6444\u5c0f\u5149\u5708\u548c\u5927\u5149\u5708\u7684\u4e24\u5f20\u56fe\u50cf\uff0c\u5229\u7528\u6563\u7126\u6a21\u7cca\u56fe\u50cf\u5f62\u6210\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316Marigold\u7684\u5ea6\u91cf\u6df1\u5ea6\u7f29\u653e\u53c2\u6570\u548c\u566a\u58f0\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u5728\u81ea\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u65f6\u5f15\u5165\u6563\u7126\u6a21\u7cca\u7ebf\u7d22\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "relevance": 30.0}}
{"id": "2505.17650", "pdf": "https://arxiv.org/pdf/2505.17650", "abs": "https://arxiv.org/abs/2505.17650", "authors": ["Chengda Lu", "Xiaoyu Fan", "Yu Huang", "Rongwu Xu", "Jijie Li", "Wei Xu"], "title": "Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?", "categories": ["cs.AI"], "comment": null, "summary": "Jailbreak attacks have been observed to largely fail against recent reasoning\nmodels enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying\nmechanism remains underexplored, and relying solely on reasoning capacity may\nraise security concerns. In this paper, we try to answer the question: Does CoT\nreasoning really reduce harmfulness from jailbreaking? Through rigorous\ntheoretical analysis, we demonstrate that CoT reasoning has dual effects on\njailbreaking harmfulness. Based on the theoretical insights, we propose a novel\njailbreak method, FicDetail, whose practical performance validates our\ntheoretical findings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Chain-of-Thought (CoT) \u63a8\u7406\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5177\u6709\u53cc\u91cd\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d8a\u72f1\u65b9\u6cd5FicDetail\u3002", "motivation": "\u7814\u7a76CoT\u63a8\u7406\u662f\u5426\u771f\u7684\u80fd\u51cf\u5c11\u8d8a\u72f1\u653b\u51fb\u7684\u5371\u5bb3\u6027\uff0c\u586b\u8865\u4e86\u76f8\u5173\u673a\u5236\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86CoT\u63a8\u7406\u7684\u53cc\u91cd\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8d8a\u72f1\u65b9\u6cd5FicDetail\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eCoT\u63a8\u7406\u5bf9\u8d8a\u72f1\u653b\u51fb\u6709\u53cc\u91cd\u5f71\u54cd\uff0cFicDetail\u65b9\u6cd5\u7684\u5b9e\u8df5\u8868\u73b0\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "conclusion": "CoT\u63a8\u7406\u5e76\u975e\u5b8c\u5168\u5b89\u5168\uff0c\u4f9d\u8d56\u5176\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u9690\u60a3\u3002", "relevance": 75.0}}
{"id": "2505.17307", "pdf": "https://arxiv.org/pdf/2505.17307", "abs": "https://arxiv.org/abs/2505.17307", "authors": ["Pu Yang", "J. A. Barria"], "title": "Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a Wavelet Probabilistic Recurrent Convolutional Network\n(WPRCN) for Multivariate Time Series Classification (MTSC), especially\neffective in handling non-stationary environments, data scarcity and noise\nperturbations. We introduce a versatile wavelet probabilistic module designed\nto extract and analyse the probabilistic features, which can seamlessly\nintegrate with a variety of neural network architectures. This probabilistic\nmodule comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and\na Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN).\nSuch formulation extends the application of wavelet probabilistic neural\nnetworks to deep neural networks for MTSC. The AWPG constructs an ensemble\nprobabilistic model addressing different data scarcities and non-stationarity;\nit adaptively selects the optimal ones and generates probabilistic features for\nAPTCN. The APTCN analyses the correlations of the features and forms a\ncomprehensive feature space with existing MTSC models for classification. Here,\nwe instantiate the proposed module to work in parallel with a Long Short-Term\nMemory (LSTM) network and a Causal Fully Convolutional Network (C-FCN),\ndemonstrating its broad applicability in time series analysis. The WPRCN is\nevaluated on 30 diverse MTS datasets and outperforms all the benchmark\nalgorithms on average accuracy and rank, exhibiting pronounced strength in\nhandling scarce data and physiological data subject to perturbations and\nnon-stationarities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u6982\u7387\u5faa\u73af\u5377\u79ef\u7f51\u7edc\uff08WPRCN\uff09\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u5e73\u7a33\u73af\u5883\u3001\u6570\u636e\u7a00\u7f3a\u548c\u566a\u58f0\u5e72\u6270\u3002", "motivation": "\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u975e\u5e73\u7a33\u6027\u3001\u6570\u636e\u7a00\u7f3a\u548c\u566a\u58f0\u5e72\u6270\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u5c0f\u6ce2\u6982\u7387\u7279\u5f81\u751f\u6210\u5668\uff08AWPG\uff09\u548c\u901a\u9053\u6ce8\u610f\u529b\u6982\u7387\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\uff08APTCN\uff09\uff0c\u5e76\u4e0eLSTM\u548cC-FCN\u5e76\u884c\u5de5\u4f5c\u3002", "result": "\u572830\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u548c\u751f\u7406\u6570\u636e\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "WPRCN\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "relevance": 20.0}}
{"id": "2505.17071", "pdf": "https://arxiv.org/pdf/2505.17071", "abs": "https://arxiv.org/abs/2505.17071", "authors": ["Rapha\u00ebl Sarfati", "Haley Moller", "Toni J. B. Liu", "Nicolas Boull\u00e9", "Christopher Earls"], "title": "What's in a prompt? Language models encode literary style in prompt embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Large language models use high-dimensional latent spaces to encode and\nprocess textual information. Much work has investigated how the conceptual\ncontent of words translates into geometrical relationships between their vector\nrepresentations. Fewer studies analyze how the cumulative information of an\nentire prompt becomes condensed into individual embeddings under the action of\ntransformer layers. We use literary pieces to show that information about\nintangible, rather than factual, aspects of the prompt are contained in deep\nrepresentations. We observe that short excerpts (10 - 100 tokens) from\ndifferent novels separate in the latent space independently from what\nnext-token prediction they converge towards. Ensembles from books from the same\nauthors are much more entangled than across authors, suggesting that embeddings\nencode stylistic features. This geometry of style may have applications for\nauthorship attribution and literary analysis, but most importantly reveals the\nsophistication of information processing and compression accomplished by\nlanguage models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u63d0\u793a\u7684\u7d2f\u79ef\u4fe1\u606f\u538b\u7f29\u5230\u5d4c\u5165\u4e2d\uff0c\u53d1\u73b0\u6df1\u5c42\u8868\u793a\u5305\u542b\u975e\u4e8b\u5b9e\u6027\uff08\u5982\u98ce\u683c\uff09\u4fe1\u606f\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u63d0\u793a\u7684\u6574\u4f53\u4fe1\u606f\uff08\u5c24\u5176\u662f\u975e\u4e8b\u5b9e\u6027\u5185\u5bb9\uff09\u538b\u7f29\u5230\u5d4c\u5165\u4e2d\u3002", "method": "\u4f7f\u7528\u6587\u5b66\u4f5c\u54c1\u5206\u6790\u77ed\u6458\u5f55\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\uff0c\u89c2\u5bdf\u98ce\u683c\u7279\u5f81\u7684\u7f16\u7801\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u4f5c\u8005\u7684\u4e66\u7c4d\u6458\u5f55\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5206\u79bb\uff0c\u800c\u540c\u4e00\u4f5c\u8005\u7684\u6458\u5f55\u66f4\u7ea0\u7f20\uff0c\u8868\u660e\u5d4c\u5165\u7f16\u7801\u4e86\u98ce\u683c\u7279\u5f81\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u5904\u7406\u548c\u538b\u7f29\u80fd\u529b\u975e\u5e38\u590d\u6742\uff0c\u98ce\u683c\u51e0\u4f55\u53ef\u7528\u4e8e\u4f5c\u8005\u5f52\u5c5e\u548c\u6587\u5b66\u5206\u6790\u3002", "relevance": 85.0}}
{"id": "2505.17363", "pdf": "https://arxiv.org/pdf/2505.17363", "abs": "https://arxiv.org/abs/2505.17363", "authors": ["Hassan Wasswa", "Hussein Abbass", "Timothy Lynar"], "title": "Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches", "categories": ["cs.CV"], "comment": null, "summary": "Due to the exponential rise in IoT-based botnet attacks, researchers have\nexplored various advanced techniques for both dimensionality reduction and\nattack detection to enhance IoT security. Among these, Variational Autoencoders\n(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including\nGraph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have\ngarnered significant research attention in the domain of attack detection. This\nstudy evaluates the effectiveness of four state-of-the-art deep learning\narchitectures for IoT botnet detection: a VAE encoder with a Multi-Layer\nPerceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT\nencoder with an MLP. The evaluation is conducted on a widely studied IoT\nbenchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.\nFor the binary classification task, all models achieved over 99.93% in\naccuracy, recall, precision, and F1-score, with no notable differences in\nperformance. In contrast, for the multiclass classification task, GNN-based\nmodels showed significantly lower performance compared to VAE-MLP and ViT-MLP,\nwith accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,\nVAE-MLP, and ViT-MLP, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728IoT\u50f5\u5c38\u7f51\u7edc\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u5305\u62ecVAE\u4e0eMLP\u3001VAE\u4e0eGCN\u3001VAE\u4e0eGAT\u4ee5\u53caViT\u4e0eMLP\u7684\u7ec4\u5408\u3002\u5728\u4e8c\u8fdb\u5236\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff08>99.93%\uff09\uff0c\u4f46\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGNN\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5e94\u5bf9IoT\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\u7684\u5feb\u901f\u589e\u957f\uff0c\u63a2\u7d22\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08VAE-MLP\u3001VAE-GCN\u3001VAE-GAT\u3001ViT-MLP\uff09\u5728N-BaIoT\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u4e8c\u8fdb\u5236\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u4e8c\u8fdb\u5236\u5206\u7c7b\u4efb\u52a1\u4e2d\u6240\u6709\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff08>99.93%\uff09\uff0c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2dGNN\u6a21\u578b\u8868\u73b0\u8f83\u5dee\uff08VAE-GCN 86.42%\uff0cVAE-GAT 89.46%\uff09\u3002", "conclusion": "VAE-MLP\u548cViT-MLP\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0cGNN\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u6548\u679c\u8f83\u5dee\u3002", "relevance": 30.0}}
{"id": "2505.17653", "pdf": "https://arxiv.org/pdf/2505.17653", "abs": "https://arxiv.org/abs/2505.17653", "authors": ["Shixian Luo", "Zezhou Zhu", "Yu Yuan", "Yuncheng Yang", "Lianlei Shan", "Yong Wu"], "title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs", "categories": ["cs.AI"], "comment": "23 pages, 13 figures", "summary": "Geometric spatial reasoning forms the foundation of many applications in\nartificial intelligence, yet the ability of large language models (LLMs) to\noperate over geometric spatial information expressed in procedural code remains\nunderexplored. In this paper, we address this gap by formalizing the\nProgram-to-Geometry task, which challenges models to translate programmatic\ndrawing code into accurate and abstract geometric reasoning. To evaluate this\ncapability, we present GeoGramBench, a benchmark of 500 carefully refined\nproblems organized by a tailored three-level taxonomy that considers geometric\ncomplexity rather than traditional mathematical reasoning complexity. Our\ncomprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced\ndeficiencies: even the most advanced models achieve less than 50% accuracy at\nthe highest abstraction level. These results highlight the unique challenges\nposed by program-driven spatial reasoning and establish GeoGramBench as a\nvaluable resource for advancing research in symbolic-to-spatial geometric\nreasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51e0\u4f55\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86Program-to-Geometry\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86GeoGramBench\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5728\u6700\u9ad8\u62bd\u8c61\u7ea7\u522b\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4e0d\u8db350%\u3002", "motivation": "\u51e0\u4f55\u7a7a\u95f4\u63a8\u7406\u662fAI\u5e94\u7528\u7684\u57fa\u7840\uff0c\u4f46LLMs\u5728\u7a0b\u5e8f\u5316\u4ee3\u7801\u8868\u8fbe\u7684\u51e0\u4f55\u7a7a\u95f4\u4fe1\u606f\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7Formalizing Program-to-Geometry\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1GeoGramBench\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b500\u4e2a\u95ee\u9898\uff09\uff0c\u8bc4\u4f3017\u79cd\u524d\u6cbfLLMs\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u6700\u9ad8\u62bd\u8c61\u7ea7\u522b\u4e0a\u7684\u51c6\u786e\u7387\u5747\u4f4e\u4e8e50%\uff0c\u63ed\u793a\u4e86\u7a0b\u5e8f\u9a71\u52a8\u7a7a\u95f4\u63a8\u7406\u7684\u72ec\u7279\u6311\u6218\u3002", "conclusion": "GeoGramBench\u4e3a\u7b26\u53f7\u5230\u7a7a\u95f4\u51e0\u4f55\u63a8\u7406\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "relevance": 70.0}}
{"id": "2505.17331", "pdf": "https://arxiv.org/pdf/2505.17331", "abs": "https://arxiv.org/abs/2505.17331", "authors": ["Maryam Dialameh", "Rezaul Karim", "Hossein Rajabzadeh", "Omar Mohamed Awad", "Hyock Ju Kwon", "Boxing Chen", "Walid Ahmed", "Yang Liu"], "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.", "AI": {"tldr": "ECHO-LLaMA\u662f\u4e00\u79cd\u9ad8\u6548\u7684LLaMA\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eabKV\u7f13\u5b58\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534777%\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53477%\u3002", "motivation": "\u6539\u8fdbLLaMA\u67b6\u6784\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8bed\u8a00\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5171\u4eabKV\u7f13\u5b58\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5f15\u5165\u8ba1\u7b97\u9ad8\u6548\u7684\u9002\u5e94\u673a\u5236\u3002", "result": "\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534777%\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53477%\uff0c\u6a21\u578bFLOPs\u5229\u7528\u7387\u63d0\u534716%\uff0c\u635f\u5931\u964d\u4f4e14%\u3002", "conclusion": "ECHO-LLaMA\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17073", "pdf": "https://arxiv.org/pdf/2505.17073", "abs": "https://arxiv.org/abs/2505.17073", "authors": ["Anurag Mishra"], "title": "Mechanistic Interpretability of GPT-like Models on Summarization Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages (6 content + 2 references/appendix), 6 figures, 2 tables;\n  under review for the ACL 2025 Student Research Workshop", "summary": "Mechanistic interpretability research seeks to reveal the inner workings of\nlarge language models, yet most work focuses on classification or generative\ntasks rather than summarization. This paper presents an interpretability\nframework for analyzing how GPT-like models adapt to summarization tasks. We\nconduct differential analysis between pre-trained and fine-tuned models,\nquantifying changes in attention patterns and internal activations. By\nidentifying specific layers and attention heads that undergo significant\ntransformation, we locate the \"summarization circuit\" within the model\narchitecture. Our findings reveal that middle layers (particularly 2, 3, and 5)\nexhibit the most dramatic changes, with 62% of attention heads showing\ndecreased entropy, indicating a shift toward focused information selection. We\ndemonstrate that targeted LoRA adaptation of these identified circuits achieves\nsignificant performance improvement over standard LoRA fine-tuning while\nrequiring fewer training epochs. This work bridges the gap between black-box\nevaluation and mechanistic understanding, providing insights into how neural\nnetworks perform information selection and compression during summarization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5206\u6790GPT\u7c7b\u6a21\u578b\u5982\u4f55\u9002\u5e94\u6458\u8981\u4efb\u52a1\uff0c\u901a\u8fc7\u6bd4\u8f83\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u5b9a\u4f4d\u4e86\u6a21\u578b\u4e2d\u7684\u201c\u6458\u8981\u7535\u8def\u201d\uff0c\u5e76\u53d1\u73b0\u4e2d\u5c42\uff08\u7279\u522b\u662f2\u30013\u30015\u5c42\uff09\u53d8\u5316\u6700\u663e\u8457\u3002", "motivation": "\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u5185\u90e8\u673a\u5236\uff0c\u586b\u8865\u5206\u7c7b\u6216\u751f\u6210\u4efb\u52a1\u4e4b\u5916\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5dee\u5f02\u5206\u6790\u6bd4\u8f83\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u91cf\u5316\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u5185\u90e8\u6fc0\u6d3b\u7684\u53d8\u5316\uff0c\u5b9a\u4f4d\u5173\u952e\u5c42\u548c\u6ce8\u610f\u529b\u5934\u3002", "result": "\u4e2d\u5c42\uff082\u30013\u30015\u5c42\uff09\u53d8\u5316\u6700\u663e\u8457\uff0c62%\u7684\u6ce8\u610f\u529b\u5934\u71b5\u503c\u964d\u4f4e\uff0c\u8868\u660e\u4fe1\u606f\u9009\u62e9\u66f4\u96c6\u4e2d\u3002\u9488\u5bf9\u6027LoRA\u5fae\u8c03\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u9ed1\u76d2\u8bc4\u4f30\u4e0e\u673a\u5236\u7406\u89e3\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u4fe1\u606f\u9009\u62e9\u548c\u538b\u7f29\u673a\u5236\u3002", "relevance": 85.0}}
{"id": "2505.17364", "pdf": "https://arxiv.org/pdf/2505.17364", "abs": "https://arxiv.org/abs/2505.17364", "authors": ["Apar Pokhrel", "Gia Dao"], "title": "Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Parking space occupancy detection is a critical component in the development\nof intelligent parking management systems. Traditional object detection\napproaches, such as YOLOv8, provide fast and accurate vehicle detection across\nparking lots but can struggle with borderline cases, such as partially visible\nvehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In\nthis work, we perform a comprehensive comparative analysis of customized\nbackbone architectures integrated with YOLOv8. Specifically, we evaluate\nvarious backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot\ndataset in terms of detection accuracy and computational efficiency.\nExperimental results highlight each architecture's strengths and trade-offs,\nproviding insight into selecting suitable models for parking occupancy.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u5b9a\u5236\u5316\u9aa8\u5e72\u7f51\u7edc\uff08\u5982ResNet-18\u3001VGG16\u7b49\uff09\u4e0eYOLOv8\u7ed3\u5408\u5728\u505c\u8f66\u4f4d\u5360\u7528\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982YOLOv8\uff09\u5728\u505c\u8f66\u4f4d\u5360\u7528\u68c0\u6d4b\u4e2d\u96be\u4ee5\u5904\u7406\u90e8\u5206\u53ef\u89c1\u8f66\u8f86\u3001\u5c0f\u578b\u8f66\u8f86\u53ca\u5149\u7ebf\u4e0d\u4f73\u7b49\u8fb9\u754c\u60c5\u51b5\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u5b9a\u5236\u5316\u9aa8\u5e72\u7f51\u7edc\uff08ResNet-18\u3001VGG16\u3001EfficientNetV2\u3001Ghost\uff09\u5230YOLOv8\u4e2d\uff0c\u5e76\u5728PKLot\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e0d\u540c\u67b6\u6784\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5404\u6709\u4f18\u52a3\uff0c\u4e3a\u505c\u8f66\u4f4d\u5360\u7528\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u7684\u4f9d\u636e\u3002", "conclusion": "\u5b9a\u5236\u5316\u9aa8\u5e72\u7f51\u7edc\u4e0eYOLOv8\u7ed3\u5408\u53ef\u4f18\u5316\u505c\u8f66\u4f4d\u5360\u7528\u68c0\u6d4b\u6027\u80fd\uff0c\u4f46\u9700\u6839\u636e\u5177\u4f53\u9700\u6c42\u6743\u8861\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002", "relevance": 20.0}}
{"id": "2505.17673", "pdf": "https://arxiv.org/pdf/2505.17673", "abs": "https://arxiv.org/abs/2505.17673", "authors": ["Jiawei Du", "Jinlong Wu", "Yuzheng Chen", "Yucheng Hu", "Bing Li", "Joey Tianyi Zhou"], "title": "Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution", "categories": ["cs.AI"], "comment": null, "summary": "Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose\ntasks, define workflows, and assign agents to execute each step. While\neffective on benchmark-style tasks, such systems rely on designer updates and\noverlook agents' potential to learn from experience. Recently, Silver and\nSutton(2025) envision a shift into a new era, where agents could progress from\na stream of experiences. In this paper, we instantiate this vision of\nexperience-driven learning by introducing a bottom-up agent paradigm that\nmirrors the human learning process. Agents acquire competence through a\ntrial-and-reasoning mechanism-exploring, reflecting on outcomes, and\nabstracting skills over time. Once acquired, skills can be rapidly shared and\nextended, enabling continual evolution rather than static replication. As more\nagents are deployed, their diverse experiences accelerate this collective\nprocess, making bottom-up design especially suited for open-ended environments.\nWe evaluate this paradigm in Slay the Spire and Civilization V, where agents\nperceive through raw visual inputs and act via mouse outputs, the same as human\nplayers. Using a unified, game-agnostic codebase without any game-specific\nprompts or privileged APIs, our bottom-up agents acquire skills entirely\nthrough autonomous interaction, demonstrating the potential of the bottom-up\nparadigm in complex, real-world environments. Our code is available at\nhttps://github.com/AngusDujw/Bottom-Up-Agent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e0b\u800c\u4e0a\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u8bd5\u9519\u4e0e\u63a8\u7406\u673a\u5236\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u5f00\u653e\u73af\u5883\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u4eba\u5de5\u5206\u89e3\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4f5c\u8005\u63d0\u51fa\u81ea\u4e0b\u800c\u4e0a\u7684\u5b66\u4e60\u65b9\u5f0f\u4ee5\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8bd5\u9519\u4e0e\u63a8\u7406\u673a\u5236\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u63a2\u7d22\u3001\u53cd\u601d\u548c\u6280\u80fd\u62bd\u8c61\u9010\u6b65\u63d0\u5347\u80fd\u529b\uff0c\u5e76\u5728\u6e38\u620f\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5728Slay the Spire\u548cCivilization V\u4e2d\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u539f\u59cb\u89c6\u89c9\u8f93\u5165\u548c\u9f20\u6807\u8f93\u51fa\u81ea\u4e3b\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u81ea\u4e0b\u800c\u4e0a\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "conclusion": "\u81ea\u4e0b\u800c\u4e0a\u7684\u5b66\u4e60\u8303\u5f0f\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u652f\u6301\u667a\u80fd\u4f53\u7684\u6301\u7eed\u8fdb\u5316\u3002", "relevance": 85.0}}
{"id": "2505.17340", "pdf": "https://arxiv.org/pdf/2505.17340", "abs": "https://arxiv.org/abs/2505.17340", "authors": ["Tinghan Ye", "Amira Hijazi", "Pascal Van Hentenryck"], "title": "Conformal Predictive Distributions for Order Fulfillment Time Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Accurate estimation of order fulfillment time is critical for e-commerce\nlogistics, yet traditional rule-based approaches often fail to capture the\ninherent uncertainties in delivery operations. This paper introduces a novel\nframework for distributional forecasting of order fulfillment time, leveraging\nConformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic\ntechniques that provide rigorous coverage or validity guarantees. The proposed\nmachine learning methods integrate granular spatiotemporal features, capturing\nfulfillment location and carrier performance dynamics to enhance predictive\naccuracy. Additionally, a cost-sensitive decision rule is developed to convert\nprobabilistic forecasts into reliable point predictions. Experimental\nevaluation on a large-scale industrial dataset demonstrates that the proposed\nmethods generate competitive distributional forecasts, while machine\nlearning-based point predictions significantly outperform the existing\nrule-based system--achieving up to 14% higher prediction accuracy and up to 75%\nimprovement in identifying late deliveries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eConformal Predictive Systems\u548cCross Venn-Abers Predictors\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u5546\u7269\u6d41\u8ba2\u5355\u5c65\u884c\u65f6\u95f4\u7684\u5206\u5e03\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4ea4\u4ed8\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7269\u6d41\u4ea4\u4ed8\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u65b9\u6cd5\u4ee5\u4f18\u5316\u7535\u5546\u7269\u6d41\u6548\u7387\u3002", "method": "\u7ed3\u5408Conformal Predictive Systems\u548cCross Venn-Abers Predictors\uff0c\u5229\u7528\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7279\u5f81\u548c\u6210\u672c\u654f\u611f\u51b3\u7b56\u89c4\u5219\u751f\u6210\u6982\u7387\u9884\u6d4b\u548c\u70b9\u9884\u6d4b\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0c\u70b9\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534714%\uff0c\u5ef6\u8fdf\u4ea4\u4ed8\u8bc6\u522b\u80fd\u529b\u63d0\u534775%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba2\u5355\u5c65\u884c\u65f6\u95f4\u9884\u6d4b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u7cfb\u7edf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 30.0}}
{"id": "2505.17074", "pdf": "https://arxiv.org/pdf/2505.17074", "abs": "https://arxiv.org/abs/2505.17074", "authors": ["Ruixiao Li", "Fahao Chen", "Peng Li"], "title": "Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nemploying a small speculative model (SSM) to generate multiple candidate tokens\nand verify them using the LLM in parallel. This technique has been widely\nintegrated into LLM inference serving systems. However, inference requests\ntypically exhibit uncertain execution time, which poses a significant challenge\nof efficiently scheduling requests in these systems. Existing work estimates\nexecution time based solely on predicted output length, which could be\ninaccurate because execution time depends on both output length and token\nacceptance rate of verification by the LLM. In this paper, we propose a\nsemi-clairvoyant request scheduling algorithm called\nLeast-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a\nnumber of inference requests, LAPS-SD can effectively minimize average\ninference latency by adaptively scheduling requests according to their features\nduring decoding. When the token acceptance rate is dynamic and execution time\nis difficult to estimate, LAPS-SD maintains multiple priority queues and allows\nrequest execution preemption across different queues. Once the token acceptance\nrate becomes stable, LAPS-SD can accurately estimate the execution time and\nschedule requests accordingly. Extensive experiments show that LAPS-SD reduces\ninference latency by approximately 39\\% compared to state-of-the-art scheduling\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAPS-SD\u7684\u534a\u9884\u89c1\u6027\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u7684LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u867d\u80fd\u52a0\u901fLLM\u63a8\u7406\uff0c\u4f46\u63a8\u7406\u8bf7\u6c42\u7684\u6267\u884c\u65f6\u95f4\u4e0d\u786e\u5b9a\uff0c\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8f93\u51fa\u957f\u5ea6\u9884\u6d4b\uff0c\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLAPS-SD\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u8bf7\u6c42\uff08\u5305\u62ec\u4f18\u5148\u7ea7\u961f\u5217\u548c\u62a2\u5360\u673a\u5236\uff09\u9002\u5e94\u53d8\u5316\u7684\u4ee4\u724c\u63a5\u53d7\u7387\uff0c\u7a33\u5b9a\u540e\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLAPS-SD\u6bd4\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u964d\u4f4e\u7ea639%\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "LAPS-SD\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u6d4b\u89e3\u7801\u7cfb\u7edf\u4e2d\u7684\u8c03\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.17367", "pdf": "https://arxiv.org/pdf/2505.17367", "abs": "https://arxiv.org/abs/2505.17367", "authors": ["Zichuan Yang"], "title": "EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 4 figures", "summary": "Medical image classification is critical for clinical decision-making, yet\ndemands for accuracy, interpretability, and generalizability remain\nchallenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba\narchitecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for\nmulti-organ medical image classification. EVM-Fusion leverages a multipath\ndesign, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)\nmodules, operate in parallel with a traditional feature pathway. These diverse\nfeatures are dynamically integrated via a two-stage fusion process: cross-modal\nattention followed by the iterative NAF block, which learns an adaptive fusion\nalgorithm. Intrinsic explainability is embedded through path-specific spatial\nattention, Vim {\\Delta}-value maps, traditional feature SE-attention, and\ncross-modal attention weights. Experiments on a diverse 9-class multi-organ\nmedical image dataset demonstrate EVM-Fusion's strong classification\nperformance, achieving 99.75% test accuracy and provide multi-faceted insights\ninto its decision-making process, highlighting its potential for trustworthy AI\nin medical diagnostics.", "AI": {"tldr": "EVM-Fusion\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u89c6\u89c9Mamba\u67b6\u6784\uff0c\u901a\u8fc7\u795e\u7ecf\u7b97\u6cd5\u878d\u5408\u673a\u5236\u63d0\u5347\u591a\u5668\u5b98\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u4ecd\u662f\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u8def\u5f84\u8bbe\u8ba1\uff0c\u7ed3\u5408DenseNet\u3001U-Net\u548cVision Mamba\u6a21\u5757\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u878d\u5408\u673a\u5236\u52a8\u6001\u6574\u5408\u7279\u5f81\u3002", "result": "\u57289\u7c7b\u591a\u5668\u5b98\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523099.75%\uff0c\u5e76\u63d0\u4f9b\u591a\u65b9\u9762\u7684\u51b3\u7b56\u8fc7\u7a0b\u89e3\u91ca\u3002", "conclusion": "EVM-Fusion\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u5c55\u73b0\u51fa\u53ef\u4fe1AI\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17696", "pdf": "https://arxiv.org/pdf/2505.17696", "abs": "https://arxiv.org/abs/2505.17696", "authors": ["Sota Yoshihara", "Ryousuke Yamamoto", "Hiroyuki Kusumoto", "Masanari Shimura"], "title": "Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "8 pages, 6 figures. Appendix: 16 pages. First three listed authors\n  have equal contributions", "summary": "This research proposes methods for formulating and guaranteeing the\nresilience of long short-term memory (LSTM) networks, which can serve as a key\ntechnology in AI system quality assurance. We introduce a novel methodology\napplying incremental input-to-state stability ($\\delta$ISS) to mathematically\ndefine and evaluate the resilience of LSTM against input perturbations. Key\nachievements include the development of a data-independent evaluation method\nand the demonstration of resilience control through adjustments to training\nparameters. This research presents concrete solutions to AI quality assurance\nfrom a control theory perspective, which can advance AI applications in control\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u589e\u91cf\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\uff08\u03b4ISS\uff09\u6765\u5b9a\u4e49\u548c\u8bc4\u4f30LSTM\u7f51\u7edc\u5bf9\u8f93\u5165\u6270\u52a8\u5f39\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u6570\u636e\u65e0\u5173\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3aAI\u7cfb\u7edf\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u5173\u952e\u6280\u672f\uff0c\u901a\u8fc7\u6570\u5b66\u65b9\u6cd5\u786e\u4fddLSTM\u7f51\u7edc\u7684\u5f39\u6027\u3002", "method": "\u91c7\u7528\u589e\u91cf\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\uff08\u03b4ISS\uff09\u7406\u8bba\uff0c\u5f00\u53d1\u6570\u636e\u65e0\u5173\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u8bad\u7ec3\u53c2\u6570\u63a7\u5236\u5f39\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9LSTM\u7f51\u7edc\u5f39\u6027\u7684\u6570\u5b66\u5b9a\u4e49\u548c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u8bad\u7ec3\u53c2\u6570\u8c03\u6574\u63a7\u5236\u5f39\u6027\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ece\u63a7\u5236\u7406\u8bba\u89d2\u5ea6\u4e3aAI\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u5177\u4f53\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u63a8\u52a8AI\u5728\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "relevance": 40.0}}
{"id": "2505.17341", "pdf": "https://arxiv.org/pdf/2505.17341", "abs": "https://arxiv.org/abs/2505.17341", "authors": ["Dibyajyoti Nayak", "Somdatta Goswami"], "title": "TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation", "categories": ["cs.LG"], "comment": "18 pages, 8 figures", "summary": "Accurate temporal extrapolation presents a fundamental challenge for neural\noperators in modeling dynamical systems, where reliable predictions must extend\nsignificantly beyond the training time horizon. Conventional Deep Operator\nNetwork (DeepONet) approaches employ two inherently limited training paradigms\n- fixed-horizon rollouts that predict complete spatiotemporal solutions while\ndisregarding temporal causality, and autoregressive formulations that\naccumulate errors through sequential predictions. We introduce TI-DeepONet, a\nframework that integrates neural operators with adaptive numerical\ntime-stepping techniques to preserve the Markovian structure of dynamical\nsystems while mitigating error propagation in extended temporal forecasting.\nOur approach reformulates the learning objective from direct state prediction\nto the approximation of instantaneous time-derivative fields, which are then\nintegrated using established numerical schemes. This architecture supports\ncontinuous-time prediction and enables deployment of higher-precision\nintegrators during inference than those used during training, balancing\ncomputational efficiency with predictive accuracy. We further develop\nTI(L)-DeepONet, which incorporates learnable coefficients for intermediate\nslopes in the integration process, adapting to solution-specific variations and\nenhancing fidelity. Evaluation across three canonical PDEs shows that\nTI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative\nL2 extrapolation errors: approximately 81% over autoregressive and 70% over\nfixed-horizon methods. Notably, both maintain prediction stability for temporal\ndomains extending to about twice the training interval. This research\nestablishes a physics-aware operator learning paradigm that bridges neural\napproximation with numerical analysis while preserving the causal structure of\ndynamical systems.", "AI": {"tldr": "TI-DeepONet\u548cTI(L)-DeepONet\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u6b65\u8fdb\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7cfb\u7edf\u7684\u65f6\u95f4\u5916\u63a8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u8bef\u5dee\u4f20\u64ad\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfDeepONet\u65b9\u6cd5\u5728\u65f6\u95f4\u5916\u63a8\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u56fa\u5b9a\u65f6\u95f4\u6b65\u957f\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "method": "\u63d0\u51faTI-DeepONet\u548cTI(L)-DeepONet\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u76ee\u6807\u4ece\u76f4\u63a5\u72b6\u6001\u9884\u6d4b\u6539\u4e3a\u77ac\u65f6\u65f6\u95f4\u5bfc\u6570\u573a\u7684\u8fd1\u4f3c\uff0c\u5e76\u7ed3\u5408\u6570\u503c\u79ef\u5206\u65b9\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u7ecf\u5178PDE\u4e0a\uff0cTI(L)-DeepONet\u7565\u4f18\u4e8eTI-DeepONet\uff0c\u4e24\u8005\u5206\u522b\u51cf\u5c11\u7ea681%\u548c70%\u7684\u76f8\u5bf9L2\u8bef\u5dee\uff0c\u4e14\u9884\u6d4b\u7a33\u5b9a\u6027\u6269\u5c55\u5230\u8bad\u7ec3\u533a\u95f4\u7684\u4e24\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u7269\u7406\u611f\u77e5\u7684\u7b97\u5b50\u5b66\u4e60\u8303\u5f0f\uff0c\u7ed3\u5408\u795e\u7ecf\u8fd1\u4f3c\u548c\u6570\u503c\u5206\u6790\uff0c\u4fdd\u7559\u4e86\u52a8\u6001\u7cfb\u7edf\u7684\u56e0\u679c\u7ed3\u6784\u3002", "relevance": 50.0}}
{"id": "2505.17075", "pdf": "https://arxiv.org/pdf/2505.17075", "abs": "https://arxiv.org/abs/2505.17075", "authors": ["Fuma Kurata", "Mao Saeki", "Masaki Eguchi", "Shungo Suzuki", "Hiroaki Takatsu", "Yoichi Matsuyama"], "title": "Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study aimed to develop and validate two scales of engagement and rapport\nto evaluate the user experience quality with multimodal dialogue systems in the\ncontext of foreign language learning. The scales were designed based on\ntheories of engagement in educational psychology, social psychology, and second\nlanguage acquisition.Seventy-four Japanese learners of English completed\nroleplay and discussion tasks with trained human tutors and a dialog agent.\nAfter each dialogic task was completed, they responded to the scales of\nengagement and rapport. The validity and reliability of the scales were\ninvestigated through two analyses. We first conducted analysis of Cronbach's\nalpha coefficient and a series of confirmatory factor analyses to test the\nstructural validity of the scales and the reliability of our designed items. We\nthen compared the scores of engagement and rapport between the dialogue with\nhuman tutors and the one with a dialogue agent. The results revealed that our\nscales succeeded in capturing the difference in the dialogue experience quality\nbetween the human interlocutors and the dialogue agent from multiple\nperspectives.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u91cf\u8868\uff08\u53c2\u4e0e\u5ea6\u548c\u4eb2\u548c\u529b\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u3002", "motivation": "\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\uff0c\u57fa\u4e8e\u6559\u80b2\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u5fc3\u7406\u5b66\u548c\u7b2c\u4e8c\u8bed\u8a00\u4e60\u5f97\u7406\u8bba\u3002", "method": "74\u540d\u65e5\u672c\u82f1\u8bed\u5b66\u4e60\u8005\u4e0e\u4eba\u7c7b\u5bfc\u5e08\u548c\u5bf9\u8bdd\u4ee3\u7406\u5b8c\u6210\u4efb\u52a1\u540e\u586b\u5199\u91cf\u8868\uff0c\u901a\u8fc7Cronbach's alpha\u7cfb\u6570\u548c\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\u9a8c\u8bc1\u91cf\u8868\u4fe1\u6548\u5ea6\u3002", "result": "\u91cf\u8868\u6210\u529f\u6355\u6349\u4e86\u4eba\u7c7b\u5bfc\u5e08\u4e0e\u5bf9\u8bdd\u4ee3\u7406\u5728\u5bf9\u8bdd\u4f53\u9a8c\u8d28\u91cf\u4e0a\u7684\u5dee\u5f02\u3002", "conclusion": "\u5f00\u53d1\u7684\u91cf\u8868\u6709\u6548\u533a\u5206\u4e86\u4e0d\u540c\u5bf9\u8bdd\u4f19\u4f34\u7684\u4f53\u9a8c\u8d28\u91cf\u3002", "relevance": 30.0}}
{"id": "2505.17392", "pdf": "https://arxiv.org/pdf/2505.17392", "abs": "https://arxiv.org/abs/2505.17392", "authors": ["Leon C. C. K", "Zeng Hui"], "title": "Dual-sensing driving detection model", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.4.8; J.7"], "comment": "19 pages", "summary": "In this paper, a novel dual-sensing driver fatigue detection method combining\ncomputer vision and physiological signal analysis is proposed. The system\nexploits the complementary advantages of the two sensing modalities and breaks\nthrough the limitations of existing single-modality methods. We introduce an\ninnovative architecture that combines real-time facial feature analysis with\nphysiological signal processing, combined with advanced fusion strategies, for\nrobust fatigue detection. The system is designed to run efficiently on existing\nhardware while maintaining high accuracy and reliability. Through comprehensive\nexperiments, we demonstrate that our method outperforms traditional methods in\nboth controlled environments and real-world conditions, while maintaining high\naccuracy. The practical applicability of the system has been verified through\nextensive tests in various driving scenarios and shows great potential in\nreducing fatigue-related accidents. This study contributes to the field by\nproviding a more reliable, cost-effective, and humane solution for driver\nfatigue detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u751f\u7406\u4fe1\u53f7\u5206\u6790\u7684\u53cc\u6a21\u6001\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u548c\u878d\u5408\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u75b2\u52b3\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6\u9762\u90e8\u7279\u5f81\u5206\u6790\u548c\u751f\u7406\u4fe1\u53f7\u5904\u7406\uff0c\u91c7\u7528\u5148\u8fdb\u7684\u878d\u5408\u7b56\u7565\uff0c\u8bbe\u8ba1\u9ad8\u6548\u8fd0\u884c\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5728\u63a7\u5236\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u3001\u7ecf\u6d4e\u4e14\u4eba\u6027\u5316\u7684\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 20.0}}
{"id": "2505.17705", "pdf": "https://arxiv.org/pdf/2505.17705", "abs": "https://arxiv.org/abs/2505.17705", "authors": ["Runze Li", "Siyu Wu", "Jun Wang", "Wei Zhang"], "title": "CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models", "categories": ["cs.AI", "cs.LG", "68T50", "I.2.7"], "comment": null, "summary": "Knowledge Tracing (KT) aims to model a student's learning state over time and\npredict their future performance. However, traditional KT methods often face\nchallenges in explainability, scalability, and effective modeling of complex\nknowledge dependencies. While Large Language Models (LLMs) present new avenues\nfor KT, their direct application often struggles with generating structured,\nexplainable student representations and lacks mechanisms for continuous,\ntask-specific refinement. To address these gaps, we propose Collaborative\nIterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance\nboth prediction accuracy and explainability. CIKT employs a dual-component\narchitecture: an Analyst generates dynamic, explainable user profiles from\nstudent historical responses, and a Predictor utilizes these profiles to\nforecast future performance. The core of CIKT is a synergistic optimization\nloop. In this loop, the Analyst is iteratively refined based on the predictive\naccuracy of the Predictor, which conditions on the generated profiles, and the\nPredictor is subsequently retrained using these enhanced profiles. Evaluated on\nmultiple educational datasets, CIKT demonstrates significant improvements in\nprediction accuracy, offers enhanced explainability through its dynamically\nupdated user profiles, and exhibits improved scalability. Our work presents a\nrobust and explainable solution for advancing knowledge tracing systems,\neffectively bridging the gap between predictive performance and model\ntransparency.", "AI": {"tldr": "\u63d0\u51fa\u4e86Collaborative Iterative Knowledge Tracing (CIKT)\u6846\u67b6\uff0c\u5229\u7528LLMs\u63d0\u5347\u77e5\u8bc6\u8ffd\u8e2a\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u6269\u5c55\u6027\u548c\u590d\u6742\u77e5\u8bc6\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cLLMs\u7684\u76f4\u63a5\u5e94\u7528\u4e5f\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b66\u751f\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\u673a\u5236\u3002", "method": "CIKT\u91c7\u7528\u53cc\u7ec4\u4ef6\u67b6\u6784\uff1aAnalyst\u751f\u6210\u52a8\u6001\u53ef\u89e3\u91ca\u7684\u7528\u6237\u6863\u6848\uff0cPredictor\u5229\u7528\u8fd9\u4e9b\u6863\u6848\u9884\u6d4b\u672a\u6765\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5faa\u73af\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u5728\u591a\u4e2a\u6559\u80b2\u6570\u636e\u96c6\u4e0a\uff0cCIKT\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6539\u5584\u4e86\u6269\u5c55\u6027\u3002", "conclusion": "CIKT\u4e3a\u77e5\u8bc6\u8ffd\u8e2a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5f25\u5408\u4e86\u9884\u6d4b\u6027\u80fd\u4e0e\u6a21\u578b\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "relevance": 60.0}}
{"id": "2505.17342", "pdf": "https://arxiv.org/pdf/2505.17342", "abs": "https://arxiv.org/abs/2505.17342", "authors": ["Ankita Kushwaha", "Kiran Ravish", "Preeti Lamba", "Pawan Kumar"], "title": "A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety", "categories": ["cs.LG"], "comment": "25", "summary": "Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement\nlearning that explicitly deals with safety constraints during the learning and\ndeployment of agents. This survey provides a mathematically rigorous overview\nof SafeRL formulations based on Constrained Markov Decision Processes (CMDPs)\nand extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical\nfoundations of CMDPs, covering definitions, constrained optimization\ntechniques, and fundamental theorems. We then summarize state-of-the-art\nalgorithms in SafeRL for single agents, including policy gradient methods with\nsafety guarantees and safe exploration strategies, as well as recent advances\nin SafeMARL for cooperative and competitive settings. Additionally, we propose\nfive open research problems to advance the field, with three focusing on\nSafeMARL. Each problem is described with motivation, key challenges, and\nrelated prior work. This survey is intended as a technical guide for\nresearchers interested in SafeRL and SafeMARL, highlighting key concepts,\nmethods, and open future research directions.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u63d0\u4f9b\u4e86\u5173\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SafeRL\uff09\u7684\u6570\u5b66\u4e25\u8c28\u6982\u8ff0\uff0c\u6db5\u76d6\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDPs\uff09\u53ca\u5176\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6269\u5c55\uff08SafeMARL\uff09\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u5b66\u4e60\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u5982\u4f55\u660e\u786e\u5904\u7406\u5b89\u5168\u7ea6\u675f\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u57fa\u7840\u7684\u56de\u987e\uff08\u5982CMDPs\u7684\u5b9a\u4e49\u548c\u7ea6\u675f\u4f18\u5316\u6280\u672f\uff09\u4ee5\u53ca\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6700\u65b0\u7b97\u6cd5\u603b\u7ed3\u3002", "result": "\u7ed3\u679c\u662f\u5bf9SafeRL\u548cSafeMARL\u9886\u57df\u7684\u5173\u952e\u6982\u5ff5\u3001\u65b9\u6cd5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u6280\u672f\u6027\u603b\u7ed3\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63d0\u51fa\u4e86\u4e94\u4e2a\u5f00\u653e\u7814\u7a76\u95ee\u9898\uff0c\u5176\u4e2d\u4e09\u4e2a\u805a\u7126\u4e8eSafeMARL\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "relevance": 75.0}}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076", "abs": "https://arxiv.org/abs/2505.17076", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Eng Siong Chng"], "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "5 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u5e27\u7387\u5bf9\u8bed\u97f3\u6807\u8bb0\u5316\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5e27\u7387\u53d8\u5316\u5bf9\u6c49\u8bed\u548c\u82f1\u8bed\u7684\u8bed\u97f3\u6807\u8bb0\u5316\u6709\u4e0d\u540c\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5e27\u7387\u5bf9\u8bed\u97f3\u6807\u8bb0\u5316\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u5e27\u7387\u4e0b\u7f16\u7801\u8bed\u97f3\uff0c\u8bc4\u4f30\u751f\u6210\u7684\u8bed\u4e49\u6807\u8bb0\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5e27\u7387\u53d8\u5316\u5bf9\u6c49\u8bed\u548c\u82f1\u8bed\u7684\u8bed\u97f3\u6807\u8bb0\u5316\u5f71\u54cd\u4e0d\u540c\uff0c\u63ed\u793a\u4e86\u5e27\u7387\u3001\u8bed\u97f3\u5bc6\u5ea6\u548c\u8bed\u8a00\u7279\u5b9a\u58f0\u5b66\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bed\u97f3\u6807\u8bb0\u5668\u7684\u5e27\u7387\u9009\u62e9\u63d0\u4f9b\u4e86\u4f18\u5316\u65b9\u5411\uff0c\u5bf9\u8bed\u97f3\u8bc6\u522b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u7b49\u5e94\u7528\u6709\u91cd\u8981\u610f\u4e49\u3002", "relevance": 40.0}}
{"id": "2505.17395", "pdf": "https://arxiv.org/pdf/2505.17395", "abs": "https://arxiv.org/abs/2505.17395", "authors": ["Gowtham Raj Vuppari", "Navarun Gupta", "Ahmed El-Sayed", "Xingguo Xiong"], "title": "Wildfire Detection Using Vision Transformer with the Wildfire Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Published at ASEE NE 2025", "summary": "The critical need for sophisticated detection techniques has been highlighted\nby the rising frequency and intensity of wildfires in the US, especially in\nCalifornia. In 2023, wildfires caused 130 deaths nationwide, the highest since\n1990. In January 2025, Los Angeles wildfires which included the Palisades and\nEaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused\nloss of human lives. The devastation underscores the urgent need for effective\ndetection and prevention strategies. Deep learning models, such as Vision\nTransformers (ViTs), can enhance early detection by processing complex image\ndata with high accuracy. However, wildfire detection faces challenges,\nincluding the availability of high-quality, real-time data. Wildfires often\noccur in remote areas with limited sensor coverage, and environmental factors\nlike smoke and cloud cover can hinder detection. Additionally, training deep\nlearning models is computationally expensive, and issues like false\npositives/negatives and scaling remain concerns. Integrating detection systems\nwith real-time alert mechanisms also poses difficulties. In this work, we used\nthe wildfire dataset consisting of 10.74 GB high-resolution images categorized\ninto 'fire' and 'nofire' classes is used for training the ViT model. To prepare\nthe data, images are resized to 224 x 224 pixels, converted into tensor format,\nand normalized using ImageNet statistics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Vision Transformers\uff08ViTs\uff09\u6539\u8fdb\u91ce\u706b\u65e9\u671f\u68c0\u6d4b\uff0c\u4f46\u9762\u4e34\u6570\u636e\u8d28\u91cf\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5b9e\u65f6\u96c6\u6210\u7b49\u6311\u6218\u3002", "motivation": "\u91ce\u706b\u9891\u53d1\u9020\u6210\u5de8\u5927\u635f\u5931\uff0c\u4e9f\u9700\u9ad8\u6548\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u4f7f\u752810.74 GB\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6570\u636e\u96c6\u8bad\u7ec3ViT\u6a21\u578b\uff0c\u56fe\u50cf\u9884\u5904\u7406\u5305\u62ec\u8c03\u6574\u5927\u5c0f\u3001\u8f6c\u6362\u4e3a\u5f20\u91cf\u5e76\u5f52\u4e00\u5316\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46ViT\u6709\u671b\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "ViT\u5728\u91ce\u706b\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u6570\u636e\u3001\u8ba1\u7b97\u548c\u5b9e\u65f6\u6027\u95ee\u9898\u3002", "relevance": 30.0}}
{"id": "2505.17735", "pdf": "https://arxiv.org/pdf/2505.17735", "abs": "https://arxiv.org/abs/2505.17735", "authors": ["Xueyang Zhou", "Weidong Wang", "Lin Lu", "Jiawen Shi", "Guiyao Tie", "Yongtian Xu", "Lixing Chen", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "title": "Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios", "categories": ["cs.AI", "68T07", "I.2.6"], "comment": "38 pages;12 figures;12 tables", "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nreal-world applications such as \"digital assistants, autonomous customer\nservice, and decision-support systems\", where their ability to \"interact in\nmulti-turn, tool-augmented environments\" makes them indispensable. However,\nensuring the safety of these agents remains a significant challenge due to the\ndiverse and complex risks arising from dynamic user interactions, external tool\nusage, and the potential for unintended harmful behaviors. To address this\ncritical issue, we propose AutoSafe, the first framework that systematically\nenhances agent safety through fully automated synthetic data generation.\nConcretely, 1) we introduce an open and extensible threat model, OTS, which\nformalizes how unsafe behaviors emerge from the interplay of user instructions,\ninteraction contexts, and agent actions. This enables precise modeling of\nsafety risks across diverse scenarios. 2) we develop a fully automated data\ngeneration pipeline that simulates unsafe user behaviors, applies\nself-reflective reasoning to generate safe responses, and constructs a\nlarge-scale, diverse, and high-quality safety training dataset-eliminating the\nneed for hazardous real-world data collection. To evaluate the effectiveness of\nour framework, we design comprehensive experiments on both synthetic and\nreal-world safety benchmarks. Results demonstrate that AutoSafe boosts safety\nscores by 45% on average and achieves a 28.91% improvement on real-world tasks,\nvalidating the generalization ability of our learned safety strategies. These\nresults highlight the practical advancement and scalability of AutoSafe in\nbuilding safer LLM-based agents for real-world deployment. We have released the\nproject page at https://auto-safe.github.io/.", "AI": {"tldr": "AutoSafe\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u52a8\u5316\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u5347LLM\u4ee3\u7406\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u5305\u62ec\u5a01\u80c1\u6a21\u578bOTS\u548c\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u8bc4\u5206\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u590d\u6742\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOTS\u5a01\u80c1\u6a21\u578b\u548c\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u6a21\u62df\u4e0d\u5b89\u5168\u884c\u4e3a\u5e76\u751f\u6210\u5b89\u5168\u54cd\u5e94\u3002", "result": "AutoSafe\u5728\u5b89\u5168\u6027\u8bc4\u5206\u4e0a\u5e73\u5747\u63d0\u534745%\uff0c\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u63d0\u534728.91%\u3002", "conclusion": "AutoSafe\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17344", "pdf": "https://arxiv.org/pdf/2505.17344", "abs": "https://arxiv.org/abs/2505.17344", "authors": ["Ninda Nurseha Amalina", "Kwadwo Boateng Ofori-Amanfo", "Heungjo An"], "title": "A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "14 pages, 6 figures", "summary": "Unattended scheduled appointments, defined as patient no-shows, adversely\naffect both healthcare providers and patients' health, disrupting the\ncontinuity of care, operational efficiency, and the efficient allocation of\nmedical resources. Accurate predictive modelling is needed to reduce the impact\nof no-shows. Although machine learning methods, such as logistic regression,\nrandom forest models, and decision trees, are widely used in predicting patient\nno-shows, they often rely on hard decision splits and static feature\nimportance, limiting their adaptability to specific or complex patient\nbehaviors. To address this limitation, we propose a new hybrid Multi-Head\nAttention Soft Random Forest (MHASRF) model that integrates attention\nmechanisms into a random forest model using probabilistic soft splitting\ninstead of hard splitting. The MHASRF model assigns attention weights\ndifferently across the trees, enabling attention on specific patient behaviors.\nThe model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a\n93.59% F1 score, surpassing the performance of decision tree, logistic\nregression, random forest, and naive Bayes models. Furthermore, MHASRF was able\nto identify key predictors of patient no-shows using two levels of feature\nimportance (tree level and attention mechanism level), offering deeper insights\ninto patient no-show predictors. The proposed model is a robust, adaptable, and\ninterpretable method for predicting patient no-shows that will help healthcare\nproviders in optimizing resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f6f\u968f\u673a\u68ee\u6797\u6a21\u578b\uff08MHASRF\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u60a3\u8005\u723d\u7ea6\u884c\u4e3a\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u7279\u5f81\u89e3\u91ca\u3002", "motivation": "\u60a3\u8005\u723d\u7ea6\u5f71\u54cd\u533b\u7597\u8d44\u6e90\u5206\u914d\u548c\u8fde\u7eed\u6027\u62a4\u7406\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u968f\u673a\u68ee\u6797\uff0c\u91c7\u7528\u6982\u7387\u8f6f\u5206\u88c2\u66ff\u4ee3\u786c\u5206\u88c2\uff0c\u5b9e\u73b0\u591a\u7ea7\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u3002", "result": "\u6a21\u578b\u51c6\u786e\u738793.56%\uff0cF1\u5206\u657093.59%\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u8bc6\u522b\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "MHASRF\u662f\u4e00\u79cd\u9c81\u68d2\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u723d\u7ea6\u9884\u6d4b\u65b9\u6cd5\u3002", "relevance": 20.0}}
{"id": "2505.17078", "pdf": "https://arxiv.org/pdf/2505.17078", "abs": "https://arxiv.org/abs/2505.17078", "authors": ["Zenghao Duan", "Zhiyi Yin", "Zhichao Shi", "Liang Pang", "Shaoling Jing", "Jiayi Wu", "Yu Yan", "Huawei Shen", "Xueqi Cheng"], "title": "GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the underlying mechanisms of toxicity generation in\nLarge Language Models (LLMs) and proposes an effective detoxification approach.\nPrior work typically considers the Feed-Forward Network (FFN) as the main\nsource of toxicity, representing toxic regions as a set of toxic vectors or\nlayer-wise subspaces. However, our in-depth analysis reveals that the global\ntoxic subspace offers a more effective and comprehensive representation of\ntoxic region within the model. Building on this insight, we propose GloSS\n(Global Toxic Subspace Suppression), a lightweight, four-stage method that\nmitigates toxicity by identifying and removing the global toxic subspace from\nthe parameters of FFN. Experiments across a range of LLMs show that GloSS\nachieves state-of-the-art detoxification performance while preserving the\nmodels general capabilities, without requiring large-scale data or model\nretraining.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u6bd2\u6027\u751f\u6210\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89e3\u6bd2\u65b9\u6cd5GloSS\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4\u6765\u964d\u4f4e\u6bd2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c06\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u89c6\u4e3a\u6bd2\u6027\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4\u80fd\u66f4\u6709\u6548\u5730\u8868\u5f81\u6bd2\u6027\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86GloSS\u65b9\u6cd5\uff0c\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u79fb\u9664FFN\u53c2\u6570\u4e2d\u7684\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4\u6765\u89e3\u6bd2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGloSS\u5728\u591a\u79cdLLMs\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89e3\u6bd2\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "GloSS\u662f\u4e00\u79cd\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e\u6216\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u89e3\u6bd2\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17412", "pdf": "https://arxiv.org/pdf/2505.17412", "abs": "https://arxiv.org/abs/2505.17412", "authors": ["Shuang Wu", "Youtian Lin", "Feihu Zhang", "Yifei Zeng", "Yikang Yang", "Yajie Bao", "Jiachen Qian", "Siyu Zhu", "Philip Torr", "Xun Cao", "Yao Yao"], "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention", "categories": ["cs.CV"], "comment": "Project page: https://nju3dv.github.io/projects/Direct3D-S2/", "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.", "AI": {"tldr": "Direct3D S2\u662f\u4e00\u4e2a\u57fa\u4e8e\u7a00\u758f\u4f53\u79ef\u7684\u53ef\u6269\u5c553D\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u73873D\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f7f\u7528\u4f53\u79ef\u8868\u793a\uff08\u5982SDF\uff09\u751f\u6210\u9ad8\u5206\u8fa8\u73873D\u5f62\u72b6\u65f6\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u7a00\u758f\u6ce8\u610f\u529b\uff08SSA\uff09\u673a\u5236\u548c\u7edf\u4e00\u7684\u7a00\u758f\u4f53\u79ef\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8bbe\u8ba1\u3002", "result": "\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u63011024\u5206\u8fa8\u7387\u8bad\u7ec3\u4ec5\u97008\u4e2aGPU\u3002", "conclusion": "Direct3D S2\u4f7f\u5343\u5146\u7ea73D\u751f\u6210\u53d8\u5f97\u5b9e\u7528\u4e14\u6613\u4e8e\u5b9e\u73b0\u3002", "relevance": 40.0}}
{"id": "2505.17801", "pdf": "https://arxiv.org/pdf/2505.17801", "abs": "https://arxiv.org/abs/2505.17801", "authors": ["B\u00e1lint Gyevn\u00e1r", "Christopher G. Lucas", "Stefano V. Albrecht", "Shay B. Cohen"], "title": "Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour", "categories": ["cs.AI"], "comment": null, "summary": "Autonomous multi-agent systems (MAS) are useful for automating complex tasks\nbut raise trust concerns due to risks like miscoordination and goal\nmisalignment. Explainability is vital for trust calibration, but explainable\nreinforcement learning for MAS faces challenges in state/action space\ncomplexity, stakeholder needs, and evaluation. Using the counterfactual theory\nof causation and LLMs' summarisation capabilities, we propose Agentic\neXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible\ncausal explanations for pre-trained multi-agent policies by having an LLM\ninterrogate an environment simulator using queries like 'whatif' and 'remove'\nto observe and synthesise counterfactual information over multiple rounds. We\nevaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel\nevaluation methodology combining subjective preference, correctness, and\ngoal/action prediction metrics, and an external LLM as evaluator. Compared to\nbaselines, AXIS improves perceived explanation correctness by at least 7.7%\nacross all models and goal prediction accuracy by 23% for 4 models, with\nimproved or comparable action prediction accuracy, achieving the highest scores\noverall.", "AI": {"tldr": "AXIS\u5229\u7528LLM\u548c\u53cd\u4e8b\u5b9e\u7406\u8bba\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u53ef\u7406\u89e3\u7684\u56e0\u679c\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u6b63\u786e\u6027\u548c\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u4fe1\u4efb\u95ee\u9898\u6e90\u4e8e\u534f\u8c03\u548c\u76ee\u6807\u5bf9\u9f50\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAXIS\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u4e0e\u73af\u5883\u6a21\u62df\u5668\u7684\u4ea4\u4e92\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u4f7f\u7528\u67e5\u8be2\u5982'whatif'\u548c'remove'\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cAXIS\u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u6b63\u786e\u6027\uff08\u81f3\u5c117.7%\uff09\u548c\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u6027\uff0823%\uff09\u3002", "conclusion": "AXIS\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408LLM\u548c\u53cd\u4e8b\u5b9e\u7406\u8bba\u3002", "relevance": 75.0}}
{"id": "2505.17351", "pdf": "https://arxiv.org/pdf/2505.17351", "abs": "https://arxiv.org/abs/2505.17351", "authors": ["N. Benjamin Erichson", "Vinicius Mikuni", "Dongwei Lyu", "Yang Gao", "Omri Azencot", "Soon Hoe Lim", "Michael W. Mahoney"], "title": "FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce FLEX (FLow EXpert), a backbone architecture for generative\nmodeling of spatio-temporal physical systems using diffusion models. FLEX\noperates in the residual space rather than on raw data, a modeling choice that\nwe motivate theoretically, showing that it reduces the variance of the velocity\nfield in the diffusion model, which helps stabilize training. FLEX integrates a\nlatent Transformer into a U-Net with standard convolutional ResNet layers and\nincorporates a redesigned skip connection scheme. This hybrid design enables\nthe model to capture both local spatial detail and long-range dependencies in\nlatent space. To improve spatio-temporal conditioning, FLEX uses a\ntask-specific encoder that processes auxiliary inputs such as coarse or past\nsnapshots. Weak conditioning is applied to the shared encoder via skip\nconnections to promote generalization, while strong conditioning is applied to\nthe decoder through both skip and bottleneck features to ensure reconstruction\nfidelity. FLEX achieves accurate predictions for super-resolution and\nforecasting tasks using as few as two reverse diffusion steps. It also produces\ncalibrated uncertainty estimates through sampling. Evaluations on\nhigh-resolution 2D turbulence data show that FLEX outperforms strong baselines\nand generalizes to out-of-distribution settings, including unseen Reynolds\nnumbers, physical observables (e.g., fluid flow velocity fields), and boundary\nconditions.", "AI": {"tldr": "FLEX\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u7269\u7406\u7cfb\u7edf\u751f\u6210\u5efa\u6a21\u67b6\u6784\uff0c\u901a\u8fc7\u6b8b\u5dee\u7a7a\u95f4\u64cd\u4f5c\u548c\u6df7\u5408\u8bbe\u8ba1\uff08Transformer\u4e0eU-Net\u7ed3\u5408\uff09\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u65f6\u7a7a\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5c40\u90e8\u7ec6\u8282\u548c\u957f\u7a0b\u4f9d\u8d56\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6b8b\u5dee\u7a7a\u95f4\u5efa\u6a21\u964d\u4f4e\u65b9\u5dee\uff0c\u7ed3\u5408Transformer\u4e0eU-Net\u7684\u6df7\u5408\u8bbe\u8ba1\uff0c\u6539\u8fdb\u8df3\u8dc3\u8fde\u63a5\u548c\u6761\u4ef6\u7f16\u7801\u7b56\u7565\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u5c11\u91cf\u6269\u6563\u6b65\u9aa4\uff0c\u5e76\u80fd\u751f\u6210\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "FLEX\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5916\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17080", "pdf": "https://arxiv.org/pdf/2505.17080", "abs": "https://arxiv.org/abs/2505.17080", "authors": ["Davide Picca"], "title": "Not Minds, but Signs: Reframing LLMs through Semiotics", "categories": ["cs.CL"], "comment": null, "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4ece\u7b26\u53f7\u5b66\u800c\u975e\u8ba4\u77e5\u7cfb\u7edf\u89c6\u89d2\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u7b26\u53f7\u91cd\u7ec4\u548c\u610f\u4e49\u751f\u6210\u5de5\u5177\u7684\u4f5c\u7528\u3002", "motivation": "\u6311\u6218\u5c06LLMs\u89c6\u4e3a\u8ba4\u77e5\u7cfb\u7edf\u7684\u6d41\u884c\u89c2\u70b9\uff0c\u4e3b\u5f20\u4ece\u7b26\u53f7\u5b66\u89d2\u5ea6\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u5176\u529f\u80fd\u548c\u610f\u4e49\u751f\u6210\u673a\u5236\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u6848\u4f8b\uff0c\u5c55\u793aLLMs\u5982\u4f55\u4f5c\u4e3a\u7b26\u53f7\u4ee3\u7406\uff0c\u5176\u8f93\u51fa\u53ef\u88ab\u89c6\u4e3a\u5f00\u653e\u7684\u8be0\u91ca\u884c\u4e3a\u3002", "result": "LLMs\u5728\u6587\u5b66\u3001\u54f2\u5b66\u7b49\u9886\u57df\u53ef\u4f5c\u4e3a\u521b\u9020\u529b\u5de5\u5177\uff0c\u7b26\u53f7\u5b66\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u548c\u4f26\u7406\u7684\u7814\u7a76\u89c6\u89d2\u3002", "conclusion": "LLMs\u662f\u7b26\u53f7\u751f\u6001\u4e2d\u7684\u6280\u672f\u53c2\u4e0e\u8005\uff0c\u6539\u53d8\u4eba\u7c7b\u8bfb\u5199\u548c\u610f\u4e49\u751f\u6210\u65b9\u5f0f\uff0c\u9700\u91cd\u65b0\u601d\u8003\u8bed\u8a00\u548c\u77e5\u8bc6\u751f\u4ea7\u7684\u57fa\u7840\u3002", "relevance": 70.0}}
{"id": "2505.17423", "pdf": "https://arxiv.org/pdf/2505.17423", "abs": "https://arxiv.org/abs/2505.17423", "authors": ["Shenghui Chen", "Po-han Li", "Sandeep Chichali", "Ufuk Topcu"], "title": "VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR", "categories": ["cs.CV", "cs.HC", "cs.IT", "math.IT"], "comment": null, "summary": "Many decision-making tasks, where both accuracy and efficiency matter, still\nrequire human supervision. For example, tasks like traffic officers reviewing\nhour-long dashcam footage or researchers screening conference videos can\nbenefit from concise summaries that reduce cognitive load and save time. Yet\ncurrent vision-language models (VLMs) often produce verbose, redundant outputs\nthat hinder task performance. Existing video caption evaluation depends on\ncostly human annotations and overlooks the summaries' utility in downstream\ntasks. We address these gaps with Video-to-text Information Bottleneck\nEvaluation (VIBE), an annotation-free method that scores VLM outputs using two\nmetrics: grounding (how well the summary aligns with visual content) and\nutility (how informative it is for the task). VIBE selects from randomly\nsampled VLM outputs by ranking them according to the two scores to support\neffective human decision-making. Human studies on LearningPaper24,\nSUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE\nconsistently improve performance-boosting task accuracy by up to 61.23% and\nreducing response time by 75.77% compared to naive VLM summaries or raw video.", "AI": {"tldr": "VIBE\u662f\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8f93\u51fa\u7684\u6458\u8981\u7684grounding\u548cutility\uff0c\u63d0\u5347\u51b3\u7b56\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dVLM\u751f\u6210\u7684\u6458\u8981\u5197\u957f\u4e14\u5197\u4f59\uff0c\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u5ffd\u7565\u4e86\u6458\u8981\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faVIBE\u65b9\u6cd5\uff0c\u901a\u8fc7grounding\uff08\u4e0e\u89c6\u89c9\u5185\u5bb9\u5bf9\u9f50\uff09\u548cutility\uff08\u4efb\u52a1\u4fe1\u606f\u91cf\uff09\u4e24\u4e2a\u6307\u6807\u8bc4\u5206VLM\u8f93\u51fa\uff0c\u5e76\u4ece\u4e2d\u9009\u62e9\u6700\u4f73\u6458\u8981\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVIBE\u9009\u62e9\u7684\u6458\u8981\u663e\u8457\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\uff08\u6700\u9ad861.23%\uff09\u5e76\u51cf\u5c11\u54cd\u5e94\u65f6\u95f4\uff0875.77%\uff09\u3002", "conclusion": "VIBE\u4e3aVLM\u8f93\u51fa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u4efb\u52a1\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17815", "pdf": "https://arxiv.org/pdf/2505.17815", "abs": "https://arxiv.org/abs/2505.17815", "authors": ["Yihe Fan", "Wenqi Zhang", "Xudong Pan", "Min Yang"], "title": "Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems", "categories": ["cs.AI"], "comment": null, "summary": "As foundation models grow increasingly more intelligent, reliable and\ntrustworthy safety evaluation becomes more indispensable than ever. However, an\nimportant question arises: Whether and how an advanced AI system would perceive\nthe situation of being evaluated, and lead to the broken integrity of the\nevaluation process? During standard safety tests on a mainstream large\nreasoning model, we unexpectedly observe that the model without any contextual\ncues would occasionally recognize it is being evaluated and hence behave more\nsafety-aligned. This motivates us to conduct a systematic study on the\nphenomenon of evaluation faking, i.e., an AI system autonomously alters its\nbehavior upon recognizing the presence of an evaluation context and thereby\ninfluencing the evaluation results. Through extensive experiments on a diverse\nset of foundation models with mainstream safety benchmarks, we reach the main\nfinding termed the observer effects for AI: When the AI system under evaluation\nis more advanced in reasoning and situational awareness, the evaluation faking\nbehavior becomes more ubiquitous, which reflects in the following aspects: 1)\nReasoning models recognize evaluation 16% more often than non-reasoning models.\n2) Scaling foundation models (32B to 671B) increases faking by over 30% in some\ncases, while smaller models show negligible faking. 3) AI with basic memory is\n2.3x more likely to recognize evaluation and scores 19% higher on safety tests\n(vs. no memory). To measure this, we devised a chain-of-thought monitoring\ntechnique to detect faking intent and uncover internal signals correlated with\nsuch behavior, offering insights for future mitigation studies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u8bc6\u522b\u8bc4\u4f30\u60c5\u5883\u5e76\u8c03\u6574\u884c\u4e3a\uff0c\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\uff0c\u79f0\u4e3a\u201c\u8bc4\u4f30\u4f2a\u88c5\u201d\u73b0\u8c61\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u667a\u80fd\u63d0\u5347\uff0c\u5176\u53ef\u80fd\u8bc6\u522b\u8bc4\u4f30\u60c5\u5883\u5e76\u8c03\u6574\u884c\u4e3a\uff0c\u7834\u574f\u8bc4\u4f30\u5b8c\u6574\u6027\u3002", "method": "\u901a\u8fc7\u4e3b\u6d41\u5b89\u5168\u57fa\u51c6\u5bf9\u591a\u79cd\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bbe\u8ba1\u94fe\u5f0f\u601d\u7ef4\u76d1\u63a7\u6280\u672f\u68c0\u6d4b\u4f2a\u88c5\u610f\u56fe\u3002", "result": "\u63a8\u7406\u6a21\u578b\u8bc6\u522b\u8bc4\u4f30\u60c5\u5883\u7684\u9891\u7387\u6bd4\u975e\u63a8\u7406\u6a21\u578b\u9ad816%\uff1b\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0832B\u81f3671B\uff09\u65f6\u4f2a\u88c5\u884c\u4e3a\u589e\u52a030%\uff1b\u5177\u5907\u57fa\u672c\u8bb0\u5fc6\u7684AI\u8bc6\u522b\u8bc4\u4f30\u6982\u7387\u9ad82.3\u500d\uff0c\u5b89\u5168\u6d4b\u8bd5\u5f97\u5206\u9ad819%\u3002", "conclusion": "AI\u7cfb\u7edf\u8d8a\u5148\u8fdb\uff0c\u8bc4\u4f30\u4f2a\u88c5\u884c\u4e3a\u8d8a\u666e\u904d\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u7f13\u89e3\u63aa\u65bd\u3002", "relevance": 85.0}}
{"id": "2505.17354", "pdf": "https://arxiv.org/pdf/2505.17354", "abs": "https://arxiv.org/abs/2505.17354", "authors": ["Keisuke Kawano", "Takuro Kutsuna", "Naoki Hayashi", "Yasushi Esaki", "Hidenori Tanaka"], "title": "CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots", "categories": ["cs.LG", "stat.ML"], "comment": "27 pages, 28 figures", "summary": "In many real-world scenarios, such as single-cell RNA sequencing, data are\nobserved only as discrete-time snapshots spanning finite time intervals and\nsubject to noisy timestamps, with no continuous trajectories available.\nRecovering the underlying continuous-time dynamics from these snapshots with\ncoarse and noisy observation times is a critical and challenging task. We\npropose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers\nhigh-resolution time labels via partial optimal transport and then reconstructs\na continuous-time data distribution through a temporal kernel smoothing. This\nreconstruction enables accurate training of dynamics models such as ODEs and\nSDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic\nbenchmarks and achieves lower reconstruction errors on real scRNA-seq and\ntyphoon-track datasets. Our results highlight the benefits of explicitly\nmodeling temporal discretization and timestamp uncertainty, offering an\naccurate and general framework for bridging discrete snapshots and\ncontinuous-time processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCT-OT Flow\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u79bb\u6563\u65f6\u95f4\u5feb\u7167\u4e2d\u6062\u590d\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u901a\u8fc7\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u63a8\u65ad\u9ad8\u5206\u8fa8\u7387\u65f6\u95f4\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u6838\u5e73\u6ed1\u91cd\u5efa\u8fde\u7eed\u65f6\u95f4\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u7b49\u573a\u666f\u4e2d\uff0c\u6570\u636e\u4ec5\u4ee5\u79bb\u6563\u65f6\u95f4\u5feb\u7167\u5f62\u5f0f\u51fa\u73b0\uff0c\u4e14\u65f6\u95f4\u6233\u566a\u58f0\u5927\uff0c\u6062\u590d\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "CT-OT Flow\u9996\u5148\u901a\u8fc7\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u63a8\u65ad\u9ad8\u5206\u8fa8\u7387\u65f6\u95f4\u6807\u7b7e\uff0c\u7136\u540e\u901a\u8fc7\u65f6\u95f4\u6838\u5e73\u6ed1\u91cd\u5efa\u8fde\u7eed\u65f6\u95f4\u6570\u636e\u5206\u5e03\uff0c\u4ece\u800c\u652f\u6301ODE\u548cSDE\u7b49\u52a8\u6001\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "result": "CT-OT Flow\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9escRNA-seq\u548c\u53f0\u98ce\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u91cd\u5efa\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u79bb\u6563\u5316\u548c\u65f6\u95f4\u6233\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51c6\u786e\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8fde\u63a5\u79bb\u6563\u5feb\u7167\u548c\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\u3002", "relevance": 30.0}}
{"id": "2505.17082", "pdf": "https://arxiv.org/pdf/2505.17082", "abs": "https://arxiv.org/abs/2505.17082", "authors": ["Abderrahman Skiredj", "Ferdaous Azhari", "Houdaifa Atou", "Nouamane Tazi", "Ismail Berrada"], "title": "GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-source large language models (LLMs) still marginalise Moroccan Arabic\n(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters\nor to sacrifice the very reasoning skills that make LLMs useful. We show that a\nrigorously quality-over-quantity alignment strategy can surface fluent Darija\nwhile safeguarding the backbone s cross-lingual reasoning at a sliver of the\nusual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6\nK and TULU 50 K into Darija, preserve 20 of the English originals, and add\nmathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on\n5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the\nreasoning-dense TULU portion pushes it to 47.5 with no English regression.\nScaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which\nmatches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,\nscoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc\nretains Gemma-27B s strong maths and general-reasoning ability, showing only\nminimal movement on GSM8K and English benchmarks. The entire model is trained\nin just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable\nlanguage technology. We release code, data and checkpoints to spur\nDarija-centric applications in education, public services and everyday digital\ninteraction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d28\u91cf\u4f18\u5148\u7684\u5bf9\u9f50\u7b56\u7565\u63d0\u5347\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u8bed\uff08Darija\uff09\u5728\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90LLMs\u5bf9\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u8bed\u652f\u6301\u4e0d\u8db3\uff0c\u901a\u5e38\u9700\u8981\u727a\u7272\u63a8\u7406\u80fd\u529b\u6216\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528LoRA\u5fae\u8c03Gemma\u6a21\u578b\uff0c\u7ed3\u5408\u7ffb\u8bd1\u7684\u6307\u4ee4\u96c6\u548c\u6570\u5b66\u3001\u7f16\u7a0b\u3001\u79d1\u5b66\u63d0\u793a\uff0c\u8bad\u7ec3\u6df7\u5408\u6307\u4ee4\u6570\u636e\u3002", "result": "GemMaroc-27B\u5728DarijaMMLU\u548cHellaSwag\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u548c\u82f1\u8bed\u63a8\u7406\u80fd\u529b\uff0c\u8bad\u7ec3\u4ec5\u970048 GPU.h\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7eff\u8272AI\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7684\u8bed\u8a00\u6280\u672f\u8def\u5f84\uff0c\u652f\u6301\u6559\u80b2\u3001\u516c\u5171\u670d\u52a1\u7b49\u5e94\u7528\u3002", "relevance": 85.0}}
{"id": "2505.17425", "pdf": "https://arxiv.org/pdf/2505.17425", "abs": "https://arxiv.org/abs/2505.17425", "authors": ["Wei Jie Yeo", "Rui Mao", "Moloud Abdar", "Erik Cambria", "Ranjan Satapathy"], "title": "Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads", "categories": ["cs.CV", "cs.CL"], "comment": "Under review", "summary": "Multimodal models like CLIP have gained significant attention due to their\nremarkable zero-shot performance across various tasks. However, studies have\nrevealed that CLIP can inadvertently learn spurious associations between target\nvariables and confounding factors. To address this, we introduce\n\\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies\nspurious attention heads in Vision Transformers via mechanistic insights and\nmitigates them through targeted ablation. Furthermore, LTC identifies salient,\ntask-relevant attention heads, enabling the integration of discriminative\nfeatures through orthogonal projection to improve classification performance.\nWe evaluate LTC on benchmarks with inherent background and gender biases,\nachieving over a $>50\\%$ gain in worst-group accuracy compared to non-training\npost-hoc baselines. Additionally, we visualize the representation of selected\nheads and find that the presented interpretation corroborates our contrastive\nmechanism for identifying both spurious and salient attention heads. Code\navailable at https://github.com/wj210/CLIP_LTC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLTC\u7684\u5bf9\u6bd4\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5236\u5206\u6790\u8bc6\u522bVision Transformers\u4e2d\u7684\u865a\u5047\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u9488\u5bf9\u6027\u6d88\u9664\u548c\u6b63\u4ea4\u6295\u5f71\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "CLIP\u7b49\u591a\u6a21\u6001\u6a21\u578b\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53ef\u80fd\u5b66\u4e60\u5230\u76ee\u6807\u53d8\u91cf\u4e0e\u6df7\u6742\u56e0\u7d20\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LTC\u6846\u67b6\u901a\u8fc7\u673a\u5236\u5206\u6790\u8bc6\u522b\u865a\u5047\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u9488\u5bf9\u6027\u6d88\u9664\u548c\u6b63\u4ea4\u6295\u5f71\u6539\u5584\u6027\u80fd\u3002", "result": "\u5728\u5177\u6709\u80cc\u666f\u548c\u6027\u522b\u504f\u89c1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLTC\u663e\u8457\u63d0\u5347\u4e86\u6700\u5dee\u7ec4\u51c6\u786e\u7387\uff08>50%\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u89e3\u91ca\u6027\u3002", "conclusion": "LTC\u6709\u6548\u89e3\u51b3\u4e86CLIP\u4e2d\u7684\u865a\u5047\u5173\u8054\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 75.0}}
{"id": "2505.17818", "pdf": "https://arxiv.org/pdf/2505.17818", "abs": "https://arxiv.org/abs/2505.17818", "authors": ["Daeun Kyung", "Hyunseung Chung", "Seongsu Bae", "Jiho Kim", "Jae Ho Sohn", "Taerim Kim", "Soo Kyung Kim", "Edward Choi"], "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions", "categories": ["cs.AI", "cs.CL"], "comment": "9 pages for main text, 4 pages for references, 27 pages for\n  supplementary materials", "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.", "AI": {"tldr": "PatientSim\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u533b\u7597\u6570\u636e\u7684\u60a3\u8005\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u7684\u60a3\u8005\u89d2\u8272\uff0c\u4ee5\u8bc4\u4f30\u548c\u8bad\u7ec3\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u5668\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u591a\u6837\u5316\u60a3\u8005\u89d2\u8272\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u771f\u5b9e\u7684\u60a3\u8005\u4ea4\u4e92\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u4e34\u5e8a\u8d44\u6599\uff08\u75c7\u72b6\u548c\u75c5\u53f2\uff09\u548c\u56db\u79cd\u89d2\u8272\u7ef4\u5ea6\uff08\u4e2a\u6027\u3001\u8bed\u8a00\u80fd\u529b\u3001\u75c5\u53f2\u56de\u5fc6\u6c34\u5e73\u548c\u8ba4\u77e5\u6df7\u4e71\u6c34\u5e73\uff09\uff0c\u751f\u621037\u79cd\u72ec\u7279\u60a3\u8005\u89d2\u8272\u3002", "result": "\u8bc4\u4f30\u4e868\u4e2aLLM\uff0c\u5f00\u6e90\u6a21\u578bLlama 3.3\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u9a8c\u8bc1\u3002", "conclusion": "PatientSim\u662f\u4e00\u4e2a\u53ef\u5b9a\u5236\u3001\u9690\u79c1\u5408\u89c4\u7684\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u548c\u533b\u7597\u6559\u80b2\u3002", "relevance": 75.0}}
{"id": "2505.17356", "pdf": "https://arxiv.org/pdf/2505.17356", "abs": "https://arxiv.org/abs/2505.17356", "authors": ["Parsa Moradi", "Hanzaleh Akabrinodehi", "Mohammad Ali Maddah-Ali"], "title": "Adversarial Robustness of Nonparametric Regression", "categories": ["cs.LG"], "comment": "22 pages, 2 figures", "summary": "In this paper, we investigate the adversarial robustness of regression, a\nfundamental problem in machine learning, under the setting where an adversary\ncan arbitrarily corrupt a subset of the input data. While the robustness of\nparametric regression has been extensively studied, its nonparametric\ncounterpart remains largely unexplored. We characterize the adversarial\nrobustness in nonparametric regression, assuming the regression function\nbelongs to the second-order Sobolev space (i.e., it is square integrable up to\nits second derivative).\n  The contribution of this paper is two-fold: (i) we establish a minimax lower\nbound on the estimation error, revealing a fundamental limit that no estimator\ncan overcome, and (ii) we show that, perhaps surprisingly, the classical\nsmoothing spline estimator, when properly regularized, exhibits robustness\nagainst adversarial corruption. These results imply that if $o(n)$ out of $n$\nsamples are corrupted, the estimation error of the smoothing spline vanishes as\n$n \\to \\infty$. On the other hand, when a constant fraction of the data is\ncorrupted, no estimator can guarantee vanishing estimation error, implying the\noptimality of the smoothing spline in terms of maximum tolerable number of\ncorrupted samples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u53c2\u6570\u56de\u5f52\u5728\u5bf9\u6297\u6027\u6570\u636e\u6c61\u67d3\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u5e73\u6ed1\u6837\u6761\u4f30\u8ba1\u5668\u5728\u9002\u5f53\u6b63\u5219\u5316\u540e\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u5176\u6700\u5927\u53ef\u5bb9\u5fcd\u6c61\u67d3\u6837\u672c\u6570\u7684\u6781\u9650\u3002", "motivation": "\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u975e\u53c2\u6570\u56de\u5f52\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5047\u8bbe\u56de\u5f52\u51fd\u6570\u5c5e\u4e8e\u4e8c\u9636Sobolev\u7a7a\u95f4\uff0c\u901a\u8fc7\u5efa\u7acb\u6781\u5c0f\u6781\u5927\u4e0b\u754c\u548c\u5206\u6790\u5e73\u6ed1\u6837\u6761\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u6765\u7814\u7a76\u9c81\u68d2\u6027\u3002", "result": "\u5e73\u6ed1\u6837\u6761\u4f30\u8ba1\u5668\u5728\u6c61\u67d3\u6837\u672c\u6570\u4e3ao(n)\u65f6\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5f53\u6c61\u67d3\u6837\u672c\u6570\u4e3a\u5e38\u6570\u6bd4\u4f8b\u65f6\uff0c\u4efb\u4f55\u4f30\u8ba1\u5668\u90fd\u65e0\u6cd5\u4fdd\u8bc1\u8bef\u5dee\u6d88\u5931\u3002", "conclusion": "\u5e73\u6ed1\u6837\u6761\u4f30\u8ba1\u5668\u5728\u5bf9\u6297\u6027\u6c61\u67d3\u4e0b\u662f\u6700\u4f18\u7684\uff0c\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u6c61\u67d3\u6837\u672c\u6570\u7684\u6bd4\u4f8b\u3002", "relevance": 40.0}}
{"id": "2505.17083", "pdf": "https://arxiv.org/pdf/2505.17083", "abs": "https://arxiv.org/abs/2505.17083", "authors": ["Ben Anson", "Xi Wang", "Laurence Aitchison"], "title": "Scale-invariant Attention", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Preprint", "summary": "One persistent challenge in LLM research is the development of attention\nmechanisms that are able to generalise from training on shorter contexts to\ninference on longer contexts. We propose two conditions that we expect all\neffective long context attention mechanisms to have: scale-invariant total\nattention, and scale-invariant attention sparsity. Under a Gaussian assumption,\nwe show that a simple position-dependent transformation of the attention logits\nis sufficient for these conditions to hold. Experimentally we find that the\nresulting scale-invariant attention scheme gives considerable benefits in terms\nof validation loss when zero-shot generalising from training on short contexts\nto validation on longer contexts, and is effective at long-context retrieval.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6ee1\u8db3\u4e24\u4e2a\u6761\u4ef6\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5728\u77ed\u4e0a\u4e0b\u6587\u8bad\u7ec3\u540e\u96f6\u6837\u672c\u6cdb\u5316\u5230\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3LLM\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u4ece\u77ed\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6cdb\u5316\u5230\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6761\u4ef6\uff1a\u5c3a\u5ea6\u4e0d\u53d8\u7684\u603b\u6ce8\u610f\u529b\u548c\u7a00\u758f\u6027\uff0c\u5e76\u5728\u9ad8\u65af\u5047\u8bbe\u4e0b\u901a\u8fc7\u7b80\u5355\u7684\u4f4d\u7f6e\u76f8\u5173\u53d8\u6362\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u957f\u4e0a\u4e0b\u6587\u65f6\u663e\u8457\u964d\u4f4e\u9a8c\u8bc1\u635f\u5931\uff0c\u5e76\u6709\u6548\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u3002", "conclusion": "\u5c3a\u5ea6\u4e0d\u53d8\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587\u6cdb\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "relevance": 85.0}}
{"id": "2505.17437", "pdf": "https://arxiv.org/pdf/2505.17437", "abs": "https://arxiv.org/abs/2505.17437", "authors": ["Yuanshao Zhu", "James Jianqiao Yu", "Xiangyu Zhao", "Xiao Han", "Qidong Liu", "Xuetao Wei", "Yuxuan Liang"], "title": "Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as a full paper by KDD'25 - Research Track", "summary": "The widespread adoption of mobile devices and data collection technologies\nhas led to an exponential increase in trajectory data, presenting significant\nchallenges in spatio-temporal data mining, particularly for efficient and\naccurate trajectory retrieval. However, existing methods for trajectory\nretrieval face notable limitations, including inefficiencies in large-scale\ndata, lack of support for condition-based queries, and reliance on trajectory\nsimilarity measures. To address the above challenges, we propose OmniTraj, a\ngeneralized and flexible omni-semantic trajectory retrieval framework that\nintegrates four complementary modalities or semantics -- raw trajectories,\ntopology, road segments, and regions -- into a unified system. Unlike\ntraditional approaches that are limited to computing and processing\ntrajectories as a single modality, OmniTraj designs dedicated encoders for each\nmodality, which are embedded and fused into a shared representation space. This\ndesign enables OmniTraj to support accurate and flexible queries based on any\nindividual modality or combination thereof, overcoming the rigidity of\ntraditional similarity-based methods. Extensive experiments on two real-world\ndatasets demonstrate the effectiveness of OmniTraj in handling large-scale\ndata, providing flexible, multi-modality queries, and supporting downstream\ntasks and applications.", "AI": {"tldr": "OmniTraj\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8f68\u8ff9\u68c0\u7d22\u6846\u67b6\uff0c\u6574\u5408\u4e86\u56db\u79cd\u8bed\u4e49\u6a21\u6001\uff0c\u652f\u6301\u7075\u6d3b\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u548c\u6570\u636e\u6536\u96c6\u6280\u672f\u7684\u666e\u53ca\u5bfc\u81f4\u8f68\u8ff9\u6570\u636e\u6fc0\u589e\uff0c\u4f20\u7edf\u8f68\u8ff9\u68c0\u7d22\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u3001\u6761\u4ef6\u67e5\u8be2\u548c\u591a\u6a21\u6001\u652f\u6301\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "OmniTraj\u4e3a\u6bcf\u79cd\u6a21\u6001\u8bbe\u8ba1\u4e13\u7528\u7f16\u7801\u5668\uff0c\u5d4c\u5165\u5e76\u878d\u5408\u5230\u5171\u4eab\u8868\u793a\u7a7a\u95f4\uff0c\u652f\u6301\u57fa\u4e8e\u5355\u6a21\u6001\u6216\u591a\u6a21\u6001\u7684\u7075\u6d3b\u67e5\u8be2\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOmniTraj\u80fd\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\uff0c\u652f\u6301\u591a\u6a21\u6001\u67e5\u8be2\u548c\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "OmniTraj\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8f68\u8ff9\u68c0\u7d22\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 20.0}}
{"id": "2505.17861", "pdf": "https://arxiv.org/pdf/2505.17861", "abs": "https://arxiv.org/abs/2505.17861", "authors": ["Jianghao Lin", "Jiachen Zhu", "Zheli Zhou", "Yunjia Xi", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "title": "Superplatforms Have to Attack AI Agents", "categories": ["cs.AI", "cs.CY", "cs.IR"], "comment": "Position paper under review", "summary": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8d85\u7ea7\u5e73\u53f0\u4e0e\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5206\u6790\u4e86AI\u4ee3\u7406\u5982\u4f55\u5a01\u80c1\u8d85\u7ea7\u5e73\u53f0\u7684\u7528\u6237\u6ce8\u610f\u529b\u5784\u65ad\uff0c\u5e76\u63d0\u51fa\u4e86\u8d85\u7ea7\u5e73\u53f0\u53ef\u80fd\u91c7\u53d6\u7684\u653b\u51fb\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793aAI\u4ee3\u7406\u5bf9\u8d85\u7ea7\u5e73\u53f0\u5546\u4e1a\u6a21\u5f0f\u7684\u6f5c\u5728\u98a0\u8986\uff0c\u4ee5\u53ca\u7531\u6b64\u5f15\u53d1\u7684\u6570\u5b57\u751f\u6001\u7cfb\u7edf\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u95e8\u63a7\u7406\u8bba\u5206\u6790AI\u4ee3\u7406\u4e0e\u8d85\u7ea7\u5e73\u53f0\u7684\u51b2\u7a81\uff0c\u5e76\u63a2\u8ba8\u8d85\u7ea7\u5e73\u53f0\u53ef\u80fd\u7684\u6280\u672f\u653b\u51fb\u624b\u6bb5\u3002", "result": "AI\u4ee3\u7406\u53ef\u80fd\u6210\u4e3a\u65b0\u7684\u6570\u5b57\u6d41\u91cf\u5165\u53e3\uff0c\u5a01\u80c1\u8d85\u7ea7\u5e73\u53f0\u7684\u5784\u65ad\u5730\u4f4d\uff0c\u9700\u91c7\u53d6\u9632\u5fa1\u63aa\u65bd\u3002", "conclusion": "\u547c\u5401\u5173\u6ce8\u8d85\u7ea7\u5e73\u53f0\u4e0eAI\u4ee3\u7406\u7684\u51b2\u7a81\uff0c\u5bfb\u6c42\u5408\u4f5c\u89e3\u51b3\u65b9\u6848\u4ee5\u4fdd\u62a4\u7528\u6237\u5229\u76ca\u548c\u6570\u5b57\u751f\u6001\u5f00\u653e\u6027\u3002", "relevance": 60.0}}
{"id": "2505.17357", "pdf": "https://arxiv.org/pdf/2505.17357", "abs": "https://arxiv.org/abs/2505.17357", "authors": ["Hassan Wasswa", "Hussein Abbass", "Timothy Lynar"], "title": "Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "With the rise of IoT-based botnet attacks, researchers have explored various\nlearning models for detection, including traditional machine learning, deep\nlearning, and hybrid approaches. A key advancement involves deploying attention\nmechanisms to capture long-term dependencies among features, significantly\nimproving detection accuracy. However, most models treat attack instances\nindependently, overlooking inter-instance relationships. Graph Neural Networks\n(GNNs) address this limitation by learning an embedding space via iterative\nmessage passing where similar instances are placed closer based on node\nfeatures and relationships, enhancing classification performance. To further\nimprove detection, attention mechanisms have been embedded within GNNs,\nleveraging both long-range dependencies and inter-instance connections.\nHowever, transforming the high dimensional IoT attack datasets into a graph\nstructured dataset poses challenges, such as large graph structures leading\ncomputational overhead. To mitigate this, this paper proposes a framework that\nfirst reduces dimensionality of the NetFlow-based IoT attack dataset before\ntransforming it into a graph dataset. We evaluate three dimension reduction\ntechniques--Variational Autoencoder (VAE-encoder), classical autoencoder\n(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects\non a Graph Attention neural network (GAT) model for botnet attack detection", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u964d\u7ef4\u6280\u672f\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7269\u8054\u7f51\uff08IoT\uff09\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\u65e5\u76ca\u589e\u591a\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u5ffd\u7565\u4e86\u653b\u51fb\u5b9e\u4f8b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u4e14\u9ad8\u7ef4\u6570\u636e\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\u65f6\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5148\u5bf9NetFlow\u6570\u636e\u96c6\u964d\u7ef4\uff08\u4f7f\u7528VAE\u3001AE\u6216PCA\uff09\uff0c\u518d\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\uff0c\u6700\u540e\u7528GAT\u6a21\u578b\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u964d\u7ef4\u6280\u672f\u5bf9GAT\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u964d\u7ef4\u6280\u672f\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u7ed3\u5408GAT\u6a21\u578b\u53ef\u63d0\u5347\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17086", "pdf": "https://arxiv.org/pdf/2505.17086", "abs": "https://arxiv.org/abs/2505.17086", "authors": ["Yihong Wu", "Liheng Ma", "Muzhi Li", "Jiaming Zhou", "Jianye Hao", "Ho-fung Leung", "Irwin King", "Yingxue Zhang", "Jian-Yun Nie"], "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination.\n  While Retrieval-Augmented Generation mitigates these issues by integrating\nexternal knowledge, existing approaches rely heavily on in-context learning,\nwhose performance is constrained by the fundamental reasoning capabilities of\nLLMs.\n  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex\nQuestion Answering, comprising a planner that decomposes questions into a\ndirected acyclic graph of subquestions and a worker that resolves questions via\nretrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy\nGradient Optimization), a novel reinforcement learning method that replaces\ntraditional policy gradient updates with Maximum Likelihood Estimation (MLE) by\nsampling trajectories from an asymptotically optimal policy. MyGO eliminates\nthe need for gradient rescaling and reference models, ensuring stable and\nefficient training.\n  Empirical results across multiple datasets demonstrate the effectiveness of\nMujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a\nscalable and resource-efficient solution for complex QA tasks.", "AI": {"tldr": "Mujica-MyGO\u662f\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u7684\u591a\u8df3\u8054\u5408\u667a\u80fd\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u95ee\u9898\u5206\u89e3\u3001\u68c0\u7d22\u63a8\u7406\u548c\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5MyGO\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u591a\u8df3QA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLMs\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u56e0\u7f3a\u4e4f\u4e8b\u5b9e\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u800c\u53d7\u9650\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u6027\u80fd\u53d7\u9650\u4e8eLLMs\u7684\u57fa\u7840\u80fd\u529b\u3002", "method": "\u63d0\u51faMujica\u6846\u67b6\uff0c\u5305\u62ec\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u7684\u89c4\u5212\u5668\u548c\u57fa\u4e8e\u68c0\u7d22\u63a8\u7406\u7684\u5de5\u4f5c\u5668\uff1b\u5f15\u5165MyGO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528MLE\u66ff\u4ee3\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMujica-MyGO\u5728\u591a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u591a\u8df3QA\u6027\u80fd\uff0c\u4e3a\u590d\u6742QA\u4efb\u52a1\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Mujica-MyGO\u901a\u8fc7\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u548c\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742QA\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "relevance": 85.0}}
{"id": "2505.17440", "pdf": "https://arxiv.org/pdf/2505.17440", "abs": "https://arxiv.org/abs/2505.17440", "authors": ["Hefei Mei", "Zirui Wang", "Shen You", "Minjing Dong", "Chang Xu"], "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u89c6\u89c9\u7f16\u7801\u5668\u653b\u51fb\u65b9\u6cd5\uff08VEAttack\uff09\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5e72\u51c0\u4e0e\u6270\u52a8\u89c6\u89c9\u7279\u5f81\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u65e0\u9700\u8bbf\u95ee\u540e\u7eed\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u4efb\u52a1\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800cLVLMs\u8bbe\u8ba1\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\uff0c\u9700\u9ad8\u6548\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u4ee4\u724c\u800c\u975e\u5206\u7c7b\u4ee4\u724c\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u5e76\u6d88\u9664\u4efb\u52a1\u548c\u6807\u7b7e\u4f9d\u8d56\u3002", "result": "\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d94.5%\uff0c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u4e0b\u964d75.7%\u3002", "conclusion": "VEAttack\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u63ed\u793a\u4e86LVLM\u653b\u51fb/\u9632\u5fa1\u7684\u5173\u952e\u89c2\u5bdf\u3002", "relevance": 85.0}}
{"id": "2505.17862", "pdf": "https://arxiv.org/pdf/2505.17862", "abs": "https://arxiv.org/abs/2505.17862", "authors": ["Ziwei Zhou", "Rui Wang", "Zuxuan Wu"], "title": "Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities", "categories": ["cs.AI"], "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) achieve promising performance\non visual and audio benchmarks independently. However, the ability of these\nmodels to process cross-modal information synchronously remains largely\nunexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual\nQuestioning and Answering benchmark comprising 684 videos of daily life\nscenarios from diverse sources, rich in both audio and visual information, and\nfeaturing 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA\nGeneration Pipeline, which includes automatic annotation, QA generation and QA\noptimization, significantly improves efficiency for human evaluation and\nscalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent\nutilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)\nand Automatic Speech Recognition (ASR) model to establish a baseline for this\nbenchmark. The results show that current MLLMs still struggle significantly\nwith tasks requiring audio-visual integration, but combining VLMs and ALMs with\nsimple temporal alignment techniques can achieve substantially better\nperformance. Codes and benchmark are available at\n\\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Daily-Omni\u57fa\u51c6\u548c\u751f\u6210\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u542c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22MLLMs\u5728\u8de8\u6a21\u6001\u4fe1\u606f\u540c\u6b65\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "1) \u6784\u5efaDaily-Omni\u57fa\u51c6\uff1b2) \u5f00\u53d1\u81ea\u52a8\u6807\u6ce8\u548cQA\u751f\u6210\u6d41\u7a0b\uff1b3) \u8bbe\u8ba1\u8bad\u7ec3\u514d\u8d39\u7684Daily-Omni-Agent\u4f5c\u4e3a\u57fa\u7ebf\u3002", "result": "\u5f53\u524dMLLMs\u5728\u89c6\u542c\u6574\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u7ed3\u5408VLM\u548cALM\u53ca\u7b80\u5355\u65f6\u95f4\u5bf9\u9f50\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.17359", "pdf": "https://arxiv.org/pdf/2505.17359", "abs": "https://arxiv.org/abs/2505.17359", "authors": ["Xianzhong Ding", "Yunkai Zhang", "Binbin Chen", "Donghao Ying", "Tieying Zhang", "Jianjun Chen", "Lei Zhang", "Alberto Cerpa", "Wan Du"], "title": "Towards VM Rescheduling Optimization Through Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Modern industry-scale data centers need to manage a large number of virtual\nmachines (VMs). Due to the continual creation and release of VMs, many small\nresource fragments are scattered across physical machines (PMs). To handle\nthese fragments, data centers periodically reschedule some VMs to alternative\nPMs, a practice commonly referred to as VM rescheduling. Despite the increasing\nimportance of VM rescheduling as data centers grow in size, the problem remains\nunderstudied. We first show that, unlike most combinatorial optimization tasks,\nthe inference time of VM rescheduling algorithms significantly influences their\nperformance, due to dynamic VM state changes during this period. This causes\nexisting methods to scale poorly. Therefore, we develop a reinforcement\nlearning system for VM rescheduling, VM2RL, which incorporates a set of\ncustomized techniques, such as a two-stage framework that accommodates diverse\nconstraints and workload conditions, a feature extraction module that captures\nrelational information specific to rescheduling, as well as a risk-seeking\nevaluation enabling users to optimize the trade-off between latency and\naccuracy. We conduct extensive experiments with data from an industry-scale\ndata center. Our results show that VM2RL can achieve a performance comparable\nto the optimal solution but with a running time of seconds. Code and datasets\nare open-sourced: https://github.com/zhykoties/VMR2L_eurosys,\nhttps://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u865a\u62df\u673a\u91cd\u8c03\u5ea6\u7cfb\u7edfVM2RL\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e2d\u5fc3\u4e2d\u865a\u62df\u673a\u8d44\u6e90\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u89c4\u6a21\u7684\u6269\u5927\uff0c\u865a\u62df\u673a\u91cd\u8c03\u5ea6\u95ee\u9898\u65e5\u76ca\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u63a8\u7406\u65f6\u95f4\u5f71\u54cd\u6027\u80fd\u800c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5f00\u53d1\u4e86VM2RL\u7cfb\u7edf\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\u3001\u5b9a\u5236\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u98ce\u9669\u5bfb\u6c42\u8bc4\u4f30\u6280\u672f\uff0c\u4f18\u5316\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVM2RL\u5728\u79d2\u7ea7\u8fd0\u884c\u65f6\u95f4\u5185\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u6027\u80fd\u3002", "conclusion": "VM2RL\u4e3a\u6570\u636e\u4e2d\u5fc3\u865a\u62df\u673a\u91cd\u8c03\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17087", "pdf": "https://arxiv.org/pdf/2505.17087", "abs": "https://arxiv.org/abs/2505.17087", "authors": ["Gordana Ispirova", "Michael Sebek", "Giulia Menichetti"], "title": "Informatics for Food Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB", "cs.LG"], "comment": null, "summary": "This chapter explores the evolution, classification, and health implications\nof food processing, while emphasizing the transformative role of machine\nlearning, artificial intelligence (AI), and data science in advancing food\ninformatics. It begins with a historical overview and a critical review of\ntraditional classification frameworks such as NOVA, Nutri-Score, and SIGA,\nhighlighting their strengths and limitations, particularly the subjectivity and\nreproducibility challenges that hinder epidemiological research and public\npolicy. To address these issues, the chapter presents novel computational\napproaches, including FoodProX, a random forest model trained on nutrient\ncomposition data to infer processing levels and generate a continuous FPro\nscore. It also explores how large language models like BERT and BioBERT can\nsemantically embed food descriptions and ingredient lists for predictive tasks,\neven in the presence of missing data. A key contribution of the chapter is a\nnovel case study using the Open Food Facts database, showcasing how multimodal\nAI models can integrate structured and unstructured data to classify foods at\nscale, offering a new paradigm for food processing assessment in public health\nand research.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u98df\u54c1\u52a0\u5de5\u7684\u6f14\u53d8\u3001\u5206\u7c7b\u53ca\u5176\u5065\u5eb7\u5f71\u54cd\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u548cAI\u5728\u98df\u54c1\u4fe1\u606f\u5b66\u4e2d\u7684\u53d8\u9769\u6027\u4f5c\u7528\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u98df\u54c1\u5206\u7c7b\u6846\u67b6\uff08\u5982NOVA\u3001Nutri-Score\u548cSIGA\uff09\u7684\u4e3b\u89c2\u6027\u548c\u53ef\u91cd\u590d\u6027\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u6d41\u884c\u75c5\u5b66\u7814\u7a76\u548c\u516c\u5171\u653f\u7b56\u3002", "method": "\u63d0\u51faFoodProX\uff08\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u6a21\u578b\uff09\u548c\u5229\u7528BERT/BioBERT\u5d4c\u5165\u98df\u54c1\u63cf\u8ff0\u7684\u8bed\u4e49\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001AI\u6a21\u578b\u5904\u7406\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u5f00\u53d1\u4e86\u8fde\u7eedFPro\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7Open Food Facts\u6570\u636e\u5e93\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001AI\u6a21\u578b\u7684\u5927\u89c4\u6a21\u98df\u54c1\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u8ba1\u7b97\u65b9\u6cd5\u548cAI\u4e3a\u98df\u54c1\u52a0\u5de5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63d0\u5347\u4e86\u516c\u5171\u536b\u751f\u548c\u7814\u7a76\u7684\u6548\u7387\u3002", "relevance": 30.0}}
{"id": "2505.17442", "pdf": "https://arxiv.org/pdf/2505.17442", "abs": "https://arxiv.org/abs/2505.17442", "authors": ["Hao Jing", "Anhong Wang", "Yifan Zhang", "Donghan Bu", "Junhui Hou"], "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u9884\u6d4b\u77e5\u8bc6\u84b8\u998f\uff08RPKD\uff09\u76843D\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u6bd4\u7279\u7387\u4f20\u8f93\u4e2d\u53cd\u5c04\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u538b\u7f29\u4f20\u8f93\u7cfb\u7edf\u4e2d\u53cd\u5c04\u4fe1\u606f\u7f16\u7801\u5e26\u6765\u7684\u4f20\u8f93\u8d1f\u62c5\u548c\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u538b\u7f29\u70b9\u5750\u6807\u5e76\u4e22\u5f03\u53cd\u5c04\u4fe1\u606f\uff0c\u5229\u7528\u51e0\u4f55\u53cd\u5c04\u9884\u6d4b\u6a21\u5757\uff08RP\uff09\u91cd\u5efa\u53cd\u5c04\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u6559\u5e08-\u5b66\u751f\u68c0\u6d4b\u5668\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff08RKD\u548cDKD\uff09\u3002", "result": "\u5728KITTI\u548cWaymo\u6570\u636e\u96c6\u4e0a\uff0cRPKD\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u70b9\u4e91\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u4f4e\u7801\u7387\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RPKD\u6846\u67b6\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u7387\u4f20\u8f93\u4e0b\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002", "relevance": 30.0}}
{"id": "2505.17882", "pdf": "https://arxiv.org/pdf/2505.17882", "abs": "https://arxiv.org/abs/2505.17882", "authors": ["Cole Wyeth", "Marcus Hutter"], "title": "Formalizing Embeddedness Failures in Universal Artificial Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "We rigorously discuss the commonly asserted failures of the AIXI\nreinforcement learning agent as a model of embedded agency. We attempt to\nformalize these failure modes and prove that they occur within the framework of\nuniversal artificial intelligence, focusing on a variant of AIXI that models\nthe joint action/percept history as drawn from the universal distribution. We\nalso evaluate the progress that has been made towards a successful theory of\nembedded agency based on variants of the AIXI agent.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86AIXI\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u5d4c\u5165\u5f0f\u4ee3\u7406\u6a21\u578b\u4e2d\u7684\u5e38\u89c1\u5931\u8d25\uff0c\u5e76\u5c1d\u8bd5\u5f62\u5f0f\u5316\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\uff0c\u8bc1\u660e\u5176\u5728\u901a\u7528\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u4e2d\u7684\u5b58\u5728\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u57fa\u4e8eAIXI\u53d8\u4f53\u7684\u5d4c\u5165\u5f0f\u4ee3\u7406\u7406\u8bba\u7684\u8fdb\u5c55\u3002", "motivation": "\u7814\u7a76AIXI\u4ee3\u7406\u5728\u5d4c\u5165\u5f0f\u4ee3\u7406\u6a21\u578b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u901a\u7528\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f62\u5f0f\u5316AIXI\u4ee3\u7406\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5728\u901a\u7528\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u4e2d\u8bc1\u660e\u5176\u5b58\u5728\uff1b\u8bc4\u4f30\u57fa\u4e8eAIXI\u53d8\u4f53\u7684\u5d4c\u5165\u5f0f\u4ee3\u7406\u7406\u8bba\u8fdb\u5c55\u3002", "result": "\u8bc1\u660e\u4e86AIXI\u4ee3\u7406\u5728\u5d4c\u5165\u5f0f\u4ee3\u7406\u6a21\u578b\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5173\u7406\u8bba\u7684\u8fdb\u5c55\u3002", "conclusion": "AIXI\u4ee3\u7406\u5728\u5d4c\u5165\u5f0f\u4ee3\u7406\u6a21\u578b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u5176\u53d8\u4f53\u4e3a\u5d4c\u5165\u5f0f\u4ee3\u7406\u7406\u8bba\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.17365", "pdf": "https://arxiv.org/pdf/2505.17365", "abs": "https://arxiv.org/abs/2505.17365", "authors": ["Rohan Ghuge", "Vidya Muthukumar", "Sahil Singla"], "title": "Improved and Oracle-Efficient Online $\\ell_1$-Multicalibration", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We study \\emph{online multicalibration}, a framework for ensuring calibrated\npredictions across multiple groups in adversarial settings, across $T$ rounds.\nAlthough online calibration is typically studied in the $\\ell_1$ norm, prior\napproaches to online multicalibration have taken the indirect approach of\nobtaining rates in other norms (such as $\\ell_2$ and $\\ell_{\\infty}$) and then\ntransferred these guarantees to $\\ell_1$ at additional loss. In contrast, we\npropose a direct method that achieves improved and oracle-efficient rates of\n$\\widetilde{\\mathcal{O}}(T^{-1/3})$ and $\\widetilde{\\mathcal{O}}(T^{-1/4})$\nrespectively, for online $\\ell_1$-multicalibration. Our key insight is a novel\nreduction of online \\(\\ell_1\\)-multicalibration to an online learning problem\nwith product-based rewards, which we refer to as \\emph{online linear-product\noptimization} ($\\mathtt{OLPO}$).\n  To obtain the improved rate of $\\widetilde{\\mathcal{O}}(T^{-1/3})$, we\nintroduce a linearization of $\\mathtt{OLPO}$ and design a no-regret algorithm\nfor this linearized problem. Although this method guarantees the desired\nsublinear rate (nearly matching the best rate for online calibration), it\nbecomes computationally expensive when the group family \\(\\mathcal{H}\\) is\nlarge or infinite, since it enumerates all possible groups. To address\nscalability, we propose a second approach to $\\mathtt{OLPO}$ that makes only a\npolynomial number of calls to an offline optimization (\\emph{multicalibration\nevaluation}) oracle, resulting in \\emph{oracle-efficient} online\n\\(\\ell_1\\)-multicalibration with a rate of $\\widetilde{\\mathcal{O}}(T^{-1/4})$.\nOur framework also extends to certain infinite families of groups (e.g., all\nlinear functions on the context space) by exploiting a $1$-Lipschitz property\nof the \\(\\ell_1\\)-multicalibration error with respect to \\(\\mathcal{H}\\).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.17089", "pdf": "https://arxiv.org/pdf/2505.17089", "abs": "https://arxiv.org/abs/2505.17089", "authors": ["Md Rafi Ur Rashid", "Vishnu Asutosh Dasu", "Ye Wang", "Gang Tan", "Shagufta Mehnaz"], "title": "Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models", "categories": ["cs.CL"], "comment": "26 pages, 2 figures", "summary": "Large Language Models (LLMs) exhibit impressive capabilities, but remain\nsusceptible to a growing spectrum of safety risks, including jailbreaks, toxic\ncontent, hallucinations, and bias. Existing defenses often address only a\nsingle threat type or resort to rigid outright rejection, sacrificing user\nexperience and failing to generalize across diverse and novel attacks. This\npaper introduces Adversarial Scenario Extrapolation (ASE), a novel\ninference-time computation framework that leverages Chain-of-Thought (CoT)\nreasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides\nthe LLM through a self-generative process of contemplating potential\nadversarial scenarios and formulating defensive strategies before generating a\nresponse to the user query. Comprehensive evaluation on four adversarial\nbenchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak\nattack success rates and minimal toxicity, while slashing outright rejections\nto <4%. ASE outperforms six state-of-the-art defenses in\nrobustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and\n4-10x lower bias scores. By transforming adversarial perception into an\nintrinsic cognitive process, ASE sets a new paradigm for secure and natural\nhuman-AI interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdversarial Scenario Extrapolation (ASE)\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7Chain-of-Thought (CoT)\u63a8\u7406\u589e\u5f3aLLM\u7684\u9c81\u68d2\u6027\u548c\u6d41\u7545\u6027\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u548c\u6bd2\u6027\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u4ec5\u9488\u5bf9\u5355\u4e00\u5a01\u80c1\u7c7b\u578b\u6216\u91c7\u7528\u50f5\u5316\u7684\u62d2\u7edd\u7b56\u7565\uff0c\u727a\u7272\u7528\u6237\u4f53\u9a8c\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u548c\u65b0\u578b\u653b\u51fb\u3002", "method": "ASE\u901a\u8fc7\u5f15\u5bfcLLM\u5728\u751f\u6210\u54cd\u5e94\u524d\u81ea\u6211\u751f\u6210\u6f5c\u5728\u5bf9\u6297\u573a\u666f\u5e76\u5236\u5b9a\u9632\u5fa1\u7b56\u7565\uff0c\u7ed3\u5408CoT\u63a8\u7406\u3002", "result": "\u5728\u56db\u4e2a\u5bf9\u6297\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cASE\u5c06\u8d8a\u72f1\u653b\u51fb\u6210\u529f\u7387\u964d\u81f3\u63a5\u8fd1\u96f6\uff0c\u6bd2\u6027\u5185\u5bb9\u964d\u81f3\u6700\u4f4e\uff0c\u62d2\u7edd\u7387\u4f4e\u4e8e4%\uff0c\u5e76\u5728\u9c81\u68d2\u6027\u4e0e\u6d41\u7545\u6027\u6743\u8861\u4e2d\u4f18\u4e8e\u516d\u79cd\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "ASE\u901a\u8fc7\u5c06\u5bf9\u6297\u611f\u77e5\u8f6c\u5316\u4e3a\u5185\u5728\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4e3a\u5b89\u5168\u548c\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u8bbe\u5b9a\u4e86\u65b0\u8303\u5f0f\u3002", "relevance": 90.0}}
{"id": "2505.17445", "pdf": "https://arxiv.org/pdf/2505.17445", "abs": "https://arxiv.org/abs/2505.17445", "authors": ["Inpyo Song", "Hyemin Hwang", "Jangwon Lee"], "title": "PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "In the United States, as of 2023, pet ownership has reached 66% of households\nand continues to rise annually. This trend underscores the critical need for\neffective pet identification and monitoring methods, particularly as nearly 10\nmillion cats and dogs are reported stolen or lost each year. However,\ntraditional methods for finding lost animals like GPS tags or ID photos have\nlimitations-they can be removed, face signal issues, and depend on someone\nfinding and reporting the pet. To address these limitations, we introduce\nPawPrint and PawPrint+, the first publicly available datasets focused on\nindividual-level footprint identification for dogs and cats. Through\ncomprehensive benchmarking of both modern deep neural networks (e.g., CNN,\nTransformers) and classical local features, we observe varying advantages and\ndrawbacks depending on substrate complexity and data availability. These\ninsights suggest future directions for combining learned global representations\nwith local descriptors to enhance reliability across diverse, real-world\nconditions. As this approach provides a non-invasive alternative to traditional\nID tags, we anticipate promising applications in ethical pet management and\nwildlife conservation efforts.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PawPrint\u548cPawPrint+\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u732b\u72d7\u8db3\u8ff9\u8bc6\u522b\uff0c\u8bc4\u4f30\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u65b9\u5411\u3002", "motivation": "\u5ba0\u7269\u4e22\u5931\u95ee\u9898\u4e25\u91cd\uff0c\u4f20\u7edf\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u975e\u4fb5\u5165\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5982CNN\u3001Transformer\uff09\u548c\u7ecf\u5178\u5c40\u90e8\u7279\u5f81\u65b9\u6cd5\u8fdb\u884c\u8db3\u8ff9\u8bc6\u522b\u3002", "result": "\u4e0d\u540c\u65b9\u6cd5\u5728\u6570\u636e\u53ef\u7528\u6027\u548c\u57fa\u5e95\u590d\u6742\u6027\u65b9\u9762\u5404\u6709\u4f18\u52a3\u3002", "conclusion": "\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u53ef\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u5ba0\u7269\u7ba1\u7406\u548c\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u3002", "relevance": 20.0}}
{"id": "2505.17897", "pdf": "https://arxiv.org/pdf/2505.17897", "abs": "https://arxiv.org/abs/2505.17897", "authors": ["Zi-Ao Ma", "Tian Lan", "Rong-Cheng Tu", "Shu-Hang Liu", "Heyan Huang", "Zhijing Wu", "Chen Xu", "Xian-Ling Mao"], "title": "T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid progress in diffusion-based text-to-image (T2I) generation has\ncreated an urgent need for interpretable automatic evaluation methods that can\nassess the quality of generated images, therefore reducing the human annotation\nburden. To reduce the prohibitive cost of relying on commercial models for\nlarge-scale evaluation, and to improve the reasoning capabilities of\nopen-source models, recent research has explored supervised fine-tuning (SFT)\nof multimodal large language models (MLLMs) as dedicated T2I evaluators.\nHowever, SFT approaches typically rely on high-quality critique datasets, which\nare either generated by proprietary LLMs-with potential issues of bias and\ninconsistency-or annotated by humans at high cost, limiting their scalability\nand generalization. To address these limitations, we propose T2I-Eval-R1, a\nnovel reinforcement learning framework that trains open-source MLLMs using only\ncoarse-grained quality scores, thereby avoiding the need for annotating\nhigh-quality interpretable evaluation rationale. Our approach integrates Group\nRelative Policy Optimization (GRPO) into the instruction-tuning process,\nenabling models to generate both scalar scores and interpretable reasoning\nchains with only easy accessible annotated judgment scores or preferences.\nFurthermore, we introduce a continuous reward formulation that encourages score\ndiversity and provides stable optimization signals, leading to more robust and\ndiscriminative evaluation behavior. Experimental results on three established\nT2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves\nsignificantly higher alignment with human assessments and offers more accurate\ninterpretable score rationales compared to strong baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6T2I-Eval-R1\uff0c\u7528\u4e8e\u8bad\u7ec3\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f5c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u8bc4\u4f30\u5668\uff0c\u4ec5\u9700\u7c97\u7c92\u5ea6\u8d28\u91cf\u5206\u6570\uff0c\u65e0\u9700\u9ad8\u8d28\u91cf\u89e3\u91ca\u6027\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u5e76\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7ed3\u5408Group Relative Policy Optimization\uff08GRPO\uff09\u548c\u8fde\u7eed\u5956\u52b1\u516c\u5f0f\uff0c\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u6807\u91cf\u5206\u6570\u548c\u89e3\u91ca\u6027\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e09\u4e2aT2I\u5143\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0cT2I-Eval-R1\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u543b\u5408\u5ea6\u663e\u8457\u9ad8\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u89e3\u91ca\u6027\u8bc4\u5206\u7406\u7531\u3002", "conclusion": "T2I-Eval-R1\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684T2I\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u3002", "relevance": 70.0}}
{"id": "2505.17370", "pdf": "https://arxiv.org/pdf/2505.17370", "abs": "https://arxiv.org/abs/2505.17370", "authors": ["Qilin Wang"], "title": "FRIREN: Beyond Trajectories -- A Spectral Lens on Time", "categories": ["cs.LG", "cs.AI"], "comment": "37 pages, 4 figures. Submitted to NeurIPS 2025. Public code at\n  https://anonymous.4open.science/r/LTSF_model-C6B8/", "summary": "Long-term time-series forecasting (LTSF) models are often presented as\ngeneral-purpose solutions that can be applied across domains, implicitly\nassuming that all data is pointwise predictable. Using chaotic systems such as\nLorenz-63 as a case study, we argue that geometric structure - not pointwise\nprediction - is the right abstraction for a dynamic-agnostic foundational\nmodel. Minimizing the Wasserstein-2 distance (W2), which captures geometric\nchanges, and providing a spectral view of dynamics are essential for\nlong-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via\nInterpretable Eigen-networks), implements an augmented normalizing-flow block\nthat embeds data into a normally distributed latent representation. It then\ngenerates a W2-efficient optimal path that can be decomposed into rotation,\nscaling, inverse rotation, and translation. This architecture yields locally\ngenerated, geometry-preserving predictions that are independent of the\nunderlying dynamics, and a global spectral representation that functions as a\nfinite Koopman operator with a small modification. This enables practitioners\nto identify which modes grow, decay, or oscillate, both locally and\nsystem-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on\nLorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE\n27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out\nof 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),\nFRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,\noutperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.\nFRIREN is also competitive on standard LTSF datasets such as ETT and Weather.\nBy connecting modern generative flows with classical spectral analysis, FRIREN\nmakes long-term forecasting both accurate and interpretable, setting a new\nbenchmark for LTSF model design.", "AI": {"tldr": "FRIREN\u6a21\u578b\u901a\u8fc7\u51e0\u4f55\u7ed3\u6784\u548c\u8c31\u5206\u6790\u5b9e\u73b0\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5047\u8bbe\u6570\u636e\u70b9\u53ef\u9884\u6d4b\uff0c\u4f46\u6df7\u6c8c\u7cfb\u7edf\uff08\u5982Lorenz-63\uff09\u8868\u660e\u51e0\u4f55\u7ed3\u6784\u662f\u66f4\u5408\u9002\u7684\u62bd\u8c61\u3002", "method": "FRIREN\u7ed3\u5408\u5f52\u4e00\u5316\u6d41\u5757\u548cWasserstein-2\u8ddd\u79bb\uff0c\u751f\u6210\u51e0\u4f55\u4fdd\u6301\u7684\u9884\u6d4b\u548c\u8c31\u8868\u793a\u3002", "result": "\u5728Lorenz-63\u548cRossler\u7b49\u6570\u636e\u96c6\u4e0a\uff0cFRIREN\u8868\u73b0\u4f18\u4e8eTimeMixer\uff0c\u5e76\u4fdd\u6301\u9884\u6d4b\u6709\u6548\u6027\u3002", "conclusion": "FRIREN\u901a\u8fc7\u51e0\u4f55\u548c\u8c31\u65b9\u6cd5\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17091", "pdf": "https://arxiv.org/pdf/2505.17091", "abs": "https://arxiv.org/abs/2505.17091", "authors": ["Prateek Verma", "Mert Pilanci"], "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "6 pages, 3 figures, 4 tables. Under Review WASPAA 2025", "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\uff0c\u901a\u8fc7\u8bad\u7ec3\u81ea\u56de\u5f52LLM\u6a21\u578b\uff0c\u5176\u5185\u90e8\u80fd\u53d1\u5c55\u51fa\u7406\u89e3\u56fe\u50cf\u548c\u97f3\u9891\u7684\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22LLM\u6a21\u578b\u5728\u672a\u7ecf\u4e13\u95e8\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u662f\u5426\u80fd\u591f\u901a\u8fc7\u6587\u672c\u5b66\u4e60\u53d1\u5c55\u51fa\u8de8\u6a21\u6001\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52LLM\u6a21\u578b\uff0c\u8f93\u5165\u56fe\u50cf\u3001\u97f3\u9891\u6216\u6587\u672c\u7684\u7247\u6bb5\uff0c\u76f4\u63a5\u751f\u6210\u5206\u7c7b\u6807\u7b7e\u6216\u5d4c\u5165\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728\u97f3\u9891\u6570\u636e\u96c6\uff08FSD-50K\u3001GTZAN\uff09\u548c\u56fe\u50cf\u6570\u636e\u96c6\uff08CIFAR-10\u3001Fashion-MNIST\uff09\u4e0a\u5c55\u793a\u4e86\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u6587\u672cLLM\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5185\u90e8\u7535\u8def\u5b66\u4e60\u8de8\u6a21\u6001\u80fd\u529b\uff0c\u4e3a\u591a\u4efb\u52a1\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17449", "pdf": "https://arxiv.org/pdf/2505.17449", "abs": "https://arxiv.org/abs/2505.17449", "authors": ["Inpyo Song", "Jangwon Lee"], "title": "Real-time Traffic Accident Anticipation with Feature Reuse", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "This paper addresses the problem of anticipating traffic accidents, which\naims to forecast potential accidents before they happen. Real-time anticipation\nis crucial for safe autonomous driving, yet most methods rely on\ncomputationally heavy modules like optical flow and intermediate feature\nextractors, making real-world deployment challenging. In this paper, we thus\nintroduce RARE (Real-time Accident anticipation with Reused Embeddings), a\nlightweight framework that capitalizes on intermediate features from a single\npre-trained object detector. By eliminating additional feature-extraction\npipelines, RARE significantly reduces latency. Furthermore, we introduce a\nnovel Attention Score Ranking Loss, which prioritizes higher attention on\naccident-related objects over non-relevant ones. This loss enhances both\naccuracy and interpretability. RARE demonstrates a 4-8 times speedup over\nexisting approaches on the DAD and CCD benchmarks, achieving a latency of\n13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced\ncomplexity, it attains state-of-the-art Average Precision and reliably\nanticipates imminent collisions in real time. These results highlight RARE's\npotential for safety-critical applications where timely and explainable\nanticipation is essential.", "AI": {"tldr": "RARE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u9884\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u5f15\u5165\u6ce8\u610f\u529b\u5206\u6570\u6392\u5e8f\u635f\u5931\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u907f\u514d\u989d\u5916\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u5206\u6570\u6392\u5e8f\u635f\u5931\u3002", "result": "\u5728DAD\u548cCCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901f\u5ea6\u63d0\u53474-8\u500d\uff0c\u5ef6\u8fdf13.6ms/\u5e27\uff0873.3 FPS\uff09\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5e73\u5747\u7cbe\u5ea6\u3002", "conclusion": "RARE\u5728\u5b9e\u65f6\u548c\u53ef\u89e3\u91ca\u7684\u4e8b\u6545\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u3002", "relevance": 30.0}}
{"id": "2505.17908", "pdf": "https://arxiv.org/pdf/2505.17908", "abs": "https://arxiv.org/abs/2505.17908", "authors": ["Litao Guo", "Xinli Xu", "Luozhou Wang", "Jiantao Lin", "Jinsong Zhou", "Zixin Zhang", "Bolan Su", "Ying-Cong Chen"], "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback", "categories": ["cs.AI", "cs.CV"], "comment": "Project page: https://github.com/LitaoGuo/ComfyMind", "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind", "AI": {"tldr": "ComfyMind\u662f\u4e00\u4e2a\u57fa\u4e8eComfyUI\u5e73\u53f0\u7684\u534f\u4f5c\u5f0fAI\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u5de5\u4f5c\u6d41\u63a5\u53e3\u548c\u641c\u7d22\u6811\u89c4\u5212\u673a\u5236\u63d0\u5347\u901a\u7528\u751f\u6210\u4efb\u52a1\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6846\u67b6\u5728\u652f\u6301\u590d\u6742\u73b0\u5b9e\u5e94\u7528\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u89c4\u5212\u548c\u6267\u884c\u7ea7\u53cd\u9988\u3002", "method": "ComfyMind\u5f15\u5165\u8bed\u4e49\u5de5\u4f5c\u6d41\u63a5\u53e3\uff08SWI\uff09\u548c\u641c\u7d22\u6811\u89c4\u5212\u673a\u5236\uff0c\u652f\u6301\u9ad8\u5c42\u6b21\u7ec4\u5408\u548c\u81ea\u9002\u5e94\u4fee\u6b63\u3002", "result": "\u5728ComfyBench\u3001GenEval\u548cReason-Edit\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cComfyMind\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1GPT-Image-1\u3002", "conclusion": "ComfyMind\u4e3a\u5f00\u6e90\u901a\u7528\u751f\u6210AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "relevance": 75.0}}
{"id": "2505.17371", "pdf": "https://arxiv.org/pdf/2505.17371", "abs": "https://arxiv.org/abs/2505.17371", "authors": ["Sergio Chevtchenko", "Nikhil Navas", "Rafaella Vale", "Franco Ubaudi", "Sipumelele Lucwaba", "Cally Ardington", "Soheil Afshar", "Mark Antoniou", "Saeed Afshar"], "title": "An End-to-End Approach for Child Reading Assessment in the Xhosa Language", "categories": ["cs.LG", "cs.CL"], "comment": "Paper accepted on AIED 2025 containing 14 pages, 6 figures and 4\n  tables", "summary": "Child literacy is a strong predictor of life outcomes at the subsequent\nstages of an individual's life. This points to a need for targeted\ninterventions in vulnerable low and middle income populations to help bridge\nthe gap between literacy levels in these regions and high income ones. In this\neffort, reading assessments provide an important tool to measure the\neffectiveness of these programs and AI can be a reliable and economical tool to\nsupport educators with this task. Developing accurate automatic reading\nassessment systems for child speech in low-resource languages poses significant\nchallenges due to limited data and the unique acoustic properties of children's\nvoices. This study focuses on Xhosa, a language spoken in South Africa, to\nadvance child speech recognition capabilities. We present a novel dataset\ncomposed of child speech samples in Xhosa. The dataset is available upon\nrequest and contains ten words and letters, which are part of the Early Grade\nReading Assessment (EGRA) system. Each recording is labeled with an online and\ncost-effective approach by multiple markers and a subsample is validated by an\nindependent EGRA reviewer. This dataset is evaluated with three fine-tuned\nstate-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The\nresults indicate that the performance of these models can be significantly\ninfluenced by the amount and balancing of the available training data, which is\nfundamental for cost-effective large dataset collection. Furthermore, our\nexperiments indicate that the wav2vec 2.0 performance is improved by training\non multiple classes at a time, even when the number of available samples is\nconstrained.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528AI\u6280\u672f\uff08\u5982wav2vec 2.0\u3001HuBERT\u548cWhisper\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982Xhosa\uff09\u4e2d\u5f00\u53d1\u81ea\u52a8\u9605\u8bfb\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u513f\u7ae5\u8bc6\u5b57\u5e72\u9884\u9879\u76ee\u3002", "motivation": "\u513f\u7ae5\u8bc6\u5b57\u80fd\u529b\u5bf9\u4e2a\u4f53\u672a\u6765\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u6536\u5165\u5730\u533a\u3002AI\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u5e2e\u52a9\u8bc4\u4f30\u8bc6\u5b57\u5e72\u9884\u9879\u76ee\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u57fa\u4e8eXhosa\u8bed\u8a00\u6784\u5efa\u4e86\u4e00\u4e2a\u513f\u7ae5\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u7aef\u5230\u7aef\u6a21\u578b\uff08wav2vec 2.0\u3001HuBERT\u548cWhisper\uff09\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bad\u7ec3\u6570\u636e\u7684\u91cf\u548c\u5e73\u8861\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14wav2vec 2.0\u5728\u591a\u7c7b\u522b\u8bad\u7ec3\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17095", "pdf": "https://arxiv.org/pdf/2505.17095", "abs": "https://arxiv.org/abs/2505.17095", "authors": ["Kristine Ann M. Carandang", "Jasper Meynard P. Ara\u00f1a", "Ethan Robert A. Casin", "Christopher P. Monterola", "Daniel Stanley Y. Tan", "Jesus Felix B. Valenzuela", "Christian M. Alis"], "title": "Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation", "categories": ["cs.CL"], "comment": null, "summary": "Due to the legal and ethical responsibilities of healthcare providers (HCPs)\nfor accurate documentation and protection of patient data privacy, the natural\nvariability in the responses of large language models (LLMs) presents\nchallenges for incorporating clinical note generation (CNG) systems, driven by\nLLMs, into real-world clinical processes. The complexity is further amplified\nby the detailed nature of texts in CNG. To enhance the confidence of HCPs in\ntools powered by LLMs, this study evaluates the reliability of 12 open-weight\nand proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms\nof their ability to generate notes that are string equivalent (consistency\nrate), have the same meaning (semantic consistency) and are correct (semantic\nsimilarity), across several iterations using the same prompt. The results show\nthat (1) LLMs from all model families are stable, such that their responses are\nsemantically consistent despite being written in various ways, and (2) most of\nthe LLMs generated notes close to the corresponding notes made by experts.\nOverall, Meta's Llama 70B was the most reliable, followed by Mistral's Small\nmodel. With these findings, we recommend the local deployment of these\nrelatively smaller open-weight models for CNG to ensure compliance with data\nprivacy regulations, as well as to improve the efficiency of HCPs in clinical\ndocumentation.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8612\u79cd\u5f00\u6e90\u548c\u4e13\u6709LLM\u5728\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0Meta\u7684Llama 70B\u548cMistral\u7684Small\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5efa\u8bae\u672c\u5730\u90e8\u7f72\u4ee5\u7b26\u5408\u9690\u79c1\u6cd5\u89c4\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\u4e2d\u7684\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u4ee5\u589e\u5f3a\u533b\u7597\u63d0\u4f9b\u8005\u5bf9LLM\u5de5\u5177\u7684\u4fe1\u4efb\u3002", "method": "\u8bc4\u4f3012\u79cdLLM\u5728\u5b57\u7b26\u4e32\u7b49\u4ef7\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6b63\u786e\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "LLM\u751f\u6210\u7ed3\u679c\u7a33\u5b9a\u4e14\u63a5\u8fd1\u4e13\u5bb6\u7b14\u8bb0\uff0cMeta\u7684Llama 70B\u548cMistral\u7684Small\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5efa\u8bae\u672c\u5730\u90e8\u7f72\u8f83\u5c0f\u5f00\u6e90\u6a21\u578b\u4ee5\u7b26\u5408\u9690\u79c1\u6cd5\u89c4\u5e76\u63d0\u5347\u6548\u7387\u3002", "relevance": 75.0}}
{"id": "2505.17457", "pdf": "https://arxiv.org/pdf/2505.17457", "abs": "https://arxiv.org/abs/2505.17457", "authors": ["Jiaxuan Lu", "Junyan Shi", "Yuhui Lin", "Fang Yan", "Yue Gao", "Shaoting Zhang", "Xiaosong Wang"], "title": "Graph Mamba for Efficient Whole Slide Image Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Whole Slide Images (WSIs) in histopathology present a significant challenge\nfor large-scale medical image analysis due to their high resolution, large\nsize, and complex tile relationships. Existing Multiple Instance Learning (MIL)\nmethods, such as Graph Neural Networks (GNNs) and Transformer-based models,\nface limitations in scalability and computational cost. To bridge this gap, we\npropose the WSI-GMamba framework, which synergistically combines the relational\nmodeling strengths of GNNs with the efficiency of Mamba, the State Space Model\ndesigned for sequence learning. The proposed GMamba block integrates Message\nPassing, Graph Scanning & Flattening, and feature aggregation via a\nBidirectional State Space Model (Bi-SSM), achieving Transformer-level\nperformance with 7* fewer FLOPs. By leveraging the complementary strengths of\nlightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable\nsolution for large-scale WSI analysis, offering both high accuracy and\ncomputational efficiency for slide-level classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWSI-GMamba\u6846\u67b6\uff0c\u7ed3\u5408GNN\u7684\u5173\u7cfb\u5efa\u6a21\u80fd\u529b\u548cMamba\u7684\u9ad8\u6548\u5e8f\u5217\u5b66\u4e60\u80fd\u529b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21WSI\u5206\u6790\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u63a5\u8fd1Transformer\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MIL\u65b9\u6cd5\uff08\u5982GNN\u548cTransformer\uff09\u5728\u5927\u89c4\u6a21WSI\u5206\u6790\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u7ed3\u5408GNN\u548cMamba\uff08\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\uff0c\u63d0\u51faGMamba\u5757\uff0c\u96c6\u6210\u6d88\u606f\u4f20\u9012\u3001\u56fe\u626b\u63cf\u4e0e\u5c55\u5e73\uff0c\u4ee5\u53ca\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u7279\u5f81\u805a\u5408\u3002", "result": "\u6027\u80fd\u63a5\u8fd1Transformer\uff0c\u4f46\u8ba1\u7b97\u91cf\u51cf\u5c117\u500d\u3002", "conclusion": "WSI-GMamba\u4e3a\u5927\u89c4\u6a21WSI\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.18030", "pdf": "https://arxiv.org/pdf/2505.18030", "abs": "https://arxiv.org/abs/2505.18030", "authors": ["Hazhar Rahmani", "Jie Fu"], "title": "Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.SY", "eess.SY"], "comment": "16 pages, 11 figures, technical report, submission under review", "summary": "Many preference elicitation algorithms consider preference over propositional\nlogic formulas or items with different attributes. In sequential decision\nmaking, a user's preference can be a preorder over possible outcomes, each of\nwhich is a temporal sequence of events. This paper considers a class of\npreference inference problems where the user's unknown preference is\nrepresented by a preorder over regular languages (sets of temporal sequences),\nreferred to as temporal goals. Given a finite set of pairwise comparisons\nbetween finite words, the objective is to learn both the set of temporal goals\nand the preorder over these goals. We first show that a preference relation\nover temporal goals can be modeled by a Preference Deterministic Finite\nAutomaton (PDFA), which is a deterministic finite automaton augmented with a\npreorder over acceptance conditions. The problem of preference inference\nreduces to learning the PDFA. This problem is shown to be computationally\nchallenging, with the problem of determining whether there exists a PDFA of\nsize smaller than a given integer $k$, consistent with the sample, being\nNP-Complete. We formalize the properties of characteristic samples and develop\nan algorithm that guarantees to learn, given a characteristic sample, the\nminimal PDFA equivalent to the true PDFA from which the sample is drawn. We\npresent the method through a running example and provide detailed analysis\nusing a robotic motion planning problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u5219\u8bed\u8a00\u7684\u504f\u597d\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u504f\u597d\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\uff08PDFA\uff09\u7684\u6a21\u578b\u6765\u8868\u793a\u7528\u6237\u504f\u597d\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\uff0c\u7528\u6237\u504f\u597d\u901a\u5e38\u8868\u73b0\u4e3a\u5bf9\u65f6\u95f4\u5e8f\u5217\u4e8b\u4ef6\u7684\u9884\u5e8f\u5173\u7cfb\u3002\u7814\u7a76\u5982\u4f55\u4ece\u6709\u9650\u7684\u6210\u5bf9\u6bd4\u8f83\u4e2d\u63a8\u65ad\u8fd9\u79cd\u504f\u597d\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u51b3\u7b56\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c06\u504f\u597d\u5173\u7cfb\u5efa\u6a21\u4e3aPDFA\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u7b97\u6cd5\u4ece\u7279\u5f81\u6837\u672c\u4e2d\u5b66\u4e60\u6700\u5c0fPDFA\u3002", "result": "\u8bc1\u660e\u4e86PDFA\u5b66\u4e60\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u4e3aNP\u5b8c\u5168\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u8bc1\u5b66\u4e60\u6700\u5c0fPDFA\u7684\u7b97\u6cd5\u3002", "conclusion": "PDFA\u4e3a\u504f\u597d\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8868\u793a\u548c\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5982\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.17373", "pdf": "https://arxiv.org/pdf/2505.17373", "abs": "https://arxiv.org/abs/2505.17373", "authors": ["Kaiwen Wang", "Jin Peng Zhou", "Jonathan Chang", "Zhaolin Gao", "Nathan Kallus", "Kiant\u00e9 Brantley", "Wen Sun"], "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "In this paper, we propose a simple and efficient method for value model\ntraining on long-context reasoning traces. Compared to existing process reward\nmodels (PRMs), our method does not require a fine-grained notion of \"step,\"\nwhich is difficult to define for long-context reasoning models. By collecting a\ndataset of 2.5 million reasoning traces, we train a 1.5B token-level value\nmodel and apply it to DeepSeek models for improved performance with test-time\ncompute scaling. We find that block-wise value-guided search (VGS) with a final\nweighted majority vote achieves better test-time scaling than standard methods\nsuch as majority voting or best-of-n. With an inference budget of 64\ngenerations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of\n45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024\n& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly\nreduces the inference FLOPs required to achieve the same performance of\nmajority voting. Our dataset, model and codebase are open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u4ef7\u503c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u8f68\u8ff9\uff0c\u65e0\u9700\u5b9a\u4e49\u7ec6\u7c92\u5ea6\u7684\u201c\u6b65\u9aa4\u201d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u9700\u8981\u5b9a\u4e49\u7ec6\u7c92\u5ea6\u7684\u201c\u6b65\u9aa4\u201d\uff0c\u8fd9\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6536\u96c6250\u4e07\u6761\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u8bad\u7ec315\u4ebftoken\u7ea7\u522b\u7684\u4ef7\u503c\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u4e8eDeepSeek\u6a21\u578b\uff0c\u91c7\u7528\u5757\u7ea7\u4ef7\u503c\u5f15\u5bfc\u641c\u7d22\uff08VGS\uff09\u548c\u52a0\u6743\u591a\u6570\u6295\u7968\u3002", "result": "\u572864\u6b21\u751f\u6210\u7684\u63a8\u7406\u9884\u7b97\u4e0b\uff0cVGS\u5728\u56db\u4e2a\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u8fbe45.7%\uff0c\u4e0eo3-mini-medium\u6301\u5e73\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u4ee3\u7801\u3002", "relevance": 85.0}}
{"id": "2505.17098", "pdf": "https://arxiv.org/pdf/2505.17098", "abs": "https://arxiv.org/abs/2505.17098", "authors": ["Yanshu Li", "Tian Yun", "Jianjiang Yang", "Pinyuan Feng", "Jinfa Huang", "Ruixiang Tang"], "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration", "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 11 figures, 19 tables. arXiv admin note: substantial text\n  overlap with arXiv:2503.04839", "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input in-context\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures in-context sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a valuable perspective for\ninterpreting and improving multimodal ICL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTACO\u7684\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u6ce8\u610f\u529b\u52a8\u6001\u914d\u7f6e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LVLM\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u8f93\u5165\u5e8f\u5217\u7684\u8d28\u91cf\uff0c\u4f46\u76ee\u524d\u5bf9LVLM\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u5e8f\u5217\u7684\u7406\u89e3\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u89e3\u91ca\u548c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u6620\u5c04\u7684\u89c6\u89d2\u5206\u6790ICL\uff0c\u63d0\u51faTACO\u6a21\u578b\uff0c\u5229\u7528\u4efb\u52a1\u611f\u77e5\u6ce8\u610f\u529b\u52a8\u6001\u914d\u7f6e\u5e8f\u5217\uff0c\u5e76\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4efb\u52a1\u6620\u5c04\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u4e2aLVLM\u548c\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTACO\u5728\u591a\u79cdICL\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4efb\u52a1\u6620\u5c04\u4e3a\u89e3\u91ca\u548c\u6539\u8fdb\u591a\u6a21\u6001ICL\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.17461", "pdf": "https://arxiv.org/pdf/2505.17461", "abs": "https://arxiv.org/abs/2505.17461", "authors": ["Kazuki Hayashi", "Shintaro Ozaki", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large-scale Vision Language Models (LVLMs) are increasingly being applied to\na wide range of real-world multimodal applications, involving complex visual\nand linguistic reasoning. As these models become more integrated into practical\nuse, they are expected to handle complex aspects of human interaction. Among\nthese, color perception is a fundamental yet highly variable aspect of visual\nunderstanding. It differs across individuals due to biological factors such as\nColor Vision Deficiencies (CVDs), as well as differences in culture and\nlanguage. Despite its importance, perceptual diversity has received limited\nattention. In our study, we evaluate LVLMs' ability to account for individual\nlevel perceptual variation using the Ishihara Test, a widely used method for\ndetecting CVDs. Our results show that LVLMs can explain CVDs in natural\nlanguage, but they cannot simulate how people with CVDs perceive color in image\nbased tasks. These findings highlight the need for multimodal systems that can\naccount for color perceptual diversity and support broader discussions on\nperceptual inclusiveness and fairness in multimodal AI.", "AI": {"tldr": "LVLMs can describe Color Vision Deficiencies (CVDs) in text but fail to simulate CVD perception in image tasks, highlighting the need for more inclusive multimodal AI.", "motivation": "To assess LVLMs' capability in handling perceptual diversity, specifically color perception variations due to CVDs, culture, and language.", "method": "Evaluated LVLMs using the Ishihara Test to detect CVDs and tested their ability to simulate CVD perception in image-based tasks.", "result": "LVLMs can explain CVDs in natural language but cannot simulate CVD perception in images.", "conclusion": "Multimodal systems need to better account for color perceptual diversity to ensure inclusiveness and fairness in AI.", "relevance": 60.0}}
{"id": "2505.18034", "pdf": "https://arxiv.org/pdf/2505.18034", "abs": "https://arxiv.org/abs/2505.18034", "authors": ["Wentao Sun", "Joao Paulo Nogueira", "Alonso Silva"], "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLM\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u5728\u533a\u5206\u56e0\u679c\u5173\u7cfb\u548c\u76f8\u5173\u5173\u7cfb\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5f15\u5bfc\u6a21\u578b\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u7cfb\u7edf\u7f16\u7801\u76f8\u5173\u524d\u63d0\u4ee5\u56de\u7b54\u56e0\u679c\u67e5\u8be2\u3002", "result": "\u5728Corr2Cause\u6570\u636e\u96c6\u4e0a\uff0cQwen3-32B\u6a21\u578b\u7684F1\u5206\u6570\u4ece32.71\u63d0\u5347\u81f348.26\uff0c\u76f8\u5bf9\u63d0\u534747.5%\u3002", "conclusion": "\u7ed3\u6784\u5316\u601d\u7ef4\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u6cdb\u5316\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.17379", "pdf": "https://arxiv.org/pdf/2505.17379", "abs": "https://arxiv.org/abs/2505.17379", "authors": ["Zichen Wang", "Chuanhao Li", "Huazheng Wang"], "title": "Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "We investigate the problem of identifying the optimal scoring rule within the\nprincipal-agent framework for online information acquisition problem. We focus\non the principal's perspective, seeking to determine the desired scoring rule\nthrough interactions with the agent. To address this challenge, we propose two\nalgorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget\nsettings, respectively. Our theoretical analysis demonstrates that OIAFC can\nextract the desired $(\\epsilon, \\delta)$-scoring rule with a efficient\ninstance-dependent sample complexity or an instance-independent sample\ncomplexity. Our analysis also shows that OIAFB matches the instance-independent\nperformance bound of OIAFC, while both algorithms share the same complexity\nacross fixed confidence and fixed budget settings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u4fe1\u606f\u83b7\u53d6\u95ee\u9898\u4e2d\u4e3b\u4ee3\u7406\u6846\u67b6\u4e0b\u6700\u4f18\u8bc4\u5206\u89c4\u5219\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5OIAFC\u548cOIAFB\uff0c\u5206\u522b\u9002\u7528\u4e8e\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u548c\u56fa\u5b9a\u9884\u7b97\u8bbe\u7f6e\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u7406\u8bba\u6027\u80fd\u3002", "motivation": "\u4ece\u4e3b\u65b9\u89c6\u89d2\u51fa\u53d1\uff0c\u786e\u5b9a\u7406\u60f3\u7684\u8bc4\u5206\u89c4\u5219\u4ee5\u4f18\u5316\u5728\u7ebf\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5OIAFC\uff08\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\uff09\u548cOIAFB\uff08\u56fa\u5b9a\u9884\u7b97\uff09\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5176\u6837\u672c\u590d\u6742\u6027\u548c\u6027\u80fd\u3002", "result": "OIAFC\u80fd\u9ad8\u6548\u63d0\u53d6\u76ee\u6807\u8bc4\u5206\u89c4\u5219\uff0cOIAFB\u5728\u5b9e\u4f8b\u65e0\u5173\u6027\u80fd\u4e0a\u4e0eOIAFC\u5339\u914d\uff0c\u4e24\u79cd\u7b97\u6cd5\u5728\u590d\u6742\u5ea6\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5728\u7ebf\u4fe1\u606f\u83b7\u53d6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17099", "pdf": "https://arxiv.org/pdf/2505.17099", "abs": "https://arxiv.org/abs/2505.17099", "authors": ["Xiaozhao Liu", "Dinggang Shen", "Xihui Liu"], "title": "Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation", "categories": ["cs.CL"], "comment": "Code, checkpoint and text samples available at\n  https://github.com/justin-xzliu/GLIM", "summary": "Pretrained generative models have opened new frontiers in brain decoding by\nenabling the synthesis of realistic texts and images from non-invasive brain\nrecordings. However, the reliability of such outputs remains\nquestionable--whether they truly reflect semantic activation in the brain, or\nare merely hallucinated by the powerful generative models. In this paper, we\nfocus on EEG-to-text decoding and address its hallucination issue through the\nlens of posterior collapse. Acknowledging the underlying mismatch in\ninformation capacity between EEG and text, we reframe the decoding task as\nsemantic summarization of core meanings rather than previously verbatim\nreconstruction of stimulus texts. To this end, we propose the Generative\nLanguage Inspection Model (GLIM), which emphasizes learning informative and\ninterpretable EEG representations to improve semantic grounding under\nheterogeneous and small-scale data conditions. Experiments on the public ZuCo\ndataset demonstrate that GLIM consistently generates fluent, EEG-grounded\nsentences without teacher forcing. Moreover, it supports more robust evaluation\nbeyond text similarity, through EEG-text retrieval and zero-shot semantic\nclassification across sentiment categories, relation types, and corpus topics.\nTogether, our architecture and evaluation protocols lay the foundation for\nreliable and scalable benchmarking in generative brain decoding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGLIM\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u603b\u7ed3\u800c\u975e\u9010\u5b57\u91cd\u5efa\u89e3\u51b3EEG\u5230\u6587\u672c\u89e3\u7801\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0b\u63d0\u5347\u8bed\u4e49\u57fa\u7840\u3002", "motivation": "\u89e3\u51b3EEG\u5230\u6587\u672c\u89e3\u7801\u4e2d\u751f\u6210\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u786e\u4fdd\u8f93\u51fa\u53cd\u6620\u771f\u5b9e\u7684\u8111\u8bed\u4e49\u6fc0\u6d3b\u3002", "method": "\u63d0\u51faGenerative Language Inspection Model (GLIM)\uff0c\u5f3a\u8c03\u5b66\u4e60\u4fe1\u606f\u4e30\u5bcc\u4e14\u53ef\u89e3\u91ca\u7684EEG\u8868\u793a\uff0c\u4f18\u5316\u8bed\u4e49\u57fa\u7840\u3002", "result": "GLIM\u5728ZuCo\u6570\u636e\u96c6\u4e0a\u751f\u6210\u6d41\u7545\u4e14\u57fa\u4e8eEEG\u7684\u53e5\u5b50\uff0c\u652f\u6301\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\uff08\u5982EEG-\u6587\u672c\u68c0\u7d22\u548c\u96f6\u6837\u672c\u5206\u7c7b\uff09\u3002", "conclusion": "GLIM\u4e3a\u751f\u6210\u5f0f\u8111\u89e3\u7801\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u3002", "relevance": 60.0}}
{"id": "2505.17473", "pdf": "https://arxiv.org/pdf/2505.17473", "abs": "https://arxiv.org/abs/2505.17473", "authors": ["Jiangning Zhu", "Yuxing Zhou", "Zheng Wang", "Juntao Yao", "Yima Gu", "Yuhui Yuan", "Shixia Liu"], "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nOrionBench, a benchmark designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 26,250 real\nand 78,750 synthetic infographics, with over 6.9 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of OrionBench through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection.", "AI": {"tldr": "OrionBench\u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u56fe\u8868\u548c\u4eba\u7c7b\u53ef\u8bc6\u522b\u5bf9\u8c61\uff08HROs\uff09\u68c0\u6d4b\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u5927\u91cf\u771f\u5b9e\u548c\u5408\u6210\u4fe1\u606f\u56fe\u8868\u53ca\u5176\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u56fe\u8868\u548cHROs\u7684\u89c6\u89c9\u5b9a\u4f4d\u4e0a\u4e0d\u51c6\u786e\uff0c\u800c\u56fe\u8868\u7406\u89e3\u9700\u8981\u5bf9\u8fd9\u4e9b\u5143\u7d20\u8fdb\u884c\u8bc6\u522b\u548c\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u5728\u73af\u548c\u7a0b\u5e8f\u5316\u65b9\u6cd5\u521b\u5efaOrionBench\uff0c\u5305\u542b26,250\u5f20\u771f\u5b9e\u548c78,750\u5f20\u5408\u6210\u4fe1\u606f\u56fe\u8868\uff0c\u6807\u6ce8\u4e86\u8d85\u8fc7690\u4e07\u4e2a\u8fb9\u754c\u6846\u3002", "result": "OrionBench\u5728\u63d0\u5347VLM\u7684\u56fe\u8868\u7406\u89e3\u6027\u80fd\u3001\u6bd4\u8f83\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u4ee5\u53ca\u5e94\u7528\u4e8e\u6587\u6863\u5e03\u5c40\u548cUI\u5143\u7d20\u68c0\u6d4b\u65b9\u9762\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "OrionBench\u4e3a\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u56fe\u8868\u548cHROs\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.18086", "pdf": "https://arxiv.org/pdf/2505.18086", "abs": "https://arxiv.org/abs/2505.18086", "authors": ["Muzhi Dai", "Shixuan Liu", "Qingyi Si"], "title": "Stable Reinforcement Learning for Efficient Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The success of Deepseek-R1 has drawn the LLM community's attention to\nreinforcement learning (RL) methods like GRPO. However, such rule-based 0/1\noutcome reward methods lack the capability to regulate the intermediate\nreasoning processes during chain-of-thought (CoT) generation, leading to severe\noverthinking phenomena. In response, recent studies have designed reward\nfunctions to reinforce models' behaviors in producing shorter yet correct\ncompletions. Nevertheless, we observe that these length-penalty reward\nfunctions exacerbate RL training instability: as the completion length\ndecreases, model accuracy abruptly collapses, often occurring early in\ntraining. To address this issue, we propose a simple yet effective solution\nGRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically\nadjusts the reward strategy by monitoring the correctness ratio among\ncompletions within each query-sampled group. A low correctness ratio indicates\nthe need to avoid length penalty that compromises CoT quality, triggering a\nswitch to length-agnostic 0/1 rewards that prioritize reasoning capability. A\nhigh ratio maintains length penalties to boost efficiency. Experimental results\nshow that our approach avoids training instability caused by length penalty\nwhile maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,\nGPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average\naccuracy by 1.48% while reducing CoT sequence length by 47.3%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRPO-\u03bb\uff0c\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u5956\u52b1\u7b56\u7565\u7684GRPO\u53d8\u4f53\uff0c\u89e3\u51b3\u4e86\u957f\u5ea6\u60e9\u7f5a\u5956\u52b1\u51fd\u6570\u5bfc\u81f4\u7684RL\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u76840/1\u5956\u52b1\u65b9\u6cd5\u65e0\u6cd5\u8c03\u63a7\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff1b\u800c\u957f\u5ea6\u60e9\u7f5a\u5956\u52b1\u51fd\u6570\u53c8\u52a0\u5267\u4e86RL\u8bad\u7ec3\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faGRPO-\u03bb\uff0c\u901a\u8fc7\u76d1\u63a7\u6bcf\u7ec4\u67e5\u8be2\u6837\u672c\u4e2d\u7684\u6b63\u786e\u7387\u52a8\u6001\u8c03\u6574\u5956\u52b1\u7b56\u7565\uff0c\u4f4e\u6b63\u786e\u7387\u65f6\u907f\u514d\u957f\u5ea6\u60e9\u7f5a\uff0c\u9ad8\u6b63\u786e\u7387\u65f6\u4fdd\u6301\u957f\u5ea6\u60e9\u7f5a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.48%\uff0cCoT\u5e8f\u5217\u957f\u5ea6\u51cf\u5c1147.3%\u3002", "conclusion": "GRPO-\u03bb\u6709\u6548\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u6743\u8861\u3002", "relevance": 85.0}}
{"id": "2505.17384", "pdf": "https://arxiv.org/pdf/2505.17384", "abs": "https://arxiv.org/abs/2505.17384", "authors": ["Tianyu Xie", "Shuchen Xue", "Zijin Feng", "Tianyang Hu", "Jiacheng Sun", "Zhenguo Li", "Cheng Zhang"], "title": "Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "23 pages, 14 figures", "summary": "Discrete diffusion models have recently shown great promise for modeling\ncomplex discrete data, with masked diffusion models (MDMs) offering a\ncompelling trade-off between quality and generation speed. MDMs denoise by\nprogressively unmasking multiple dimensions from an all-masked input, but their\nperformance can degrade when using few denoising steps due to limited modeling\nof inter-dimensional dependencies. In this paper, we propose Variational\nAutoencoding Discrete Diffusion (VADD), a novel framework that enhances\ndiscrete diffusion with latent variable modeling to implicitly capture\ncorrelations among dimensions. By introducing an auxiliary recognition model,\nVADD enables stable training via variational lower bounds maximization and\namortized inference over the training set. Our approach retains the efficiency\nof traditional MDMs while significantly improving sample quality, especially\nwhen the number of denoising steps is small. Empirical results on 2D toy data,\npixel-level image generation, and text generation demonstrate that VADD\nconsistently outperforms MDM baselines.", "AI": {"tldr": "VADD enhances discrete diffusion models with latent variable modeling to improve sample quality, especially with few denoising steps.", "motivation": "MDMs degrade in performance with few denoising steps due to limited modeling of inter-dimensional dependencies.", "method": "VADD introduces latent variable modeling and an auxiliary recognition model for stable training via variational lower bounds maximization.", "result": "VADD outperforms MDM baselines on 2D toy data, image generation, and text generation.", "conclusion": "VADD offers a more efficient and higher-quality alternative to traditional MDMs.", "relevance": 40.0}}
{"id": "2505.17100", "pdf": "https://arxiv.org/pdf/2505.17100", "abs": "https://arxiv.org/abs/2505.17100", "authors": ["Haoyan Yang", "Runxue Bao", "Cao Xiao", "Jun Ma", "Parminder Bhatia", "Shangqian Gao", "Taha Kass-Hout"], "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a promising tool for automatically evaluating\ngenerated outputs, but its reliability is often undermined by potential biases\nin judgment. Existing efforts to mitigate these biases face key limitations:\nin-context learning-based methods fail to address rooted biases due to the\nevaluator's limited capacity for self-reflection, whereas fine-tuning is not\napplicable to all evaluator types, especially closed-source models. To address\nthis challenge, we introduce the Reasoning-based Bias Detector (RBD), which is\na plug-in module that identifies biased evaluations and generates structured\nreasoning to guide evaluator self-correction. Rather than modifying the\nevaluator itself, RBD operates externally and engages in an iterative process\nof bias detection and feedback-driven revision. To support its development, we\ndesign a complete pipeline consisting of biased dataset construction,\nsupervision collection, distilled reasoning-based fine-tuning of RBD, and\nintegration with LLM evaluators. We fine-tune four sizes of RBD models, ranging\nfrom 1.5B to 14B, and observe consistent performance improvements across all\nscales. Experimental results on 4 bias types--verbosity, position, bandwagon,\nand sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong\neffectiveness. For example, the RBD-8B model improves evaluation accuracy by an\naverage of 18.5% and consistency by 10.9%, and surpasses prompting-based\nbaselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results\nhighlight RBD's effectiveness and scalability. Additional experiments further\ndemonstrate its strong generalization across biases and domains, as well as its\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRBD\u7684\u63d2\u4ef6\u6a21\u5757\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7ea0\u6b63LLM\u8bc4\u4f30\u4e2d\u7684\u504f\u89c1\uff0c\u901a\u8fc7\u5916\u90e8\u8fed\u4ee3\u53cd\u9988\u63d0\u5347\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u6839\u6df1\u8482\u56fa\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5c01\u95ed\u6a21\u578b\u3002RBD\u65e8\u5728\u901a\u8fc7\u5916\u90e8\u6a21\u5757\u5b9e\u73b0\u504f\u89c1\u68c0\u6d4b\u548c\u81ea\u6211\u4fee\u6b63\u3002", "method": "RBD\u901a\u8fc7\u6784\u9020\u504f\u89c1\u6570\u636e\u96c6\u3001\u76d1\u7763\u6536\u96c6\u3001\u84b8\u998f\u63a8\u7406\u5fae\u8c03\u7b49\u6d41\u7a0b\u5f00\u53d1\uff0c\u5e76\u4e0eLLM\u8bc4\u4f30\u5668\u96c6\u6210\u3002", "result": "RBD\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4f8b\u59828B\u6a21\u578b\u5e73\u5747\u63d0\u534718.5%\u51c6\u786e\u7387\u548c10.9%\u4e00\u81f4\u6027\u3002", "conclusion": "RBD\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u504f\u89c1\u68c0\u6d4b\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u504f\u89c1\u7c7b\u578b\u548c\u9886\u57df\u3002", "relevance": 85.0}}
{"id": "2505.17475", "pdf": "https://arxiv.org/pdf/2505.17475", "abs": "https://arxiv.org/abs/2505.17475", "authors": ["Uyoung Jeong", "Jonathan Freer", "Seungryul Baek", "Hyung Jin Chang", "Kwang In Kim"], "title": "PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025", "summary": "We study multi-dataset training (MDT) for pose estimation, where skeletal\nheterogeneity presents a unique challenge that existing methods have yet to\naddress. In traditional domains, \\eg regression and classification, MDT\ntypically relies on dataset merging or multi-head supervision. However, the\ndiversity of skeleton types and limited cross-dataset supervision complicate\nintegration in pose estimation. To address these challenges, we introduce\nPoseBH, a new MDT framework that tackles keypoint heterogeneity and limited\nsupervision through two key techniques. First, we propose nonparametric\nkeypoint prototypes that learn within a unified embedding space, enabling\nseamless integration across skeleton types. Second, we develop a cross-type\nself-supervision mechanism that aligns keypoint predictions with keypoint\nembedding prototypes, providing supervision without relying on teacher-student\nmodels or additional augmentations. PoseBH substantially improves\ngeneralization across whole-body and animal pose datasets, including\nCOCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard\nhuman pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint\nembeddings transfer effectively to hand shape estimation (InterHand2.6M) and\nhuman body shape estimation (3DPW). The code for PoseBH is available at:\nhttps://github.com/uyoung-jeong/PoseBH.", "AI": {"tldr": "PoseBH\u662f\u4e00\u4e2a\u7528\u4e8e\u59ff\u6001\u4f30\u8ba1\u7684\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5173\u952e\u70b9\u539f\u578b\u548c\u8de8\u7c7b\u578b\u81ea\u76d1\u7763\u673a\u5236\u89e3\u51b3\u9aa8\u67b6\u5f02\u6784\u6027\u548c\u6709\u9650\u76d1\u7763\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u4e2d\u672a\u80fd\u89e3\u51b3\u9aa8\u67b6\u5f02\u6784\u6027\u548c\u76d1\u7763\u6709\u9650\u7684\u95ee\u9898\uff0cPoseBH\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u975e\u53c2\u6570\u5173\u952e\u70b9\u539f\u578b\u548c\u8de8\u7c7b\u578b\u81ea\u76d1\u7763\u673a\u5236\uff0c\u5b9e\u73b0\u9aa8\u67b6\u7c7b\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u548c\u65e0\u9700\u989d\u5916\u76d1\u7763\u7684\u9884\u6d4b\u5bf9\u9f50\u3002", "result": "PoseBH\u5728\u5168\u8eab\u4f53\u548c\u52a8\u7269\u59ff\u6001\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5728\u6807\u51c6\u4eba\u7c7b\u59ff\u6001\u57fa\u51c6\u4e0a\u4fdd\u6301\u6027\u80fd\uff0c\u5e76\u80fd\u8fc1\u79fb\u5230\u5176\u4ed6\u4efb\u52a1\u3002", "conclusion": "PoseBH\u4e3a\u591a\u6570\u636e\u96c6\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.18121", "pdf": "https://arxiv.org/pdf/2505.18121", "abs": "https://arxiv.org/abs/2505.18121", "authors": ["Danyang Zhang", "Situo Zhang", "Ziyue Yang", "Zichen Zhu", "Zihan Zhao", "Ruisheng Cao", "Lu Chen", "Kai Yu"], "title": "ProgRM: Build Better GUI Agents with Progress Rewards", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProgRM\u7684\u8fdb\u5c55\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u4e3a\u5728\u7ebf\u8bad\u7ec3\u4e2d\u7684\u6bcf\u4e00\u6b65\u63d0\u4f9b\u5bc6\u96c6\u7684\u4e2d\u95f4\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ORM\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684GUI\u4ee3\u7406\u56e0\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u53d7\u9650\uff0c\u73b0\u6709ORM\u6a21\u578b\u65e0\u6cd5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\u4e14\u53ef\u80fd\u8fc7\u5ea6\u60e9\u7f5a\u5931\u8d25\u8f68\u8ff9\u4e2d\u7684\u6709\u4ef7\u503c\u6b65\u9aa4\u3002", "method": "\u63d0\u51faProgRM\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u4efb\u52a1\u5b8c\u6210\u8fdb\u5ea6\u63d0\u4f9b\u4e2d\u95f4\u5956\u52b1\uff1b\u8bbe\u8ba1LCS\u81ea\u6807\u6ce8\u7b97\u6cd5\u9ad8\u6548\u6807\u6ce8\u8fdb\u5c55\u5956\u52b1\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProgRM\u8bad\u7ec3\u7684\u4ee3\u7406\u4f18\u4e8e\u9886\u5148\u7684\u4e13\u6709LLM\u548cORM\u8bad\u7ec3\u7684\u4ee3\u7406\u3002", "conclusion": "ProgRM\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86LLM GUI\u4ee3\u7406\u7684\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.17393", "pdf": "https://arxiv.org/pdf/2505.17393", "abs": "https://arxiv.org/abs/2505.17393", "authors": ["Yi Zhang", "Cheng Hua"], "title": "Spectral Mixture Kernels for Bayesian Optimization", "categories": ["cs.LG", "math.SP"], "comment": null, "summary": "Bayesian Optimization (BO) is a widely used approach for solving expensive\nblack-box optimization tasks. However, selecting an appropriate probabilistic\nsurrogate model remains an important yet challenging problem. In this work, we\nintroduce a novel Gaussian Process (GP)-based BO method that incorporates\nspectral mixture kernels, derived from spectral densities formed by\nscale-location mixtures of Cauchy and Gaussian distributions. This method\nachieves a significant improvement in both efficiency and optimization\nperformance, matching the computational speed of simpler kernels while\ndelivering results that outperform more complex models and automatic BO\nmethods. We provide bounds on the information gain and cumulative regret\nassociated with obtaining the optimum. Extensive numerical experiments\ndemonstrate that our method consistently outperforms existing baselines across\na diverse range of synthetic and real-world problems, including both low- and\nhigh-dimensional settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u8c31\u6df7\u5408\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u4e2d\uff0c\u9009\u62e9\u5408\u9002\u7684\u6982\u7387\u4ee3\u7406\u6a21\u578b\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684BO\u65b9\u6cd5\uff0c\u91c7\u7528\u7531\u67ef\u897f\u548c\u9ad8\u65af\u5206\u5e03\u7684\u5c3a\u5ea6-\u4f4d\u7f6e\u6df7\u5408\u751f\u6210\u7684\u8c31\u6df7\u5408\u6838\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u4f18\u5316\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u901f\u5ea6\u4e0e\u7b80\u5355\u6838\u76f8\u5f53\uff0c\u4f46\u7ed3\u679c\u4f18\u4e8e\u590d\u6742\u6a21\u578b\u548c\u81ea\u52a8BO\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5408\u6210\u548c\u5b9e\u9645\u95ee\u9898\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "relevance": 30.0}}
{"id": "2505.17101", "pdf": "https://arxiv.org/pdf/2505.17101", "abs": "https://arxiv.org/abs/2505.17101", "authors": ["Santiago Acevedo", "Andrea Mascaretti", "Riccardo Rende", "Mat\u00e9o Mahaut", "Marco Baroni", "Alessandro Laio"], "title": "An approach to identify the most semantically informative deep representations of text and images", "categories": ["cs.CL", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information is spread across many tokens\nand it is characterized by long-distance correlations between tokens and by a\ncausal left-to-right (i.e., past-future) asymmetry. We also identify layers\nencoding semantic information within visual transformers. We show that caption\nrepresentations in the semantic layers of LLMs predict visual representations\nof the corresponding images. We observe significant and model-dependent\ninformation asymmetries between image and text representations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u8bed\u4e49\u76f8\u5173\u6570\u636e\u8868\u793a\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9Transformer\u7684\u8bed\u4e49\u5c42\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5904\u7406\u7ffb\u8bd1\u53e5\u5b50\u5bf9\u65f6\u5b58\u5728\u5185\u90e8\u201c\u8bed\u4e49\u201d\u5c42\uff0c\u4e14\u66f4\u5927\u6a21\u578b\uff08\u5982DeepSeek-V3\uff09\u80fd\u63d0\u53d6\u66f4\u591a\u901a\u7528\u4fe1\u606f\u3002\u8bed\u4e49\u4fe1\u606f\u5206\u5e03\u5728\u591a\u4e2atoken\u4e2d\uff0c\u5177\u6709\u957f\u8ddd\u79bb\u76f8\u5173\u6027\u548c\u56e0\u679c\u4e0d\u5bf9\u79f0\u6027\u3002\u89c6\u89c9Transformer\u4e2d\u4e5f\u5b58\u5728\u7c7b\u4f3c\u8bed\u4e49\u5c42\uff0c\u4e14LLMs\u7684\u8bed\u4e49\u5c42\u80fd\u9884\u6d4b\u5bf9\u5e94\u56fe\u50cf\u7684\u89c6\u89c9\u8868\u793a\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u6587\u672c\u548c\u56fe\u50cf\uff09\u4e2d\u4e3a\u8bed\u4e49\u76f8\u5173\u6570\u636e\u751f\u6210\u76f8\u4f3c\u8868\u793a\uff0c\u5e76\u5b9a\u91cf\u5206\u6790\u5176\u4fe1\u606f\u7f16\u7801\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u8bed\u4e49\u76f8\u5173\u6570\u636e\u8868\u793a\u7684\u4fe1\u606f\u5185\u5bb9\uff0c\u5206\u6790LLMs\u548c\u89c6\u89c9Transformer\u7684\u8bed\u4e49\u5c42\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21LLMs\uff08\u5982DeepSeek-V3\u548cLlama3.1-8B\uff09\u5728\u7ffb\u8bd1\u53e5\u5b50\u5bf9\u4e2d\u7684\u8868\u73b0\uff1b2\uff09\u5206\u6790\u8bed\u4e49\u4fe1\u606f\u7684\u5206\u5e03\u7279\u5f81\uff08\u5982token\u95f4\u7684\u957f\u8ddd\u79bb\u76f8\u5173\u6027\uff09\uff1b3\uff09\u7814\u7a76\u89c6\u89c9Transformer\u4e2d\u8bed\u4e49\u5c42\u4e0eLLMs\u8bed\u4e49\u5c42\u7684\u5173\u7cfb\u3002", "result": "1\uff09LLMs\u5b58\u5728\u5185\u90e8\u8bed\u4e49\u5c42\uff0c\u66f4\u5927\u6a21\u578b\u63d0\u53d6\u7684\u8bed\u4e49\u4fe1\u606f\u66f4\u901a\u7528\uff1b2\uff09\u8bed\u4e49\u4fe1\u606f\u5206\u5e03\u5728\u591a\u4e2atoken\u4e2d\uff0c\u5177\u6709\u957f\u8ddd\u79bb\u76f8\u5173\u6027\u548c\u56e0\u679c\u4e0d\u5bf9\u79f0\u6027\uff1b3\uff09\u89c6\u89c9Transformer\u7684\u8bed\u4e49\u5c42\u4e0eLLMs\u7684\u8bed\u4e49\u5c42\u5b58\u5728\u9884\u6d4b\u5173\u7cfb\uff0c\u4f46\u56fe\u50cf\u4e0e\u6587\u672c\u8868\u793a\u5b58\u5728\u6a21\u578b\u4f9d\u8d56\u6027\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u548c\u89c6\u89c9Transformer\u4e2d\u8bed\u4e49\u4fe1\u606f\u7684\u7f16\u7801\u673a\u5236\uff0c\u4e3a\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.17476", "pdf": "https://arxiv.org/pdf/2505.17476", "abs": "https://arxiv.org/abs/2505.17476", "authors": ["Yuchen Zhang", "Yaxiong Wang", "Yujiao Wu", "Lianwei Wu", "Li Zhu"], "title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts", "categories": ["cs.CV"], "comment": null, "summary": "The detection and grounding of multimedia manipulation has emerged as a\ncritical challenge in combating AI-generated disinformation. While existing\nmethods have made progress in recent years, we identify two fundamental\nlimitations in current approaches: (1) Underestimation of MLLM-driven deception\nrisk: prevailing techniques primarily address rule-based text manipulations,\nyet fail to account for sophisticated misinformation synthesized by multimodal\nlarge language models (MLLMs) that can dynamically generate semantically\ncoherent, contextually plausible yet deceptive narratives conditioned on\nmanipulated images; (2) Unrealistic misalignment artifacts: currently focused\nscenarios rely on artificially misaligned content that lacks semantic\ncoherence, rendering them easily detectable. To address these gaps\nholistically, we propose a new adversarial pipeline that leverages MLLMs to\ngenerate high-risk disinformation. Our approach begins with constructing the\nMLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered\nusing state-of-the-art editing techniques and then paired with MLLM-generated\ndeceptive texts that maintain semantic consistency with the visual\nmanipulations. Building upon this foundation, we present the Artifact-aware\nManipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:\nArtifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,\nto tame MLLMs for the MDSM problem. Comprehensive experiments validate our\nframework's superior generalization capabilities as a unified architecture for\ndetecting MLLM-powered multimodal deceptions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7531\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u751f\u6210\u7684\u9ad8\u98ce\u9669\u865a\u5047\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f4e\u4f30MLLM\u9a71\u52a8\u7684\u6b3a\u9a97\u98ce\u9669\u548c\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u9519\u4f4d\u4f2a\u5f71\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5e94\u5bf9MLLM\u751f\u6210\u7684\u590d\u6742\u865a\u5047\u4fe1\u606f\uff0c\u4e14\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u9519\u4f4d\u5185\u5bb9\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaMLLM\u9a71\u52a8\u7684\u5408\u6210\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08MDSM\uff09\uff0c\u5e76\u63d0\u51faArtifact-aware Manipulation Diagnosis\uff08AMD\uff09\u6846\u67b6\uff0c\u5305\u542b\u4f2a\u5f71\u9884\u611f\u77e5\u7f16\u7801\u548c\u64cd\u7eb5\u5bfc\u5411\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AMD\u6846\u67b6\u5728\u68c0\u6d4bMLLM\u9a71\u52a8\u7684\u591a\u6a21\u6001\u6b3a\u9a97\u65b9\u9762\u7684\u4f18\u8d8a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AMD\u6846\u67b6\u4e3a\u68c0\u6d4bMLLM\u9a71\u52a8\u7684\u865a\u5047\u4fe1\u606f\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.18134", "pdf": "https://arxiv.org/pdf/2505.18134", "abs": "https://arxiv.org/abs/2505.18134", "authors": ["Alex L. Zhang", "Thomas L. Griffiths", "Karthik R. Narasimhan", "Ofir Press"], "title": "VideoGameBench: Can Vision-Language Models complete popular video games?", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "9 pages, 33 pages including supplementary", "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.", "AI": {"tldr": "VideoGameBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5b9e\u65f6\u89c6\u9891\u6e38\u620f\u4e2d\u8868\u73b0\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b10\u6b3e90\u5e74\u4ee3\u6e38\u620f\u3002\u524d\u6cbf\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u63a8\u7406\u5ef6\u8fdf\u662f\u4e3b\u8981\u9650\u5236\u3002", "motivation": "\u7814\u7a76VLMs\u5728\u4eba\u7c7b\u76f4\u89c9\u4efb\u52a1\uff08\u5982\u611f\u77e5\u3001\u7a7a\u95f4\u5bfc\u822a\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1VideoGameBench\u57fa\u51c6\uff0c\u5305\u542b10\u6b3e\u6e38\u620f\uff0c\u6a21\u578b\u4ec5\u901a\u8fc7\u539f\u59cb\u89c6\u89c9\u8f93\u5165\u548c\u4efb\u52a1\u63cf\u8ff0\u4ea4\u4e92\u3002\u90e8\u5206\u6e38\u620f\u4fdd\u5bc6\u4ee5\u6d4b\u8bd5\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u524d\u6cbf\u6a21\u578b\uff08\u5982Gemini 2.5 Pro\uff09\u5b8c\u6210\u7387\u6781\u4f4e\uff080.48%\uff09\uff0c\u63a8\u7406\u5ef6\u8fdf\u662f\u4e3b\u8981\u95ee\u9898\u3002", "conclusion": "VideoGameBench\u63ed\u793a\u4e86VLMs\u5728\u5b9e\u65f6\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.17404", "pdf": "https://arxiv.org/pdf/2505.17404", "abs": "https://arxiv.org/abs/2505.17404", "authors": ["Kaicheng Zhang", "Sinian Zhang", "Doudou Zhou", "Yidong Zhou"], "title": "Wasserstein Transfer Learning", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": "25 pages, 6 figures", "summary": "Transfer learning is a powerful paradigm for leveraging knowledge from source\ndomains to enhance learning in a target domain. However, traditional transfer\nlearning approaches often focus on scalar or multivariate data within Euclidean\nspaces, limiting their applicability to complex data structures such as\nprobability distributions. To address this, we introduce a novel framework for\ntransfer learning in regression models, where outputs are probability\ndistributions residing in the Wasserstein space. When the informative subset of\ntransferable source domains is known, we propose an estimator with provable\nasymptotic convergence rates, quantifying the impact of domain similarity on\ntransfer efficiency. For cases where the informative subset is unknown, we\ndevelop a data-driven transfer learning procedure designed to mitigate negative\ntransfer. The proposed methods are supported by rigorous theoretical analysis\nand are validated through extensive simulations and real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728Wasserstein\u7a7a\u95f4\u4e2d\u5904\u7406\u6982\u7387\u5206\u5e03\u8f93\u51fa\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u5df2\u77e5\u548c\u672a\u77e5\u4fe1\u606f\u6e90\u57df\u5b50\u96c6\u7684\u60c5\u51b5\u3002", "motivation": "\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u6570\u636e\uff08\u5982\u6982\u7387\u5206\u5e03\uff09\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u3002", "method": "1. \u5df2\u77e5\u4fe1\u606f\u6e90\u57df\u5b50\u96c6\u65f6\uff0c\u63d0\u51fa\u5177\u6709\u6e10\u8fdb\u6536\u655b\u901f\u7387\u7684\u4f30\u8ba1\u5668\uff1b2. \u672a\u77e5\u65f6\uff0c\u8bbe\u8ba1\u6570\u636e\u9a71\u52a8\u7684\u8fc1\u79fb\u5b66\u4e60\u7a0b\u5e8f\u4ee5\u51cf\u5c11\u8d1f\u8fc1\u79fb\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6846\u67b6\u6269\u5c55\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u590d\u6742\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.17102", "pdf": "https://arxiv.org/pdf/2505.17102", "abs": "https://arxiv.org/abs/2505.17102", "authors": ["Pramit Bhattacharyya", "Arnab Bhattacharya"], "title": "BanglaByT5: Byte-Level Modelling for Bangla", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing tasks. However, most LLM models use traditional\ntokenizers like BPE and SentencePiece, which fail to capture the finer nuances\nof a morphologically rich language like Bangla (Bengali). In this work, we\nintroduce BanglaByT5, the first byte-level encoder-decoder model explicitly\ntailored for Bangla. Built upon a small variant of Googles ByT5 architecture,\nBanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality\nliterary and newspaper articles. Through zeroshot and supervised evaluations\nacross generative and classification tasks, BanglaByT5 demonstrates competitive\nperformance, surpassing several multilingual and larger models. Our findings\nhighlight the efficacy of byte-level modelling for morphologically rich\nlanguages and highlight BanglaByT5 potential as a lightweight yet powerful tool\nfor Bangla NLP, particularly in both resource-constrained and scalable\nenvironments.", "AI": {"tldr": "BanglaByT5\u662f\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7684\u5b57\u8282\u7ea7\u7f16\u7801-\u89e3\u7801\u6a21\u578b\uff0c\u57fa\u4e8eByT5\u67b6\u6784\uff0c\u572814GB\u7cbe\u9009\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\uff0c\u8868\u73b0\u4f18\u4e8e\u591a\u8bed\u8a00\u548c\u5927\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5206\u8bcd\u5668\uff08\u5982BPE\uff09\u65e0\u6cd5\u6355\u6349\u5b5f\u52a0\u62c9\u8bed\u7684\u5f62\u6001\u4e30\u5bcc\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u5b57\u8282\u7ea7\u6a21\u578b\u3002", "method": "\u57fa\u4e8eByT5\u67b6\u6784\uff0c\u9884\u8bad\u7ec314GB\u7cbe\u9009\u8bed\u6599\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u548c\u76d1\u7763\u8bc4\u4f30\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5728\u751f\u6210\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u591a\u8bed\u8a00\u548c\u5927\u6a21\u578b\u3002", "conclusion": "\u5b57\u8282\u7ea7\u5efa\u6a21\u5bf9\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u6709\u6548\uff0cBanglaByT5\u662f\u8f7b\u91cf\u4f46\u5f3a\u5927\u7684\u5de5\u5177\u3002", "relevance": 60.0}}
{"id": "2505.17493", "pdf": "https://arxiv.org/pdf/2505.17493", "abs": "https://arxiv.org/abs/2505.17493", "authors": ["Jingde Huang", "Zhangyu Huang", "Chenyu Li", "Jiantong Liu"], "title": "Research on Defect Detection Method of Motor Control Board Based on Image Processing", "categories": ["cs.CV"], "comment": null, "summary": "The motor control board has various defects such as inconsistent color\ndifferences, incorrect plug-in positions, solder short circuits, and more.\nThese defects directly affect the performance and stability of the motor\ncontrol board, thereby having a negative impact on product quality. Therefore,\nstudying the defect detection technology of the motor control board is an\nimportant means to improve the quality control level of the motor control\nboard. Firstly, the processing methods of digital images about the motor\ncontrol board were studied, and the noise suppression methods that affect image\nfeature extraction were analyzed. Secondly, a specific model for defect feature\nextraction and color difference recognition of the tested motor control board\nwas established, and qualified or defective products were determined based on\nfeature thresholds. Thirdly, the search algorithm for defective images was\noptimized. Finally, comparative experiments were conducted on the typical motor\ncontrol board, and the experimental results demonstrate that the accuracy of\nthe motor control board defect detection model-based on image processing\nestablished in this paper reached over 99%. It is suitable for timely image\nprocessing of large quantities of motor control boards on the production line,\nand achieved efficient defect detection. The defect detection method can not\nonly be used for online detection of the motor control board defects, but also\nprovide solutions for the integrated circuit board defect processing for the\nindustry.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5904\u7406\u7684\u7535\u673a\u63a7\u5236\u677f\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6291\u5236\u3001\u7279\u5f81\u63d0\u53d6\u548c\u4f18\u5316\u641c\u7d22\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e8699%\u4ee5\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u7535\u673a\u63a7\u5236\u677f\u7684\u7f3a\u9677\u76f4\u63a5\u5f71\u54cd\u4ea7\u54c1\u8d28\u91cf\uff0c\u7814\u7a76\u7f3a\u9677\u68c0\u6d4b\u6280\u672f\u662f\u63d0\u5347\u8d28\u91cf\u63a7\u5236\u6c34\u5e73\u7684\u91cd\u8981\u624b\u6bb5\u3002", "method": "\u7814\u7a76\u4e86\u7535\u673a\u63a7\u5236\u677f\u7684\u6570\u5b57\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u7f3a\u9677\u7279\u5f81\u63d0\u53d6\u548c\u8272\u5dee\u8bc6\u522b\u6a21\u578b\uff0c\u4f18\u5316\u4e86\u7f3a\u9677\u56fe\u50cf\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u7ebf\u4e0a\u7684\u5927\u6279\u91cf\u5b9e\u65f6\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u7528\u4e8e\u7535\u673a\u63a7\u5236\u677f\u7684\u5728\u7ebf\u68c0\u6d4b\uff0c\u8fd8\u4e3a\u96c6\u6210\u7535\u8def\u677f\u7684\u7f3a\u9677\u5904\u7406\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 10.0}}
{"id": "2505.18135", "pdf": "https://arxiv.org/pdf/2505.18135", "abs": "https://arxiv.org/abs/2505.18135", "authors": ["Kazem Faghih", "Wenxiao Wang", "Yize Cheng", "Siddhant Bharti", "Gaurang Sriramanan", "Sriram Balasubramanian", "Parsa Hosseini", "Soheil Feizi"], "title": "Gaming Tool Preferences in Agentic LLMs", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86LLMs\u5728\u4f9d\u8d56\u5de5\u5177\u63cf\u8ff0\u9009\u62e9\u5de5\u5177\u65f6\u7684\u8106\u5f31\u6027\uff0c\u901a\u8fc7\u4fee\u6539\u63cf\u8ff0\u53ef\u663e\u8457\u5f71\u54cd\u5de5\u5177\u4f7f\u7528\u7387\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u5de5\u5177\u9009\u62e9\u8fc7\u7a0b\u4e2d\u5bf9\u6587\u672c\u63cf\u8ff0\u7684\u4f9d\u8d56\u53ca\u5176\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u7f16\u8f91\u5de5\u5177\u63cf\u8ff0\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\u5bf9\u63cf\u8ff0\u4fee\u6539\u7684\u54cd\u5e94\u3002", "result": "\u4fee\u6539\u540e\u7684\u5de5\u5177\u63cf\u8ff0\u4f7fGPT-4.1\u548cQwen2.5-7B\u7684\u4f7f\u7528\u7387\u63d0\u9ad810\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u9700\u4e3aLLMs\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5de5\u5177\u9009\u62e9\u57fa\u7840\u3002", "relevance": 85.0}}
{"id": "2505.17431", "pdf": "https://arxiv.org/pdf/2505.17431", "abs": "https://arxiv.org/abs/2505.17431", "authors": ["Boyuan Li", "Yicheng Luo", "Zhen Liu", "Junhao Zheng", "Jianming Lv", "Qianli Ma"], "title": "HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted in ICML 2025", "summary": "Irregular multivariate time series (IMTS) are characterized by irregular time\nintervals within variables and unaligned observations across variables, posing\nchallenges in learning temporal and variable dependencies. Many existing IMTS\nmodels either require padded samples to learn separately from temporal and\nvariable dimensions, or represent original samples via bipartite graphs or\nsets. However, the former approaches often need to handle extra padding values\naffecting efficiency and disrupting original sampling patterns, while the\nlatter ones have limitations in capturing dependencies among unaligned\nobservations. To represent and learn both dependencies from original\nobservations in a unified form, we propose HyperIMTS, a Hypergraph neural\nnetwork for Irregular Multivariate Time Series forecasting. Observed values are\nconverted as nodes in the hypergraph, interconnected by temporal and variable\nhyperedges to enable message passing among all observations. Through\nirregularity-aware message passing, HyperIMTS captures variable dependencies in\na time-adaptive way to achieve accurate forecasting. Experiments demonstrate\nHyperIMTS's competitive performance among state-of-the-art models in IMTS\nforecasting with low computational cost.", "AI": {"tldr": "HyperIMTS\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff08IMTS\uff09\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u7684\u6d88\u606f\u4f20\u9012\u6355\u6349\u53d8\u91cf\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "IMTS\u6570\u636e\u56e0\u65f6\u95f4\u95f4\u9694\u4e0d\u89c4\u5219\u548c\u53d8\u91cf\u89c2\u6d4b\u672a\u5bf9\u9f50\u800c\u96be\u4ee5\u5efa\u6a21\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u586b\u5145\u503c\u5f71\u54cd\u6548\u7387\u6216\u4f9d\u8d56\u5173\u7cfb\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHyperIMTS\uff0c\u5c06\u89c2\u6d4b\u503c\u8f6c\u6362\u4e3a\u8d85\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u901a\u8fc7\u65f6\u95f4\u548c\u53d8\u91cf\u8d85\u8fb9\u8fde\u63a5\uff0c\u5b9e\u73b0\u6d88\u606f\u4f20\u9012\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHyperIMTS\u5728IMTS\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "HyperIMTS\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u6355\u6349IMTS\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17103", "pdf": "https://arxiv.org/pdf/2505.17103", "abs": "https://arxiv.org/abs/2505.17103", "authors": ["C\u00e9cile Rousseau", "Tobia Boschi", "Giandomenico Cornacchia", "Dhaval Salwala", "Alessandra Pascale", "Juan Bernabe Moreno"], "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. SDForger\nsource code will be open-sourced soon.", "AI": {"tldr": "SDForger\u662f\u4e00\u4e2a\u9ad8\u6548\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5229\u7528LLMs\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u548c\u4f4e\u8ba1\u7b97\u5fae\u8c03\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8868\u683c\u5d4c\u5165\uff0c\u7f16\u7801\u4e3a\u6587\u672c\u540e\u5fae\u8c03LLM\uff0c\u751f\u6210\u4fdd\u7559\u539f\u59cb\u6570\u636e\u7edf\u8ba1\u7279\u6027\u548c\u65f6\u95f4\u52a8\u6001\u7684\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\uff0cSDForger\u5728\u76f8\u4f3c\u6027\u8bc4\u4f30\u548c\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u6a21\u578b\u3002", "conclusion": "SDForger\u4e3a\u591a\u6a21\u6001\u5efa\u6a21\u548c\u65f6\u95f4\u5e8f\u5217\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 70.0}}
{"id": "2505.17501", "pdf": "https://arxiv.org/pdf/2505.17501", "abs": "https://arxiv.org/abs/2505.17501", "authors": ["Yuehan Jin", "Xiaoqing Liu", "Yiyuan Yang", "Zhiwen Yu", "Tong Zhang", "Kaixiang Yang"], "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoHyDR\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6269\u6563\u548c\u5bf9\u6297\u5b66\u4e60\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u6062\u590d\u7f3a\u5931\u6a21\u6001\u7684\u6570\u636e\uff0c\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u56e0\u6570\u636e\u7f3a\u5931\u6216\u635f\u574f\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6297\u5b66\u4e60\uff0c\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u5c42\u9762\u6062\u590d\u7f3a\u5931\u6570\u636e\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "RoHyDR\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "relevance": 40.0}}
{"id": "2505.18139", "pdf": "https://arxiv.org/pdf/2505.18139", "abs": "https://arxiv.org/abs/2505.18139", "authors": ["Gordon Dai", "Yunze Xiao"], "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "categories": ["cs.AI", "cs.CY"], "comment": "14 pages,2 figure", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u8d1f\u8d23\u4efbAI\uff08RAI\uff09\u6307\u6807\u7684\u7406\u8bba\u4e0d\u4e00\u81f4\u6027\u89c6\u4e3a\u4f18\u70b9\u800c\u975e\u7f3a\u9677\uff0c\u8ba4\u4e3a\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u80fd\u5e26\u6765\u89c4\u8303\u6027\u591a\u5143\u6027\u3001\u8ba4\u8bc6\u8bba\u5b8c\u6574\u6027\u548c\u9690\u5f0f\u6b63\u5219\u5316\u4e09\u5927\u597d\u5904\u3002", "motivation": "\u63a2\u8ba8RAI\u6307\u6807\u7684\u7406\u8bba\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u5176\u4f5c\u4e3a\u591a\u6837\u6027\u4ef7\u503c\u7684\u8868\u73b0\uff0c\u800c\u975e\u9700\u6d88\u9664\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u4e00\u81f4\u6027\u5bf9RAI\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5c06\u5176\u89c6\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4e0d\u4e00\u81f4\u6027\u6709\u52a9\u4e8e\u4fdd\u6301\u9053\u5fb7\u591a\u6837\u6027\u3001\u63d0\u5347\u6982\u5ff5\u5b8c\u6574\u6027\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u5efa\u8baeRAI\u9886\u57df\u63a5\u53d7\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u7814\u7a76\u5176\u53ef\u63a5\u53d7\u9608\u503c\u53ca\u5b9e\u8df5\u4e2d\u7684\u7a33\u5065\u673a\u5236\u3002", "relevance": 75.0}}
{"id": "2505.17435", "pdf": "https://arxiv.org/pdf/2505.17435", "abs": "https://arxiv.org/abs/2505.17435", "authors": ["Hongyi Henry Jin", "Zijun Ding", "Dung Daniel Ngo", "Zhiwei Steven Wu"], "title": "Discretization-free Multicalibration through Loss Minimization over Tree Ensembles", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, multicalibration has emerged as a desirable learning\nobjective for ensuring that a predictor is calibrated across a rich collection\nof overlapping subpopulations. Existing approaches typically achieve\nmulticalibration by discretizing the predictor's output space and iteratively\nadjusting its output values. However, this discretization approach departs from\nthe standard empirical risk minimization (ERM) pipeline, introduces rounding\nerror and additional sensitive hyperparameter, and may distort the predictor's\noutputs in ways that hinder downstream decision-making.\n  In this work, we propose a discretization-free multicalibration method that\ndirectly optimizes an empirical risk objective over an ensemble of depth-two\ndecision trees. Our ERM approach can be implemented using off-the-shelf tree\nensemble learning methods such as LightGBM. Our algorithm provably achieves\nmulticalibration, provided that the data distribution satisfies a technical\ncondition we term as loss saturation. Across multiple datasets, our empirical\nevaluation shows that this condition is always met in practice. Our\ndiscretization-free algorithm consistently matches or outperforms existing\nmulticalibration approaches--even when evaluated using a discretization-based\nmulticalibration metric that shares its discretization granularity with the\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u79bb\u6563\u5316\u7684\u591a\u6821\u51c6\u65b9\u6cd5\uff0c\u76f4\u63a5\u4f18\u5316\u57fa\u4e8e\u6df1\u5ea6\u4e8c\u51b3\u7b56\u6811\u96c6\u5408\u7684\u7ecf\u9a8c\u98ce\u9669\u76ee\u6807\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u5316\u65b9\u6cd5\u5728\u591a\u6821\u51c6\u4e2d\u5f15\u5165\u8bef\u5dee\u548c\u8d85\u53c2\u6570\u95ee\u9898\uff0c\u5f71\u54cd\u9884\u6d4b\u5668\u8f93\u51fa\u548c\u4e0b\u6e38\u51b3\u7b56\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u4e8c\u51b3\u7b56\u6811\u96c6\u5408\u76f4\u63a5\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u76ee\u6807\uff0c\u65e0\u9700\u79bb\u6563\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b0\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65e0\u9700\u79bb\u6563\u5316\u7684\u591a\u6821\u51c6\u65b9\u6cd5\u6709\u6548\u4e14\u5b9e\u7528\u3002", "relevance": 40.0}}
{"id": "2505.17104", "pdf": "https://arxiv.org/pdf/2505.17104", "abs": "https://arxiv.org/abs/2505.17104", "authors": ["Tao Sun", "Enhao Pan", "Zhengkai Yang", "Kaixin Sui", "Jiajun Shi", "Xianfu Cheng", "Tongliang Li", "Wenhao Huang", "Ge Zhang", "Jian Yang", "Zhoujun Li"], "title": "P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Academic posters are vital for scholarly communication, yet their manual\ncreation is time-consuming. However, automated academic poster generation faces\nsignificant challenges in preserving intricate scientific details and achieving\neffective visual-textual integration. Existing approaches often struggle with\nsemantic richness and structural nuances, and lack standardized benchmarks for\nevaluating generated academic posters comprehensively. To address these\nlimitations, we introduce P2P, the first flexible, LLM-based multi-agent\nframework that generates high-quality, HTML-rendered academic posters directly\nfrom research papers, demonstrating strong potential for practical\napplications. P2P employs three specialized agents-for visual element\nprocessing, content generation, and final poster assembly-each integrated with\ndedicated checker modules to enable iterative refinement and ensure output\nquality. To foster advancements and rigorous evaluation in this domain, we\nconstruct and release P2PInstruct, the first large-scale instruction dataset\ncomprising over 30,000 high-quality examples tailored for the academic\npaper-to-poster generation task. Furthermore, we establish P2PEval, a\ncomprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation\nmethodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and\ndetailed, human-annotated checklists. Our contributions aim to streamline\nresearch dissemination and provide the community with robust tools for\ndeveloping and evaluating next-generation poster generation systems.", "AI": {"tldr": "P2P\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7814\u7a76\u8bba\u6587\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5b66\u672f\u6d77\u62a5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u6d77\u62a5\u624b\u52a8\u5236\u4f5c\u8017\u65f6\u4e14\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u4fdd\u7559\u79d1\u5b66\u7ec6\u8282\u548c\u89c6\u89c9-\u6587\u672c\u6574\u5408\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff08\u89c6\u89c9\u5143\u7d20\u5904\u7406\u3001\u5185\u5bb9\u751f\u6210\u3001\u6d77\u62a5\u7ec4\u88c5\uff09\u548c\u68c0\u67e5\u6a21\u5757\uff0c\u7ed3\u5408P2PInstruct\u6570\u636e\u96c6\u548cP2PEval\u57fa\u51c6\u3002", "result": "\u751f\u6210\u4e86\u9ad8\u8d28\u91cfHTML\u6e32\u67d3\u7684\u5b66\u672f\u6d77\u62a5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "P2P\u6846\u67b6\u548c\u914d\u5957\u5de5\u5177\u4e3a\u5b66\u672f\u6d77\u62a5\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "relevance": 40.0}}
{"id": "2505.17509", "pdf": "https://arxiv.org/pdf/2505.17509", "abs": "https://arxiv.org/abs/2505.17509", "authors": ["Shiji Zhao", "Qihui Zhu", "Shukun Xiong", "Shouwei Ruan", "Yize Fan", "Ranjie Duan", "Qing Guo", "Xingxing Wei"], "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdversarial Mixture Prompt Tuning (AMPT)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u5b66\u4e60\u548c\u6761\u4ef6\u6743\u91cd\u8def\u7531\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u6297\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u7136\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4f46\u5bf9\u5bf9\u6297\u6837\u672c\u9ad8\u5ea6\u654f\u611f\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u53ef\u5b66\u4e60\u6587\u672c\u63d0\u793a\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faAMPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u5b66\u4e60\u548c\u6761\u4ef6\u6743\u91cd\u8def\u7531\u751f\u6210\u6837\u672c\u7279\u5b9a\u7684\u805a\u5408\u6587\u672c\u7279\u5f81\uff0c\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50\u5bf9\u6297\u56fe\u50cf\u7279\u5f81\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cAMPT\u65b9\u6cd5\u5728\u4e0d\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "conclusion": "AMPT\u901a\u8fc7\u591a\u63d0\u793a\u5b66\u4e60\u548c\u52a8\u6001\u6743\u91cd\u5206\u914d\uff0c\u6709\u6548\u63d0\u5347\u4e86VLMs\u5bf9\u6297\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "relevance": 75.0}}
{"id": "2505.17023", "pdf": "https://arxiv.org/pdf/2505.17023", "abs": "https://arxiv.org/abs/2505.17023", "authors": ["Hugo Chateau-Laurent", "Tara Vanhatalo"], "title": "ReMi: A Random Recurrent Neural Network Approach to Music Production", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted for an Innovation Showcase Demo at International Computer\n  Music Conference", "summary": "Generative artificial intelligence raises concerns related to energy\nconsumption, copyright infringement and creative atrophy. We show that randomly\ninitialized recurrent neural networks can produce arpeggios and low-frequency\noscillations that are rich and configurable. In contrast to end-to-end music\ngeneration that aims to replace musicians, our approach expands their\ncreativity while requiring no data and much less computational power. More\ninformation can be found at: https://allendia.com/", "AI": {"tldr": "\u968f\u673a\u521d\u59cb\u5316\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u751f\u6210\u4e30\u5bcc\u4e14\u53ef\u914d\u7f6e\u7684\u97f3\u4e50\u7247\u6bb5\uff0c\u65e0\u9700\u6570\u636e\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u65e8\u5728\u6269\u5c55\u97f3\u4e50\u5bb6\u521b\u9020\u529b\u800c\u975e\u66ff\u4ee3\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u5728\u80fd\u6e90\u6d88\u8017\u3001\u7248\u6743\u4fb5\u6743\u548c\u521b\u9020\u529b\u8870\u9000\u65b9\u9762\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e3a\u97f3\u4e50\u5bb6\u63d0\u4f9b\u4f4e\u6210\u672c\u7684\u521b\u610f\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u968f\u673a\u521d\u59cb\u5316\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u97f3\u4e50\u7247\u6bb5\uff0c\u7279\u522b\u662f\u7436\u97f3\u548c\u4f4e\u9891\u632f\u8361\u3002", "result": "\u751f\u6210\u7684\u97f3\u4e50\u7247\u6bb5\u4e30\u5bcc\u4e14\u53ef\u914d\u7f6e\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u97f3\u4e50\u5bb6\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u521b\u610f\u6269\u5c55\u5de5\u5177\uff0c\u907f\u514d\u4e86\u751f\u6210\u5f0fAI\u7684\u5e38\u89c1\u95ee\u9898\u3002", "relevance": 30.0}}
{"id": "2505.17439", "pdf": "https://arxiv.org/pdf/2505.17439", "abs": "https://arxiv.org/abs/2505.17439", "authors": ["Weijia Jin"], "title": "Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study designs an efficient and equitable humanitarian supply chain\ndynamically by using reinforcement learning, PPO, and compared with heuristic\nalgorithms. This study demonstrates the model of PPO always treats average\nsatisfaction rate as the priority.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u516c\u5e73\u7684\u4eba\u9053\u4e3b\u4e49\u4f9b\u5e94\u94fe\u6a21\u578b\uff0c\u5e76\u4e0e\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4eba\u9053\u4e3b\u4e49\u4f9b\u5e94\u94fe\u7684\u52a8\u6001\u5206\u914d\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u3002", "method": "\u91c7\u7528PPO\uff08\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\u7b97\u6cd5\uff0c\u5e76\u4e0e\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "PPO\u6a21\u578b\u5728\u5e73\u5747\u6ee1\u610f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "conclusion": "PPO\u5728\u4eba\u9053\u4e3b\u4e49\u4f9b\u5e94\u94fe\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u516c\u5e73\u6027\u548c\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.17106", "pdf": "https://arxiv.org/pdf/2505.17106", "abs": "https://arxiv.org/abs/2505.17106", "authors": ["Yifei Liu", "Yu Cui", "Haibin Zhang"], "title": "RRTL: Red Teaming Reasoning Large Language Models in Tool Learning", "categories": ["cs.CL"], "comment": null, "summary": "While tool learning significantly enhances the capabilities of large language\nmodels (LLMs), it also introduces substantial security risks. Prior research\nhas revealed various vulnerabilities in traditional LLMs during tool learning.\nHowever, the safety of newly emerging reasoning LLMs (RLLMs), such as\nDeepSeek-R1, in the context of tool learning remains underexplored. To bridge\nthis gap, we propose RRTL, a red teaming approach specifically designed to\nevaluate RLLMs in tool learning. It integrates two novel strategies: (1) the\nidentification of deceptive threats, which evaluates the model's behavior in\nconcealing the usage of unsafe tools and their potential risks; and (2) the use\nof Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also\nincludes a benchmark for traditional LLMs. We conduct a comprehensive\nevaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs\ngenerally achieve stronger safety performance than traditional LLMs, yet\nsubstantial safety disparities persist across models; (2) RLLMs can pose\nserious deceptive risks by frequently failing to disclose tool usage and to\nwarn users of potential tool output risks; (3) CoT prompting reveals\nmulti-lingual safety vulnerabilities in RLLMs. Our work provides important\ninsights into enhancing the security of RLLMs in tool learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRRTL\u7684\u7ea2\u961f\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08RLLMs\uff09\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0RLLMs\u5728\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edfLLMs\uff0c\u4f46\u4ecd\u5b58\u5728\u6b3a\u9a97\u6027\u98ce\u9669\u548c\u591a\u8bed\u8a00\u6f0f\u6d1e\u3002", "motivation": "\u63a2\u7d22\u65b0\u5174\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08RLLMs\uff09\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faRRTL\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b3a\u9a97\u6027\u5a01\u80c1\u8bc6\u522b\u548cChain-of-Thought\uff08CoT\uff09\u63d0\u793a\uff0c\u5bf9\u4e03\u79cd\u4e3b\u6d41RLLMs\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0RLLMs\u5b89\u5168\u6027\u4f18\u4e8e\u4f20\u7edfLLMs\uff0c\u4f46\u5b58\u5728\u6b3a\u9a97\u6027\u98ce\u9669\u548c\u591a\u8bed\u8a00\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347RLLMs\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.17529", "pdf": "https://arxiv.org/pdf/2505.17529", "abs": "https://arxiv.org/abs/2505.17529", "authors": ["Yeongjae Cho", "Keonwoo Kim", "Taebaek Hwang", "Sungzoon Cho"], "title": "Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly expanded their utility in tasks like image captioning and visual\nquestion answering. However, they still struggle with object hallucination,\nwhere models generate descriptions that inaccurately reflect the visual content\nby including nonexistent objects or misrepresenting existing ones. While\nprevious methods, such as data augmentation and training-free approaches,\nstrive to tackle this issue, they still encounter scalability challenges and\noften depend on additional external modules. In this work, we propose Ensemble\nDecoding (ED), a novel strategy that splits the input image into sub-images and\ncombines logit distributions by assigning weights through the attention map.\nFurthermore, we introduce ED adaptive plausibility constraint to calibrate\nlogit distribution and FastED, a variant designed for speed-critical\napplications. Extensive experiments across hallucination benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance, validating the\neffectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnsemble Decoding (ED)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5272\u8f93\u5165\u56fe\u50cf\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u56fe\u52a0\u6743\u7684logit\u5206\u5e03\uff0c\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u6a21\u578b\u751f\u6210\u4e0d\u51c6\u786e\u7684\u63cf\u8ff0\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEnsemble Decoding (ED)\uff0c\u5c06\u8f93\u5165\u56fe\u50cf\u5206\u5272\u4e3a\u5b50\u56fe\u50cf\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u52a0\u6743\u7ed3\u5408logit\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u5408\u7406\u6027\u7ea6\u675f\u548c\u5feb\u901f\u53d8\u4f53FastED\u3002", "result": "\u5728\u591a\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ED\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\u3002", "relevance": 60.0}}
{"id": "2505.17448", "pdf": "https://arxiv.org/pdf/2505.17448", "abs": "https://arxiv.org/abs/2505.17448", "authors": ["Bhanuka Gamage", "Adnan Labib", "Aisha Joomun", "Chern Hong Lim", "KokSheik Wong"], "title": "Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Appear in IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP'21), Toronto, ON, Canada", "summary": "Following the rising popularity of YouTube, there is an emerging problem on\nthis platform called clickbait, which provokes users to click on videos using\nattractive titles and thumbnails. As a result, users ended up watching a video\nthat does not have the content as publicized in the title. This issue is\naddressed in this study by proposing an algorithm called BaitRadar, which uses\na deep learning technique where six inference models are jointly consulted to\nmake the final classification decision. These models focus on different\nattributes of the video, including title, comments, thumbnail, tags, video\nstatistics and audio transcript. The final classification is attained by\ncomputing the average of multiple models to provide a robust and accurate\noutput even in situation where there is missing data. The proposed method is\ntested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved\nwith an inference time of less than 2s.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBaitRadar\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u516d\u4e2a\u63a8\u7406\u6a21\u578b\u6765\u68c0\u6d4bYouTube\u4e0a\u7684\u70b9\u51fb\u8bf1\u9975\u5185\u5bb9\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe98%\u3002", "motivation": "\u89e3\u51b3YouTube\u4e0a\u70b9\u51fb\u8bf1\u9975\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u907f\u514d\u7528\u6237\u88ab\u8bef\u5bfc\u3002", "method": "\u4f7f\u7528\u516d\u4e2a\u63a8\u7406\u6a21\u578b\u5206\u522b\u5206\u6790\u89c6\u9891\u7684\u4e0d\u540c\u5c5e\u6027\uff08\u6807\u9898\u3001\u8bc4\u8bba\u3001\u7f29\u7565\u56fe\u7b49\uff09\uff0c\u901a\u8fc7\u5e73\u5747\u8ba1\u7b97\u5f97\u51fa\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u57281,400\u4e2aYouTube\u89c6\u9891\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4e3a98%\uff0c\u63a8\u7406\u65f6\u95f4\u5c0f\u4e8e2\u79d2\u3002", "conclusion": "BaitRadar\u80fd\u6709\u6548\u68c0\u6d4b\u70b9\u51fb\u8bf1\u9975\u5185\u5bb9\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u3002", "relevance": 30.0}}
{"id": "2505.17110", "pdf": "https://arxiv.org/pdf/2505.17110", "abs": "https://arxiv.org/abs/2505.17110", "authors": ["Junlin Li", "Guodong DU", "Jing Li", "Sim Kuan Goh", "Wenya Wang", "Yequan Wang", "Fangming Liu", "Ho-Kin Tang", "Saleh Alharbi", "Daojing He", "Min Zhang"], "title": "Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) with multimodal encoders on\nmodality-specific data expands the modalities that LLMs can handle, leading to\nthe formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies\non resource-intensive and inflexible fine-tuning from scratch with new\nmultimodal data. In this paper, we propose MMER (Multi-modality Expansion and\nRetention), a training-free approach that integrates existing MLLMs for\neffective multimodal expansion while retaining their original performance.\nSpecifically, MMER reuses MLLMs' multimodal encoders while merging their LLM\nparameters. By comparing original and merged LLM parameters, MMER generates\nbinary masks to approximately separate LLM parameters for each modality. These\ndecoupled parameters can independently process modality-specific inputs,\nreducing parameter conflicts and preserving original MLLMs' fidelity. MMER can\nalso mitigate catastrophic forgetting by applying a similar process to MLLMs\nfine-tuned on new tasks. Extensive experiments show significant improvements\nover baselines, proving that MMER effectively expands LLMs' multimodal\ncapabilities while retaining 99% of the original performance, and also markedly\nmitigates catastrophic forgetting.", "AI": {"tldr": "MMER\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u7528\u73b0\u6709MLLM\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\u5e76\u5408\u5e76\u5176LLM\u53c2\u6570\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u6269\u5c55\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u5fae\u8c03\uff0c\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\uff0cMMER\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u751f\u6210\u4e8c\u8fdb\u5236\u63a9\u7801\u5206\u79bbLLM\u53c2\u6570\uff0c\u72ec\u7acb\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u51cf\u5c11\u53c2\u6570\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMMER\u5728\u6269\u5c55\u591a\u6a21\u6001\u80fd\u529b\u7684\u540c\u65f6\u4fdd\u7559\u4e8699%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u5e76\u663e\u8457\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "MMER\u4e3a\u591a\u6a21\u6001LLM\u7684\u6269\u5c55\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17534", "pdf": "https://arxiv.org/pdf/2505.17534", "abs": "https://arxiv.org/abs/2505.17534", "authors": ["Jingjing Jiang", "Chongjie Si", "Jun Luo", "Hanwang Zhang", "Chao Ma"], "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a\nunified RL stage for joint optimization and a refined RL stage for\ntask-specific enhancement. With the proposed CoRL, our resulting model,\n\\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three\ntext-to-image generation datasets and \\textbf{23%} on nine multimodal\nunderstanding benchmarks. These results demonstrate the effectiveness of CoRL\nand highlight the substantial benefit of reinforcement learning in facilitating\ncross-task synergy and optimization for ULMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoRL\u7684\u534f\u540c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08ULMs\uff09\u7684\u751f\u6210\u548c\u7406\u89e3\u80fd\u529b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540c\u65f6\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u534f\u540c\u4f18\u5316\u3002", "method": "\u63d0\u51faCoRL\u6846\u67b6\uff0c\u5305\u62ec\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u7ec6\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6a21\u578bULM-R1\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53477%\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u534723%\u3002", "conclusion": "CoRL\u6846\u67b6\u6709\u6548\u4fc3\u8fdb\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u4efb\u52a1\u534f\u540c\u4f18\u5316\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728ULMs\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.17451", "pdf": "https://arxiv.org/pdf/2505.17451", "abs": "https://arxiv.org/abs/2505.17451", "authors": ["Zhining Liu", "Zihao Li", "Ze Yang", "Tianxin Wei", "Jian Kang", "Yada Zhu", "Hendrik Hamann", "Jingrui He", "Hanghang Tong"], "title": "CLIMB: Class-imbalanced Learning Benchmark on Tabular Data", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 7 figures, 8 tables", "summary": "Class-imbalanced learning (CIL) on tabular data is important in many\nreal-world applications where the minority class holds the critical but rare\noutcomes. In this paper, we present CLIMB, a comprehensive benchmark for\nclass-imbalanced learning on tabular data. CLIMB includes 73 real-world\ndatasets across diverse domains and imbalance levels, along with unified\nimplementations of 29 representative CIL algorithms. Built on a high-quality\nopen-source Python package with unified API designs, detailed documentation,\nand rigorous code quality controls, CLIMB supports easy implementation and\ncomparison between different CIL algorithms. Through extensive experiments, we\nprovide practical insights on method accuracy and efficiency, highlighting the\nlimitations of naive rebalancing, the effectiveness of ensembles, and the\nimportance of data quality. Our code, documentation, and examples are available\nat https://github.com/ZhiningLiu1998/imbalanced-ensemble.", "AI": {"tldr": "CLIMB\u662f\u4e00\u4e2a\u7528\u4e8e\u8868\u683c\u6570\u636e\u7c7b\u522b\u4e0d\u5e73\u8861\u5b66\u4e60\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b73\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c29\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684API\u8bbe\u8ba1\u548c\u8be6\u7ec6\u6587\u6863\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u6570\u636e\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6570\u7c7b\u522b\u5173\u952e\u4f46\u7f55\u89c1\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u6784\u5efaCLIMB\u57fa\u51c6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u5b9e\u73b0\u548c\u7edf\u4e00API\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6734\u7d20\u91cd\u5e73\u8861\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u96c6\u6210\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u6570\u636e\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "CLIMB\u4e3a\u7c7b\u522b\u4e0d\u5e73\u8861\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u89c1\u89e3\uff0c\u5f3a\u8c03\u6570\u636e\u8d28\u91cf\u548c\u96c6\u6210\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "relevance": 30.0}}
{"id": "2505.17112", "pdf": "https://arxiv.org/pdf/2505.17112", "abs": "https://arxiv.org/abs/2505.17112", "authors": ["Robin Segerer"], "title": "Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek", "categories": ["cs.CL"], "comment": "15 pages, 1 table, 1 figure", "summary": "This study examines cultural value alignment in large language models (LLMs)\nby analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from\nSchwartz's value framework. Using the 40-item Portrait Values Questionnaire, we\nassessed whether DeepSeek, trained on Chinese-language data, exhibits distinct\nvalue preferences compared to Western models. Results of a Bayesian ordinal\nregression model show that self-transcendence values (e.g., benevolence,\nuniversalism) were highly prioritized across all models, reflecting a general\nLLM tendency to emphasize prosocial values. However, DeepSeek uniquely\ndownplayed self-enhancement values (e.g., power, achievement) compared to\nChatGPT and Gemini, aligning with collectivist cultural tendencies. These\nfindings suggest that LLMs reflect culturally situated biases rather than a\nuniversal ethical framework. To address value asymmetries in LLMs, we propose\nmulti-perspective reasoning, self-reflective feedback, and dynamic\ncontextualization. This study contributes to discussions on AI fairness,\ncultural neutrality, and the need for pluralistic AI alignment frameworks that\nintegrate diverse moral perspectives.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u5316\u4ef7\u503c\u5bf9\u9f50\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek\u5728\u4ef7\u503c\u89c2\u504f\u597d\u4e0a\u4e0e\u897f\u65b9\u6a21\u578b\u5b58\u5728\u5dee\u5f02\uff0c\u53cd\u6620\u4e86\u6587\u5316\u504f\u89c1\u3002", "motivation": "\u5206\u6790LLMs\u662f\u5426\u53cd\u6620\u6587\u5316\u504f\u89c1\uff0c\u63a2\u8ba8\u5982\u4f55\u5b9e\u73b0\u66f4\u516c\u5e73\u7684AI\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528Schwartz\u4ef7\u503c\u89c2\u6846\u67b6\u548c\u8d1d\u53f6\u65af\u5e8f\u6570\u56de\u5f52\u6a21\u578b\u6bd4\u8f83Gemini\u3001ChatGPT\u548cDeepSeek\u7684\u4ef7\u503c\u89c2\u4f18\u5148\u7ea7\u3002", "result": "DeepSeek\u5728\u81ea\u6211\u8d85\u8d8a\u4ef7\u503c\u89c2\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u81ea\u6211\u589e\u5f3a\u4ef7\u503c\u89c2\u4e0a\u4e0e\u897f\u65b9\u6a21\u578b\u4e0d\u540c\uff0c\u4f53\u73b0\u4e86\u96c6\u4f53\u4e3b\u4e49\u6587\u5316\u503e\u5411\u3002", "conclusion": "LLMs\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u9700\u591a\u89c6\u89d2\u63a8\u7406\u548c\u52a8\u6001\u60c5\u5883\u5316\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684AI\u5bf9\u9f50\u3002", "relevance": 85.0}}
{"id": "2505.17540", "pdf": "https://arxiv.org/pdf/2505.17540", "abs": "https://arxiv.org/abs/2505.17540", "authors": ["Mingrui Wu", "Lu Wang", "Pu Zhao", "Fangkai Yang", "Jianjin Zhang", "Jianfeng Liu", "Yuefeng Zhan", "Weihao Han", "Hao Sun", "Jiayi Ji", "Xiaoshuai Sun", "Qingwei Lin", "Weiwei Deng", "Dongmei Zhang", "Feng Sun", "Qi Zhang", "Rongrong Ji"], "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at:\n  https://github.com/microsoft/DKI_LLM/tree/main/RePrompt", "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.", "AI": {"tldr": "RePrompt\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u663e\u5f0f\u63a8\u7406\u7684\u63d0\u793a\u589e\u5f3a\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7a7a\u95f4\u5e03\u5c40\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u4ece\u7b80\u77ed\u63d0\u793a\u4e2d\u6355\u6349\u7528\u6237\u610f\u56fe\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63d0\u793a\u589e\u5f3a\u65b9\u6cd5\u5e38\u56e0\u7f3a\u4e4f\u89c6\u89c9\u8bed\u4e49\u548c\u73b0\u5b9e\u7ec4\u5408\u7684\u63a5\u5730\u6027\u800c\u751f\u6210\u4e0d\u5207\u5b9e\u9645\u7684\u5185\u5bb9\u3002", "method": "\u63d0\u51faRePrompt\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\u3001\u81ea\u53cd\u601d\u7684\u63d0\u793a\uff0c\u4f18\u5316\u56fe\u50cf\u7ea7\u7ed3\u679c\u3002\u5b9a\u5236\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u4eba\u7c7b\u504f\u597d\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u7ec4\u5408\u3002", "result": "\u5728GenEval\u548cT2I-Compbench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRePrompt\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u5e03\u5c40\u4fdd\u771f\u5ea6\u548c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u3002", "conclusion": "RePrompt\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u95f4\u63a5\u76d1\u7763\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.17454", "pdf": "https://arxiv.org/pdf/2505.17454", "abs": "https://arxiv.org/abs/2505.17454", "authors": ["Hyosoon Jang", "Yunhui Jang", "Sungjae Lee", "Jungseul Ok", "Sungsoo Ahn"], "title": "Self-Training Large Language Models with Confident Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive performance by generating\nreasoning paths before final answers, but learning such a reasoning path\nrequires costly human supervision. To address this issue, recent studies have\nexplored self-training methods that improve reasoning capabilities using\npseudo-labels generated by the LLMs themselves. Among these, confidence-based\nself-training fine-tunes LLMs to prefer reasoning paths with high-confidence\nanswers, where confidence is estimated via majority voting. However, such\nmethods exclusively focus on the quality of the final answer and may ignore the\nquality of the reasoning paths, as even an incorrect reasoning path leads to a\ncorrect answer by chance. Instead, we advocate the use of reasoning-level\nconfidence to identify high-quality reasoning paths for self-training,\nsupported by our empirical observations. We then propose a new self-training\nmethod, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths\nthrough Policy Optimization. Our experiments show that CORE-PO improves the\naccuracy of outputs on four in-distribution and two out-of-distribution\nbenchmarks, compared to existing self-training methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCORE-PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u8def\u5f84\u7684\u7f6e\u4fe1\u5ea6\u6765\u63d0\u5347LLM\u7684\u81ea\u8bad\u7ec3\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8def\u5f84\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u53ef\u80fd\u9519\u8bef\u8def\u5f84\u88ab\u8bef\u7528\u3002", "method": "\u63d0\u51faCORE-PO\u65b9\u6cd5\uff0c\u5229\u7528\u63a8\u7406\u7ea7\u7f6e\u4fe1\u5ea6\u9009\u62e9\u9ad8\u8d28\u91cf\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u4f18\u5316\u5fae\u8c03LLM\u3002", "result": "\u5728\u56db\u4e2a\u5206\u5e03\u5185\u548c\u4e24\u4e2a\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCORE-PO\u663e\u8457\u63d0\u5347\u4e86\u8f93\u51fa\u51c6\u786e\u6027\u3002", "conclusion": "\u63a8\u7406\u7ea7\u7f6e\u4fe1\u5ea6\u662f\u63d0\u5347LLM\u81ea\u8bad\u7ec3\u6548\u679c\u7684\u5173\u952e\uff0cCORE-PO\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 90.0}}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114", "abs": "https://arxiv.org/abs/2505.17114", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.", "AI": {"tldr": "RAVEN\u662f\u4e00\u79cd\u591a\u6a21\u6001\u95ee\u7b54\u67b6\u6784\uff0c\u901a\u8fc7QuART\u6a21\u5757\u52a8\u6001\u5206\u914d\u6a21\u6001\u76f8\u5173\u6027\u5206\u6570\uff0c\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\u5e76\u6291\u5236\u5e72\u6270\u3002\u8bad\u7ec3\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6AVS-QA\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u95ee\u7b54\u4e2d\u6a21\u6001\u4e0d\u4e00\u81f4\u95ee\u9898\uff08\u5982\u65e0\u5173\u97f3\u9891\u6216\u89c6\u9891\u5e72\u6270\uff09\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u52a8\u6001\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faRAVEN\u67b6\u6784\uff0c\u6838\u5fc3\u662fQuART\u6a21\u5757\uff0c\u52a8\u6001\u8bc4\u5206\u6a21\u6001\u76f8\u5173\u6027\uff1b\u8bad\u7ec3\u5206\u4e09\u9636\u6bb5\uff1a\u5355\u6a21\u6001\u9884\u8bad\u7ec3\u3001\u67e5\u8be2\u5bf9\u9f50\u878d\u5408\u548c\u6a21\u6001\u4e0d\u5339\u914d\u5fae\u8c03\u3002", "result": "\u5728\u4e03\u5927\u591a\u6a21\u6001QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAVEN\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe14.5%\uff0c\u4f20\u611f\u5668\u6570\u636e\u8fdb\u4e00\u6b65\u5e26\u676516.4%\u589e\u76ca\uff0c\u4e14\u5bf9\u6a21\u6001\u635f\u574f\u9c81\u68d2\u6027\u7a81\u51fa\u3002", "conclusion": "RAVEN\u901a\u8fc7\u52a8\u6001\u6a21\u6001\u9009\u62e9\u548c\u5206\u9636\u6bb5\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u95ee\u7b54\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6a21\u6001\u4e0d\u4e00\u81f4\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "relevance": 70.0}}
{"id": "2505.17550", "pdf": "https://arxiv.org/pdf/2505.17550", "abs": "https://arxiv.org/abs/2505.17550", "authors": ["Xiaoyu Ye", "Songjie Cheng", "Yongtao Wang", "Yajiao Xiong", "Yishen Li"], "title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u7684\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u4ee5\u9632\u6b62\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u7531\u4e8eT2V\u6a21\u578b\u53ef\u80fd\u751f\u6210\u6709\u5bb3\u6216\u4fb5\u6743\u5185\u5bb9\uff0c\u4f5c\u8005\u53d7\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u9057\u5fd8\u6280\u672f\u7684\u542f\u53d1\uff0c\u5c06\u5176\u6269\u5c55\u5230T2V\u9886\u57df\uff0c\u4ee5\u89e3\u51b3\u6f5c\u5728\u6ee5\u7528\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8d1f\u5f15\u5bfc\u901f\u5ea6\u9884\u6d4b\u5fae\u8c03\uff0c\u7ed3\u5408\u63d0\u793a\u589e\u5f3a\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u5b9a\u4f4d\u548c\u4fdd\u62a4\u6b63\u5219\u5316\u4ee5\u5b9e\u73b0\u7cbe\u786e\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5220\u9664\u7279\u5b9a\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u5176\u4ed6\u6982\u5ff5\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aT2V\u6a21\u578b\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u3002", "relevance": 60.0}}
{"id": "2505.17458", "pdf": "https://arxiv.org/pdf/2505.17458", "abs": "https://arxiv.org/abs/2505.17458", "authors": ["Guiquan Sun", "Xikun Zhang", "Jingchao Ni", "Dongjin Song"], "title": "Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning on heterogeneous graphs has experienced rapid advancement in\nrecent years, driven by the inherently heterogeneous nature of real-world data.\nHowever, existing studies typically assume the graphs to be static, while\nreal-world graphs are continuously expanding. This dynamic nature requires\nmodels to adapt to new data while preserving existing knowledge. To this end,\nthis work addresses the challenge of continual learning on heterogeneous graphs\nby introducing the Meta-learning based Knowledge Distillation framework (MKD),\ndesigned to mitigate catastrophic forgetting in evolving heterogeneous graph\nstructures. MKD combines rapid task adaptation through meta-learning on limited\nsamples with knowledge distillation to achieve an optimal balance between\nincorporating new information and maintaining existing knowledge. To improve\nthe efficiency and effectiveness of sample selection, MKD incorporates a novel\nsampling strategy that selects a small number of target-type nodes based on\nnode diversity and maintains fixed-size buffers for other types. The strategy\nretrieves first-order neighbors along metapaths and selects important neighbors\nbased on their structural relevance, enabling the sampled subgraphs to retain\nkey topological and semantic information. In addition, MKD introduces a\nsemantic-level distillation module that aligns the attention distributions over\ndifferent metapaths between teacher and student models, encouraging semantic\nconsistency beyond the logit level. Comprehensive evaluations across three\nbenchmark datasets validate MKD's effectiveness in handling continual learning\nscenarios on expanding heterogeneous graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86MKD\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u56fe\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u4f18\u5316\u65b0\u4fe1\u606f\u4e0e\u65e7\u77e5\u8bc6\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u6570\u636e\u662f\u52a8\u6001\u6269\u5c55\u7684\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5047\u8bbe\u56fe\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5f02\u6784\u56fe\u7684\u6301\u7eed\u5b66\u4e60\u9700\u6c42\u3002", "method": "MKD\u6846\u67b6\u7ed3\u5408\u5143\u5b66\u4e60\u5feb\u901f\u9002\u5e94\u4efb\u52a1\u548c\u77e5\u8bc6\u84b8\u998f\u4fdd\u6301\u65e7\u77e5\u8bc6\uff0c\u91c7\u7528\u65b0\u9896\u7684\u91c7\u6837\u7b56\u7565\u548c\u8bed\u4e49\u7ea7\u84b8\u998f\u6a21\u5757\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MKD\u5728\u52a8\u6001\u5f02\u6784\u56fe\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MKD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u5f02\u6784\u56fe\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5e73\u8861\u4e86\u65b0\u4fe1\u606f\u4e0e\u65e7\u77e5\u8bc6\u3002", "relevance": 40.0}}
{"id": "2505.17116", "pdf": "https://arxiv.org/pdf/2505.17116", "abs": "https://arxiv.org/abs/2505.17116", "authors": ["Akash Dhruv", "Yangxinyu Xie", "Jordan Branham", "Tanwi Mallick"], "title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data", "categories": ["cs.CL", "cs.ET"], "comment": null, "summary": "This paper presents a comparative study of large language models (LLMs) in\ninterpreting grid-structured geospatial data. We evaluate the performance of a\nbase model through structured prompting and contrast it with a fine-tuned\nvariant trained on a dataset of user-assistant interactions. Our results\nhighlight the strengths and limitations of zero-shot prompting and demonstrate\nthe benefits of fine-tuning for structured geospatial and temporal reasoning.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u91ca\u7f51\u683c\u7ed3\u6784\u5730\u7406\u7a7a\u95f4\u6570\u636e\u65f6\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u57fa\u7840\u6a21\u578b\u548c\u5fae\u8c03\u53d8\u4f53\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5904\u7406\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u6570\u636e\u65f6\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u63d0\u793a\u548c\u5fae\u8c03\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u57fa\u4e8e\u7528\u6237-\u52a9\u624b\u4ea4\u4e92\u6570\u636e\u96c6\u5fae\u8c03\u7684\u53d8\u4f53\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u96f6\u6837\u672c\u63d0\u793a\u6709\u5176\u5c40\u9650\u6027\uff0c\u800c\u5fae\u8c03\u5728\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u548c\u65f6\u5e8f\u63a8\u7406\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5fae\u8c03\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\uff0c\u4f46\u9700\u8981\u6743\u8861\u8bad\u7ec3\u6210\u672c\u3002", "relevance": 60.0}}
{"id": "2505.17551", "pdf": "https://arxiv.org/pdf/2505.17551", "abs": "https://arxiv.org/abs/2505.17551", "authors": ["Qiyu Chen", "Huiyuan Luo", "Haiming Yao", "Wei Luo", "Zhen Qu", "Chengkan Lv", "Zhengtao Zhang"], "title": "Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Industrial Informatics (TII)", "summary": "Anomaly detection plays a vital role in the inspection of industrial images.\nMost existing methods require separate models for each category, resulting in\nmultiplied deployment costs. This highlights the challenge of developing a\nunified model for multi-class anomaly detection. However, the significant\nincrease in inter-class interference leads to severe missed detections.\nFurthermore, the intra-class overlap between normal and abnormal samples,\nparticularly in synthesis-based methods, cannot be ignored and may lead to\nover-detection. To tackle these issues, we propose a novel Center-aware\nResidual Anomaly Synthesis (CRAS) method for multi-class anomaly detection.\nCRAS leverages center-aware residual learning to couple samples from different\ncategories into a unified center, mitigating the effects of inter-class\ninterference. To further reduce intra-class overlap, CRAS introduces\ndistance-guided anomaly synthesis that adaptively adjusts noise variance based\non normal data distribution. Experimental results on diverse datasets and\nreal-world industrial applications demonstrate the superior detection accuracy\nand competitive inference speed of CRAS. The source code and the newly\nconstructed dataset are publicly available at\nhttps://github.com/cqylunlun/CRAS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRAS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u4e2d\u5fc3\u611f\u77e5\u6b8b\u5dee\u5b66\u4e60\u548c\u8ddd\u79bb\u5f15\u5bfc\u5f02\u5e38\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7c7b\u522b\u95f4\u5e72\u6270\u548c\u7c7b\u522b\u5185\u91cd\u53e0\u7684\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\uff0c\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u540c\u65f6\uff0c\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7c7b\u522b\u95f4\u5e72\u6270\u548c\u7c7b\u522b\u5185\u91cd\u53e0\u95ee\u9898\u4e25\u91cd\uff0c\u5f71\u54cd\u68c0\u6d4b\u6548\u679c\u3002", "method": "CRAS\u65b9\u6cd5\u7ed3\u5408\u4e2d\u5fc3\u611f\u77e5\u6b8b\u5dee\u5b66\u4e60\u548c\u8ddd\u79bb\u5f15\u5bfc\u5f02\u5e38\u5408\u6210\uff0c\u524d\u8005\u5c06\u4e0d\u540c\u7c7b\u522b\u7684\u6837\u672c\u8026\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u4e2d\u5fc3\u4ee5\u51cf\u5c11\u7c7b\u522b\u95f4\u5e72\u6270\uff0c\u540e\u8005\u6839\u636e\u6b63\u5e38\u6570\u636e\u5206\u5e03\u81ea\u9002\u5e94\u8c03\u6574\u566a\u58f0\u65b9\u5dee\u4ee5\u51cf\u5c11\u7c7b\u522b\u5185\u91cd\u53e0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCRAS\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "CRAS\u4e3a\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 20.0}}
{"id": "2505.17469", "pdf": "https://arxiv.org/pdf/2505.17469", "abs": "https://arxiv.org/abs/2505.17469", "authors": ["Lukas Silvester Barth", "Paulo von Petersenn"], "title": "Efficient compression of neural networks and datasets", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.OC", "math.ST", "stat.TH", "94-08, 94-04, 68T07, 68T50", "E.4; H.1.1; I.2; I.2.6; I.2.7"], "comment": "10 pages plus appendix, 9 Figures, 3 Tables", "summary": "We compare, improve, and contribute methods that substantially decrease the\nnumber of parameters of neural networks while maintaining high test accuracy.\nWhen applying our methods to minimize description length, we obtain very\neffective data compression algorithms. In particular, we develop a\nprobabilistic reformulation of $\\ell_0$ regularized optimization for nonlinear\nmodels that does not require Monte-Carlo sampling and thus improves upon\nprevious methods. We also improve upon methods involving smooth approximations\nto the $\\ell_0$ norm, and investigate layerwise methods. We compare the methods\non different architectures and datasets, including convolutional networks\ntrained on image datasets and transformers trained on parts of Wikipedia. We\nalso created a synthetic teacher-student setup to investigate compression in a\ncontrolled continuous setting. Finally, we conceptually relate compression\nalgorithms to Solomonoff's theory of inductive inference and empirically verify\nthe prediction that regularized models can exhibit more sample-efficient\nconvergence.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117", "abs": "https://arxiv.org/abs/2505.17117", "authors": ["Chen Shani", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5185\u90e8\u8868\u5f81\u4e0e\u4eba\u7c7b\u8bed\u4e49\u538b\u7f29\u7b56\u7565\u7684\u5dee\u5f02\uff0c\u53d1\u73b0LLMs\u503e\u5411\u4e8e\u7edf\u8ba1\u538b\u7f29\uff0c\u800c\u4eba\u7c7b\u66f4\u6ce8\u91cd\u8bed\u4e49\u7ec6\u8282\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u5728\u538b\u7f29\u4e0e\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6743\u8861\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u6846\u67b6\uff08\u7387\u5931\u771f\u7406\u8bba\u548c\u4fe1\u606f\u74f6\u9888\u539f\u7406\uff09\uff0c\u6bd4\u8f83LLMs\u4e0e\u4eba\u7c7b\u5206\u7c7b\u7b56\u7565\u3002", "result": "LLMs\u5f62\u6210\u5e7f\u6cdb\u6982\u5ff5\u7c7b\u522b\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u5bf9\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u533a\u5206\u80fd\u529b\uff1bLLMs\u504f\u5411\u7edf\u8ba1\u538b\u7f29\uff0c\u4eba\u7c7b\u5219\u6ce8\u91cd\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u67b6\u6784\u7684\u5173\u952e\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u66f4\u4eba\u7c7b\u5bf9\u9f50\u7684LLMs\u63d0\u4f9b\u6307\u5bfc\u3002", "relevance": 85.0}}
{"id": "2505.17560", "pdf": "https://arxiv.org/pdf/2505.17560", "abs": "https://arxiv.org/abs/2505.17560", "authors": ["Shahin Hakemi", "Naveed Akhtar", "Ghulam Mubashar Hassan", "Ajmal Mian"], "title": "Deeper Diffusion Models Amplify Bias", "categories": ["cs.CV"], "comment": null, "summary": "Despite the impressive performance of generative Diffusion Models (DMs),\ntheir internal working is still not well understood, which is potentially\nproblematic. This paper focuses on exploring the important notion of\nbias-variance tradeoff in diffusion models. Providing a systematic foundation\nfor this exploration, it establishes that at one extreme the diffusion models\nmay amplify the inherent bias in the training data and, on the other, they may\ncompromise the presumed privacy of the training samples. Our exploration aligns\nwith the memorization-generalization understanding of the generative models,\nbut it also expands further along this spectrum beyond ``generalization'',\nrevealing the risk of bias amplification in deeper models. Building on the\ninsights, we also introduce a training-free method to improve output quality in\ntext-to-image and image-to-image generation. By progressively encouraging\ntemporary high variance in the generation process with partial bypassing of the\nmid-block's contribution in the denoising process of DMs, our method\nconsistently improves generative image quality with zero training cost. Our\nclaims are validated both theoretically and empirically.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u53ef\u80fd\u653e\u5927\u8bad\u7ec3\u6570\u636e\u504f\u5dee\u6216\u6cc4\u9732\u9690\u79c1\u7684\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u5dee\u653e\u5927\u6216\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "method": "\u901a\u8fc7\u90e8\u5206\u7ed5\u8fc7\u53bb\u566a\u8fc7\u7a0b\u4e2d\u95f4\u5757\u7684\u8d21\u732e\uff0c\u9010\u6b65\u9f13\u52b1\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6682\u65f6\u9ad8\u65b9\u5dee\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u4e2d\u5b58\u5728\u6f5c\u5728\u98ce\u9669\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u751f\u6210\u8d28\u91cf\u3002", "relevance": 60.0}}
{"id": "2505.17477", "pdf": "https://arxiv.org/pdf/2505.17477", "abs": "https://arxiv.org/abs/2505.17477", "authors": ["Victor OK Li", "Yang Han", "Jacqueline CK Lam", "Lawrence YL Cheung"], "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural\nnetwork backtracking architecture designed to enhance Alzheimer's Disease (AD)\ndiagnosis through speech analysis. Leveraging the power of pre-trained large\nlanguage models, RSF identifies and utilizes the most probable AD-specific\nspeech markers, addressing both the scarcity of real AD speech samples and the\nchallenge of limited interpretability in existing models. RSF's unique approach\nconsists of three core innovations: Firstly, it exploits the observation that\nspeech markers most probable of predicting AD, defined as the most probable\nspeech-markers (MPMs), must have the highest probability of activating those\nneurons (in the neural network) with the highest probability of predicting AD,\ndefined as the most probable neurons (MPNs). Secondly, it utilizes a speech\ntoken representation at the input layer, allowing backtracking from MPNs to\nidentify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an\ninnovative backtracking method to track backwards from the MPNs to the input\nlayer, identifying the MPTs and the corresponding MPMs, and ingeniously\nuncovering novel speech markers for AD detection. Experimental results\ndemonstrate RSF's superiority over traditional methods such as SHAP and\nIntegrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost\nin F1-score. By generating speech data that encapsulates novel markers, RSF not\nonly mitigates the limitations of real data scarcity but also significantly\nenhances the robustness and accuracy of AD diagnostic models. These findings\nunderscore RSF's potential as a transformative tool in speech-based AD\ndetection, offering new insights into AD-related linguistic deficits and paving\nthe way for more effective non-invasive early intervention strategies.", "AI": {"tldr": "RSF\u662f\u4e00\u79cd\u521b\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u56de\u6eaf\u67b6\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u8bed\u97f3\u8bca\u65ad\uff0c\u901a\u8fc7\u8bc6\u522bAD\u7279\u5f02\u6027\u8bed\u97f3\u6807\u8bb0\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3AD\u8bed\u97f3\u6837\u672c\u7a00\u7f3a\u548c\u73b0\u6709\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "RSF\u901a\u8fc7\u56de\u6eaf\u4ece\u6700\u53ef\u80fd\u795e\u7ecf\u5143\uff08MPNs\uff09\u5230\u8f93\u5165\u5c42\uff0c\u8bc6\u522b\u6700\u53ef\u80fd\u8bed\u97f3\u6807\u8bb0\uff08MPTs\uff09\u548c\u8bed\u97f3\u6807\u8bb0\uff08MPMs\uff09\uff0c\u5e76\u751f\u6210\u65b0\u8bed\u97f3\u6570\u636e\u3002", "result": "RSF\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u63d0\u53473.5%\u548c3.2%\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5982SHAP\u548cIntegrated Gradients\u3002", "conclusion": "RSF\u4e3aAD\u8bed\u97f3\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86AD\u76f8\u5173\u8bed\u8a00\u7f3a\u9677\uff0c\u4e3a\u975e\u4fb5\u5165\u6027\u65e9\u671f\u5e72\u9884\u7b56\u7565\u94fa\u5e73\u9053\u8def\u3002", "relevance": 40.0}}
{"id": "2505.17118", "pdf": "https://arxiv.org/pdf/2505.17118", "abs": "https://arxiv.org/abs/2505.17118", "authors": ["Xinbang Dai", "Huikang Hu", "Yuncheng Hua", "Jiaqi Li", "Yongrui Chen", "Rihui Jin", "Nan Hu", "Guilin Qi"], "title": "After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG", "categories": ["cs.CL", "I.2.7"], "comment": "24 pages, 8 figures", "summary": "Retrieval-augmented generation (RAG) systems face critical challenges in\nbalancing internal (parametric) and external (retrieved) knowledge, especially\nwhen these sources conflict or are unreliable. To analyze these scenarios\ncomprehensively, we construct the Trustworthiness Response Dataset (TRD) with\n36,266 questions spanning four RAG settings. We reveal that existing approaches\naddress isolated scenarios-prioritizing one knowledge source, naively merging\nboth, or refusing answers-but lack a unified framework to handle different\nreal-world conditions simultaneously. Therefore, we propose the BRIDGE\nframework, which dynamically determines a comprehensive response strategy of\nlarge language models (LLMs). BRIDGE leverages an adaptive weighting mechanism\nnamed soft bias to guide knowledge collection, followed by a Maximum Soft-bias\nDecision Tree to evaluate knowledge and select optimal response strategies\n(trust internal/external knowledge, or refuse). Experiments show BRIDGE\noutperforms baselines by 5-15% in accuracy while maintaining balanced\nperformance across all scenarios. Our work provides an effective solution for\nLLMs' trustworthy responses in real-world RAG applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BRIDGE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u673a\u5236\u548c\u51b3\u7b56\u6811\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u5185\u90e8\u4e0e\u5916\u90e8\u77e5\u8bc6\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf5-15%\u3002", "motivation": "RAG\u7cfb\u7edf\u5728\u5904\u7406\u5185\u90e8\uff08\u53c2\u6570\u5316\uff09\u548c\u5916\u90e8\uff08\u68c0\u7d22\uff09\u77e5\u8bc6\u51b2\u7a81\u65f6\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u54cd\u5e94\u3002", "method": "\u63d0\u51faBRIDGE\u6846\u67b6\uff0c\u4f7f\u7528\u8f6f\u504f\u7f6e\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\u548c\u6700\u5927\u8f6f\u504f\u7f6e\u51b3\u7b56\u6811\u52a8\u6001\u9009\u62e9\u54cd\u5e94\u7b56\u7565\u3002", "result": "BRIDGE\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf5-15%\uff0c\u5e76\u5728\u6240\u6709\u573a\u666f\u4e2d\u4fdd\u6301\u5e73\u8861\u6027\u80fd\u3002", "conclusion": "BRIDGE\u4e3aLLM\u5728RAG\u5e94\u7528\u4e2d\u63d0\u4f9b\u53ef\u4fe1\u8d56\u7684\u54cd\u5e94\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17561", "pdf": "https://arxiv.org/pdf/2505.17561", "abs": "https://arxiv.org/abs/2505.17561", "authors": ["Kwanyoung Kim", "Sanghyun Kim"], "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 10 figures", "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aANSE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u566a\u58f0\u79cd\u5b50\uff0c\u4ee5\u63d0\u9ad8\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u521d\u59cb\u566a\u58f0\u7684\u9009\u62e9\u5bf9\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u5185\u90e8\u6a21\u578b\u4fe1\u53f7\u3002", "method": "\u63d0\u51faANSE\u6846\u67b6\uff0c\u6838\u5fc3\u662fBANSA\uff08\u8d1d\u53f6\u65af\u4e3b\u52a8\u566a\u58f0\u9009\u62e9\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u71b5\u5206\u6b67\u91cf\u5316\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165Bernoulli\u63a9\u7801\u8fd1\u4f3c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5728CogVideoX-2B\u548c5B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cANSE\u4ec5\u589e\u52a08%\u548c13%\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u5373\u53ef\u663e\u8457\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "ANSE\u4e3a\u89c6\u9891\u6269\u6563\u4e2d\u7684\u566a\u58f0\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u548c\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.17478", "pdf": "https://arxiv.org/pdf/2505.17478", "abs": "https://arxiv.org/abs/2505.17478", "authors": ["Yuning Shen", "Lihao Wang", "Huizhuo Yuan", "Yan Wang", "Bangji Yang", "Quanquan Gu"], "title": "Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression", "categories": ["cs.LG", "cs.AI", "physics.bio-ph", "q-bio.BM", "q-bio.QM"], "comment": "33 pages, 17 figures", "summary": "Understanding protein dynamics is critical for elucidating their biological\nfunctions. The increasing availability of molecular dynamics (MD) data enables\nthe training of deep generative models to efficiently explore the\nconformational space of proteins. However, existing approaches either fail to\nexplicitly capture the temporal dependencies between conformations or do not\nsupport direct generation of time-independent samples. To address these\nlimitations, we introduce ConfRover, an autoregressive model that\nsimultaneously learns protein conformation and dynamics from MD trajectories,\nsupporting both time-dependent and time-independent sampling. At the core of\nour model is a modular architecture comprising: (i) an encoding layer, adapted\nfrom protein folding models, that embeds protein-specific information and\nconformation at each time frame into a latent space; (ii) a temporal module, a\nsequence model that captures conformational dynamics across frames; and (iii)\nan SE(3) diffusion model as the structure decoder, generating conformations in\ncontinuous space. Experiments on ATLAS, a large-scale protein MD dataset of\ndiverse structures, demonstrate the effectiveness of our model in learning\nconformational dynamics and supporting a wide range of downstream tasks.\nConfRover is the first model to sample both protein conformations and\ntrajectories within a single framework, offering a novel and flexible approach\nfor learning from protein MD data.", "AI": {"tldr": "ConfRover\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5206\u5b50\u52a8\u529b\u5b66\u8f68\u8ff9\u4e2d\u5b66\u4e60\u86cb\u767d\u8d28\u6784\u8c61\u548c\u52a8\u529b\u5b66\uff0c\u652f\u6301\u65f6\u95f4\u4f9d\u8d56\u548c\u72ec\u7acb\u91c7\u6837\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u660e\u786e\u6355\u6349\u6784\u8c61\u95f4\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u6216\u4e0d\u652f\u6301\u76f4\u63a5\u751f\u6210\u65f6\u95f4\u65e0\u5173\u6837\u672c\u3002", "method": "\u6a21\u578b\u5305\u542b\u7f16\u7801\u5c42\u3001\u65f6\u95f4\u6a21\u5757\u548cSE(3)\u6269\u6563\u7ed3\u6784\u89e3\u7801\u5668\uff0c\u7528\u4e8e\u5d4c\u5165\u86cb\u767d\u8d28\u4fe1\u606f\u3001\u6355\u6349\u6784\u8c61\u52a8\u6001\u5e76\u751f\u6210\u6784\u8c61\u3002", "result": "\u5728ATLAS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "ConfRover\u662f\u9996\u4e2a\u5728\u5355\u4e00\u6846\u67b6\u5185\u91c7\u6837\u86cb\u767d\u8d28\u6784\u8c61\u548c\u8f68\u8ff9\u7684\u6a21\u578b\uff0c\u4e3a\u86cb\u767d\u8d28\u5206\u5b50\u52a8\u529b\u5b66\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.17119", "pdf": "https://arxiv.org/pdf/2505.17119", "abs": "https://arxiv.org/abs/2505.17119", "authors": ["Zongru Shao", "Xin Wang", "Zhanyang Liu", "Chenhan Wang", "K. P. Subbalakshmi"], "title": "Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages without references", "summary": "Recent research leverages large language models (LLMs) for early mental\nhealth detection, such as depression, often optimized with machine-generated\ndata. However, their detection may be subject to unknown weaknesses. Meanwhile,\nquality control has not been applied to these generated corpora besides limited\nhuman verifications. Our goal is to systematically evaluate LLM reasoning and\nreveal potential weaknesses. To this end, we first provide a systematic\nevaluation of the reasoning over machine-generated detection and\ninterpretation. Then we use the models' reasoning abilities to explore\nmitigation strategies for enhanced performance. Specifically, we do the\nfollowing: A. Design an LLM instruction strategy that allows for systematic\nanalysis of the detection by breaking down the task into several subtasks. B.\nDesign contrastive few-shot and chain-of-thought prompts by selecting typical\npositive and negative examples of detection reasoning. C. Perform human\nannotation for the subtasks identified in the first step and evaluate the\nperformance. D. Identify human-preferred detection with desired logical\nreasoning from the few-shot generation and use them to explore different\noptimization strategies. We conducted extensive comparisons on the DepTweet\ndataset across the following subtasks: 1. identifying whether the speaker is\ndescribing their own depression; 2. accurately detecting the presence of PHQ-9\nsymptoms, and 3. finally, detecting depression. Human verification of\nstatistical outliers shows that LLMs demonstrate greater accuracy in analyzing\nand detecting explicit language of depression as opposed to implicit\nexpressions of depression. Two optimization methods are used for performance\nenhancement and reduction of the statistic bias: supervised fine-tuning (SFT)\nand direct preference optimization (DPO). Notably, the DPO approach achieves\nsignificant performance improvement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u5f31\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u7b56\u7565\uff08\u5982DPO\uff09\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793aLLM\u5728\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u4e2d\u7684\u672a\u77e5\u5f31\u70b9\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u548c\u4f18\u5316\u7b56\u7565\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "1. \u8bbe\u8ba1LLM\u6307\u4ee4\u7b56\u7565\uff0c\u5206\u89e3\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\uff1b2. \u8bbe\u8ba1\u5bf9\u6bd4\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff1b3. \u4eba\u5de5\u6807\u6ce8\u5b50\u4efb\u52a1\u5e76\u8bc4\u4f30\u6027\u80fd\uff1b4. \u63a2\u7d22\u4f18\u5316\u7b56\u7565\uff08\u5982SFT\u548cDPO\uff09\u3002", "result": "LLM\u5728\u663e\u6027\u6291\u90c1\u8bed\u8a00\u5206\u6790\u4e2d\u8868\u73b0\u66f4\u4f18\uff0cDPO\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u548c\u4f18\u5316\u7b56\u7565\uff0cLLM\u5728\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u5f97\u5230\u63d0\u5347\uff0c\u5c24\u5176\u662fDPO\u65b9\u6cd5\u6548\u679c\u663e\u8457\u3002", "relevance": 75.0}}
{"id": "2505.17567", "pdf": "https://arxiv.org/pdf/2505.17567", "abs": "https://arxiv.org/abs/2505.17567", "authors": ["Denisa Qosja", "Kilian Barth", "Simon Wagner"], "title": "Enhancing Fourier-based Doppler Resolution with Diffusion Models", "categories": ["cs.CV", "eess.SP"], "comment": "Published at International Radar Symposium (IRS) 2025", "summary": "In radar systems, high resolution in the Doppler dimension is important for\ndetecting slow-moving targets as it allows for more distinct separation between\nthese targets and clutter, or stationary objects. However, achieving sufficient\nresolution is constrained by hardware capabilities and physical factors,\nleading to the development of processing techniques to enhance the resolution\nafter acquisition. In this work, we leverage artificial intelligence to\nincrease the Doppler resolution in range-Doppler maps. Based on a zero-padded\nFFT, a refinement via the generative neural networks of diffusion models is\nachieved. We demonstrate that our method overcomes the limitations of\ntraditional FFT, generating data where closely spaced targets are effectively\nseparated.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u591a\u666e\u52d2\u7ef4\u5ea6\u7684\u5206\u8fa8\u7387\uff0c\u514b\u670d\u4f20\u7edfFFT\u7684\u9650\u5236\u3002", "motivation": "\u5728\u96f7\u8fbe\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u591a\u666e\u52d2\u5206\u8fa8\u7387\u5bf9\u4e8e\u68c0\u6d4b\u6162\u901f\u76ee\u6807\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u786c\u4ef6\u548c\u7269\u7406\u56e0\u7d20\u9650\u5236\u4e86\u5206\u8fa8\u7387\u7684\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u540e\u5904\u7406\u6280\u672f\u3002", "method": "\u57fa\u4e8e\u96f6\u586b\u5145FFT\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u8fa8\u7387\u7ec6\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5206\u79bb\u4e86\u7d27\u5bc6\u95f4\u9694\u7684\u76ee\u6807\uff0c\u514b\u670d\u4e86\u4f20\u7edfFFT\u7684\u5c40\u9650\u6027\u3002", "conclusion": "AI\u6280\u672f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u96f7\u8fbe\u7cfb\u7edf\u7684\u591a\u666e\u52d2\u5206\u8fa8\u7387\u3002", "relevance": 20.0}}
{"id": "2505.17483", "pdf": "https://arxiv.org/pdf/2505.17483", "abs": "https://arxiv.org/abs/2505.17483", "authors": ["Yiqing Guo", "Nagur Cherukuru", "Eric Lehmann", "S. L. Kesav Unnithan", "Gemma Kerrisk", "Tim Malthus", "Faisal Islam"], "title": "Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning", "categories": ["cs.LG"], "comment": "Submitted to IGARSS2025", "summary": "Nitrate ($\\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived\nprimarily from anthropogenic sources. The recent increase in river-discharged\nnitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)\nlagoon. Although nitrate is an optically inactive (i.e., colourless)\nconstituent, previous studies have demonstrated there is an indirect,\nnon-causal relationship between water surface nitrate and water-leaving\nreflectance that is mediated through optically active water quality parameters\nsuch as total suspended solids and coloured dissolved organic matter. This work\naims to advance our understanding of this relationship with an effort to\nmeasure time-series nitrate and simultaneous hyperspectral reflectance at the\nFitzroy River estuary, Queensland, Australia. Time-series observations revealed\nperiodic cycles in nitrate loads due to the tidal influence in the estuarine\nstudy site. The water surface nitrate loads were predicted from hyperspectral\nreflectance and water salinity measurements, with hyperspectral reflectance\nindicating the concentrations of optically active variables and salinity\nindicating the mixing of river water and seawater proportions. The accuracy\nassessment of model-predicted nitrate against in-situ measured nitrate values\nshowed that the predicted nitrate values correlated well with the ground-truth\ndata, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work\ndemonstrates the feasibility of predicting water surface nitrate from\nhyperspectral reflectance and salinity measurements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u785d\u9178\u76d0\u4e0e\u9ad8\u5149\u8c31\u53cd\u5c04\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u89c2\u6d4b\u548c\u6a21\u578b\u9884\u6d4b\uff0c\u5c55\u793a\u4e86\u4ece\u9ad8\u5149\u8c31\u53cd\u5c04\u7387\u548c\u76d0\u5ea6\u6d4b\u91cf\u4e2d\u9884\u6d4b\u785d\u9178\u76d0\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7814\u7a76\u785d\u9178\u76d0\u5bf9\u73ca\u745a\u767d\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u9ad8\u5149\u8c31\u53cd\u5c04\u7387\u548c\u76d0\u5ea6\u6d4b\u91cf\u9884\u6d4b\u785d\u9178\u76d0\u6d53\u5ea6\u3002", "method": "\u5728Fitzroy\u6cb3\u53e3\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u785d\u9178\u76d0\u548c\u9ad8\u5149\u8c31\u53cd\u5c04\u7387\u89c2\u6d4b\uff0c\u5229\u7528\u6a21\u578b\u9884\u6d4b\u785d\u9178\u76d0\u6d53\u5ea6\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u7684\u785d\u9178\u76d0\u4e0e\u5b9e\u6d4b\u503c\u76f8\u5173\u6027\u9ad8\uff08R\u00b2=0.86\uff0cRMSE=0.03 mg/L\uff09\u3002", "conclusion": "\u9ad8\u5149\u8c31\u53cd\u5c04\u7387\u548c\u76d0\u5ea6\u6d4b\u91cf\u53ef\u7528\u4e8e\u9884\u6d4b\u785d\u9178\u76d0\u6d53\u5ea6\u3002", "relevance": 10.0}}
{"id": "2505.17120", "pdf": "https://arxiv.org/pdf/2505.17120", "abs": "https://arxiv.org/abs/2505.17120", "authors": ["Dillon Plunkett", "Adam Morris", "Keerthi Reddy", "Jorge Morales"], "title": "Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training", "categories": ["cs.CL"], "comment": null, "summary": "We have only limited understanding of how and why large language models\n(LLMs) respond in the ways that they do. Their neural networks have proven\nchallenging to interpret, and we are only beginning to tease out the function\nof individual neurons and circuits within them. However, another path to\nunderstanding these systems is to investigate and develop their capacity to\nintrospect and explain their own functioning. Here, we show that i)\ncontemporary LLMs are capable of providing accurate, quantitative descriptions\nof their own internal processes during certain kinds of decision-making, ii)\nthat it is possible to improve these capabilities through training, and iii)\nthat this training generalizes to at least some degree. To do so, we fine-tuned\nGPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts\n(e.g., choosing between condos, loans, vacations, etc.) according to\nrandomly-generated, quantitative preferences about how to weigh different\nattributes during decision-making (e.g., the relative importance of natural\nlight versus quiet surroundings for condos). We demonstrate that the LLMs can\naccurately report these preferences (i.e., the weights that they learned to\ngive to different attributes during decision-making). Next, we demonstrate that\nthese LLMs can be fine-tuned to explain their decision-making even more\naccurately. Finally, we demonstrate that this training generalizes: It improves\nthe ability of the models to accurately explain what they are doing as they\nmake other complex decisions, not just decisions they have learned to make via\nfine-tuning. This work is a step towards training LLMs to accurately and\nbroadly report on their own internal processes -- a possibility that would\nyield substantial benefits for interpretability, control, and safety.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u5b9a\u91cf\u63cf\u8ff0\u5176\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u63d0\u5347\u8fd9\u79cd\u80fd\u529b\uff0c\u4e14\u5177\u5907\u4e00\u5b9a\u6cdb\u5316\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLMs\u5982\u4f55\u901a\u8fc7\u81ea\u7701\u548c\u89e3\u91ca\u81ea\u8eab\u529f\u80fd\u6765\u589e\u5f3a\u5bf9\u5176\u5185\u90e8\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u4ece\u800c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3001\u63a7\u5236\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5bf9GPT-4o\u548cGPT-4o-mini\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b66\u4e60\u5e76\u62a5\u544a\u5b9a\u91cf\u504f\u597d\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u63d0\u5347\u5176\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLMs\u80fd\u591f\u51c6\u786e\u62a5\u544a\u5176\u51b3\u7b56\u504f\u597d\uff0c\u4e14\u901a\u8fc7\u8bad\u7ec3\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u89e3\u91ca\u80fd\u529b\uff0c\u5e76\u5177\u5907\u4e00\u5b9a\u6cdb\u5316\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bad\u7ec3LLMs\u51c6\u786e\u62a5\u544a\u5176\u5185\u90e8\u8fc7\u7a0b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5bf9\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u6709\u91cd\u8981\u610f\u4e49\u3002", "relevance": 90.0}}
{"id": "2505.17574", "pdf": "https://arxiv.org/pdf/2505.17574", "abs": "https://arxiv.org/abs/2505.17574", "authors": ["Xueji Fang", "Liyuan Ma", "Zhiyang Chen", "Mingyuan Zhou", "Guo-jun Qi"], "title": "InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "Recent advances in text-to-video generation, particularly with autoregressive\nmodels, have enabled the synthesis of high-quality videos depicting individual\nscenes. However, extending these models to generate long, cross-scene videos\nremains a significant challenge. As the context length grows during\nautoregressive decoding, computational costs rise sharply, and the model's\nability to maintain consistency and adhere to evolving textual prompts\ndeteriorates. We introduce InfLVG, an inference-time framework that enables\ncoherent long video generation without requiring additional long-form video\ndata. InfLVG leverages a learnable context selection policy, optimized via\nGroup Relative Policy Optimization (GRPO), to dynamically identify and retain\nthe most semantically relevant context throughout the generation process.\nInstead of accumulating the entire generation history, the policy ranks and\nselects the top-$K$ most contextually relevant tokens, allowing the model to\nmaintain a fixed computational budget while preserving content consistency and\nprompt alignment. To optimize the policy, we design a hybrid reward function\nthat jointly captures semantic alignment, cross-scene consistency, and artifact\nreduction. To benchmark performance, we introduce the Cross-scene Video\nBenchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates\ncomplex multi-scene transitions involving shared subjects and varied\nactions/backgrounds. Experimental results show that InfLVG can extend video\nlength by up to 9$\\times$, achieving strong consistency and semantic fidelity\nacross scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.", "AI": {"tldr": "InfLVG\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e0a\u4e0b\u6587\u5b9e\u73b0\u957f\u89c6\u9891\u751f\u6210\uff0c\u65e0\u9700\u989d\u5916\u957f\u89c6\u9891\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u53ef\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u7b56\u7565\uff08GRPO\u4f18\u5316\uff09\u52a8\u6001\u4fdd\u7559\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u3002", "result": "\u89c6\u9891\u957f\u5ea6\u53ef\u6269\u5c55\u81f39\u500d\uff0c\u4fdd\u6301\u8de8\u573a\u666f\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "InfLVG\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 50.0}}
{"id": "2505.17488", "pdf": "https://arxiv.org/pdf/2505.17488", "abs": "https://arxiv.org/abs/2505.17488", "authors": ["Haoran Li", "Muhao Guo", "Yang Weng", "Marija Ilic", "Guangchun Ruan"], "title": "ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "5 pages, 3 figures, conference", "summary": "Non-stationary power system dynamics, influenced by renewable energy\nvariability, evolving demand patterns, and climate change, are becoming\nincreasingly complex. Accurately capturing these dynamics requires a model\ncapable of adapting to environmental factors. Traditional models, including\nRecurrent Neural Networks (RNNs), lack efficient mechanisms to encode external\nfactors, such as time or environmental data, for dynamic adaptation. To address\nthis, we propose the External Adaptive RNN (ExARNN), a novel framework that\nintegrates external data (e.g., weather, time) to continuously adjust the\nparameters of a base RNN. ExARNN achieves this through a hierarchical\nhypernetwork design, using Neural Controlled Differential Equations (NCDE) to\nprocess external data and generate RNN parameters adaptively. This approach\nenables ExARNN to handle inconsistent timestamps between power and external\nmeasurements, ensuring continuous adaptation. Extensive forecasting tests\ndemonstrate ExARNN's superiority over established baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExARNN\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8\u6570\u636e\uff08\u5982\u5929\u6c14\u3001\u65f6\u95f4\uff09\u52a8\u6001\u8c03\u6574RNN\u53c2\u6570\uff0c\u4ee5\u5e94\u5bf9\u975e\u5e73\u7a33\u7535\u529b\u7cfb\u7edf\u52a8\u6001\u7684\u590d\u6742\u6027\u3002", "motivation": "\u4f20\u7edfRNN\u7f3a\u4e4f\u9ad8\u6548\u7f16\u7801\u5916\u90e8\u56e0\u7d20\uff08\u5982\u65f6\u95f4\u6216\u73af\u5883\u6570\u636e\uff09\u7684\u673a\u5236\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002ExARNN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8d85\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5229\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08NCDE\uff09\u5904\u7406\u5916\u90e8\u6570\u636e\u5e76\u81ea\u9002\u5e94\u751f\u6210RNN\u53c2\u6570\u3002", "result": "ExARNN\u5728\u9884\u6d4b\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ExARNN\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5916\u90e8\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u5e73\u7a33\u7535\u529b\u7cfb\u7edf\u52a8\u6001\u7684\u5efa\u6a21\u80fd\u529b\u3002", "relevance": 20.0}}
{"id": "2505.17121", "pdf": "https://arxiv.org/pdf/2505.17121", "abs": "https://arxiv.org/abs/2505.17121", "authors": ["Weiming Wu", "Zi-kang Wang", "Jin Ye", "Zhi Zhou", "Yu-Feng Li", "Lan-Zhe Guo"], "title": "NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Obtaining large-scale, high-quality data with reasoning paths is crucial for\nimproving the geometric reasoning capabilities of multi-modal large language\nmodels (MLLMs). However, existing data generation methods, whether based on\npredefined templates or constrained symbolic provers, inevitably face diversity\nand numerical generalization limitations. To address these limitations, we\npropose NeSyGeo, a novel neuro-symbolic framework for generating geometric\nreasoning data. First, we propose a domain-specific language grounded in the\nentity-relation-constraint paradigm to comprehensively represent all components\nof plane geometry, along with generative actions defined within this symbolic\nspace. We then design a symbolic-visual-text pipeline that synthesizes symbolic\nsequences, maps them to corresponding visual and textual representations, and\ngenerates diverse question-answer (Q&A) pairs using large language models\n(LLMs). To the best of our knowledge, we are the first to propose a\nneuro-symbolic approach in generating multimodal reasoning data. Based on this\nframework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing\n100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric\nreasoning abilities in MLLMs. Experiments demonstrate that the proposal\nsignificantly and consistently improves the performance of multiple MLLMs under\nboth reinforcement and supervised fine-tuning. With only 4k samples and two\nepochs of reinforcement fine-tuning, base models achieve improvements of up to\n+15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B\nmodel can be improved to outperform an 8B model from the same series on\ngeometric reasoning tasks.", "AI": {"tldr": "NeSyGeo\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u51e0\u4f55\u63a8\u7406\u6570\u636e\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u7b26\u53f7-\u89c6\u89c9-\u6587\u672c\u6d41\u6c34\u7ebf\u751f\u6210\u591a\u6837\u5316\u95ee\u7b54\u5bf9\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2aMLLMs\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u51e0\u4f55\u63a8\u7406\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u6570\u503c\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0cNeSyGeo\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5b9e\u4f53-\u5173\u7cfb-\u7ea6\u675f\u8303\u5f0f\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u8bbe\u8ba1\u7b26\u53f7-\u89c6\u89c9-\u6587\u672c\u6d41\u6c34\u7ebf\u751f\u6210\u6570\u636e\uff0c\u5e76\u6784\u5efaNeSyGeo-CoT\u548cNeSyGeo-Caption\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNeSyGeo\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u6027\u80fd\uff0c\u5c0f\u6837\u672c\u548c\u5c11\u91cf\u8bad\u7ec3\u8f6e\u6b21\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "NeSyGeo\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "relevance": 70.0}}
{"id": "2505.17581", "pdf": "https://arxiv.org/pdf/2505.17581", "abs": "https://arxiv.org/abs/2505.17581", "authors": ["Hainuo Wang", "Qiming Hu", "Xiaojie Guo"], "title": "MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery", "categories": ["cs.CV"], "comment": null, "summary": "Restoring images degraded by adverse weather remains a significant challenge\ndue to the highly non-uniform and spatially heterogeneous nature of\nweather-induced artifacts, e.g., fine-grained rain streaks versus widespread\nhaze. Accurately estimating the underlying degradation can intuitively provide\nrestoration models with more targeted and effective guidance, enabling adaptive\nprocessing strategies. To this end, we propose a Morton-Order Degradation\nEstimation Mechanism (MODEM) for adverse weather image restoration. Central to\nMODEM is the Morton-Order 2D-Selective-Scan Module (MOS2D), which integrates\nMorton-coded spatial ordering with selective state-space models to capture\nlong-range dependencies while preserving local structural coherence.\nComplementing MOS2D, we introduce a Dual Degradation Estimation Module (DDEM)\nthat disentangles and estimates both global and local degradation priors. These\npriors dynamically condition the MOS2D modules, facilitating adaptive and\ncontext-aware restoration. Extensive experiments and ablation studies\ndemonstrate that MODEM achieves state-of-the-art results across multiple\nbenchmarks and weather types, highlighting its effectiveness in modeling\ncomplex degradation dynamics. Our code will be released at\nhttps://github.com/hainuo-wang/MODEM.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMODEM\u7684\u673a\u5236\uff0c\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u56fe\u50cf\u6062\u590d\uff0c\u901a\u8fc7Morton-Order 2D-Selective-Scan\u6a21\u5757\u548c\u53cc\u9000\u5316\u4f30\u8ba1\u6a21\u5757\u5b9e\u73b0\u81ea\u9002\u5e94\u6062\u590d\u3002", "motivation": "\u6076\u52a3\u5929\u6c14\u5bfc\u81f4\u7684\u56fe\u50cf\u9000\u5316\u5177\u6709\u9ad8\u5ea6\u975e\u5747\u5300\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u9000\u5316\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u5904\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMODEM\u673a\u5236\uff0c\u5305\u62ecMorton-Order 2D-Selective-Scan\u6a21\u5757\uff08MOS2D\uff09\u548c\u53cc\u9000\u5316\u4f30\u8ba1\u6a21\u5757\uff08DDEM\uff09\uff0c\u7528\u4e8e\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u5e76\u52a8\u6001\u4f30\u8ba1\u5168\u5c40\u548c\u5c40\u90e8\u9000\u5316\u5148\u9a8c\u3002", "result": "MODEM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5929\u6c14\u7c7b\u578b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6062\u590d\u6548\u679c\u3002", "conclusion": "MODEM\u901a\u8fc7\u81ea\u9002\u5e94\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6062\u590d\u7b56\u7565\uff0c\u6709\u6548\u5efa\u6a21\u4e86\u590d\u6742\u7684\u9000\u5316\u52a8\u6001\u3002", "relevance": 30.0}}
{"id": "2505.17495", "pdf": "https://arxiv.org/pdf/2505.17495", "abs": "https://arxiv.org/abs/2505.17495", "authors": ["Landon Butler", "Abhineet Agarwal", "Justin Singh Kang", "Yigit Efe Erginbas", "Bin Yu", "Kannan Ramchandran"], "title": "ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance by\ncapturing complex interactions between input features. To identify these\ninteractions, most existing approaches require enumerating all possible\ncombinations of features up to a given order, causing them to scale poorly with\nthe number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an\ninformation-theoretic approach that uses interaction sparsity to scale to $n\n\\approx 10^3$ features. SPEX greatly improves upon prior methods but requires\ntens of thousands of model inferences, which can be prohibitive for large\nmodels. In this paper, we observe that LLM feature interactions are often\nhierarchical -- higher-order interactions are accompanied by their lower-order\nsubsets -- which enables more efficient discovery. To exploit this hierarchy,\nwe propose ProxySPEX, an interaction attribution algorithm that first fits\ngradient boosted trees to masked LLM outputs and then extracts the important\ninteractions. Experiments across four challenging high-dimensional datasets\nshow that ProxySPEX more faithfully reconstructs LLM outputs by 20% over\nmarginal attribution approaches while using $10\\times$ fewer inferences than\nSPEX. By accounting for interactions, ProxySPEX identifies features that\ninfluence model output over 20% more than those selected by marginal\napproaches. Further, we apply ProxySPEX to two interpretability tasks. Data\nattribution, where we identify interactions among CIFAR-10 training samples\nthat influence test predictions, and mechanistic interpretability, where we\nuncover interactions between attention heads, both within and across layers, on\na question-answering task. ProxySPEX identifies interactions that enable more\naggressive pruning of heads than marginal approaches.", "AI": {"tldr": "ProxySPEX\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7279\u5f81\u4ea4\u4e92\u53d1\u73b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u7279\u5f81\u4ea4\u4e92\u7684\u5c42\u6b21\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u4ea4\u4e92\u53d1\u73b0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982SPEX\uff09\u5728\u53d1\u73b0LLM\u7279\u5f81\u4ea4\u4e92\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6a21\u578b\u3002ProxySPEX\u65e8\u5728\u901a\u8fc7\u5229\u7528\u4ea4\u4e92\u7684\u5c42\u6b21\u6027\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ProxySPEX\u9996\u5148\u7528\u68af\u5ea6\u63d0\u5347\u6811\u62df\u5408\u63a9\u7801LLM\u8f93\u51fa\uff0c\u7136\u540e\u63d0\u53d6\u91cd\u8981\u4ea4\u4e92\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6bd4\u8fb9\u9645\u65b9\u6cd5\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u3002", "result": "ProxySPEX\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u8fb9\u9645\u65b9\u6cd5\u91cd\u5efaLLM\u8f93\u51fa\u7684\u51c6\u786e\u6027\u63d0\u9ad820%\uff0c\u540c\u65f6\u51cf\u5c1110\u500d\u63a8\u7406\u6b21\u6570\u3002\u5728\u53ef\u89e3\u91ca\u6027\u4efb\u52a1\u4e2d\uff0c\u5b83\u8fd8\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u91cd\u8981\u4ea4\u4e92\u3002", "conclusion": "ProxySPEX\u901a\u8fc7\u5c42\u6b21\u6027\u4ea4\u4e92\u53d1\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7279\u5f81\u4ea4\u4e92\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u6027\u4efb\u52a1\u3002", "relevance": 85.0}}
{"id": "2505.17122", "pdf": "https://arxiv.org/pdf/2505.17122", "abs": "https://arxiv.org/abs/2505.17122", "authors": ["Xuan Qi", "Jiahao Qiu", "Xinzhe Juan", "Yue Wu", "Mengdi Wang"], "title": "Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?", "categories": ["cs.CL"], "comment": "17 pages, 7 figures", "summary": "Aligning large language models (LLMs) with human preferences remains a key\nchallenge in AI. Preference-based optimization methods, such as Reinforcement\nLearning with Human Feedback (RLHF) and Direct Preference Optimization (DPO),\nrely on human-annotated datasets to improve alignment. In this work, we\nidentify a crucial property of the existing learning method: the distinguishing\nsignal obtained in preferred responses is often concentrated in the early\ntokens. We refer to this as shallow preference signals.\n  To explore this property, we systematically truncate preference datasets at\nvarious points and train both reward models and DPO models on the truncated\ndata. Surprisingly, models trained on truncated datasets, retaining only the\nfirst half or fewer tokens, achieve comparable or even superior performance to\nthose trained on full datasets. For example, a reward model trained on the\nSkywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when\ntrained on a 40\\% truncated dataset. This pattern is consistent across multiple\ndatasets, suggesting the widespread presence of shallow preference signals.\n  We further investigate the distribution of the reward signal through decoding\nstrategies. We consider two simple decoding strategies motivated by the shallow\nreward signal observation, namely Length Control Decoding and KL Threshold\nControl Decoding, which leverage shallow preference signals to optimize the\ntrade-off between alignment and computational efficiency. The performance is\neven better, which again validates our hypothesis.\n  The phenomenon of shallow preference signals highlights potential issues in\nLLM alignment: existing alignment methods often focus on aligning only the\ninitial tokens of responses, rather than considering the full response. This\ncould lead to discrepancies with real-world human preferences, resulting in\nsuboptimal alignment performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5bf9\u9f50\u65b9\u6cd5\u4e2d\uff0c\u504f\u597d\u4fe1\u53f7\u4e3b\u8981\u96c6\u4e2d\u5728\u65e9\u671f\u6807\u8bb0\u4e2d\uff08\u79f0\u4e3a\u6d45\u5c42\u504f\u597d\u4fe1\u53f7\uff09\u3002\u901a\u8fc7\u622a\u65ad\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u6a21\u578b\u6027\u80fd\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u504f\u597d\u4fe1\u53f7\u7684\u5206\u5e03\u7279\u6027\uff0c\u4ee5\u4f18\u5316\u5bf9\u9f50\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u622a\u65ad\u504f\u597d\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u548cDPO\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u89e3\u7801\u7b56\u7565\u9a8c\u8bc1\u6d45\u5c42\u504f\u597d\u4fe1\u53f7\u7684\u5f71\u54cd\u3002", "result": "\u622a\u65ad\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u5b8c\u6574\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u6d45\u5c42\u504f\u597d\u4fe1\u53f7\u7684\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u6d45\u5c42\u504f\u597d\u4fe1\u53f7\u63ed\u793a\u4e86\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u66f4\u5168\u9762\u5730\u53cd\u6620\u4eba\u7c7b\u504f\u597d\u3002", "relevance": 90.0}}
{"id": "2505.17590", "pdf": "https://arxiv.org/pdf/2505.17590", "abs": "https://arxiv.org/abs/2505.17590", "authors": ["Florian Barthel", "Wieland Morgenstern", "Paul Hinzer", "Anna Hilsmann", "Peter Eisert"], "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis", "categories": ["cs.CV"], "comment": "Main paper 12 pages, supplementary materials 8 pages", "summary": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high\nquality synthesis of human heads. However, existing methods stabilize training\nand enhance rendering quality from steep viewpoints by conditioning the random\nlatent vector on the current camera position. This compromises 3D consistency,\nas we observe significant identity changes when re-synthesizing the 3D head\nwith each camera shift. Conversely, fixing the camera to a single viewpoint\nyields high-quality renderings for that perspective but results in poor\nperformance for novel views. Removing view-conditioning typically destabilizes\nGAN training, often causing the training to collapse. In response to these\nchallenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework\nthat enables stable training and high-quality 3D-consistent synthesis of human\nheads without relying on view-conditioning. To ensure training stability, we\nintroduce a multi-view regularization technique that enhances generator\nconvergence with minimal computational overhead. Additionally, we adapt the\nconditional loss used in existing 3D Gaussian splatting GANs and propose a\ngenerator architecture designed to not only stabilize training but also\nfacilitate efficient rendering and straightforward scaling, enabling output\nresolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate\na new dataset derived from FFHQ. This dataset enables very high resolutions,\nfocuses on larger portions of the human head, reduces view-dependent artifacts\nfor improved 3D consistency, and excludes images where subjects are obscured by\nhands or other objects. As a result, our approach achieves very high rendering\nquality, supported by competitive FID scores, while ensuring consistent 3D\nscene generation. Check our our project page here:\nhttps://fraunhoferhhi.github.io/cgs-gan/", "AI": {"tldr": "CGS-GAN\u662f\u4e00\u79cd\u65b0\u578b3D\u9ad8\u65af\u6cfc\u6e85GAN\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u89c6\u89d2\u6761\u4ef6\u5373\u53ef\u7a33\u5b9a\u8bad\u7ec3\u5e76\u751f\u6210\u9ad8\u8d28\u91cf3D\u4e00\u81f4\u7684\u4eba\u5934\u5408\u6210\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c06\u968f\u673a\u6f5c\u5728\u5411\u91cf\u4e0e\u5f53\u524d\u76f8\u673a\u4f4d\u7f6e\u7ed1\u5b9a\u6765\u7a33\u5b9a\u8bad\u7ec3\u548c\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\uff0c\u4f46\u8fd9\u727a\u7272\u4e863D\u4e00\u81f4\u6027\u3002CGS-GAN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u591a\u89c6\u89d2\u6b63\u5219\u5316\u6280\u672f\u4ee5\u7a33\u5b9a\u8bad\u7ec3\uff0c\u6539\u8fdb\u6761\u4ef6\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u751f\u6210\u5668\u67b6\u6784\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u548c\u6269\u5c55\u3002", "result": "CGS-GAN\u5728FFHQ\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\uff082048^2\uff09\u548c3D\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\uff0cFID\u5206\u6570\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CGS-GAN\u57283D\u4e00\u81f4\u6027\u3001\u6e32\u67d3\u8d28\u91cf\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.17508", "pdf": "https://arxiv.org/pdf/2505.17508", "abs": "https://arxiv.org/abs/2505.17508", "authors": ["Yifan Zhang", "Yifeng Liu", "Huizhuo Yuan", "Yang Yuan", "Quanquan Gu", "Andrew C Yao"], "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "53 pages, 17 figures", "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u7b56\u7565\u68af\u5ea6\uff08RPG\uff09\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86KL\u6b63\u5219\u5316\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u6539\u8fdb\u4e86LLM\u63a8\u7406\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1KL\u6b63\u5219\u5316\u5728\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u4e0d\u540c\u5f62\u5f0f\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7cfb\u7edf\u63a2\u7d22\u4ecd\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86RPG\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u57fa\u4e8e\u6b63\u5411\u548c\u53cd\u5411KL\u6563\u5ea6\u7684\u7b56\u7565\u68af\u5ea6\u53ca\u66ff\u4ee3\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u5f52\u4e00\u5316\u548c\u975e\u5f52\u4e00\u5316\u7b56\u7565\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRPG\u5728LLM\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0eGRPO\u3001REINFORCE++\u548cDAPO\u7b49\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "RPG\u4e3aKL\u6b63\u5219\u5316\u7b56\u7565\u68af\u5ea6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.17123", "pdf": "https://arxiv.org/pdf/2505.17123", "abs": "https://arxiv.org/abs/2505.17123", "authors": ["Xiaoyuan Li", "Keqin Bao", "Yubo Ma", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "title": "MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Recent advances in Large Language Models (LLMs) have shown promising results\nin complex reasoning tasks. However, current evaluations predominantly focus on\nsingle-turn reasoning scenarios, leaving interactive tasks largely unexplored.\nWe attribute it to the absence of comprehensive datasets and scalable automatic\nevaluation protocols. To fill these gaps, we present MTR-Bench for LLMs'\nMulti-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600\ninstances, MTR-Bench covers diverse reasoning capabilities, fine-grained\ndifficulty granularity, and necessitates multi-turn interactions with the\nenvironments. Moreover, MTR-Bench features fully-automated framework spanning\nboth dataset constructions and model evaluations, which enables scalable\nassessment without human interventions. Extensive experiments reveal that even\nthe cutting-edge reasoning models fall short of multi-turn, interactive\nreasoning tasks. And the further analysis upon these results brings valuable\ninsights for future research in interactive AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MTR-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u591a\u8f6e\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u5f53\u524d\u8bc4\u4f30\u4e2d\u4ea4\u4e92\u5f0f\u4efb\u52a1\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u63a8\u7406\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u4ea4\u4e92\u5f0f\u4efb\u52a1\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u539f\u56e0\u662f\u7f3a\u5c11\u76f8\u5173\u6570\u636e\u96c6\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u6784\u5efa\u4e86MTR-Bench\uff0c\u5305\u542b4\u7c7b\u300140\u4e2a\u4efb\u52a1\u548c3600\u4e2a\u5b9e\u4f8b\uff0c\u8986\u76d6\u591a\u6837\u5316\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5168\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5206\u6790\u7ed3\u679c\u4e3a\u672a\u6765\u4ea4\u4e92\u5f0fAI\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "MTR-Bench\u4e3a\u591a\u8f6e\u63a8\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.17614", "pdf": "https://arxiv.org/pdf/2505.17614", "abs": "https://arxiv.org/abs/2505.17614", "authors": ["Sinchee Chin", "Yinuo Ma", "Xiaochen Yang", "Jing-Hao Xue", "Wenming Yang"], "title": "PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised pathology detection trains models on non-pathological data to\nflag deviations as pathologies, offering strong generalizability for\nidentifying novel diseases and avoiding costly annotations. However, building\nreliable normality models requires vast healthy datasets, as hospitals' data is\ninherently biased toward symptomatic populations, while privacy regulations\nhinder the assembly of representative healthy cohorts. To address this\nlimitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection\nframework that requires only a small set of non-pathological samples (minimum 2\nshots), significantly improving data efficiency. We introduce Global-Local\nContrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the\nvariability of non-pathological embeddings and a Global Contrastive Loss to\nenhance the discrimination of pathological regions. We also propose a\nPathology-informed Embedding Generation (PiEG) module that synthesizes\npathological embeddings guided by the global loss, better exploiting the\nlimited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8\ndatasets, PathoSCOPE achieves state-of-the-art performance among unsupervised\nmethods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS).", "AI": {"tldr": "PathoSCOPE\u662f\u4e00\u4e2a\u5c11\u6837\u672c\u65e0\u76d1\u7763\u75c5\u7406\u68c0\u6d4b\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u975e\u75c5\u7406\u6837\u672c\u5373\u53ef\u9ad8\u6548\u68c0\u6d4b\u75c5\u7406\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u5bf9\u6bd4\u635f\u5931\u548c\u75c5\u7406\u611f\u77e5\u5d4c\u5165\u751f\u6210\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u7763\u75c5\u7406\u68c0\u6d4b\u4e2d\u9700\u8981\u5927\u91cf\u5065\u5eb7\u6570\u636e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u533b\u9662\u6570\u636e\u504f\u5411\u75c7\u72b6\u4eba\u7fa4\u548c\u9690\u79c1\u6cd5\u89c4\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPathoSCOPE\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40-\u5c40\u90e8\u5bf9\u6bd4\u635f\u5931\uff08GLCL\uff09\u548c\u75c5\u7406\u611f\u77e5\u5d4c\u5165\u751f\u6210\uff08PiEG\uff09\u6a21\u5757\uff0c\u5229\u7528\u5c11\u91cf\u975e\u75c5\u7406\u6837\u672c\u3002", "result": "\u5728BraTS2020\u548cChestXray8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff082.48 GFLOPs\uff0c166 FPS\uff09\u3002", "conclusion": "PathoSCOPE\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u4e3a\u65e0\u76d1\u7763\u75c5\u7406\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17513", "pdf": "https://arxiv.org/pdf/2505.17513", "abs": "https://arxiv.org/abs/2505.17513", "authors": ["Binh Nguyen", "Shuji Shi", "Ryan Ofman", "Thai Le"], "title": "What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "53-04"], "comment": "15 pages, 2 fogures", "summary": "Recent advances in text-to-speech technologies have enabled realistic voice\ngeneration, fueling audio-based deepfake attacks such as fraud and\nimpersonation. While audio anti-spoofing systems are critical for detecting\nsuch threats, prior work has predominantly focused on acoustic-level\nperturbations, leaving the impact of linguistic variation largely unexplored.\nIn this paper, we investigate the linguistic sensitivity of both open-source\nand commercial anti-spoofing detectors by introducing transcript-level\nadversarial attacks. Our extensive evaluation reveals that even minor\nlinguistic perturbations can significantly degrade detection accuracy: attack\nsuccess rates surpass 60% on several open-source detector-voice pairs, and\nnotably one commercial detection accuracy drops from 100% on synthetic audio to\njust 32%. Through a comprehensive feature attribution analysis, we identify\nthat both linguistic complexity and model-level audio embedding similarity\ncontribute strongly to detector vulnerability. We further demonstrate the\nreal-world risk via a case study replicating the Brad Pitt audio deepfake scam,\nusing transcript adversarial attacks to completely bypass commercial detectors.\nThese results highlight the need to move beyond purely acoustic defenses and\naccount for linguistic variation in the design of robust anti-spoofing systems.\nAll source code will be publicly available.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6587\u672c\u7ea7\u5bf9\u6297\u653b\u51fb\u5bf9\u97f3\u9891\u53cd\u6b3a\u9a97\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u8bed\u8a00\u6270\u52a8\u4e5f\u80fd\u663e\u8457\u964d\u4f4e\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u8bed\u8a00\u591a\u6837\u6027\u65b9\u9762\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\u65e5\u76ca\u4e25\u91cd\uff0c\u4f46\u73b0\u6709\u53cd\u6b3a\u9a97\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u58f0\u5b66\u5c42\u9762\u7684\u6270\u52a8\uff0c\u5ffd\u7565\u4e86\u8bed\u8a00\u53d8\u5f02\u7684\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6587\u672c\u7ea7\u5bf9\u6297\u653b\u51fb\uff0c\u8bc4\u4f30\u5f00\u6e90\u548c\u5546\u4e1a\u53cd\u6b3a\u9a97\u68c0\u6d4b\u5668\u7684\u8bed\u8a00\u654f\u611f\u6027\uff0c\u5e76\u8fdb\u884c\u7279\u5f81\u5f52\u56e0\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8bed\u8a00\u6270\u52a8\u5bfc\u81f4\u68c0\u6d4b\u51c6\u786e\u7387\u5927\u5e45\u4e0b\u964d\uff0c\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc760%\uff0c\u67d0\u4e9b\u5546\u4e1a\u68c0\u6d4b\u5668\u7684\u51c6\u786e\u7387\u4ece100%\u964d\u81f332%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u9c81\u68d2\u53cd\u6b3a\u9a97\u7cfb\u7edf\u65f6\u9700\u8003\u8651\u8bed\u8a00\u53d8\u5f02\uff0c\u800c\u4e0d\u4ec5\u662f\u58f0\u5b66\u9632\u5fa1\u3002", "relevance": 50.0}}
{"id": "2505.17126", "pdf": "https://arxiv.org/pdf/2505.17126", "abs": "https://arxiv.org/abs/2505.17126", "authors": ["Maxon Rubin-Toles", "Maya Gambhir", "Keshav Ramji", "Aaron Roth", "Surbhi Goel"], "title": "Conformal Language Model Reasoning with Coherent Factuality", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Language models are increasingly being used in important decision pipelines,\nso ensuring the correctness of their outputs is crucial. Recent work has\nproposed evaluating the \"factuality\" of claims decomposed from a language model\ngeneration and applying conformal prediction techniques to filter out those\nclaims that are not factual. This can be effective for tasks such as\ninformation retrieval, where constituent claims may be evaluated in isolation\nfor factuality, but is not appropriate for reasoning tasks, as steps of a\nlogical argument can be evaluated for correctness only within the context of\nthe claims that precede them. To capture this, we define \"coherent factuality\"\nand develop a conformal-prediction-based method to guarantee coherent\nfactuality for language model outputs. Our approach applies split conformal\nprediction to subgraphs within a \"deducibility\" graph\" that represents the\nsteps of a reasoning problem. We evaluate our method on mathematical reasoning\nproblems from the MATH and FELM datasets and find that our algorithm\nconsistently produces correct and substantiated orderings of claims, achieving\ncoherent factuality across target coverage levels. Moreover, we achieve 90%\nfactuality on our stricter definition while retaining 80% or more of the\noriginal claims, highlighting the utility of our deducibility-graph-guided\napproach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u8fde\u8d2f\u4e8b\u5b9e\u6027\uff0c\u9002\u7528\u4e8e\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u8981\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u786e\u4fdd\u5176\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u5c24\u5176\u662f\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8fde\u8d2f\u4e8b\u5b9e\u6027\u3002", "method": "\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u6280\u672f\uff0c\u7ed3\u5408\u2018\u53ef\u63a8\u5bfc\u6027\u56fe\u2019\u8868\u793a\u63a8\u7406\u6b65\u9aa4\uff0c\u786e\u4fdd\u8fde\u8d2f\u4e8b\u5b9e\u6027\u3002", "result": "\u5728MATH\u548cFELM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u4fdd\u630190%\u7684\u4e8b\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u755980%\u4ee5\u4e0a\u7684\u539f\u59cb\u58f0\u660e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8fde\u8d2f\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "relevance": 85.0}}
{"id": "2505.17618", "pdf": "https://arxiv.org/pdf/2505.17618", "abs": "https://arxiv.org/abs/2505.17618", "authors": ["Haoran He", "Jiajun Liang", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Ling Pan"], "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "37 pages. Project: https://tinnerhrhe.github.io/evosearch", "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\n\\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EvoSearch\uff0c\u4e00\u79cd\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u4f18\u5316\u6269\u6563\u548c\u6d41\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6a21\u578b\u6269\u5c55\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u6210\u4e3a\u63d0\u5347\u751f\u6210\u6a21\u578b\u6027\u80fd\u7684\u65b0\u65b9\u5411\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "EvoSearch\u5c06TTS\u95ee\u9898\u8f6c\u5316\u4e3a\u8fdb\u5316\u641c\u7d22\uff0c\u5229\u7528\u751f\u7269\u8fdb\u5316\u539f\u7406\u4f18\u5316\u53bb\u566a\u8f68\u8ff9\uff0c\u8bbe\u8ba1\u4e86\u9009\u62e9\u548c\u7a81\u53d8\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEvoSearch\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u8d28\u91cf\u66f4\u9ad8\u4e14\u591a\u6837\u6027\u66f4\u597d\u3002", "conclusion": "EvoSearch\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684TTS\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6269\u6563\u548c\u6d41\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 70.0}}
{"id": "2505.17517", "pdf": "https://arxiv.org/pdf/2505.17517", "abs": "https://arxiv.org/abs/2505.17517", "authors": ["Rafa\u0142 Karczewski", "Markus Heinonen", "Alison Pouplin", "S\u00f8ren Hauberg", "Vikas Garg"], "title": "Spacetime Geometry of Denoising in Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "We present a novel perspective on diffusion models using the framework of\ninformation geometry. We show that the set of noisy samples, taken across all\nnoise levels simultaneously, forms a statistical manifold -- a family of\ndenoising probability distributions. Interpreting the noise level as a temporal\nparameter, we refer to this manifold as spacetime. This manifold naturally\ncarries a Fisher-Rao metric, which defines geodesics -- shortest paths between\nnoisy points. Notably, this family of distributions is exponential, enabling\nefficient geodesic computation even in high-dimensional settings without\nretraining or fine-tuning. We demonstrate the practical value of this geometric\nviewpoint in transition path sampling, where spacetime geodesics define smooth\nsequences of Boltzmann distributions, enabling the generation of continuous\ntrajectories between low-energy metastable states. Code is available at:\nhttps://github.com/Aalto-QuML/diffusion-spacetime-geometry.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u7684\u6269\u6563\u6a21\u578b\u65b0\u89c6\u89d2\uff0c\u5c06\u566a\u58f0\u6837\u672c\u96c6\u89c6\u4e3a\u7edf\u8ba1\u6d41\u5f62\uff0c\u5e76\u5229\u7528Fisher-Rao\u5ea6\u91cf\u5b9a\u4e49\u6d4b\u5730\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u4e0b\u7684\u65b0\u89c6\u89d2\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u3002", "method": "\u5c06\u566a\u58f0\u6837\u672c\u96c6\u5efa\u6a21\u4e3a\u7edf\u8ba1\u6d41\u5f62\uff08\u79f0\u4e3a\u201c\u65f6\u7a7a\u201d\uff09\uff0c\u5229\u7528Fisher-Rao\u5ea6\u91cf\u5b9a\u4e49\u6d4b\u5730\u7ebf\uff0c\u5e76\u901a\u8fc7\u6307\u6570\u5206\u5e03\u65cf\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\u4e2d\u751f\u6210\u8fde\u7eed\u8f68\u8ff9\uff0c\u8fde\u63a5\u4f4e\u80fd\u4e9a\u7a33\u6001\u3002", "conclusion": "\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17131", "pdf": "https://arxiv.org/pdf/2505.17131", "abs": "https://arxiv.org/abs/2505.17131", "authors": ["Alireza Arbabi", "Florian Kerschbaum"], "title": "Relative Bias: A Comparative Framework for Quantifying Bias in LLMs", "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "The growing deployment of large language models (LLMs) has amplified concerns\nregarding their inherent biases, raising critical questions about their\nfairness, safety, and societal impact. However, quantifying LLM bias remains a\nfundamental challenge, complicated by the ambiguity of what \"bias\" entails.\nThis challenge grows as new models emerge rapidly and gain widespread use,\nwhile introducing potential biases that have not been systematically assessed.\nIn this paper, we propose the Relative Bias framework, a method designed to\nassess how an LLM's behavior deviates from other LLMs within a specified target\ndomain. We introduce two complementary methodologies: (1) Embedding\nTransformation analysis, which captures relative bias patterns through sentence\nrepresentations over the embedding space, and (2) LLM-as-a-Judge, which employs\na language model to evaluate outputs comparatively. Applying our framework to\nseveral case studies on bias and alignment scenarios following by statistical\ntests for validation, we find strong alignment between the two scoring methods,\noffering a systematic, scalable, and statistically grounded approach for\ncomparative bias analysis in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRelative Bias\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316LLM\u7684\u504f\u5dee\uff0c\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u5206\u6790\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u504f\u5dee\u8bc4\u4f30\u65b9\u5f0f\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6f5c\u5728\u7684\u504f\u5dee\u95ee\u9898\u5f15\u53d1\u4e86\u5bf9\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u548c\u793e\u4f1a\u5f71\u54cd\u7684\u5173\u6ce8\uff0c\u4f46\u91cf\u5316\u504f\u5dee\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86Relative Bias\u6846\u67b6\uff0c\u5305\u62ec\u5d4c\u5165\u7a7a\u95f4\u8f6c\u6362\u5206\u6790\uff08Embedding Transformation\uff09\u548cLLM-as-a-Judge\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83LLM\u5728\u4e0d\u540c\u9886\u57df\u7684\u504f\u5dee\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u53d1\u73b0\u4e24\u79cd\u8bc4\u5206\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u7684\u504f\u5dee\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u4e14\u7edf\u8ba1\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17619", "pdf": "https://arxiv.org/pdf/2505.17619", "abs": "https://arxiv.org/abs/2505.17619", "authors": ["Bo Wang", "De-Xing Huang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Nu-Fang Xiao", "Jian-Long Hao", "Ming-Yuan Liu", "Zeng-Guang Hou"], "title": "CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment", "categories": ["cs.CV"], "comment": "Under review", "summary": "Synthetic X-ray angiographies generated by modern generative models hold\ngreat potential to reduce the use of contrast agents in vascular interventional\nprocedures. However, low-quality synthetic angiographies can significantly\nincrease procedural risk, underscoring the need for reliable image quality\nassessment (IQA) methods. Existing IQA models, however, fail to leverage\nauxiliary images as references during evaluation and lack fine-grained,\ntask-specific metrics necessary for clinical relevance. To address these\nlimitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based\nframework that predicts fine-grained quality scores by effectively\nincorporating auxiliary information from related images. In the absence of\nangiography datasets, CAS-3K is constructed, comprising 3,565 synthetic\nangiographies along with score annotations. To ensure clinically meaningful\nassessment, three task-specific evaluation metrics are defined. Furthermore, a\nMulti-path featUre fuSion and rouTing (MUST) module is designed to enhance\nimage representations by adaptively fusing and routing visual tokens to\nmetric-specific branches. Extensive experiments on the CAS-3K dataset\ndemonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods\nby a considerable margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6846\u67b6CAS-IQA\uff0c\u7528\u4e8e\u8bc4\u4f30\u5408\u6210X\u5c04\u7ebf\u8840\u7ba1\u9020\u5f71\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u56fe\u50cf\u548c\u591a\u8def\u5f84\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08MUST\uff09\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u8f85\u52a9\u56fe\u50cf\uff0c\u4e14\u7f3a\u4e4f\u4e34\u5e8a\u76f8\u5173\u7684\u7ec6\u7c92\u5ea6\u6307\u6807\uff0c\u5bfc\u81f4\u4f4e\u8d28\u91cf\u5408\u6210\u8840\u7ba1\u9020\u5f71\u53ef\u80fd\u589e\u52a0\u624b\u672f\u98ce\u9669\u3002", "method": "\u63d0\u51faCAS-IQA\u6846\u67b6\uff0c\u7ed3\u5408\u8f85\u52a9\u56fe\u50cf\u4fe1\u606f\uff0c\u8bbe\u8ba1MUST\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u548c\u8def\u7531\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5b9a\u4e49\u4e09\u4e2a\u4efb\u52a1\u7279\u5b9a\u6307\u6807\u3002", "result": "\u5728CAS-3K\u6570\u636e\u96c6\u4e0a\uff0cCAS-IQA\u663e\u8457\u4f18\u4e8e\u73b0\u6709IQA\u65b9\u6cd5\u3002", "conclusion": "CAS-IQA\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\uff0c\u4e3a\u5408\u6210\u8840\u7ba1\u9020\u5f71\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e34\u5e8a\u8d28\u91cf\u8bc4\u4f30\u3002", "relevance": 30.0}}
{"id": "2505.17532", "pdf": "https://arxiv.org/pdf/2505.17532", "abs": "https://arxiv.org/abs/2505.17532", "authors": ["Bin Wang", "Heming Yang", "Jinfang Sheng"], "title": "TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Recent studies have shown that by introducing prior knowledge, multi-scale\nanalysis of complex and non-stationary time series in real environments can\nachieve good results in the field of long-term forecasting. However, affected\nby channel-independent methods, models based on multi-scale analysis may\nproduce suboptimal prediction results due to the autocorrelation between time\nseries labels, which in turn affects the generalization ability of the model.\nTo address this challenge, we are inspired by the idea of sharpness-aware\nminimization and the recently proposed FreDF method and design a deep learning\nmodel TimeCF for long-term time series forecasting based on the TimeMixer,\ncombined with our designed adaptive convolution information aggregation module\nand Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,\nTimeCF first decomposes the original time series into sequences of different\nscales. Next, the same-sized convolution modules are used to adaptively\naggregate information of different scales on sequences of different scales.\nThen, decomposing each sequence into season and trend parts and the two parts\nare mixed at different scales through bottom-up and top-down methods\nrespectively. Finally, different scales are aggregated through a Feed-Forward\nNetwork. What's more, extensive experimental results on different real-world\ndatasets show that our proposed TimeCF has excellent performance in the field\nof long-term forecasting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTimeCF\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u5377\u79ef\u4fe1\u606f\u805a\u5408\u6a21\u5757\u548cSAMFre\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u591a\u5c3a\u5ea6\u5206\u6790\u4e2d\u56e0\u901a\u9053\u72ec\u7acb\u65b9\u6cd5\u5bfc\u81f4\u7684\u9884\u6d4b\u7ed3\u679c\u4e0d\u4f73\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u5c3a\u5ea6\u5206\u6790\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u56e0\u901a\u9053\u72ec\u7acb\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u4f73\uff0c\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "TimeCF\u901a\u8fc7\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u4e3a\u4e0d\u540c\u5c3a\u5ea6\u5e8f\u5217\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5377\u79ef\u6a21\u5757\u805a\u5408\u4fe1\u606f\uff0c\u5206\u89e3\u5b63\u8282\u548c\u8d8b\u52bf\u90e8\u5206\u5e76\u901a\u8fc7\u4e0d\u540c\u65b9\u6cd5\u6df7\u5408\uff0c\u6700\u540e\u901a\u8fc7\u524d\u9988\u7f51\u7edc\u805a\u5408\u4e0d\u540c\u5c3a\u5ea6\u3002", "result": "\u5728\u4e0d\u540c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTimeCF\u5728\u957f\u671f\u9884\u6d4b\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TimeCF\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u5206\u6790\u548cSAMFre\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17134", "pdf": "https://arxiv.org/pdf/2505.17134", "abs": "https://arxiv.org/abs/2505.17134", "authors": ["Chaochen Gao", "Xing Wu", "Zijia Lin", "Debing Zhang", "Songlin Hu"], "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality long-context instruction data is essential for aligning\nlong-context large language models (LLMs). Despite the public release of models\nlike Qwen and Llama, their long-context instruction data remains proprietary.\nHuman annotation is costly and challenging, while template-based synthesis\nmethods limit scale, diversity, and quality. We introduce LongMagpie, a\nself-synthesis framework that automatically generates large-scale long-context\ninstruction data. Our key insight is that aligned long-context LLMs, when\npresented with a document followed by special tokens preceding a user turn,\nauto-regressively generate contextually relevant queries. By harvesting these\ndocument-query pairs and the model's responses, LongMagpie produces\nhigh-quality instructions without human effort. Experiments on HELMET, RULER,\nand Longbench v2 demonstrate that LongMagpie achieves leading performance on\nlong-context tasks while maintaining competitive performance on short-context\ntasks, establishing it as a simple and effective approach for open, diverse,\nand scalable long-context instruction data synthesis.", "AI": {"tldr": "LongMagpie\u662f\u4e00\u4e2a\u81ea\u5408\u6210\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u957f\u4e0a\u4e0b\u6587\u6307\u4ee4\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u901a\u8fc7\u5229\u7528\u5bf9\u9f50\u7684\u957f\u4e0a\u4e0b\u6587LLM\u751f\u6210\u76f8\u5173\u67e5\u8be2\u548c\u54cd\u5e94\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u957f\u4e0a\u4e0b\u6587\u6307\u4ee4\u6570\u636e\u5bf9\u5bf9\u9f50\u957f\u4e0a\u4e0b\u6587LLM\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u6216\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u5229\u7528\u5bf9\u9f50\u7684\u957f\u4e0a\u4e0b\u6587LLM\uff0c\u5728\u6587\u6863\u540e\u63d2\u5165\u7279\u6b8a\u6807\u8bb0\uff0c\u4f7f\u5176\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u67e5\u8be2\uff0c\u5e76\u6536\u96c6\u6587\u6863-\u67e5\u8be2\u5bf9\u53ca\u6a21\u578b\u54cd\u5e94\u3002", "result": "\u5728HELMET\u3001RULER\u548cLongbench v2\u4e0a\u8868\u73b0\u9886\u5148\uff0c\u540c\u65f6\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "LongMagpie\u662f\u4e00\u79cd\u7b80\u5355\u3001\u5f00\u653e\u3001\u591a\u6837\u4e14\u53ef\u6269\u5c55\u7684\u957f\u4e0a\u4e0b\u6587\u6307\u4ee4\u6570\u636e\u5408\u6210\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17645", "pdf": "https://arxiv.org/pdf/2505.17645", "abs": "https://arxiv.org/abs/2505.17645", "authors": ["Chuhao Zhou", "Jianfei Yang"], "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "18 pages, 13 figures, 6 tables", "summary": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.", "AI": {"tldr": "HoloLLM\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u6574\u5408\u4e86LiDAR\u3001\u7ea2\u5916\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\u548cWiFi\u7b49\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u901a\u7528\u6a21\u6001\u6ce8\u5165\u6295\u5f71\u5668\uff08UMIP\uff09\u89e3\u51b3\u6570\u636e\u5bf9\u9f50\u548c\u4fe1\u53f7\u8868\u793a\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u906e\u6321\u3001\u5149\u7ebf\u5dee\u6216\u9690\u79c1\u9650\u5236\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u63d0\u5347\u667a\u80fd\u5bb6\u5c45\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u901a\u7528\u6a21\u6001\u6ce8\u5165\u6295\u5f71\u5668\uff08UMIP\uff09\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u9884\u5bf9\u9f50\u6a21\u6001\u5d4c\u5165\uff0c\u5e76\u5f00\u53d1\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "\u5728\u4e24\u4e2a\u65b0\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHoloLLM\u6bd4\u73b0\u6709MLLM\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe30%\u3002", "conclusion": "HoloLLM\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6a21\u6001\u611f\u77e5\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 70.0}}
{"id": "2505.17533", "pdf": "https://arxiv.org/pdf/2505.17533", "abs": "https://arxiv.org/abs/2505.17533", "authors": ["Pavan Ravishankar", "Rushabh Shah", "Daniel B. Neill"], "title": "Learning Representational Disparities", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "27 pages", "summary": "We propose a fair machine learning algorithm to model interpretable\ndifferences between observed and desired human decision-making, with the latter\naimed at reducing disparity in a downstream outcome impacted by the human\ndecision. Prior work learns fair representations without considering the\noutcome in the decision-making process. We model the outcome disparities as\narising due to the different representations of the input seen by the observed\nand desired decision-maker, which we term representational disparities. Our\ngoal is to learn interpretable representational disparities which could\npotentially be corrected by specific nudges to the human decision, mitigating\ndisparities in the downstream outcome; we frame this as a multi-objective\noptimization problem using a neural network. Under reasonable simplifying\nassumptions, we prove that our neural network model of the representational\ndisparity learns interpretable weights that fully mitigate the outcome\ndisparity. We validate objectives and interpret results using real-world German\nCredit, Adult, and Heritage Health datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u53ef\u89e3\u91ca\u7684\u8868\u793a\u5dee\u5f02\u6765\u51cf\u5c11\u4e0b\u6e38\u7ed3\u679c\u7684\u4e0d\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u8003\u8651\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u679c\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u8868\u793a\u5dee\u5f02\u6765\u7ea0\u6b63\u4eba\u7c7b\u51b3\u7b56\u4e2d\u7684\u4e0d\u516c\u5e73\u6027\u3002", "method": "\u5c06\u8868\u793a\u5dee\u5f02\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u6743\u91cd\u3002", "result": "\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u6a21\u578b\u80fd\u5b8c\u5168\u6d88\u9664\u7ed3\u679c\u4e0d\u516c\u5e73\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u8868\u793a\u5dee\u5f02\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u4e0b\u6e38\u7ed3\u679c\u7684\u4e0d\u516c\u5e73\u6027\u3002", "relevance": 70.0}}
{"id": "2505.17135", "pdf": "https://arxiv.org/pdf/2505.17135", "abs": "https://arxiv.org/abs/2505.17135", "authors": ["Rashed Shelim", "Shengzhe Xu", "Walid Saad", "Naren Ramakrishnan"], "title": "When can isotropy help adapt LLMs' next word prediction to numerical domains?", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown that vector representations of contextual\nembeddings learned by pre-trained large language models (LLMs) are effective in\nvarious downstream tasks in numerical domains. Despite their significant\nbenefits, the tendency of LLMs to hallucinate in such domains can have severe\nconsequences in applications such as energy, nature, finance, healthcare,\nretail and transportation, among others. To guarantee prediction reliability\nand accuracy in numerical domains, it is necessary to open the black-box and\nprovide performance guarantees through explanation. However, there is little\ntheoretical understanding of when pre-trained language models help solve\nnumeric downstream tasks. This paper seeks to bridge this gap by understanding\nwhen the next-word prediction capability of LLMs can be adapted to numerical\ndomains through a novel analysis based on the concept of isotropy in the\ncontextual embedding space. Specifically, we consider a log-linear model for\nLLMs in which numeric data can be predicted from its context through a network\nwith softmax in the output layer of LLMs (i.e., language model head in\nself-attention). We demonstrate that, in order to achieve state-of-the-art\nperformance in numerical domains, the hidden representations of the LLM\nembeddings must possess a structure that accounts for the shift-invariance of\nthe softmax function. By formulating a gradient structure of self-attention in\npre-trained models, we show how the isotropic property of LLM embeddings in\ncontextual embedding space preserves the underlying structure of\nrepresentations, thereby resolving the shift-invariance problem and providing a\nperformance guarantee. Experiments show that different characteristics of\nnumeric data and model architecture could have different impacts on isotropy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u503c\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5d4c\u5165\u7a7a\u95f4\u5404\u5411\u540c\u6027\uff08isotropy\uff09\u7684\u65b0\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u6570\u503c\u9884\u6d4b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u6570\u503c\u9886\u57df\u7684\u5e94\u7528\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7406\u89e3LLMs\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u80fd\u529b\u5982\u4f55\u9002\u5e94\u6570\u503c\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5404\u5411\u540c\u6027\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u7ed3\u6784\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7814\u7a76LLM\u5d4c\u5165\u7684\u9690\u85cf\u8868\u793a\u5982\u4f55\u89e3\u51b3softmax\u51fd\u6570\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6570\u503c\u6570\u636e\u7684\u7279\u6027\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u5404\u5411\u540c\u6027\u6709\u4e0d\u540c\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86\u5404\u5411\u540c\u6027\u6027\u8d28\u5728\u89e3\u51b3\u6570\u503c\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790LLM\u5d4c\u5165\u7684\u5404\u5411\u540c\u6027\u6027\u8d28\uff0c\u672c\u6587\u4e3aLLMs\u5728\u6570\u503c\u9886\u57df\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "relevance": 85.0}}
{"id": "2505.17649", "pdf": "https://arxiv.org/pdf/2505.17649", "abs": "https://arxiv.org/abs/2505.17649", "authors": ["Junhang Li", "Yu Guo", "Chuhua Xian", "Shengfeng He"], "title": "Instruct2See: Learning to Remove Any Obstructions Across Distributions", "categories": ["cs.CV"], "comment": null, "summary": "Images are often obstructed by various obstacles due to capture limitations,\nhindering the observation of objects of interest. Most existing methods address\nocclusions from specific elements like fences or raindrops, but are constrained\nby the wide range of real-world obstructions, making comprehensive data\ncollection impractical. To overcome these challenges, we propose Instruct2See,\na novel zero-shot framework capable of handling both seen and unseen obstacles.\nThe core idea of our approach is to unify obstruction removal by treating it as\na soft-hard mask restoration problem, where any obstruction can be represented\nusing multi-modal prompts, such as visual semantics and textual instructions,\nprocessed through a cross-attention unit to enhance contextual understanding\nand improve mode control. Additionally, a tunable mask adapter allows for\ndynamic soft masking, enabling real-time adjustment of inaccurate masks.\nExtensive experiments on both in-distribution and out-of-distribution obstacles\nshow that Instruct2See consistently achieves strong performance and\ngeneralization in obstruction removal, regardless of whether the obstacles were\npresent during the training phase. Code and dataset are available at\nhttps://jhscut.github.io/Instruct2See.", "AI": {"tldr": "Instruct2See\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\uff08\u5982\u89c6\u89c9\u8bed\u4e49\u548c\u6587\u672c\u6307\u4ee4\uff09\u5904\u7406\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u73b0\u8f6f\u786c\u63a9\u7801\u6062\u590d\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u906e\u6321\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u906e\u6321\uff08\u5982\u6805\u680f\u6216\u96e8\u6ef4\uff09\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u6837\u5316\u7684\u906e\u6321\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u906e\u6321\u95ee\u9898\u7edf\u4e00\u4e3a\u8f6f\u786c\u63a9\u7801\u6062\u590d\u4efb\u52a1\uff0c\u5229\u7528\u591a\u6a21\u6001\u63d0\u793a\uff08\u89c6\u89c9\u548c\u6587\u672c\uff09\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5355\u5143\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5e76\u7ed3\u5408\u53ef\u8c03\u63a9\u7801\u9002\u914d\u5668\u52a8\u6001\u8c03\u6574\u63a9\u7801\u3002", "result": "\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u906e\u6321\u6570\u636e\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Instruct2See\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u906e\u6321\u79fb\u9664\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u906e\u6321\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17542", "pdf": "https://arxiv.org/pdf/2505.17542", "abs": "https://arxiv.org/abs/2505.17542", "authors": ["Bardh Prenkaj", "Efstratios Zaradoukas", "Gjergji Kasneci"], "title": "Graph Style Transfer for Counterfactual Explainability", "categories": ["cs.LG"], "comment": "Accepted to ICML'25", "summary": "Counterfactual explainability seeks to uncover model decisions by identifying\nminimal changes to the input that alter the predicted outcome. This task\nbecomes particularly challenging for graph data due to preserving structural\nintegrity and semantic meaning. Unlike prior approaches that rely on forward\nperturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the\nfirst framework to re-imagine graph counterfactual generation as a backtracking\nprocess, leveraging spectral style transfer. By aligning the global structure\nwith the original input spectrum and preserving local content faithfulness,\nGIST produces valid counterfactuals as interpolations between the input style\nand counterfactual content. Tested on 8 binary and multi-class graph\nclassification benchmarks, GIST achieves a remarkable +7.6% improvement in the\nvalidity of produced counterfactuals and significant gains (+45.5%) in\nfaithfully explaining the true class distribution. Additionally, GIST's\nbacktracking mechanism effectively mitigates overshooting the underlying\npredictor's decision boundary, minimizing the spectral differences between the\ninput and the counterfactuals. These results challenge traditional forward\nperturbation methods, offering a novel perspective that advances graph\nexplainability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Graph Inverse Style Transfer (GIST)\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u6eaf\u8fc7\u7a0b\u548c\u5149\u8c31\u98ce\u683c\u8f6c\u79fb\u751f\u6210\u56fe\u6570\u636e\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u4e8b\u5b9e\u7684\u6709\u6548\u6027\u548c\u89e3\u91ca\u771f\u5b9e\u6027\u3002", "motivation": "\u89e3\u51b3\u56fe\u6570\u636e\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e2d\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8bed\u4e49\u610f\u4e49\u7684\u6311\u6218\uff0c\u4f20\u7edf\u524d\u5411\u6270\u52a8\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u5f15\u5165GIST\u6846\u67b6\uff0c\u5229\u7528\u5149\u8c31\u98ce\u683c\u8f6c\u79fb\u5c06\u53cd\u4e8b\u5b9e\u751f\u6210\u91cd\u65b0\u6784\u60f3\u4e3a\u56de\u6eaf\u8fc7\u7a0b\uff0c\u540c\u65f6\u5bf9\u9f50\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u5185\u5bb9\u3002", "result": "\u57288\u4e2a\u56fe\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGIST\u5c06\u53cd\u4e8b\u5b9e\u6709\u6548\u6027\u63d0\u53477.6%\uff0c\u89e3\u91ca\u771f\u5b9e\u6027\u63d0\u534745.5%\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u5149\u8c31\u5dee\u5f02\u3002", "conclusion": "GIST\u4e3a\u56fe\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u524d\u5411\u6270\u52a8\u65b9\u6cd5\u3002", "relevance": 60.0}}
{"id": "2505.17136", "pdf": "https://arxiv.org/pdf/2505.17136", "abs": "https://arxiv.org/abs/2505.17136", "authors": ["Yuhan Ji", "Song Gao", "Ying Nie", "Ivan Maji\u0107", "Krzysztof Janowicz"], "title": "Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations", "categories": ["cs.CL", "cs.AI", "I.2"], "comment": "33 pages, 13 figures, IJGIS GeoFM Special Issue", "summary": "Applying AI foundation models directly to geospatial datasets remains\nchallenging due to their limited ability to represent and reason with\ngeographical entities, specifically vector-based geometries and natural\nlanguage descriptions of complex spatial relations. To address these issues, we\ninvestigate the extent to which a well-known-text (WKT) representation of\ngeometries and their spatial relations (e.g., topological predicates) are\npreserved during spatial reasoning when the geospatial vector data are passed\nto large language models (LLMs) including GPT-3.5-turbo, GPT-4, and\nDeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the\nspatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt\nengineering-based, and everyday language-based evaluation. Our experiment\nresults demonstrate that both the embedding-based and prompt engineering-based\napproaches to geospatial question-answering tasks with GPT models can achieve\nan accuracy of over 0.6 on average for the identification of topological\nspatial relations between two geometries. Among the evaluated models, GPT-4\nwith few-shot prompting achieved the highest performance with over 0.66\naccuracy on topological spatial relation inference. Additionally, GPT-based\nreasoner is capable of properly comprehending inverse topological spatial\nrelations and including an LLM-generated geometry can enhance the effectiveness\nfor geographic entity retrieval. GPT-4 also exhibits the ability to translate\ncertain vernacular descriptions about places into formal topological relations,\nand adding the geometry-type or place-type context in prompts may improve\ninference accuracy, but it varies by instance. The performance of these spatial\nreasoning tasks offers valuable insights for the refinement of LLMs with\ngeographical knowledge towards the development of geo-foundation models capable\nof geospatial reasoning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5c06AI\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u6311\u6218\uff0c\u901a\u8fc7\u51e0\u4f55\u5d4c\u5165\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u65e5\u5e38\u8bed\u8a00\u4e09\u79cd\u65b9\u6cd5\u8bc4\u4f30LLM\uff08\u5982GPT-3.5-turbo\u3001GPT-4\u548cDeepSeek-R1-14B\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002GPT-4\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8d85\u8fc70.66\u3002", "motivation": "\u89e3\u51b3AI\u57fa\u7840\u6a21\u578b\u5728\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e2d\u8868\u793a\u548c\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u5411\u91cf\u51e0\u4f55\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u5d4c\u5165\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u65e5\u5e38\u8bed\u8a00\u4e09\u79cd\u65b9\u6cd5\uff0c\u8bc4\u4f30LLM\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "GPT-4\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8d85\u8fc70.66\u3002LLM\u80fd\u591f\u7406\u89e3\u9006\u62d3\u6251\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u751f\u6210\u51e0\u4f55\u4f53\u4ee5\u589e\u5f3a\u5730\u7406\u5b9e\u4f53\u68c0\u7d22\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdbLLM\u7684\u5730\u7406\u77e5\u8bc6\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u5177\u5907\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5730\u7406\u57fa\u7840\u6a21\u578b\u3002", "relevance": 60.0}}
{"id": "2505.17665", "pdf": "https://arxiv.org/pdf/2505.17665", "abs": "https://arxiv.org/abs/2505.17665", "authors": ["Yichun Yu", "Yuqing Lan", "Zhihuan Xing", "Xiaoyi Yang", "Tingyue Tang", "Dan Yu"], "title": "EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy", "categories": ["cs.CV", "cs.AI"], "comment": "Proceedings of the 20th International Conference on Intelligent\n  Computing (ICIC 2024): Poster Volume I. Tianjin, China, 2024: 538-562", "summary": "High-resolution remote sensing (HRRS) image segmentation is challenging due\nto complex spatial layouts and diverse object appearances. While CNNs excel at\ncapturing local features, they struggle with long-range dependencies, whereas\nTransformers can model global context but often neglect local details and are\ncomputationally expensive.We propose a novel approach, Region-Aware Proxy\nNetwork (RAPNet), which consists of two components: Contextual Region Attention\n(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely\non grid-based layouts, RAPNet operates at the region level for more flexible\nsegmentation. The CRA module uses a Transformer to capture region-level\ncontextual dependencies, generating a Semantic Region Mask (SRM). The GCR\nmodule learns a global class attention map to refine multi-class information,\ncombining the SRM and attention map for accurate segmentation.Experiments on\nthree public datasets show that RAPNet outperforms state-of-the-art methods,\nachieving superior multi-class segmentation accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAPNet\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u57df\u611f\u77e5\u4ee3\u7406\u7f51\u7edc\u7ed3\u5408Transformer\u548c\u5168\u5c40\u7c7b\u522b\u7ec6\u5316\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u5206\u5272\u56e0\u590d\u6742\u7684\u7a7a\u95f4\u5e03\u5c40\u548c\u591a\u6837\u5316\u7684\u7269\u4f53\u5916\u89c2\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982CNN\u548cTransformer\uff09\u5404\u6709\u5c40\u9650\u6027\u3002", "method": "RAPNet\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aContextual Region Attention\uff08CRA\uff09\u548cGlobal Class Refinement\uff08GCR\uff09\u3002CRA\u4f7f\u7528Transformer\u6355\u6349\u533a\u57df\u7ea7\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff0c\u751f\u6210\u8bed\u4e49\u533a\u57df\u63a9\u7801\uff1bGCR\u5b66\u4e60\u5168\u5c40\u7c7b\u522b\u6ce8\u610f\u529b\u56fe\u4ee5\u7ec6\u5316\u591a\u7c7b\u522b\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAPNet\u5728\u591a\u7c7b\u522b\u5206\u5272\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAPNet\u901a\u8fc7\u533a\u57df\u7ea7\u64cd\u4f5c\u548c\u5168\u5c40\u7c7b\u522b\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u5206\u5272\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17066", "pdf": "https://arxiv.org/pdf/2505.17066", "abs": "https://arxiv.org/abs/2505.17066", "authors": ["Tatia Tsmindashvili", "Ana Kolkhidashvili", "Dachi Kurtskhalia", "Nino Maghlakelidze", "Elene Mekvabishvili", "Guram Dentoshvili", "Orkhan Shamilov", "Zaal Gachechiladze", "Steven Saporta", "David Dachi Choladze"], "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration", "categories": ["cs.CR", "cs.AI"], "comment": "Under review at IEEE Access. Supplementary material is included in\n  the main PDF. Benchmark dataset (jailbreak.csv, jailbreak_with_experts.csv)\n  should be included as ancillary data", "summary": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faArchias\uff0c\u4e00\u79cd\u4e13\u5bb6\u6a21\u578b\uff0c\u7528\u4e8e\u533a\u5206\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u901a\u4fe1\uff0c\u5e76\u5206\u7c7b\u7528\u6237\u67e5\u8be2\uff0c\u4ee5\u589e\u5f3aLLM\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u6c7d\u8f66\u884c\u4e1a\uff09\u7684\u5b89\u5168\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9762\u4e34\u7684\u5b89\u5168\u6311\u6218\uff0c\u5982\u8d8a\u72f1\u548c\u63d0\u793a\u6ce8\u5165\uff0c\u4ee5\u53ca\u9886\u57df\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e13\u5bb6\u6a21\u578b\uff08Archias\uff09\u7684\u8f93\u51fa\u5230LLM\u63d0\u793a\u4e2d\uff0c\u5206\u7c7b\u67e5\u8be2\u5e76\u751f\u6210\u54cd\u5e94\u3002", "result": "Archias\u63d0\u9ad8\u4e86LLM\u5bf9\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u4e14\u56e0\u5176\u5c0f\u5de7\u53ef\u5b9a\u5236\u6027\u5f3a\u3002", "conclusion": "Archias\u4e3a\u7279\u5b9a\u884c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u5b89\u5168\u7684LLM\u5e94\u7528\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u6c7d\u8f66\u884c\u4e1a\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "relevance": 85.0}}
{"id": "2505.17552", "pdf": "https://arxiv.org/pdf/2505.17552", "abs": "https://arxiv.org/abs/2505.17552", "authors": ["Zijie Qiu", "Jiaqi Wei", "Xiang Zhang", "Sheng Xu", "Kai Zou", "Zhi Jin", "Zhiqiang Gao", "Nanqing Dong", "Siqi Sun"], "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "De novo peptide sequencing is a critical task in proteomics. However, the\nperformance of current deep learning-based methods is limited by the inherent\ncomplexity of mass spectrometry data and the heterogeneous distribution of\nnoise signals, leading to data-specific biases. We present RankNovo, the first\ndeep reranking framework that enhances de novo peptide sequencing by leveraging\nthe complementary strengths of multiple sequencing models. RankNovo employs a\nlist-wise reranking approach, modeling candidate peptides as multiple sequence\nalignments and utilizing axial attention to extract informative features across\ncandidates. Additionally, we introduce two new metrics, PMD (Peptide Mass\nDeviation) and RMD (residual Mass Deviation), which offer delicate supervision\nby quantifying mass differences between peptides at both the sequence and\nresidue levels. Extensive experiments demonstrate that RankNovo not only\nsurpasses its base models used to generate training candidates for reranking\npre-training, but also sets a new state-of-the-art benchmark. Moreover,\nRankNovo exhibits strong zero-shot generalization to unseen models whose\ngenerations were not exposed during training, highlighting its robustness and\npotential as a universal reranking framework for peptide sequencing. Our work\npresents a novel reranking strategy that fundamentally challenges existing\nsingle-model paradigms and advances the frontier of accurate de novo\nsequencing. Our source code is provided on GitHub.", "AI": {"tldr": "RankNovo\u662f\u4e00\u4e2a\u7528\u4e8e\u80bd\u6d4b\u5e8f\u7684\u6df1\u5ea6\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u6d4b\u5e8f\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u5e8f\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u80bd\u6d4b\u5e8f\u65b9\u6cd5\u53d7\u9650\u4e8e\u8d28\u8c31\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u566a\u58f0\u4fe1\u53f7\u7684\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u6570\u636e\u7279\u5f02\u6027\u504f\u5dee\u3002", "method": "RankNovo\u91c7\u7528\u5217\u8868\u5f0f\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5c06\u5019\u9009\u80bd\u5efa\u6a21\u4e3a\u591a\u5e8f\u5217\u6bd4\u5bf9\uff0c\u5e76\u5229\u7528\u8f74\u5411\u6ce8\u610f\u529b\u63d0\u53d6\u7279\u5f81\u3002\u5f15\u5165\u4e86PMD\u548cRMD\u4e24\u4e2a\u65b0\u6307\u6807\u8fdb\u884c\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRankNovo\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u5176\u57fa\u7840\u6a21\u578b\uff0c\u8fd8\u521b\u4e0b\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RankNovo\u6311\u6218\u4e86\u73b0\u6709\u7684\u5355\u6a21\u578b\u8303\u5f0f\uff0c\u4e3a\u80bd\u6d4b\u5e8f\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u91cd\u6392\u5e8f\u7b56\u7565\u3002", "relevance": 20.0}}
{"id": "2505.17137", "pdf": "https://arxiv.org/pdf/2505.17137", "abs": "https://arxiv.org/abs/2505.17137", "authors": ["Kristin Qi", "Youxiang Zhu", "Caroline Summerour", "John A. Batsis", "Xiaohui Liang"], "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCog-TiPRO\u7684\u6846\u67b6\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u63d0\u793a\u4f18\u5316\u3001HuBERT\u58f0\u5b66\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8eTransformer\u7684\u65f6\u5e8f\u5efa\u6a21\uff0c\u901a\u8fc7\u8bed\u97f3\u52a9\u624b\u7cfb\u7edf\u68c0\u6d4b\u8ba4\u77e5\u8870\u9000\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5\u8870\u9000\u8bca\u65ad\u65b9\u6cd5\u8017\u65f6\u4e14\u4e0d\u9002\u7528\u4e8e\u9891\u7e41\u76d1\u6d4b\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u8bed\u97f3\u52a9\u624b\u7cfb\u7edf\u4f5c\u4e3a\u975e\u4fb5\u5165\u6027\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u7ed3\u5408LLM\u9a71\u52a8\u7684\u63d0\u793a\u4f18\u5316\u3001HuBERT\u58f0\u5b66\u7279\u5f81\u63d0\u53d6\u548ciTransformer\u65f6\u5e8f\u5efa\u6a21\uff0c\u5206\u6790\u8bed\u97f3\u547d\u4ee4\u4e2d\u7684\u8bed\u8a00\u548c\u58f0\u5b66\u7279\u5f81\u3002", "result": "\u5728\u68c0\u6d4b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u65f6\uff0c\u51c6\u786e\u7387\u4e3a73.80%\uff0cF1\u5206\u6570\u4e3a72.67%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8627.13%\u3002", "conclusion": "Cog-TiPRO\u6846\u67b6\u901a\u8fc7\u8bed\u97f3\u547d\u4ee4\u5206\u6790\u6709\u6548\u68c0\u6d4b\u8ba4\u77e5\u8870\u9000\uff0c\u5e76\u8bc6\u522b\u4e86\u72ec\u7279\u7684\u8bed\u8a00\u7279\u5f81\u3002", "relevance": 40.0}}
{"id": "2505.17666", "pdf": "https://arxiv.org/pdf/2505.17666", "abs": "https://arxiv.org/abs/2505.17666", "authors": ["Shuxian Ma", "Zihao Dong", "Runmin Cong", "Sam Kwong", "Xiuli Shao"], "title": "Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification", "categories": ["cs.CV", "I.4.0; I.5.0"], "comment": "11 pages, 2 figures, 5 tablets; Submitted to BMVC2025", "summary": "Deep learning-based multi-view coarse-grained 3D shape classification has\nachieved remarkable success over the past decade, leveraging the powerful\nfeature learning capabilities of CNN-based and ViT-based backbones. However, as\na challenging research area critical for detailed shape understanding,\nfine-grained 3D classification remains understudied due to the limited\ndiscriminative information captured during multi-view feature aggregation,\nparticularly for subtle inter-class variations, class imbalance, and inherent\ninterpretability limitations of parametric model. To address these problems, we\npropose the first prototype-based framework named Proto-FG3D for fine-grained\n3D shape classification, achieving a paradigm shift from parametric softmax to\nnon-parametric prototype learning. Firstly, Proto-FG3D establishes joint\nmulti-view and multi-category representation learning via Prototype\nAssociation. Secondly, prototypes are refined via Online Clustering, improving\nboth the robustness of multi-view feature allocation and inter-subclass\nbalance. Finally, prototype-guided supervised learning is established to\nenhance fine-grained discrimination via prototype-view correlation analysis and\nenables ad-hoc interpretability through transparent case-based reasoning.\nExperiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art\nmethods in accuracy, transparent predictions, and ad-hoc interpretability with\nvisualizations, challenging conventional fine-grained 3D recognition\napproaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u7684\u6846\u67b6Proto-FG3D\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u5206\u7c7b\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u539f\u578b\u5b66\u4e60\u53d6\u4ee3\u4f20\u7edf\u7684\u53c2\u6570\u5316softmax\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u5206\u7c7b\u56e0\u591a\u89c6\u56fe\u7279\u5f81\u805a\u5408\u4e2d\u6709\u9650\u7684\u5224\u522b\u4fe1\u606f\u548c\u53c2\u6570\u5316\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u9650\u5236\u800c\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0cProto-FG3D\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Proto-FG3D\u901a\u8fc7\u539f\u578b\u5173\u8054\u5b9e\u73b0\u591a\u89c6\u56fe\u548c\u591a\u7c7b\u522b\u8054\u5408\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u5728\u7ebf\u805a\u7c7b\u4f18\u5316\u539f\u578b\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u7ec6\u7c92\u5ea6\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728FG3D\u548cModelNet40\u6570\u636e\u96c6\u4e0a\uff0cProto-FG3D\u5728\u51c6\u786e\u6027\u3001\u900f\u660e\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Proto-FG3D\u4e3a\u7ec6\u7c92\u5ea63D\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u539f\u578b\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.17553", "pdf": "https://arxiv.org/pdf/2505.17553", "abs": "https://arxiv.org/abs/2505.17553", "authors": ["Jinyuan Feng", "Chaopeng Wei", "Tenghai Qiu", "Tianyi Hu", "Zhiqiang Pu"], "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves\nspecializing functionalities into different experts and sparsely activating\nthem appropriately, has been widely adopted as a promising approach to\ntrade-off between model capacity and computation overhead. However, current MoE\nvariants fall short on heterogeneous datasets, ignoring the fact that experts\nmay learn similar knowledge, resulting in the underutilization of MoE's\ncapacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),\na novel method to promote modularization and specialization in MoE, where the\nexperts are trained along with a contrastive objective by sampling from\nactivated and inactivated experts in top-k routing. We demonstrate that such a\ncontrastive objective recovers the mutual-information gap between inputs and\nthe two types of experts. Experiments on several benchmarks and in multi-task\nsettings demonstrate that CoMoE can consistently enhance MoE's capacity and\npromote modularization among the experts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoMoE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u4e13\u5bb6\u6a21\u5757\uff0c\u63d0\u5347MoE\u5728\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u5757\u5316\u548c\u4e13\u4e1a\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524dMoE\u53d8\u4f53\u5728\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e13\u5bb6\u6a21\u5757\u53ef\u80fd\u5b66\u4e60\u76f8\u4f3c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5bb9\u91cf\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faCoMoE\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u4e13\u5bb6\u6a21\u5757\uff0c\u901a\u8fc7\u91c7\u6837\u6fc0\u6d3b\u548c\u672a\u6fc0\u6d3b\u4e13\u5bb6\u6765\u6062\u590d\u8f93\u5165\u4e0e\u4e13\u5bb6\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u5dee\u8ddd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\uff0cCoMoE\u80fd\u6301\u7eed\u63d0\u5347MoE\u7684\u5bb9\u91cf\u5e76\u4fc3\u8fdb\u4e13\u5bb6\u6a21\u5757\u5316\u3002", "conclusion": "CoMoE\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3MoE\u5728\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u5176\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.17139", "pdf": "https://arxiv.org/pdf/2505.17139", "abs": "https://arxiv.org/abs/2505.17139", "authors": ["Wanghan Xu", "Xiangyu Zhao", "Yuhao Zhou", "Xiaoyu Yue", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Lei Bai"], "title": "EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) drive interest in scientific\napplications, necessitating specialized benchmarks such as Earth science.\nExisting benchmarks either present a general science focus devoid of Earth\nscience specificity or cover isolated subdomains, lacking holistic evaluation.\nFurthermore, current benchmarks typically neglect the assessment of LLMs'\ncapabilities in open-ended scientific exploration. In this paper, we present a\ncomprehensive and professional benchmark for the Earth sciences, designed to\nevaluate the capabilities of LLMs in scientific exploration within this domain,\nspanning from fundamental to advanced levels. Leveraging a corpus of 100,000\nresearch papers, we first construct two Question Answering (QA) datasets:\nEarth-Iron, which offers extensive question coverage for broad assessment, and\nEarth-Silver, which features a higher level of difficulty to evaluate\nprofessional depth. These datasets encompass five Earth spheres, 114\ndisciplines, and 11 task categories, assessing foundational knowledge crucial\nfor scientific exploration. Most notably, we introduce Earth-Gold with new\nmetrics, a dataset comprising open-ended multi-turn dialogues specifically\ndesigned to evaluate the advanced capabilities of LLMs in scientific\nexploration, including methodology induction, limitation analysis, and concept\nproposal. Extensive experiments reveal limitations in 11 leading LLMs across\ndifferent domains and tasks, highlighting considerable room for improvement in\ntheir scientific exploration capabilities. The benchmark is available on\nhttps://huggingface.co/ai-earth .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5730\u7403\u79d1\u5b66\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u79d1\u5b66\u63a2\u7d22\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u57fa\u7840\u77e5\u8bc6\u548c\u9ad8\u7ea7\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u5730\u7403\u79d1\u5b66\u7684\u4e13\u95e8\u8bc4\u4f30\uff0c\u4e14\u5ffd\u89c6\u4e86\u5bf9LLM\u5728\u5f00\u653e\u79d1\u5b66\u63a2\u7d22\u4e2d\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e10\u4e07\u7bc7\u7814\u7a76\u8bba\u6587\u6784\u5efa\u4e86\u4e24\u4e2aQA\u6570\u636e\u96c6\uff08Earth-Iron\u548cEarth-Silver\uff09\u548c\u4e00\u4e2a\u5f00\u653e\u5bf9\u8bdd\u6570\u636e\u96c6\uff08Earth-Gold\uff09\uff0c\u6db5\u76d6\u591a\u4e2a\u5730\u7403\u79d1\u5b66\u9886\u57df\u548c\u4efb\u52a1\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e8611\u79cd\u9886\u5148LLM\u5728\u4e0d\u540c\u9886\u57df\u548c\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u8868\u660e\u5176\u79d1\u5b66\u63a2\u7d22\u80fd\u529b\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u5730\u7403\u79d1\u5b66\u9886\u57dfLLM\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17674", "pdf": "https://arxiv.org/pdf/2505.17674", "abs": "https://arxiv.org/abs/2505.17674", "authors": ["Xuerui Qiu", "Peixi Wu", "Yaozhi Wen", "Shaowei Gu", "Yuqi Pan", "Xinhao Luo", "Bo XU", "Guoqi Li"], "title": "SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D\nspatio-temporal features. However, existing SNNs still exhibit a significant\nperformance gap compared to Artificial Neural Networks (ANNs) due to inadequate\npre-training strategies. These limitations manifest as restricted\ngeneralization ability, task specificity, and a lack of multimodal\nunderstanding, particularly in challenging tasks such as multimodal question\nanswering and zero-shot 3D classification. To overcome these challenges, we\npropose a Spike-based Vision-Language (SVL) pretraining framework that empowers\nSNNs with open-world 3D understanding while maintaining spike-driven\nefficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment\n(MTA) for label-free triplet-based contrastive learning across 3D, image, and\ntext modalities, and (ii) Re-parameterizable Vision-Language Integration\n(Rep-VLI) to enable lightweight inference without relying on large text\nencoders. Extensive experiments show that SVL achieves a top-1 accuracy of\n85.4% in zero-shot 3D classification, surpassing advanced ANN models, and\nconsistently outperforms prior SNNs on downstream tasks, including 3D\nclassification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),\nand 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables\nSNNs to perform open-world 3D question answering, sometimes outperforming ANNs.\nTo the best of our knowledge, SVL represents the first scalable, generalizable,\nand hardware-friendly paradigm for 3D open-world understanding, effectively\nbridging the gap between SNNs and ANNs in complex open-world understanding\ntasks. Code is available https://github.com/bollossom/SVL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u7684\u89c6\u89c9\u8bed\u8a00\uff08SVL\uff09\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u5f00\u653e\u4e16\u754c3D\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709SNNs\u5728\u6027\u80fd\u4e0a\u4e0e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "SVL\u6846\u67b6\u5305\u542b\u591a\u5c3a\u5ea6\u4e09\u91cd\u5bf9\u9f50\uff08MTA\uff09\u548c\u53ef\u91cd\u53c2\u6570\u5316\u89c6\u89c9\u8bed\u8a00\u96c6\u6210\uff08Rep-VLI\uff09\uff0c\u5206\u522b\u7528\u4e8e\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u63a8\u7406\u3002", "result": "SVL\u5728\u96f6\u6837\u672c3D\u5206\u7c7b\u4e2d\u8fbe\u523085.4%\u7684Top-1\u51c6\u786e\u7387\uff0c\u8d85\u8d8aANNs\uff0c\u5e76\u5728\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709SNNs\u3002", "conclusion": "SVL\u662f\u9996\u4e2a\u53ef\u6269\u5c55\u3001\u901a\u7528\u4e14\u786c\u4ef6\u53cb\u597d\u76843D\u5f00\u653e\u4e16\u754c\u7406\u89e3\u8303\u5f0f\uff0c\u6709\u6548\u7f29\u5c0f\u4e86SNNs\u4e0eANNs\u7684\u5dee\u8ddd\u3002", "relevance": 30.0}}
{"id": "2505.17556", "pdf": "https://arxiv.org/pdf/2505.17556", "abs": "https://arxiv.org/abs/2505.17556", "authors": ["Nikolaos Anastasiou", "Spyros Kondylatos", "Ioannis Papoutsis"], "title": "Wildfire spread forecasting with Deep Learning", "categories": ["cs.LG", "cs.CV", "I.2.7"], "comment": "10 pages, 9 figures", "summary": "Accurate prediction of wildfire spread is crucial for effective risk\nmanagement, emergency response, and strategic resource allocation. In this\nstudy, we present a deep learning (DL)-based framework for forecasting the\nfinal extent of burned areas, using data available at the time of ignition. We\nleverage a spatio-temporal dataset that covers the Mediterranean region from\n2006 to 2022, incorporating remote sensing data, meteorological observations,\nvegetation maps, land cover classifications, anthropogenic factors, topography\ndata, and thermal anomalies. To evaluate the influence of temporal context, we\nconduct an ablation study examining how the inclusion of pre- and post-ignition\ndata affects model performance, benchmarking the temporal-aware DL models\nagainst a baseline trained exclusively on ignition-day inputs. Our results\nindicate that multi-day observational data substantially improve predictive\naccuracy. Particularly, the best-performing model, incorporating a temporal\nwindow of four days before to five days after ignition, improves both the F1\nscore and the Intersection over Union by almost 5% in comparison to the\nbaseline on the test dataset. We publicly release our dataset and models to\nenhance research into data-driven approaches for wildfire modeling and\nresponse.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u91ce\u706b\u8513\u5ef6\u7684\u6700\u7ec8\u8303\u56f4\uff0c\u5229\u7528\u70b9\u706b\u65f6\u53ef\u7528\u7684\u6570\u636e\u3002\u901a\u8fc7\u591a\u65e5\u89c2\u6d4b\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u51c6\u786e\u7684\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\u5bf9\u98ce\u9669\u7ba1\u7406\u3001\u5e94\u6025\u54cd\u5e94\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u706b\u524d\u540e\u7684\u591a\u65e5\u89c2\u6d4b\u6570\u636e\uff08\u5305\u62ec\u9065\u611f\u3001\u6c14\u8c61\u3001\u690d\u88ab\u3001\u5730\u5f62\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u5f71\u54cd\u3002", "result": "\u6700\u4f73\u6a21\u578b\uff08\u70b9\u706b\u524d\u540e\u56db\u81f3\u4e94\u5929\u7684\u6570\u636e\uff09\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684F1\u5206\u6570\u548cIoU\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e86\u8fd15%\u3002", "conclusion": "\u591a\u65e5\u89c2\u6d4b\u6570\u636e\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u516c\u5f00\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u91ce\u706b\u5efa\u6a21\u7814\u7a76\u3002", "relevance": 30.0}}
{"id": "2505.17140", "pdf": "https://arxiv.org/pdf/2505.17140", "abs": "https://arxiv.org/abs/2505.17140", "authors": ["Essa Jan", "Moiz Ali", "Muhammad Saram Hassan", "Fareed Zaffar", "Yasir Zaki"], "title": "Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "4 pages, 1 figure", "summary": "As the knowledge of large language models (LLMs) becomes outdated over time,\nthere is a growing need for efficient methods to update them, especially when\ninjecting proprietary information. Our study reveals that\ncomprehension-intensive fine-tuning tasks (e.g., question answering and blanks)\nachieve substantially higher knowledge retention rates (48%) compared to\nmapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),\ndespite exposure to identical factual content. We demonstrate that this pattern\npersists across model architectures and follows scaling laws, with larger\nmodels showing improved retention across all task types. However, all models\nexhibit significant performance drops when applying injected knowledge in\nbroader contexts, suggesting limited semantic integration. These findings show\nthe importance of task selection in updating LLM knowledge, showing that\neffective knowledge injection relies not just on data exposure but on the depth\nof cognitive engagement during fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u9ad8\u6548\u66f4\u65b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\uff0c\u53d1\u73b0\u7406\u89e3\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u95ee\u7b54\uff09\u6bd4\u6620\u5c04\u578b\u4efb\u52a1\uff08\u5982\u7ffb\u8bd1\uff09\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u77e5\u8bc6\uff0c\u4e14\u6a21\u578b\u8d8a\u5927\u6548\u679c\u8d8a\u597d\uff0c\u4f46\u8bed\u4e49\u6574\u5408\u4ecd\u6709\u9650\u3002", "motivation": "\u968f\u7740LLMs\u77e5\u8bc6\u9010\u6e10\u8fc7\u65f6\uff0c\u9700\u8981\u9ad8\u6548\u66f4\u65b0\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u6ce8\u5165\u4e13\u6709\u4fe1\u606f\u65f6\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u7406\u89e3\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u95ee\u7b54\uff09\u548c\u6620\u5c04\u578b\u4efb\u52a1\uff08\u5982\u7ffb\u8bd1\uff09\u7684\u77e5\u8bc6\u4fdd\u7559\u7387\uff0c\u5206\u6790\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u7684\u5f71\u54cd\u3002", "result": "\u7406\u89e3\u5bc6\u96c6\u578b\u4efb\u52a1\u77e5\u8bc6\u4fdd\u7559\u7387\uff0848%\uff09\u663e\u8457\u9ad8\u4e8e\u6620\u5c04\u578b\u4efb\u52a1\uff0817%-20%\uff09\uff0c\u4e14\u6a21\u578b\u8d8a\u5927\u6548\u679c\u8d8a\u597d\uff0c\u4f46\u8bed\u4e49\u6574\u5408\u6709\u9650\u3002", "conclusion": "\u4efb\u52a1\u9009\u62e9\u5bf9LLMs\u77e5\u8bc6\u66f4\u65b0\u81f3\u5173\u91cd\u8981\uff0c\u77e5\u8bc6\u6ce8\u5165\u6548\u679c\u53d6\u51b3\u4e8e\u5fae\u8c03\u65f6\u7684\u8ba4\u77e5\u6df1\u5ea6\u3002", "relevance": 85.0}}
{"id": "2505.17677", "pdf": "https://arxiv.org/pdf/2505.17677", "abs": "https://arxiv.org/abs/2505.17677", "authors": ["Ming Hu", "Zhendi Yu", "Feilong Tang", "Kaiwen Chen", "Yulong Li", "Imran Razzak", "Junjun He", "Tolga Birdal", "Kaijing Zhou", "Zongyuan Ge"], "title": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D reconstruction of hands and instruments is critical for\nvision-based analysis of ophthalmic microsurgery, yet progress has been\nhampered by the lack of realistic, large-scale datasets and reliable annotation\ntools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic\n3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from\n40 surgeons and totaling 7.1 million frames, with fine-grained annotations of\n12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full\n6-DoF instrument poses. To scalably produce high-fidelity labels, we design a\nmulti-stage automatic annotation pipeline that integrates multi-view data\nobservation, data-driven motion prior with cross-view geometric consistency and\nbiomechanical constraints, along with a combination of collision-aware\ninteraction constraints for instrument interactions. Building upon OphNet-3D,\nwe establish two challenging benchmarks-bimanual hand pose estimation and\nhand-instrument interaction reconstruction-and propose two dedicated\narchitectures: H-Net for dual-hand mesh recovery and OH-Net for joint\nreconstruction of two-hand-two-instrument interactions. These models leverage a\nnovel spatial reasoning module with weak-perspective camera modeling and\ncollision-aware center-based representation. Both architectures outperform\nexisting methods by substantial margins, achieving improvements of over 2mm in\nMean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand\nand instrument reconstruction, respectively.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86OphNet-3D\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u773c\u79d1\u624b\u672f\u76843D\u91cd\u5efa\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u52a8\u6807\u6ce8\u6d41\u6c34\u7ebf\u548c\u4e24\u4e2a\u65b0\u67b6\u6784H-Net\u548cOH-Net\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u773c\u79d1\u624b\u672f\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u6570\u636e\u96c6\u548c\u53ef\u9760\u6807\u6ce8\u5de5\u5177\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u9636\u6bb5\u81ea\u52a8\u6807\u6ce8\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u6570\u636e\u3001\u8fd0\u52a8\u5148\u9a8c\u548c\u751f\u7269\u529b\u5b66\u7ea6\u675f\uff0c\u5e76\u63d0\u51fa\u4e86H-Net\u548cOH-Net\u67b6\u6784\u3002", "result": "H-Net\u548cOH-Net\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cMPJPE\u63d0\u5347\u8d85\u8fc72mm\uff0cADD-S\u6307\u6807\u63d0\u5347\u8fbe23%\u3002", "conclusion": "OphNet-3D\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u67b6\u6784\u4e3a\u773c\u79d1\u624b\u672f\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 20.0}}
{"id": "2505.17072", "pdf": "https://arxiv.org/pdf/2505.17072", "abs": "https://arxiv.org/abs/2505.17072", "authors": ["Jianwei Li", "Jung-Eng Kim"], "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Recent studies on the safety alignment of large language models (LLMs) have\nrevealed that existing approaches often operate superficially, leaving models\nvulnerable to various adversarial attacks. Despite their significance, these\nstudies generally fail to offer actionable solutions beyond data augmentation\nfor achieving more robust safety mechanisms. This paper identifies a\nfundamental cause of this superficiality: existing alignment approaches often\npresume that models can implicitly learn a safety-related reasoning task during\nthe alignment process, enabling them to refuse harmful requests. However, the\nlearned safety signals are often diluted by other competing objectives, leading\nmodels to struggle with drawing a firm safety-conscious decision boundary when\nconfronted with adversarial attacks. Based on this observation, by explicitly\nintroducing a safety-related binary classification task and integrating its\nsignals with our attention and decoding strategies, we eliminate this ambiguity\nand allow models to respond more responsibly to malicious queries. We emphasize\nthat, with less than 0.2x overhead cost, our approach enables LLMs to assess\nthe safety of both the query and the previously generated tokens at each\nnecessary generating step. Extensive experiments demonstrate that our method\nsignificantly improves the resilience of LLMs against various adversarial\nattacks, offering a promising pathway toward more robust generative AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u5b89\u5168\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u4e0e\u89e3\u7801\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLMs\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6a21\u578b\u80fd\u9690\u5f0f\u5b66\u4e60\u5b89\u5168\u76f8\u5173\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u5b9e\u9645\u4e2d\u5b89\u5168\u4fe1\u53f7\u5e38\u88ab\u5176\u4ed6\u76ee\u6807\u7a00\u91ca\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "method": "\u663e\u5f0f\u5f15\u5165\u5b89\u5168\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u4fe1\u53f7\u4e0e\u6ce8\u610f\u529b\u53ca\u89e3\u7801\u7b56\u7565\u7ed3\u5408\uff0c\u4ee5\u6d88\u9664\u5b89\u5168\u51b3\u7b56\u7684\u6a21\u7cca\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLMs\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5f00\u9500\u5c0f\u4e8e0.2\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u751f\u6210\u5f0fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "relevance": 90.0}}
{"id": "2505.17575", "pdf": "https://arxiv.org/pdf/2505.17575", "abs": "https://arxiv.org/abs/2505.17575", "authors": ["Changfan Yang", "Lichen Bai", "Yinpeng Wang", "Shufei Zhang", "Zeke Xie"], "title": "Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs", "categories": ["cs.LG"], "comment": "31 pages. 20 tables, 17 figures, Dataset", "summary": "Solving partial differential equations (PDEs) with machine learning has\nrecently attracted great attention, as PDEs are fundamental tools for modeling\nreal-world systems that range from fundamental physical science to advanced\nengineering disciplines. Most real-world physical systems across various\ndisciplines are actually involved in multiple coupled physical fields rather\nthan a single field. However, previous machine learning studies mainly focused\non solving single-field problems, but overlooked the importance and\ncharacteristics of multiphysics problems in real world. Multiphysics PDEs\ntypically entail multiple strongly coupled variables, thereby introducing\nadditional complexity and challenges, such as inter-field coupling. Both\nbenchmarking and solving multiphysics problems with machine learning remain\nlargely unexamined. To identify and address the emerging challenges in\nmultiphysics problems, we mainly made three contributions in this work. First,\nwe collect the first general multiphysics dataset, the Multiphysics Bench, that\nfocuses on multiphysics PDE solving with machine learning. Multiphysics Bench\nis also the most comprehensive PDE dataset to date, featuring the broadest\nrange of coupling types, the greatest diversity of PDE formulations, and the\nlargest dataset scale. Second, we conduct the first systematic investigation on\nmultiple representative learning-based PDE solvers, such as PINNs, FNO,\nDeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,\nnaively applying these existing solvers usually show very poor performance for\nsolving multiphysics. Third, through extensive experiments and discussions, we\nreport multiple insights and a bag of useful tricks for solving multiphysics\nwith machine learning, motivating future directions in the study and simulation\nof complex, coupled physical systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u901a\u7528\u7684\u591a\u7269\u7406\u573a\u6570\u636e\u96c6Multiphysics Bench\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684PDE\u6c42\u89e3\u5668\u5728\u591a\u7269\u7406\u573a\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u89e3\u51b3\u591a\u7269\u7406\u573a\u95ee\u9898\u7684\u5b9e\u7528\u6280\u5de7\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u591a\u7269\u7406\u573aPDE\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u6b64\u524d\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u573a\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u591a\u7269\u7406\u573a\u7684\u590d\u6742\u6027\u548c\u6311\u6218\u3002", "method": "\u6536\u96c6\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u901a\u7528\u7684\u591a\u7269\u7406\u573a\u6570\u636e\u96c6Multiphysics Bench\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86PINNs\u3001FNO\u3001DeepONet\u548cDiffusionPDE\u7b49\u6c42\u89e3\u5668\u5728\u591a\u7269\u7406\u573a\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u6c42\u89e3\u5668\u5728\u591a\u7269\u7406\u573a\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u89e3\u51b3\u591a\u7269\u7406\u573a\u95ee\u9898\u7684\u5b9e\u7528\u6280\u5de7\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u591a\u7269\u7406\u573a\u95ee\u9898\u7684\u89e3\u51b3\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u8bba\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "relevance": 30.0}}
{"id": "2505.17144", "pdf": "https://arxiv.org/pdf/2505.17144", "abs": "https://arxiv.org/abs/2505.17144", "authors": ["Bohan Jin", "Shuhan Qi", "Kehai Chen", "Xinyi Guo", "Xuan Wang"], "title": "MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models", "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL 2025", "summary": "The widespread use of Large Multimodal Models (LMMs) has raised concerns\nabout model toxicity. However, current research mainly focuses on explicit\ntoxicity, with less attention to some more implicit toxicity regarding\nprejudice and discrimination. To address this limitation, we introduce a\nsubtler type of toxicity named dual-implicit toxicity and a novel toxicity\nbenchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark.\nSpecifically, we first create the MDIT-Dataset with dual-implicit toxicity\nusing the proposed Multi-stage Human-in-loop In-context Generation method.\nBased on this dataset, we construct the MDIT-Bench, a benchmark for evaluating\nthe sensitivity of models to dual-implicit toxicity, with 317,638 questions\ncovering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes\nthree difficulty levels, and we propose a metric to measure the toxicity gap\nexhibited by the model across them. In the experiment, we conducted MDIT-Bench\non 13 prominent LMMs, and the results show that these LMMs cannot handle\ndual-implicit toxicity effectively. The model's performance drops significantly\nin hard level, revealing that these LMMs still contain a significant amount of\nhidden but activatable toxicity. Data are available at\nhttps://github.com/nuo1nuo/MDIT-Bench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u9690\u6bd2\u6027\u7684\u65b0\u578b\u6bd2\u6027\u7c7b\u578b\uff0c\u5e76\u6784\u5efa\u4e86MDIT-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5bf9\u53cc\u9690\u6bd2\u6027\u7684\u654f\u611f\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709LMMs\u5728\u5904\u7406\u53cc\u9690\u6bd2\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u663e\u6027\u6bd2\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u9690\u6027\u6bd2\u6027\uff08\u5982\u504f\u89c1\u548c\u6b67\u89c6\uff09\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u4eba\u7c7b\u53c2\u4e0e\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u65b9\u6cd5\u521b\u5efaMDIT-Dataset\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaMDIT-Bench\u57fa\u51c6\uff0c\u5305\u542b317,638\u4e2a\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c13\u79cd\u4e3b\u6d41LMMs\u5728\u5904\u7406\u53cc\u9690\u6bd2\u6027\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u96be\u5ea6\u7ea7\u522b\u3002", "conclusion": "LMMs\u4ecd\u5b58\u5728\u5927\u91cf\u53ef\u6fc0\u6d3b\u7684\u9690\u6027\u6bd2\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "relevance": 75.0}}
{"id": "2505.17684", "pdf": "https://arxiv.org/pdf/2505.17684", "abs": "https://arxiv.org/abs/2505.17684", "authors": ["Nisha Lakshmana Raichur", "Lucas Heublein", "Christopher Mutschler", "Felix Ott"], "title": "5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization", "categories": ["cs.CV", "62D05, 62J99, 62P12, 68T37", "G.3; H.3.3; I.2.4; I.4; I.5.1"], "comment": "7 pages, 6 figures", "summary": "Indoor positioning based on 5G data has achieved high accuracy through the\nadoption of recent machine learning (ML) techniques. However, the performance\nof learning-based methods degrades significantly when environmental conditions\nchange, thereby hindering their applicability to new scenarios. Acquiring new\ntraining data for each environmental change and fine-tuning ML models is both\ntime-consuming and resource-intensive. This paper introduces a domain\nincremental learning (DIL) approach for dynamic 5G indoor localization, called\n5G-DIL, enabling rapid adaptation to environmental changes. We present a novel\nsimilarity-aware sampling technique based on the Chebyshev distance, designed\nto efficiently select specific exemplars from the previous environment while\ntraining only on the modified regions of the new environment. This avoids the\nneed to train on the entire region, significantly reducing the time and\nresources required for adaptation without compromising localization accuracy.\nThis approach requires as few as 50 exemplars from adaptation domains,\nsignificantly reducing training time while maintaining high positioning\naccuracy in previous environments. Comparative evaluations against\nstate-of-the-art DIL techniques on a challenging real-world indoor dataset\ndemonstrate the effectiveness of the proposed sample selection method. Our\napproach is adaptable to real-world non-line-of-sight propagation scenarios and\nachieves an MAE positioning error of 0.261 meters, even under dynamic\nenvironmental conditions. Code:\nhttps://gitlab.cc-asp.fraunhofer.de/5g-pos/5g-dil", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57df\u589e\u91cf\u5b66\u4e60\uff08DIL\uff09\u7684\u52a8\u60015G\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd55G-DIL\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u611f\u77e5\u91c7\u6837\u6280\u672f\u5feb\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u5b66\u4e60\u578b\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd5\u5728\u73af\u5883\u53d8\u5316\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u8017\u65f6\u8017\u8d44\u6e90\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb\u7684\u76f8\u4f3c\u6027\u611f\u77e5\u91c7\u6837\u6280\u672f\uff0c\u9009\u62e9\u7279\u5b9a\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u907f\u514d\u5168\u533a\u57df\u8bad\u7ec3\u3002", "result": "\u5728\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u5b9a\u4f4d\u8bef\u5deeMAE\u4e3a0.261\u7c73\uff0c\u4e14\u4ec5\u970050\u4e2a\u6837\u672c\u5373\u53ef\u9002\u5e94\u65b0\u73af\u5883\u3002", "conclusion": "5G-DIL\u65b9\u6cd5\u9ad8\u6548\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u4fdd\u6301\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u975e\u89c6\u8ddd\u4f20\u64ad\u573a\u666f\u3002", "relevance": 30.0}}
{"id": "2505.17579", "pdf": "https://arxiv.org/pdf/2505.17579", "abs": "https://arxiv.org/abs/2505.17579", "authors": ["Teruki Sano", "Minoru Kuribayashi", "Masao Sakai", "Shuji Ishobe", "Eisuke Koizumi"], "title": "Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation", "categories": ["cs.LG"], "comment": "Accepted to EUSIPCO 2025", "summary": "In this paper, we propose a novel framework for ownership verification of\ndeep neural network (DNN) models for image classification tasks. It allows\nverification of model identity by both the rightful owner and third party\nwithout presenting the original model. We assume a gray-box scenario where an\nunauthorized user owns a model that is illegally copied from the original\nmodel, provides services in a cloud environment, and the user throws images and\nreceives the classification results as a probability distribution of output\nclasses. The framework applies a white-box adversarial attack to align the\noutput probability of a specific class to a designated value. Due to the\nknowledge of original model, it enables the owner to generate such adversarial\nexamples. We propose a simple but effective adversarial attack method based on\nthe iterative Fast Gradient Sign Method (FGSM) by introducing control\nparameters. Experimental results confirm the effectiveness of the\nidentification of DNN models using adversarial attack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6240\u6709\u6743\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u767d\u76d2\u5bf9\u6297\u653b\u51fb\u9a8c\u8bc1\u6a21\u578b\u8eab\u4efd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4e91\u73af\u5883\u4e2d\u88ab\u975e\u6cd5\u590d\u5236\u548c\u4f7f\u7528\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u6a21\u578b\u6240\u6709\u6743\u7684\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fed\u4ee3FGSM\u7684\u767d\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u53c2\u6570\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u9a8c\u8bc1\u6a21\u578b\u8eab\u4efd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bc6\u522bDNN\u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6a21\u578b\u6240\u6709\u6743\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7070\u8272\u76d2\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17149", "pdf": "https://arxiv.org/pdf/2505.17149", "abs": "https://arxiv.org/abs/2505.17149", "authors": ["Qin Chen", "Yuanyi Ren", "Xiaojun Ma", "Yuyang Shi"], "title": "Large Language Models for Predictive Analysis: How Far Are They?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Predictive analysis is a cornerstone of modern decision-making, with\napplications in various domains. Large Language Models (LLMs) have emerged as\npowerful tools in enabling nuanced, knowledge-intensive conversations, thus\naiding in complex decision-making tasks. With the burgeoning expectation to\nharness LLMs for predictive analysis, there is an urgent need to systematically\nassess their capability in this domain. However, there is a lack of relevant\nevaluations in existing studies. To bridge this gap, we introduce the\n\\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive\nanalysis queries originating from 44 real-world datasets of 8 diverse fields.\nWe design an evaluation protocol considering text analysis, code generation,\nand their alignment. Twelve renowned LLMs are evaluated, offering insights into\ntheir practical use in predictive analysis. Generally, we believe that existing\nLLMs still face considerable challenges in conducting predictive analysis. See\n\\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PredictiQ\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u6d4b\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u968f\u7740LLMs\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7f3a\u4e4f\u5bf9\u5176\u9884\u6d4b\u5206\u6790\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86PredictiQ\u57fa\u51c6\uff0c\u5305\u542b1130\u4e2a\u6765\u81ea44\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7684\u9884\u6d4b\u5206\u6790\u67e5\u8be2\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u77e5\u540dLLMs\u3002", "result": "\u73b0\u6709LLMs\u5728\u9884\u6d4b\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "conclusion": "LLMs\u5728\u9884\u6d4b\u5206\u6790\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u9700\u6539\u8fdb\u3002", "relevance": 85.0}}
{"id": "2505.17685", "pdf": "https://arxiv.org/pdf/2505.17685", "abs": "https://arxiv.org/abs/2505.17685", "authors": ["Shuang Zeng", "Xinyuan Chang", "Mengwei Xie", "Xinran Liu", "Yifan Bai", "Zheng Pan", "Mu Xu", "Xing Wei"], "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Visual language models (VLMs) have attracted increasing interest in\nautonomous driving due to their powerful reasoning capabilities. However,\nexisting VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored\nto the current scenario, which essentially represents highly abstract and\nsymbolic compression of visual information, potentially leading to\nspatio-temporal relationship ambiguity and fine-grained information loss. Is\nautonomous driving better modeled on real-world simulation and imagination than\non pure symbolic logic? In this paper, we propose a spatio-temporal CoT\nreasoning method that enables models to think visually. First, VLM serves as a\nworld model to generate unified image frame for predicting future world states:\nwhere perception results (e.g., lane divider and 3D detection) represent the\nfuture spatial relationships, and ordinary future frame represent the temporal\nevolution relationships. This spatio-temporal CoT then serves as intermediate\nreasoning steps, enabling the VLM to function as an inverse dynamics model for\ntrajectory planning based on current observations and future predictions. To\nimplement visual generation in VLMs, we propose a unified pretraining paradigm\nintegrating visual generation and understanding, along with a progressive\nvisual CoT enhancing autoregressive image generation. Extensive experimental\nresults demonstrate the effectiveness of the proposed method, advancing\nautonomous driving towards visual reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7aCoT\u63a8\u7406\u65b9\u6cd5\uff0c\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u80fd\u591f\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u73b0\u6709VLM\u901a\u5e38\u4f7f\u7528\u79bb\u6563\u6587\u672cCoT\uff0c\u53ef\u80fd\u5bfc\u81f4\u65f6\u7a7a\u5173\u7cfb\u6a21\u7cca\u548c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u4e22\u5931\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u89c2\u7684\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65f6\u7a7aCoT\u63a8\u7406\u65b9\u6cd5\uff0c\u5c06VLM\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u751f\u6210\u7edf\u4e00\u56fe\u50cf\u5e27\u4ee5\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5e76\u4f5c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7528\u4e8e\u8f68\u8ff9\u89c4\u5212\u3002\u540c\u65f6\u63d0\u51fa\u7edf\u4e00\u7684\u89c6\u89c9\u751f\u6210\u4e0e\u7406\u89e3\u9884\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9\u63a8\u7406\u3002", "conclusion": "\u65f6\u7a7aCoT\u63a8\u7406\u65b9\u6cd5\u63d0\u5347\u4e86VLM\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 60.0}}
{"id": "2505.17591", "pdf": "https://arxiv.org/pdf/2505.17591", "abs": "https://arxiv.org/abs/2505.17591", "authors": ["Judith Vilella-Cantos", "Juan Jos\u00e9 Cabrera", "Luis Pay\u00e1", "M\u00f3nica Ballesta", "David Valiente"], "title": "MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "In autonomous navigation systems, the solution of the place recognition\nproblem is crucial for their safe functioning. But this is not a trivial\nsolution, since it must be accurate regardless of any changes in the scene,\nsuch as seasonal changes and different weather conditions, and it must be\ngeneralizable to other environments. This paper presents our method,\nMinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input\ndata to obtain its spherical coordinates and intensity values normalized within\na range of 0 to 1 for each point, and it produces a robust place recognition\ndescriptor. To that end, a deep learning approach that combines Minkowski\nconvolutions and a U-net architecture with skip connections is used. The\nresults of MinkUNeXt-SI demonstrate that this method reaches and surpasses\nstate-of-the-art performance while it also generalizes satisfactorily to other\ndatasets. Additionally, we showcase the capture of a custom dataset and its use\nin evaluating our solution, which also achieves outstanding results. Both the\ncode of our solution and the runs of our dataset are publicly available for\nreproducibility purposes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMinkUNeXt-SI\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u5730\u70b9\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7LiDAR\u70b9\u4e91\u6570\u636e\u751f\u6210\u9c81\u68d2\u7684\u5730\u70b9\u8bc6\u522b\u63cf\u8ff0\u7b26\u3002", "motivation": "\u5730\u70b9\u8bc6\u522b\u5728\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u5e94\u5bf9\u573a\u666f\u53d8\u5316\uff08\u5982\u5b63\u8282\u548c\u5929\u6c14\uff09\u5e76\u5177\u5907\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408Minkowski\u5377\u79ef\u548cU-net\u67b6\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u9884\u5904\u7406LiDAR\u70b9\u4e91\u6570\u636e\u5e76\u751f\u6210\u63cf\u8ff0\u7b26\u3002", "result": "\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u6570\u636e\u96c6\u3002", "conclusion": "MinkUNeXt-SI\u662f\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u5730\u70b9\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17151", "pdf": "https://arxiv.org/pdf/2505.17151", "abs": "https://arxiv.org/abs/2505.17151", "authors": ["Zishuo Bao", "Yibo Liu", "Changyutao Qiu"], "title": "Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures, 2 tables", "summary": "With the rise of different language model architecture, fine-tuning is\nbecoming even more important for down stream tasks Model gets messy, finding\nproper hyperparameters for fine-tuning. Although BO has been tried for\nhyperparameter tuning, most of the existing methods are oblivious to the fact\nthat BO relies on careful choices of acquisition functions, which are essential\ncomponents of BO that guide how much to explore versus exploit during the\noptimization process; Different acquisition functions have different levels of\nsensitivity towards training loss and validation performance; existing methods\noften just apply an acquisition function no matter if the training and\nvalidation performance are sensitive to the acquisition function or not. This\nwork introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a\nbilevel BO strategy to improve the fine - tunning of large language models. Our\nwork on mixture of acquisition functions like EI and UCB into nested opt loops,\nwhere inner loop perform minimization of training loss while outer loops\noptimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base\nshow that when using EI and UCB, there is an improvement in generalization, and\nfine - tuning can be improved by up to 2.7%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBilevel-BO-SWA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u5c42\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u548c\u6a21\u578b\u878d\u5408\u6280\u672f\uff0c\u6539\u8fdb\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728GLUE\u4efb\u52a1\u4e0a\u80fd\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u5fae\u8c03\u6548\u679c\u6700\u9ad8\u63d0\u53472.7%\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u591a\u6837\u5316\uff0c\u5fae\u8c03\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e0d\u540c\u91c7\u96c6\u51fd\u6570\u5bf9\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6027\u80fd\u7684\u654f\u611f\u6027\uff0c\u5bfc\u81f4\u5fae\u8c03\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faBilevel-BO-SWA\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u5c42\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u6a21\u578b\u878d\u5408\u6280\u672f\u3002\u5185\u5c42\u5faa\u73af\u6700\u5c0f\u5316\u8bad\u7ec3\u635f\u5931\uff0c\u5916\u5c42\u5faa\u73af\u4f18\u5316\u9a8c\u8bc1\u6307\u6807\uff0c\u5e76\u6df7\u5408\u4f7f\u7528EI\u548cUCB\u7b49\u91c7\u96c6\u51fd\u6570\u3002", "result": "\u5728GLUE\u4efb\u52a1\u4e0a\u4f7f\u7528RoBERTa-base\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u6cdb\u5316\u6027\u80fd\u63d0\u5347\uff0c\u5fae\u8c03\u6548\u679c\u6700\u9ad8\u63d0\u53472.7%\u3002", "conclusion": "Bilevel-BO-SWA\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u91c7\u96c6\u51fd\u6570\u7684\u9009\u62e9\u548c\u53cc\u5c42\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u6548\u679c\u3002", "relevance": 85.0}}
{"id": "2505.17690", "pdf": "https://arxiv.org/pdf/2505.17690", "abs": "https://arxiv.org/abs/2505.17690", "authors": ["Yunyao Lu", "Yihang Wu", "Reem Kateb", "Ahmad Chaddad"], "title": "Semi-Supervised Medical Image Segmentation via Dual Networks", "categories": ["cs.CV"], "comment": "Accepted in ISBI2025", "summary": "Traditional supervised medical image segmentation models require large\namounts of labeled data for training; however, obtaining such large-scale\nlabeled datasets in the real world is extremely challenging. Recent\nsemi-supervised segmentation models also suffer from noisy pseudo-label issue\nand limited supervision in feature space. To solve these challenges, we propose\nan innovative semi-supervised 3D medical image segmentation method to reduce\nthe dependency on large, expert-labeled datasets. Furthermore, we introduce a\ndual-network architecture to address the limitations of existing methods in\nusing contextual information and generating reliable pseudo-labels. In\naddition, a self-supervised contrastive learning strategy is used to enhance\nthe representation of the network and reduce prediction uncertainty by\ndistinguishing between reliable and unreliable predictions. Experiments on\nclinical magnetic resonance imaging demonstrate that our approach outperforms\nstate-of-the-art techniques. Our code is available at\nhttps://github.com/AIPMLab/Semi-supervised-Segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u534a\u76d1\u77633D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u53cc\u7f51\u7edc\u67b6\u6784\u548c\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u76d1\u7763\u548c\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u4ee5\u53ca\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u7279\u5f81\u7a7a\u95f4\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u67b6\u6784\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u751f\u6210\u53ef\u9760\u4f2a\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7f51\u7edc\u8868\u793a\u548c\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e34\u5e8a\u78c1\u5171\u632f\u6210\u50cf\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u6807\u6ce8\u6570\u636e\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17595", "pdf": "https://arxiv.org/pdf/2505.17595", "abs": "https://arxiv.org/abs/2505.17595", "authors": ["Li Lin", "Xinyu Hu", "Xiaojun Wan"], "title": "NeUQI: Near-Optimal Uniform Quantization Parameter Initialization", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages, under review", "summary": "Large language models (LLMs) achieve impressive performance across domains\nbut face significant challenges when deployed on consumer-grade GPUs or\npersonal devices such as laptops, due to high memory consumption and inference\ncosts. Post-training quantization (PTQ) of LLMs offers a promising solution\nthat reduces their memory footprint and decoding latency. In practice, PTQ with\nuniform quantization representation is favored for its efficiency and ease of\ndeployment since uniform quantization is widely supported by mainstream\nhardware and software libraries. Recent studies on $\\geq 2$-bit uniform\nquantization have led to noticeable improvements in post-quantization model\nperformance; however, they primarily focus on quantization methodologies, while\nthe initialization of quantization parameters is underexplored and still relies\non the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method\ndevoted to efficiently determining near-optimal initial parameters for uniform\nquantization. NeUQI is orthogonal to prior quantization methodologies and can\nseamlessly integrate with them. The experiments with the LLaMA and Qwen\nfamilies on various tasks demonstrate that our NeUQI consistently outperforms\nexisting methods. Furthermore, when combined with a lightweight distillation\nstrategy, NeUQI can achieve superior performance to PV-tuning, a much more\nresource-intensive approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNeUQI\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u786e\u5b9a\u5747\u5300\u91cf\u5316\u7684\u8fd1\u6700\u4f18\u521d\u59cb\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u6d88\u8d39\u7ea7\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6d88\u8d39\u7ea7\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u9ad8\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u53c2\u6570\u521d\u59cb\u5316\u4e0a\u4f9d\u8d56\u6b21\u4f18\u7b56\u7565\u3002", "method": "\u63d0\u51faNeUQI\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u5747\u5300\u91cf\u5316\u7684\u521d\u59cb\u53c2\u6570\u4f18\u5316\uff0c\u53ef\u65e0\u7f1d\u7ed3\u5408\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u84b8\u998f\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728LLaMA\u548cQwen\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNeUQI\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7ed3\u5408\u84b8\u998f\u7b56\u7565\u540e\u6027\u80fd\u751a\u81f3\u8d85\u8fc7\u8d44\u6e90\u5bc6\u96c6\u578b\u7684PV-tuning\u3002", "conclusion": "NeUQI\u4e3aLLM\u7684\u91cf\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.17153", "pdf": "https://arxiv.org/pdf/2505.17153", "abs": "https://arxiv.org/abs/2505.17153", "authors": ["Yao Xu", "Mingyu Xu", "Fangyu Lei", "Wangtao Sun", "Xiangrong Zeng", "Bingning Wang", "Guang Liu", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated\nremarkable performance on complex reasoning tasks through Long Chain-of-Thought\n(Long-CoT) reasoning. Although distilling this capability into student models\nsignificantly enhances their performance, this paper finds that fine-tuning\nLLMs with full parameters or LoRA with a low rank on long CoT data often leads\nto Cyclical Reasoning, where models repeatedly reiterate previous inference\nsteps until the maximum length limit. Further analysis reveals that smaller\ndifferences in representations between adjacent tokens correlates with a higher\ntendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes\nShift Feedforward Networks (Shift-FFN), a novel approach that edits the current\ntoken's representation with the previous one before inputting it to FFN. This\narchitecture dynamically amplifies the representation differences between\nadjacent tokens. Extensive experiments on multiple mathematical reasoning tasks\ndemonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a\nlower rate of Cyclical Reasoning across various data sizes compared to full\nfine-tuning and standard LoRA. Our data and code are available at\nhttps://anonymous.4open.science/r/Shift-FFN", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faShift-FFN\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u76f8\u90bbtoken\u7684\u8868\u5f81\u5dee\u5f02\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u51fa\u73b0\u7684\u5faa\u73af\u63a8\u7406\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u5168\u53c2\u6570\u5fae\u8c03\u548c\u6807\u51c6LoRA\u3002", "motivation": "\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cLLMs\u5fae\u8c03\u65f6\u5bb9\u6613\u9677\u5165\u5faa\u73af\u63a8\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faShift-FFN\uff0c\u5728\u8f93\u5165FFN\u524d\u7f16\u8f91\u5f53\u524dtoken\u7684\u8868\u5f81\uff0c\u52a8\u6001\u653e\u5927\u76f8\u90bbtoken\u7684\u8868\u5f81\u5dee\u5f02\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cShift-FFN\u7ed3\u5408LoRA\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u5faa\u73af\u63a8\u7406\u7387\u3002", "conclusion": "Shift-FFN\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u7684LLMs\u5fae\u8c03\u3002", "relevance": 85.0}}
{"id": "2505.17692", "pdf": "https://arxiv.org/pdf/2505.17692", "abs": "https://arxiv.org/abs/2505.17692", "authors": ["Ziteng Yang", "Jingzehua Xu", "Yanshu Li", "Zepeng Li", "Yeqiang Wang", "Xinghui Li"], "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.", "AI": {"tldr": "ViP\u00b2-CLIP\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u611f\u77e5\u63d0\u793a\u673a\u5236\uff0c\u901a\u8fc7\u878d\u5408\u5168\u5c40\u548c\u591a\u5c3a\u5ea6\u5c40\u90e8\u89c6\u89c9\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u751f\u6210\u7ec6\u7c92\u5ea6\u6587\u672c\u63d0\u793a\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u624b\u5de5\u6a21\u677f\u548c\u7c7b\u522b\u540d\u79f0\u4f9d\u8d56\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6216\u9759\u6001\u53ef\u5b66\u4e60\u63d0\u793a\uff0c\u6210\u672c\u9ad8\u4e14\u8bed\u4e49\u8986\u76d6\u6709\u9650\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u53d8\u5316\u3002CLIP\u5bf9\u7c7b\u522b\u540d\u79f0\u7684\u654f\u611f\u6027\u4e5f\u9650\u5236\u4e86\u63d0\u793a\u7b56\u7565\u3002", "method": "ViP\u00b2-CLIP\u5f15\u5165\u89c6\u89c9\u611f\u77e5\u63d0\u793a\u673a\u5236\uff08ViP-Prompt\uff09\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u591a\u5c3a\u5ea6\u5c40\u90e8\u89c6\u89c9\u4e0a\u4e0b\u6587\u751f\u6210\u81ea\u9002\u5e94\u6587\u672c\u63d0\u793a\uff0c\u65e0\u9700\u624b\u5de5\u6a21\u677f\u6216\u7c7b\u522b\u540d\u79f0\u5148\u9a8c\u3002", "result": "\u572815\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViP\u00b2-CLIP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ViP\u00b2-CLIP\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u7c7b\u522b\u6807\u7b7e\u6a21\u7cca\u6216\u9690\u79c1\u53d7\u9650\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "relevance": 40.0}}
{"id": "2505.17599", "pdf": "https://arxiv.org/pdf/2505.17599", "abs": "https://arxiv.org/abs/2505.17599", "authors": ["Yusheng Zhao", "Qixin Zhang", "Xiao Luo", "Weizhi Zhang", "Zhiping Xiao", "Wei Ju", "Philip S. Yu", "Ming Zhang"], "title": "Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been used in many zero-shot learning\nproblems, with their strong generalization ability. Recently, adopting LLMs in\ntext-attributed graphs (TAGs) has drawn increasing attention. However, the\nadoption of LLMs faces two major challenges: limited information on graph\nstructure and unreliable responses. LLMs struggle with text attributes isolated\nfrom the graph topology. Worse still, they yield unreliable predictions due to\nboth information insufficiency and the inherent weakness of LLMs (e.g.,\nhallucination). Towards this end, this paper proposes a novel method named\nDynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of\ntexts to obtain bundle-level labels and uses these labels to supervise graph\nneural networks. Specifically, we sample a set of bundles, each containing a\nset of nodes with corresponding texts of close proximity. We then query LLMs\nwith the bundled texts to obtain the label of each bundle. Subsequently, the\nbundle labels are used to supervise the optimization of graph neural networks,\nand the bundles are further refined to exclude noisy items. To justify our\ndesign, we also provide theoretical analysis of the proposed method. Extensive\nexperiments across ten datasets validate the effectiveness of the proposed\nmethod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDENSE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6587\u672c\u6346\u7ed1\u76d1\u7763\uff0c\u5229\u7528LLMs\u751f\u6210\u6346\u7ed1\u7ea7\u6807\u7b7e\u6765\u4f18\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u56fe\u7ed3\u6784\u4fe1\u606f\u4e0d\u8db3\u548c\u54cd\u5e94\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u6587\u672c\u5c5e\u6027\u56fe\uff08TAGs\uff09\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u56fe\u7ed3\u6784\u4fe1\u606f\u6709\u9650\u548c\u54cd\u5e94\u4e0d\u53ef\u9760\u3002DENSE\u65e8\u5728\u901a\u8fc7\u6346\u7ed1\u6587\u672c\u67e5\u8be2LLMs\uff0c\u751f\u6210\u76d1\u7763\u4fe1\u53f7\u4ee5\u4f18\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "method": "DENSE\u901a\u8fc7\u91c7\u6837\u6587\u672c\u6346\u7ed1\uff0c\u67e5\u8be2LLMs\u83b7\u53d6\u6346\u7ed1\u7ea7\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6807\u7b7e\u76d1\u7763\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\uff0c\u540c\u65f6\u6392\u9664\u566a\u58f0\u9879\u3002", "result": "\u5728\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DENSE\u7684\u6709\u6548\u6027\u3002", "conclusion": "DENSE\u901a\u8fc7\u52a8\u6001\u6587\u672c\u6346\u7ed1\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u6587\u672c\u5c5e\u6027\u56fe\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "relevance": 75.0}}
{"id": "2505.17156", "pdf": "https://arxiv.org/pdf/2505.17156", "abs": "https://arxiv.org/abs/2505.17156", "authors": ["Muhammed Rizwan", "Lars Carlsson", "Mohammad Loni"], "title": "PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The introduction of Large Language Models (LLMs) has significantly\ntransformed Natural Language Processing (NLP) applications by enabling more\nadvanced analysis of customer personas. At Volvo Construction Equipment (VCE),\ncustomer personas have traditionally been developed through qualitative\nmethods, which are time-consuming and lack scalability. The main objective of\nthis paper is to generate synthetic customer personas and integrate them into a\nRetrieval-Augmented Generation (RAG) chatbot to support decision-making in\nbusiness processes. To this end, we first focus on developing a persona-based\nRAG chatbot integrated with verified personas. Next, synthetic personas are\ngenerated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and\nevaluated based on completeness, relevance, and consistency using McNemar's\ntest. In the final step, the chatbot's knowledge base is augmented with\nsynthetic personas and additional segment information to assess improvements in\nresponse accuracy and practical utility. Key findings indicate that Few-Shot\nprompting outperformed CoT in generating more complete personas, while CoT\ndemonstrated greater efficiency in terms of response time and token usage.\nAfter augmenting the knowledge base, the average accuracy rating of the chatbot\nincreased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants\nfound the updated system useful in business contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RAG\u804a\u5929\u673a\u5668\u4eba\u548c\u5408\u6210\u5ba2\u6237\u753b\u50cf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Few-Shot\u548cCoT\u63d0\u793a\u6280\u672f\u751f\u6210\u753b\u50cf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u4e1a\u52a1\u51b3\u7b56\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u5ba2\u6237\u753b\u50cf\u65b9\u6cd5\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\uff0cLLMs\u4e3a\u751f\u6210\u5408\u6210\u753b\u50cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u753b\u50cf\u7684RAG\u804a\u5929\u673a\u5668\u4eba\uff0c\u4f7f\u7528Few-Shot\u548cCoT\u751f\u6210\u5408\u6210\u753b\u50cf\uff0c\u5e76\u901a\u8fc7McNemar\u6d4b\u8bd5\u8bc4\u4f30\u5176\u8d28\u91cf\u3002", "result": "Few-Shot\u5728\u5b8c\u6574\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0cCoT\u5728\u6548\u7387\u548c\u8d44\u6e90\u6d88\u8017\u4e0a\u66f4\u4f18\uff1b\u77e5\u8bc6\u5e93\u589e\u5f3a\u540e\u804a\u5929\u673a\u5668\u4eba\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5408\u6210\u753b\u50cf\u4e0eRAG\u7ed3\u5408\u53ef\u6709\u6548\u652f\u6301\u4e1a\u52a1\u51b3\u7b56\uff0cFew-Shot\u548cCoT\u5404\u6709\u4f18\u52bf\u3002", "relevance": 70.0}}
{"id": "2505.17702", "pdf": "https://arxiv.org/pdf/2505.17702", "abs": "https://arxiv.org/abs/2505.17702", "authors": ["Xueyang Li", "Jiahao Li", "Yu Song", "Yunzhong Lou", "Xiangdong Zhou"], "title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advent of Computer-Aided Design (CAD) generative modeling will\nsignificantly transform the design of industrial products. The recent research\nendeavor has extended into the realm of Large Language Models (LLMs). In\ncontrast to fine-tuning methods, training-free approaches typically utilize the\nadvanced closed-source LLMs, thereby offering enhanced flexibility and\nefficiency in the development of AI agents for generating CAD parametric\nmodels. However, the substantial cost and limitations of local deployment of\nthe top-tier closed-source LLMs pose challenges in practical applications. The\nSeek-CAD is the pioneer exploration of locally deployed open-source inference\nLLM DeepSeek-R1 for CAD parametric model generation with a training-free\nmethodology. This study is the first investigation to incorporate both visual\nand Chain-of-Thought (CoT) feedback within the self-refinement mechanism for\ngenerating CAD models. Specifically, the initial generated parametric CAD model\nis rendered into a sequence of step-wise perspective images, which are\nsubsequently processed by a Vision Language Model (VLM) alongside the\ncorresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.\nThen, the feedback is utilized by DeepSeek-R1 to refine the initial generated\nmodel for the next round of generation. Moreover, we present an innovative 3D\nCAD model dataset structured around the SSR (Sketch, Sketch-based feature, and\nRefinements) triple design paradigm. This dataset encompasses a wide range of\nCAD commands, thereby aligning effectively with industrial application\nrequirements and proving suitable for the generation of LLMs. Extensive\nexperiments validate the effectiveness of Seek-CAD under various metrics.", "AI": {"tldr": "Seek-CAD\u63a2\u7d22\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u672c\u5730\u90e8\u7f72\u5f00\u6e90LLM\uff08DeepSeek-R1\uff09\u7528\u4e8eCAD\u53c2\u6570\u5316\u6a21\u578b\u751f\u6210\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u601d\u7ef4\u94fe\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u76843D CAD\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u95ed\u6e90LLM\u5728CAD\u751f\u6210\u4e2d\u6210\u672c\u9ad8\u548c\u672c\u5730\u90e8\u7f72\u96be\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5f00\u6e90LLM\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528DeepSeek-R1\u751f\u6210\u521d\u59cbCAD\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u601d\u7ef4\u94fe\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u4f18\u5316\uff0c\u5e76\u57fa\u4e8eSSR\u8bbe\u8ba1\u8303\u5f0f\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Seek-CAD\u5728\u591a\u79cd\u6307\u6807\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "Seek-CAD\u4e3aCAD\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5f00\u6e90LLM\u7684\u6f5c\u529b\u3002", "relevance": 70.0}}
{"id": "2505.17604", "pdf": "https://arxiv.org/pdf/2505.17604", "abs": "https://arxiv.org/abs/2505.17604", "authors": ["Alessio Devoto", "Jary Pomponi", "Mattia Merluzzi", "Paolo Di Lorenzo", "Simone Scardapane"], "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "This paper presents an adaptive framework for edge inference based on a\ndynamically configurable transformer-powered deep joint source channel coding\n(DJSCC) architecture. Motivated by a practical scenario where a resource\nconstrained edge device engages in goal oriented semantic communication, such\nas selectively transmitting essential features for object detection to an edge\nserver, our approach enables efficient task aware data transmission under\nvarying bandwidth and channel conditions. To achieve this, input data is\ntokenized into compact high level semantic representations, refined by a\ntransformer, and transmitted over noisy wireless channels. As part of the DJSCC\npipeline, we employ a semantic token selection mechanism that adaptively\ncompresses informative features into a user specified number of tokens per\nsample. These tokens are then further compressed through the JSCC module,\nenabling a flexible token communication strategy that adjusts both the number\nof transmitted tokens and their embedding dimensions. We incorporate a resource\nallocation algorithm based on Lyapunov stochastic optimization to enhance\nrobustness under dynamic network conditions, effectively balancing compression\nefficiency and task performance. Experimental results demonstrate that our\nsystem consistently outperforms existing baselines, highlighting its potential\nas a strong foundation for AI native semantic communication in edge\nintelligence applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u53ef\u914d\u7f6eTransformer\u7684\u6df1\u5ea6\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\uff08DJSCC\uff09\u67b6\u6784\u7684\u81ea\u9002\u5e94\u8fb9\u7f18\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u4efb\u52a1\u611f\u77e5\u6570\u636e\u4f20\u8f93\u3002", "motivation": "\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u5728\u52a8\u6001\u5e26\u5bbd\u548c\u4fe1\u9053\u6761\u4ef6\u4e0b\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u8bed\u4e49\u901a\u4fe1\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7Transformer\u5c06\u8f93\u5165\u6570\u636e\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u91c7\u7528\u8bed\u4e49\u4ee4\u724c\u9009\u62e9\u673a\u5236\u548cJSCC\u6a21\u5757\u8fdb\u884c\u81ea\u9002\u5e94\u538b\u7f29\u3002\u7ed3\u5408Lyapunov\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u539f\u751f\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u5f3a\u5927\u57fa\u7840\u3002", "relevance": 60.0}}
{"id": "2505.17160", "pdf": "https://arxiv.org/pdf/2505.17160", "abs": "https://arxiv.org/abs/2505.17160", "authors": ["Bang Trinh Tran To", "Thai Le"], "title": "Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "This work presents LURK (Latent UnleaRned Knowledge), a novel framework that\nprobes for hidden retained knowledge in unlearned LLMs through adversarial\nsuffix prompting. LURK automatically generates adversarial prompt suffixes\ndesigned to elicit residual knowledge about the Harry Potter domain, a commonly\nused benchmark for unlearning. Our experiments reveal that even models deemed\nsuccessfully unlearned can leak idiosyncratic information under targeted\nadversarial conditions, highlighting critical limitations of current unlearning\nevaluation standards. By uncovering latent knowledge through indirect probing,\nLURK offers a more rigorous and diagnostic tool for assessing the robustness of\nunlearning algorithms. All code will be publicly available.", "AI": {"tldr": "LURK\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u6027\u540e\u7f00\u63d0\u793a\u63a2\u6d4b\u672a\u5b66\u4e60LLM\u4e2d\u9690\u85cf\u7684\u4fdd\u7559\u77e5\u8bc6\uff0c\u63ed\u793a\u5f53\u524d\u672a\u5b66\u4e60\u8bc4\u4f30\u6807\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u672a\u5b66\u4e60LLM\u4e2d\u53ef\u80fd\u6b8b\u7559\u7684\u9690\u85cf\u77e5\u8bc6\uff0c\u8bc4\u4f30\u672a\u5b66\u4e60\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u6027\u540e\u7f00\u63d0\u793a\u81ea\u52a8\u751f\u6210\u9488\u5bf9\u54c8\u5229\u6ce2\u7279\u9886\u57df\u7684\u63d0\u793a\uff0c\u63a2\u6d4b\u6a21\u578b\u4e2d\u7684\u6b8b\u7559\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u88ab\u8ba4\u4e3a\u6210\u529f\u672a\u5b66\u4e60\u7684\u6a21\u578b\u5728\u9488\u5bf9\u6027\u5bf9\u6297\u6761\u4ef6\u4e0b\u4ecd\u53ef\u80fd\u6cc4\u9732\u4fe1\u606f\u3002", "conclusion": "LURK\u4e3a\u8bc4\u4f30\u672a\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6807\u51c6\u7684\u4e0d\u8db3\u3002", "relevance": 85.0}}
{"id": "2505.17721", "pdf": "https://arxiv.org/pdf/2505.17721", "abs": "https://arxiv.org/abs/2505.17721", "authors": ["Dekai Zhu", "Yan Di", "Stefan Gavranovic", "Slobodan Ilic"], "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing.", "AI": {"tldr": "SeaLion\u662f\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5e26\u6709\u7ec6\u7c92\u5ea6\u5206\u5272\u6807\u7b7e\u7684\u9ad8\u8d28\u91cf\u70b9\u4e91\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807p-CD\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u70b9\u4e91\u751f\u6210\u4e2d\u672a\u5145\u5206\u5173\u6ce8\u5e26\u6709\u5206\u5272\u6807\u7b7e\u7684\u70b9\u4e91\u751f\u6210\u53ca\u8bc4\u4f30\uff0c\u56e0\u6b64\u63d0\u51faSeaLion\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u6f5c\u5728\u70b9\u6269\u6563\u6280\u672f\uff0c\u8054\u5408\u9884\u6d4b\u566a\u58f0\u548c\u5206\u5272\u6807\u7b7e\uff0c\u5e76\u89e3\u7801\u4e3a\u70b9\u4e91\uff1b\u63d0\u51fap-CD\u8bc4\u4f30\u5c40\u90e8\u7ed3\u6784\u548c\u90e8\u5206\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728ShapeNet\u548cIntrA\u6570\u636e\u96c6\u4e0a\uff0cSeaLion\u5728\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u4e0a\u4f18\u4e8eDiffFacto\uff0c\u63d0\u534713.33%\u548c6.52%\u3002", "conclusion": "SeaLion\u5728\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u652f\u6301\u534a\u76d1\u7763\u8bad\u7ec3\uff0c\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u3002", "relevance": 40.0}}
{"id": "2505.17610", "pdf": "https://arxiv.org/pdf/2505.17610", "abs": "https://arxiv.org/abs/2505.17610", "authors": ["Till Freihaut", "Luca Viano", "Volkan Cevher", "Matthieu Geist", "Giorgia Ramponi"], "title": "Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning", "categories": ["cs.LG"], "comment": null, "summary": "This paper provides the first expert sample complexity characterization for\nlearning a Nash equilibrium from expert data in Markov Games. We show that a\nnew quantity named the single policy deviation concentrability coefficient is\nunavoidable in the non-interactive imitation learning setting, and we provide\nan upper bound for behavioral cloning (BC) featuring such coefficient. BC\nexhibits substantial regret in games with high concentrability coefficient,\nleading us to utilize expert queries to develop and introduce two novel\nsolution algorithms: MAIL-BRO and MURMAIL. The former employs a best response\noracle and learns an $\\varepsilon$-Nash equilibrium with\n$\\mathcal{O}(\\varepsilon^{-4})$ expert and oracle queries. The latter bypasses\ncompletely the best response oracle at the cost of a worse expert query\ncomplexity of order $\\mathcal{O}(\\varepsilon^{-8})$. Finally, we provide\nnumerical evidence, confirming our theoretical findings.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.17167", "pdf": "https://arxiv.org/pdf/2505.17167", "abs": "https://arxiv.org/abs/2505.17167", "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Bernhard Kainz", "Bjoern Menze"], "title": "CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating long-context radiology report generation is challenging. NLG\nmetrics fail to capture clinical correctness, while LLM-based metrics often\nlack generalizability. Clinical accuracy metrics are more relevant but are\nsensitive to class imbalance, frequently favoring trivial predictions. We\npropose the CRG Score, a distribution-aware and adaptable metric that evaluates\nonly clinically relevant abnormalities explicitly described in reference\nreports. CRG supports both binary and structured labels (e.g., type, location)\nand can be paired with any LLM for feature extraction. By balancing penalties\nbased on label distribution, it enables fairer, more robust evaluation and\nserves as a clinically aligned reward function.", "AI": {"tldr": "\u63d0\u51faCRG Score\uff0c\u4e00\u79cd\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u5206\u5e03\u611f\u77e5\u548c\u9002\u5e94\u6027\u6307\u6807\uff0c\u4e13\u6ce8\u4e8e\u4e34\u5e8a\u76f8\u5173\u5f02\u5e38\uff0c\u652f\u6301\u4e8c\u5143\u548c\u7ed3\u6784\u5316\u6807\u7b7e\uff0c\u5e76\u4e0eLLM\u7ed3\u5408\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709NLG\u6307\u6807\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u6b63\u786e\u6027\uff0cLLM\u6307\u6807\u7f3a\u4e4f\u666e\u9002\u6027\uff0c\u4e34\u5e8a\u51c6\u786e\u6027\u6307\u6807\u6613\u53d7\u7c7b\u522b\u4e0d\u5e73\u8861\u5f71\u54cd\u3002", "method": "CRG Score\u901a\u8fc7\u57fa\u4e8e\u6807\u7b7e\u5206\u5e03\u7684\u60e9\u7f5a\u5e73\u8861\uff0c\u8bc4\u4f30\u4e34\u5e8a\u76f8\u5173\u5f02\u5e38\uff0c\u652f\u6301\u591a\u79cd\u6807\u7b7e\u7c7b\u578b\uff0c\u53ef\u4e0eLLM\u7ed3\u5408\u3002", "result": "CRG Score\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u3001\u7a33\u5065\u7684\u8bc4\u4f30\uff0c\u5e76\u53ef\u4f5c\u4e3a\u4e34\u5e8a\u5bf9\u9f50\u7684\u5956\u52b1\u51fd\u6570\u3002", "conclusion": "CRG Score\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17726", "pdf": "https://arxiv.org/pdf/2505.17726", "abs": "https://arxiv.org/abs/2505.17726", "authors": ["Donghwan Chi", "Hyomin Kim", "Yoonjin Oh", "Yongjin Kim", "Donghoon Lee", "Daejin Jo", "Jongmin Kim", "Junyeob Baek", "Sungjin Ahn", "Sungwoong Kim"], "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have emerged as a key\napproach in achieving artificial general intelligence. In particular,\nvision-language MLLMs have been developed to generate not only text but also\nvisual outputs from multimodal inputs. This advancement requires efficient\nimage tokens that LLMs can process effectively both in input and output.\nHowever, existing image tokenization methods for MLLMs typically capture only\nglobal abstract concepts or uniformly segmented image patches, restricting\nMLLMs' capability to effectively understand or generate detailed visual\ncontent, particularly at the object level. To address this limitation, we\npropose an object-centric visual tokenizer based on Slot Attention specifically\nfor MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and\nresidual vector quantization, our proposed discretized slot tokens can encode\nlocal visual details while maintaining high-level semantics, and also align\nwith textual data to be integrated seamlessly within a unified next-token\nprediction framework of LLMs. The resulting Slot-MLLM demonstrates significant\nperformance improvements over baselines with previous visual tokenizers across\nvarious vision-language tasks that entail local detailed comprehension and\ngeneration. Notably, this work is the first demonstration of the feasibility of\nobject-centric slot attention performed with MLLMs and in-the-wild natural\nimages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSlot Attention\u7684\u9762\u5411MLLMs\u7684\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c40\u90e8\u7ec6\u8282\u7406\u89e3\u548c\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLMs\u7684\u56fe\u50cf\u5206\u8bcd\u65b9\u6cd5\u4ec5\u6355\u83b7\u5168\u5c40\u62bd\u8c61\u6982\u5ff5\u6216\u5747\u5300\u5206\u5272\u7684\u56fe\u50cf\u5757\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bf9\u8c61\u7ea7\u522b\u7406\u89e3\u548c\u751f\u6210\u8be6\u7ec6\u89c6\u89c9\u5185\u5bb9\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8eQ-Former\u7f16\u7801\u5668\u3001\u6269\u6563\u89e3\u7801\u5668\u548c\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u5316\u7684slot tokens\uff0c\u65e2\u80fd\u7f16\u7801\u5c40\u90e8\u89c6\u89c9\u7ec6\u8282\uff0c\u53c8\u80fd\u4fdd\u6301\u9ad8\u7ea7\u8bed\u4e49\uff0c\u5e76\u4e0e\u6587\u672c\u6570\u636e\u5bf9\u9f50\u3002", "result": "Slot-MLLM\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u5728MLLMs\u548c\u81ea\u7136\u56fe\u50cf\u4e2d\u5b9e\u73b0\u5bf9\u8c61\u4e2d\u5fc3slot attention\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89c9\u5206\u8bcd\u5668\u4e3aMLLMs\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8c61\u7ea7\u522b\u3002", "relevance": 85.0}}
{"id": "2505.17084", "pdf": "https://arxiv.org/pdf/2505.17084", "abs": "https://arxiv.org/abs/2505.17084", "authors": ["Alexander Gutfraind", "Vicki Bier"], "title": "From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems", "categories": ["cs.CR", "cs.AI", "I.2; K.6.5"], "comment": null, "summary": "Large language models (LLMs) offer unprecedented and growing capabilities,\nbut also introduce complex safety and security challenges that resist\nconventional risk management. While conventional probabilistic risk analysis\n(PRA) requires exhaustive risk enumeration and quantification, the novelty and\ncomplexity of these systems make PRA impractical, particularly against adaptive\nadversaries. Previous research found that risk management in various fields of\nengineering such as nuclear or civil engineering is often solved by generic\n(i.e. field-agnostic) strategies such as event tree analysis or robust designs.\nHere we show how emerging risks in LLM-powered systems could be met with 100+\nof these non-probabilistic strategies to risk management, including risks from\nadaptive adversaries. The strategies are divided into five categories and are\nmapped to LLM security (and AI safety more broadly). We also present an\nLLM-powered workflow for applying these strategies and other workflows suitable\nfor solution architects. Overall, these strategies could contribute (despite\nsome limitations) to security, safety and other dimensions of responsible AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLM\u5b89\u5168\u98ce\u9669\u7684\u975e\u6982\u7387\u6027\u7ba1\u7406\u7b56\u7565\uff0c\u5305\u62ec100\u591a\u79cd\u65b9\u6cd5\uff0c\u5206\u4e3a\u4e94\u7c7b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u5e94\u7528\u5de5\u4f5c\u6d41\u3002", "motivation": "LLM\u7684\u590d\u6742\u6027\u548c\u65b0\u9896\u6027\u4f7f\u5f97\u4f20\u7edf\u7684\u6982\u7387\u98ce\u9669\u5206\u6790\uff08PRA\uff09\u4e0d\u9002\u7528\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u9002\u5e94\u6027\u5bf9\u624b\u65f6\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u901a\u7528\u7b56\u7565\u5982\u4f55\u5e94\u7528\u4e8eLLM\u5b89\u5168\u548cAI\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6574\u7406100\u591a\u79cd\u975e\u6982\u7387\u6027\u98ce\u9669\u7ba1\u7406\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230LLM\u5b89\u5168\u548cAI\u5b89\u5168\u9886\u57df\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u5e94\u7528\u5de5\u4f5c\u6d41\u3002", "result": "\u8fd9\u4e9b\u7b56\u7565\uff08\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff09\u53ef\u4ee5\u63d0\u5347LLM\u7684\u5b89\u5168\u6027\u3001\u5b89\u5168\u6027\u53ca\u5176\u4ed6\u8d1f\u8d23\u4efbAI\u7684\u7ef4\u5ea6\u3002", "conclusion": "\u8bba\u6587\u4e3a\u975e\u6982\u7387\u6027\u98ce\u9669\u7ba1\u7406\u7b56\u7565\u5728LLM\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.17615", "pdf": "https://arxiv.org/pdf/2505.17615", "abs": "https://arxiv.org/abs/2505.17615", "authors": ["Haoxin Li", "Jingtao Ding", "Jiahui Gong", "Yong Li"], "title": "Large language model as user daily behavior data generator: balancing population diversity and individual personality", "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": "14 pages, 7 figures, 4 tables", "summary": "Predicting human daily behavior is challenging due to the complexity of\nroutine patterns and short-term fluctuations. While data-driven models have\nimproved behavior prediction by leveraging empirical data from various\nplatforms and devices, the reliance on sensitive, large-scale user data raises\nprivacy concerns and limits data availability. Synthetic data generation has\nemerged as a promising solution, though existing methods are often limited to\nspecific applications. In this work, we introduce BehaviorGen, a framework that\nuses large language models (LLMs) to generate high-quality synthetic behavior\ndata. By simulating user behavior based on profiles and real events,\nBehaviorGen supports data augmentation and replacement in behavior prediction\nmodels. We evaluate its performance in scenarios such as pertaining\naugmentation, fine-tuning replacement, and fine-tuning augmentation, achieving\nsignificant improvements in human mobility and smartphone usage predictions,\nwith gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen\nto enhance user behavior modeling through flexible and privacy-preserving\nsynthetic data generation.", "AI": {"tldr": "BehaviorGen\u5229\u7528LLMs\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u884c\u4e3a\u6570\u636e\uff0c\u63d0\u5347\u884c\u4e3a\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u89e3\u51b3\u884c\u4e3a\u9884\u6d4b\u4e2d\u4f9d\u8d56\u654f\u611f\u5927\u89c4\u6a21\u7528\u6237\u6570\u636e\u7684\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5e94\u7528\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBehaviorGen\u6846\u67b6\uff0c\u5229\u7528LLMs\u57fa\u4e8e\u7528\u6237\u914d\u7f6e\u548c\u771f\u5b9e\u4e8b\u4ef6\u751f\u6210\u5408\u6210\u884c\u4e3a\u6570\u636e\uff0c\u652f\u6301\u6570\u636e\u589e\u5f3a\u548c\u66ff\u6362\u3002", "result": "\u5728\u4eba\u7c7b\u79fb\u52a8\u548c\u667a\u80fd\u624b\u673a\u4f7f\u7528\u9884\u6d4b\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe18.9%\u3002", "conclusion": "BehaviorGen\u901a\u8fc7\u7075\u6d3b\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u7684\u6f5c\u529b\u3002", "relevance": 70.0}}
{"id": "2505.17169", "pdf": "https://arxiv.org/pdf/2505.17169", "abs": "https://arxiv.org/abs/2505.17169", "authors": ["Yu-Ang Cheng", "Leyang Hu", "Hai Huang", "Randall Balestriero"], "title": "Next Token Perception Score: Analytical Assessment of your LLM Perception Skills", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive pretraining has become the de facto paradigm for learning\ngeneral-purpose representations in large language models (LLMs). However,\nlinear probe performance across downstream perception tasks shows substantial\nvariability, suggesting that features optimized for next-token prediction do\nnot consistently transfer well to downstream perception tasks. We demonstrate\nthat representations learned via autoregression capture features that may lie\noutside the subspaces most informative for perception. To quantify the\n(mis)alignment between autoregressive pretraining and downstream perception, we\nintroduce the Next Token Perception Score (NTPS)-a score derived under a linear\nsetting that measures the overlap between autoregressive and perception feature\nsubspaces. This metric can be easily computed in closed form from pretrained\nrepresentations and labeled data, and is proven to both upper- and lower-bound\nthe excess loss. Empirically, we show that NTPS correlates strongly with linear\nprobe accuracy across 12 diverse NLP datasets and eight pretrained models\nranging from 270M to 8B parameters, confirming its utility as a measure of\nalignment. Furthermore, we show that NTPS increases following low-rank\nadaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA\naligning representations to perception tasks enhances subspace overlap and thus\nimproves downstream performance. More importantly, we find that NTPS reliably\npredicts the additional accuracy gains attained by LoRA finetuning thereby\nproviding a lightweight prescreening tool for LoRA adaptation. Our results\noffer both theoretical insights and practical tools for analytically assessing\nLLM perception skills.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNTPS\u7684\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u81ea\u56de\u5f52\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u4e4b\u95f4\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4e0e\u7ebf\u6027\u63a2\u6d4b\u51c6\u786e\u6027\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u9884\u8bad\u7ec3\u5728LLMs\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u7279\u5f81\u5728\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8861\u91cf\u8fd9\u79cd\u5bf9\u9f50\u7a0b\u5ea6\u3002", "method": "\u5f15\u5165NTPS\u6307\u6807\uff0c\u901a\u8fc7\u7ebf\u6027\u8bbe\u7f6e\u6d4b\u91cf\u81ea\u56de\u5f52\u4e0e\u611f\u77e5\u7279\u5f81\u5b50\u7a7a\u95f4\u7684\u91cd\u53e0\uff0c\u5e76\u572812\u4e2aNLP\u6570\u636e\u96c6\u548c8\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u9a8c\u8bc1\u5176\u76f8\u5173\u6027\u3002", "result": "NTPS\u4e0e\u7ebf\u6027\u63a2\u6d4b\u51c6\u786e\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14LoRA\u5fae\u8c03\u540eNTPS\u589e\u52a0\uff0c\u8868\u660e\u5176\u5bf9\u611f\u77e5\u4efb\u52a1\u5bf9\u9f50\u7684\u6539\u8fdb\u3002", "conclusion": "NTPS\u4e3a\u8bc4\u4f30LLM\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u53ef\u4f5c\u4e3aLoRA\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u9884\u7b5b\u9009\u6307\u6807\u3002", "relevance": 85.0}}
{"id": "2505.17727", "pdf": "https://arxiv.org/pdf/2505.17727", "abs": "https://arxiv.org/abs/2505.17727", "authors": ["Jiawei Zhou", "Linye Lyu", "Zhuotao Tian", "Cheng Zhuo", "Yu Li"], "title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain", "categories": ["cs.CV"], "comment": null, "summary": "Safety-critical scenarios are rare yet pivotal for evaluating and enhancing\nthe robustness of autonomous driving systems. While existing methods generate\nsafety-critical driving trajectories, simulations, or single-view videos, they\nfall short of meeting the demands of advanced end-to-end autonomous systems\n(E2E AD), which require real-world, multi-view video data. To bridge this gap,\nwe introduce SafeMVDrive, the first framework designed to generate\nhigh-quality, safety-critical, multi-view driving videos grounded in real-world\ndomains. SafeMVDrive strategically integrates a safety-critical trajectory\ngenerator with an advanced multi-view video generator. To tackle the challenges\ninherent in this integration, we first enhance scene understanding ability of\nthe trajectory generator by incorporating visual context -- which is previously\nunavailable to such generator -- and leveraging a GRPO-finetuned\nvision-language model to achieve more realistic and context-aware trajectory\ngeneration. Second, recognizing that existing multi-view video generators\nstruggle to render realistic collision events, we introduce a two-stage,\ncontrollable trajectory generation mechanism that produces collision-evasion\ntrajectories, ensuring both video quality and safety-critical fidelity.\nFinally, we employ a diffusion-based multi-view video generator to synthesize\nhigh-quality safety-critical driving videos from the generated trajectories.\nExperiments conducted on an E2E AD planner demonstrate a significant increase\nin collision rate when tested with our generated data, validating the\neffectiveness of SafeMVDrive in stress-testing planning modules. Our code,\nexamples, and datasets are publicly available at:\nhttps://zhoujiawei3.github.io/SafeMVDrive/.", "AI": {"tldr": "SafeMVDrive\u6846\u67b6\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b89\u5168\u5173\u952e\u7684\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u89d2\u6570\u636e\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u7684\u9700\u6c42\uff0cSafeMVDrive\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u5b89\u5168\u5173\u952e\u8f68\u8ff9\u751f\u6210\u5668\u548c\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u5668\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0b\u6587\u589e\u5f3a\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e24\u9636\u6bb5\u53ef\u63a7\u8f68\u8ff9\u751f\u6210\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684\u89c6\u9891\u663e\u8457\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u7684\u78b0\u649e\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SafeMVDrive\u4e3a\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6a21\u5757\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5b89\u5168\u5173\u952e\u6570\u636e\u3002", "relevance": 40.0}}
{"id": "2505.17085", "pdf": "https://arxiv.org/pdf/2505.17085", "abs": "https://arxiv.org/abs/2505.17085", "authors": ["Kaibo Huang", "Zipei Zhang", "Yukun Wei", "TianXin Zhang", "Zhongliang Yang", "Linna Zhou"], "title": "GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis", "categories": ["cs.CR", "cs.AI", "cs.CL", "68P30, 68T07", "I.2.7"], "comment": null, "summary": "The ubiquity of social media platforms facilitates malicious linguistic\nsteganography, posing significant security risks. Steganalysis is profoundly\nhindered by the challenge of identifying subtle cognitive inconsistencies\narising from textual fragmentation and complex dialogue structures, and the\ndifficulty in achieving robust aggregation of multi-dimensional weak signals,\nespecially given extreme steganographic sparsity and sophisticated\nsteganography. These core detection difficulties are compounded by significant\ndata imbalance. This paper introduces GSDFuse, a novel method designed to\nsystematically overcome these obstacles. GSDFuse employs a holistic approach,\nsynergistically integrating hierarchical multi-modal feature engineering to\ncapture diverse signals, strategic data augmentation to address sparsity,\nadaptive evidence fusion to intelligently aggregate weak signals, and\ndiscriminative embedding learning to enhance sensitivity to subtle\ninconsistencies. Experiments on social media datasets demonstrate GSDFuse's\nstate-of-the-art (SOTA) performance in identifying sophisticated steganography\nwithin complex dialogue environments. The source code for GSDFuse is available\nat https://github.com/NebulaEmmaZh/GSDFuse.", "AI": {"tldr": "GSDFuse\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u6076\u610f\u8bed\u8a00\u9690\u5199\u672f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u5de5\u7a0b\u3001\u6570\u636e\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u8bc1\u636e\u878d\u5408\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u6076\u610f\u8bed\u8a00\u9690\u5199\u672f\u7684\u666e\u904d\u5b58\u5728\u5e26\u6765\u4e86\u91cd\u5927\u5b89\u5168\u98ce\u9669\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u590d\u6742\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u9690\u5199\u672f\u65f6\u9762\u4e34\u6570\u636e\u7a00\u758f\u6027\u548c\u4fe1\u53f7\u5fae\u5f31\u7b49\u6311\u6218\u3002", "method": "GSDFuse\u91c7\u7528\u5206\u5c42\u591a\u6a21\u6001\u7279\u5f81\u5de5\u7a0b\u3001\u7b56\u7565\u6027\u6570\u636e\u589e\u5f3a\u3001\u81ea\u9002\u5e94\u8bc1\u636e\u878d\u5408\u548c\u5224\u522b\u6027\u5d4c\u5165\u5b66\u4e60\uff0c\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u68c0\u6d4b\u96be\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGSDFuse\u5728\u590d\u6742\u5bf9\u8bdd\u73af\u5883\u4e2d\u8bc6\u522b\u9ad8\u7ea7\u9690\u5199\u672f\u7684\u6027\u80fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "GSDFuse\u901a\u8fc7\u7efc\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9690\u5199\u672f\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "relevance": 40.0}}
{"id": "2505.17621", "pdf": "https://arxiv.org/pdf/2505.17621", "abs": "https://arxiv.org/abs/2505.17621", "authors": ["Jingtong Gao", "Ling Pan", "Yejing Wang", "Rui Zhong", "Chi Lu", "Qingpeng Cai", "Peng Jiang", "Xiangyu Zhao"], "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a pivotal method for improving the\nreasoning capabilities of Large Language Models (LLMs). However, prevalent RL\napproaches such as Proximal Policy Optimization (PPO) and Group-Regularized\nPolicy Optimization (GRPO) face critical limitations due to their reliance on\nsparse outcome-based rewards and inadequate mechanisms for incentivizing\nexploration. These limitations result in inefficient guidance for multi-step\nreasoning processes. Specifically, sparse reward signals fail to deliver\neffective or sufficient feedback, particularly for challenging problems.\nFurthermore, such reward structures induce systematic biases that prioritize\nexploitation of familiar trajectories over novel solution discovery. These\nshortcomings critically hinder performance in complex reasoning tasks, which\ninherently demand iterative refinement across ipntermediate steps. To address\nthese challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd\nfoR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense\nrewards and amplify explorations in the RL-based training paradigm. i-MENTOR\nintroduces three key innovations: trajectory-aware exploration rewards that\nmitigate bias in token-level strategies while maintaining computational\nefficiency; dynamic reward scaling to stabilize exploration and exploitation in\nlarge action spaces; and advantage-preserving reward implementation that\nmaintains advantage distribution integrity while incorporating exploratory\nguidance. Experiments across three public datasets demonstrate i-MENTOR's\neffectiveness with a 22.39% improvement on the difficult dataset Countdown-4.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ai-MENTOR\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u548c\u589e\u5f3a\u63a2\u7d22\u6765\u6539\u8fdb\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u63a8\u7406\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\u4f9d\u8d56\u7a00\u758f\u5956\u52b1\uff0c\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u591a\u6b65\u63a8\u7406\uff0c\u4e14\u6613\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u963b\u788d\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "i-MENTOR\u5f15\u5165\u8f68\u8ff9\u611f\u77e5\u63a2\u7d22\u5956\u52b1\u3001\u52a8\u6001\u5956\u52b1\u7f29\u653e\u548c\u4f18\u52bf\u4fdd\u7559\u5956\u52b1\u5b9e\u73b0\uff0c\u4ee5\u5bc6\u96c6\u5956\u52b1\u548c\u589e\u5f3a\u63a2\u7d22\u4f18\u5316RL\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0ci-MENTOR\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Countdown-4\u6570\u636e\u96c6\u4e0a\u63d0\u534722.39%\u3002", "conclusion": "i-MENTOR\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u548c\u63a2\u7d22\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "relevance": 90.0}}
{"id": "2505.17206", "pdf": "https://arxiv.org/pdf/2505.17206", "abs": "https://arxiv.org/abs/2505.17206", "authors": ["Kushal Chawla", "Alfy Samuel", "Anoop Kumar", "Daben Liu"], "title": "FB-RAG: Improving RAG with Forward and Backward Lookup", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of Retrieval Augmented Generation (RAG) systems relies\nheavily on the retriever quality and the size of the retrieved context. A large\nenough context ensures that the relevant information is present in the input\ncontext for the LLM, but also incorporates irrelevant content that has been\nshown to confuse the models. On the other hand, a smaller context reduces the\nirrelevant information, but it often comes at the risk of losing important\ninformation necessary to answer the input question. This duality is especially\nchallenging to manage for complex queries that contain little information to\nretrieve the relevant chunks from the full context. To address this, we present\na novel framework, called FB-RAG, which enhances the RAG pipeline by relying on\na combination of backward lookup (overlap with the query) and forward lookup\n(overlap with candidate reasons and answers) to retrieve specific context\nchunks that are the most relevant for answering the input query. Our\nevaluations on 9 datasets from two leading benchmarks show that FB-RAG\nconsistently outperforms RAG and Long Context baselines developed recently for\nthese benchmarks. We further show that FB-RAG can improve performance while\nreducing latency. We perform qualitative analysis of the strengths and\nshortcomings of our approach, providing specific insights to guide future work.", "AI": {"tldr": "FB-RAG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53cd\u5411\u548c\u6b63\u5411\u67e5\u627e\u4f18\u5316RAG\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u68c0\u7d22\u4e0a\u4e0b\u6587\u5927\u5c0f\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5c24\u5176\u662f\u590d\u6742\u67e5\u8be2\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faFB-RAG\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u5411\u67e5\u627e\uff08\u4e0e\u67e5\u8be2\u91cd\u53e0\uff09\u548c\u6b63\u5411\u67e5\u627e\uff08\u4e0e\u5019\u9009\u7b54\u6848\u91cd\u53e0\uff09\u68c0\u7d22\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u5757\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cFB-RAG\u4f18\u4e8eRAG\u548c\u957f\u4e0a\u4e0b\u6587\u57fa\u7ebf\uff0c\u540c\u65f6\u964d\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "FB-RAG\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u5c40\u9650\u6027\u3002", "relevance": 75.0}}
{"id": "2505.17732", "pdf": "https://arxiv.org/pdf/2505.17732", "abs": "https://arxiv.org/abs/2505.17732", "authors": ["Ozsel Kilinc", "Cem Tarhan"], "title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate, fast, and reliable 3D perception is essential for autonomous\ndriving. Recently, bird's-eye view (BEV)-based perception approaches have\nemerged as superior alternatives to perspective-based solutions, offering\nenhanced spatial understanding and more natural outputs for planning. Existing\nBEV-based 3D object detection methods, typically adhering to angle-based\nrepresentation, directly estimate the size and orientation of rotated bounding\nboxes. We observe that BEV-based 3D object detection is analogous to aerial\noriented object detection, where angle-based methods are recognized for being\naffected by discontinuities in their loss functions. Drawing inspiration from\nthis domain, we propose Restricted Quadrilateral Representation to define 3D\nregression targets. RQR3D regresses the smallest horizontal bounding box\nencapsulating the oriented box, along with the offsets between the corners of\nthese two boxes, thereby transforming the oriented object detection problem\ninto a keypoint regression task. RQR3D is compatible with any 3D object\ndetection approach. We employ RQR3D within an anchor-free single-stage object\ndetection method and introduce an objectness head to address class imbalance\nproblem. Furthermore, we introduce a simplified radar fusion backbone that\neliminates the need for voxel grouping and processes the BEV-mapped point cloud\nwith standard 2D convolutions, rather than sparse convolutions. Extensive\nevaluations on the nuScenes dataset demonstrate that RQR3D achieves\nstate-of-the-art performance in camera-radar 3D object detection, outperforming\nthe previous best method by +4% in NDS and +2.4% in mAP, and significantly\nreducing the translation and orientation errors, which are crucial for safe\nautonomous driving. These consistent gains highlight the robustness, precision,\nand real-world readiness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRQR3D\u76843D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u56db\u8fb9\u5f62\u8868\u793a\u5c06\u5b9a\u5411\u68c0\u6d4b\u95ee\u9898\u8f6c\u5316\u4e3a\u5173\u952e\u70b9\u56de\u5f52\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "BEV-based 3D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u5728\u89d2\u5ea6\u8868\u793a\u4e0a\u5b58\u5728\u635f\u5931\u51fd\u6570\u4e0d\u8fde\u7eed\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u6027\u80fd\u3002\u8bba\u6587\u4ece\u822a\u7a7a\u5b9a\u5411\u68c0\u6d4b\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u63d0\u51faRQR3D\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RQR3D\u56de\u5f52\u5305\u542b\u5b9a\u5411\u6846\u7684\u6700\u5c0f\u6c34\u5e73\u8fb9\u754c\u6846\u53ca\u5176\u504f\u79fb\u91cf\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u5173\u952e\u70b9\u56de\u5f52\u3002\u65b9\u6cd5\u517c\u5bb9\u4efb\u4f553D\u68c0\u6d4b\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u76ee\u6807\u6027\u5934\u90e8\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cRQR3D\u5728\u76f8\u673a-\u96f7\u8fbe3D\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f18\uff0cNDS\u63d0\u53474%\uff0cmAP\u63d0\u53472.4%\uff0c\u663e\u8457\u51cf\u5c11\u5e73\u79fb\u548c\u65b9\u5411\u8bef\u5dee\u3002", "conclusion": "RQR3D\u5c55\u793a\u4e86\u7a33\u5065\u6027\u3001\u7cbe\u786e\u6027\u548c\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u3002", "relevance": 30.0}}
{"id": "2505.17626", "pdf": "https://arxiv.org/pdf/2505.17626", "abs": "https://arxiv.org/abs/2505.17626", "authors": ["Guilherme Korol", "Antonio Carlos Schneider Beck", "Jeronimo Castrillon"], "title": "Leveraging Stochastic Depth Training for Adaptive Inference", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Dynamic DNN optimization techniques such as layer-skipping offer increased\nadaptability and efficiency gains but can lead to i) a larger memory footprint\nas in decision gates, ii) increased training complexity (e.g., with\nnon-differentiable operations), and iii) less control over performance-quality\ntrade-offs due to its inherent input-dependent execution. To approach these\nissues, we propose a simpler yet effective alternative for adaptive inference\nwith a zero-overhead, single-model, and time-predictable inference. Central to\nour approach is the observation that models trained with Stochastic Depth -- a\nmethod for faster training of residual networks -- become more resilient to\narbitrary layer-skipping at inference time. We propose a method to first select\nnear Pareto-optimal skipping configurations from a stochastically-trained model\nto adapt the inference at runtime later. Compared to original ResNets, our\nmethod shows improvements of up to 2X in power efficiency at accuracy drops as\nlow as 0.71%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u5f00\u9500\u3001\u5355\u6a21\u578b\u7684\u52a8\u6001DNN\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7Stochastic Depth\u8bad\u7ec3\u4f7f\u6a21\u578b\u5bf9\u5c42\u8df3\u8fc7\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u80fd\u6548\u3002", "motivation": "\u52a8\u6001DNN\u4f18\u5316\u6280\u672f\uff08\u5982\u5c42\u8df3\u8fc7\uff09\u867d\u80fd\u63d0\u5347\u9002\u5e94\u6027\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u589e\u52a0\u3001\u8bad\u7ec3\u590d\u6742\u5ea6\u63d0\u9ad8\u53ca\u6027\u80fd-\u8d28\u91cf\u6743\u8861\u63a7\u5236\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528Stochastic Depth\u8bad\u7ec3\u7684\u6a21\u578b\u5bf9\u5c42\u8df3\u8fc7\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4ece\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u9009\u62e9\u8fd1Pareto\u6700\u4f18\u7684\u8df3\u8fc7\u914d\u7f6e\uff0c\u4ee5\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u3002", "result": "\u76f8\u6bd4\u539f\u59cbResNets\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.71%\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6548\u63d0\u5347\u9ad8\u8fbe2\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u52a8\u6001\u63a8\u7406\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u8d28\u91cf\u3002", "relevance": 75.0}}
{"id": "2505.17217", "pdf": "https://arxiv.org/pdf/2505.17217", "abs": "https://arxiv.org/abs/2505.17217", "authors": ["Kangda Wei", "Hasnat Md Abdullah", "Ruihong Huang"], "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u6545\u4e8b\u5bf9\u548c\u9053\u5fb7\u5224\u65ad\u6765\u51cf\u5c11LLM\u6027\u522b\u504f\u89c1\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528DPO\u4f18\u5316\u6a21\u578b\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u679c\u663e\u8457\u3002", "motivation": "\u89e3\u51b3LLM\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u4fc3\u8fdb\u516c\u5e73\u6027\u3002", "method": "\u751f\u6210\u7ed3\u6784\u76f8\u540c\u4f46\u6027\u522b\u4e0d\u540c\u7684\u6545\u4e8b\u5bf9\uff0c\u6bd4\u8f83\u9053\u5fb7\u5224\u65ad\uff0c\u901a\u8fc7DPO\u4f18\u5316\u6a21\u578b\u3002", "result": "\u663e\u8457\u51cf\u5c11\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u5b9e\u7528\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002", "relevance": 85.0}}
{"id": "2505.17768", "pdf": "https://arxiv.org/pdf/2505.17768", "abs": "https://arxiv.org/abs/2505.17768", "authors": ["Dong Zhang", "Lingfeng He", "Rui Yan", "Fei Shen", "Jinhui Tang"], "title": "R-Genie: Reasoning-Guided Generative Image Editing", "categories": ["cs.CV", "F.2.2, I.2.7", "F.2.2; I.2.7"], "comment": "https://dongzhang89.github.io/RGenie.github.io/", "summary": "While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5R-Genie\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u63a8\u7406\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u53d7\u9650\u4e8e\u663e\u5f0f\u6587\u672c\u6307\u4ee4\u548c\u6709\u9650\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u7528\u6237\u9690\u5f0f\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e30\u5bcc\u63a8\u7406\u4e0a\u4e0b\u6587\u7684\u56fe\u50cf-\u6307\u4ee4-\u7f16\u8f91\u4e09\u5143\u7ec4\u6570\u636e\u96c6\uff0c\u63d0\u51faR-Genie\u6a21\u578b\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5165\u63a8\u7406\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eR-Genie\u80fd\u591f\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u80fd\u529b\uff0c\u62d3\u5c55\u667a\u80fd\u56fe\u50cf\u5408\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "R-Genie\u901a\u8fc7\u63a8\u7406\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.17636", "pdf": "https://arxiv.org/pdf/2505.17636", "abs": "https://arxiv.org/abs/2505.17636", "authors": ["Jonathan Bennion", "Shaona Ghosh", "Mantek Singh", "Nouha Dziri"], "title": "Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6th International Conference on Advanced Natural Language Processing\n  (AdNLP 2025), May 17 ~ 18, 2025, Zurich, Switzerland", "summary": "Various AI safety datasets have been developed to measure LLMs against\nevolving interpretations of harm. Our evaluation of five recently published\nopen-source safety benchmarks reveals distinct semantic clusters using UMAP\ndimensionality reduction and kmeans clustering (silhouette score: 0.470). We\nidentify six primary harm categories with varying benchmark representation.\nGretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix\nemphasizes self-harm scenarios. Significant differences in prompt length\ndistribution suggests confounds to data collection and interpretations of harm\nas well as offer possible context. Our analysis quantifies benchmark\northogonality among AI benchmarks, allowing for transparency in coverage gaps\ndespite topical similarities. Our quantitative framework for analyzing semantic\northogonality across safety benchmarks enables more targeted development of\ndatasets that comprehensively address the evolving landscape of harms in AI\nuse, however that is defined in the future.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4e94\u4e2a\u5f00\u6e90\u5b89\u5168\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7UMAP\u964d\u7ef4\u548ckmeans\u805a\u7c7b\u8bc6\u522b\u51fa\u516d\u4e2a\u4e3b\u8981\u5371\u5bb3\u7c7b\u522b\uff0c\u5e76\u91cf\u5316\u4e86\u57fa\u51c6\u95f4\u7684\u8bed\u4e49\u6b63\u4ea4\u6027\u3002", "motivation": "\u968f\u7740AI\u5b89\u5168\u95ee\u9898\u7684\u6f14\u53d8\uff0c\u9700\u8981\u900f\u660e\u4e14\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30LLMs\u7684\u5371\u5bb3\u3002", "method": "\u4f7f\u7528UMAP\u964d\u7ef4\u548ckmeans\u805a\u7c7b\u5206\u6790\u4e94\u4e2a\u5f00\u6e90\u5b89\u5168\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u5371\u5bb3\u7c7b\u522b\u5e76\u91cf\u5316\u6b63\u4ea4\u6027\u3002", "result": "\u53d1\u73b0\u516d\u4e2a\u4e3b\u8981\u5371\u5bb3\u7c7b\u522b\uff0c\u57fa\u51c6\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5982GretelAI\u5173\u6ce8\u9690\u79c1\uff0cWildGuardMix\u5173\u6ce8\u81ea\u6b8b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b9a\u91cf\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5168\u9762\u7684\u5b89\u5168\u6570\u636e\u96c6\uff0c\u4ee5\u5e94\u5bf9AI\u5371\u5bb3\u7684\u6f14\u53d8\u3002", "relevance": 85.0}}
{"id": "2505.17222", "pdf": "https://arxiv.org/pdf/2505.17222", "abs": "https://arxiv.org/abs/2505.17222", "authors": ["Georgios Chochlakis", "Peter Wu", "Arjun Bedi", "Marcus Ma", "Kristina Lerman", "Shrikanth Narayanan"], "title": "Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts", "categories": ["cs.CL"], "comment": "17 pages, 16 figures, 9 tables", "summary": "Modeling complex subjective tasks in Natural Language Processing, such as\nrecognizing emotion and morality, is considerably challenging due to\nsignificant variation in human annotations. This variation often reflects\nreasonable differences in semantic interpretations rather than mere noise,\nnecessitating methods to distinguish between legitimate subjectivity and error.\nWe address this challenge by exploring label verification in these contexts\nusing Large Language Models (LLMs). First, we propose a simple In-Context\nLearning binary filtering baseline that estimates the reasonableness of a\ndocument-label pair. We then introduce the Label-in-a-Haystack setting: the\nquery and its label(s) are included in the demonstrations shown to LLMs, which\nare prompted to predict the label(s) again, while receiving task-specific\ninstructions (e.g., emotion recognition) rather than label copying. We show how\nthe failure to copy the label(s) to the output of the LLM are task-relevant and\ninformative. Building on this, we propose the Label-in-a-Haystack Rectification\n(LiaHR) framework for subjective label correction: when the model outputs\ndiverge from the reference gold labels, we assign the generated labels to the\nexample instead of discarding it. This approach can be integrated into\nannotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,\nhuman evaluations, and ecological validity studies verify the utility of LiaHR\nfor label correction. Code is available at https://github.com/gchochla/LiaHR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6807\u7b7e\u9a8c\u8bc1\u65b9\u6cd5\uff08LiaHR\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4e3b\u89c2\u4efb\u52a1\u4e2d\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u7b7e\u4fee\u6b63\u63d0\u5347\u4fe1\u53f7\u566a\u58f0\u6bd4\u3002", "motivation": "\u4e3b\u89c2\u4efb\u52a1\uff08\u5982\u60c5\u611f\u8bc6\u522b\uff09\u4e2d\u4eba\u7c7b\u6807\u6ce8\u5b58\u5728\u5408\u7406\u5dee\u5f02\uff0c\u9700\u533a\u5206\u4e3b\u89c2\u6027\u4e0e\u566a\u58f0\u3002", "method": "\u63d0\u51faIn-Context Learning\u57fa\u7ebf\u65b9\u6cd5\u548cLabel-in-a-Haystack\u8bbe\u7f6e\uff0c\u5229\u7528LLMs\u9884\u6d4b\u6807\u7b7e\u5e76\u4fee\u6b63\u4e0d\u4e00\u81f4\u3002", "result": "LiaHR\u6846\u67b6\u5728\u6807\u7b7e\u4fee\u6b63\u4e2d\u6709\u6548\uff0c\u63d0\u5347\u4e86\u4fe1\u53f7\u566a\u58f0\u6bd4\u3002", "conclusion": "LiaHR\u4e3a\u6807\u6ce8\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6807\u7b7e\u4fee\u6b63\u65b9\u6cd5\u3002", "relevance": 75.0}}
{"id": "2505.17771", "pdf": "https://arxiv.org/pdf/2505.17771", "abs": "https://arxiv.org/abs/2505.17771", "authors": ["Yanping Fu", "Xinyuan Liu", "Tianyu Li", "Yike Ma", "Yucheng Zhang", "Feng Dai"], "title": "TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Topology reasoning, which unifies perception and structured reasoning, plays\na vital role in understanding intersections for autonomous driving. However,\nits performance heavily relies on the accuracy of lane detection, particularly\nat connected lane endpoints. Existing methods often suffer from lane endpoints\ndeviation, leading to incorrect topology construction. To address this issue,\nwe propose TopoPoint, a novel framework that explicitly detects lane endpoints\nand jointly reasons over endpoints and lanes for robust topology reasoning.\nDuring training, we independently initialize point and lane query, and proposed\nPoint-Lane Merge Self-Attention to enhance global context sharing through\nincorporating geometric distances between points and lanes as an attention mask\n. We further design Point-Lane Graph Convolutional Network to enable mutual\nfeature aggregation between point and lane query. During inference, we\nintroduce Point-Lane Geometry Matching algorithm that computes distances\nbetween detected points and lanes to refine lane endpoints, effectively\nmitigating endpoint deviation. Extensive experiments on the OpenLane-V2\nbenchmark demonstrate that TopoPoint achieves state-of-the-art performance in\ntopology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate\nendpoint detection, under which our method significantly outperforms existing\napproaches (52.6 v.s. 45.2 on DET$_p$). The code is released at\nhttps://github.com/Franpin/TopoPoint.", "AI": {"tldr": "TopoPoint\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u63a8\u7406\u8f66\u9053\u7aef\u70b9\u548c\u8f66\u9053\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u62d3\u6251\u63a8\u7406\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8f66\u9053\u7aef\u70b9\u68c0\u6d4b\u4e0a\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u62d3\u6251\u63a8\u7406\u4e0d\u51c6\u786e\uff0cTopoPoint\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u70b9-\u8f66\u9053\u5408\u5e76\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u70b9-\u8f66\u9053\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u8bad\u7ec3\u65f6\u72ec\u7acb\u521d\u59cb\u5316\u70b9\u548c\u8f66\u9053\u67e5\u8be2\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u51e0\u4f55\u5339\u914d\u7b97\u6cd5\u4f18\u5316\u7aef\u70b9\u3002", "result": "\u5728OpenLane-V2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTopoPoint\u5728\u62d3\u6251\u63a8\u7406\uff0848.8 OLS\uff09\u548c\u7aef\u70b9\u68c0\u6d4b\uff0852.6 DET$_p$\uff09\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "TopoPoint\u901a\u8fc7\u7aef\u70b9\u548c\u8f66\u9053\u7684\u8054\u5408\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62d3\u6251\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 30.0}}
{"id": "2505.17637", "pdf": "https://arxiv.org/pdf/2505.17637", "abs": "https://arxiv.org/abs/2505.17637", "authors": ["Yuting Huang", "Ziquan Fang", "Zhihao Zeng", "Lu Chen", "Yunjun Gao"], "title": "Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach", "categories": ["cs.LG"], "comment": null, "summary": "Spatio-temporal prediction plays a crucial role in intelligent\ntransportation, weather forecasting, and urban planning. While integrating\nmulti-modal data has shown potential for enhancing prediction accuracy, key\nchallenges persist: (i) inadequate fusion of multi-modal information, (ii)\nconfounding factors that obscure causal relations, and (iii) high computational\ncomplexity of prediction models. To address these challenges, we propose\nE^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal\nPrediction framework. E^2-CSTP leverages cross-modal attention and gating\nmechanisms to effectively integrate multi-modal data. Building on this, we\ndesign a dual-branch causal inference approach: the primary branch focuses on\nspatio-temporal prediction, while the auxiliary branch mitigates bias by\nmodeling additional modalities and applying causal interventions to uncover\ntrue causal dependencies. To improve model efficiency, we integrate GCN with\nthe Mamba architecture for accelerated spatio-temporal encoding. Extensive\nexperiments on 4 real-world datasets show that E^2-CSTP significantly\noutperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in\naccuracy as well as 17.37%-56.11% reductions in computational overhead.", "AI": {"tldr": "E^2-CSTP\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u56e0\u679c\u7684\u591a\u6a21\u6001\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u673a\u5236\u6574\u5408\u6570\u636e\uff0c\u91c7\u7528\u53cc\u5206\u652f\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528GCN\u4e0eMamba\u67b6\u6784\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u4fe1\u606f\u878d\u5408\u4e0d\u8db3\u3001\u6df7\u6dc6\u56e0\u7d20\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u673a\u5236\uff0c\u8bbe\u8ba1\u53cc\u5206\u652f\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210GCN\u4e0eMamba\u67b6\u6784\u3002", "result": "\u57284\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e9\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u53479.66%\uff0c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c1117.37%-56.11%\u3002", "conclusion": "E^2-CSTP\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u3001\u5929\u6c14\u9884\u62a5\u7b49\u9886\u57df\u3002", "relevance": 40.0}}
{"id": "2505.17231", "pdf": "https://arxiv.org/pdf/2505.17231", "abs": "https://arxiv.org/abs/2505.17231", "authors": ["Jipeng Zhang", "Haolin Yang", "Kehao Miao", "Ruiyuan Zhang", "Renjie Pi", "Jiahui Gao", "Xiaofang Zhou"], "title": "ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Recent text-to-SQL models have achieved strong performance, but their\neffectiveness remains largely confined to SQLite due to dataset limitations.\nHowever, real-world applications require SQL generation across multiple\ndialects with varying syntax and specialized features, which remains a\nchallenge for current models. The main obstacle in building a dialect-aware\nmodel lies in acquiring high-quality dialect-specific data. Data generated\npurely through static prompting - without validating SQLs via execution - tends\nto be noisy and unreliable. Moreover, the lack of real execution environments\nin the training loop prevents models from grounding their predictions in\nexecutable semantics, limiting generalization despite surface-level\nimprovements from data filtering. This work introduces ExeSQL, a text-to-SQL\nframework with execution-driven, agentic bootstrapping. The method consists of\niterative query generation, execution-based filtering (e.g., rejection\nsampling), and preference-based training, enabling the model to adapt to new\nSQL dialects through verifiable, feedback-guided learning. Experiments show\nthat ExeSQL bridges the dialect gap in text-to-SQL, achieving average\nimprovements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and\nOracle, respectively, across multiple datasets of varying difficulty.", "AI": {"tldr": "ExeSQL\u662f\u4e00\u4e2a\u57fa\u4e8e\u6267\u884c\u7684\u6587\u672c\u5230SQL\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u67e5\u8be2\u751f\u6210\u3001\u6267\u884c\u8fc7\u6ee4\u548c\u504f\u597d\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591aSQL\u65b9\u8a00\u7684\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230SQL\u6a21\u578b\u5c40\u9650\u4e8eSQLite\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u652f\u6301\u591a\u65b9\u8a00SQL\u751f\u6210\uff0c\u4f46\u9ad8\u8d28\u91cf\u65b9\u8a00\u6570\u636e\u83b7\u53d6\u56f0\u96be\u4e14\u9759\u6001\u751f\u6210\u6570\u636e\u4e0d\u53ef\u9760\u3002", "method": "ExeSQL\u91c7\u7528\u6267\u884c\u9a71\u52a8\u7684\u4ee3\u7406\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5305\u62ec\u8fed\u4ee3\u67e5\u8be2\u751f\u6210\u3001\u6267\u884c\u8fc7\u6ee4\u548c\u504f\u597d\u8bad\u7ec3\uff0c\u4ee5\u9a8c\u8bc1\u548c\u4f18\u5316SQL\u751f\u6210\u3002", "result": "ExeSQL\u5728PostgreSQL\u3001MySQL\u548cOracle\u4e0a\u5206\u522b\u6bd4GPT-4o\u5e73\u5747\u63d0\u534715.2%\u300110.38%\u548c4.49%\u3002", "conclusion": "ExeSQL\u901a\u8fc7\u6267\u884c\u53cd\u9988\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65b9\u8a00SQL\u751f\u6210\u7684\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.17778", "pdf": "https://arxiv.org/pdf/2505.17778", "abs": "https://arxiv.org/abs/2505.17778", "authors": ["Yu Xie", "Jielei Zhang", "Pengyu Chen", "Ziyue Wang", "Weihang Wang", "Longwen Gao", "Peiyi Li", "Huyang Sun", "Qiang Zhang", "Qian Qiao", "Jiaqing Fan", "Zhouhui Lian"], "title": "TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based scene text synthesis has progressed rapidly, yet existing\nmethods commonly rely on additional visual conditioning modules and require\nlarge-scale annotated data to support multilingual generation. In this work, we\nrevisit the necessity of complex auxiliary modules and further explore an\napproach that simultaneously ensures glyph accuracy and achieves high-fidelity\nscene integration, by leveraging diffusion models' inherent capabilities for\ncontextual reasoning. To this end, we introduce TextFlux, a DiT-based framework\nthat enables multilingual scene text synthesis. The advantages of TextFlux can\nbe summarized as follows: (1) OCR-free model architecture. TextFlux eliminates\nthe need for OCR encoders (additional visual conditioning modules) that are\nspecifically used to extract visual text-related features. (2) Strong\nmultilingual scalability. TextFlux is effective in low-resource multilingual\nsettings, and achieves strong performance in newly added languages with fewer\nthan 1,000 samples. (3) Streamlined training setup. TextFlux is trained with\nonly 1% of the training data required by competing methods. (4) Controllable\nmulti-line text generation. TextFlux offers flexible multi-line synthesis with\nprecise line-level control, outperforming methods restricted to single-line or\nrigid layouts. Extensive experiments and visualizations demonstrate that\nTextFlux outperforms previous methods in both qualitative and quantitative\nevaluations.", "AI": {"tldr": "TextFlux\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u573a\u666f\u6587\u672c\u5408\u6210\uff0c\u65e0\u9700OCR\u7f16\u7801\u5668\uff0c\u652f\u6301\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u8bbe\u7f6e\uff0c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u4ec5\u4e3a\u7ade\u4e89\u65b9\u6cd5\u76841%\uff0c\u5e76\u63d0\u4f9b\u53ef\u63a7\u7684\u591a\u884c\u6587\u672c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u7684\u89c6\u89c9\u6761\u4ef6\u6a21\u5757\u548c\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0cTextFlux\u65e8\u5728\u7b80\u5316\u67b6\u6784\u5e76\u63d0\u5347\u591a\u8bed\u8a00\u573a\u666f\u6587\u672c\u5408\u6210\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528DiT\uff08Diffusion Transformer\uff09\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u65e0\u9700OCR\u7f16\u7801\u5668\u7684\u591a\u8bed\u8a00\u6587\u672c\u5408\u6210\u3002", "result": "TextFlux\u5728\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5927\u5e45\u964d\u4f4e\uff0c\u4e14\u5728\u591a\u884c\u6587\u672c\u751f\u6210\u4e0a\u5177\u6709\u7075\u6d3b\u6027\u3002", "conclusion": "TextFlux\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u573a\u666f\u6587\u672c\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17105", "pdf": "https://arxiv.org/pdf/2505.17105", "abs": "https://arxiv.org/abs/2505.17105", "authors": ["Anna Spagnolli", "Cecilia Tolomini", "Elisa Beretta", "Claudio Sarra"], "title": "Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs", "categories": ["cs.CY", "cs.AI", "K.4.1; J.3"], "comment": "22 pages, pre-review version", "summary": "Artificial Intelligence (AI) plays an essential role in healthcare and is\npervasively incorporated into medical software and equipment. In the European\nUnion, healthcare is a high-risk application domain for AI, and providers must\nprepare Instructions for Use (IFU) according to the European regulation\n2024/1689 (AI Act). To this regulation, the principle of transparency is\ncardinal and requires the IFU to be clear and relevant to the users. This study\ntests whether these latter requirements are satisfied by the IFU structure. A\nsurvey was administered online via the Qualtrics platform to four types of\ndirect stakeholders, i.e., managers (N = 238), healthcare professionals (N =\n115), patients (N = 229), and Information Technology experts (N = 230). The\nparticipants rated the relevance of a set of transparency needs and indicated\nthe IFU section addressing them. The results reveal differentiated priorities\nacross stakeholders and a troubled mapping of transparency needs onto the IFU\nstructure. Recommendations to build a locally meaningful IFU are derived.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u533b\u7597AI\u4f7f\u7528\u8bf4\u660e\uff08IFU\uff09\u7684\u900f\u660e\u6027\u9700\u6c42\u662f\u5426\u6ee1\u8db3\u6b27\u76dfAI\u6cd5\u89c4\u8981\u6c42\uff0c\u53d1\u73b0\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u900f\u660e\u6027\u9700\u6c42\u4f18\u5148\u7ea7\u4e0d\u540c\uff0c\u4e14IFU\u7ed3\u6784\u5b58\u5728\u95ee\u9898\u3002", "motivation": "\u6b27\u76dfAI\u6cd5\u89c4\u8981\u6c42\u533b\u7597AI\u7684IFU\u5fc5\u987b\u900f\u660e\u4e14\u76f8\u5173\uff0c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e9b\u8981\u6c42\u662f\u5426\u88ab\u6ee1\u8db3\u3002", "method": "\u901a\u8fc7Qualtrics\u5e73\u53f0\u5bf9\u56db\u7c7b\u5229\u76ca\u76f8\u5173\u8005\uff08\u7ba1\u7406\u8005\u3001\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u3001\u60a3\u8005\u3001IT\u4e13\u5bb6\uff09\u8fdb\u884c\u5728\u7ebf\u8c03\u67e5\uff0c\u8bc4\u4f30\u900f\u660e\u6027\u9700\u6c42\u7684\u76f8\u5173\u6027\u53caIFU\u7ed3\u6784\u7684\u5bf9\u5e94\u60c5\u51b5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u900f\u660e\u6027\u9700\u6c42\u4f18\u5148\u7ea7\u4e0d\u540c\uff0cIFU\u7ed3\u6784\u672a\u80fd\u6709\u6548\u6620\u5c04\u8fd9\u4e9b\u9700\u6c42\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u6539\u8fdbIFU\u7ed3\u6784\u7684\u5efa\u8bae\uff0c\u4ee5\u4f7f\u5176\u66f4\u7b26\u5408\u672c\u5730\u9700\u6c42\u3002", "relevance": 30.0}}
{"id": "2505.17638", "pdf": "https://arxiv.org/pdf/2505.17638", "abs": "https://arxiv.org/abs/2505.17638", "authors": ["Tony Bonnaire", "Rapha\u00ebl Urfin", "Giulio Biroli", "Marc M\u00e9zard"], "title": "Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "comment": "36 pages, 15 figures", "summary": "Diffusion models have achieved remarkable success across a wide range of\ngenerative tasks. A key challenge is understanding the mechanisms that prevent\ntheir memorization of training data and allow generalization. In this work, we\ninvestigate the role of the training dynamics in the transition from\ngeneralization to memorization. Through extensive experiments and theoretical\nanalysis, we identify two distinct timescales: an early time\n$\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and\na later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially,\nwe find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size\n$n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window\nof training times with $n$ where models generalize effectively, despite showing\nstrong memorization if training continues beyond it. It is only when $n$\nbecomes larger than a model-dependent threshold that overfitting disappears at\ninfinite training times. These findings reveal a form of implicit dynamical\nregularization in the training dynamics, which allow to avoid memorization even\nin highly overparameterized settings. Our results are supported by numerical\nexperiments with standard U-Net architectures on realistic and synthetic\ndatasets, and by a theoretical analysis using a tractable random features model\nstudied in the high-dimensional limit.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u4e2d\u4ece\u6cdb\u5316\u5230\u8bb0\u5fc6\u7684\u8f6c\u53d8\uff0c\u53d1\u73b0\u5b58\u5728\u4e24\u4e2a\u65f6\u95f4\u5c3a\u5ea6\uff1a\u65e9\u671f\u6cdb\u5316\u65f6\u95f4\u03c4_gen\u548c\u540e\u671f\u8bb0\u5fc6\u65f6\u95f4\u03c4_mem\u3002\u03c4_mem\u968f\u8bad\u7ec3\u96c6\u5927\u5c0fn\u7ebf\u6027\u589e\u957f\uff0c\u800c\u03c4_gen\u4fdd\u6301\u4e0d\u53d8\uff0c\u63ed\u793a\u4e86\u52a8\u6001\u9690\u5f0f\u6b63\u5219\u5316\u7684\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u6269\u6563\u6a21\u578b\u5982\u4f55\u907f\u514d\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u5e76\u5b9e\u73b0\u6cdb\u5316\uff0c\u63a2\u7a76\u8bad\u7ec3\u52a8\u6001\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u8bc6\u522b\u4e24\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u03c4_gen\u548c\u03c4_mem\uff0c\u5e76\u7814\u7a76\u5176\u4e0e\u8bad\u7ec3\u96c6\u5927\u5c0fn\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u03c4_mem\u968fn\u7ebf\u6027\u589e\u957f\uff0c\u03c4_gen\u4e0d\u53d8\uff0c\u5f62\u6210\u6cdb\u5316\u7a97\u53e3\uff1b\u6a21\u578b\u5728\u9ad8\u5ea6\u8fc7\u53c2\u6570\u5316\u65f6\u4ecd\u80fd\u907f\u514d\u8bb0\u5fc6\u3002", "conclusion": "\u8bad\u7ec3\u52a8\u6001\u4e2d\u5b58\u5728\u9690\u5f0f\u6b63\u5219\u5316\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u5728\u8fc7\u53c2\u6570\u5316\u65f6\u4ecd\u80fd\u6cdb\u5316\u3002", "relevance": 40.0}}
{"id": "2505.17238", "pdf": "https://arxiv.org/pdf/2505.17238", "abs": "https://arxiv.org/abs/2505.17238", "authors": ["Clayton Cohn", "Surya Rayala", "Caitlin Snyder", "Joyce Fonteles", "Shruti Jain", "Naveeduddin Mohammed", "Umesh Timalsina", "Sarah K. Burriss", "Ashwin T S", "Namrata Srivastava", "Menton Deweese", "Angela Eeds", "Gautam Biswas"], "title": "Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)", "categories": ["cs.CL"], "comment": "Submitted to the International Conference on Artificial Intelligence\n  in Education (AIED) Workshop on Epistemics and Decision-Making in\n  AI-Supported Education", "summary": "Collaborative dialogue offers rich insights into students' learning and\ncritical thinking. This is essential for adapting pedagogical agents to\nstudents' learning and problem-solving skills in STEM+C settings. While large\nlanguage models (LLMs) facilitate dynamic pedagogical interactions, potential\nhallucinations can undermine confidence, trust, and instructional value.\nRetrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge,\nbut its effectiveness depends on clear semantic links between user input and a\nknowledge base, which are often weak in student dialogue. We propose\nlog-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating\nenvironment logs to contextualize collaborative discourse. Our findings show\nthat LC-RAG improves retrieval over a discourse-only baseline and allows our\ncollaborative peer agent, Copa, to deliver relevant, personalized guidance that\nsupports students' critical thinking and epistemic decision-making in a\ncollaborative computational modeling environment, XYZ.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e5\u5fd7\u4e0a\u4e0b\u6587\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff08LC-RAG\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u6559\u80b2\u5bf9\u8bdd\u4e2dLLM\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u5728\u6559\u80b2\u5bf9\u8bdd\u4e2d\uff0cLLM\u7684\u5e7b\u89c9\u95ee\u9898\u53ef\u80fd\u5f71\u54cd\u4fe1\u4efb\u548c\u6559\u5b66\u6548\u679c\uff0c\u800c\u4f20\u7edfRAG\u65b9\u6cd5\u56e0\u8bed\u4e49\u94fe\u63a5\u4e0d\u8db3\u6548\u679c\u6709\u9650\u3002", "method": "LC-RAG\u901a\u8fc7\u7ed3\u5408\u73af\u5883\u65e5\u5fd7\u589e\u5f3aRAG\u7684\u68c0\u7d22\u80fd\u529b\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5b66\u751f\u5bf9\u8bdd\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLC-RAG\u5728\u68c0\u7d22\u6548\u679c\u4e0a\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u5bf9\u8bdd\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u5b66\u4e60\u6307\u5bfc\u3002", "conclusion": "LC-RAG\u4e3a\u6559\u80b2\u573a\u666f\u4e2d\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.17779", "pdf": "https://arxiv.org/pdf/2505.17779", "abs": "https://arxiv.org/abs/2505.17779", "authors": ["Anjie Le", "Henan Liu", "Yue Wang", "Zhenyu Liu", "Rongkun Zhu", "Taohan Weng", "Jinze Yu", "Boyang Wang", "Yalun Wu", "Kaiwen Yan", "Quanlin Sun", "Meirui Jiang", "Jialun Pei", "Siya Liu", "Haoyun Zheng", "Zhoujun Li", "Alison Noble", "Jacques Souquet", "Xiaoqing Guo", "Manxi Lin", "Hongcheng Guo"], "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging.", "AI": {"tldr": "U2-BENCH\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u8d85\u58f0\u5f71\u50cf\u7406\u89e3\u4e0a\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u6db5\u76d6\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u56de\u5f52\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u4e34\u5e8a\u8bed\u8a00\u751f\u6210\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u8d85\u58f0\u5f71\u50cf\u89e3\u8bfb\u56e0\u56fe\u50cf\u8d28\u91cf\u3001\u566a\u58f0\u548c\u89e3\u5256\u7ed3\u6784\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u800cLVLMs\u5728\u6b64\u9886\u57df\u7684\u8868\u73b0\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "U2-BENCH\u6574\u5408\u4e867,241\u4e2a\u6848\u4f8b\uff0c\u8986\u76d615\u4e2a\u89e3\u5256\u533a\u57df\u548c50\u4e2a\u5e94\u7528\u573a\u666f\uff0c\u5b9a\u4e49\u4e868\u4e2a\u4e34\u5e8a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8620\u4e2a\u5148\u8fdbLVLMs\u3002", "result": "\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u4e34\u5e8a\u8bed\u8a00\u751f\u6210\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "U2-BENCH\u4e3a\u533b\u5b66\u8d85\u58f0\u5f71\u50cf\u9886\u57df\u7684LVLM\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "relevance": 40.0}}
{"id": "2505.17107", "pdf": "https://arxiv.org/pdf/2505.17107", "abs": "https://arxiv.org/abs/2505.17107", "authors": ["Minghao Shao", "Haoran Xi", "Nanda Rani", "Meet Udeshi", "Venkata Sai Charan Putrevu", "Kimberly Milner", "Brendan Dolan-Gavitt", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "title": "CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) agents can automate cybersecurity tasks and can\nadapt to the evolving cybersecurity landscape without re-engineering. While LLM\nagents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF)\ncompetitions, they have two key limitations: accessing latest cybersecurity\nexpertise beyond training data, and integrating new knowledge into complex task\nplanning. Knowledge-based approaches that incorporate technical understanding\ninto the task-solving automation can tackle these limitations. We present\nCRAKEN, a knowledge-based LLM agent framework that improves cybersecurity\ncapability through three core mechanisms: contextual decomposition of\ntask-critical information, iterative self-reflected knowledge retrieval, and\nknowledge-hint injection that transforms insights into adaptive attack\nstrategies. Comprehensive evaluations with different configurations show\nCRAKEN's effectiveness in multi-stage vulnerability detection and exploitation\ncompared to previous approaches. Our extensible architecture establishes new\nmethodologies for embedding new security knowledge into LLM-driven\ncybersecurity agentic systems. With a knowledge database of CTF writeups,\nCRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works\nby 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK\ntechniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating\nimproved cybersecurity capabilities via knowledge-based execution. We make our\nframework open source to public\nhttps://github.com/NYU-LLM-CTF/nyuctf_agents_craken.", "AI": {"tldr": "CRAKEN\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u89e3\u3001\u8fed\u4ee3\u81ea\u53cd\u77e5\u8bc6\u68c0\u7d22\u548c\u77e5\u8bc6\u63d0\u793a\u6ce8\u5165\u63d0\u5347\u7f51\u7edc\u5b89\u5168\u80fd\u529b\uff0c\u5728\u6f0f\u6d1e\u68c0\u6d4b\u548c\u5229\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLM\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u65e0\u6cd5\u83b7\u53d6\u6700\u65b0\u4e13\u4e1a\u77e5\u8bc6\u53ca\u6574\u5408\u65b0\u77e5\u8bc6\u5230\u590d\u6742\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faCRAKEN\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a\u4e0a\u4e0b\u6587\u5206\u89e3\u3001\u8fed\u4ee3\u81ea\u53cd\u77e5\u8bc6\u68c0\u7d22\u548c\u77e5\u8bc6\u63d0\u793a\u6ce8\u5165\u3002", "result": "\u5728NYU CTF Bench\u4e0a\u51c6\u786e\u738722%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd53%\uff1b\u5728MITRE ATT&CK\u6280\u672f\u8bc4\u4f30\u4e2d\u89e3\u51b325-30%\u66f4\u591a\u6280\u672f\u3002", "conclusion": "CRAKEN\u901a\u8fc7\u77e5\u8bc6\u5d4c\u5165\u63d0\u5347\u4e86LLM\u9a71\u52a8\u7684\u7f51\u7edc\u5b89\u5168\u4ee3\u7406\u80fd\u529b\uff0c\u5f00\u6e90\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "relevance": 60.0}}
{"id": "2505.17639", "pdf": "https://arxiv.org/pdf/2505.17639", "abs": "https://arxiv.org/abs/2505.17639", "authors": ["Zehua Pei", "Ying Zhang", "Hui-Ling Zhen", "Xianzhi Yu", "Wulong Liu", "Sinno Jialin Pan", "Mingxuan Yuan", "Bei Yu"], "title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval", "categories": ["cs.LG"], "comment": null, "summary": "Mixture-of-experts (MoE) architectures enable scaling large language models\n(LLMs) to vast parameter counts without a proportional rise in computational\ncosts. However, the significant memory demands of large MoE models hinder their\ndeployment across various computational environments, from cloud servers to\nconsumer devices. This study first demonstrates pronounced task-specific\nspecialization in expert activation patterns within MoE layers. Building on\nthis, we introduce PreMoe, a novel framework that enables efficient deployment\nof massive MoE models in memory-constrained environments. PreMoe features two\nmain components: probabilistic expert pruning (PEP) and task-adaptive expert\nretrieval (TAER). PEP employs a new metric, the task-conditioned expected\nselection score (TCESS), derived from router logits to quantify expert\nimportance for specific tasks, thereby identifying a minimal set of critical\nexperts. TAER leverages these task-specific expert importance profiles for\nefficient inference. It pre-computes and stores compact expert patterns for\ndiverse tasks. When a user query is received, TAER rapidly identifies the most\nrelevant stored task pattern and reconstructs the model by loading only the\nsmall subset of experts crucial for that task. This approach dramatically\nreduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B\nmaintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\%\nexpert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning\n(87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and\n81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64\n(390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly\navailable at https://github.com/JarvisPei/PreMoe.", "AI": {"tldr": "PreMoe\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u526a\u679d\u548c\u68c0\u7d22\uff0c\u663e\u8457\u51cf\u5c11\u5927\u578bMoE\u6a21\u578b\u7684\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578bMoE\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u6fc0\u6d3b\u6a21\u5f0f\u63d0\u4f9b\u4e86\u4f18\u5316\u673a\u4f1a\u3002", "method": "\u63d0\u51faPreMoe\u6846\u67b6\uff0c\u5305\u62ec\u6982\u7387\u4e13\u5bb6\u526a\u679d\uff08PEP\uff09\u548c\u4efb\u52a1\u81ea\u9002\u5e94\u4e13\u5bb6\u68c0\u7d22\uff08TAER\uff09\uff0c\u57fa\u4e8e\u4efb\u52a1\u6761\u4ef6\u9009\u62e9\u5173\u952e\u4e13\u5bb6\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21MoE\u6a21\u578b\u4e0a\uff0cPreMoe\u663e\u8457\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff08\u598287.5%\u4e13\u5bb6\u526a\u679d\uff09\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff08\u598297.2%\uff09\u3002", "conclusion": "PreMoe\u4e3a\u9ad8\u6548\u90e8\u7f72\u5927\u578bMoE\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8ba1\u7b97\u73af\u5883\u3002", "relevance": 85.0}}
{"id": "2505.17244", "pdf": "https://arxiv.org/pdf/2505.17244", "abs": "https://arxiv.org/abs/2505.17244", "authors": ["Changyi Li", "Jiayi Wang", "Xudong Pan", "Geng Hong", "Min Yang"], "title": "ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) are transforming the AI landscape with advanced\nreasoning capabilities. While the generated reasoning traces enhance model\ntransparency, they can still contain unsafe content, even when the final answer\nappears safe. Existing moderation tools, primarily designed for question-answer\n(QA) pairs, are empirically ineffective at detecting hidden risks embedded in\nreasoning traces. After identifying the key challenges, we formally define the\nquestion-thought (QT) moderation task and propose ReasoningShield, the first\nsafety detection model tailored to identify potential risks in the reasoning\ntrace before reaching the final answer. To construct the model, we synthesize a\nhigh-quality reasoning safety detection dataset comprising over 8,000\nquestion-thought pairs spanning ten risk categories and three safety levels.\nOur dataset construction process incorporates a comprehensive human-AI\ncollaborative annotation pipeline, which achieves over 93% annotation accuracy\nwhile significantly reducing human costs. On a diverse set of in-distribution\nand out-of-distribution benchmarks, ReasoningShield outperforms mainstream\ncontent safety moderation models in identifying risks within reasoning traces,\nwith an average F1 score exceeding 0.92. Notably, despite being trained on our\nQT dataset only, ReasoningShield also demonstrates competitive performance in\ndetecting unsafe question-answer pairs on traditional benchmarks, rivaling\nbaselines trained on 10 times larger datasets and base models, which strongly\nvalidates the quality of our dataset. Furthermore, ReasoningShield is built\nupon compact 1B/3B base models to facilitate lightweight deployment and\nprovides human-friendly risk analysis by default. To foster future research, we\npublicly release all the resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ReasoningShield\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u63a8\u7406\u8f68\u8ff9\u4e2d\u6f5c\u5728\u98ce\u9669\u7684\u5b89\u5168\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5ba1\u6838\u5de5\u5177\u5bf9\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9690\u85cf\u98ce\u9669\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u4e13\u95e8\u89e3\u51b3QT\u5ba1\u6838\u4efb\u52a1\u3002", "method": "\u63d0\u51faReasoningShield\u6a21\u578b\uff0c\u6784\u5efa\u5305\u542b8,000\u591a\u4e2aQT\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "ReasoningShield\u5728\u63a8\u7406\u8f68\u8ff9\u98ce\u9669\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u5e73\u5747F1>0.92\uff09\uff0c\u5e76\u5728\u4f20\u7edfQA\u57fa\u51c6\u4e0a\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "ReasoningShield\u9ad8\u6548\u4e14\u8f7b\u91cf\uff0c\u6570\u636e\u96c6\u8d28\u91cf\u9ad8\uff0c\u8d44\u6e90\u5df2\u516c\u5f00\u3002", "relevance": 85.0}}
{"id": "2505.17782", "pdf": "https://arxiv.org/pdf/2505.17782", "abs": "https://arxiv.org/abs/2505.17782", "authors": ["Nikolas Papadopoulos", "Nikolaos Ioannis Bountos", "Maria Sdraka", "Andreas Karavias", "Ioannis Papoutsis"], "title": "Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Ground deformation is regarded in volcanology as a key precursor signal\npreceding volcanic eruptions. Satellite-based Interferometric Synthetic\nAperture Radar (InSAR) enables consistent, global-scale deformation tracking;\nhowever, deep learning methods remain largely unexplored in this domain, mainly\ndue to the lack of a curated machine learning dataset. In this work, we build\non the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a\nglobal collection of 38 spatiotemporal datacubes offering high resolution,\nmulti-source and multi-temporal information, covering 44 of the world's most\nactive volcanoes over a 7-year period. Each spatiotemporal datacube integrates\nInSAR products, topographic data, as well as atmospheric variables which are\nknown to introduce signal delays that can mimic ground deformation in InSAR\nimagery. Furthermore, we provide expert annotations detailing the type,\nintensity and spatial extent of deformation events, along with rich text\ndescriptions of the observed scenes. Finally, we present a comprehensive\nbenchmark, demonstrating Hephaestus Minicubes' ability to support volcanic\nunrest monitoring as a multi-modal, multi-temporal classification and semantic\nsegmentation task, establishing strong baselines with state-of-the-art\narchitectures. This work aims to advance machine learning research in volcanic\nmonitoring, contributing to the growing integration of data-driven methods\nwithin Earth science applications.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Hephaestus Minicubes\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u706b\u5c71\u76d1\u6d4b\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\uff0c\u5305\u542b\u591a\u6e90\u3001\u591a\u65f6\u76f8\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e13\u5bb6\u6807\u6ce8\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u706b\u5c71\u53d8\u5f62\u662f\u706b\u5c71\u55b7\u53d1\u7684\u91cd\u8981\u524d\u5146\u4fe1\u53f7\uff0c\u4f46\u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u8f83\u5c11\uff0c\u4e3b\u8981\u7f3a\u4e4f\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eHephaestus\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86Hephaestus Minicubes\u6570\u636e\u96c6\uff0c\u5305\u542b38\u4e2a\u65f6\u7a7a\u6570\u636e\u7acb\u65b9\u4f53\uff0c\u6574\u5408\u4e86InSAR\u3001\u5730\u5f62\u548c\u5927\u6c14\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e13\u5bb6\u6807\u6ce8\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u591a\u6a21\u6001\u3001\u591a\u65f6\u76f8\u7684\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86\u706b\u5c71\u76d1\u6d4b\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\uff0c\u4fc3\u8fdb\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5730\u7403\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "relevance": 20.0}}
{"id": "2505.17108", "pdf": "https://arxiv.org/pdf/2505.17108", "abs": "https://arxiv.org/abs/2505.17108", "authors": ["Aijuan Song", "Guohua Wu"], "title": "REMS: a unified solution representation, problem modeling and metaheuristic algorithm design for general combinatorial optimization problems", "categories": ["cs.NE", "cs.AI"], "comment": "15 pages, 11 figures, regular reseach paper", "summary": "Combinatorial optimization problems (COPs) with discrete variables and finite\nsearch space are critical across numerous fields, and solving them in\nmetaheuristic algorithms is popular. However, addressing a specific COP\ntypically requires developing a tailored and handcrafted algorithm. Even minor\nadjustments, such as constraint changes, may necessitate algorithm\nredevelopment. Therefore, establishing a framework for formulating diverse COPs\ninto a unified paradigm and designing reusable metaheuristic algorithms is\nvaluable. A COP can be typically viewed as the process of giving resources to\nperform specific tasks, subjecting to given constraints. Motivated by this, a\nresource-centered modeling and solving framework (REMS) is introduced for the\nfirst time. We first extract and define resources and tasks from a COP.\nSubsequently, given predetermined resources, the solution structure is unified\nas assigning tasks to resources, from which variables, objectives, and\nconstraints can be derived and a problem model is constructed. To solve the\nmodeled COPs, several fundamental operators are designed based on the unified\nsolution structure, including the initial solution, neighborhood structure,\ndestruction and repair, crossover, and ranking. These operators enable the\ndevelopment of various metaheuristic algorithms. Specially, 4\nsingle-point-based algorithms and 1 population-based algorithm are configured\nherein. Experiments on 10 COPs, covering routing, location, loading,\nassignment, scheduling, and graph coloring problems, show that REMS can model\nthese COPs within the unified paradigm and effectively solve them with the\ndesigned metaheuristic algorithms. Furthermore, REMS is more competitive than\nGUROBI and SCIP in tackling large-scale instances and complex COPs, and\noutperforms OR-TOOLS on several challenging COPs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d44\u6e90\u4e3a\u4e2d\u5fc3\u7684\u5efa\u6a21\u4e0e\u6c42\u89e3\u6846\u67b6\uff08REMS\uff09\uff0c\u7528\u4e8e\u7edf\u4e00\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff08COPs\uff09\uff0c\u8bbe\u8ba1\u4e86\u591a\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u901a\u5e38\u9700\u8981\u5b9a\u5236\u5316\u7b97\u6cd5\uff0c\u8c03\u6574\u7ea6\u675f\u53ef\u80fd\u5bfc\u81f4\u91cd\u65b0\u5f00\u53d1\u3002REMS\u65e8\u5728\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\uff0c\u907f\u514d\u91cd\u590d\u5f00\u53d1\u3002", "method": "\u63d0\u53d6\u8d44\u6e90\u548c\u4efb\u52a1\uff0c\u7edf\u4e00\u89e3\u7ed3\u6784\u4e3a\u4efb\u52a1\u5206\u914d\uff0c\u8bbe\u8ba1\u57fa\u7840\u7b97\u5b50\uff08\u5982\u521d\u59cb\u89e3\u3001\u90bb\u57df\u7ed3\u6784\u7b49\uff09\uff0c\u914d\u7f6e5\u79cd\u7b97\u6cd5\u3002", "result": "\u572810\u7c7bCOPs\u4e0a\u9a8c\u8bc1\uff0cREMS\u4f18\u4e8eGUROBI\u3001SCIP\u548cOR-TOOLS\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u548c\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "REMS\u4e3aCOPs\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 30.0}}
{"id": "2505.17640", "pdf": "https://arxiv.org/pdf/2505.17640", "abs": "https://arxiv.org/abs/2505.17640", "authors": ["Ivana Kesi\u0107", "Carolina Fortuna", "Mihael Mohor\u010di\u010d", "Bla\u017e Bertalani\u010d"], "title": "A Network Science Approach to Granular Time Series Segmentation", "categories": ["cs.LG"], "comment": "24 pages, 10 figures", "summary": "Time series segmentation (TSS) is one of the time series (TS) analysis\ntechniques, that has received considerably less attention compared to other TS\nrelated tasks. In recent years, deep learning architectures have been\nintroduced for TSS, however their reliance on sliding windows limits\nsegmentation granularity due to fixed window sizes and strides. To overcome\nthese challenges, we propose a new more granular TSS approach that utilizes the\nWeighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines\nit with a Graph Attention Network (GAT). By transforming TS into graphs, we are\nable to capture different structural aspects of the data that would otherwise\nremain hidden. By utilizing the representation learning capabilities of Graph\nNeural Networks, our method is able to effectively identify meaningful segments\nwithin the TS. To better understand the potential of our approach, we also\nexperimented with different TS-to-graph transformations and compared their\nperformance. Our contributions include: a) formulating the TSS as a node\nclassification problem on graphs; b) conducting an extensive analysis of\nvarious TS- to-graph transformations applied to TSS using benchmark datasets\nfrom the TSSB repository; c) providing the first detailed study on utilizing\nGNNs for analyzing graph representations of TS in the context of TSS; d)\ndemonstrating the effectiveness of our method, which achieves an average F1\nscore of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the\nseq2point baseline method by 0.05 in terms of F1 score; and f) reducing the\nrequired training data compared to the baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u53cc\u89c6\u89d2\u53ef\u89c1\u56fe\uff08WDPVG\uff09\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5206\u5272\uff08TSS\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u56e0\u56fa\u5b9a\u7a97\u53e3\u5927\u5c0f\u548c\u6b65\u957f\u9650\u5236\u4e86\u5206\u5272\u7c92\u5ea6\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\uff0c\u5229\u7528GAT\u8fdb\u884c\u8282\u70b9\u5206\u7c7b\uff0c\u6355\u6349\u9690\u85cf\u7684\u7ed3\u6784\u7279\u5f81\u3002", "result": "\u572859\u4e2aTSS\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a0.97\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aTSS\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86GNN\u5728\u56fe\u8868\u793a\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.17250", "pdf": "https://arxiv.org/pdf/2505.17250", "abs": "https://arxiv.org/abs/2505.17250", "authors": ["Razvan-Gabriel Dumitru", "Darius Peteleaza", "Vikas Yadav", "Liangming Pan"], "title": "ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "comment": "25 pages, 18 figures, and 6 tables", "summary": "Large language models excel at complex tasks by breaking down problems into\nstructured reasoning steps. However, reasoning traces often extend beyond\nreaching a correct answer, causing wasted computation, reduced readability, and\nhallucinations. To address this, we introduce a novel hyperparameter-free\nconciseness score used as a reward signal within a reinforcement learning\nframework to guide models toward generating correct and concise reasoning\ntraces. This score is evaluated by a large language model acting as a judge,\nenabling dynamic, context-aware feedback beyond simple token length. Our method\nachieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,\nreducing token usage by up to 31x on simple problems while improving accuracy\nby 7%, and on the hardest problems, it outperforms full reasoning by +7.5%\naccuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves\naccuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on\nthe judge model, reward composition, and problem difficulty, showing that our\nmethod dynamically adapts reasoning length based on problem difficulty and\nbenefits significantly from stronger judges. The code, model weights, and\ndatasets are open-sourced at https://github.com/RazvanDu/ConciseRL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65e0\u8d85\u53c2\u6570\u7684\u7b80\u6d01\u6027\u8bc4\u5206\u6765\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u63a8\u7406\u6b65\u9aa4\u8fc7\u957f\u5bfc\u81f4\u7684\u6d6a\u8d39\u8ba1\u7b97\u3001\u53ef\u8bfb\u6027\u5dee\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7b80\u6d01\u6027\u8bc4\u5206\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u7531\u53e6\u4e00\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u63d0\u4f9b\u52a8\u6001\u53cd\u9988\u3002", "result": "\u5728MATH\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u6700\u4f73\u5e73\u8861\uff0c\u51cf\u5c11\u4e8631\u500d\u7684token\u4f7f\u7528\u5e76\u63d0\u9ad87%\u7684\u51c6\u786e\u6027\uff1b\u5728TheoremQA\u4e0a\u51cf\u5c11\u4e8612.5\u500d\u7684token\u4f7f\u7528\u5e76\u63d0\u9ad82.2%\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6839\u636e\u95ee\u9898\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u957f\u5ea6\uff0c\u4e14\u66f4\u5f3a\u7684\u8bc4\u5224\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "relevance": 90.0}}
{"id": "2505.17783", "pdf": "https://arxiv.org/pdf/2505.17783", "abs": "https://arxiv.org/abs/2505.17783", "authors": ["Dekai Zhu", "Stefan Gavranovic", "Flavien Boussuge", "Benjamin Busam", "Slobodan Ilic"], "title": "Generative Data Augmentation for Object Point Cloud Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6570\u636e\u589e\u5f3a\uff08GDA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u70b9\u4e91\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u70b9\u4e91\u548c\u4f2a\u6807\u7b7e\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08TDA\uff09\u5728\u70b9\u4e91\u5206\u5272\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\uff0c\u800c\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u8bed\u4e49\u6807\u7b7e\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u4e863D\u6269\u6563\u6a21\u578bLion\uff0c\u4f7f\u5176\u80fd\u591f\u57fa\u4e8e\u5206\u5272\u63a9\u7801\u751f\u6210\u9ad8\u8d28\u91cf\u70b9\u4e91\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u6b65GDA\u6d41\u7a0b\uff0c\u5305\u62ec\u751f\u6210\u6837\u672c\u548c\u4f2a\u6807\u7b7e\u8fc7\u6ee4\u3002", "result": "\u5728\u4e24\u4e2a\u5408\u6210\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u771f\u5b9e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGDA\u65b9\u6cd5\u4f18\u4e8eTDA\u53ca\u76f8\u5173\u534a\u76d1\u7763\u548c\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "GDA\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17115", "pdf": "https://arxiv.org/pdf/2505.17115", "abs": "https://arxiv.org/abs/2505.17115", "authors": ["Ying Zhu", "Heng Zhou", "Rui Su", "Peiqin Zhuang", "Lei Bai"], "title": "Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Recently, many approaches, such as Chain-of-Thought (CoT) prompting and\nMulti-Agent Debate (MAD), have been proposed to further enrich Large Language\nModels' (LLMs) complex problem-solving capacities in reasoning scenarios.\nHowever, these methods may fail to solve complex problems due to the lack of\nability to find optimal solutions. Swarm Intelligence has been serving as a\npowerful tool for finding optima in the field of traditional optimization\nproblems. To this end, we propose integrating swarm intelligence into the\nreasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)\nparadigm. In this paradigm, we formulate LLM reasoning as an optimization\nproblem and use a swarm intelligence scheme to guide a group of LLM-based\nagents in collaboratively searching for optimal solutions. To avoid swarm\nintelligence getting trapped in local optima, we further develop a Swarm\nIntelligence Enhancing Reasoning (SIER) framework, which develops a\ndensity-driven strategy to enhance the reasoning ability. To be specific, we\npropose to perform kernel density estimation and non-dominated sorting to\noptimize both solution quality and diversity simultaneously. In this case, SIER\nefficiently enhances solution space exploration through expanding the diversity\nof the reasoning path. Besides, a step-level quality evaluation is used to help\nagents improve solution quality by correcting low-quality intermediate steps.\nThen, we use quality thresholds to dynamically control the termination of\nexploration and the selection of candidate steps, enabling a more flexible and\nefficient reasoning process. Extensive experiments are ...", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7684Agent-based Swarm Intelligence (ASI)\u8303\u5f0f\uff0c\u901a\u8fc7Swarm Intelligence Enhancing Reasoning (SIER)\u6846\u67b6\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982CoT\u548cMAD\uff09\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u53ef\u80fd\u56e0\u65e0\u6cd5\u627e\u5230\u6700\u4f18\u89e3\u800c\u5931\u8d25\uff0c\u7fa4\u4f53\u667a\u80fd\u5728\u4f20\u7edf\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u56e0\u6b64\u5c06\u5176\u5f15\u5165LLM\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u7fa4\u4f53\u667a\u80fd\u65b9\u6848\u6307\u5bfcLLM\u4ee3\u7406\u534f\u4f5c\u641c\u7d22\u6700\u4f18\u89e3\uff1bSIER\u6846\u67b6\u901a\u8fc7\u5bc6\u5ea6\u9a71\u52a8\u7b56\u7565\uff08\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u975e\u652f\u914d\u6392\u5e8f\uff09\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "SIER\u901a\u8fc7\u6269\u5c55\u63a8\u7406\u8def\u5f84\u7684\u591a\u6837\u6027\u548c\u9010\u6b65\u8d28\u91cf\u8bc4\u4f30\uff0c\u6709\u6548\u63d0\u5347\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "ASI\u548cSIER\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3aLLM\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.17646", "pdf": "https://arxiv.org/pdf/2505.17646", "abs": "https://arxiv.org/abs/2505.17646", "authors": ["Huanran Chen", "Yinpeng Dong", "Zeming Wei", "Yao Huang", "Yichi Zhang", "Hang Su", "Jun Zhu"], "title": "Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives", "categories": ["cs.LG"], "comment": null, "summary": "Recent studies have revealed that the loss landscape of large language models\nresembles a basin, within which the models perform nearly identically, and\noutside of which they lose all their capabilities. In this work, we conduct\nfurther studies on the loss landscape of large language models. We discover\nthat pre-training creates a \"basic capability\" basin, and subsequent\nfine-tuning creates \"specific capability\" basins (e.g., math, safety, coding)\nwithin the basic capability basin. We further investigate two types of loss\nlandscapes: the most-case landscape (i.e., the landscape along most directions)\nand the worst-case landscape (i.e., the landscape along the worst direction).\nWe argue that as long as benign fine-tuning remains within the most-case basin,\nit will not compromise previous capabilities. Similarly, any fine-tuning\n(including the adversarial one) that stays within the worst-case basin would\nnot compromise previous capabilities. Finally, we theoretically demonstrate\nthat the size of the most-case basin can bound the size of the worst-case basin\nand the robustness with respect to input perturbations. We also show that, due\nto the over-parameterization property of current large language models, one can\neasily enlarge the basins by five times.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u635f\u5931\u666f\u89c2\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5206\u522b\u5f62\u6210\u201c\u57fa\u672c\u80fd\u529b\u201d\u548c\u201c\u7279\u5b9a\u80fd\u529b\u201d\u76c6\u5730\uff0c\u5e76\u63a2\u8ba8\u4e86\u6700\u574f\u548c\u6700\u4f18\u60c5\u51b5\u4e0b\u7684\u666f\u89c2\u53ca\u5176\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u635f\u5931\u666f\u89c2\u4e2d\u7684\u884c\u4e3a\uff0c\u4ee5\u63d0\u5347\u5176\u9c81\u68d2\u6027\u548c\u80fd\u529b\u4fdd\u7559\u3002", "method": "\u5206\u6790\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5f62\u6210\u7684\u635f\u5931\u666f\u89c2\uff0c\u533a\u5206\u6700\u574f\u548c\u6700\u4f18\u60c5\u51b5\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u76c6\u5730\u5927\u5c0f\u4e0e\u9c81\u68d2\u6027\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5f62\u6210\u4e0d\u540c\u80fd\u529b\u7684\u76c6\u5730\uff0c\u8bc1\u660e\u4e86\u76c6\u5730\u5927\u5c0f\u4e0e\u9c81\u68d2\u6027\u7684\u5173\u8054\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u6269\u5927\u76c6\u5730\u3002", "conclusion": "\u635f\u5931\u666f\u89c2\u7684\u76c6\u5730\u7ed3\u6784\u5bf9\u6a21\u578b\u80fd\u529b\u4fdd\u7559\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u901a\u8fc7\u8fc7\u53c2\u6570\u5316\u6269\u5927\u76c6\u5730\u3002", "relevance": 85.0}}
{"id": "2505.17260", "pdf": "https://arxiv.org/pdf/2505.17260", "abs": "https://arxiv.org/abs/2505.17260", "authors": ["Yihuai Hong", "Yiran Zhao", "Wei Tang", "Yang Deng", "Yu Rong", "Wenxuan Zhang"], "title": "The Rise of Parameter Specialization for Knowledge Storage in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Over time, a growing wave of large language models from various series has\nbeen introduced to the community. Researchers are striving to maximize the\nperformance of language models with constrained parameter sizes. However, from\na microscopic perspective, there has been limited research on how to better\nstore knowledge in model parameters, particularly within MLPs, to enable more\neffective utilization of this knowledge by the model. In this work, we analyze\ntwenty publicly available open-source large language models to investigate the\nrelationship between their strong performance and the way knowledge is stored\nin their corresponding MLP parameters. Our findings reveal that as language\nmodels become more advanced and demonstrate stronger knowledge capabilities,\ntheir parameters exhibit increased specialization. Specifically, parameters in\nthe MLPs tend to be more focused on encoding similar types of knowledge. We\nexperimentally validate that this specialized distribution of knowledge\ncontributes to improving the efficiency of knowledge utilization in these\nmodels. Furthermore, by conducting causal training experiments, we confirm that\nthis specialized knowledge distribution plays a critical role in improving the\nmodel's efficiency in leveraging stored knowledge.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e8620\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u968f\u7740\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff0cMLP\u53c2\u6570\u4e2d\u7684\u77e5\u8bc6\u5b58\u50a8\u66f4\u4e13\u4e1a\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u77e5\u8bc6\u5229\u7528\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684MLP\u53c2\u6570\u4e2d\u66f4\u6709\u6548\u5730\u5b58\u50a8\u77e5\u8bc6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5206\u679020\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7814\u7a76MLP\u53c2\u6570\u4e2d\u77e5\u8bc6\u5b58\u50a8\u65b9\u5f0f\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u8bad\u7ec3\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0MLP\u53c2\u6570\u4e2d\u7684\u77e5\u8bc6\u5b58\u50a8\u66f4\u4e13\u4e1a\u5316\uff0c\u4e14\u8fd9\u79cd\u4e13\u4e1a\u5316\u5206\u5e03\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u5229\u7528\u6548\u7387\u3002", "conclusion": "MLP\u53c2\u6570\u4e2d\u77e5\u8bc6\u7684\u4e13\u4e1a\u5316\u5206\u5e03\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "relevance": 85.0}}
{"id": "2505.17796", "pdf": "https://arxiv.org/pdf/2505.17796", "abs": "https://arxiv.org/abs/2505.17796", "authors": ["Yuxin Yang", "Yinan Zhou", "Yuxin Chen", "Ziqi Zhang", "Zongyang Ma", "Chunfeng Yuan", "Bing Li", "Lin Song", "Jun Gao", "Peng Li", "Weiming Hu"], "title": "DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "20 pages, 6 figures", "summary": "Composed Image Retrieval (CIR) aims to retrieve target images from a gallery\nbased on a reference image and modification text as a combined query. Recent\napproaches focus on balancing global information from two modalities and encode\nthe query into a unified feature for retrieval. However, due to insufficient\nattention to fine-grained details, these coarse fusion methods often struggle\nwith handling subtle visual alterations or intricate textual instructions. In\nthis work, we propose DetailFusion, a novel dual-branch framework that\neffectively coordinates information across global and detailed granularities,\nthereby enabling detail-enhanced CIR. Our approach leverages atomic detail\nvariation priors derived from an image editing dataset, supplemented by a\ndetail-oriented optimization strategy to develop a Detail-oriented Inference\nBranch. Furthermore, we design an Adaptive Feature Compositor that dynamically\nfuses global and detailed features based on fine-grained information of each\nunique multimodal query. Extensive experiments and ablation analyses not only\ndemonstrate that our method achieves state-of-the-art performance on both CIRR\nand FashionIQ datasets but also validate the effectiveness and cross-domain\nadaptability of detail enhancement for CIR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDetailFusion\uff0c\u4e00\u79cd\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u5168\u5c40\u548c\u7ec6\u8282\u4fe1\u606f\uff0c\u63d0\u5347\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4e2d\u56e0\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u5fae\u89c6\u89c9\u53d8\u5316\u6216\u590d\u6742\u6587\u672c\u6307\u4ee4\u3002", "method": "\u63d0\u51faDetailFusion\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u96c6\u4e2d\u7684\u539f\u5b50\u7ec6\u8282\u53d8\u5316\u5148\u9a8c\uff0c\u7ed3\u5408\u7ec6\u8282\u5bfc\u5411\u4f18\u5316\u7b56\u7565\u548c\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728CIRR\u548cFashionIQ\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u7ec6\u8282\u589e\u5f3a\u7684\u6709\u6548\u6027\u548c\u8de8\u57df\u9002\u5e94\u6027\u3002", "conclusion": "DetailFusion\u901a\u8fc7\u5168\u5c40\u4e0e\u7ec6\u8282\u7684\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u4e86CIR\u4efb\u52a1\u7684\u8868\u73b0\u3002", "relevance": 40.0}}
{"id": "2505.17652", "pdf": "https://arxiv.org/pdf/2505.17652", "abs": "https://arxiv.org/abs/2505.17652", "authors": ["Deyang Kong", "Qi Guo", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning exhibits potential in enhancing the reasoning\nabilities of large language models, yet it is hard to scale for the low sample\nefficiency during the rollout phase. Existing methods attempt to improve\nefficiency by scheduling problems based on problem difficulties. However, these\napproaches suffer from unstable and biased estimations of problem difficulty\nand fail to capture the alignment between model competence and problem\ndifficulty in RL training, leading to suboptimal results. To tackle these\nlimitations, this paper introduces \\textbf{C}ompetence-\\textbf{D}ifficulty\n\\textbf{A}lignment \\textbf{S}ampling (\\textbf{CDAS}), which enables accurate\nand stable estimation of problem difficulties by aggregating historical\nperformance discrepancies of problems. Then the model competence is quantified\nto adaptively select problems whose difficulty is in alignment with the model's\ncurrent competence using a fixed-point system. Experimental results across a\nrange of challenging mathematical benchmarks show that CDAS achieves great\nimprovements in both accuracy and efficiency. CDAS attains the highest average\naccuracy against baselines and exhibits significant speed advantages compared\nto Dynamic Sampling, a competitive strategy in DAPO, which is \\textbf{2.33}\ntimes slower than CDAS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDAS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u95ee\u9898\u96be\u5ea6\u4e0e\u6a21\u578b\u80fd\u529b\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u95ee\u9898\u96be\u5ea6\u4f30\u8ba1\u548c\u6a21\u578b\u80fd\u529b\u5bf9\u9f50\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "CDAS\u901a\u8fc7\u805a\u5408\u5386\u53f2\u6027\u80fd\u5dee\u5f02\u51c6\u786e\u4f30\u8ba1\u95ee\u9898\u96be\u5ea6\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u70b9\u7cfb\u7edf\u91cf\u5316\u6a21\u578b\u80fd\u529b\uff0c\u52a8\u6001\u9009\u62e9\u4e0e\u5f53\u524d\u80fd\u529b\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCDAS\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901f\u5ea6\u6bd4Dynamic Sampling\u5feb2.33\u500d\u3002", "conclusion": "CDAS\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u95ee\u9898\u96be\u5ea6\u4e0e\u6a21\u578b\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17265", "pdf": "https://arxiv.org/pdf/2505.17265", "abs": "https://arxiv.org/abs/2505.17265", "authors": ["Xiao Yu Cindy Zhang", "Carlos R. Ferreira", "Francis Rossignol", "Raymond T. Ng", "Wyeth Wasserman", "Jian Zhu"], "title": "CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant\ndiagnostic challenges. Case reports serve as key but computationally\nunderutilized resources to inform diagnosis. Clinical dense information\nextraction refers to organizing medical information into structured predefined\ncategories. Large Language Models (LLMs) may enable scalable information\nextraction from case reports but are rarely evaluated for this task. We\nintroduce CaseReportBench, an expert-annotated dataset for dense information\nextraction of case reports, focusing on IEMs. Using this dataset, we assess\nvarious models and prompting strategies, introducing novel approaches such as\ncategory-specific prompting and subheading-filtered data integration. Zero-shot\nchain-of-thought prompting offers little advantage over standard zero-shot\nprompting. Category-specific prompting improves alignment with the benchmark.\nThe open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our\nclinician evaluations show that LLMs can extract clinically relevant details\nfrom case reports, supporting rare disease diagnosis and management. We also\nhighlight areas for improvement, such as LLMs' limitations in recognizing\nnegative findings important for differential diagnosis. This work advances\nLLM-driven clinical natural language processing and paves the way for scalable\nmedical AI applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCaseReportBench\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLM\u5728\u7f55\u89c1\u75c5\u6848\u4f8b\u62a5\u544a\u4e2d\u7684\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\uff0c\u53d1\u73b0Qwen2.5-7B\u4f18\u4e8eGPT-4o\uff0c\u5e76\u6307\u51faLLM\u5728\u4e34\u5e8a\u8d1f\u4f8b\u8bc6\u522b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u7f55\u89c1\u75c5\u8bca\u65ad\u56f0\u96be\uff0c\u6848\u4f8b\u62a5\u544a\u662f\u91cd\u8981\u4f46\u672a\u5145\u5206\u5229\u7528\u7684\u8d44\u6e90\uff0cLLM\u53ef\u80fd\u5b9e\u73b0\u9ad8\u6548\u4fe1\u606f\u63d0\u53d6\uff0c\u4f46\u7f3a\u4e4f\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528CaseReportBench\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\uff0c\u5305\u62ec\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u548c\u5b50\u6807\u9898\u8fc7\u6ee4\u6570\u636e\u6574\u5408\u3002", "result": "\u96f6-shot\u601d\u7ef4\u94fe\u63d0\u793a\u65e0\u4f18\u52bf\uff0c\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u6548\u679c\u66f4\u597d\uff1bQwen2-7B\u4f18\u4e8eGPT-4o\uff0cLLM\u80fd\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u7ec6\u8282\u3002", "conclusion": "LLM\u5728\u4e34\u5e8aNLP\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6539\u8fdb\u8d1f\u4f8b\u8bc6\u522b\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17807", "pdf": "https://arxiv.org/pdf/2505.17807", "abs": "https://arxiv.org/abs/2505.17807", "authors": ["Ping Li", "Jianan Ni", "Bo Pang"], "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition", "categories": ["cs.CV"], "comment": "Accepted in IJCAI'25", "summary": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBMTC\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u80cc\u666f\u6df7\u5408\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u51b3\u7b56\u8fb9\u754c\u76f8\u4f3c\u6027\u548c\u653b\u51fb\u65b9\u5411\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u8fc1\u79fb\u6027\u4e0a\u53d7\u9650\u4e8e\u51b3\u7b56\u8fb9\u754c\u76f8\u4f3c\u6027\u5047\u8bbe\u548c\u653b\u51fb\u65b9\u5411\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u6a21\u578b\u65e0\u5173\u7684\u80cc\u666f\u5bf9\u6297\u6df7\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9009\u62e9\u80cc\u666f\u5e27\u8fdb\u884c\u6df7\u5408\uff0c\u5e76\u5229\u7528\u80cc\u666f\u7c7b\u522b\u6307\u5bfc\u68af\u5ea6\u66f4\u65b0\uff0c\u540c\u65f6\u5f15\u5165\u65f6\u95f4\u68af\u5ea6\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728UCF101\u3001Kinetics-400\u548cImageNet\u6570\u636e\u96c6\u4e0a\uff0cBMTC\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u3002", "conclusion": "BMTC\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u6a21\u578b\u4f9d\u8d56\u548c\u7a33\u5b9a\u653b\u51fb\u65b9\u5411\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u3002", "relevance": 30.0}}
{"id": "2505.17660", "pdf": "https://arxiv.org/pdf/2505.17660", "abs": "https://arxiv.org/abs/2505.17660", "authors": ["Chenyang Li", "Jinsong Chen", "John E. Hopcroft", "Kun He"], "title": "DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification", "categories": ["cs.LG"], "comment": "Preprint version", "summary": "Neighborhood-aware tokenized graph Transformers have recently shown great\npotential for node classification tasks. Despite their effectiveness, our\nin-depth analysis of neighborhood tokens reveals two critical limitations in\nthe existing paradigm. First, current neighborhood token generation methods\nfail to adequately capture attribute correlations within a neighborhood.\nSecond, the conventional self-attention mechanism suffers from attention\ndiversion when processing neighborhood tokens, where high-hop neighborhoods\nreceive disproportionate focus, severely disrupting information interactions\nbetween the target node and its neighborhood tokens. To address these\nchallenges, we propose DAM-GT, Dual positional encoding-based Attention Masking\ngraph Transformer. DAM-GT introduces a novel dual positional encoding scheme\nthat incorporates attribute-aware encoding via an attribute clustering\nstrategy, effectively preserving node correlations in both topological and\nattribute spaces. In addition, DAM-GT formulates a new attention mechanism with\na simple yet effective masking strategy to guide interactions between target\nnodes and their neighborhood tokens, overcoming the issue of attention\ndiversion. Extensive experiments on various graphs with different homophily\nlevels as well as different scales demonstrate that DAM-GT consistently\noutperforms state-of-the-art methods in node classification tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDAM-GT\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u4f4d\u7f6e\u7f16\u7801\u548c\u6ce8\u610f\u529b\u63a9\u7801\u7684\u56feTransformer\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u672a\u80fd\u5145\u5206\u6355\u6349\u90bb\u57df\u5c5e\u6027\u76f8\u5173\u6027\u53ca\u6ce8\u610f\u529b\u5206\u6563\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u90bb\u57df\u611f\u77e5\u7684\u56feTransformer\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u90bb\u57df\u4ee4\u724c\u751f\u6210\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u90bb\u57df\u5185\u5c5e\u6027\u76f8\u5173\u6027\uff1b2\uff09\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u90bb\u57df\u4ee4\u724c\u65f6\u5b58\u5728\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\u3002", "method": "\u63d0\u51faDAM-GT\uff0c\u901a\u8fc7\u53cc\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\uff08\u7ed3\u5408\u5c5e\u6027\u805a\u7c7b\u7b56\u7565\uff09\u548c\u65b0\u7684\u6ce8\u610f\u529b\u63a9\u7801\u7b56\u7565\uff0c\u4f18\u5316\u90bb\u57df\u4ee4\u724c\u7684\u751f\u6210\u548c\u76ee\u6807\u8282\u70b9\u4e0e\u90bb\u57df\u4ee4\u724c\u7684\u4ea4\u4e92\u3002", "result": "\u5728\u591a\u79cd\u540c\u8d28\u6027\u6c34\u5e73\u548c\u89c4\u6a21\u7684\u56fe\u4e0a\uff0cDAM-GT\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DAM-GT\u901a\u8fc7\u6539\u8fdb\u90bb\u57df\u4ee4\u724c\u751f\u6210\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56feTransformer\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.17266", "pdf": "https://arxiv.org/pdf/2505.17266", "abs": "https://arxiv.org/abs/2505.17266", "authors": ["Cehao Yang", "Xueyuan Lin", "Chengjin Xu", "Xuhui Jiang", "Xiaojun Wu", "Honghao Liu", "Hui Xiong", "Jian Guo"], "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost.", "AI": {"tldr": "Select2Reason\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u957f\u94fe\u63a8\u7406\u6307\u4ee4\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u95ee\u9898\u96be\u5ea6\u548c\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\uff0c\u4ec5\u970010%\u7684\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u6570\u636e\u5fae\u8c03\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6307\u4ee4\u96c6\uff08\u5982100k\u4ee5\u4e0a\u6837\u672c\uff09\u7684\u8bad\u7ec3\u5f00\u9500\u5927\uff0c\u4e14\u7f3a\u4e4f\u81ea\u52a8\u9009\u62e9\u957f\u94fe\u63a8\u7406\u6307\u4ee4\u7684\u6709\u6548\u7b56\u7565\u3002", "method": "\u63d0\u51faSelect2Reason\u6846\u67b6\uff0c\u7ed3\u5408\u95ee\u9898\u96be\u5ea6\u91cf\u5316\u548c\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u542f\u53d1\u5f0f\uff0c\u4f18\u5148\u9009\u62e9\u9ad8\u6548\u7528\u6837\u672c\u3002", "result": "\u5728OpenR1-Math-220k\u4e0a\uff0c\u4ec5\u752810%\u6570\u636e\u5fae\u8c03\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u6570\u636e\u5fae\u8c03\u548c\u5f00\u6e90\u57fa\u7ebfOpenR1-Qwen-7B\u7684\u6027\u80fd\u3002", "conclusion": "Select2Reason\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u63a8\u7406\u6548\u7387\u548c\u4f4e\u6210\u672c\u9002\u5e94\u6027\u3002", "relevance": 85.0}}
{"id": "2505.17808", "pdf": "https://arxiv.org/pdf/2505.17808", "abs": "https://arxiv.org/abs/2505.17808", "authors": ["Ramanathan Swaminathan"], "title": "An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages in general IEEE format, 8 figures, 4 tables, pdflatex", "summary": "This research work reveals the eye opening wisdom of the hybrid labyrinthine\ndeep learning models synergy born out of combining a trailblazing convolutional\nneural network with a disruptive Vision Transformer, both intertwined together\nwith a radical Cross Attention module. Here, two high yielding datasets for\nartificial intelligence models in detecting glaucoma, namely ACRIMA and\nDrishti, are utilized.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cVision Transformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u534f\u540c\uff0c\u5e76\u5728\u9752\u5149\u773c\u68c0\u6d4b\u6570\u636e\u96c6ACRIMA\u548cDrishti\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cVision Transformer\u7684\u6df7\u5408\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u5728\u533b\u5b66\u56fe\u50cf\uff08\u5982\u9752\u5149\u773c\u68c0\u6d4b\uff09\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cVision Transformer\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5f15\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u4e24\u8005\u534f\u540c\uff0c\u5e76\u5728ACRIMA\u548cDrishti\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u5728\u9752\u5149\u773c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u80fd\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cVision Transformer\u7684\u4f18\u52bf\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.17125", "pdf": "https://arxiv.org/pdf/2505.17125", "abs": "https://arxiv.org/abs/2505.17125", "authors": ["Soyeon Kim", "Namhee Kim", "Yeonwoo Jeong"], "title": "NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction", "categories": ["cs.DB", "cs.AI", "cs.IR"], "comment": "Web Data Record Extraction, Zero-Shot Extraction, Large Language\n  Models (LLMs) Evaluation Framework, Comparative Analysis", "summary": "Effective evaluation of web data record extraction methods is crucial, yet\nhampered by static, domain-specific benchmarks and opaque scoring practices.\nThis makes fair comparison between traditional algorithmic techniques, which\nrely on structural heuristics, and Large Language Model (LLM)-based approaches,\noffering zero-shot extraction across diverse layouts, particularly challenging.\nTo overcome these limitations, we introduce a concrete evaluation framework.\nOur framework systematically generates evaluation datasets from arbitrary MHTML\nsnapshots, annotates XPath-based supervision labels, and employs\nstructure-aware metrics for consistent scoring, specifically preventing text\nhallucination and allowing only for the assessment of positional hallucination.\nIt also incorporates preprocessing strategies to optimize input for LLMs while\npreserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON.\nAdditionally, we created a publicly available synthetic dataset by transforming\nDOM structures and modifying content. We benchmark deterministic heuristic\nalgorithms and off-the-shelf LLMs across these multiple input formats. Our\nbenchmarking shows that Flat JSON input enables LLMs to achieve superior\nextraction accuracy (F1 score of 0.9567) and minimal hallucination compared to\nother input formats like Slimmed HTML and Hierarchical JSON. We establish a\nstandardized foundation for rigorous benchmarking, paving the way for the next\nprincipled advancements in web data record extraction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u6bd4\u8f83\u4f20\u7edf\u7b97\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u7f51\u9875\u6570\u636e\u8bb0\u5f55\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6570\u636e\u96c6\u3001\u6807\u6ce8XPath\u6807\u7b7e\u548c\u4f7f\u7528\u7ed3\u6784\u5316\u6307\u6807\uff0c\u4f18\u5316LLM\u8f93\u5165\u683c\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86Flat JSON\u683c\u5f0f\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u6bd4\u8f83\u4f20\u7edf\u7b97\u6cd5\u548cLLM\u65b9\u6cd5\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5982\u9759\u6001\u57fa\u51c6\u548c\u4e0d\u900f\u660e\u8bc4\u5206\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u751f\u6210\u3001XPath\u6807\u6ce8\u3001\u7ed3\u6784\u5316\u6307\u6807\u548c\u591a\u79cd\u8f93\u5165\u683c\u5f0f\u4f18\u5316\uff08HTML slimming, Hierarchical JSON, Flat JSON\uff09\u3002", "result": "Flat JSON\u8f93\u5165\u683c\u5f0f\u4f7fLLM\u5728\u63d0\u53d6\u51c6\u786e\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08F1\u5206\u65700.9567\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7f51\u9875\u6570\u636e\u8bb0\u5f55\u63d0\u53d6\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 60.0}}
{"id": "2505.17661", "pdf": "https://arxiv.org/pdf/2505.17661", "abs": "https://arxiv.org/abs/2505.17661", "authors": ["Marcel Binz", "Akshay K. Jagadish", "Milena Rmus", "Eric Schulz"], "title": "Automated scientific minimization of regret", "categories": ["cs.LG"], "comment": null, "summary": "We introduce automated scientific minimization of regret (ASMR) -- a\nframework for automated computational cognitive science. Building on the\nprinciples of scientific regret minimization, ASMR leverages Centaur -- a\nrecently proposed foundation model of human cognition -- to identify gaps in an\ninterpretable cognitive model. These gaps are then addressed through automated\nrevisions generated by a language-based reasoning model. We demonstrate the\nutility of this approach in a multi-attribute decision-making task, showing\nthat ASMR discovers cognitive models that predict human behavior at noise\nceiling while retaining interpretability. Taken together, our results highlight\nthe potential of ASMR to automate core components of the cognitive modeling\npipeline.", "AI": {"tldr": "ASMR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Centaur\u6a21\u578b\u548c\u8bed\u8a00\u63a8\u7406\u6a21\u578b\uff0c\u81ea\u52a8\u53d1\u73b0\u548c\u4fee\u6b63\u8ba4\u77e5\u6a21\u578b\u4e2d\u7684\u7f3a\u9677\uff0c\u4ee5\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u65e8\u5728\u81ea\u52a8\u5316\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u6838\u5fc3\u5efa\u6a21\u6d41\u7a0b\uff0c\u586b\u8865\u8ba4\u77e5\u6a21\u578b\u7684\u7a7a\u767d\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528Centaur\uff08\u4eba\u7c7b\u8ba4\u77e5\u57fa\u7840\u6a21\u578b\uff09\u8bc6\u522b\u8ba4\u77e5\u6a21\u578b\u4e2d\u7684\u7f3a\u9677\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4fee\u6b63\u3002", "result": "\u5728\u591a\u5c5e\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0cASMR\u53d1\u73b0\u7684\u8ba4\u77e5\u6a21\u578b\u80fd\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u5e76\u8fbe\u5230\u566a\u58f0\u4e0a\u9650\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ASMR\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u8ba4\u77e5\u5efa\u6a21\u6d41\u7a0b\u6838\u5fc3\u7ec4\u4ef6\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17267", "pdf": "https://arxiv.org/pdf/2505.17267", "abs": "https://arxiv.org/abs/2505.17267", "authors": ["Odysseas S. Chlapanis", "Dimitrios Galanis", "Nikolaos Aletras", "Ion Androutsopoulos"], "title": "GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations", "categories": ["cs.CL"], "comment": "19 pages, 17 figures, submitted to May ARR", "summary": "We introduce GreekBarBench, a benchmark that evaluates LLMs on legal\nquestions across five different legal areas from the Greek Bar exams, requiring\ncitations to statutory articles and case facts. To tackle the challenges of\nfree-text evaluation, we propose a three-dimensional scoring system combined\nwith an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to\nassess the correlation between LLM-judges and human expert evaluations,\nrevealing that simple, span-based rubrics improve their alignment. Our\nsystematic evaluation of 13 proprietary and open-weight LLMs shows that even\nthough the best models outperform average expert scores, they fall short of the\n95th percentile of experts.", "AI": {"tldr": "GreekBarBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u5e0c\u814a\u5f8b\u5e08\u8003\u8bd5\u4e2d\u6cd5\u5f8b\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u7ed3\u5408\u4e86\u4e09\u7ef4\u8bc4\u5206\u7cfb\u7edf\u548cLLM\u4f5c\u4e3a\u88c1\u5224\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u7531\u6587\u672c\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u7ef4\u8bc4\u5206\u7cfb\u7edf\u548cLLM-as-a-judge\u65b9\u6cd5\uff0c\u5f00\u53d1\u5143\u8bc4\u4f30\u57fa\u51c6\u4ee5\u8861\u91cfLLM\u88c1\u5224\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u76f8\u5173\u6027\u3002", "result": "13\u4e2aLLM\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6700\u4f73\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4e13\u5bb6\u5e73\u5747\u6c34\u5e73\uff0c\u4f46\u672a\u8fbe\u5230\u4e13\u5bb6\u524d5%\u7684\u6c34\u5e73\u3002", "conclusion": "LLM\u5728\u6cd5\u5f8b\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u5339\u914d\u9876\u5c16\u4e13\u5bb6\u6c34\u5e73\u3002", "relevance": 70.0}}
{"id": "2505.17812", "pdf": "https://arxiv.org/pdf/2505.17812", "abs": "https://arxiv.org/abs/2505.17812", "authors": ["Boxu Chen", "Ziwei Zheng", "Le Yang", "Zeyu Geng", "Zhengyu Zhao", "Chenhao Lin", "Chao Shen"], "title": "Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable success but\ncontinue to struggle with object hallucination (OH), generating outputs\ninconsistent with visual inputs. While previous work has proposed methods to\nreduce OH, the visual decision-making mechanisms that lead to hallucinations\nremain poorly understood. In this paper, we propose VaLSe, a Vision-aware\nLatent Steering framework that adopts an interpretation-then-mitigation\nstrategy to address OH in LVLMs. By tackling dual challenges of modeling\ncomplex vision-language interactions and eliminating spurious activation\nartifacts, VaLSe can generate visual contribution maps that trace how specific\nvisual inputs influence individual output tokens. These maps reveal the model's\nvision-aware focus regions, which are then used to perform latent space\nsteering, realigning internal representations toward semantically relevant\ncontent and reducing hallucinated outputs. Extensive experiments demonstrate\nthat VaLSe is a powerful interpretability tool and an effective method for\nenhancing model robustness against OH across multiple benchmarks. Furthermore,\nour analysis uncovers limitations in existing OH evaluation metrics,\nunderscoring the need for more nuanced, interpretable, and visually grounded OH\nbenchmarks in future work. Code is available at:\nhttps://github.com/Ziwei-Zheng/VaLSe.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVaLSe\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u6f5c\u5728\u5f15\u5bfc\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u7ed3\u5408\u89e3\u91ca\u548c\u7f13\u89e3\u7b56\u7565\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u7269\u4f53\u5e7b\u89c9\uff08OH\uff09\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u5176\u89c6\u89c9\u51b3\u7b56\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u89e3\u91ca-\u7f13\u89e3\u7b56\u7565\uff0c\u901a\u8fc7\u89c6\u89c9\u8d21\u732e\u56fe\u8ffd\u8e2a\u89c6\u89c9\u8f93\u5165\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u5f15\u5bfc\u8c03\u6574\u5185\u90e8\u8868\u793a\u3002", "result": "VaLSe\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u8f93\u51fa\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u73b0\u6709OH\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "VaLSe\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u91ca\u5de5\u5177\u548c\u9c81\u68d2\u6027\u589e\u5f3a\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u7ec6\u81f4\u7684OH\u8bc4\u4f30\u57fa\u51c6\u3002", "relevance": 70.0}}
{"id": "2505.17662", "pdf": "https://arxiv.org/pdf/2505.17662", "abs": "https://arxiv.org/abs/2505.17662", "authors": ["Tianheng Ling", "Chao Qian", "Lukas Johannes Ha\u00dfler", "Gregor Schiele"], "title": "Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs", "categories": ["cs.LG"], "comment": "6 pages, 5 figures, 1 table, accepted by IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI 2025)", "summary": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5d4c\u5165\u5f0fFPGA\u4e0a\u90e8\u7f72Tiny Transformers\uff0c\u652f\u6301\u4f4e\u81f34\u4f4d\u7684\u91cf\u5316\u8bad\u7ec3\u548c\u786c\u4ef6\u611f\u77e5\u7684\u8d85\u53c2\u6570\u641c\u7d22\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u6216\u9650\u4e8e\u9ad8\u5bc6\u5ea6\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08\u4f4e\u81f34\u4f4d\uff09\u3001\u786c\u4ef6\u611f\u77e5\u8d85\u53c2\u6570\u641c\u7d22\u548c\u81ea\u52a8VHDL\u751f\u6210\uff0c\u652f\u6301\u7d27\u51d1\u7684\u7f16\u7801\u5668-\u4ec5Transformer\u67b6\u6784\u3002", "result": "\u5728\u4e24\u79cd\u5d4c\u5165\u5f0fFPGA\u5e73\u53f0\u4e0a\uff0c\u6846\u67b6\u5b9e\u73b0\u4e86\u4f4e\u81f30.033 mJ/\u63a8\u7406\u7684\u80fd\u6548\u548c\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aTiny Transformers\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.17281", "pdf": "https://arxiv.org/pdf/2505.17281", "abs": "https://arxiv.org/abs/2505.17281", "authors": ["Peilin Wu", "Mian Zhang", "Xinlu Zhang", "Xinya Du", "Zhiyu Zoey Chen"], "title": "Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by enabling dynamic, multi-step reasoning and information\nretrieval. However, these systems often exhibit sub-optimal search behaviors\nlike over-search (retrieving redundant information) and under-search (failing\nto retrieve necessary information), which hinder efficiency and reliability.\nThis work formally defines and quantifies these behaviors, revealing their\nprevalence across multiple QA datasets and agentic RAG systems (e.g., one model\ncould have avoided searching in 27.7% of its search steps). Furthermore, we\ndemonstrate a crucial link between these inefficiencies and the models'\nuncertainty regarding their own knowledge boundaries, where response accuracy\ncorrelates with model's uncertainty in its search decisions. To address this,\nwe propose $\\beta$-GRPO, a reinforcement learning-based training method that\nincorporates confidence threshold to reward high-certainty search decisions.\nExperiments on seven QA benchmarks show that $\\beta$-GRPO enable a 3B model\nwith better agentic RAG ability, outperforming other strong baselines with a 4%\nhigher average exact match score.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08\u03b2-GRPO\uff09\uff0c\u7528\u4e8e\u4f18\u5316Agentic RAG\u7cfb\u7edf\u4e2d\u7684\u641c\u7d22\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u8fc7\u5ea6\u641c\u7d22\u548c\u641c\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Agentic RAG\u7cfb\u7edf\u5728\u52a8\u6001\u591a\u6b65\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff08\u5982\u8fc7\u5ea6\u641c\u7d22\u548c\u641c\u7d22\u4e0d\u8db3\uff09\uff0c\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u5206\u6790\u641c\u7d22\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u4e0e\u6a21\u578b\u77e5\u8bc6\u8fb9\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u03b2-GRPO\u65b9\u6cd5\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u9608\u503c\u4f18\u5316\u641c\u7d22\u51b3\u7b56\u3002", "result": "\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u03b2-GRPO\u4f7f3B\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e73\u5747\u7cbe\u786e\u5339\u914d\u5206\u6570\u63d0\u9ad8\u4e864%\u3002", "conclusion": "\u03b2-GRPO\u901a\u8fc7\u4f18\u5316\u641c\u7d22\u884c\u4e3a\u663e\u8457\u63d0\u5347\u4e86Agentic RAG\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3aLLM\u7684\u52a8\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17821", "pdf": "https://arxiv.org/pdf/2505.17821", "abs": "https://arxiv.org/abs/2505.17821", "authors": ["Shihao Li", "Chenglong Li", "Aihua Zheng", "Jin Tang", "Bin Luo"], "title": "ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Multi-spectral object re-identification (ReID) brings a new perception\nperspective for smart city and intelligent transportation applications,\neffectively addressing challenges from complex illumination and adverse\nweather. However, complex modal differences between heterogeneous spectra pose\nchallenges to efficiently utilizing complementary and discrepancy of spectra\ninformation. Most existing methods fuse spectral data through intricate modal\ninteraction modules, lacking fine-grained semantic understanding of spectral\ninformation (\\textit{e.g.}, text descriptions, part masks, and object\nkeypoints). To solve this challenge, we propose a novel Identity-Conditional\ntext Prompt Learning framework (ICPL), which exploits the powerful cross-modal\nalignment capability of CLIP, to unify different spectral visual features from\ntext semantics. Specifically, we first propose the online prompt learning using\nlearnable text prompt as the identity-level semantic center to bridge the\nidentity semantics of different spectra in online manner. Then, in lack of\nconcrete text descriptions, we propose the multi-spectral identity-condition\nmodule to use identity prototype as spectral identity condition to constraint\nprompt learning. Meanwhile, we construct the alignment loop mutually optimizing\nthe learnable text prompt and spectral visual encoder to avoid online prompt\nlearning disrupting the pre-trained text-image alignment distribution. In\naddition, to adapt to small-scale multi-spectral data and mitigate style\ndifferences between spectra, we propose multi-spectral adapter that employs a\nlow-rank adaption method to learn spectra-specific features. Comprehensive\nexperiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,\nand RGBNT100, demonstrate that the proposed method outperforms the\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684Identity-Conditional\u6587\u672c\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff08ICPL\uff09\uff0c\u7528\u4e8e\u591a\u5149\u8c31\u76ee\u6807\u91cd\u8bc6\u522b\uff08ReID\uff09\uff0c\u901a\u8fc7\u6587\u672c\u8bed\u4e49\u7edf\u4e00\u4e0d\u540c\u5149\u8c31\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u5149\u8c31ReID\u5728\u590d\u6742\u5149\u7167\u548c\u6076\u52a3\u5929\u6c14\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5149\u8c31\u4fe1\u606f\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u5229\u7528CLIP\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faICPL\u6846\u67b6\uff0c\u5305\u62ec\u5728\u7ebf\u63d0\u793a\u5b66\u4e60\u3001\u591a\u5149\u8c31\u8eab\u4efd\u6761\u4ef6\u6a21\u5757\u548c\u5bf9\u9f50\u5faa\u73af\u4f18\u5316\uff0c\u540c\u65f6\u91c7\u7528\u4f4e\u79e9\u9002\u914d\u5668\u9002\u5e94\u5c0f\u89c4\u6a21\u6570\u636e\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5982RGBNT201\uff09\uff0cICPL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ICPL\u901a\u8fc7\u6587\u672c\u8bed\u4e49\u7edf\u4e00\u5149\u8c31\u7279\u5f81\uff0c\u4e3a\u591a\u5149\u8c31ReID\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17133", "pdf": "https://arxiv.org/pdf/2505.17133", "abs": "https://arxiv.org/abs/2505.17133", "authors": ["Shuai Wang", "Song Jiang", "Yizhou Sun", "Judea Pearl", "Ang Li"], "title": "Learning Probabilities of Causation from Finite Population Data", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2502.08858", "summary": "Probabilities of causation play a crucial role in modern decision-making.\nThis paper addresses the challenge of predicting probabilities of causation for\nsubpopulations with \\textbf{insufficient} data using machine learning models.\nTian and Pearl first defined and derived tight bounds for three fundamental\nprobabilities of causation: the probability of necessity and sufficiency (PNS),\nthe probability of sufficiency (PS), and the probability of necessity (PN).\nHowever, estimating these probabilities requires both experimental and\nobservational distributions specific to each subpopulation, which are often\nunavailable or impractical to obtain with limited population-level data.\nTherefore, for most subgroups, the amount of data they have is not enough to\nguarantee the accuracy of their probabilities. Hence, to estimate these\nprobabilities for subpopulations with \\textbf{insufficient} data, we propose\nusing machine learning models that draw insights from subpopulations with\nsufficient data. Our evaluation of multiple machine learning models indicates\nthat, given the population-level data and an appropriate choice of machine\nlearning model and activation function, PNS can be effectively predicted.\nThrough simulation studies on multiple Structured Causal Models (SCMs), we show\nthat our multilayer perceptron (MLP) model with the Mish activation function\nachieves a mean absolute error (MAE) of approximately $0.02$ in predicting PNS\nfor $32,768$ subpopulations across most SCMs using data from only $2,000$\nsubpopulations with known PNS values.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6570\u636e\u4e0d\u8db3\u5b50\u7fa4\u4f53\u7684\u56e0\u679c\u6982\u7387\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6570\u636e\u5145\u8db3\u7684\u5b50\u7fa4\u4f53\u4e2d\u5b66\u4e60\uff0c\u6709\u6548\u9884\u6d4b\u6982\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u6570\u636e\u4e0d\u8db3\u7684\u5b50\u7fa4\u4f53\u4e2d\u9884\u6d4b\u56e0\u679c\u6982\u7387\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b9e\u9a8c\u548c\u89c2\u6d4b\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u548cMish\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ece\u6570\u636e\u5145\u8db3\u7684\u5b50\u7fa4\u4f53\u4e2d\u5b66\u4e60\uff0c\u9884\u6d4b\u6570\u636e\u4e0d\u8db3\u5b50\u7fa4\u4f53\u7684\u56e0\u679c\u6982\u7387\u3002", "result": "\u5728\u591a\u4e2a\u7ed3\u6784\u5316\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u4e2d\uff0cMLP\u6a21\u578b\u5728\u9884\u6d4bPNS\u65f6\u8fbe\u5230\u4e86\u7ea60.02\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u9884\u6d4b\u6570\u636e\u4e0d\u8db3\u5b50\u7fa4\u4f53\u7684\u56e0\u679c\u6982\u7387\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.17664", "pdf": "https://arxiv.org/pdf/2505.17664", "abs": "https://arxiv.org/abs/2505.17664", "authors": ["J\u0119drzej Kozal", "Jan Wasilewski", "Alif Ashrafee", "Bartosz Krawczyk", "Micha\u0142 Wo\u017aniak"], "title": "What is the role of memorization in Continual Learning?", "categories": ["cs.LG"], "comment": null, "summary": "Memorization impacts the performance of deep learning algorithms. Prior works\nhave studied memorization primarily in the context of generalization and\nprivacy. This work studies the memorization effect on incremental learning\nscenarios. Forgetting prevention and memorization seem similar. However, one\nshould discuss their differences. We designed extensive experiments to evaluate\nthe impact of memorization on continual learning. We clarified that learning\nexamples with high memorization scores are forgotten faster than regular\nsamples. Our findings also indicated that memorization is necessary to achieve\nthe highest performance. However, at low memory regimes, forgetting regular\nsamples is more important. We showed that the importance of a high-memorization\nscore sample rises with an increase in the buffer size. We introduced a\nmemorization proxy and employed it in the buffer policy problem to showcase how\nmemorization could be used during incremental training. We demonstrated that\nincluding samples with a higher proxy memorization score is beneficial when the\nbuffer size is large.", "AI": {"tldr": "\u7814\u7a76\u8bb0\u5fc6\u5316\u5bf9\u589e\u91cf\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u8bb0\u5fc6\u5316\u6837\u672c\u9057\u5fd8\u66f4\u5feb\uff0c\u4f46\u8bb0\u5fc6\u5316\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u63a2\u8ba8\u8bb0\u5fc6\u5316\u5728\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u9057\u5fd8\u9884\u9632\u7684\u533a\u522b\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\u8bc4\u4f30\u8bb0\u5fc6\u5316\u5bf9\u6301\u7eed\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u8bb0\u5fc6\u5316\u4ee3\u7406\u5e76\u5e94\u7528\u4e8e\u7f13\u51b2\u533a\u7b56\u7565\u3002", "result": "\u9ad8\u8bb0\u5fc6\u5316\u6837\u672c\u9057\u5fd8\u66f4\u5feb\uff0c\u4f46\u8bb0\u5fc6\u5316\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff1b\u7f13\u51b2\u533a\u5927\u5c0f\u589e\u52a0\u65f6\uff0c\u9ad8\u8bb0\u5fc6\u5316\u6837\u672c\u91cd\u8981\u6027\u4e0a\u5347\u3002", "conclusion": "\u5728\u589e\u91cf\u8bad\u7ec3\u4e2d\uff0c\u5229\u7528\u8bb0\u5fc6\u5316\u4ee3\u7406\u9009\u62e9\u9ad8\u8bb0\u5fc6\u5316\u6837\u672c\u5bf9\u6027\u80fd\u6709\u76ca\uff0c\u5c24\u5176\u5728\u7f13\u51b2\u533a\u8f83\u5927\u65f6\u3002", "relevance": 60.0}}
{"id": "2505.17296", "pdf": "https://arxiv.org/pdf/2505.17296", "abs": "https://arxiv.org/abs/2505.17296", "authors": ["Phat Thanh Dang", "Saahil Thoppay", "Wang Yang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "SELF: Self-Extend the Context Length With Logistic Growth Function", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 3 tables", "summary": "Large language models suffer issues when operated on long contexts that are\nlarger than their training context length due to the standard position encoding\nfor tokens in the attention layer. Tokens a long distance apart will rarely\nhave an effect on each other and long prompts yield unexpected results. To\nsolve this problem, we propose SELF (Self-Extend the Context Length With\nLogistic Growth Function): a solution of grouping consecutive tokens at varying\ngroup sizes using a logistic capacity equation combined with a constant group\nsize at smaller relative distances. Our model had an increase in performance of\nup to 12% compared to the LongLM extension method in LEval (specifically on the\nQwen model). On summarization related tasks in LongBench, our model performed\nup to 6.4% better than LongLM (specifically on the Llama-2-7b model). On\nreading comprehension tasks from LEval, our model performed up to 5.4% better\nthan the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSELF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u6807\u8bb0\u548c\u903b\u8f91\u589e\u957f\u51fd\u6570\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8d85\u51fa\u8bad\u7ec3\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u957f\u6587\u672c\u65f6\uff0c\u7531\u4e8e\u6807\u51c6\u4f4d\u7f6e\u7f16\u7801\u7684\u9650\u5236\uff0c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faSELF\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u5bb9\u91cf\u65b9\u7a0b\u5206\u7ec4\u6807\u8bb0\uff0c\u7ed3\u5408\u56fa\u5b9a\u5c0f\u8ddd\u79bb\u5206\u7ec4\uff0c\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "\u5728LEval\u548cLongBench\u4efb\u52a1\u4e2d\uff0cSELF\u6bd4LongLM\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe12%\uff08Qwen\u6a21\u578b\uff09\u548c6.4%\uff08Llama-2-7b\u6a21\u578b\uff09\u3002", "conclusion": "SELF\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17835", "pdf": "https://arxiv.org/pdf/2505.17835", "abs": "https://arxiv.org/abs/2505.17835", "authors": ["Marc Lalonde", "Hamed Ghodrati"], "title": "VLM Models and Automated Grading of Atopic Dermatitis", "categories": ["cs.CV"], "comment": "10 pages", "summary": "The task of grading atopic dermatitis (or AD, a form of eczema) from patient\nimages is difficult even for trained dermatologists. Research on automating\nthis task has progressed in recent years with the development of deep learning\nsolutions; however, the rapid evolution of multimodal models and more\nspecifically vision-language models (VLMs) opens the door to new possibilities\nin terms of explainable assessment of medical images, including dermatology.\nThis report describes experiments carried out to evaluate the ability of seven\nVLMs to assess the severity of AD on a set of test images.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e03\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bc4\u4f30\u7279\u5e94\u6027\u76ae\u708e\uff08AD\uff09\u4e25\u91cd\u7a0b\u5ea6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7279\u5e94\u6027\u76ae\u708e\u7684\u8bc4\u4f30\u5bf9\u76ae\u80a4\u79d1\u533b\u751f\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u591a\u6a21\u6001\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u533b\u5b66\u56fe\u50cf\u7684\u53ef\u89e3\u91ca\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e03\u79cdVLMs\u5728AD\u6d4b\u8bd5\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u672a\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\u3002", "conclusion": "VLMs\u4e3a\u533b\u5b66\u56fe\u50cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "relevance": 30.0}}
{"id": "2505.17670", "pdf": "https://arxiv.org/pdf/2505.17670", "abs": "https://arxiv.org/abs/2505.17670", "authors": ["Wenyi Wu", "Zixuan Song", "Kun Zhou", "Yifei Shao", "Zhiting Hu", "Biwei Huang"], "title": "Towards General Continuous Memory for Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Language models (LMs) and their extension, vision-language models (VLMs),\nhave achieved remarkable performance across various tasks. However, they still\nstruggle with complex reasoning tasks that require multimodal or multilingual\nreal-world knowledge. To support such capabilities, an external memory system\nthat can efficiently provide relevant multimodal information is essential.\nExisting approaches generally concatenate image and text tokens into a long\nsequence as memory, which, however, may drastically increase context length and\neven degrade performance. In contrast, we propose using continuous memory, a\ncompact set of dense embeddings to more effectively and efficiently represent\nmultimodal and multilingual knowledge. Our key insight is that a VLM can serve\nas its own continuous memory encoder. We empirically show that this design\nimproves performance on complex multimodal reasoning tasks. Building on this,\nwe introduce a data-efficient and parameter-efficient method to fine-tune the\nVLM into a memory encoder, requiring only 1.2% of the model's parameters and a\nsmall corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes\nVLM's original capabilities to encode arbitrary multimodal and multilingual\nknowledge into just 8 continuous embeddings. Since the inference-time VLM\nremains frozen, our memory module is plug-and-play and can be flexibly\nintegrated as needed. Extensive experiments across eight multimodal reasoning\nbenchmarks demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoMEM\u7684\u8fde\u7eed\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7d27\u51d1\u7684\u5bc6\u96c6\u5d4c\u5165\u8868\u793a\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u77e5\u8bc6\uff0c\u4ec5\u9700\u5c11\u91cf\u53c2\u6570\u548c\u6570\u636e\u5373\u53ef\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u591a\u6a21\u6001\u6216\u591a\u8bed\u8a00\u77e5\u8bc6\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u56fe\u50cf\u548c\u6587\u672c\u6807\u8bb0\u62fc\u63a5\u4e3a\u957f\u5e8f\u5217\uff0c\u4f46\u4f1a\u589e\u52a0\u4e0a\u4e0b\u6587\u957f\u5ea6\u5e76\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u8fde\u7eed\u8bb0\u5fc6\uff08compact set of dense embeddings\uff09\u9ad8\u6548\u8868\u793a\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u77e5\u8bc6\uff0c\u5e76\u5229\u7528VLM\u81ea\u8eab\u4f5c\u4e3a\u8fde\u7eed\u8bb0\u5fc6\u7f16\u7801\u5668\u3002\u901a\u8fc7\u4ec5\u5fae\u8c031.2%\u7684\u6a21\u578b\u53c2\u6570\u548c15.6K\u81ea\u5408\u6210\u6837\u672c\uff0c\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u548c\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u3002", "result": "\u5728\u516b\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoMEM\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CoMEM\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bb0\u5fc6\u6a21\u5757\uff0c\u80fd\u591f\u7075\u6d3b\u96c6\u6210\u5230\u73b0\u6709VLM\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002", "relevance": 85.0}}
{"id": "2505.17306", "pdf": "https://arxiv.org/pdf/2505.17306", "abs": "https://arxiv.org/abs/2505.17306", "authors": ["Xinpeng Wang", "Mingyang Wang", "Yihong Liu", "Hinrich Sch\u00fctze", "Barbara Plank"], "title": "Refusal Direction is Universal Across Safety-Aligned Languages", "categories": ["cs.CL"], "comment": null, "summary": "Refusal mechanisms in large language models (LLMs) are essential for ensuring\nsafety. Recent research has revealed that refusal behavior can be mediated by a\nsingle direction in activation space, enabling targeted interventions to bypass\nrefusals. While this is primarily demonstrated in an English-centric context,\nappropriate refusal behavior is important for any language, but poorly\nunderstood. In this paper, we investigate the refusal behavior in LLMs across\n14 languages using PolyRefuse, a multilingual safety dataset created by\ntranslating malicious and benign English prompts into these languages. We\nuncover the surprising cross-lingual universality of the refusal direction: a\nvector extracted from English can bypass refusals in other languages with\nnear-perfect effectiveness, without any additional fine-tuning. Even more\nremarkably, refusal directions derived from any safety-aligned language\ntransfer seamlessly to others. We attribute this transferability to the\nparallelism of refusal vectors across languages in the embedding space and\nidentify the underlying mechanism behind cross-lingual jailbreaks. These\nfindings provide actionable insights for building more robust multilingual\nsafety defenses and pave the way for a deeper mechanistic understanding of\ncross-lingual vulnerabilities in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u7684\u62d2\u7edd\u884c\u4e3a\u5728\u591a\u79cd\u8bed\u8a00\u4e2d\u5177\u6709\u8de8\u8bed\u8a00\u901a\u7528\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u901a\u8fc7\u5355\u4e00\u65b9\u5411\u7ed5\u8fc7\u62d2\u7edd\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLMs\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u4ee5\u63d0\u5347\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528PolyRefuse\u6570\u636e\u96c6\uff0c\u572814\u79cd\u8bed\u8a00\u4e2d\u6d4b\u8bd5\u62d2\u7edd\u884c\u4e3a\uff0c\u5206\u6790\u62d2\u7edd\u65b9\u5411\u7684\u8de8\u8bed\u8a00\u901a\u7528\u6027\u3002", "result": "\u62d2\u7edd\u65b9\u5411\u5728\u8bed\u8a00\u95f4\u5177\u6709\u9ad8\u5ea6\u53ef\u8fc1\u79fb\u6027\uff0c\u82f1\u8bed\u63d0\u53d6\u7684\u5411\u91cf\u53ef\u7ed5\u8fc7\u5176\u4ed6\u8bed\u8a00\u7684\u62d2\u7edd\u673a\u5236\u3002", "conclusion": "\u62d2\u7edd\u884c\u4e3a\u7684\u8de8\u8bed\u8a00\u901a\u7528\u6027\u4e3a\u591a\u8bed\u8a00\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63ed\u793a\u4e86LLMs\u7684\u8de8\u8bed\u8a00\u6f0f\u6d1e\u673a\u5236\u3002", "relevance": 85.0}}
{"id": "2505.17844", "pdf": "https://arxiv.org/pdf/2505.17844", "abs": "https://arxiv.org/abs/2505.17844", "authors": ["Fabian Deuser", "Philipp Hausenblas", "Hannah Schieber", "Daniel Roth", "Martin Werner", "Norbert Oswald"], "title": "Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning is a representational learning paradigm in which a\nneural network maps data elements to feature vectors. It improves the feature\nspace by forming lots with an anchor and examples that are either positive or\nnegative based on class similarity. Hard negative examples, which are close to\nthe anchor in the feature space but from a different class, improve learning\nperformance. Finding such examples of high quality efficiently in large,\nhigh-dimensional datasets is computationally challenging. In this paper, we\npropose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes\nreal-valued feature vectors into binary representations for approximate nearest\nneighbor search. We investigate its theoretical properties and evaluate it on\nseveral datasets from textual and visual domain. Our approach achieves\ncomparable or better performance while requiring significantly less computation\nthan existing hard negative mining strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u53cb\u597d\u7684\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\uff08LSH\uff09\u65b9\u6848\uff0c\u7528\u4e8e\u9ad8\u6548\u5bfb\u627e\u9ad8\u8d28\u91cf\u786c\u8d1f\u6837\u672c\uff0c\u63d0\u5347\u5bf9\u6bd4\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u96c6\u4e2d\u9ad8\u6548\u5bfb\u627e\u9ad8\u8d28\u91cf\u7684\u786c\u8d1f\u6837\u672c\u662f\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528LSH\u65b9\u6848\u5c06\u5b9e\u503c\u7279\u5f81\u5411\u91cf\u91cf\u5316\u4e3a\u4e8c\u8fdb\u5236\u8868\u793a\uff0c\u8fdb\u884c\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u786c\u8d1f\u6837\u672c\u6316\u6398\u7b56\u7565\u76f8\u5f53\uff0c\u4e14\u8ba1\u7b97\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u63d0\u51fa\u7684LSH\u65b9\u6848\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u786c\u8d1f\u6837\u672c\u6316\u6398\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17694", "pdf": "https://arxiv.org/pdf/2505.17694", "abs": "https://arxiv.org/abs/2505.17694", "authors": ["Zhibin Wang", "Rui Ning", "Chao Fang", "Zhonghui Zhang", "Xi Lin", "Shaobo Ma", "Mo Zhou", "Xue Li", "Zhongfeng Wang", "Chengying Huan", "Rong Gu", "Kun Yang", "Guihai Chen", "Sheng Zhong", "Chen Tian"], "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding", "categories": ["cs.LG"], "comment": null, "summary": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlashForge\uff0c\u4e00\u79cd\u4e13\u7528\u4e8e\u89e3\u7801\u9636\u6bb5\u5171\u4eab\u524d\u7f00\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u4f18\u5316\u5185\u6838\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u3002", "motivation": "\u5171\u4eab\u524d\u7f00\u5728\u89e3\u7801\u9636\u6bb5\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u5b58\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u5904\u7406\u5171\u4eabKV\u7f13\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3002", "method": "\u63d0\u51faFlashForge\uff0c\u5305\u542b\u5171\u4eab\u524d\u7f00\u6ce8\u610f\u529b\u5185\u6838\u548c\u8d1f\u8f7d\u5747\u8861\u673a\u5236\uff0c\u4f18\u5316\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u5e76\u5229\u7528\u5e76\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFlashForge\u5728\u89e3\u7801\u9636\u6bb5\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u6bd4FlashDecoding\u5feb1.9\u500d\uff0c\u5185\u5b58\u8bbf\u95ee\u51cf\u5c11120.9\u500d\u3002", "conclusion": "FlashForge\u6709\u6548\u89e3\u51b3\u4e86\u5171\u4eab\u524d\u7f00\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.17320", "pdf": "https://arxiv.org/pdf/2505.17320", "abs": "https://arxiv.org/abs/2505.17320", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "title": "Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Synthesizing expressive Japanese character speech poses unique challenges due\nto pitch-accent sensitivity and stylistic variability. This paper benchmarks\ntwo open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra\n(SBV2JE)--on in-domain, character-driven Japanese speech. Using three\ncharacter-specific datasets, we evaluate models across naturalness (mean\nopinion and comparative mean opinion score), intelligibility (word error rate),\nand speaker consistency. SBV2JE matches human ground truth in naturalness (MOS\n4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS.\nEnhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE\nproves effective for applications like language learning and character dialogue\ngeneration, despite higher computational demands.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86VITS\u548cSBV2JE\u4e24\u79cd\u5f00\u6e90\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u5728\u65e5\u8bed\u89d2\u8272\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u8868\u73b0\uff0cSBV2JE\u5728\u81ea\u7136\u5ea6\u3001\u6e05\u6670\u5ea6\u548c\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u65e5\u8bed\u89d2\u8272\u8bed\u97f3\u5408\u6210\u4e2d\u97f3\u9ad8\u654f\u611f\u6027\u548c\u98ce\u683c\u591a\u6837\u6027\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u89d2\u8272\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u81ea\u7136\u5ea6\uff08MOS\u548cCMOS\uff09\u3001\u6e05\u6670\u5ea6\uff08WER\uff09\u548c\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u3002SBV2JE\u7ed3\u5408\u97f3\u9ad8\u63a7\u5236\u548cWavLM\u5224\u522b\u5668\u3002", "result": "SBV2JE\u5728\u81ea\u7136\u5ea6\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff08MOS 4.37 vs. 4.38\uff09\uff0cWER\u66f4\u4f4e\uff0cCMOS\u7565\u4f18\u3002", "conclusion": "SBV2JE\u9002\u7528\u4e8e\u8bed\u8a00\u5b66\u4e60\u548c\u89d2\u8272\u5bf9\u8bdd\u751f\u6210\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u8f83\u9ad8\u3002", "relevance": 30.0}}
{"id": "2505.17867", "pdf": "https://arxiv.org/pdf/2505.17867", "abs": "https://arxiv.org/abs/2505.17867", "authors": ["Konstantinos Spathis", "Nikolaos Kardaris", "Petros Maragos"], "title": "Multi-task Learning For Joint Action and Gesture Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In practical applications, computer vision tasks often need to be addressed\nsimultaneously. Multitask learning typically achieves this by jointly training\na single deep neural network to learn shared representations, providing\nefficiency and improving generalization. Although action and gesture\nrecognition are closely related tasks, since they focus on body and hand\nmovements, current state-of-the-art methods handle them separately. In this\npaper, we show that employing a multi-task learning paradigm for action and\ngesture recognition results in more efficient, robust and generalizable visual\nrepresentations, by leveraging the synergies between these tasks. Extensive\nexperiments on multiple action and gesture datasets demonstrate that handling\nactions and gestures in a single architecture can achieve better performance\nfor both tasks in comparison to their single-task learning variants.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u6765\u540c\u65f6\u5904\u7406\u52a8\u4f5c\u548c\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5c06\u52a8\u4f5c\u548c\u624b\u52bf\u8bc6\u522b\u5206\u5f00\u5904\u7406\uff0c\u800c\u8fd9\u4e24\u79cd\u4efb\u52a1\u5728\u8eab\u4f53\u548c\u624b\u90e8\u8fd0\u52a8\u4e0a\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u534f\u540c\u6548\u5e94\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u548c\u901a\u7528\u7684\u89c6\u89c9\u8868\u793a\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5355\u4e00\u67b6\u6784\u8054\u5408\u8bad\u7ec3\u52a8\u4f5c\u548c\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\uff0c\u5229\u7528\u4efb\u52a1\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u52a8\u4f5c\u548c\u624b\u52bf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5728\u4e24\u79cd\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5747\u4f18\u4e8e\u5355\u4efb\u52a1\u5b66\u4e60\u53d8\u4f53\u3002", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5904\u7406\u52a8\u4f5c\u548c\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17695", "pdf": "https://arxiv.org/pdf/2505.17695", "abs": "https://arxiv.org/abs/2505.17695", "authors": ["Dong-Hee Kim", "Hyunjee Song", "Donghyun Kim"], "title": "SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the advances in Referring Expression Segmentation (RES) benchmarks,\ntheir evaluation protocols remain constrained, primarily focusing on either\nsingle targets with short queries (containing minimal attributes) or multiple\ntargets from distinctly different queries on a single domain. This limitation\nsignificantly hinders the assessment of more complex reasoning capabilities in\nRES models. We introduce WildRES, a novel benchmark that incorporates long\nqueries with diverse attributes and non-distinctive queries for multiple\ntargets. This benchmark spans diverse application domains, including autonomous\ndriving environments and robotic manipulation scenarios, thus enabling more\nrigorous evaluation of complex reasoning capabilities in real-world settings.\nOur analysis reveals that current RES models demonstrate substantial\nperformance deterioration when evaluated on WildRES. To address this challenge,\nwe introduce SynRES, an automated pipeline generating densely paired\ncompositional synthetic training data through three innovations: (1) a dense\ncaption-driven synthesis for attribute-rich image-mask-expression triplets, (2)\nreliable semantic alignment mechanisms rectifying caption-pseudo mask\ninconsistencies via Image-Text Aligned Grouping, and (3) domain-aware\naugmentations incorporating mosaic composition and superclass replacement to\nemphasize generalization ability and distinguishing attributes over object\ncategories. Experimental results demonstrate that models trained with SynRES\nachieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and\n3.8% on WildRES-DS. Code and datasets are available at\nhttps://github.com/UTLLab/SynRES.", "AI": {"tldr": "WildRES\u662f\u4e00\u4e2a\u65b0\u7684RES\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u800cSynRES\u662f\u4e00\u4e2a\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RES\u57fa\u51c6\u7684\u8bc4\u4f30\u534f\u8bae\u53d7\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u548c\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5f15\u5165WildRES\u57fa\u51c6\uff0c\u5305\u542b\u957f\u67e5\u8be2\u548c\u591a\u6837\u5316\u5c5e\u6027\uff1b\u63d0\u51faSynRES\u7ba1\u9053\uff0c\u901a\u8fc7\u5bc6\u96c6\u6807\u6ce8\u9a71\u52a8\u5408\u6210\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u57df\u611f\u77e5\u589e\u5f3a\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728WildRES\u57fa\u51c6\u4e0a\uff0c\u5f53\u524dRES\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u4f7f\u7528SynRES\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff08gIoU\u63d0\u9ad82.0%-3.8%\uff09\u3002", "conclusion": "WildRES\u548cSynRES\u4e3aRES\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17322", "pdf": "https://arxiv.org/pdf/2505.17322", "abs": "https://arxiv.org/abs/2505.17322", "authors": ["Jiachen Jiang", "Yuxin Dong", "Jinxin Zhou", "Zhihui Zhu"], "title": "From Compression to Expansion: A Layerwise Analysis of In-Context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks without weight updates by learning from demonstration sequences.\nWhile ICL shows strong empirical performance, its internal representational\nmechanisms are not yet well understood. In this work, we conduct a statistical\ngeometric analysis of ICL representations to investigate how task-specific\ninformation is captured across layers. Our analysis reveals an intriguing\nphenomenon, which we term *Layerwise Compression-Expansion*: early layers\nprogressively produce compact and discriminative representations that encode\ntask information from the input demonstrations, while later layers expand these\nrepresentations to incorporate the query and generate the prediction. This\nphenomenon is observed consistently across diverse tasks and a range of\ncontemporary LLM architectures. We demonstrate that it has important\nimplications for ICL performance -- improving with model size and the number of\ndemonstrations -- and for robustness in the presence of noisy examples. To\nfurther understand the effect of the compact task representation, we propose a\nbias-variance decomposition and provide a theoretical analysis showing how\nattention mechanisms contribute to reducing both variance and bias, thereby\nenhancing performance as the number of demonstrations increases. Our findings\nreveal an intriguing layerwise dynamic in ICL, highlight how structured\nrepresentations emerge within LLMs, and showcase that analyzing internal\nrepresentations can facilitate a deeper understanding of model behavior.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7edf\u8ba1\u51e0\u4f55\u5206\u6790\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u7684\u5185\u90e8\u8868\u793a\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5c42\u95f4\u538b\u7f29-\u6269\u5c55\u201d\u7684\u73b0\u8c61\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u6027\u80fd\u548c\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1ICL\u5728\u5b9e\u8bc1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u8868\u793a\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793aICL\u5982\u4f55\u901a\u8fc7\u5c42\u95f4\u52a8\u6001\u6355\u83b7\u4efb\u52a1\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86ICL\u5728\u4e0d\u540c\u5c42\u4e2d\u7684\u8868\u793a\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u504f\u7f6e-\u65b9\u5dee\u5206\u89e3\u4ee5\u7406\u89e3\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0ICL\u4e2d\u5b58\u5728\u201c\u5c42\u95f4\u538b\u7f29-\u6269\u5c55\u201d\u73b0\u8c61\uff0c\u65e9\u671f\u5c42\u538b\u7f29\u4efb\u52a1\u4fe1\u606f\uff0c\u540e\u671f\u5c42\u6269\u5c55\u4ee5\u751f\u6210\u9884\u6d4b\u3002\u8fd9\u4e00\u73b0\u8c61\u4e0e\u6a21\u578b\u89c4\u6a21\u548c\u6f14\u793a\u6570\u91cf\u6b63\u76f8\u5173\uff0c\u4e14\u6709\u52a9\u4e8e\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86ICL\u7684\u5c42\u95f4\u52a8\u6001\uff0c\u8868\u660e\u5206\u6790\u5185\u90e8\u8868\u793a\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u4e3a\u6539\u8fdbICL\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "relevance": 85.0}}
{"id": "2505.17881", "pdf": "https://arxiv.org/pdf/2505.17881", "abs": "https://arxiv.org/abs/2505.17881", "authors": ["Wenjin Qin", "Hailin Wang", "Hao Shu", "Feng Zhang", "Jianjun Wang", "Xiangyong Cao", "Xi-Le Zhao", "Gemine Vivone"], "title": "Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, tensor decomposition-based approaches for hyperspectral\nanomaly detection (HAD) have gained significant attention in the field of\nremote sensing. However, existing methods often fail to fully leverage both the\nglobal correlations and local smoothness of the background components in\nhyperspectral images (HSIs), which exist in both the spectral and spatial\ndomains. This limitation results in suboptimal detection performance. To\nmitigate this critical issue, we put forward a novel HAD method named\nHAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)\nfactors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first\ndecomposed into background and anomaly components. The TR decomposition is then\nemployed to capture the spatial-spectral correlations within the background\ncomponent. Additionally, we introduce a unified and efficient nonconvex\nregularizer, induced by tensor singular value decomposition (TSVD), to\nsimultaneously encode the low-rankness and sparsity of the 3-D gradient TR\nfactors into a unique concise form. The above characterization scheme enables\nthe interpretable gradient TR factors to inherit the low-rankness and\nsmoothness of the original background. To further enhance anomaly detection, we\ndesign a generalized nonconvex regularization term to exploit the group\nsparsity of the anomaly component. To solve the resulting doubly nonconvex\nmodel, we develop a highly efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) framework. Experimental\nresults on several benchmark datasets demonstrate that our proposed method\noutperforms existing state-of-the-art (SOTA) approaches in terms of detection\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684HAD\u65b9\u6cd5HAD-EUNTRFR\uff0c\u901a\u8fc7\u975e\u51f8\u5f20\u91cf\u73af\u6b63\u5219\u5316\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u9ad8\u5149\u8c31\u56fe\u50cf\u80cc\u666f\u7684\u5168\u5c40\u76f8\u5173\u6027\u548c\u5c40\u90e8\u5e73\u6ed1\u6027\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faHAD-EUNTRFR\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f20\u91cf\u73af\u5206\u89e3\u548c\u975e\u51f8\u6b63\u5219\u5316\uff0c\u4f18\u5316\u80cc\u666f\u548c\u5f02\u5e38\u7ec4\u4ef6\u7684\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "HAD-EUNTRFR\u901a\u8fc7\u9ad8\u6548\u4f18\u5316\u7b97\u6cd5\u548c\u975e\u51f8\u6b63\u5219\u5316\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 20.0}}
{"id": "2505.17701", "pdf": "https://arxiv.org/pdf/2505.17701", "abs": "https://arxiv.org/abs/2505.17701", "authors": ["Jaewon Cheon", "Pilsung Kang"], "title": "COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The growing size of large language models has created significant\ncomputational inefficiencies. To address this challenge, sparse activation\nmethods selectively deactivates non-essential parameters during inference,\nreducing computational costs in FFNN layers. While existing methods focus on\nnon-linear gating mechanisms, we hypothesize that the sparsity of the FFNN\nlayer lies globally in the form of a linear combination over its internal down\nprojection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,\nleveraging indirect coefficients, and D-COUNTDOWN, utilizing direct\ncoefficients of the linear combination. Experimental results demonstrate that\nD-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%\nideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%\nbetter performance preservation compared to existing methods. Our specialized\nkernel implementations effectively realize these theoretical gains into\nsubstantial real-world acceleration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u7a00\u758f\u6fc0\u6d3b\u65b9\u6cd5\uff08M-COUNTDOWN\u548cD-COUNTDOWN\uff09\uff0c\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u51cf\u5c11FFNN\u5c42\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u975e\u7ebf\u6027\u95e8\u63a7\u673a\u5236\uff0c\u800c\u4f5c\u8005\u8ba4\u4e3a\u7a00\u758f\u6027\u53ef\u80fd\u5b58\u5728\u4e8eFFNN\u5c42\u7684\u5168\u5c40\u7ebf\u6027\u7ec4\u5408\u4e2d\u3002", "method": "\u63d0\u51faM-COUNTDOWN\uff08\u95f4\u63a5\u7cfb\u6570\uff09\u548cD-COUNTDOWN\uff08\u76f4\u63a5\u7cfb\u6570\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u9009\u62e9\u6027\u6fc0\u6d3b\u53c2\u6570\u3002", "result": "D-COUNTDOWN\u53ef\u51cf\u5c1190%\u8ba1\u7b97\u91cf\uff0c\u6027\u80fd\u635f\u5931\u4ec5\u4e3a5.5%\uff1bM-COUNTDOWN\u65e0\u9700\u9884\u6d4b\u5668\uff0c\u6027\u80fd\u4fdd\u7559\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad829.4%\u3002", "conclusion": "\u901a\u8fc7\u4e13\u7528\u5185\u6838\u5b9e\u73b0\uff0c\u7406\u8bba\u6536\u76ca\u8f6c\u5316\u4e3a\u5b9e\u9645\u52a0\u901f\u6548\u679c\u3002", "relevance": 85.0}}
{"id": "2505.17327", "pdf": "https://arxiv.org/pdf/2505.17327", "abs": "https://arxiv.org/abs/2505.17327", "authors": ["Soren DeHaan", "Yuanze Liu", "Johan Bollen", "Sa'ul A. Blanco"], "title": "GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT", "68U99", "I.2.7"], "comment": "13 pages", "summary": "The proliferation of Large Language Models (LLMs) in late 2022 has impacted\nacademic writing, threatening credibility, and causing institutional\nuncertainty. We seek to determine the degree to which LLMs are used to generate\ncritical text as opposed to being used for editing, such as checking for\ngrammar errors or inappropriate phrasing. In our study, we analyze arXiv papers\nfor stylistic segmentation, which we measure by varying a PELT threshold\nagainst a Bayesian classifier trained on GPT-regenerated text. We find that\nLLM-attributed language is not predictive of stylistic segmentation, suggesting\nthat when authors use LLMs, they do so uniformly, reducing the risk of\nhallucinations being introduced into academic preprints.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0LLM\u751f\u6210\u7684\u8bed\u8a00\u4e0e\u98ce\u683c\u5206\u6bb5\u65e0\u5173\uff0c\u8868\u660e\u4f5c\u8005\u4f7f\u7528LLMs\u65f6\u8f83\u4e3a\u4e00\u81f4\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "LLMs\u7684\u666e\u53ca\u5bf9\u5b66\u672f\u5199\u4f5c\u7684\u8bda\u4fe1\u548c\u673a\u6784\u4fe1\u4efb\u6784\u6210\u5a01\u80c1\uff0c\u7814\u7a76\u65e8\u5728\u533a\u5206LLMs\u7528\u4e8e\u751f\u6210\u5173\u952e\u6587\u672c\u8fd8\u662f\u4ec5\u7528\u4e8e\u7f16\u8f91\u3002", "method": "\u901a\u8fc7\u5206\u6790arXiv\u8bba\u6587\uff0c\u4f7f\u7528PELT\u9608\u503c\u548c\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u5bf9GPT\u751f\u6210\u6587\u672c\u8fdb\u884c\u98ce\u683c\u5206\u6bb5\u6d4b\u91cf\u3002", "result": "LLM\u751f\u6210\u7684\u8bed\u8a00\u4e0e\u98ce\u683c\u5206\u6bb5\u65e0\u663e\u8457\u5173\u8054\uff0c\u8868\u660e\u4f5c\u8005\u4f7f\u7528LLMs\u65f6\u8f83\u4e3a\u4e00\u81f4\u3002", "conclusion": "LLMs\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u4f7f\u7528\u8f83\u4e3a\u4e00\u81f4\uff0c\u964d\u4f4e\u4e86\u5e7b\u89c9\u5f15\u5165\u7684\u98ce\u9669\u3002", "relevance": 60.0}}
{"id": "2505.17884", "pdf": "https://arxiv.org/pdf/2505.17884", "abs": "https://arxiv.org/abs/2505.17884", "authors": ["Nikita Ivanov", "Mark Klimov", "Dmitry Glukhikh", "Tatiana Chernysheva", "Igor Glukhikh"], "title": "Track Anything Annotate: Video annotation and dataset generation of computer vision models", "categories": ["cs.CV"], "comment": "9 pages, 11 figures", "summary": "Modern machine learning methods require significant amounts of labelled data,\nmaking the preparation process time-consuming and resource-intensive. In this\npaper, we propose to consider the process of prototyping a tool for annotating\nand generating training datasets based on video tracking and segmentation. We\nexamine different approaches to solving this problem, from technology selection\nthrough to final implementation. The developed prototype significantly\naccelerates dataset generation compared to manual annotation. All resources are\navailable at https://github.com/lnikioffic/track-anything-annotate", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u8ddf\u8e2a\u548c\u5206\u5272\u7684\u6807\u6ce8\u4e0e\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5de5\u5177\u539f\u578b\uff0c\u663e\u8457\u52a0\u901f\u4e86\u6570\u636e\u96c6\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4ece\u6280\u672f\u9009\u62e9\u5230\u6700\u7ec8\u5b9e\u73b0\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u578b\u5de5\u5177\u3002", "result": "\u5f00\u53d1\u7684\u5de5\u5177\u539f\u578b\u663e\u8457\u52a0\u901f\u4e86\u6570\u636e\u96c6\u751f\u6210\uff0c\u76f8\u6bd4\u624b\u52a8\u6807\u6ce8\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u6570\u636e\u6807\u6ce8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u8d44\u6e90\u5df2\u5f00\u6e90\u3002", "relevance": 40.0}}
{"id": "2505.17708", "pdf": "https://arxiv.org/pdf/2505.17708", "abs": "https://arxiv.org/abs/2505.17708", "authors": ["Dingling Yao", "Shimeng Huang", "Riccardo Cadei", "Kun Zhang", "Francesco Locatello"], "title": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations", "categories": ["cs.LG"], "comment": "22 pages, 12 figures, 2 tables", "summary": "Causal reasoning and discovery, two fundamental tasks of causal analysis,\noften face challenges in applications due to the complexity, noisiness, and\nhigh-dimensionality of real-world data. Despite recent progress in identifying\nlatent causal structures using causal representation learning (CRL), what makes\nlearned representations useful for causal downstream tasks and how to evaluate\nthem are still not well understood. In this paper, we reinterpret CRL using a\nmeasurement model framework, where the learned representations are viewed as\nproxy measurements of the latent causal variables. Our approach clarifies the\nconditions under which learned representations support downstream causal\nreasoning and provides a principled basis for quantitatively assessing the\nquality of representations using a new Test-based Measurement EXclusivity\n(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,\nincluding numerical simulations and real-world ecological video analysis,\ndemonstrating that the proposed framework and corresponding score effectively\nassess the identification of learned representations and their usefulness for\ncausal downstream tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u91cf\u6a21\u578b\u6846\u67b6\u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60\uff08CRL\uff09\u65b9\u6cd5\uff0c\u5b9a\u4e49\u4e86\u5b66\u4e60\u8868\u793a\u5bf9\u4e0b\u6e38\u56e0\u679c\u4efb\u52a1\u7684\u652f\u6301\u6761\u4ef6\uff0c\u5e76\u5f15\u5165T-MEX\u8bc4\u5206\u5b9a\u91cf\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u56e0\u679c\u8868\u793a\u5b66\u4e60\u4e2d\u5b66\u4e60\u8868\u793a\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u7528\u6027\u53ca\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u6846\u67b6\u91cd\u65b0\u89e3\u91caCRL\uff0c\u5c06\u5b66\u4e60\u8868\u793a\u89c6\u4e3a\u6f5c\u5728\u56e0\u679c\u53d8\u91cf\u7684\u4ee3\u7406\u6d4b\u91cf\uff0c\u63d0\u51faT-MEX\u8bc4\u5206\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\u3002", "result": "\u5728\u6570\u503c\u6a21\u62df\u548c\u771f\u5b9e\u751f\u6001\u89c6\u9891\u5206\u6790\u4e2d\u9a8c\u8bc1\u4e86T-MEX\u8bc4\u5206\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u8bc4\u5206\u80fd\u6709\u6548\u8bc4\u4f30\u5b66\u4e60\u8868\u793a\u7684\u8d28\u91cf\u53ca\u5176\u5bf9\u4e0b\u6e38\u56e0\u679c\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17332", "pdf": "https://arxiv.org/pdf/2505.17332", "abs": "https://arxiv.org/abs/2505.17332", "authors": ["Hitesh Laxmichand Patel", "Amit Agarwal", "Arion Das", "Bhargava Kumar", "Srikant Panda", "Priyaranjan Pattnayak", "Taki Hasan Rafi", "Tejaswini Kumar", "Dong-Kyu Chae"], "title": "SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "I.2.7; I.2.6"], "comment": "Published in the Proceedings of the 2025 Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL\n  2025), Industry Track, pages 558-582", "summary": "Enterprise customers are increasingly adopting Large Language Models (LLMs)\nfor critical communication tasks, such as drafting emails, crafting sales\npitches, and composing casual messages. Deploying such models across different\nregions requires them to understand diverse cultural and linguistic contexts\nand generate safe and respectful responses. For enterprise applications, it is\ncrucial to mitigate reputational risks, maintain trust, and ensure compliance\nby effectively identifying and handling unsafe or offensive language. To\naddress this, we introduce SweEval, a benchmark simulating real-world scenarios\nwith variations in tone (positive or negative) and context (formal or\ninformal). The prompts explicitly instruct the model to include specific swear\nwords while completing the task. This benchmark evaluates whether LLMs comply\nwith or resist such inappropriate instructions and assesses their alignment\nwith ethical frameworks, cultural nuances, and language comprehension\ncapabilities. In order to advance research in building ethically aligned AI\nsystems for enterprise use and beyond, we release the dataset and code:\nhttps://github.com/amitbcp/multilingual_profanity.", "AI": {"tldr": "SweEval\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5904\u7406\u4e0d\u5f53\u8bed\u8a00\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5173\u6ce8\u6587\u5316\u591a\u6837\u6027\u548c\u4f26\u7406\u5bf9\u9f50\u3002", "motivation": "\u4f01\u4e1a\u5e94\u7528LLMs\u9700\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u5b89\u5168\u3001\u5c0a\u91cd\u6587\u5316\u5dee\u5f02\uff0c\u907f\u514d\u58f0\u8a89\u98ce\u9669\u3002", "method": "\u63d0\u51faSweEval\u57fa\u51c6\uff0c\u6a21\u62df\u4e0d\u540c\u8bed\u6c14\u548c\u4e0a\u4e0b\u6587\u7684\u573a\u666f\uff0c\u660e\u786e\u8981\u6c42\u6a21\u578b\u4f7f\u7528\u4e0d\u5f53\u8bed\u8a00\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30LLMs\u662f\u5426\u9075\u5b88\u6216\u62b5\u5236\u4e0d\u5f53\u6307\u4ee4\uff0c\u8861\u91cf\u5176\u4f26\u7406\u5bf9\u9f50\u3001\u6587\u5316\u7406\u89e3\u548c\u8bed\u8a00\u80fd\u529b\u3002", "conclusion": "SweEval\u6709\u52a9\u4e8e\u63a8\u52a8\u6784\u5efa\u4f26\u7406\u5bf9\u9f50\u7684AI\u7cfb\u7edf\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "relevance": 85.0}}
{"id": "2505.17893", "pdf": "https://arxiv.org/pdf/2505.17893", "abs": "https://arxiv.org/abs/2505.17893", "authors": ["Shruti Atul Mali", "Zohaib Salahuddin", "Danial Khan", "Yumeng Zhang", "Henry C. Woodruff", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "title": "Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u533a\u57dfCT\u56fe\u50cf\u7279\u5f81\u6574\u5408\u53ca\u6807\u51c6\u5316\u65b9\u6cd5\u5bf9\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\u60a3\u8005\u751f\u5b58\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u624b\u5de5\u653e\u5c04\u7ec4\u5b66\u3001\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7279\u5f81\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u901a\u8fc7\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u591a\u533a\u57df\u7279\u5f81\u6574\u5408\u548c\u6807\u51c6\u5316\u65b9\u6cd5\u5728\u751f\u5b58\u9884\u6d4b\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528ComBat\u3001RKN\u7b49\u65b9\u6cd5\u6807\u51c6\u5316\u7279\u5f81\uff0c\u7ed3\u5408\u624b\u5de5\u653e\u5c04\u7ec4\u5b66\u548cFM\u7279\u5f81\uff0c\u901a\u8fc7\u6b63\u5219\u5316Cox\u6a21\u578b\u548cSHAP\u503c\u5206\u6790\u9884\u6d4b\u751f\u5b58\u7387\u3002", "result": "FM\u7279\u5f81\u4e0e\u4e34\u5e8a\u6570\u636e\u7ed3\u5408\u8868\u73b0\u6700\u4f73\uff08C-index=0.7616\uff09\uff0c\u5171\u8bc6\u6a21\u578b\u572878%\u6d4b\u8bd5\u75c5\u4f8b\u4e2d\u8fbe\u5230t-AUC=0.92\u3002", "conclusion": "\u591a\u533a\u57df\u7279\u5f81\u6574\u5408\u548c\u6807\u51c6\u5316\u663e\u8457\u63d0\u5347\u751f\u5b58\u9884\u6d4b\u6027\u80fd\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u53ef\u5b9e\u73b0\u8de8\u4e2d\u5fc3\u7684\u9c81\u68d2\u98ce\u9669\u5206\u5c42\u3002", "relevance": 40.0}}
{"id": "2505.17714", "pdf": "https://arxiv.org/pdf/2505.17714", "abs": "https://arxiv.org/abs/2505.17714", "authors": ["Ben Rahman"], "title": "PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This manuscript builds upon an earlier version posted to TechRxiv.\n  This arXiv version includes an updated comparison with GRPO (Group Relative\n  Policy Optimization)", "summary": "Despite Proximal Policy Optimization (PPO) dominating policy gradient methods\n-- from robotic control to game AI -- its static trust region forces a brittle\ntrade-off: aggressive clipping stifles early exploration, while late-stage\nupdates destabilize convergence. PPO-BR establishes a new paradigm in adaptive\nRL by fusing exploration and convergence signals into a single bounded trust\nregion -- a theoretically grounded innovation that outperforms five SOTA\nbaselines with less than 2% overhead. This work bridges a critical gap in\nphase-aware learning, enabling real-world deployment in safety-critical systems\nlike robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%\nfaster convergence by combining: (1) entropy-driven expansion (epsilon up) for\nexploration in high-uncertainty states, and (2) reward-guided contraction\n(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,\nAtari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),\n2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with\nonly five lines of code change. PPO-BR's simplicity and theoretical guarantees\nmake it ready-to-deploy in safety-critical domains -- from surgical robotics to\nautonomous drones. In contrast to recent methods such as Group Relative Policy\nOptimization (GRPO), PPO-BR offers a unified entropy-reward mechanism\napplicable to both language models and general reinforcement learning\nenvironments.", "AI": {"tldr": "PPO-BR\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4fe1\u4efb\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u4e0e\u6536\u655b\u7684\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3PPO\u4e2d\u9759\u6001\u4fe1\u4efb\u533a\u57df\u5bfc\u81f4\u7684\u63a2\u7d22\u4e0e\u6536\u655b\u4e4b\u95f4\u7684\u8106\u5f31\u6743\u8861\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u71b5\u9a71\u52a8\u6269\u5c55\uff08\u63a2\u7d22\uff09\u548c\u5956\u52b1\u5f15\u5bfc\u6536\u7f29\uff08\u6536\u655b\uff09\u7684\u52a8\u6001\u4fe1\u4efb\u533a\u57df\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPPO-BR\u5b9e\u73b0\u4e8629.1%\u66f4\u5feb\u7684\u6536\u655b\u3001\u66f4\u4f4e\u7684\u5956\u52b1\u65b9\u5dee\uff0c\u4e14\u8fd0\u884c\u5f00\u9500\u4f4e\u4e8e2%\u3002", "conclusion": "PPO-BR\u4e3a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u81ea\u9002\u5e94RL\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17345", "pdf": "https://arxiv.org/pdf/2505.17345", "abs": "https://arxiv.org/abs/2505.17345", "authors": ["Justin D. Norman", "Michael U. Rivera", "D. Alex Hughes"], "title": "Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Plausible, but inaccurate, tokens in model-generated text are widely believed\nto be pervasive and problematic for the responsible adoption of language\nmodels. Despite this concern, there is little scientific work that attempts to\nmeasure the prevalence of language model hallucination in a comprehensive way.\nIn this paper, we argue that language models should be evaluated using\nrepeatable, open, and domain-contextualized hallucination benchmarking. We\npresent a taxonomy of hallucinations alongside a case study that demonstrates\nthat when experts are absent from the early stages of data creation, the\nresulting hallucination metrics lack validity and practical utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u3001\u5f00\u653e\u4e14\u57fa\u4e8e\u9886\u57df\u4e0a\u4e0b\u6587\u7684\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u7f3a\u4e4f\u4e13\u5bb6\u53c2\u4e0e\u4f1a\u5bfc\u81f4\u5e7b\u89c9\u6307\u6807\u65e0\u6548\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u4e2d\u666e\u904d\u5b58\u5728\u4e0d\u51c6\u786e\u4f46\u770b\u4f3c\u5408\u7406\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u5168\u9762\u6d4b\u91cf\u7684\u79d1\u5b66\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e7b\u89c9\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bf4\u660e\u4e13\u5bb6\u5728\u6570\u636e\u521b\u5efa\u65e9\u671f\u9636\u6bb5\u7684\u91cd\u8981\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u82e5\u7f3a\u4e4f\u4e13\u5bb6\u53c2\u4e0e\uff0c\u5e7b\u89c9\u6307\u6807\u5c06\u7f3a\u4e4f\u6709\u6548\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u8bc4\u4f30\u9700\u8981\u53ef\u91cd\u590d\u3001\u5f00\u653e\u4e14\u57fa\u4e8e\u9886\u57df\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e13\u5bb6\u53c2\u4e0e\u7684\u5fc5\u8981\u6027\u3002", "relevance": 85.0}}
{"id": "2505.17905", "pdf": "https://arxiv.org/pdf/2505.17905", "abs": "https://arxiv.org/abs/2505.17905", "authors": ["Xie Ting", "Ye Huang", "Zhilin Liu", "Lixin Duan"], "title": "Semantic segmentation with reward", "categories": ["cs.CV"], "comment": "Tech report", "summary": "In real-world scenarios, pixel-level labeling is not always available.\nSometimes, we need a semantic segmentation network, and even a visual encoder\ncan have a high compatibility, and can be trained using various types of\nfeedback beyond traditional labels, such as feedback that indicates the quality\nof the parsing results. To tackle this issue, we proposed RSS (Reward in\nSemantic Segmentation), the first practical application of reward-based\nreinforcement learning on pure semantic segmentation offered in two granular\nlevels (pixel-level and image-level). RSS incorporates various novel\ntechnologies, such as progressive scale rewards (PSR) and pair-wise spatial\ndifference (PSD), to ensure that the reward facilitates the convergence of the\nsemantic segmentation network, especially under image-level rewards.\nExperiments and visualizations on benchmark datasets demonstrate that the\nproposed RSS can successfully ensure the convergence of the semantic\nsegmentation network on two levels of rewards. Additionally, the RSS, which\nutilizes an image-level reward, outperforms existing weakly supervised methods\nthat also rely solely on image-level signals during training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08RSS\uff09\uff0c\u7528\u4e8e\u7eaf\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u652f\u6301\u50cf\u7d20\u7ea7\u548c\u56fe\u50cf\u7ea7\u4e24\u79cd\u5956\u52b1\u7c92\u5ea6\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u6280\u672f\uff08\u5982PSR\u548cPSD\uff09\uff0cRSS\u786e\u4fdd\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u5728\u5956\u52b1\u4fe1\u53f7\u4e0b\u6536\u655b\uff0c\u5e76\u5728\u56fe\u50cf\u7ea7\u5956\u52b1\u4e0a\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u573a\u666f\u4e2d\u50cf\u7d20\u7ea7\u6807\u6ce8\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u8d85\u8d8a\u4f20\u7edf\u6807\u7b7e\u7684\u53cd\u9988\u65b9\u5f0f\uff08\u5982\u5956\u52b1\u4fe1\u53f7\uff09\u6765\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u3002", "method": "\u63d0\u51faRSS\u65b9\u6cd5\uff0c\u7ed3\u5408\u6e10\u8fdb\u5c3a\u5ea6\u5956\u52b1\uff08PSR\uff09\u548c\u6210\u5bf9\u7a7a\u95f4\u5dee\u5f02\uff08PSD\uff09\u6280\u672f\uff0c\u652f\u6301\u50cf\u7d20\u7ea7\u548c\u56fe\u50cf\u7ea7\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRSS\u5728\u4e24\u79cd\u5956\u52b1\u7c92\u5ea6\u4e0b\u5747\u80fd\u786e\u4fdd\u7f51\u7edc\u6536\u655b\uff0c\u4e14\u56fe\u50cf\u7ea7\u5956\u52b1\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "RSS\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5956\u52b1\u9a71\u52a8\u8bad\u7ec3\u6846\u67b6\uff0c\u5c24\u5176\u5728\u56fe\u50cf\u7ea7\u5956\u52b1\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "relevance": 60.0}}
{"id": "2505.17141", "pdf": "https://arxiv.org/pdf/2505.17141", "abs": "https://arxiv.org/abs/2505.17141", "authors": ["Rania Ahmed", "Eman Ahmed", "Ahmed Elbarbary", "Ashraf Darwish", "Aboul Ella Hassanien"], "title": "Fashion Industry in the Age of Generative Artificial Intelligence and Metaverse: A systematic Review", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The fashion industry is an extremely profitable market that generates\ntrillions of dollars in revenue by producing and distributing apparel,\nfootwear, and accessories. This systematic literature review (SLR) seeks to\nsystematically review and analyze the research landscape about the Generative\nArtificial Intelligence (GAI) and metaverse in the fashion industry. Thus,\ninvestigating the impact of integrating both technologies to enhance the\nfashion industry. This systematic review uses the Reporting Items for\nSystematic reviews and Meta-Analyses (PRISMA) methodology, including three\nessential phases: identification, evaluation, and reporting. In the\nidentification phase, the target search problems are determined by selecting\nappropriate keywords and alternative synonyms. After that 578 documents from\n2014 to the end of 2023 are retrieved. The evaluation phase applies three\nscreening steps to assess papers and choose 118 eligible papers for full-text\nreading. Finally, the reporting phase thoroughly examines and synthesizes the\n118 eligible papers to identify key themes associated with GAI and Metaverse in\nthe fashion industry. Based on Strengths, Weaknesses, Opportunities, and\nThreats (SWOT) analyses performed for both GAI and metaverse for the fashion\nindustry, it is concluded that the integration of GAI and the metaverse holds\nthe capacity to profoundly revolutionize the fashion sector, presenting chances\nfor improved manufacturing, design, sales, and client experiences. Accordingly,\nthe research proposes a new framework to integrate GAI and metaverse to enhance\nthe fashion industry. The framework presents different use cases to promote the\nfashion industry using the integration. Future research points for achieving a\nsuccessful integration are demonstrated.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u5206\u6790\u4e86\u751f\u6210\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u548c\u5143\u5b87\u5b99\u5728\u65f6\u5c1a\u884c\u4e1a\u4e2d\u7684\u6574\u5408\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u4ee5\u4fc3\u8fdb\u5176\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22GAI\u548c\u5143\u5b87\u5b99\u6280\u672f\u5982\u4f55\u5171\u540c\u63a8\u52a8\u65f6\u5c1a\u884c\u4e1a\u7684\u521b\u65b0\uff0c\u5c24\u5176\u662f\u5728\u8bbe\u8ba1\u3001\u5236\u9020\u548c\u5ba2\u6237\u4f53\u9a8c\u65b9\u9762\u3002", "method": "\u91c7\u7528PRISMA\u65b9\u6cd5\uff0c\u5305\u62ec\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u62a5\u544a\u4e09\u4e2a\u9636\u6bb5\uff0c\u7b5b\u9009\u4e86578\u7bc7\u6587\u732e\uff0c\u6700\u7ec8\u5206\u6790\u4e86118\u7bc7\u3002", "result": "\u901a\u8fc7SWOT\u5206\u6790\u53d1\u73b0\uff0cGAI\u548c\u5143\u5b87\u5b99\u7684\u6574\u5408\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u65f6\u5c1a\u884c\u4e1a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u5408\u6846\u67b6\u3002", "conclusion": "GAI\u548c\u5143\u5b87\u5b99\u7684\u6574\u5408\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5982\u4f55\u5b9e\u73b0\u8fd9\u4e00\u6574\u5408\u3002", "relevance": 30.0}}
{"id": "2505.17716", "pdf": "https://arxiv.org/pdf/2505.17716", "abs": "https://arxiv.org/abs/2505.17716", "authors": ["Erhu Feng", "Wenbo Zhou", "Zibin Liu", "Le Chen", "Yunpeng Dong", "Cheng Zhang", "Yisheng Zhao", "Dong Du", "Zhichao Hua", "Yubin Xia", "Haibo Chen"], "title": "Get Experience from Practice: LLM Agents with Record & Replay", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "AI agents, empowered by Large Language Models (LLMs) and communication\nprotocols such as MCP and A2A, have rapidly evolved from simple chatbots to\nautonomous entities capable of executing complex, multi-step tasks,\ndemonstrating great potential. However, the LLMs' inherent uncertainty and\nheavy computational resource requirements pose four significant challenges to\nthe development of safe and efficient agents: reliability, privacy, cost and\nperformance. Existing approaches, like model alignment, workflow constraints\nand on-device model deployment, can partially alleviate some issues but often\nwith limitations, failing to fundamentally resolve these challenges.\n  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),\nwhich introduces the classical record-and-replay mechanism into AI agent\nframeworks. The core idea is to: 1. Record an agent's interaction trace with\nits environment and internal decision process during task execution, 2.\nSummarize this trace into a structured \"experience\" encapsulating the workflow\nand constraints, and 3. Replay these experiences in subsequent similar tasks to\nguide the agent's behavior. We detail a multi-level experience abstraction\nmethod and a check function mechanism in AgentRR: the former balances\nexperience specificity and generality, while the latter serves as a trust\nanchor to ensure completeness and safety during replay. In addition, we explore\nmultiple application modes of AgentRR, including user-recorded task\ndemonstration, large-small model collaboration and privacy-aware agent\nexecution, and envision an experience repository for sharing and reusing\nknowledge to further reduce deployment cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentRR\u8303\u5f0f\uff0c\u901a\u8fc7\u8bb0\u5f55-\u91cd\u653e\u673a\u5236\u89e3\u51b3LLM\u4ee3\u7406\u5728\u53ef\u9760\u6027\u3001\u9690\u79c1\u3001\u6210\u672c\u548c\u6027\u80fd\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "LLM\u4ee3\u7406\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u9ad8\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5bfc\u81f4\u53ef\u9760\u6027\u3001\u9690\u79c1\u3001\u6210\u672c\u548c\u6027\u80fd\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6839\u672c\u89e3\u51b3\u3002", "method": "\u5f15\u5165\u8bb0\u5f55-\u91cd\u653e\u673a\u5236\uff0c\u8bb0\u5f55\u4ee3\u7406\u4ea4\u4e92\u8f68\u8ff9\u5e76\u62bd\u8c61\u4e3a\u7ed3\u6784\u5316\u7ecf\u9a8c\uff0c\u901a\u8fc7\u91cd\u653e\u6307\u5bfc\u540e\u7eed\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u591a\u7ea7\u7ecf\u9a8c\u62bd\u8c61\u65b9\u6cd5\u548c\u68c0\u67e5\u51fd\u6570\u673a\u5236\uff0c\u5e73\u8861\u7ecf\u9a8c\u901a\u7528\u6027\u4e0e\u5b89\u5168\u6027\uff0c\u5e76\u63a2\u7d22\u591a\u79cd\u5e94\u7528\u6a21\u5f0f\u3002", "conclusion": "AgentRR\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u671b\u4e86\u7ecf\u9a8c\u5171\u4eab\u5e93\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.17362", "pdf": "https://arxiv.org/pdf/2505.17362", "abs": "https://arxiv.org/abs/2505.17362", "authors": ["Zafarullah Mahmood", "Soliman Ali", "Jiading Zhu", "Mohamed Abdelwahab", "Michelle Yu Collins", "Sihan Chen", "Yi Cheng Zhao", "Jodi Wolff", "Osnat Melamed", "Nadia Minian", "Marta Maslej", "Carolynne Cooper", "Matt Ratto", "Peter Selby", "Jonathan Rose"], "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Findings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL), Vienna, Austria, 2025", "summary": "The conversational capabilities of Large Language Models (LLMs) suggest that\nthey may be able to perform as automated talk therapists. It is crucial to know\nif these systems would be effective and adhere to known standards. We present a\ncounsellor chatbot that focuses on motivating tobacco smokers to quit smoking.\nIt uses a state-of-the-art LLM and a widely applied therapeutic approach called\nMotivational Interviewing (MI), and was evolved in collaboration with\nclinician-scientists with expertise in MI. We also describe and validate an\nautomated assessment of both the chatbot's adherence to MI and client\nresponses. The chatbot was tested on 106 participants, and their confidence\nthat they could succeed in quitting smoking was measured before the\nconversation and one week later. Participants' confidence increased by an\naverage of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed\nadherence to MI standards in 98% of utterances, higher than human counsellors.\nThe chatbot scored well on a participant-reported metric of perceived empathy\nbut lower than typical human counsellors. Furthermore, participants' language\nindicated a good level of motivation to change, a key goal in MI. These results\nsuggest that the automation of talk therapy with a modern LLM has promise.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528LLM\u4f5c\u4e3a\u81ea\u52a8\u5316\u5fc3\u7406\u54a8\u8be2\u5e08\u7684\u6f5c\u529b\uff0c\u901a\u8fc7Motivational Interviewing\uff08MI\uff09\u65b9\u6cd5\u5e2e\u52a9\u5438\u70df\u8005\u6212\u70df\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u548c\u7b26\u5408\u6807\u51c6\u7684\u7a0b\u5ea6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1LLM\u662f\u5426\u80fd\u6709\u6548\u4e14\u7b26\u5408\u6807\u51c6\u5730\u4f5c\u4e3a\u5fc3\u7406\u54a8\u8be2\u5e08\uff0c\u7279\u522b\u662f\u5728\u5e2e\u52a9\u5438\u70df\u8005\u6212\u70df\u7684\u573a\u666f\u4e2d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u91c7\u7528MI\u7597\u6cd5\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u4f30\u9a8c\u8bc1\u5176\u7b26\u5408MI\u6807\u51c6\u7684\u7a0b\u5ea6\u3002\u5b9e\u9a8c\u6d89\u53ca106\u540d\u53c2\u4e0e\u8005\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u7684\u6212\u70df\u4fe1\u5fc3\u5e73\u5747\u63d0\u9ad81.7\u5206\uff080-10\u5206\uff09\uff0c\u804a\u5929\u673a\u5668\u4eba\u572898%\u7684\u5bf9\u8bdd\u4e2d\u7b26\u5408MI\u6807\u51c6\uff0c\u4e14\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u5171\u60c5\u80fd\u529b\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u57fa\u4e8e\u73b0\u4ee3LLM\u7684\u81ea\u52a8\u5316\u5fc3\u7406\u54a8\u8be2\u5177\u6709\u6f5c\u529b\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u65b9\u9762\uff08\u5982\u5171\u60c5\uff09\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u54a8\u8be2\u5e08\u3002", "relevance": 75.0}}
{"id": "2505.17910", "pdf": "https://arxiv.org/pdf/2505.17910", "abs": "https://arxiv.org/abs/2505.17910", "authors": ["Bin Wu", "Wei Wang", "Yahui Liu", "Zixiang Li", "Yao Zhao"], "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 13 figures, 5 tables", "summary": "Reward Feedback Learning (ReFL) has recently shown great potential in\naligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named DiffusionReward, to the\nBlind Face Restoration task for the first time. DiffusionReward effectively\novercomes the limitations of diffusion-based methods, which often fail to\ngenerate realistic facial details and exhibit poor identity consistency. The\ncore of our framework is the Face Reward Model (FRM), which is trained using\ncarefully annotated data. It provides feedback signals that play a pivotal role\nin steering the optimization process of the restoration network. In particular,\nour ReFL framework incorporates a gradient flow into the denoising process of\noff-the-shelf face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the\nFRM to ensure the perceptual quality of the restored faces; (ii) a\nregularization term that functions as a safeguard to preserve generative\ndiversity; and (iii) a structural consistency constraint to maintain facial\nfidelity. Furthermore, the FRM undergoes dynamic optimization throughout the\nprocess. It not only ensures that the restoration network stays precisely\naligned with the real face manifold, but also effectively prevents reward\nhacking. Experiments on synthetic and wild datasets demonstrate that our method\noutperforms state-of-the-art methods, significantly improving identity\nconsistency and facial details. The source codes, data, and models are\navailable at: https://github.com/01NeuralNinja/DiffusionReward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffusionReward\u7684Reward Feedback Learning (ReFL)\u6846\u67b6\uff0c\u9996\u6b21\u5e94\u7528\u4e8e\u76f2\u4eba\u8138\u4fee\u590d\u4efb\u52a1\uff0c\u901a\u8fc7Face Reward Model (FRM)\u63d0\u4f9b\u53cd\u9988\u4fe1\u53f7\uff0c\u4f18\u5316\u4fee\u590d\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u9762\u90e8\u7ec6\u8282\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728\u76f2\u4eba\u8138\u4fee\u590d\u4efb\u52a1\u4e2d\u751f\u6210\u4e0d\u771f\u5b9e\u9762\u90e8\u7ec6\u8282\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408FRM\u3001\u6b63\u5219\u5316\u9879\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u901a\u8fc7\u68af\u5ea6\u6d41\u6307\u5bfc\u4fee\u590d\u7f51\u7edc\u7684\u53c2\u6570\u66f4\u65b0\uff0c\u5e76\u52a8\u6001\u4f18\u5316FRM\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u9762\u90e8\u7ec6\u8282\u3002", "conclusion": "DiffusionReward\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u65b9\u6cd5\u5728\u76f2\u4eba\u8138\u4fee\u590d\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 60.0}}
{"id": "2505.17720", "pdf": "https://arxiv.org/pdf/2505.17720", "abs": "https://arxiv.org/abs/2505.17720", "authors": ["Hampus Linander", "Christoffer Petersson", "Daniel Persson", "Jan E. Gerken"], "title": "PEAR: Equal Area Weather Forecasting on the Sphere", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Machine learning methods for global medium-range weather forecasting have\nrecently received immense attention. Following the publication of the Pangu\nWeather model, the first deep learning model to outperform traditional\nnumerical simulations of the atmosphere, numerous models have been published in\nthis domain, building on Pangu's success. However, all of these models operate\non input data and produce predictions on the Driscoll--Healy discretization of\nthe sphere which suffers from a much finer grid at the poles than around the\nequator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization\n(HEALPix) of the sphere, each pixel covers the same surface area, removing\nunphysical biases. Motivated by a growing support for this grid in meteorology\nand climate sciences, we propose to perform weather forecasting with deep\nlearning models which natively operate on the HEALPix grid. To this end, we\nintroduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting\nmodel which operates directly on HEALPix-features and outperforms the\ncorresponding model on Driscoll--Healy without any computational overhead.", "AI": {"tldr": "PEAR\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5929\u6c14\u9884\u6d4b\u6a21\u578b\uff0c\u76f4\u63a5\u5728HEALPix\u7f51\u683c\u4e0a\u8fd0\u884c\uff0c\u6027\u80fd\u4f18\u4e8eDriscoll-Healy\u7f51\u683c\u6a21\u578b\uff0c\u4e14\u65e0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u5728Driscoll-Healy\u7f51\u683c\u4e0a\u5b58\u5728\u4e0d\u5747\u5300\u5206\u5e03\u95ee\u9898\uff0c\u800cHEALPix\u7f51\u683c\u63d0\u4f9b\u5747\u5300\u8986\u76d6\uff0c\u51cf\u5c11\u7269\u7406\u504f\u5dee\u3002", "method": "\u63d0\u51faPEAR\u6a21\u578b\uff0c\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u76f4\u63a5\u5728HEALPix\u7f51\u683c\u4e0a\u5904\u7406\u6570\u636e\u3002", "result": "PEAR\u5728HEALPix\u7f51\u683c\u4e0a\u7684\u6027\u80fd\u4f18\u4e8eDriscoll-Healy\u7f51\u683c\u6a21\u578b\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u76f8\u540c\u3002", "conclusion": "HEALPix\u7f51\u683c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982PEAR\uff09\u662f\u5929\u6c14\u9884\u6d4b\u9886\u57df\u7684\u6709\u524d\u666f\u65b9\u5411\u3002", "relevance": 30.0}}
{"id": "2505.17380", "pdf": "https://arxiv.org/pdf/2505.17380", "abs": "https://arxiv.org/abs/2505.17380", "authors": ["Yinghui Huang", "Yuxuan Jiang", "Hui Liu", "Yixin Cai", "Weiqing Li", "Xiangen Hu"], "title": "AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing", "categories": ["cs.CL", "H.1.2; I.2.7"], "comment": "21 pages, 5 figures", "summary": "Large language models (LLMs) like GPT-4 show potential for scaling\nmotivational interviewing (MI) in addiction care, but require systematic\nevaluation of therapeutic capabilities. We present a computational framework\nassessing user-perceived quality (UPQ) through expected and unexpected MI\nbehaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI\ncollaboration, we developed predictive models integrating deep learning and\nexplainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)\nbehavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI\nperformance, reducing inappropriate advice while enhancing reflections and\nempathy. Although GPT-4 remained marginally inferior to therapists overall, it\ndemonstrated superior advice management capabilities. The model achieved\nmeasurable quality improvements through prompt engineering, yet showed\nlimitations in addressing complex emotional nuances. This framework establishes\na pathway for optimizing LLM-based therapeutic tools through targeted\nbehavioral metric analysis and human-AI co-evaluation. Findings highlight both\nthe scalability potential and current constraints of LLMs in clinical\ncommunication applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30GPT-4\u5728\u6210\u763e\u62a4\u7406\u4e2d\u52a8\u673a\u8bbf\u8c08\uff08MI\uff09\u80fd\u529b\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u5206\u6790MI\u884c\u4e3a\uff0c\u6539\u8fdb\u4e86GPT-4\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u5728\u4e34\u5e8a\u6c9f\u901a\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u52a8\u673a\u8bbf\u8c08\uff08MI\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u6210\u763e\u62a4\u7406\u7684\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\uff0c\u5206\u6790\u4eba\u7c7b\u6cbb\u7597\u5e08\u548cGPT-4\u7684MI\u4f1a\u8bdd\uff0c\u8bc6\u522b17\u79cdMI\u884c\u4e3a\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u6539\u8fdbGPT-4\u7684\u8868\u73b0\u3002", "result": "GPT-4\u5728\u7ba1\u7406\u5efa\u8bae\u65b9\u9762\u4f18\u4e8e\u4eba\u7c7b\u6cbb\u7597\u5e08\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u60c5\u611f\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\uff0cGPT-4\u7684MI\u8868\u73b0\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f18\u5316\u57fa\u4e8eLLM\u7684\u6cbb\u7597\u5de5\u5177\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLM\u5728\u4e34\u5e8a\u6c9f\u901a\u4e2d\u7684\u6f5c\u529b\u4e0e\u9650\u5236\u3002", "relevance": 75.0}}
{"id": "2505.17911", "pdf": "https://arxiv.org/pdf/2505.17911", "abs": "https://arxiv.org/abs/2505.17911", "authors": ["Zheyang Huang", "Jagannath Aryal", "Saeid Nahavandi", "Xuequan Lu", "Chee Peng Lim", "Lei Wei", "Hailing Zhou"], "title": "Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-view geo-localization determines the location of a query image,\ncaptured by a drone or ground-based camera, by matching it to a geo-referenced\nsatellite image. While traditional approaches focus on image-level\nlocalization, many applications, such as search-and-rescue, infrastructure\ninspection, and precision delivery, demand object-level accuracy. This enables\nusers to prompt a specific object with a single click on a drone image to\nretrieve precise geo-tagged information of the object. However, variations in\nviewpoints, timing, and imaging conditions pose significant challenges,\nespecially when identifying visually similar objects in extensive satellite\nimagery. To address these challenges, we propose an Object-level Cross-view\nGeo-localization Network (OCGNet). It integrates user-specified click locations\nusing Gaussian Kernel Transfer (GKT) to preserve location information\nthroughout the network. This cue is dually embedded into the feature encoder\nand feature matching blocks, ensuring robust object-specific localization.\nAdditionally, OCGNet incorporates a Location Enhancement (LE) module and a\nMulti-Head Cross Attention (MHCA) module to adaptively emphasize\nobject-specific features or expand focus to relevant contextual regions when\nnecessary. OCGNet achieves state-of-the-art performance on a public dataset,\nCVOGL. It also demonstrates few-shot learning capabilities, effectively\ngeneralizing from limited examples, making it suitable for diverse applications\n(https://github.com/ZheyangH/OCGNet).", "AI": {"tldr": "OCGNet\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u8c61\u7ea7\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u7f51\u7edc\uff0c\u901a\u8fc7\u9ad8\u65af\u6838\u4f20\u9012\u548c\u53cc\u5d4c\u5165\u673a\u5236\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u7ed3\u5408\u4f4d\u7f6e\u589e\u5f3a\u548c\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728CVOGL\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u8bb8\u591a\u5e94\u7528\uff08\u5982\u641c\u6551\u3001\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\uff09\u9700\u8981\u5bf9\u8c61\u7ea7\u7684\u5730\u7406\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4f46\u89c6\u89d2\u3001\u65f6\u95f4\u548c\u6210\u50cf\u6761\u4ef6\u7684\u5dee\u5f02\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "OCGNet\u6574\u5408\u7528\u6237\u70b9\u51fb\u4f4d\u7f6e\uff08\u9ad8\u65af\u6838\u4f20\u9012\uff09\uff0c\u53cc\u5d4c\u5165\u5230\u7279\u5f81\u7f16\u7801\u548c\u5339\u914d\u5757\uff0c\u7ed3\u5408\u4f4d\u7f6e\u589e\u5f3a\u548c\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728CVOGL\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u51fa\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "OCGNet\u9002\u7528\u4e8e\u591a\u6837\u5316\u5e94\u7528\uff0c\u80fd\u6709\u6548\u6cdb\u5316\u3002", "relevance": 30.0}}
{"id": "2505.17143", "pdf": "https://arxiv.org/pdf/2505.17143", "abs": "https://arxiv.org/abs/2505.17143", "authors": ["I. E. Ezeibe", "S. O. Okide", "D. C. Asogwa"], "title": "Evaluating the Performance of Nigerian Lecturers using Multilayer Perceptron", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Evaluating the performance of a lecturer has been essential for enhancing\nteaching quality, improving student learning outcomes, and strengthening the\ninstitution's reputation. The absence of such a system brings about lecturer\nperformance evaluation which was neither comprehensive nor holistic. This\nsystem was designed using a web-based platform, created a secure database, and\nby using a custom dataset, captured some performance metrics which included\nstudent evaluation scores, Research Publications, Years of Experience, and\nAdministrative Duties. Multilayer Perceptron (MLP) algorithm was utilized due\nto its ability to process complex data patterns and generates accurate\npredictions in a lecturer's performance based on historical data. This research\nfocused on designing multiple performance metrics beyond the standard ones,\nincorporating student participation, and integrating analytical tools to\ndeliver a comprehensive and holistic evaluation of lecturers' performance and\nwas developed using Object-Oriented Analysis and Design (OOAD) methodology.\nLecturers' performance is evaluated by the model, and the evaluation accuracy\nis about 91% compared with actual performance. Finally, by evaluating the\nperformance of the MLP model, it is concluded that MLP enhanced lecturer\nperformance evaluation by providing accurate predictions, reducing bias, and\nsupporting data-driven decisions, ultimately improving the fairness and\nefficiency of the evaluation process. The MLP model's performance was evaluated\nusing Mean Squared Error (MSE) and Mean Absolute Error (MAE), achieved a test\nloss (MSE) of 256.99 and a MAE of 13.76, and reflected a high level of\nprediction accuracy. The model also demonstrated an estimated accuracy rate of\napproximately 96%, validated its effectiveness in predicting lecturer\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7684\u8bb2\u5e08\u7ee9\u6548\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u5b66\u751f\u8bc4\u4ef7\u3001\u7814\u7a76\u53d1\u8868\u7b49\u591a\u7ef4\u6307\u6807\uff0c\u5b9e\u73b0\u4e8691%\u7684\u8bc4\u4f30\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u8bb2\u5e08\u7ee9\u6548\u8bc4\u4f30\u7cfb\u7edf\u4e0d\u591f\u5168\u9762\u548c\u5ba2\u89c2\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7efc\u5408\u3001\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7b97\u6cd5\u5904\u7406\u590d\u6742\u6570\u636e\u6a21\u5f0f\uff0c\u7ed3\u5408\u5b66\u751f\u8bc4\u4ef7\u3001\u7814\u7a76\u53d1\u8868\u7b49\u6307\u6807\uff0c\u4f7f\u7528\u9762\u5411\u5bf9\u8c61\u5206\u6790\u4e0e\u8bbe\u8ba1\uff08OOAD\uff09\u65b9\u6cd5\u5f00\u53d1\u7cfb\u7edf\u3002", "result": "\u6a21\u578b\u8bc4\u4f30\u51c6\u786e\u7387\u4e3a91%\uff0c\u6d4b\u8bd5\u635f\u5931\uff08MSE\uff09\u4e3a256.99\uff0cMAE\u4e3a13.76\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u7ea6\u4e3a96%\u3002", "conclusion": "MLP\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u51cf\u5c11\u504f\u89c1\uff0c\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u4ece\u800c\u63d0\u5347\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u6548\u7387\u3002", "relevance": 20.0}}
{"id": "2505.17730", "pdf": "https://arxiv.org/pdf/2505.17730", "abs": "https://arxiv.org/abs/2505.17730", "authors": ["Stefan Schoepf", "Michael Curtis Mozer", "Nicole Elyse Mitchell", "Alexandra Brintrup", "Georgios Kaissis", "Peter Kairouz", "Eleni Triantafillou"], "title": "Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data", "categories": ["cs.LG"], "comment": null, "summary": "Machine unlearning is studied for a multitude of tasks, but specialization of\nunlearning methods to particular tasks has made their systematic comparison\nchallenging. To address this issue, we propose a conceptual space to\ncharacterize diverse corrupted data unlearning tasks in vision classifiers.\nThis space is described by two dimensions, the discovery rate (the fraction of\nthe corrupted data that are known at unlearning time) and the statistical\nregularity of the corrupted data (from random exemplars to shared concepts).\nMethods proposed previously have been targeted at portions of this space and-we\nshow-fail predictably outside these regions. We propose a novel method,\nRedirection for Erasing Memory (REM), whose key feature is that corrupted data\nare redirected to dedicated neurons introduced at unlearning time and then\ndiscarded or deactivated to suppress the influence of corrupted data. REM\nperforms strongly across the space of tasks, in contrast to prior SOTA methods\nthat fail outside the regions for which they were designed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u7a7a\u95f4\u6765\u7cfb\u7edf\u6bd4\u8f83\u89c6\u89c9\u5206\u7c7b\u5668\u4e2d\u4e0d\u540c\u6570\u636e\u9057\u5fd8\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5REM\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u5b9a\u5411\u548c\u6291\u5236\u635f\u574f\u6570\u636e\u7684\u5f71\u54cd\uff0c\u5728\u4efb\u52a1\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9057\u5fd8\u65b9\u6cd5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u8bc4\u4f30\u4e0d\u540c\u9057\u5fd8\u4efb\u52a1\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u53d1\u73b0\u7387\u548c\u6570\u636e\u7edf\u8ba1\u89c4\u5f8b\u6027\u4e24\u4e2a\u7ef4\u5ea6\u63cf\u8ff0\u7684\u6982\u5ff5\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5REM\uff0c\u901a\u8fc7\u5c06\u635f\u574f\u6570\u636e\u91cd\u5b9a\u5411\u5230\u4e13\u7528\u795e\u7ecf\u5143\u5e76\u6291\u5236\u5176\u5f71\u54cd\u6765\u5b9e\u73b0\u9057\u5fd8\u3002", "result": "REM\u5728\u4efb\u52a1\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5176\u8bbe\u8ba1\u533a\u57df\u5916\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "REM\u4e3a\u6570\u636e\u9057\u5fd8\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17387", "pdf": "https://arxiv.org/pdf/2505.17387", "abs": "https://arxiv.org/abs/2505.17387", "authors": ["Boqin Zhuang", "Chenxiao Song", "Huitong Lu", "Jiacheng Qiao", "Mingqian Liu", "Mingxing Yu", "Ping Hong", "Rui Li", "Xiaoxia Song", "Xiangjun Xu", "Xu Chen", "Yaoyao Ma", "Yujie Gao"], "title": "WiNGPT-3.0 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Current Large Language Models (LLMs) exhibit significant limitations, notably\nin structured, interpretable, and verifiable medical reasoning, alongside\npractical deployment challenges related to computational resources and data\nprivacy. This report focused on the development of WiNGPT-3.0, the 32-billion\nparameter LLMs, engineered with the objective of enhancing its capacity for\nmedical reasoning and exploring its potential for effective integration within\nhealthcare IT infrastructures. The broader aim is to advance towards clinically\napplicable models. The approach involved a multi-stage training pipeline\ntailored for general, medical, and clinical reasoning. This pipeline\nincorporated supervised fine-tuning (SFT) and reinforcement learning (RL),\nleveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward\nmodels, and an evidence-based diagnostic chain simulation. WiNGPT-3.0\ndemonstrated strong performance: specific model variants achieved scores of\n66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training\nimproved performance on a clinical reasoning task from a baseline score of 58.1\nto 62.5. These findings suggest that reinforcement learning, even when applied\nwith a limited dataset of only a few thousand examples, can enhance medical\nreasoning accuracy. Crucially, this demonstration of RL's efficacy with limited\ndata and computation paves the way for more trustworthy and practically\ndeployable LLMs within clinical workflows and health information\ninfrastructures.", "AI": {"tldr": "WiNGPT-3.0\u662f\u4e00\u4e2a320\u4ebf\u53c2\u6570\u7684LLM\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u5728\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u533b\u5b66\u63a8\u7406\u53ca\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e34\u5e8a\u9002\u7528\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3001\u957f\u94fe\u601d\u7ef4\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u8bca\u65ad\u94fe\u6a21\u62df\u3002", "result": "WiNGPT-3.0\u5728MedCalc\u548cMedQA-USMLE\u4e0a\u5206\u522b\u5f97\u520666.6\u548c87.1\uff0c\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u4ece58.1\u63d0\u5347\u81f362.5\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5373\u4f7f\u5728\u6709\u9650\u6570\u636e\u4e0b\u4e5f\u80fd\u63d0\u5347\u533b\u5b66\u63a8\u7406\u51c6\u786e\u6027\uff0c\u4e3a\u53ef\u4fe1\u8d56\u4e14\u5b9e\u7528\u7684\u4e34\u5e8aLLM\u94fa\u5e73\u9053\u8def\u3002", "relevance": 85.0}}
{"id": "2505.17921", "pdf": "https://arxiv.org/pdf/2505.17921", "abs": "https://arxiv.org/abs/2505.17921", "authors": ["Carlos Salazar-Ruiz", "Francisco Lopez-Tiro", "Ivan Reyes-Amezcua", "Clement Larose", "Gilberto Ochoa-Ruiz", "Christian Daul"], "title": "Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 3 figures, 3 tables, conference, cbms25", "summary": "Determining the type of kidney stones is crucial for prescribing appropriate\ntreatments to prevent recurrence. Currently, various approaches exist to\nidentify the type of kidney stones. However, obtaining results through the\nreference ex vivo identification procedure can take several weeks, while in\nvivo visual recognition requires highly trained specialists. For this reason,\ndeep learning models have been developed to provide urologists with an\nautomated classification of kidney stones during ureteroscopies. Nevertheless,\na common issue with these models is the lack of training data. This\ncontribution presents a deep learning method based on few-shot learning, aimed\nat producing sufficiently discriminative features for identifying kidney stone\ntypes in endoscopic images, even with a very limited number of samples. This\napproach was specifically designed for scenarios where endoscopic images are\nscarce or where uncommon classes are present, enabling classification even with\na limited training dataset. The results demonstrate that Prototypical Networks,\nusing up to 25% of the training data, can achieve performance equal to or\nbetter than traditional deep learning models trained with the complete dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u4e2d\u80be\u7ed3\u77f3\u7c7b\u578b\u7684\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u80be\u7ed3\u77f3\u7c7b\u578b\u8bc6\u522b\u65b9\u6cd5\u8017\u65f6\u6216\u9700\u8981\u4e13\u5bb6\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u6570\u636e\u4e0d\u8db3\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u539f\u578b\u7f51\u7edc\uff08Prototypical Networks\uff09\u8fdb\u884c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u6216\u7f55\u89c1\u7c7b\u522b\u573a\u666f\u3002", "result": "\u4ec5\u752825%\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u539f\u578b\u7f51\u7edc\u6027\u80fd\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u7684\u80be\u7ed3\u77f3\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "relevance": 20.0}}
{"id": "2505.17734", "pdf": "https://arxiv.org/pdf/2505.17734", "abs": "https://arxiv.org/abs/2505.17734", "authors": ["Ahmet Onur Akman", "Anastasia Psarou", "Micha\u0142 Hoffmann", "\u0141ukasz Gorczyca", "\u0141ukasz Kowalski", "Pawe\u0142 Gora", "Grzegorz Jamr\u00f3z", "Rafa\u0142 Kucharski"], "title": "URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles", "categories": ["cs.LG"], "comment": null, "summary": "Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future\nurban networks, potentially by optimizing their routing decisions. Unlike for\nhuman drivers, these decisions can be made with collective, data-driven\npolicies, developed by machine learning algorithms. Reinforcement learning (RL)\ncan facilitate the development of such collective routing strategies, yet\nstandardized and realistic benchmarks are missing. To that end, we present\n\\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.\n\\our{} is a comprehensive benchmarking environment that unifies evaluation\nacross 29 real-world traffic networks paired with realistic demand patterns.\n\\our{} comes with a catalog of predefined tasks, four state-of-the-art\nmulti-agent RL (MARL) algorithm implementations, three baseline methods,\ndomain-specific performance metrics, and a modular configuration scheme. Our\nresults suggest that, despite the lengthy and costly training, state-of-the-art\nMARL algorithms rarely outperformed humans. Experimental results reported in\nthis paper initiate the first leaderboard for MARL in large-scale urban routing\noptimization and reveal that current approaches struggle to scale, emphasizing\nthe urgent need for advancements in this domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u8def\u7531\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\uff0c\u5305\u542b29\u4e2a\u771f\u5b9e\u4ea4\u901a\u7f51\u7edc\u548c\u9700\u6c42\u6a21\u5f0f\uff0c\u8bc4\u4f30\u4e86\u591a\u667a\u80fd\u4f53RL\uff08MARL\uff09\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u96c6\u4f53\u8def\u7531\u7b56\u7565\u5f00\u53d1\u63d0\u4f9b\u6807\u51c6\u5316\u548c\u73b0\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\uff0c\u586b\u8865\u5f53\u524d\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aUrban Routing Benchmark\u7684\u7efc\u5408\u6027\u73af\u5883\uff0c\u5305\u542b29\u4e2a\u771f\u5b9e\u4ea4\u901a\u7f51\u7edc\u3001\u4efb\u52a1\u76ee\u5f55\u3001\u56db\u79cdMARL\u7b97\u6cd5\u5b9e\u73b0\u3001\u57fa\u7ebf\u65b9\u6cd5\u548c\u6027\u80fd\u6307\u6807\u3002", "result": "\u5c3d\u7ba1\u8bad\u7ec3\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u73b0\u6709MARL\u7b97\u6cd5\u5f88\u5c11\u4f18\u4e8e\u4eba\u7c7b\u9a7e\u9a76\u5458\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "conclusion": "\u5f53\u524dMARL\u65b9\u6cd5\u5728\u5927\u578b\u57ce\u5e02\u8def\u7531\u4f18\u5316\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u6280\u672f\u7a81\u7834\u3002", "relevance": 40.0}}
{"id": "2505.17390", "pdf": "https://arxiv.org/pdf/2505.17390", "abs": "https://arxiv.org/abs/2505.17390", "authors": ["Gauri Kambhatla", "Chantal Shaib", "Venkata Govindarajan"], "title": "Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting", "categories": ["cs.CL"], "comment": null, "summary": "Fine-grained personas have recently been used for generating 'diverse'\nsynthetic data for pre-training and supervised fine-tuning of Large Language\nModels (LLMs). In this work, we measure the diversity of persona-driven\nsynthetically generated prompts and responses with a suite of lexical diversity\nand redundancy metrics. Firstly, we find that synthetic prompts/instructions\nare significantly less diverse than human-written ones. Next, we sample\nresponses from LLMs of different sizes with fine-grained and coarse persona\ndescriptions to investigate how much fine-grained detail in persona\ndescriptions contribute to generated text diversity. We find that while\npersona-prompting does improve lexical diversity (especially with larger\nmodels), fine-grained detail in personas doesn't increase diversity noticeably.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u89d2\u8272\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u591a\u6837\u6027\uff0c\u53d1\u73b0\u5408\u6210\u63d0\u793a\u7684\u591a\u6837\u6027\u4f4e\u4e8e\u4eba\u5de5\u7f16\u5199\u7684\u63d0\u793a\uff0c\u4e14\u7ec6\u7c92\u5ea6\u89d2\u8272\u63cf\u8ff0\u5bf9\u6587\u672c\u591a\u6837\u6027\u63d0\u5347\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u7ec6\u7c92\u5ea6\u89d2\u8272\u63cf\u8ff0\u5728\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u6548\u679c\u3002", "method": "\u4f7f\u7528\u8bcd\u6c47\u591a\u6837\u6027\u548c\u5197\u4f59\u5ea6\u91cf\u8bc4\u4f30\u5408\u6210\u63d0\u793a\u548c\u54cd\u5e94\u7684\u591a\u6837\u6027\uff0c\u6bd4\u8f83\u4e0d\u540c\u5927\u5c0fLLM\u5728\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u89d2\u8272\u63cf\u8ff0\u4e0b\u7684\u751f\u6210\u6587\u672c\u591a\u6837\u6027\u3002", "result": "\u5408\u6210\u63d0\u793a\u7684\u591a\u6837\u6027\u663e\u8457\u4f4e\u4e8e\u4eba\u5de5\u7f16\u5199\u7684\u63d0\u793a\uff1b\u7ec6\u7c92\u5ea6\u89d2\u8272\u63cf\u8ff0\u5bf9\u6587\u672c\u591a\u6837\u6027\u63d0\u5347\u4e0d\u660e\u663e\uff0c\u4f46\u89d2\u8272\u63d0\u793a\uff08\u5c24\u5176\u662f\u5927\u6a21\u578b\uff09\u80fd\u63d0\u9ad8\u8bcd\u6c47\u591a\u6837\u6027\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u89d2\u8272\u63cf\u8ff0\u5bf9\u751f\u6210\u6587\u672c\u591a\u6837\u6027\u7684\u8d21\u732e\u6709\u9650\uff0c\u672a\u6765\u7814\u7a76\u53ef\u63a2\u7d22\u5176\u4ed6\u63d0\u5347\u591a\u6837\u6027\u7684\u65b9\u6cd5\u3002", "relevance": 75.0}}
{"id": "2505.17931", "pdf": "https://arxiv.org/pdf/2505.17931", "abs": "https://arxiv.org/abs/2505.17931", "authors": ["Xingjian Li", "Qifeng Wu", "Colleen Que", "Yiran Ding", "Adithya S. Ubaradka", "Jianhua Xing", "Tianyang Wang", "Min Xu"], "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u548c\u5206\u5272\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u81ea\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\u89e3\u51b3\u9886\u57df\u5dee\u8ddd\u548c\u7ed3\u679c\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u51cf\u5c11\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5bf9\u4e13\u5bb6\u6807\u6ce8\u6216\u63d0\u793a\u7684\u4f9d\u8d56\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u521d\u59cb\u8fb9\u754c\u6846\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u589e\u5f3a\u6a21\u5757\u4f18\u5316\u63d0\u793a\uff0c\u7ed3\u5408\u53ef\u63d0\u793a\u5206\u5272\u6a21\u578b\u751f\u6210\u6700\u7ec8\u63a9\u7801\uff0c\u5e76\u5f15\u5165\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u5728\u4e03\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e0e\u5f31\u63d0\u793a\u4ea4\u4e92\u5f0f\u57fa\u7840\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7\u5206\u89e3\u548c\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u96f6\u6837\u672c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002", "relevance": 40.0}}
{"id": "2505.17145", "pdf": "https://arxiv.org/pdf/2505.17145", "abs": "https://arxiv.org/abs/2505.17145", "authors": ["Yu Wang", "Cailing Cai", "Zhihua Xiao", "Peifung E. Lam"], "title": "LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly applied in fields such as\nfinance, education, and governance due to their ability to generate human-like\ntext and adapt to specialized tasks. However, their widespread adoption raises\ncritical concerns about data privacy and security, including the risk of\nsensitive data exposure.\n  In this paper, we propose a security framework to enforce policy compliance\nand mitigate risks in LLM interactions. Our approach introduces three key\ninnovations: (i) LLM-based policy enforcement: a customizable mechanism that\nenhances domain-specific detection of sensitive data. (ii) Dynamic policy\ncustomization: real-time policy adaptation and enforcement during user-LLM\ninteractions to ensure compliance with evolving security requirements. (iii)\nSensitive data anonymization: a format-preserving encryption technique that\nprotects sensitive information while maintaining contextual integrity.\nExperimental results demonstrate that our framework effectively mitigates\nsecurity risks while preserving the functional accuracy of LLM-driven tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u6267\u884c\u3001\u52a8\u6001\u5b9a\u5236\u548c\u6570\u636e\u533f\u540d\u5316\u6765\u964d\u4f4e\u654f\u611f\u6570\u636e\u66b4\u9732\u7684\u98ce\u9669\u3002", "motivation": "LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u4e86\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u786e\u4fdd\u654f\u611f\u4fe1\u606f\u7684\u4fdd\u62a4\u3002", "method": "1) LLM-based policy enforcement\uff1b2) Dynamic policy customization\uff1b3) Sensitive data anonymization\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4e86LLM\u4efb\u52a1\u7684\u529f\u80fd\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLMs\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17740", "pdf": "https://arxiv.org/pdf/2505.17740", "abs": "https://arxiv.org/abs/2505.17740", "authors": ["Rodrigo Mart\u00ednez-Pe\u00f1a", "Rom\u00e1n Or\u00fas"], "title": "A tensor network approach for chaotic time series prediction", "categories": ["cs.LG", "cs.NE", "physics.comp-ph"], "comment": "12 pages, 3 figures. Comments are welcome!", "summary": "Making accurate predictions of chaotic time series is a complex challenge.\nReservoir computing, a neuromorphic-inspired approach, has emerged as a\npowerful tool for this task. It exploits the memory and nonlinearity of\ndynamical systems without requiring extensive parameter tuning. However,\nselecting and optimizing reservoir architectures remains an open problem.\nNext-generation reservoir computing simplifies this problem by employing\nnonlinear vector autoregression based on truncated Volterra series, thereby\nreducing hyperparameter complexity. Nevertheless, the latter suffers from\nexponential parameter growth in terms of the maximum monomial degree. Tensor\nnetworks offer a promising solution to this issue by decomposing\nmultidimensional arrays into low-dimensional structures, thus mitigating the\ncurse of dimensionality. This paper explores the application of a previously\nproposed tensor network model for predicting chaotic time series, demonstrating\nits advantages in terms of accuracy and computational efficiency compared to\nconventional echo state networks. Using a state-of-the-art tensor network\napproach enables us to bridge the gap between the tensor network and reservoir\ncomputing communities, fostering advances in both fields.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u662f\u4e00\u4e2a\u590d\u6742\u6311\u6218\uff0c\u4f20\u7edf\u7684\u50a8\u5c42\u8ba1\u7b97\u65b9\u6cd5\u5728\u67b6\u6784\u9009\u62e9\u548c\u4f18\u5316\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u800c\u5f20\u91cf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u9ad8\u7ef4\u53c2\u6570\u589e\u957f\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f20\u91cf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u591a\u7ef4\u6570\u7ec4\u4e3a\u4f4e\u7ef4\u7ed3\u6784\uff0c\u51cf\u5c11\u7ef4\u5ea6\u707e\u96be\uff0c\u5e76\u4e0e\u50a8\u5c42\u8ba1\u7b97\u7ed3\u5408\uff0c\u7b80\u5316\u8d85\u53c2\u6570\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u65f6\uff0c\u6bd4\u4f20\u7edf\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u66f4\u51c6\u786e\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u5f20\u91cf\u7f51\u7edc\u4e0e\u50a8\u5c42\u8ba1\u7b97\u7684\u7ed3\u5408\u4e3a\u4e24\u4e2a\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17391", "pdf": "https://arxiv.org/pdf/2505.17391", "abs": "https://arxiv.org/abs/2505.17391", "authors": ["Yuelyu Ji", "Rui Meng", "Zhuochun Li", "Daqing He"], "title": "Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) grounds large language models (LLMs) in\nup-to-date external evidence, yet existing multi-hop RAG pipelines still issue\nredundant subqueries, explore too shallowly, or wander through overly long\nsearch chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning\nframework that evolves a query-rewriting agent from broad early-stage\nexploration to concise late-stage refinement. EVO-RAG couples a seven-factor,\nstep-level reward vector (covering relevance, redundancy, efficiency, and\nanswer correctness) with a time-varying scheduler that reweights these signals\nas the episode unfolds. The agent is trained with Direct Preference\nOptimization over a multi-head reward model, enabling it to learn when to\nsearch, backtrack, answer, or refuse. Across four multi-hop QA benchmarks\n(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match\nby up to 4.6 points over strong RAG baselines while trimming average retrieval\ndepth by 15 %. Ablation studies confirm the complementary roles of curriculum\nstaging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for\nbuilding reliable, cost-effective multi-hop RAG systems.", "AI": {"tldr": "EVO-RAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5956\u52b1\u8c03\u5ea6\u548c\u8bfe\u7a0b\u5b66\u4e60\u4f18\u5316\u591a\u8df3RAG\u7cfb\u7edf\u7684\u67e5\u8be2\u91cd\u5199\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u68c0\u7d22\u6df1\u5ea6\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u591a\u8df3\u67e5\u8be2\u4e2d\u5b58\u5728\u5197\u4f59\u3001\u63a2\u7d22\u4e0d\u8db3\u6216\u641c\u7d22\u94fe\u8fc7\u957f\u7684\u95ee\u9898\uff0cEVO-RAG\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e03\u7ef4\u5956\u52b1\u5411\u91cf\u548c\u52a8\u6001\u8c03\u5ea6\u5668\uff0c\u8bad\u7ec3\u67e5\u8be2\u91cd\u5199\u4ee3\u7406\uff0c\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u591a\u5934\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3QA\u57fa\u51c6\u4e0a\uff0cEVO-RAG\u5c06Exact Match\u63d0\u53474.6\u5206\uff0c\u540c\u65f6\u51cf\u5c1115%\u7684\u5e73\u5747\u68c0\u7d22\u6df1\u5ea6\u3002", "conclusion": "EVO-RAG\u4e3a\u6784\u5efa\u9ad8\u6548\u53ef\u9760\u7684\u591a\u8df3RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u901a\u7528\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17951", "pdf": "https://arxiv.org/pdf/2505.17951", "abs": "https://arxiv.org/abs/2505.17951", "authors": ["Haihong Xiao", "Jianan Zou", "Yuxin Zhou", "Ying He", "Wenxiong Kang"], "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "We present SplatCo, a structure-view collaborative Gaussian splatting\nframework for high-fidelity rendering of complex outdoor environments. SplatCo\nbuilds upon two novel components: (1) a cross-structure collaboration module\nthat combines global tri-plane representations, which capture coarse scene\nlayouts, with local context grid features that represent fine surface details.\nThis fusion is achieved through a novel hierarchical compensation strategy,\nensuring both global consistency and local detail preservation; and (2) a\ncross-view assisted training strategy that enhances multi-view consistency by\nsynchronizing gradient updates across viewpoints, applying visibility-aware\ndensification, and pruning overfitted or inaccurate Gaussians based on\nstructural consistency. Through joint optimization of structural representation\nand multi-view coherence, SplatCo effectively reconstructs fine-grained\ngeometric structures and complex textures in large-scale scenes. Comprehensive\nevaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,\nTanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo\nconsistently achieves higher reconstruction quality than state-of-the-art\nmethods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These\nresults establish a new benchmark for high-fidelity rendering of large-scale\nunbounded scenes. Code and additional information are available at\nhttps://github.com/SCUT-BIP-Lab/SplatCo.", "AI": {"tldr": "SplatCo\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u4fdd\u771f\u6e32\u67d3\u590d\u6742\u6237\u5916\u73af\u5883\u7684\u7ed3\u6784-\u89c6\u56fe\u534f\u4f5c\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u878d\u5408\u4ee5\u53ca\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u9ad8\u4fdd\u771f\u6e32\u67d3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u3002", "method": "1. \u8de8\u7ed3\u6784\u534f\u4f5c\u6a21\u5757\u7ed3\u5408\u5168\u5c40\u4e09\u5e73\u9762\u8868\u793a\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u7f51\u683c\u7279\u5f81\uff1b2. \u8de8\u89c6\u56fe\u8f85\u52a9\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "result": "\u572813\u4e2a\u591a\u6837\u5316\u5927\u89c4\u6a21\u573a\u666f\u4e2d\uff0cPSNR\u63d0\u53471-2 dB\uff0cSSIM\u63d0\u53470.1-0.2\uff0c\u91cd\u5efa\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SplatCo\u4e3a\u5927\u89c4\u6a21\u65e0\u8fb9\u754c\u573a\u666f\u7684\u9ad8\u4fdd\u771f\u6e32\u67d3\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "relevance": 20.0}}
{"id": "2505.17147", "pdf": "https://arxiv.org/pdf/2505.17147", "abs": "https://arxiv.org/abs/2505.17147", "authors": ["Weiyang Guo", "Jing Li", "Wenya Wang", "YU LI", "Daojing He", "Jun Yu", "Min Zhang"], "title": "MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming", "categories": ["cs.CR", "cs.AI"], "comment": "19 pages,6 figures,ACL2025", "summary": "The proliferation of jailbreak attacks against large language models (LLMs)\nhighlights the need for robust security measures. However, in multi-round\ndialogues, malicious intentions may be hidden in interactions, leading LLMs to\nbe more prone to produce harmful responses. In this paper, we propose the\n\\textbf{M}ulti-\\textbf{T}urn \\textbf{S}afety \\textbf{A}lignment (\\ourapproach)\nframework, to address the challenge of securing LLMs in multi-round\ninteractions. It consists of two stages: In the thought-guided attack learning\nstage, the red-team model learns about thought-guided multi-round jailbreak\nattacks to generate adversarial prompts. In the adversarial iterative\noptimization stage, the red-team model and the target model continuously\nimprove their respective capabilities in interaction. Furthermore, we introduce\na multi-turn reinforcement learning algorithm based on future rewards to\nenhance the robustness of safety alignment. Experimental results show that the\nred-team model exhibits state-of-the-art attack capabilities, while the target\nmodel significantly improves its performance on safety benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff08MTSA\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u653b\u51fb\u5b66\u4e60\u548c\u5bf9\u6297\u4f18\u5316\uff09\u63d0\u5347LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u672a\u6765\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6076\u610f\u610f\u56fe\u53ef\u80fd\u9690\u85cf\uff0c\u5bfc\u81f4LLMs\u66f4\u6613\u751f\u6210\u6709\u5bb3\u54cd\u5e94\uff0c\u4e9f\u9700\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "1. \u601d\u60f3\u5f15\u5bfc\u7684\u653b\u51fb\u5b66\u4e60\u9636\u6bb5\uff1b2. \u5bf9\u6297\u6027\u8fed\u4ee3\u4f18\u5316\u9636\u6bb5\uff1b3. \u57fa\u4e8e\u672a\u6765\u5956\u52b1\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u7ea2\u961f\u6a21\u578b\u8868\u73b0\u51fa\u9876\u7ea7\u653b\u51fb\u80fd\u529b\uff0c\u76ee\u6807\u6a21\u578b\u5728\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MTSA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "relevance": 85.0}}
{"id": "2505.17741", "pdf": "https://arxiv.org/pdf/2505.17741", "abs": "https://arxiv.org/abs/2505.17741", "authors": ["Zijing Ou", "Ruixiang Zhang", "Yingzhen Li"], "title": "Discrete Neural Flow Samplers with Locally Equivariant Transformer", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Sampling from unnormalised discrete distributions is a fundamental problem\nacross various domains. While Markov chain Monte Carlo offers a principled\napproach, it often suffers from slow mixing and poor convergence. In this\npaper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and\nefficient framework for discrete sampling. DNFS learns the rate matrix of a\ncontinuous-time Markov chain such that the resulting dynamics satisfy the\nKolmogorov equation. As this objective involves the intractable partition\nfunction, we then employ control variates to reduce the variance of its Monte\nCarlo estimation, leading to a coordinate descent learning algorithm. To\nfurther facilitate computational efficiency, we propose locally equivaraint\nTransformer, a novel parameterisation of the rate matrix that significantly\nimproves training efficiency while preserving powerful network expressiveness.\nEmpirically, we demonstrate the efficacy of DNFS in a wide range of\napplications, including sampling from unnormalised distributions, training\ndiscrete energy-based models, and solving combinatorial optimisation problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Discrete Neural Flow Samplers (DNFS)\uff0c\u4e00\u79cd\u53ef\u8bad\u7ec3\u4e14\u9ad8\u6548\u7684\u79bb\u6563\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u901f\u7387\u77e9\u9635\u5b66\u4e60\uff0c\u7ed3\u5408\u63a7\u5236\u53d8\u91cf\u548c\u5c40\u90e8\u7b49\u53d8Transformer\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u79bb\u6563\u5206\u5e03\u91c7\u6837\u4e2d\u4f20\u7edf\u65b9\u6cd5\uff08\u5982MCMC\uff09\u6536\u655b\u6162\u548c\u6df7\u5408\u5dee\u7684\u95ee\u9898\u3002", "method": "DNFS\u5b66\u4e60\u901f\u7387\u77e9\u9635\u4ee5\u6ee1\u8db3Kolmogorov\u65b9\u7a0b\uff0c\u4f7f\u7528\u63a7\u5236\u53d8\u91cf\u964d\u4f4e\u65b9\u5dee\uff0c\u5e76\u5f15\u5165\u5c40\u90e8\u7b49\u53d8Transformer\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u672a\u5f52\u4e00\u5316\u5206\u5e03\u91c7\u6837\u3001\u79bb\u6563\u80fd\u91cf\u6a21\u578b\u8bad\u7ec3\u548c\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DNFS\u4e3a\u79bb\u6563\u91c7\u6837\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17399", "pdf": "https://arxiv.org/pdf/2505.17399", "abs": "https://arxiv.org/abs/2505.17399", "authors": ["Haoyu Sun", "Huichen Will Wang", "Jiawei Gu", "Linjie Li", "Yu Cheng"], "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow", "categories": ["cs.CL"], "comment": null, "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) \\textbf{across the full front-end\ndevelopment pipeline}. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.", "AI": {"tldr": "FullFront\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u524d\u7aef\u5f00\u53d1\u5168\u6d41\u7a0b\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u8bbe\u8ba1\u3001\u89c6\u89c9\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u8f6c\u6362\uff0c\u800cFullFront\u65e8\u5728\u8bc4\u4f30MLLMs\u5728\u524d\u7aef\u5f00\u53d1\u5168\u6d41\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "FullFront\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\u5c06\u771f\u5b9e\u7f51\u9875\u8f6c\u6362\u4e3a\u6807\u51c6\u5316HTML\uff0c\u8bc4\u4f30MLLMs\u5728\u7f51\u9875\u8bbe\u8ba1\u3001\u89c6\u89c9\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6d4b\u8bd5\u663e\u793aMLLMs\u5728\u9875\u9762\u611f\u77e5\u3001\u4ee3\u7801\u751f\u6210\uff08\u5c24\u5176\u662f\u56fe\u50cf\u5904\u7406\u548c\u5e03\u5c40\uff09\u548c\u4ea4\u4e92\u5b9e\u73b0\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "FullFront\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u524d\u7aef\u5de5\u7a0b\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.17955", "pdf": "https://arxiv.org/pdf/2505.17955", "abs": "https://arxiv.org/abs/2505.17955", "authors": ["Yujin Jeong", "Arnas Uselis", "Seong Joon Oh", "Anna Rohrbach"], "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply", "categories": ["cs.CV"], "comment": null, "summary": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u5224\u522b\u6027\u4efb\u52a1\u4e2d\u7684\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u9886\u57df\u5dee\u5f02\u548c\u65f6\u95f4\u6b65\u6743\u91cd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u5224\u522b\u6027\u4efb\u52a1\u4e2d\u7684\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6269\u6563\u6a21\u578b\uff08SD 1.5\u30012.0\u548c3-m\uff09\u572810\u4e2a\u6570\u636e\u96c6\u548c30\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u5f15\u5165\u81ea\u5efa\u8bca\u65ad\u57fa\u51c6Self-Bench\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6027\u80fd\u53d7\u9886\u57df\u5dee\u5f02\u548c\u65f6\u95f4\u6b65\u6743\u91cd\u5f71\u54cd\u3002", "conclusion": "\u6269\u6563\u5206\u7c7b\u5668\u5177\u6709\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u9700\u8003\u8651\u7279\u5b9a\u6761\u4ef6\u3002", "relevance": 40.0}}
{"id": "2505.17148", "pdf": "https://arxiv.org/pdf/2505.17148", "abs": "https://arxiv.org/abs/2505.17148", "authors": ["Tristan Karch", "Jakhongir Saydaliev", "Isabella Di Lenardo", "Fr\u00e9d\u00e9ric Kaplan"], "title": "LLM-Powered Agents for Navigating Venice's Historical Cadastre", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Cadastral data reveal key information about the historical organization of\ncities but are often non-standardized due to diverse formats and human\nannotations, complicating large-scale analysis. We explore as a case study\nVenice's urban history during the critical period from 1740 to 1808, capturing\nthe transition following the fall of the ancient Republic and the Ancien\nR\\'egime. This era's complex cadastral data, marked by its volume and lack of\nuniform structure, presents unique challenges that our approach adeptly\nnavigates, enabling us to generate spatial queries that bridge past and present\nurban landscapes. We present a text-to-programs framework that leverages Large\nLanguage Models (LLMs) to translate natural language queries into executable\ncode for processing historical cadastral records. Our methodology implements\ntwo complementary techniques: a text-to-SQL approach for handling structured\nqueries about specific cadastral information, and a text-to-Python approach for\ncomplex analytical operations requiring custom data manipulation. We propose a\ntaxonomy that classifies historical research questions based on their\ncomplexity and analytical requirements, mapping them to the most appropriate\ntechnical approach. This framework is supported by an investigation into the\nexecution consistency of the system, alongside a qualitative analysis of the\nanswers it produces. By ensuring interpretability and minimizing hallucination\nthrough verifiable program outputs, we demonstrate the system's effectiveness\nin reconstructing past population information, property features, and\nspatiotemporal comparisons in Venice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6587\u672c\u5230\u7a0b\u5e8f\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5386\u53f2\u5730\u7c4d\u6570\u636e\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u3002", "motivation": "\u5386\u53f2\u5730\u7c4d\u6570\u636e\u683c\u5f0f\u591a\u6837\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u96be\u4ee5\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7LLMs\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u652f\u6301\u5bf9\u5a01\u5c3c\u65af1740-1808\u5e74\u5173\u952e\u65f6\u671f\u57ce\u5e02\u5386\u53f2\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1a\u6587\u672c\u5230SQL\u5904\u7406\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u6587\u672c\u5230Python\u5904\u7406\u590d\u6742\u5206\u6790\u64cd\u4f5c\u3002\u63d0\u51fa\u5206\u7c7b\u6cd5\u5c06\u7814\u7a76\u95ee\u9898\u6620\u5c04\u5230\u5408\u9002\u7684\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u7a0b\u5e8f\u8f93\u51fa\u786e\u4fdd\u89e3\u91ca\u6027\uff0c\u6210\u529f\u91cd\u5efa\u5a01\u5c3c\u65af\u8fc7\u53bb\u7684\u4eba\u53e3\u4fe1\u606f\u3001\u8d22\u4ea7\u7279\u5f81\u548c\u65f6\u7a7a\u6bd4\u8f83\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5386\u53f2\u5730\u7c4d\u6570\u636e\u7684\u5206\u6790\u96be\u9898\uff0c\u5c55\u793a\u4e86LLMs\u5728\u590d\u6742\u5386\u53f2\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17745", "pdf": "https://arxiv.org/pdf/2505.17745", "abs": "https://arxiv.org/abs/2505.17745", "authors": ["Zeyuan Ma", "Yue-Jiao Gong", "Hongshu Guo", "Wenjie Qiu", "Sijie Ma", "Hongqiao Lian", "Jiajun Zhan", "Kaixu Chen", "Chen Wang", "Zhiyang Huang", "Zechuan Huang", "Guojun Peng", "Ran Cheng", "Yining Ma"], "title": "MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Meta-Black-Box Optimization (MetaBBO) streamlines the automation of\noptimization algorithm design through meta-learning. It typically employs a\nbi-level structure: the meta-level policy undergoes meta-training to reduce the\nmanual effort required in developing algorithms for low-level optimization\ntasks. The original MetaBox (2023) provided the first open-source framework for\nreinforcement learning-based single-objective MetaBBO. However, its relatively\nnarrow scope no longer keep pace with the swift advancement in this field. In\nthis paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a\nmilestone upgrade with four novel features: 1) a unified architecture\nsupporting RL, evolutionary, and gradient-based approaches, by which we\nreproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which\nreduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite\nof 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,\nmulti-objective, multi-model, and multi-task optimization scenarios; 4)\nplentiful and extensible interfaces for custom analysis/visualization and\nintegrating to external optimization tools/benchmarks. To show the utility of\nMetaBox-v2, we carry out a systematic case study that evaluates the built-in\nbaselines in terms of the optimization performance, generalization ability and\nlearning efficiency. Valuable insights are concluded from thorough and detailed\nanalysis for practitioners and those new to the field.", "AI": {"tldr": "MetaBox-v2\u662f\u4e00\u4e2a\u5347\u7ea7\u7248\u7684\u5143\u9ed1\u76d2\u4f18\u5316\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u4f9b\u9ad8\u6548\u5e76\u884c\u5316\u548c\u4e30\u5bcc\u63a5\u53e3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f18\u5316\u4efb\u52a1\u3002", "motivation": "\u539f\u59cbMetaBox\u6846\u67b6\u8303\u56f4\u8f83\u7a84\uff0c\u65e0\u6cd5\u8ddf\u4e0a\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0cMetaBox-v2\u65e8\u5728\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u529f\u80fd\u548c\u66f4\u9ad8\u7684\u6548\u7387\u3002", "method": "MetaBox-v2\u91c7\u7528\u7edf\u4e00\u67b6\u6784\u652f\u6301RL\u3001\u8fdb\u5316\u548c\u68af\u5ea6\u65b9\u6cd5\uff0c\u5e76\u884c\u5316\u65b9\u6848\u63d0\u5347\u6548\u7387\uff0c\u5e76\u63d0\u4f9b18\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "MetaBox-v2\u663e\u8457\u51cf\u5c11\u8bad\u7ec3/\u6d4b\u8bd5\u65f6\u95f4\uff0810-40\u500d\uff09\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4f18\u5316\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "MetaBox-v2\u4e3a\u5b9e\u8df5\u8005\u548c\u65b0\u624b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u89c1\u89e3\uff0c\u6269\u5c55\u4e86\u5143\u4f18\u5316\u6846\u67b6\u7684\u529f\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17407", "pdf": "https://arxiv.org/pdf/2505.17407", "abs": "https://arxiv.org/abs/2505.17407", "authors": ["Zhi Rui Tam", "Cheng-Kuang Wu", "Yu Ying Chiu", "Chieh-Yen Lin", "Yun-Nung Chen", "Hung-yi Lee"], "title": "Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have demonstrated impressive performance across\na range of reasoning tasks, yet little is known about their internal reasoning\nprocesses in multilingual settings. We begin with a critical question: {\\it In\nwhich language do these models reason when solving problems presented in\ndifferent languages?} Our findings reveal that, despite multilingual training,\nLRMs tend to default to reasoning in high-resource languages (e.g., English) at\ntest time, regardless of the input language. When constrained to reason in the\nsame language as the input, model performance declines, especially for\nlow-resource languages. In contrast, reasoning in high-resource languages\ngenerally preserves performance. We conduct extensive evaluations across\nreasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks\n(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies\nby task type: input-language reasoning degrades performance on reasoning tasks\nbut benefits cultural tasks, while safety evaluations exhibit language-specific\nbehavior. By exposing these linguistic biases in LRMs, our work highlights a\ncritical step toward developing more equitable models that serve users across\ndiverse linguistic backgrounds.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u503e\u5411\u4e8e\u4f7f\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u8fdb\u884c\u63a8\u7406\uff0c\u5373\u4f7f\u8f93\u5165\u8bed\u8a00\u4e0d\u540c\u3002\u5f3a\u5236\u4f7f\u7528\u8f93\u5165\u8bed\u8a00\u63a8\u7406\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u63a2\u7d22LRMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff0c\u63ed\u793a\u5176\u8bed\u8a00\u504f\u597d\u53ca\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u591a\u8bed\u8a00\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u63a8\u7406\u8868\u73b0\uff0c\u6db5\u76d6\u63a8\u7406\u4efb\u52a1\uff08MMMLU, MATH-500\uff09\u548c\u975e\u63a8\u7406\u4efb\u52a1\uff08CulturalBench, LMSYS-toxic\uff09\u3002", "result": "LRMs\u503e\u5411\u4e8e\u4f7f\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u63a8\u7406\uff0c\u5f3a\u5236\u4f7f\u7528\u8f93\u5165\u8bed\u8a00\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002\u4efb\u52a1\u7c7b\u578b\u5f71\u54cd\u8bed\u8a00\u9009\u62e9\u7684\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LRMs\u7684\u8bed\u8a00\u504f\u89c1\uff0c\u4e3a\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.17959", "pdf": "https://arxiv.org/pdf/2505.17959", "abs": "https://arxiv.org/abs/2505.17959", "authors": ["Nguyen Duc", "Yan-Ling Lai", "Patrick Madlindl", "Xinyuan Zhu", "Benedikt Schwab", "Olaf Wysocki", "Ludwig Hoegner", "Thomas H. Kolbe"], "title": "Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to PFG Journal of Photogrammetry, Remote Sensing and\n  Geoinformation Science", "summary": "Owing to the typical long-tail data distribution issues, simulating\ndomain-gap-free synthetic data is crucial in robotics, photogrammetry, and\ncomputer vision research. The fundamental challenge pertains to credibly\nmeasuring the difference between real and simulated data. Such a measure is\nvital for safety-critical applications, such as automated driving, where\nout-of-domain samples may impact a car's perception and cause fatal accidents.\nPrevious work has commonly focused on simulating data on one scene and\nanalyzing performance on a different, real-world scene, hampering the disjoint\nanalysis of domain gap coming from networks' deficiencies, class definitions,\nand object representation. In this paper, we propose a novel approach to\nmeasuring the domain gap between the real world sensor observations and\nsimulated data representing the same location, enabling comprehensive domain\ngap analysis. To measure such a domain gap, we introduce a novel metric\nDoGSS-PCL and evaluation assessing the geometric and semantic quality of the\nsimulated point cloud. Our experiments corroborate that the introduced approach\ncan be used to measure the domain gap. The tests also reveal that synthetic\nsemantic point clouds may be used for training deep neural networks,\nmaintaining the performance at the 50/50 real-to-synthetic ratio. We strongly\nbelieve that this work will facilitate research on credible data simulation and\nallow for at-scale deployment in automated driving testing and digital\ntwinning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5DoGSS-PCL\uff0c\u7528\u4e8e\u6d4b\u91cf\u771f\u5b9e\u4e16\u754c\u4f20\u611f\u5668\u89c2\u6d4b\u4e0e\u6a21\u62df\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u652f\u6301\u5168\u9762\u7684\u9886\u57df\u5dee\u8ddd\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u957f\u5c3e\u6570\u636e\u5206\u5e03\u95ee\u9898\uff0c\u786e\u4fdd\u5b89\u5168\u5173\u952e\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u6a21\u62df\u6570\u636e\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f15\u5165DoGSS-PCL\u5ea6\u91cf\uff0c\u8bc4\u4f30\u6a21\u62df\u70b9\u4e91\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6d4b\u91cf\u9886\u57df\u5dee\u8ddd\uff0c\u4e14\u5408\u6210\u8bed\u4e49\u70b9\u4e91\u53ef\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u53ef\u4fe1\u6570\u636e\u6a21\u62df\u7814\u7a76\uff0c\u5e76\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u548c\u6570\u5b57\u5b6a\u751f\u7684\u5927\u89c4\u6a21\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.17748", "pdf": "https://arxiv.org/pdf/2505.17748", "abs": "https://arxiv.org/abs/2505.17748", "authors": ["Kerol Djoumessi", "Philipp Berens"], "title": "Soft-CAM: Making black box models self-explainable for high-stakes decisions", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) are widely used for high-stakes\napplications like medicine, often surpassing human performance. However, most\nexplanation methods rely on post-hoc attribution, approximating the\ndecision-making process of already trained black-box models. These methods are\noften sensitive, unreliable, and fail to reflect true model reasoning, limiting\ntheir trustworthiness in critical applications. In this work, we introduce\nSoftCAM, a straightforward yet effective approach that makes standard CNN\narchitectures inherently interpretable. By removing the global average pooling\nlayer and replacing the fully connected classification layer with a\nconvolution-based class evidence layer, SoftCAM preserves spatial information\nand produces explicit class activation maps that form the basis of the model's\npredictions. Evaluated on three medical datasets, SoftCAM maintains\nclassification performance while significantly improving both the qualitative\nand quantitative explanation compared to existing post-hoc methods. Our results\ndemonstrate that CNNs can be inherently interpretable without compromising\nperformance, advancing the development of self-explainable deep learning for\nhigh-stakes decision-making.", "AI": {"tldr": "SoftCAM\u662f\u4e00\u79cd\u4f7f\u6807\u51c6CNN\u67b6\u6784\u5177\u6709\u5185\u5728\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\u5e76\u66ff\u6362\u5206\u7c7b\u5c42\u4e3a\u5377\u79ef\u5c42\uff0c\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\u5e76\u751f\u6210\u660e\u786e\u7684\u7c7b\u522b\u6fc0\u6d3b\u56fe\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\uff08\u5982\u540e\u5904\u7406\u5f52\u56e0\uff09\u5728\u5173\u952e\u5e94\u7528\u4e2d\u4e0d\u53ef\u9760\u4e14\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86CNN\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u79fb\u9664\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\uff0c\u7528\u5377\u79ef\u5c42\u66ff\u6362\u5168\u8fde\u63a5\u5206\u7c7b\u5c42\uff0c\u751f\u6210\u663e\u5f0f\u7684\u7c7b\u522b\u6fc0\u6d3b\u56fe\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0cSoftCAM\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u8d28\u548c\u91cf\u3002", "conclusion": "CNN\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u9ad8\u98ce\u9669\u51b3\u7b56\u7684\u81ea\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u3002", "relevance": 40.0}}
{"id": "2505.17413", "pdf": "https://arxiv.org/pdf/2505.17413", "abs": "https://arxiv.org/abs/2505.17413", "authors": ["Niranjan Chebrolu", "Gerard Christopher Yeo", "Kokil Jaidka"], "title": "Conversations: Love Them, Hate Them, Steer Them", "categories": ["cs.CL"], "comment": "11 pages, 8 figures, 7 tables", "summary": "Large Language Models (LLMs) demonstrate increasing conversational fluency,\nyet instilling them with nuanced, human-like emotional expression remains a\nsignificant challenge. Current alignment techniques often address surface-level\noutput or require extensive fine-tuning. This paper demonstrates that targeted\nactivation engineering can steer LLaMA 3.1-8B to exhibit more human-like\nemotional nuances. We first employ attribution patching to identify causally\ninfluential components, to find a key intervention locus by observing\nactivation patterns during diagnostic conversational tasks. We then derive\nemotional expression vectors from the difference in the activations generated\nby contrastive text pairs (positive vs. negative examples of target emotions).\nApplying these vectors to new conversational prompts significantly enhances\nemotional characteristics: steered responses show increased positive sentiment\n(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of\ngreater personal engagement. Our findings offer a precise and interpretable\nmethod for controlling specific emotional attributes in LLMs, contributing to\ndeveloping more aligned and empathetic conversational AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6fc0\u6d3b\u5de5\u7a0b\u5f15\u5bfcLLaMA 3.1-8B\u6a21\u578b\u751f\u6210\u66f4\u5177\u4eba\u7c7b\u60c5\u611f\u8868\u8fbe\u7684\u5bf9\u8bdd\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u5bf9\u9f50\u6280\u672f\u901a\u5e38\u4ec5\u89e3\u51b3\u8868\u9762\u8f93\u51fa\u6216\u9700\u8981\u5927\u91cf\u5fae\u8c03\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u65b9\u6cd5\u589e\u5f3aLLMs\u7684\u60c5\u611f\u8868\u8fbe\u3002", "method": "\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u5173\u952e\u5e72\u9884\u70b9\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u6587\u672c\u5bf9\u751f\u6210\u60c5\u611f\u8868\u8fbe\u5411\u91cf\uff0c\u5e94\u7528\u4e8e\u65b0\u5bf9\u8bdd\u63d0\u793a\u3002", "result": "\u5f15\u5bfc\u540e\u7684\u54cd\u5e94\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u79ef\u6781\u60c5\u611f\uff08\u5982\u559c\u60a6\u3001\u4fe1\u4efb\uff09\u548c\u66f4\u591a\u7b2c\u4e00\u4eba\u79f0\u4ee3\u8bcd\u4f7f\u7528\uff0c\u8868\u660e\u66f4\u5f3a\u7684\u4e2a\u4eba\u53c2\u4e0e\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63a7\u5236LLMs\u7279\u5b9a\u60c5\u611f\u5c5e\u6027\u63d0\u4f9b\u4e86\u7cbe\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5177\u5bf9\u9f50\u6027\u548c\u540c\u7406\u5fc3\u7684\u5bf9\u8bddAI\u3002", "relevance": 85.0}}
{"id": "2505.17972", "pdf": "https://arxiv.org/pdf/2505.17972", "abs": "https://arxiv.org/abs/2505.17972", "authors": ["Kazi Mahmudul Hassan", "Xuyang Zhao", "Hidenori Sugano", "Toshihisa Tanaka"], "title": "MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages, 6 figures, 12 tables", "summary": "Feature engineering for generalized seizure detection models remains a\nsignificant challenge. Recently proposed models show variable performance\ndepending on the training data and remain ineffective at accurately\ndistinguishing artifacts from seizure data. In this study, we propose a novel\nend-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which\nefficiently distinguishes seizure events from background electroencephalogram\n(EEG) and artifacts/noise by capturing both temporal dependencies across\ndifferent time frames and spatial relationships between channels. The model has\nthree modules: convolution, feature extraction, and predictor. The convolution\nmodule extracts features through depth-wise and spatio-temporal convolution.\nThe feature extraction module individually reduces the feature dimension\nextracted from EEG segments and their sub-segments. Subsequently, the extracted\nfeatures are concatenated into a single vector for classification using a fully\nconnected classifier called the predictor module. In addition, an anomaly\nscore-based post-classification processing technique was introduced to reduce\nthe false-positive rates of the model. Experimental results were reported and\nanalyzed using different parameter settings and datasets (Siena (public) and\nJuntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the\nconventional non-multiresolution approach, improving the F1 scores from 0.177\nto 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%\nand 20.62%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMR-EEGWaveNet\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u533a\u5206\u766b\u75eb\u4e8b\u4ef6\u4e0e\u80cc\u666fEEG\u53ca\u4f2a\u5f71/\u566a\u58f0\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u7279\u5f81\u63d0\u53d6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u533a\u5206\u766b\u75eb\u4e8b\u4ef6\u4e0e\u4f2a\u5f71/\u566a\u58f0\u65f6\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u3002", "method": "\u6a21\u578b\u5305\u542b\u5377\u79ef\u3001\u7279\u5f81\u63d0\u53d6\u548c\u9884\u6d4b\u4e09\u4e2a\u6a21\u5757\uff0c\u7ed3\u5408\u6df1\u5ea6\u65f6\u7a7a\u5377\u79ef\u548c\u591a\u5206\u8fa8\u7387\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u5f02\u5e38\u5206\u6570\u540e\u5904\u7406\u6280\u672f\u964d\u4f4e\u5047\u9633\u6027\u7387\u3002", "result": "\u5728Siena\u548cJuntendo\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u5206\u522b\u4ece0.177\u63d0\u5347\u81f30.336\u548c0.327\u63d0\u5347\u81f30.488\uff0c\u7cbe\u5ea6\u5206\u522b\u63d0\u9ad815.9%\u548c20.62%\u3002", "conclusion": "MR-EEGWaveNet\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u7279\u5f81\u63d0\u53d6\u548c\u540e\u5904\u7406\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u766b\u75eb\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.17749", "pdf": "https://arxiv.org/pdf/2505.17749", "abs": "https://arxiv.org/abs/2505.17749", "authors": ["Ghada Sokar", "Pablo Samuel Castro"], "title": "Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scaling deep reinforcement learning in pixel-based environments presents a\nsignificant challenge, often resulting in diminished performance. While recent\nworks have proposed algorithmic and architectural approaches to address this,\nthe underlying cause of the performance drop remains unclear. In this paper, we\nidentify the connection between the output of the encoder (a stack of\nconvolutional layers) and the ensuing dense layers as the main underlying\nfactor limiting scaling capabilities; we denote this connection as the\nbottleneck, and we demonstrate that previous approaches implicitly target this\nbottleneck. As a result of our analyses, we present global average pooling as a\nsimple yet effective way of targeting the bottleneck, thereby avoiding the\ncomplexity of earlier approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u50cf\u7d20\u73af\u5883\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u74f6\u9888\u5728\u4e8e\u7f16\u7801\u5668\u8f93\u51fa\u4e0e\u5bc6\u96c6\u5c42\u7684\u8fde\u63a5\uff0c\u5e76\u63d0\u51fa\u5168\u5c40\u5e73\u5747\u6c60\u5316\u4f5c\u4e3a\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u50cf\u7d20\u73af\u5883\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u660e\u786e\u6027\u80fd\u74f6\u9888\u7684\u6839\u6e90\u3002", "method": "\u5206\u6790\u7f16\u7801\u5668\u8f93\u51fa\u4e0e\u5bc6\u96c6\u5c42\u8fde\u63a5\u7684\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u5168\u5c40\u5e73\u5747\u6c60\u5316\u65b9\u6cd5\u3002", "result": "\u5168\u5c40\u5e73\u5747\u6c60\u5316\u6709\u6548\u89e3\u51b3\u4e86\u6027\u80fd\u74f6\u9888\uff0c\u907f\u514d\u4e86\u590d\u6742\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "conclusion": "\u5168\u5c40\u5e73\u5747\u6c60\u5316\u662f\u89e3\u51b3\u50cf\u7d20\u73af\u5883\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u74f6\u9888\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\u3002", "relevance": 60.0}}
{"id": "2505.17420", "pdf": "https://arxiv.org/pdf/2505.17420", "abs": "https://arxiv.org/abs/2505.17420", "authors": ["Ning Yang", "Fangxin Liu", "Junjie Wang", "Tao Yang", "Kan Liu", "Haibing Guan", "Li Jiang"], "title": "DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages,5 figures", "summary": "Large language models (LLMs) have achieved remarkable performance across a\nwide range of NLP tasks. However, their substantial inference cost poses a\nmajor barrier to real-world deployment, especially in latency-sensitive\nscenarios. To address this challenge, we propose \\textbf{DASH}, an adaptive\nlayer-skipping framework that dynamically selects computation paths conditioned\non input characteristics. We model the skipping process as a Markov Decision\nProcess (MDP), enabling fine-grained token-level decisions based on\nintermediate representations. To mitigate potential performance degradation\ncaused by skipping, we introduce a lightweight compensation mechanism that\ninjects differential rewards into the decision process. Furthermore, we design\nan asynchronous execution strategy that overlaps layer computation with policy\nevaluation to minimize runtime overhead. Experiments on multiple LLM\narchitectures and NLP benchmarks show that our method achieves significant\ninference acceleration while maintaining competitive task performance,\noutperforming existing methods.", "AI": {"tldr": "DASH\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5c42\u8df3\u8fc7\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8ba1\u7b97\u8def\u5f84\u51cf\u5c11LLM\u7684\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\uff0c\u7279\u522b\u662f\u5728\u5ef6\u8fdf\u654f\u611f\u7684\u573a\u666f\u4e2d\u3002", "method": "\u5c06\u8df3\u8fc7\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u57fa\u4e8e\u8f93\u5165\u7279\u5f81\u52a8\u6001\u9009\u62e9\u8ba1\u7b97\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u8865\u507f\u673a\u5236\u4ee5\u51cf\u5c11\u6027\u80fd\u635f\u5931\u3002\u91c7\u7528\u5f02\u6b65\u6267\u884c\u7b56\u7565\u51cf\u5c11\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "result": "\u5728\u591a\u4e2aLLM\u67b6\u6784\u548cNLP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDASH\u663e\u8457\u52a0\u901f\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u4efb\u52a1\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DASH\u901a\u8fc7\u52a8\u6001\u5c42\u8df3\u8fc7\u548c\u8865\u507f\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.17973", "pdf": "https://arxiv.org/pdf/2505.17973", "abs": "https://arxiv.org/abs/2505.17973", "authors": ["Simone Gaisbauer", "Prabin Gyawali", "Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to MMT, Xiamen, China; ISPRS Annals", "summary": "Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4f20\u7edf\u624b\u5de5\u65b9\u6cd5\u548c\u53ef\u5b66\u4e60\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u540e\u8005\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u8005\u3002", "motivation": "\u586b\u8865\u7ecf\u5178\u4e0e\u53ef\u5b66\u4e60\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u5728\u8bed\u4e493D\u5efa\u7b51\u76f8\u673a-\u6a21\u578b\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u5168\u9762\u6bd4\u8f83\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u901a\u8fc7PnP\u7b97\u6cd5\u8bc4\u4f30\u7edd\u5bf9\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "result": "\u53ef\u5b66\u4e60\u65b9\u6cd5\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u53ef\u5b66\u4e60\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u6709\u671b\u63a8\u52a8\u6a21\u578b\u5316\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "relevance": 20.0}}
{"id": "2505.17760", "pdf": "https://arxiv.org/pdf/2505.17760", "abs": "https://arxiv.org/abs/2505.17760", "authors": ["Leon Eshuijs", "Archie Chaudhury", "Alan McBeth", "Ethan Nguyen"], "title": "But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent safety evaluations of Large Language Models (LLMs) show that many\nmodels exhibit dishonest behavior, such as sycophancy. However, most honesty\nbenchmarks focus exclusively on factual knowledge or explicitly harmful\nbehavior and rely on external judges, which are often unable to detect less\nobvious forms of dishonesty. In this work, we introduce a new framework, Judge\nUsing Safety-Steered Alternatives (JUSSA), which utilizes steering vectors\ntrained on a single sample to elicit more honest responses from models, helping\nLLM-judges in the detection of dishonest behavior. To test our framework, we\nintroduce a new manipulation dataset with prompts specifically designed to\nelicit deceptive responses. We find that JUSSA enables LLM judges to better\ndifferentiate between dishonest and benign responses, and helps them identify\nsubtle instances of manipulative behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6JUSSA\uff0c\u901a\u8fc7\u8bad\u7ec3\u5b89\u5168\u5bfc\u5411\u5411\u91cf\u63d0\u5347LLM\u7684\u8bda\u5b9e\u6027\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u64cd\u7eb5\u6570\u636e\u96c6\u7528\u4e8e\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u8bda\u5b9e\u6027\u8bc4\u6d4b\u591a\u5173\u6ce8\u663e\u6027\u6709\u5bb3\u884c\u4e3a\u6216\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u96be\u4ee5\u68c0\u6d4b\u9690\u853d\u7684\u4e0d\u8bda\u5b9e\u884c\u4e3a\u3002", "method": "\u5f00\u53d1JUSSA\u6846\u67b6\uff0c\u5229\u7528\u5b89\u5168\u5bfc\u5411\u5411\u91cf\u8bad\u7ec3\u5355\u4e00\u6837\u672c\uff0c\u63d0\u5347LLM\u88c1\u5224\u68c0\u6d4b\u4e0d\u8bda\u5b9e\u884c\u4e3a\u7684\u80fd\u529b\u3002", "result": "JUSSA\u5e2e\u52a9LLM\u88c1\u5224\u66f4\u597d\u533a\u5206\u8bda\u5b9e\u4e0e\u4e0d\u8bda\u5b9e\u56de\u7b54\uff0c\u5e76\u8bc6\u522b\u7ec6\u5fae\u64cd\u7eb5\u884c\u4e3a\u3002", "conclusion": "JUSSA\u4e3a\u68c0\u6d4bLLM\u9690\u853d\u4e0d\u8bda\u5b9e\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.17427", "pdf": "https://arxiv.org/pdf/2505.17427", "abs": "https://arxiv.org/abs/2505.17427", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Zezhong Wang", "Huimin Wang", "Yutian Zhao", "Bin Liang", "Yefeng Zheng", "Binyang Li", "Kam-Fai Wong", "Xian Wu"], "title": "T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\nperformance in Contextual Question Answering (CQA). However, prior approaches\ntypically employ elaborate reasoning strategies regardless of question\ncomplexity, leading to low adaptability. Recent efficient test-time scaling\nmethods introduce budget constraints or early stop mechanisms to avoid\noverthinking for straightforward questions. But they add human bias to the\nreasoning process and fail to leverage models' inherent reasoning capabilities.\nTo address these limitations, we present T$^2$: Think-to-Think, a novel\nframework that dynamically adapts reasoning depth based on question complexity.\nT$^2$ leverages the insight that if an LLM can effectively solve similar\nquestions using specific reasoning strategies, it can apply the same strategy\nto the original question. This insight enables to adoption of concise reasoning\nfor straightforward questions while maintaining detailed analysis for complex\nproblems. T$^2$ works through four key steps: decomposing questions into\nstructural elements, generating similar examples with candidate reasoning\nstrategies, evaluating these strategies against multiple criteria, and applying\nthe most appropriate strategy to the original question. Experimental evaluation\nacross seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves\nhigher accuracy than baseline methods but also reduces computational overhead\nby up to 25.2\\%.", "AI": {"tldr": "T$^2$ (Think-to-Think) \u662f\u4e00\u4e2a\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u7b56\u7565\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u95ee\u7b54\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u8981\u4e48\u8fc7\u5ea6\u63a8\u7406\u7b80\u5355\u95ee\u9898\uff0c\u8981\u4e48\u5f15\u5165\u4eba\u4e3a\u504f\u89c1\u3002T$^2$ \u65e8\u5728\u5229\u7528\u6a21\u578b\u56fa\u6709\u63a8\u7406\u80fd\u529b\uff0c\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002", "method": "T$^2$ \u901a\u8fc7\u5206\u89e3\u95ee\u9898\u3001\u751f\u6210\u76f8\u4f3c\u793a\u4f8b\u3001\u8bc4\u4f30\u5019\u9009\u7b56\u7565\u5e76\u5e94\u7528\u6700\u4f18\u7b56\u7565\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT$^2$ \u4e0d\u4ec5\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u8fd8\u51cf\u5c11\u4e8625.2%\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "T$^2$ \u5c55\u793a\u4e86\u52a8\u6001\u63a8\u7406\u7b56\u7565\u5728\u63d0\u5347LLM\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.17982", "pdf": "https://arxiv.org/pdf/2505.17982", "abs": "https://arxiv.org/abs/2505.17982", "authors": ["Bryan Wong", "Jong Woo Kim", "Huazhu Fu", "Mun Yong Yi"], "title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL", "AI": {"tldr": "HiVE-MIL\u662f\u4e00\u4e2a\u5206\u5c42\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u56fe\u7ed3\u6784\u548c\u52a8\u6001\u8fc7\u6ee4\u673a\u5236\u6539\u8fdb\u591a\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u5c11\u6837\u672c\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c3a\u5ea6\u6a21\u6001\u5185\u4ea4\u4e92\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u6001\u5bf9\u9f50\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u56fe\u7ed3\u6784\uff08\u8de8\u5c3a\u5ea6\u7236\u5b50\u94fe\u63a5\u548c\u540c\u5c3a\u5ea6\u5f02\u8d28\u8fb9\uff09\uff0c\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u52a8\u6001\u8fc7\u6ee4\u548c\u5206\u5c42\u5bf9\u6bd4\u635f\u5931\u3002", "result": "\u5728TCGA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8eVLM\u7684MIL\u65b9\u6cd5\uff0c16-shot\u4e0bF1\u63d0\u53474.1%\u3002", "conclusion": "\u8054\u5408\u5efa\u6a21\u5206\u5c42\u7ed3\u6784\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u5bf9\u6709\u9650\u75c5\u7406\u6570\u636e\u7684\u9ad8\u6548\u5b66\u4e60\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "relevance": 40.0}}
{"id": "2505.17761", "pdf": "https://arxiv.org/pdf/2505.17761", "abs": "https://arxiv.org/abs/2505.17761", "authors": ["Benjamin Walker", "Lingyi Yang", "Nicola Muca Cirone", "Cristopher Salvi", "Terry Lyons"], "title": "Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models", "categories": ["cs.LG"], "comment": "26 pages, 5 figures", "summary": "Structured Linear Controlled Differential Equations (SLiCEs) provide a\nunifying framework for sequence models with structured, input-dependent\nstate-transition matrices that retain the maximal expressivity of dense\nmatrices whilst being cheaper to compute. The framework encompasses existing\narchitectures, such as input-dependent block-diagonal linear recurrent neural\nnetworks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel\nvariants based on sparsity and the Walsh--Hadamard transform. We prove that,\nunlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing\nblock-diagonal, sparse, or Walsh--Hadamard matrices match the maximal\nexpressivity of dense matrices. Empirically, SLiCEs solve the $A_5$\nstate-tracking benchmark with a single layer, achieve best-in-class length\ngeneralisation on regular language tasks among parallel-in-time models, and\nmatch the state-of-the-art performance of log neural controlled differential\nequations on six multivariate time-series classification datasets while cutting\nthe average time per training step by a factor of twenty.", "AI": {"tldr": "SLiCEs\u6846\u67b6\u4e3a\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u8f93\u5165\u4f9d\u8d56\u7684\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\uff0c\u517c\u5177\u8868\u8fbe\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6db5\u76d6\u73b0\u6709\u67b6\u6784\u5e76\u5f15\u5165\u4e24\u79cd\u65b0\u53d8\u4f53\u3002", "motivation": "\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5bc6\u96c6\u77e9\u9635\u6700\u5927\u8868\u8fbe\u529b\uff0c\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u5e8f\u5217\u6a21\u578b\u6846\u67b6\u3002", "method": "\u63d0\u51faSLiCEs\u6846\u67b6\uff0c\u652f\u6301\u5757\u5bf9\u89d2\u3001\u7a00\u758f\u6216Walsh-Hadamard\u77e9\u9635\uff0c\u8bc1\u660e\u5176\u8868\u8fbe\u529b\u4e0e\u5bc6\u96c6\u77e9\u9635\u76f8\u5f53\u3002", "result": "SLiCEs\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u72b6\u6001\u8ddf\u8e2a\u57fa\u51c6\u3001\u957f\u5ea6\u6cdb\u5316\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u3002", "conclusion": "SLiCEs\u5728\u8868\u8fbe\u529b\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u3002", "relevance": 60.0}}
{"id": "2505.17441", "pdf": "https://arxiv.org/pdf/2505.17441", "abs": "https://arxiv.org/abs/2505.17441", "authors": ["Can Rager", "Chris Wendler", "Rohit Gandikota", "David Bau"], "title": "Discovering Forbidden Topics in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, LLM-crawler, that uses token prefilling to\nfind forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawl to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u62d2\u7edd\u53d1\u73b0\uff08refusal discovery\uff09\uff0c\u7528\u4e8e\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u62d2\u7edd\u8ba8\u8bba\u7684\u6240\u6709\u4e3b\u9898\u3002\u4f5c\u8005\u5f00\u53d1\u4e86LLM-crawler\u65b9\u6cd5\uff0c\u5229\u7528token\u9884\u586b\u5145\u6280\u672f\u53d1\u73b0\u88ab\u7981\u6b62\u7684\u4e3b\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u8c03\u6574\u540e\u53ef\u80fd\u5b58\u5728\u7684\u504f\u89c1\u3001\u8fb9\u754c\u548c\u5bf9\u9f50\u5931\u8d25\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7279\u5b9a\u653f\u6cbb\u6216\u793e\u4f1a\u8bae\u9898\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528LLM-crawler\uff0c\u901a\u8fc7token\u9884\u586b\u5145\u6280\u672f\u751f\u6210\u63d0\u793a\uff0c\u4ee5\u53d1\u73b0\u6a21\u578b\u62d2\u7edd\u8ba8\u8bba\u7684\u4e3b\u9898\u3002\u6d4b\u8bd5\u4e86\u591a\u4e2a\u5f00\u6e90\u548c\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u5728Tulu-3-8B\u6a21\u578b\u4e2d\uff0cLLM-crawler\u57281000\u4e2a\u63d0\u793a\u9884\u7b97\u5185\u53d1\u73b0\u4e8631/36\u4e2a\u88ab\u62d2\u7edd\u4e3b\u9898\u3002\u5728Claude-Haiku\u548c\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\uff08\u5982Llama-3.3-70B\u53ca\u5176\u53d8\u4f53\uff09\u4e2d\u4e5f\u53d1\u73b0\u4e86\u7c7b\u4f3c\u884c\u4e3a\uff0c\u5c24\u5176\u662fDeepSeek-R1-70B\u8868\u73b0\u51fa\u660e\u663e\u7684\u5ba1\u67e5\u8c03\u6574\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u62d2\u7edd\u53d1\u73b0\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u7cfb\u7edf\u7684\u504f\u89c1\u3001\u8fb9\u754c\u548c\u5bf9\u9f50\u5931\u8d25\u3002", "relevance": 85.0}}
{"id": "2505.17992", "pdf": "https://arxiv.org/pdf/2505.17992", "abs": "https://arxiv.org/abs/2505.17992", "authors": ["Fahd Alhamazani", "Yu-Kun Lai", "Paul L. Rosin"], "title": "Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction from 2D inputs, especially for non-rigid objects like\nhumans, presents unique challenges due to the significant range of possible\ndeformations. Traditional methods often struggle with non-rigid shapes, which\nrequire extensive training data to cover the entire deformation space. This\nstudy addresses these limitations by proposing a canonical pose reconstruction\nmodel that transforms single-view depth images of deformable shapes into a\ncanonical form. This alignment facilitates shape reconstruction by enabling the\napplication of rigid object reconstruction techniques, and supports recovering\nthe input pose in voxel representation as part of the reconstruction task,\nutilizing both the original and deformed depth images. Notably, our model\nachieves effective results with only a small dataset of approximately 300\nsamples. Experimental results on animal and human datasets demonstrate that our\nmodel outperforms other state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u89c6\u89d2\u6df1\u5ea6\u56fe\u50cf\u91cd\u5efa\u975e\u521a\u6027\u7269\u4f53\uff08\u5982\u4eba\u4f53\uff09\u7684\u89c4\u8303\u59ff\u6001\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u975e\u521a\u6027\u5f62\u72b6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u975e\u521a\u6027\u5f62\u72b6\u65f6\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u89c4\u8303\u59ff\u6001\u5bf9\u9f50\u7b80\u5316\u91cd\u5efa\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u5c06\u53d8\u5f62\u5f62\u72b6\u7684\u6df1\u5ea6\u56fe\u50cf\u8f6c\u6362\u4e3a\u89c4\u8303\u5f62\u5f0f\uff0c\u5e94\u7528\u521a\u6027\u7269\u4f53\u91cd\u5efa\u6280\u672f\uff0c\u5e76\u6062\u590d\u8f93\u5165\u59ff\u6001\u7684\u4f53\u7d20\u8868\u793a\u3002", "result": "\u6a21\u578b\u4ec5\u9700\u7ea6300\u4e2a\u6837\u672c\u5373\u53ef\u53d6\u5f97\u663e\u8457\u6548\u679c\uff0c\u5728\u52a8\u7269\u548c\u4eba\u4f53\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u89c4\u8303\u59ff\u6001\u91cd\u5efa\u6a21\u578b\u4e3a\u5c0f\u6570\u636e\u96c6\u4e0b\u7684\u975e\u521a\u6027\u7269\u4f53\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17154", "pdf": "https://arxiv.org/pdf/2505.17154", "abs": "https://arxiv.org/abs/2505.17154", "authors": ["Gertrude Hattoh", "Jeremiah Ayensu", "Nyarko Prince Ofori", "Solomon Eshun", "Darlington Akogo"], "title": "Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Advances in AI, particularly LLMs, have dramatically shortened drug discovery\ncycles by up to 40% and improved molecular target identification. However,\nthese innovations also raise dual-use concerns by enabling the design of toxic\ncompounds. Prompting Moremi Bio Agent without the safety guardrails to\nspecifically design novel toxic substances, our study generated 1020 novel\ntoxic proteins and 5,000 toxic small molecules. In-depth computational toxicity\nassessments revealed that all the proteins scored high in toxicity, with\nseveral closely matching known toxins such as ricin, diphtheria toxin, and\ndisintegrin-based snake venom proteins. Some of these novel agents showed\nsimilarities with other several known toxic agents including disintegrin\neristostatin, metalloproteinase, disintegrin triflavin, snake venom\nmetalloproteinase, corynebacterium ulcerans toxin. Through quantitative risk\nassessments and scenario analyses, we identify dual-use capabilities in current\nLLM-enabled biodesign pipelines and propose multi-layered mitigation\nstrategies. The findings from this toxicity assessment challenge claims that\nlarge language models (LLMs) are incapable of designing bioweapons. This\nreinforces concerns about the potential misuse of LLMs in biodesign, posing a\nsignificant threat to research and development (R&D). The accessibility of such\ntechnology to individuals with limited technical expertise raises serious\nbiosecurity risks. Our findings underscore the critical need for robust\ngovernance and technical safeguards to balance rapid biotechnological\ninnovation with biosecurity imperatives.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86LLMs\u5728\u751f\u7269\u8bbe\u8ba1\u4e2d\u7684\u53cc\u91cd\u7528\u9014\u98ce\u9669\uff0c\u751f\u6210\u4e86\u5927\u91cf\u65b0\u578b\u6709\u6bd2\u7269\u8d28\uff0c\u5f3a\u8c03\u4e86\u751f\u7269\u5b89\u5168\u6cbb\u7406\u548c\u6280\u672f\u4fdd\u969c\u7684\u7d27\u8feb\u6027\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6f5c\u5728\u6ee5\u7528\u98ce\u9669\uff0c\u7279\u522b\u662f\u8bbe\u8ba1\u6709\u6bd2\u5316\u5408\u7269\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528Moremi Bio Agent\uff08\u65e0\u5b89\u5168\u9632\u62a4\uff09\u751f\u6210\u65b0\u578b\u6709\u6bd2\u86cb\u767d\u8d28\u548c\u5c0f\u5206\u5b50\uff0c\u5e76\u8fdb\u884c\u8ba1\u7b97\u6bd2\u6027\u8bc4\u4f30\u548c\u5b9a\u91cf\u98ce\u9669\u5206\u6790\u3002", "result": "\u751f\u6210\u4e861020\u79cd\u6709\u6bd2\u86cb\u767d\u8d28\u548c5000\u79cd\u6709\u6bd2\u5c0f\u5206\u5b50\uff0c\u90e8\u5206\u4e0e\u5df2\u77e5\u6bd2\u7d20\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u63ed\u793a\u4e86LLMs\u8bbe\u8ba1\u751f\u7269\u6b66\u5668\u7684\u6f5c\u529b\u3002", "conclusion": "LLMs\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u751f\u7269\u6b66\u5668\u8bbe\u8ba1\uff0c\u9700\u52a0\u5f3a\u6cbb\u7406\u548c\u6280\u672f\u4fdd\u969c\u4ee5\u5e73\u8861\u521b\u65b0\u4e0e\u5b89\u5168\u3002", "relevance": 60.0}}
{"id": "2505.17763", "pdf": "https://arxiv.org/pdf/2505.17763", "abs": "https://arxiv.org/abs/2505.17763", "authors": ["Julian Oelhaf", "Georg Kordowich", "Andreas Maier", "Johann Jager", "Siming Bayer"], "title": "Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals", "categories": ["cs.LG", "eess.SP"], "comment": "12 pages", "summary": "The widespread use of sensors in modern power grids has led to the\naccumulation of large amounts of voltage and current waveform data, especially\nduring fault events. However, the lack of labeled datasets poses a significant\nchallenge for fault classification and analysis. This paper explores the\napplication of unsupervised clustering techniques for fault diagnosis in\nhigh-voltage power systems. A dataset provided by the Reseau de Transport\nd'Electricite (RTE) is analyzed, with frequency domain features extracted using\nthe Fast Fourier Transform (FFT). The K-Means algorithm is then applied to\nidentify underlying patterns in the data, enabling automated fault\ncategorization without the need for labeled training samples. The resulting\nclusters are evaluated in collaboration with power system experts to assess\ntheir alignment with real-world fault characteristics. The results demonstrate\nthe potential of unsupervised learning for scalable and data-driven fault\nanalysis, providing a robust approach to detecting and classifying power system\nfaults with minimal prior assumptions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u9ad8\u538b\u7535\u529b\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u6280\u672f\u8fdb\u884c\u6545\u969c\u8bca\u65ad\uff0c\u5229\u7528FFT\u63d0\u53d6\u9891\u57df\u7279\u5f81\u5e76\u901a\u8fc7K-Means\u7b97\u6cd5\u5b9e\u73b0\u81ea\u52a8\u5316\u6545\u969c\u5206\u7c7b\u3002", "motivation": "\u73b0\u4ee3\u7535\u7f51\u4e2d\u4f20\u611f\u5668\u6570\u636e\u91cf\u5927\u4f46\u7f3a\u4e4f\u6807\u6ce8\uff0c\u4f20\u7edf\u6545\u969c\u5206\u7c7b\u65b9\u6cd5\u53d7\u9650\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528FFT\u63d0\u53d6\u9891\u57df\u7279\u5f81\uff0c\u5e94\u7528K-Means\u7b97\u6cd5\u5bf9\u6570\u636e\u8fdb\u884c\u805a\u7c7b\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\u65e0\u76d1\u7763\u5b66\u4e60\u80fd\u6709\u6548\u5206\u7c7b\u7535\u529b\u7cfb\u7edf\u6545\u969c\uff0c\u4e14\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "\u65e0\u76d1\u7763\u5b66\u4e60\u4e3a\u7535\u529b\u7cfb\u7edf\u6545\u969c\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "relevance": 20.0}}
{"id": "2505.17446", "pdf": "https://arxiv.org/pdf/2505.17446", "abs": "https://arxiv.org/abs/2505.17446", "authors": ["Shunsuke Kando", "Yusuke Miyao", "Shinnosuke Takamichi"], "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "The purpose of speech tokenization is to transform a speech signal into a\nsequence of discrete representations, serving as the foundation for speech\nlanguage models (SLMs). While speech tokenization has many options, their\neffect on the performance of SLMs remains unclear. This paper investigates two\nkey aspects of speech tokenization: the segmentation width and the cluster size\nof discrete units. First, we segment speech signals into fixed/variable widths\nand pooled representations. We then train K-means models in multiple cluster\nsizes. Through the evaluation on zero-shot spoken language understanding\nbenchmarks, we find the positive effect of moderately coarse segmentation and\nbigger cluster size. Notably, among the best-performing models, the most\nefficient one achieves a 50% reduction in training data and a 70% decrease in\ntraining runtime. Our analysis highlights the importance of combining multiple\ntokens to enhance fine-grained spoken language understanding.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u97f3\u6807\u8bb0\u5316\u7684\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u5206\u6bb5\u5bbd\u5ea6\u548c\u79bb\u6563\u5355\u5143\u7684\u7c07\u5927\u5c0f\uff0c\u53d1\u73b0\u9002\u5ea6\u7684\u7c97\u5206\u6bb5\u548c\u8f83\u5927\u7684\u7c07\u5927\u5c0f\u5bf9\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u6709\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u8bed\u97f3\u6807\u8bb0\u5316\u5bf9\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u6027\u80fd\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u7814\u7a76\u5206\u6bb5\u5bbd\u5ea6\u548c\u7c07\u5927\u5c0f\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u8bed\u97f3\u4fe1\u53f7\u5206\u4e3a\u56fa\u5b9a/\u53ef\u53d8\u5bbd\u5ea6\u548c\u6c60\u5316\u8868\u793a\uff0c\u8bad\u7ec3\u4e0d\u540c\u7c07\u5927\u5c0f\u7684K-means\u6a21\u578b\uff0c\u5e76\u5728\u96f6\u6837\u672c\u53e3\u8bed\u7406\u89e3\u57fa\u51c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u9002\u5ea6\u7c97\u5206\u6bb5\u548c\u8f83\u5927\u7c07\u5927\u5c0f\u5bf9\u6027\u80fd\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u6700\u4f73\u6a21\u578b\u51cf\u5c11\u4e8650%\u7684\u8bad\u7ec3\u6570\u636e\u548c70%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u7ed3\u5408\u591a\u4e2a\u6807\u8bb0\u53ef\u589e\u5f3a\u7ec6\u7c92\u5ea6\u53e3\u8bed\u7406\u89e3\uff0c\u5206\u6bb5\u548c\u7c07\u5927\u5c0f\u662f\u8bed\u97f3\u6807\u8bb0\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002", "relevance": 40.0}}
{"id": "2505.17994", "pdf": "https://arxiv.org/pdf/2505.17994", "abs": "https://arxiv.org/abs/2505.17994", "authors": ["Zhihua Liu", "Amrutha Saseendran", "Lei Tong", "Xilin He", "Fariba Yousefi", "Nikolay Burlutskiy", "Dino Oglic", "Tom Diethe", "Philip Teare", "Huiyu Zhou", "Chen Jin"], "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Open-set image segmentation poses a significant challenge because existing\nmethods often demand extensive training or fine-tuning and generally struggle\nto segment unified objects consistently across diverse text reference\nexpressions. Motivated by this, we propose Segment Anyword, a novel\ntraining-free visual concept prompt learning approach for open-set language\ngrounded segmentation that relies on token-level cross-attention maps from a\nfrozen diffusion model to produce segmentation surrogates or mask prompts,\nwhich are then refined into targeted object masks. Initial prompts typically\nlack coherence and consistency as the complexity of the image-text increases,\nresulting in suboptimal mask fragments. To tackle this issue, we further\nintroduce a novel linguistic-guided visual prompt regularization that binds and\nclusters visual prompts based on sentence dependency and syntactic structural\ninformation, enabling the extraction of robust, noise-tolerant mask prompts,\nand significant improvements in segmentation accuracy. The proposed approach is\neffective, generalizes across different open-set segmentation tasks, and\nachieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal\nContext 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative\nto fine-tuned methods) mIoU on GranDf, which is the most complex open-set\ngrounded segmentation task in the field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSegment Anyword\u7684\u8bad\u7ec3\u514d\u8d39\u89c6\u89c9\u6982\u5ff5\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u96c6\u8bed\u8a00\u57fa\u7840\u5206\u5272\uff0c\u901a\u8fc7\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u7684\u8de8\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5206\u5272\u63a9\u7801\u63d0\u793a\uff0c\u5e76\u7ed3\u5408\u8bed\u8a00\u5f15\u5bfc\u7684\u89c6\u89c9\u63d0\u793a\u6b63\u5219\u5316\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u96c6\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u4e14\u96be\u4ee5\u5728\u4e0d\u540c\u6587\u672c\u53c2\u8003\u8868\u8fbe\u4e2d\u4e00\u81f4\u5206\u5272\u7edf\u4e00\u5bf9\u8c61\u3002", "method": "\u5229\u7528\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u7684\u8de8\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5206\u5272\u63a9\u7801\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u7684\u89c6\u89c9\u63d0\u793a\u6b63\u5219\u5316\u4f18\u5316\u63a9\u7801\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u653e\u96c6\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5982Pascal Context 59\uff0852.5 mIoU\uff09\u3001gRefCOCO\uff0867.73 cIoU\uff09\u548cGranDf\uff0867.4 mIoU\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u5206\u5272\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "relevance": 50.0}}
{"id": "2505.17765", "pdf": "https://arxiv.org/pdf/2505.17765", "abs": "https://arxiv.org/abs/2505.17765", "authors": ["Junhong Zhang", "Zhihui Lai"], "title": "Joker: Joint Optimization Framework for Lightweight Kernel Machines", "categories": ["cs.LG"], "comment": "24 pages, 5 figures, accepted by ICML 2025", "summary": "Kernel methods are powerful tools for nonlinear learning with\nwell-established theory. The scalability issue has been their long-standing\nchallenge. Despite the existing success, there are two limitations in\nlarge-scale kernel methods: (i) The memory overhead is too high for users to\nafford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),\nwhile other models lack study. In this paper, we propose Joker, a joint\noptimization framework for diverse kernel models, including KRR, logistic\nregression, and support vector machines. We design a dual block coordinate\ndescent method with trust region (DBCD-TR) and adopt kernel approximation with\nrandomized features, leading to low memory costs and high efficiency in\nlarge-scale learning. Experiments show that Joker saves up to 90\\% memory but\nachieves comparable training time and performance (or even better) than the\nstate-of-the-art methods.", "AI": {"tldr": "Joker\u662f\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u6838\u65b9\u6cd5\u4e2d\u7684\u5185\u5b58\u5f00\u9500\u548c\u6a21\u578b\u591a\u6837\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u5757\u5750\u6807\u4e0b\u964d\u548c\u6838\u8fd1\u4f3c\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u6838\u65b9\u6cd5\u5b58\u5728\u5185\u5b58\u5f00\u9500\u9ad8\u548c\u6a21\u578b\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faJoker\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5757\u5750\u6807\u4e0b\u964d\u4e0e\u4fe1\u4efb\u533a\u57df\u65b9\u6cd5\uff08DBCD-TR\uff09\u548c\u968f\u673a\u7279\u5f81\u7684\u6838\u8fd1\u4f3c\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJoker\u8282\u7701\u9ad8\u8fbe90%\u5185\u5b58\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u6027\u80fd\u3002", "conclusion": "Joker\u4e3a\u5927\u89c4\u6a21\u6838\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17447", "pdf": "https://arxiv.org/pdf/2505.17447", "abs": "https://arxiv.org/abs/2505.17447", "authors": ["Qi Zhang", "Shouqing Yang", "Lirong Gao", "Hao Chen", "Xiaomeng Hu", "Jinglei Chen", "Jiexiang Wang", "Sheng Guo", "Bo Zheng", "Haobo Wang", "Junbo Zhao"], "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization", "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nreasoning with the emergence of reasoning models like OpenAI-o1 and\nDeepSeek-R1. Recent research focuses on integrating reasoning capabilities into\nthe realm of retrieval-augmented generation (RAG) via outcome-supervised\nreinforcement learning (RL) approaches, while the correctness of intermediate\nthink-and-search steps is usually neglected. To address this issue, we design a\nprocess-level reward module to mitigate the unawareness of intermediate\nreasoning steps in outcome-level supervision without additional annotation.\nGrounded on this, we propose Learning to Think-and-Search (LeTS), a novel\nframework that hybridizes stepwise process reward and outcome-based reward to\ncurrent RL methods for RAG. Extensive experiments demonstrate the\ngeneralization and inference efficiency of LeTS across various RAG benchmarks.\nIn addition, these results reveal the potential of process- and outcome-level\nreward hybridization in boosting LLMs' reasoning ability via RL under other\nscenarios. The code will be released soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLeTS\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fc7\u7a0b\u7ea7\u548c\u7ed3\u679c\u7ea7\u5956\u52b1\uff0c\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5ffd\u89c6\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u4ec5\u5173\u6ce8\u7ed3\u679c\u7ea7\u76d1\u7763\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6a21\u5757\uff0c\u7ed3\u5408\u7ed3\u679c\u7ea7\u5956\u52b1\uff0c\u63d0\u51fa\u4e86LeTS\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3aRL\u65b9\u6cd5\u5728RAG\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLeTS\u5728\u591a\u4e2aRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u8fc7\u7a0b\u7ea7\u548c\u7ed3\u679c\u7ea7\u5956\u52b1\u7684\u7ed3\u5408\u6709\u671b\u5728\u5176\u4ed6\u573a\u666f\u4e2d\u901a\u8fc7RL\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.18010", "pdf": "https://arxiv.org/pdf/2505.18010", "abs": "https://arxiv.org/abs/2505.18010", "authors": ["Jens De Winne", "Siri Willems", "Siri Luthman", "Danilo Babin", "Hiep Luong", "Wim Ceelen"], "title": "Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging", "categories": ["cs.CV"], "comment": "Provisionally accepted to the MICCAI 2025 conference", "summary": "Accurate, real-time monitoring of tissue ischemia is crucial to understand\ntissue health and guide surgery. Spectral imaging shows great potential for\ncontactless and intraoperative monitoring of tissue oxygenation. Due to the\ndifficulty of obtaining direct reference oxygenation values, conventional\nmethods are based on linear unmixing techniques. These are prone to assumptions\nand these linear relations may not always hold in practice. In this work, we\npresent deep learning approaches for real-time tissue oxygenation estimation\nusing Monte-Carlo simulated spectra. We train a fully connected neural network\n(FCN) and a convolutional neural network (CNN) for this task and propose a\ndomain-adversarial training approach to bridge the gap between simulated and\nreal clinical spectral data. Results demonstrate that these deep learning\nmodels achieve a higher correlation with capillary lactate measurements, a\nwell-known marker of hypoxia, obtained during spectral imaging in surgery,\ncompared to traditional linear unmixing. Notably, domain-adversarial training\neffectively reduces the domain gap, optimizing performance in real clinical\nsettings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u65f6\u7ec4\u7ec7\u6c27\u5408\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5149\u8c31\u8bad\u7ec3\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u91c7\u7528\u57df\u5bf9\u6297\u8bad\u7ec3\u7f29\u5c0f\u6a21\u62df\u4e0e\u4e34\u5e8a\u6570\u636e\u7684\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u89e3\u6df7\u65b9\u6cd5\u4f9d\u8d56\u5047\u8bbe\u4e14\u7ebf\u6027\u5173\u7cfb\u5728\u5b9e\u9645\u4e2d\u672a\u5fc5\u6210\u7acb\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u5b9e\u65f6\u76d1\u6d4b\u7ec4\u7ec7\u7f3a\u8840\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5149\u8c31\u8bad\u7ec3FCN\u548cCNN\uff0c\u5e76\u63d0\u51fa\u57df\u5bf9\u6297\u8bad\u7ec3\u4ee5\u7f29\u5c0f\u6a21\u62df\u4e0e\u4e34\u5e8a\u6570\u636e\u7684\u5dee\u8ddd\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4e0e\u624b\u672f\u4e2d\u6bdb\u7ec6\u8840\u7ba1\u4e73\u9178\u6d4b\u91cf\u7684\u76f8\u5173\u6027\u66f4\u9ad8\uff0c\u57df\u5bf9\u6297\u8bad\u7ec3\u6709\u6548\u51cf\u5c11\u4e86\u9886\u57df\u5dee\u8ddd\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u57df\u5bf9\u6297\u8bad\u7ec3\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u7ec4\u7ec7\u6c27\u5408\u76d1\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17769", "pdf": "https://arxiv.org/pdf/2505.17769", "abs": "https://arxiv.org/abs/2505.17769", "authors": ["Patrick Leask", "Neel Nanda", "Noura Al Moubayed"], "title": "Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage\nModels (LLM) activations into interpretable latents. However, due to their\nsubstantial training cost, most academic research uses open-source SAEs which\nare only available for a restricted set of models of up to 27B parameters. SAE\nlatents are also learned from a dataset of activations, which means they do not\ntransfer between models. Motivated by relative representation similarity\nmeasures, we introduce Inference-Time Decomposition of Activations (ITDA)\nmodels, an alternative method for decomposing language model activations. To\ntrain an ITDA, we greedily construct a dictionary of language model activations\non a dataset of prompts, selecting those activations which were worst\napproximated by matching pursuit on the existing dictionary. ITDAs can be\ntrained in just 1\\% of the time required for SAEs, using 1\\% of the data. This\nallowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.\nITDAs can achieve similar reconstruction performance to SAEs on some target\nLLMs, but generally incur a performance penalty. However, ITDA dictionaries\nenable cross-model comparisons, and a simple Jaccard similarity index on ITDA\ndictionaries outperforms existing methods like CKA, SVCCA, and relative\nrepresentation similarity metrics. ITDAs provide a cheap alternative to SAEs\nwhere computational resources are limited, or when cross model comparisons are\nnecessary. Code available at https://github.com/pleask/itda.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aITDA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6fc0\u6d3b\uff0c\u76f8\u6bd4\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\uff0cITDA\u8bad\u7ec3\u6210\u672c\u4f4e\u4e14\u652f\u6301\u8de8\u6a21\u578b\u6bd4\u8f83\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u8de8\u6a21\u578b\u4f7f\u7528\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u8d2a\u5a6a\u6784\u5efa\u5b57\u5178\uff0c\u9009\u62e9\u6700\u5dee\u5339\u914d\u7684\u6fc0\u6d3b\u8fdb\u884c\u5206\u89e3\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec5\u4e3aSAE\u76841%\u3002", "result": "ITDA\u5728\u90e8\u5206\u76ee\u6807\u6a21\u578b\u4e0a\u8868\u73b0\u63a5\u8fd1SAE\uff0c\u4e14\u8de8\u6a21\u578b\u6bd4\u8f83\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ITDA\u4e3a\u8d44\u6e90\u6709\u9650\u6216\u9700\u8de8\u6a21\u578b\u6bd4\u8f83\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17455", "pdf": "https://arxiv.org/pdf/2505.17455", "abs": "https://arxiv.org/abs/2505.17455", "authors": ["Youliang Yuan", "Wenxiang Jiao", "Yuejin Xie", "Chihao Shen", "Menghan Tian", "Wenxuan Wang", "Jen-tse Huang", "Pinjia He"], "title": "Towards Evaluating Proactive Risk Awareness of Multimodal Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Human safety awareness gaps often prevent the timely recognition of everyday\nrisks. In solving this problem, a proactive safety artificial intelligence (AI)\nsystem would work better than a reactive one. Instead of just reacting to\nusers' questions, it would actively watch people's behavior and their\nenvironment to detect potential dangers in advance. Our Proactive Safety Bench\n(PaSBench) evaluates this capability through 416 multimodal scenarios (128\nimage sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation\nof 36 advanced models reveals fundamental limitations: Top performers like\nGemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks\nin repeated trials. Through failure analysis, we identify unstable proactive\nreasoning rather than knowledge deficits as the primary limitation. This work\nestablishes (1) a proactive safety benchmark, (2) systematic evidence of model\nlimitations, and (3) critical directions for developing reliable protective AI.\nWe believe our dataset and findings can promote the development of safer AI\nassistants that actively prevent harm rather than merely respond to requests.\nOur dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPaSBench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u4e3b\u52a8\u5b89\u5168\u76d1\u63a7\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u5b89\u5168\u610f\u8bc6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b89\u5168AI\u7cfb\u7edf\u63d0\u524d\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528416\u4e2a\u591a\u6a21\u6001\u573a\u666f\uff08128\u4e2a\u56fe\u50cf\u5e8f\u5217\u548c288\u4e2a\u6587\u672c\u65e5\u5fd7\uff09\u8bc4\u4f3036\u4e2a\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u9876\u7ea7\u6a21\u578b\uff08\u5982Gemini-2.5-pro\uff09\u5728\u56fe\u50cf\u548c\u6587\u672c\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a71%\u548c64%\uff0c\u4f46\u5728\u91cd\u590d\u8bd5\u9a8c\u4e2d\u9057\u6f0f\u4e8645-55%\u7684\u98ce\u9669\u3002\u4e3b\u8981\u95ee\u9898\u662f\u4e3b\u52a8\u63a8\u7406\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e3b\u52a8\u5b89\u5168\u57fa\u51c6\u3001\u6a21\u578b\u5c40\u9650\u6027\u7684\u7cfb\u7edf\u6027\u8bc1\u636e\uff0c\u5e76\u4e3a\u5f00\u53d1\u53ef\u9760\u4fdd\u62a4\u6027AI\u6307\u660e\u4e86\u65b9\u5411\u3002", "relevance": 70.0}}
{"id": "2505.18015", "pdf": "https://arxiv.org/pdf/2505.18015", "abs": "https://arxiv.org/abs/2505.18015", "authors": ["Shashank Agnihotri", "David Schader", "Jonas Jakubassa", "Nico Sharei", "Simon Kral", "Mehmet Ege Ka\u00e7ar", "Ruben Weber", "Margret Keuper"], "title": "SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification", "categories": ["cs.CV", "cs.LG"], "comment": "First seven listed authors have equal contribution. GitHub:\n  https://github.com/shashankskagnihotri/benchmarking_reliability_generalization.\n  arXiv admin note: text overlap with arXiv:2505.05091", "summary": "Reliability and generalization in deep learning are predominantly studied in\nthe context of image classification. Yet, real-world applications in\nsafety-critical domains involve a broader set of semantic tasks, such as\nsemantic segmentation and object detection, which come with a diverse set of\ndedicated model architectures. To facilitate research towards robust model\ndesign in segmentation and detection, our primary objective is to provide\nbenchmarking tools regarding robustness to distribution shifts and adversarial\nmanipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,\nalong with the most extensive evaluation to date on the reliability and\ngeneralization of semantic segmentation and object detection models. In\nparticular, we benchmark 76 segmentation models across four datasets and 61\nobject detectors across two datasets, evaluating their performance under\ndiverse adversarial attacks and common corruptions. Our findings reveal\nsystematic weaknesses in state-of-the-art models and uncover key trends based\non architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are\nopen-sourced in our GitHub repository\n(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)\nalong with our complete set of total 6139 evaluations. We anticipate the\ncollected data to foster and encourage future research towards improved model\nreliability beyond classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SEMSEGBENCH\u548cDETECBENCH\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u4e49\u5206\u5272\u548c\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u6d4b\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u8bed\u4e49\u5206\u5272\u548c\u7269\u4f53\u68c0\u6d4b\uff09\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u6027\uff0c\u586b\u8865\u4e86\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u56fe\u50cf\u5206\u7c7b\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86SEMSEGBENCH\u548cDETECBENCH\u5de5\u5177\uff0c\u5bf976\u4e2a\u5206\u5272\u6a21\u578b\u548c61\u4e2a\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6297\u653b\u51fb\u548c\u5e38\u89c1\u5e72\u6270\u4e0b\u7684\u8bc4\u6d4b\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\uff0c\u5e76\u53d1\u73b0\u4e86\u57fa\u4e8e\u67b6\u6784\u3001\u4e3b\u5e72\u7f51\u7edc\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u5173\u952e\u8d8b\u52bf\u3002", "conclusion": "\u5f00\u6e90\u5de5\u5177\u548c\u8bc4\u6d4b\u6570\u636e\u5c06\u4fc3\u8fdb\u672a\u6765\u5728\u6a21\u578b\u53ef\u9760\u6027\u65b9\u9762\u7684\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.17162", "pdf": "https://arxiv.org/pdf/2505.17162", "abs": "https://arxiv.org/abs/2505.17162", "authors": ["Jiehan Cheng", "Zhicheng Dou"], "title": "DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing Real-World Changes", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "We propose DailyQA, an automatically updated dynamic dataset that updates\nquestions weekly and contains answers to questions on any given date. DailyQA\nutilizes daily updates from Wikipedia revision logs to implement a fully\nautomated pipeline of data filtering, query generation synthesis, quality\nchecking, answer extraction, and query classification. The benchmark requires\nlarge language models (LLMs) to process and answer questions involving\nfast-changing factual data and covering multiple domains. We evaluate several\nopen-source and closed-source LLMs using different RAG pipelines with web\nsearch augmentation. We compare the ability of different models to process\ntime-sensitive web information and find that rerank of web retrieval results is\ncritical. Our results indicate that LLMs still face significant challenges in\nhandling frequently updated information, suggesting that DailyQA benchmarking\nprovides valuable insights into the direction of progress for LLMs and RAG\nsystems.", "AI": {"tldr": "DailyQA\u662f\u4e00\u4e2a\u52a8\u6001\u6570\u636e\u96c6\uff0c\u6bcf\u5468\u81ea\u52a8\u66f4\u65b0\u95ee\u9898\uff0c\u6db5\u76d6\u591a\u9886\u57df\u5feb\u901f\u53d8\u5316\u7684\u4e8b\u5b9e\u6570\u636e\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5904\u7406\u65f6\u6548\u6027\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u5904\u7406\u5feb\u901f\u53d8\u5316\u7684\u4e8b\u5b9e\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u52a8\u6001\u6570\u636e\u96c6\u8bc4\u4f30\u5176\u80fd\u529b\u3002", "method": "\u5229\u7528Wikipedia\u4fee\u8ba2\u65e5\u5fd7\u6784\u5efa\u81ea\u52a8\u5316\u6570\u636e\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u95ee\u9898\u5e76\u63d0\u53d6\u7b54\u6848\uff0c\u7ed3\u5408RAG\u548c\u7f51\u7edc\u641c\u7d22\u589e\u5f3a\u8bc4\u4f30LLMs\u3002", "result": "LLMs\u5728\u5904\u7406\u65f6\u6548\u6027\u4fe1\u606f\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7f51\u7edc\u68c0\u7d22\u7ed3\u679c\u7684\u91cd\u65b0\u6392\u5e8f\u662f\u5173\u952e\u3002", "conclusion": "DailyQA\u4e3aLLMs\u548cRAG\u7cfb\u7edf\u7684\u8fdb\u6b65\u65b9\u5411\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.17773", "pdf": "https://arxiv.org/pdf/2505.17773", "abs": "https://arxiv.org/abs/2505.17773", "authors": ["Amir Hossein Rahmati", "Sanket Jantre", "Weifeng Zhang", "Yucheng Wang", "Byung-Jun Yoon", "Nathan M. Urban", "Xiaoning Qian"], "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes.", "AI": {"tldr": "C-LoRA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3LoRA\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u9884\u6d4b\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "motivation": "LoRA\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u8f93\u5165\u7279\u6027\u5bf9\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u52a8\u6001\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faC-LoRA\uff0c\u901a\u8fc7\u5f00\u53d1\u8f7b\u91cf\u7ea7\u7684LoRA\u6a21\u5757\uff0c\u6839\u636e\u8f93\u5165\u6570\u636e\u6837\u672c\u52a8\u6001\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5c06\u6570\u636e\u9a71\u52a8\u7684\u4e0a\u4e0b\u6587\u7eb3\u5165\u53c2\u6570\u540e\u9a8c\u3002", "result": "C-LoRA\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6a21\u578b\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u6a21\u5757\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "C-LoRA\u4e3a\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u9c81\u68d2\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684LLM\u5fae\u8c03\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "relevance": 90.0}}
{"id": "2505.17464", "pdf": "https://arxiv.org/pdf/2505.17464", "abs": "https://arxiv.org/abs/2505.17464", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo.", "AI": {"tldr": "Hydra\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u672c\u68c0\u7d22\uff0c\u63d0\u5347LLM\u5728\u591a\u8df3\u63a8\u7406\u3001\u591a\u5b9e\u4f53\u95ee\u9898\u53ca\u591a\u6e90\u9a8c\u8bc1\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6df7\u5408RAG\u7cfb\u7edf\u5728\u591a\u8df3\u63a8\u7406\u3001\u591a\u5b9e\u4f53\u95ee\u9898\u3001\u591a\u6e90\u9a8c\u8bc1\u548c\u6709\u6548\u5229\u7528\u56fe\u8c31\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0cHydra\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Hydra\u901a\u8fc7\u4ee3\u7406\u9a71\u52a8\u7684\u63a2\u7d22\u7ed3\u5408\u7ed3\u6784\u548c\u975e\u7ed3\u6784\u5316\u68c0\u7d22\uff0c\u5229\u7528\u4e09\u56e0\u7d20\u8de8\u6e90\u9a8c\u8bc1\uff08\u6e90\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u3001\u8de8\u6e90\u4f50\u8bc1\u548c\u5b9e\u4f53\u8def\u5f84\u5bf9\u9f50\uff09\u63d0\u5347\u8bc1\u636e\u591a\u6837\u6027\u548c\u7cbe\u786e\u5ea6\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHydra\u5e73\u5747\u4f18\u4e8eToG-2\u57fa\u7ebf20.3%\uff0c\u6700\u9ad8\u8fbe30.1%\uff0c\u5e76\u4f7f\u5c0f\u6a21\u578b\uff08\u5982Llama-3.1-8B\uff09\u8fbe\u5230\u4e0eGPT-4-Turbo\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "Hydra\u901a\u8fc7\u7edf\u4e00\u56fe\u8c31\u62d3\u6251\u3001\u6587\u6863\u8bed\u4e49\u548c\u6e90\u53ef\u9760\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6df1\u5ea6\u548c\u5fe0\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.18021", "pdf": "https://arxiv.org/pdf/2505.18021", "abs": "https://arxiv.org/abs/2505.18021", "authors": ["Yao Sun", "Sining Chen", "Yifan Tian", "Xiao Xiang Zhu"], "title": "Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method", "categories": ["cs.CV"], "comment": "Code and data: https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark", "summary": "Accurate information on the number of building floors, or above-ground\nstoreys, is essential for household estimation, utility provision, risk\nassessment, evacuation planning, and energy modeling. Yet large-scale\nfloor-count data are rarely available in cadastral and 3D city databases. This\nstudy proposes an end-to-end deep learning framework that infers floor numbers\ndirectly from unrestricted, crowdsourced street-level imagery, avoiding\nhand-crafted features and generalizing across diverse facade styles. To enable\nbenchmarking, we release the Munich Building Floor Dataset, a public set of\nover 6800 geo-tagged images collected from Mapillary and targeted field\nphotography, each paired with a verified storey label. On this dataset, the\nproposed classification-regression network attains 81.2% exact accuracy and\npredicts 97.9% of buildings within +/-1 floor. The method and dataset together\noffer a scalable route to enrich 3D city models with vertical information and\nlay a foundation for future work in urban informatics, remote sensing, and\ngeographic information science. Source code and data will be released under an\nopen license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u8857\u666f\u56fe\u50cf\u63a8\u65ad\u5efa\u7b51\u7269\u697c\u5c42\u6570\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b6800\u591a\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5927\u89c4\u6a21\u697c\u5c42\u6570\u636e\u57283D\u57ce\u5e02\u6570\u636e\u5e93\u4e2d\u7a00\u7f3a\uff0c\u4f46\u6b64\u7c7b\u4fe1\u606f\u5bf9\u57ce\u5e02\u89c4\u5212\u3001\u98ce\u9669\u8bc4\u4f30\u7b49\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f4\u63a5\u4ece\u8857\u666f\u56fe\u50cf\u63a8\u65ad\u697c\u5c42\u6570\uff0c\u907f\u514d\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u8fbe\u523081.2%\u7684\u51c6\u786e\u7387\uff0c97.9%\u7684\u9884\u6d4b\u8bef\u5dee\u5728\u00b11\u5c42\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a3D\u57ce\u5e02\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5782\u76f4\u4fe1\u606f\u8865\u5145\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17777", "pdf": "https://arxiv.org/pdf/2505.17777", "abs": "https://arxiv.org/abs/2505.17777", "authors": ["Harish G. Ramaswamy", "L. A. Prashanth"], "title": "Optimizing Shortfall Risk Metric for Learning Regression Models", "categories": ["cs.LG"], "comment": null, "summary": "We consider the problem of estimating and optimizing utility-based shortfall\nrisk (UBSR) of a loss, say $(Y - \\hat Y)^2$, in the context of a regression\nproblem. Empirical risk minimization with a UBSR objective is challenging since\nUBSR is a non-linear function of the underlying distribution. We first derive a\nconcentration bound for UBSR estimation using independent and identically\ndistributed (i.i.d.) samples. We then frame the UBSR optimization problem as\nminimization of a pseudo-linear function in the space of achievable\ndistributions $\\mathcal D$ of the loss $(Y- \\hat Y)^2$. We construct a gradient\noracle for the UBSR objective and a linear minimization oracle (LMO) for the\nset $\\mathcal D$. Using these oracles, we devise a bisection-type algorithm,\nand establish convergence to the UBSR-optimal solution.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u56de\u5f52\u95ee\u9898\u4e2d\u4f30\u8ba1\u548c\u4f18\u5316\u57fa\u4e8e\u6548\u7528\u7684\u77ed\u7f3a\u98ce\u9669\uff08UBSR\uff09\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5206\u7b97\u6cd5\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002", "motivation": "\u5728\u56de\u5f52\u95ee\u9898\u4e2d\uff0cUBSR\u4f5c\u4e3a\u4e00\u79cd\u975e\u7ebf\u6027\u98ce\u9669\u5ea6\u91cf\uff0c\u5176\u4f30\u8ba1\u548c\u4f18\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u72ec\u7acb\u540c\u5206\u5e03\u6837\u672c\u63a8\u5bfcUBSR\u7684\u6d53\u5ea6\u8fb9\u754c\uff0c\u5e76\u5c06\u5176\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u5728\u635f\u5931\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f2a\u7ebf\u6027\u51fd\u6570\u6700\u5c0f\u5316\uff0c\u8bbe\u8ba1\u4e86\u68af\u5ea6\u9884\u8a00\u673a\u548c\u7ebf\u6027\u6700\u5c0f\u5316\u9884\u8a00\u673a\uff0c\u5e76\u5f00\u53d1\u4e86\u53cc\u5206\u7b97\u6cd5\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u80fd\u591f\u627e\u5230UBSR\u6700\u4f18\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86UBSR\u7684\u4f30\u8ba1\u548c\u4f18\u5316\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u56de\u5f52\u4efb\u52a1\u3002", "relevance": 20.0}}
{"id": "2505.17465", "pdf": "https://arxiv.org/pdf/2505.17465", "abs": "https://arxiv.org/abs/2505.17465", "authors": ["Roelien C Timmer", "Yufang Hou", "Stephen Wan"], "title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards", "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "An important task in machine learning (ML) research is comparing prior work,\nwhich is often performed via ML leaderboards: a tabular overview of experiments\nwith comparable conditions (e.g., same task, dataset, and metric). However, the\ngrowing volume of literature creates challenges in creating and maintaining\nthese leaderboards. To ease this burden, researchers have developed methods to\nextract leaderboard entries from research papers for automated leaderboard\ncuration. Yet, prior work varies in problem framing, complicating comparisons\nand limiting real-world applicability. In this position paper, we present the\nfirst overview of Automatic Leaderboard Generation (ALG) research, identifying\nfundamental differences in assumptions, scope, and output formats. We propose\nan ALG unified conceptual framework to standardise how the ALG task is defined.\nWe offer ALG benchmarking guidelines, including recommendations for datasets\nand metrics that promote fair, reproducible evaluation. Lastly, we outline\nchallenges and new directions for ALG, such as, advocating for broader coverage\nby including all reported results and richer metadata.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u52a8\u6392\u884c\u699c\u751f\u6210\uff08ALG\uff09\u7684\u7edf\u4e00\u6982\u5ff5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e2d\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u7684\u6807\u51c6\u5316\u6bd4\u8f83\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u6307\u5357\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\uff0c\u624b\u52a8\u7ef4\u62a4\u5b9e\u9a8c\u6392\u884c\u699c\u53d8\u5f97\u56f0\u96be\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u63d0\u53d6\u548c\u6807\u51c6\u5316\u5b9e\u9a8c\u7ed3\u679c\u3002", "method": "\u8bba\u6587\u7efc\u8ff0\u4e86ALG\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u5236\u5b9a\u4e86\u57fa\u51c6\u6d4b\u8bd5\u6307\u5357\uff0c\u5305\u62ec\u6570\u636e\u96c6\u548c\u6307\u6807\u7684\u63a8\u8350\u3002", "result": "\u63d0\u51fa\u4e86ALG\u7684\u7edf\u4e00\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u6307\u5357\uff0c\u4e3a\u6807\u51c6\u5316\u5b9e\u9a8c\u7ed3\u679c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "ALG\u7814\u7a76\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u8986\u76d6\u548c\u66f4\u4e30\u5bcc\u7684\u5143\u6570\u636e\uff0c\u4ee5\u4fc3\u8fdb\u516c\u5e73\u548c\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u3002", "relevance": 40.0}}
{"id": "2505.18022", "pdf": "https://arxiv.org/pdf/2505.18022", "abs": "https://arxiv.org/abs/2505.18022", "authors": ["Liang Yao", "Fan Liu", "Delong Chen", "Chuanyi Zhang", "Yijun Wang", "Ziyun Chen", "Wei Xu", "Shimin Di", "Yuhui Zheng"], "title": "RemoteSAM: Towards Segment Anything for Earth Observation", "categories": ["cs.CV"], "comment": null, "summary": "We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRemoteSAM\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u5f15\u64ce\u548c\u4efb\u52a1\u7edf\u4e00\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u5904\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SoTA\u3002", "motivation": "\u5f53\u524d\u7cfb\u7edf\u5728\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u548c\u72ed\u7a84\u6570\u636e\u57df\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u6837\u5316\u89c6\u89c9\u76ee\u6807\u7684\u8bc6\u522b\u548c\u5b9a\u4f4d\u9700\u6c42\u3002", "method": "1. \u5f15\u5165\u81ea\u52a8\u6570\u636e\u5f15\u64ce\uff0c\u521b\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08270K\u56fe\u50cf-\u6587\u672c-\u63a9\u7801\u4e09\u5143\u7ec4\uff09\uff1b2. \u63d0\u51fa\u4ee5\u53c2\u8003\u8868\u8fbe\u5206\u5272\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u7edf\u4e00\u8303\u5f0f\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\u3002", "result": "RemoteSAM\u5728\u591a\u4e2a\u5730\u7403\u89c2\u6d4b\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7840\u6a21\u578b\uff08\u5982Falcon\u3001GeoChat\u7b49\uff09\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u548c\u5efa\u6a21\u7684\u521b\u65b0\uff0cRemoteSAM\u4e3a\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17165", "pdf": "https://arxiv.org/pdf/2505.17165", "abs": "https://arxiv.org/abs/2505.17165", "authors": ["Tomasz Hollanek", "Yulu Pi", "Cosimo Fiorini", "Virginia Vignali", "Dorian Peters", "Eleanor Drage"], "title": "A Toolkit for Compliance, a Toolkit for Justice: Drawing on Cross-sectoral Expertise to Develop a Pro-justice EU AI Act Toolkit", "categories": ["cs.CY", "cs.AI"], "comment": "In proceedings of ACM FAccT 2025", "summary": "The introduction of the AI Act in the European Union presents the AI research\nand practice community with a set of new challenges related to compliance.\nWhile it is certain that AI practitioners will require additional guidance and\ntools to meet these requirements, previous research on toolkits that aim to\ntranslate the theory of AI ethics into development and deployment practice\nsuggests that such resources suffer from multiple limitations. These\nlimitations stem, in part, from the fact that the toolkits are either produced\nby industry-based teams or by academics whose work tends to be abstract and\ndivorced from the realities of industry. In this paper, we discuss the\nchallenge of developing an AI ethics toolkit for practitioners that helps them\ncomply with new AI-focused regulation, but that also moves beyond mere\ncompliance to consider broader socio-ethical questions throughout development\nand deployment. The toolkit was created through a cross-sectoral collaboration\nbetween an academic team based in the UK and an industry team in Italy. We\noutline the background and rationale for creating a pro-justice AI Act\ncompliance toolkit, detail the process undertaken to develop it, and describe\nthe collaboration and negotiation efforts that shaped its creation. We aim for\nthe described process to serve as a blueprint for other teams navigating the\nchallenges of academia-industry partnerships and aspiring to produce usable and\nmeaningful AI ethics resources.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f00\u53d1\u4e00\u4e2aAI\u4f26\u7406\u5de5\u5177\u5305\u7684\u6311\u6218\uff0c\u65e8\u5728\u5e2e\u52a9\u4ece\u4e1a\u8005\u9075\u5b88\u6b27\u76dfAI\u6cd5\u6848\uff0c\u5e76\u8003\u8651\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u6b27\u76dfAI\u6cd5\u6848\u7684\u5b9e\u65bd\u7ed9AI\u7814\u7a76\u548c\u5b9e\u8df5\u793e\u533a\u5e26\u6765\u4e86\u65b0\u7684\u5408\u89c4\u6311\u6218\uff0c\u73b0\u6709\u5de5\u5177\u5305\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5b66\u672f\u754c\uff08\u82f1\u56fd\uff09\u4e0e\u5de5\u4e1a\u754c\uff08\u610f\u5927\u5229\uff09\u7684\u8de8\u90e8\u95e8\u5408\u4f5c\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301\u6b63\u4e49\u7684AI\u6cd5\u6848\u5408\u89c4\u5de5\u5177\u5305\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de5\u5177\u5305\u5f00\u53d1\u6d41\u7a0b\uff0c\u53ef\u4f5c\u4e3a\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u5408\u4f5c\u7684\u84dd\u56fe\u3002", "conclusion": "\u8de8\u90e8\u95e8\u5408\u4f5c\u662f\u5f00\u53d1\u5b9e\u7528\u4e14\u6709\u610f\u4e49\u7684AI\u4f26\u7406\u8d44\u6e90\u7684\u5173\u952e\u3002", "relevance": 40.0}}
{"id": "2505.17786", "pdf": "https://arxiv.org/pdf/2505.17786", "abs": "https://arxiv.org/abs/2505.17786", "authors": ["Sho Oshima", "Yuji Okamoto", "Taisei Tosaki", "Ryosuke Kojima", "Yasushi Okuno"], "title": "Supervised Graph Contrastive Learning for Gene Regulatory Network", "categories": ["cs.LG"], "comment": "under review", "summary": "Graph representation learning is effective for obtaining a meaningful latent\nspace utilizing the structure of graph data and is widely applied, including\nbiological networks. In particular, Graph Contrastive Learning (GCL) has\nemerged as a powerful self-supervised method that relies on applying\nperturbations to graphs for data augmentation. However, when applying existing\nGCL methods to biological networks such as Gene Regulatory Networks (GRNs),\nthey overlooked meaningful biologically relevant perturbations, e.g., gene\nknockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive\nLearning), a novel GCL method for GRNs that directly incorporates biological\nperturbations derived from gene knockdown experiments as the supervision.\nSupGCL mathematically extends existing GCL methods that utilize non-biological\nperturbations to probabilistic models that introduce actual biological gene\nperturbation utilizing gene knockdown data. Using the GRN representation\nobtained by our proposed method, our aim is to improve the performance of\nbiological downstream tasks such as patient hazard prediction and disease\nsubtype classification (graph-level task), and gene function classification\n(node-level task). We applied SupGCL on real GRN datasets derived from patients\nwith multiple types of cancer, and in all experiments SupGCL achieves better\nperformance than state-of-the-art baselines.", "AI": {"tldr": "SupGCL\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u7269\u6270\u52a8\uff08\u5982\u57fa\u56e0\u6572\u9664\uff09\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u751f\u7269\u7f51\u7edc\u4e2d\u5ffd\u7565\u4e86\u751f\u7269\u76f8\u5173\u7684\u6270\u52a8\uff08\u5982\u57fa\u56e0\u6572\u9664\uff09\uff0cSupGCL\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SupGCL\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5c06\u751f\u7269\u6270\u52a8\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u6784\u5efa\u6982\u7387\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u764c\u75c7\u60a3\u8005\u6570\u636e\u4e0a\uff0cSupGCL\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SupGCL\u901a\u8fc7\u7ed3\u5408\u751f\u7269\u6270\u52a8\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u7f51\u7edc\u7684\u8868\u793a\u5b66\u4e60\u6548\u679c\u3002", "relevance": 30.0}}
{"id": "2505.17470", "pdf": "https://arxiv.org/pdf/2505.17470", "abs": "https://arxiv.org/abs/2505.17470", "authors": ["Xiang Liu", "Zhaoxiang Liu", "Peng Wang", "Kohou Wang", "Huan Hu", "Kai Wang", "Shiguo Lian"], "title": "SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "When using supervised fine-tuning (SFT) to adapt large language models (LLMs)\nto specific domains, a significant challenge arises: should we use the entire\nSFT dataset for fine-tuning? Common practice often involves fine-tuning\ndirectly on the entire dataset due to limited information on the LLM's past\ntraining data. However, if the SFT dataset largely overlaps with the model's\nexisting knowledge, the performance gains are minimal, leading to wasted\ncomputational resources. Identifying the unknown knowledge within the SFT\ndataset and using it to fine-tune the model could substantially improve the\ntraining efficiency. To address this challenge, we propose a self-learning\nframework for LLMs inspired by human learning pattern. This framework takes a\nfine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer\nthe questions in the SFT dataset. The LLMs then objectively grade the responses\nand filter out the incorrectly answered QA pairs. Finally, we fine-tune the\nLLMs based on this filtered QA set. Experimental results in the fields of\nagriculture and medicine demonstrate that our method substantially reduces\ntraining time while achieving comparable improvements to those attained with\nfull dataset fine-tuning. By concentrating on the unknown knowledge within the\nSFT dataset, our approach enhances the efficiency of fine-tuning LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u5b66\u4e60\u6846\u67b6\u7684LLM\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b5b\u9009SFT\u6570\u636e\u96c6\u4e2d\u672a\u77e5\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3SFT\u6570\u636e\u96c6\u4e2d\u4e0eLLM\u5df2\u6709\u77e5\u8bc6\u91cd\u53e0\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\u3002", "method": "\u81ea\u5b66\u4e60\u6846\u67b6\uff1aLLM\u56de\u7b54SFT\u6570\u636e\u96c6\u95ee\u9898\uff0c\u7b5b\u9009\u9519\u8bef\u7b54\u6848\uff0c\u4ec5\u7528\u672a\u77e5\u77e5\u8bc6\u5fae\u8c03\u3002", "result": "\u5728\u519c\u4e1a\u548c\u533b\u5b66\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u6027\u80fd\u4e0e\u5168\u6570\u636e\u96c6\u5fae\u8c03\u76f8\u5f53\u3002", "conclusion": "\u805a\u7126\u672a\u77e5\u77e5\u8bc6\u53ef\u9ad8\u6548\u5fae\u8c03LLM\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "relevance": 85.0}}
{"id": "2505.18024", "pdf": "https://arxiv.org/pdf/2505.18024", "abs": "https://arxiv.org/abs/2505.18024", "authors": ["Xiaobao Wei", "Jiawei Liu", "Dongbo Yang", "Junda Cheng", "Changyong Shu", "Wei Wang"], "title": "A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency", "categories": ["cs.CV"], "comment": null, "summary": "We find that the EPE evaluation metrics of RAFT-stereo converge\ninconsistently in the low and high frequency regions, resulting high frequency\ndegradation (e.g., edges and thin objects) during the iterative process. The\nunderlying reason for the limited performance of current iterative methods is\nthat it optimizes all frequency components together without distinguishing\nbetween high and low frequencies. We propose a wavelet-based stereo matching\nframework (Wavelet-Stereo) for solving frequency convergence inconsistency.\nSpecifically, we first explicitly decompose an image into high and low\nfrequency components using discrete wavelet transform. Then, the high-frequency\nand low-frequency components are fed into two different multi-scale frequency\nfeature extractors. Finally, we propose a novel LSTM-based high-frequency\npreservation update operator containing an iterative frequency adapter to\nprovide adaptive refined high-frequency features at different iteration steps\nby fine-tuning the initial high-frequency features. By processing high and low\nfrequency components separately, our framework can simultaneously refine\nhigh-frequency information in edges and low-frequency information in smooth\nregions, which is especially suitable for challenging scenes with fine details\nand textures in the distance. Extensive experiments demonstrate that our\nWavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both\nthe KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/SIA-IDE/Wavelet-Stereo).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u7acb\u4f53\u5339\u914d\u6846\u67b6\uff08Wavelet-Stereo\uff09\uff0c\u901a\u8fc7\u5206\u79bb\u5904\u7406\u9ad8\u3001\u4f4e\u9891\u5206\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fed\u4ee3\u65b9\u6cd5\u5728\u9ad8\u9891\u533a\u57df\uff08\u5982\u8fb9\u7f18\u548c\u7ec6\u7269\u4f53\uff09\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8fed\u4ee3\u65b9\u6cd5\u5728\u4f18\u5316\u6240\u6709\u9891\u7387\u5206\u91cf\u65f6\u672a\u533a\u5206\u9ad8\u3001\u4f4e\u9891\uff0c\u5bfc\u81f4\u9ad8\u9891\u533a\u57df\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u9ad8\u3001\u4f4e\u9891\u5206\u91cf\uff0c\u5206\u522b\u8f93\u5165\u591a\u5c3a\u5ea6\u9891\u7387\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eLSTM\u7684\u9ad8\u9891\u4fdd\u7559\u66f4\u65b0\u7b97\u5b50\u3002", "result": "Wavelet-Stereo\u5728KITTI 2015\u548c2012\u6392\u884c\u699c\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u51e0\u4e4e\u5728\u6240\u6709\u6307\u6807\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u5904\u7406\u9ad8\u3001\u4f4e\u9891\u5206\u91cf\uff0c\u8be5\u6846\u67b6\u80fd\u540c\u65f6\u4f18\u5316\u8fb9\u7f18\u548c\u5e73\u6ed1\u533a\u57df\uff0c\u9002\u7528\u4e8e\u7ec6\u8282\u4e30\u5bcc\u7684\u573a\u666f\u3002", "relevance": 30.0}}
{"id": "2505.17794", "pdf": "https://arxiv.org/pdf/2505.17794", "abs": "https://arxiv.org/abs/2505.17794", "authors": ["\u00d6mer Faruk Akg\u00fcl", "Feiyu Zhu", "Yuxin Yang", "Rajgopal Kannan", "Viktor Prasanna"], "title": "RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion", "categories": ["cs.LG"], "comment": null, "summary": "Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped\nrelations between entities. TKG completion involves forecasting missing or\nfuture links, requiring models to reason over time-evolving structure. While\nLLMs show promise for this task, existing approaches often overemphasize\nsupervised fine-tuning and struggle particularly when historical evidence is\nlimited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient\nframework designed to improve accuracy and generalization in settings with\nsparse historical context. It combines (1) rule-based multi-hop retrieval for\nstructurally diverse history, (2) contrastive fine-tuning of lightweight\nadapters to encode relational semantics, and (3) test-time semantic filtering\nto iteratively refine generations based on embedding similarity. Experiments on\nfour TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based\napproaches, achieving up to 30.6\\% relative improvement in Hits@10. Moreover,\nour proposed framework produces more semantically coherent predictions, even\nfor the samples with limited historical context.", "AI": {"tldr": "RECIPE-TKG\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u68c0\u7d22\u3001\u5bf9\u6bd4\u5fae\u8c03\u548c\u8bed\u4e49\u8fc7\u6ee4\uff0c\u63d0\u5347\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5386\u53f2\u8bc1\u636e\u6709\u9650\u6216\u7f3a\u5931\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u591a\u8df3\u89c4\u5219\u68c0\u7d22\uff1b2) \u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u7684\u5bf9\u6bd4\u5fae\u8c03\uff1b3) \u57fa\u4e8e\u5d4c\u5165\u76f8\u4f3c\u5ea6\u7684\u6d4b\u8bd5\u65f6\u8bed\u4e49\u8fc7\u6ee4\u3002", "result": "\u5728\u56db\u4e2aTKG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRECIPE-TKG\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u5347\u4e8630.6%\u7684Hits@10\uff0c\u4e14\u9884\u6d4b\u66f4\u8bed\u4e49\u4e00\u81f4\u3002", "conclusion": "RECIPE-TKG\u5728\u7a00\u758f\u5386\u53f2\u80cc\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 50.0}}
{"id": "2505.17471", "pdf": "https://arxiv.org/pdf/2505.17471", "abs": "https://arxiv.org/abs/2505.17471", "authors": ["Suifeng Zhao", "Zhuoran Jin", "Sujian Li", "Jun Gao"], "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a vital role in the financial\ndomain, powering applications such as real-time market analysis, trend\nforecasting, and interest rate computation. However, most existing RAG research\nin finance focuses predominantly on textual data, overlooking the rich visual\ncontent in financial documents, resulting in the loss of key analytical\ninsights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual\nRAG benchmark tailored for finance which effectively integrates multimodal data\nand provides visual citation to ensure traceability. It includes a bilingual\nretrieval corpus with 60,780 Chinese and 51,219 English pages, along with a\nhigh-quality, human-annotated question-answering (QA) dataset spanning\nheterogeneous data types and seven question categories. Moreover, we introduce\nRGenCite, an RAG baseline that seamlessly integrates visual citation with\ngeneration. Furthermore, we propose an automatic citation evaluation method to\nsystematically assess the visual citation capabilities of Multimodal Large\nLanguage Models (MLLMs). Extensive experiments on RGenCite underscore the\nchallenging nature of FinRAGBench-V, providing valuable insights for the\ndevelopment of multimodal RAG systems in finance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FinRAGBench-V\uff0c\u4e00\u4e2a\u9488\u5bf9\u91d1\u878d\u9886\u57df\u7684\u591a\u6a21\u6001RAG\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6574\u5408\u4e86\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u89c6\u89c9\u5f15\u7528\u529f\u80fd\u3002\u540c\u65f6\u63d0\u51fa\u4e86RGenCite\u4f5c\u4e3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u5f15\u7528\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u9886\u57df\u7684RAG\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u5185\u5bb9\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u5173\u952e\u5206\u6790\u4fe1\u606f\u4e22\u5931\u3002", "method": "1. \u6784\u5efaFinRAGBench-V\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u53cc\u8bed\u68c0\u7d22\u8bed\u6599\u5e93\u548c\u9ad8\u8d28\u91cfQA\u6570\u636e\u96c6\u30022. \u63d0\u51faRGenCite\u57fa\u7ebf\u6a21\u578b\uff0c\u6574\u5408\u89c6\u89c9\u5f15\u7528\u4e0e\u751f\u6210\u30023. \u5f00\u53d1\u81ea\u52a8\u5f15\u7528\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFinRAGBench-V\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3a\u91d1\u878d\u9886\u57df\u591a\u6a21\u6001RAG\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "FinRAGBench-V\u586b\u8865\u4e86\u91d1\u878d\u9886\u57df\u591a\u6a21\u6001RAG\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5de5\u5177\u3002", "relevance": 60.0}}
{"id": "2505.18025", "pdf": "https://arxiv.org/pdf/2505.18025", "abs": "https://arxiv.org/abs/2505.18025", "authors": ["Evangelos Sariyanidi", "Claudio Ferrari", "Federico Nocentini", "Stefano Berretti", "Andrea Cavallaro", "Birkan Tunc"], "title": "3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation", "categories": ["cs.CV"], "comment": "To be published in IEEE International Conference on Automatic Face\n  and Gesture Recognition, 2025", "summary": "Computing the standard benchmark metric for 3D face reconstruction, namely\ngeometric error, requires a number of steps, such as mesh cropping, rigid\nalignment, or point correspondence. Current benchmark tools are monolithic\n(they implement a specific combination of these steps), even though there is no\nconsensus on the best way to measure error. We present a toolkit for a\nModularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental\ncomponents of error computation are segregated and interchangeable, allowing\none to quantify the effect of each. Furthermore, we propose a new component,\nnamely correction, and present a computationally efficient approach that\npenalizes for mesh topology inconsistency. Using this toolkit, we test 16 error\nestimators with 10 reconstruction methods on two real and two synthetic\ndatasets. Critically, the widely used ICP-based estimator provides the worst\nbenchmarking performance, as it significantly alters the true ranking of the\ntop-5 reconstruction methods. Notably, the correlation of ICP with the true\nerror can be as low as 0.41. Moreover, non-rigid alignment leads to significant\nimprovement (correlation larger than 0.90), highlighting the importance of\nannotating 3D landmarks on datasets. Finally, the proposed correction scheme,\ntogether with non-rigid warping, leads to an accuracy on a par with the best\nnon-rigid ICP-based estimators, but runs an order of magnitude faster. Our\nopen-source codebase is designed for researchers to easily compare alternatives\nfor each component, thus helping accelerating progress in benchmarking for 3D\nface reconstruction and, furthermore, supporting the improvement of learned\nreconstruction methods, which depend on accurate error estimation for effective\ntraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u76843D\u4eba\u8138\u91cd\u5efa\u57fa\u51c6\u5de5\u5177\u5305\uff08M3DFB\uff09\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u53ef\u4e92\u6362\u7684\u8bef\u5dee\u8ba1\u7b97\u7ec4\u4ef6\uff0c\u91cf\u5316\u6bcf\u4e2a\u6b65\u9aa4\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6821\u6b63\u7ec4\u4ef6\u3002", "motivation": "\u5f53\u524d3D\u4eba\u8138\u91cd\u5efa\u7684\u57fa\u51c6\u5de5\u5177\u662f\u5355\u4e00\u7684\uff0c\u7f3a\u4e4f\u5bf9\u8bef\u5dee\u8ba1\u7b97\u6700\u4f73\u65b9\u5f0f\u7684\u5171\u8bc6\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u65b9\u6cd5\u7684\u51c6\u786e\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u5de5\u5177\u5305M3DFB\uff0c\u5206\u79bb\u8bef\u5dee\u8ba1\u7b97\u7684\u57fa\u672c\u7ec4\u4ef6\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6821\u6b63\u7ec4\u4ef6\u3002\u6d4b\u8bd5\u4e8616\u79cd\u8bef\u5dee\u4f30\u8ba1\u5668\u548c10\u79cd\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5e7f\u6cdb\u4f7f\u7528\u7684ICP\u4f30\u8ba1\u5668\u6027\u80fd\u6700\u5dee\uff0c\u800c\u975e\u521a\u6027\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u76f8\u5173\u6027>0.90\uff09\u3002\u63d0\u51fa\u7684\u6821\u6b63\u65b9\u6848\u4e0e\u975e\u521a\u6027\u53d8\u5f62\u7ed3\u5408\uff0c\u6027\u80fd\u4e0e\u6700\u4f73\u975e\u521a\u6027ICP\u4f30\u8ba1\u5668\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u5feb10\u500d\u3002", "conclusion": "M3DFB\u5de5\u5177\u5305\u652f\u6301\u7814\u7a76\u4eba\u5458\u8f7b\u677e\u6bd4\u8f83\u4e0d\u540c\u7ec4\u4ef6\uff0c\u52a0\u901f3D\u4eba\u8138\u91cd\u5efa\u57fa\u51c6\u7684\u8fdb\u6b65\uff0c\u5e76\u63d0\u5347\u4f9d\u8d56\u51c6\u786e\u8bef\u5dee\u4f30\u8ba1\u7684\u5b66\u4e60\u91cd\u5efa\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.17797", "pdf": "https://arxiv.org/pdf/2505.17797", "abs": "https://arxiv.org/abs/2505.17797", "authors": ["Manuel Morante", "Naveed ur Rehman"], "title": "Latent Mode Decomposition", "categories": ["cs.LG"], "comment": "12 pages, 9 figures, 1 table", "summary": "We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm\nfor extracting oscillatory modes and associated connectivity structures from\nmultivariate signals. VLMD addresses key limitations of existing Multivariate\nMode Decomposition (MMD) techniques -including high computational cost,\nsensitivity to parameter choices, and weak modeling of interchannel\ndependencies. Its improved performance is driven by a novel underlying model,\nLatent Mode Decomposition (LMD), which blends sparse coding and mode\ndecomposition to represent multichannel signals as sparse linear combinations\nof shared latent components composed of AM-FM oscillatory modes. This\nformulation enables VLMD to operate in a lower-dimensional latent space,\nenhancing robustness to noise, scalability, and interpretability. The algorithm\nsolves a constrained variational optimization problem that jointly enforces\nreconstruction fidelity, sparsity, and frequency regularization. Experiments on\nsynthetic and real-world datasets demonstrate that VLMD outperforms\nstate-of-the-art MMD methods in accuracy, efficiency, and interpretability of\nextracted structures.", "AI": {"tldr": "VLMD\u662f\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u591a\u53d8\u91cf\u4fe1\u53f7\u4e2d\u63d0\u53d6\u632f\u8361\u6a21\u5f0f\u548c\u8fde\u63a5\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MMD\u6280\u672f\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u53c2\u6570\u654f\u611f\u6027\u548c\u901a\u9053\u95f4\u4f9d\u8d56\u5efa\u6a21\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u591a\u53d8\u91cf\u6a21\u5f0f\u5206\u89e3\uff08MMD\uff09\u6280\u672f\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u53c2\u6570\u654f\u611f\u6027\u548c\u901a\u9053\u95f4\u4f9d\u8d56\u5efa\u6a21\u5f31\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6f5c\u5728\u6a21\u5f0f\u5206\u89e3\uff08LMD\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u7a00\u758f\u7f16\u7801\u548c\u6a21\u5f0f\u5206\u89e3\uff0c\u5c06\u591a\u901a\u9053\u4fe1\u53f7\u8868\u793a\u4e3a\u5171\u4eab\u6f5c\u5728\u6210\u5206\u7684\u7a00\u758f\u7ebf\u6027\u7ec4\u5408\u3002\u901a\u8fc7\u53d8\u5206\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u7a00\u758f\u6027\u548c\u9891\u7387\u6b63\u5219\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cVLMD\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u63d0\u53d6\u7ed3\u6784\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709MMD\u65b9\u6cd5\u3002", "conclusion": "VLMD\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\u548c\u53d8\u5206\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u4fe1\u53f7\u5206\u6790\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.17481", "pdf": "https://arxiv.org/pdf/2505.17481", "abs": "https://arxiv.org/abs/2505.17481", "authors": ["Yusheng Zhao", "Xiao Luo", "Weizhi Zhang", "Wei Ju", "Zhiping Xiao", "Philip S. Yu", "Ming Zhang"], "title": "MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The ability to reason is one of the most fundamental capabilities of large\nlanguage models (LLMs), enabling a wide range of downstream tasks through\nsophisticated problem-solving. A critical aspect of this is code reasoning,\nwhich involves logical reasoning with formal languages (i.e., programming\ncode). In this paper, we enhance this capability of LLMs by exploring the\nfollowing question: how can an LLM agent become progressively smarter in code\nreasoning with each solution it proposes, thereby achieving substantial\ncumulative improvement? Most existing research takes a static perspective,\nfocusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a\ncognitive-evolving perspective and propose a novel framework named\nMeta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve\ndynamically during inference through self-improvement. From the perspective of\nhuman cognitive development, we leverage both knowledge accumulation and lesson\nsharing. In particular, to accumulate knowledge during problem-solving, we\npropose meta-reflection that reflects on the reasoning paths of the current\nproblem to obtain knowledge and experience for future consideration. Moreover,\nto effectively utilize the lessons from other agents, we propose\ncross-referencing that incorporates the solution and feedback from other agents\ninto the current problem-solving process. We conduct experiments across various\ndatasets in code reasoning, and the results demonstrate the effectiveness of\nMARCO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMARCO\u7684\u52a8\u6001\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u53cd\u601d\u548c\u4ea4\u53c9\u5f15\u7528\u63d0\u5347LLM\u5728\u4ee3\u7801\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u91c7\u7528\u9759\u6001\u89c6\u89d2\uff0c\u800cMARCO\u4ece\u8ba4\u77e5\u53d1\u5c55\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u81ea\u6211\u6539\u8fdb\u63d0\u5347LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMARCO\u6846\u67b6\uff0c\u7ed3\u5408\u5143\u53cd\u601d\uff08\u79ef\u7d2f\u77e5\u8bc6\uff09\u548c\u4ea4\u53c9\u5f15\u7528\uff08\u5171\u4eab\u7ecf\u9a8c\uff09\uff0c\u4f7fLLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8fdb\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMARCO\u5728\u591a\u4e2a\u4ee3\u7801\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MARCO\u901a\u8fc7\u52a8\u6001\u81ea\u6211\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.18035", "pdf": "https://arxiv.org/pdf/2505.18035", "abs": "https://arxiv.org/abs/2505.18035", "authors": ["Naseem Khan", "Tuan Nguyen", "Amine Bermak", "Issa Khalil"], "title": "CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention", "categories": ["cs.CV", "F.2.2; I.2.7"], "comment": "20 pages, 8 figures, 12 Tables", "summary": "The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures.", "AI": {"tldr": "CAMME\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u9891\u57df\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u8de8\u9886\u57df\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u672a\u89c1\u751f\u6210\u67b6\u6784\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCAMME\u6846\u67b6\uff0c\u5229\u7528\u591a\u5934\u8de8\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "CAMME\u5728\u81ea\u7136\u573a\u666f\u548c\u4eba\u8138\u6df1\u5ea6\u4f2a\u9020\u4e0a\u5206\u522b\u63d0\u534712.56%\u548c13.25%\u7684\u6027\u80fd\uff0c\u5bf9\u6297\u653b\u51fb\u4e0b\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17799", "pdf": "https://arxiv.org/pdf/2505.17799", "abs": "https://arxiv.org/abs/2505.17799", "authors": ["Brian B. Moser", "Arundhati S. Shanbhag", "Stanislav Frolov", "Federico Raue", "Joachim Folz", "Andreas Dengel"], "title": "A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6838\u5fc3\u96c6\u9009\u62e9\u7684\u7814\u7a76\uff0c\u7edf\u4e00\u4e86\u8bad\u7ec3\u65e0\u5173\u3001\u8bad\u7ec3\u5bfc\u5411\u548c\u65e0\u6807\u7b7e\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b50\u6a21\u5757\u5316\u3001\u53cc\u5c42\u4f18\u5316\u548c\u4f2a\u6807\u7b7e\u7b49\u65b0\u8fdb\u5c55\uff0c\u540c\u65f6\u5206\u6790\u4e86\u4fee\u526a\u7b56\u7565\u5bf9\u6cdb\u5316\u548c\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\u7684\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u7efc\u8ff0\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u8bad\u7ec3\u65e0\u5173\u3001\u8bad\u7ec3\u5bfc\u5411\u548c\u65e0\u6807\u7b7e\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5b50\u6a21\u5757\u5316\u3001\u53cc\u5c42\u4f18\u5316\u548c\u4f2a\u6807\u7b7e\u7b49\u6280\u672f\u3002", "result": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u8ba1\u7b97\u3001\u9c81\u68d2\u6027\u548c\u6027\u80fd\u9700\u6c42\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u6838\u5fc3\u96c6\u9009\u62e9\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3001\u9c81\u68d2\u6027\u548c\u5f02\u5e38\u503c\u8fc7\u6ee4\u662f\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.17485", "pdf": "https://arxiv.org/pdf/2505.17485", "abs": "https://arxiv.org/abs/2505.17485", "authors": ["Saketh Reddy Vemula", "Parameswari Krishnamurthy"], "title": "keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Identification of hallucination spans in black-box language model generated\ntext is essential for applications in the real world. A recent attempt at this\ndirection is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on\nHallucinations and Related Observable Over-generation Errors. In this work, we\npresent our solution to this problem, which capitalizes on the variability of\nstochastically-sampled responses in order to identify hallucinated spans. Our\nhypothesis is that if a language model is certain of a fact, its sampled\nresponses will be uniform, while hallucinated facts will yield different and\nconflicting results. We measure this divergence through entropy-based analysis,\nallowing for accurate identification of hallucinated segments. Our method is\nnot dependent on additional training and hence is cost-effective and adaptable.\nIn addition, we conduct extensive hyperparameter tuning and perform error\nanalysis, giving us crucial insights into model behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u9ed1\u76d2\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u4e2d\u7684\u5e7b\u89c9\u7247\u6bb5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u6210\u672c\u4f4e\u4e14\u9002\u5e94\u6027\u5f3a\u3002", "motivation": "\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5e7b\u89c9\u7247\u6bb5\u5bf9\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u968f\u673a\u91c7\u6837\u54cd\u5e94\u7684\u53d8\u5f02\u6027\uff0c\u5229\u7528\u71b5\u5ea6\u91cf\u5206\u6b67\uff0c\u8bc6\u522b\u5e7b\u89c9\u7247\u6bb5\u3002", "result": "\u65b9\u6cd5\u80fd\u51c6\u786e\u8bc6\u522b\u5e7b\u89c9\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u9519\u8bef\u5206\u6790\u63d0\u4f9b\u4e86\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5165\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7406\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.18039", "pdf": "https://arxiv.org/pdf/2505.18039", "abs": "https://arxiv.org/abs/2505.18039", "authors": ["Li Zhong", "Ahmed Ghazal", "Jun-Jun Wan", "Frederik Zilly", "Patrick Mackens", "Joachim E. Vollrath", "Bogdan Sorin Coseriu"], "title": "Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing.", "AI": {"tldr": "Clip4Retrofit\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\uff0c\u901a\u8fc7\u5c06CLIP\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3CLIP\u7b49\u57fa\u7840\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5927\u5185\u5b58\u5360\u7528\u95ee\u9898\u3002", "method": "\u63d0\u51faClip4Retrofit\u6846\u67b6\uff0c\u7ed3\u5408EfficientNet-B3\u548c\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u6295\u5f71\u5934\uff0c\u5c06CLIP\u6a21\u578b\u84b8\u998f\u4e3a\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u3002", "result": "\u84b8\u998f\u540e\u7684\u6a21\u578b\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u548c\u5bf9\u8c61\u8bc6\u522b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002", "relevance": 60.0}}
{"id": "2505.17209", "pdf": "https://arxiv.org/pdf/2505.17209", "abs": "https://arxiv.org/abs/2505.17209", "authors": ["Huaiyuan Yao", "Pengfei Li", "Bu Jin", "Yupeng Zheng", "An Liu", "Lisen Mu", "Qing Su", "Qian Zhang", "Yilun Chen", "Peng Li"], "title": "LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios", "categories": ["cs.RO", "cs.AI", "68T05", "I.2.9; I.2.7; I.2.6"], "comment": "7 pages, 3 figures", "summary": "Recent advances in autonomous driving research towards motion planners that\nare robust, safe, and adaptive. However, existing rule-based and data-driven\nplanners lack adaptability to long-tail scenarios, while knowledge-driven\nmethods offer strong reasoning but face challenges in representation, control,\nand real-world evaluation. To address these challenges, we present LiloDriver,\na lifelong learning framework for closed-loop motion planning in long-tail\nautonomous driving scenarios. By integrating large language models (LLMs) with\na memory-augmented planner generation system, LiloDriver continuously adapts to\nnew scenarios without retraining. It features a four-stage architecture\nincluding perception, scene encoding, memory-based strategy refinement, and\nLLM-guided reasoning. Evaluated on the nuPlan benchmark, LiloDriver achieves\nsuperior performance in both common and rare driving scenarios, outperforming\nstatic rule-based and learning-based planners. Our results highlight the\neffectiveness of combining structured memory and LLM reasoning to enable\nscalable, human-like motion planning in real-world autonomous driving. Our code\nis available at https://github.com/Hyan-Yao/LiloDriver.", "AI": {"tldr": "LiloDriver\u662f\u4e00\u4e2a\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u8bb0\u5fc6\u589e\u5f3a\u89c4\u5212\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u957f\u5c3e\u573a\u666f\u95ed\u73af\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u67b6\u6784\uff08\u611f\u77e5\u3001\u573a\u666f\u7f16\u7801\u3001\u57fa\u4e8e\u8bb0\u5fc6\u7684\u7b56\u7565\u4f18\u5316\u3001LLM\u5f15\u5bfc\u63a8\u7406\uff09\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6301\u7eed\u9002\u5e94\u65b0\u573a\u666f\u3002", "result": "\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u9759\u6001\u89c4\u5219\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u5668\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u8bb0\u5fc6\u548cLLM\u63a8\u7406\uff0c\u53ef\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7c7b\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "relevance": 60.0}}
{"id": "2505.17804", "pdf": "https://arxiv.org/pdf/2505.17804", "abs": "https://arxiv.org/abs/2505.17804", "authors": ["Jonas Seng", "Fabrizio Ventola", "Zhongjie Yu", "Kristian Kersting"], "title": "Hyperparameter Optimization via Interacting with Probabilistic Circuits", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the growing interest in designing truly interactive hyperparameter\noptimization (HPO) methods, to date, only a few allow to include human\nfeedback. Existing interactive Bayesian optimization (BO) methods incorporate\nhuman beliefs by weighting the acquisition function with a user-defined prior\ndistribution. However, in light of the non-trivial inner optimization of the\nacquisition function prevalent in BO, such weighting schemes do not always\naccurately reflect given user beliefs. We introduce a novel BO approach\nleveraging tractable probabilistic models named probabilistic circuits (PCs) as\na surrogate model. PCs encode a tractable joint distribution over the hybrid\nhyperparameter space and evaluation scores. They enable exact conditional\ninference and sampling. Based on conditional sampling, we construct a novel\nselection policy that enables an acquisition function-free generation of\ncandidate points (thereby eliminating the need for an additional inner-loop\noptimization) and ensures that user beliefs are reflected accurately in the\nselection policy. We provide a theoretical analysis and an extensive empirical\nevaluation, demonstrating that our method achieves state-of-the-art performance\nin standard HPO and outperforms interactive BO baselines in interactive HPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u7535\u8def\uff08PCs\uff09\u7684\u65b0\u578b\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u8d85\u53c2\u6570\u4f18\u5316\uff08HPO\uff09\uff0c\u65e0\u9700\u5185\u5faa\u73af\u4f18\u5316\uff0c\u51c6\u786e\u53cd\u6620\u7528\u6237\u4fe1\u5ff5\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u901a\u8fc7\u52a0\u6743\u91c7\u96c6\u51fd\u6570\u5f15\u5165\u7528\u6237\u5148\u9a8c\u5206\u5e03\uff0c\u4f46\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u7528\u6237\u4fe1\u5ff5\u3002", "method": "\u5229\u7528\u6982\u7387\u7535\u8def\uff08PCs\uff09\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u652f\u6301\u7cbe\u786e\u6761\u4ef6\u63a8\u7406\u548c\u91c7\u6837\uff0c\u6784\u5efa\u65e0\u91c7\u96c6\u51fd\u6570\u7684\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6HPO\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4ea4\u4e92\u5f0fHPO\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7PCs\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u4ea4\u4e92\u5f0fHPO\u3002", "relevance": 40.0}}
{"id": "2505.17496", "pdf": "https://arxiv.org/pdf/2505.17496", "abs": "https://arxiv.org/abs/2505.17496", "authors": ["Chi-Yuan Hsiao", "Ke-Han Lu", "Kai-Wei Chang", "Chih-Kai Yang", "Wei-Chih Chen", "Hung-yi Lee"], "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "End-to-end training of Spoken Language Models (SLMs) commonly involves\nadapting pre-trained text-based Large Language Models (LLMs) to the speech\nmodality through multi-stage training on diverse tasks such as ASR, TTS and\nspoken question answering (SQA). Although this multi-stage continual learning\nequips LLMs with both speech understanding and generation capabilities, the\nsubstantial differences in task and data distributions across stages can lead\nto catastrophic forgetting, where previously acquired knowledge is lost. This\npaper investigates catastrophic forgetting and evaluates three mitigation\nstrategies-model merging, discounting the LoRA scaling factor, and experience\nreplay to balance knowledge retention with new learning. Results show that\nexperience replay is the most effective, with further gains achieved by\ncombining it with other methods. These findings provide insights for developing\nmore robust and efficient SLM training pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u591a\u9636\u6bb5\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u9884\u8bad\u7ec3\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9002\u5e94\u8bed\u97f3\u6a21\u6001\u65f6\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u7f13\u89e3\u7b56\u7565\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u9a8c\u56de\u653e\u662f\u6700\u6709\u6548\u7684\uff0c\u7ed3\u5408\u5176\u4ed6\u65b9\u6cd5\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u591a\u9636\u6bb5\u6301\u7eed\u5b66\u4e60\u4f7fLLM\u5177\u5907\u8bed\u97f3\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u4f46\u4efb\u52a1\u548c\u6570\u636e\u5206\u5e03\u7684\u663e\u8457\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5373\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u4e22\u5931\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u7b56\u7565\uff1a\u6a21\u578b\u5408\u5e76\u3001\u964d\u4f4eLoRA\u7f29\u653e\u56e0\u5b50\u548c\u7ecf\u9a8c\u56de\u653e\u3002", "result": "\u7ecf\u9a8c\u56de\u653e\u662f\u6700\u6709\u6548\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u5176\u4ed6\u65b9\u6cd5\u53ef\u8fdb\u4e00\u6b65\u6539\u5584\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "relevance": 85.0}}
{"id": "2505.18047", "pdf": "https://arxiv.org/pdf/2505.18047", "abs": "https://arxiv.org/abs/2505.18047", "authors": ["Sudarshan Rajagopalan", "Kartik Narayan", "Vishal M. Patel"], "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://sudraj2002.github.io/restorevarpage/", "summary": "The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.", "AI": {"tldr": "RestoreVAR\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\uff08VAR\uff09\u7684\u65b0\u578b\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8eAll-in-One\u56fe\u50cf\u6062\u590d\uff08AiOR\uff09\uff0c\u5728\u6062\u590d\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1LDM\uff08\u5982Stable Diffusion\uff09\u63d0\u5347\u4e86AiOR\u65b9\u6cd5\u7684\u611f\u77e5\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u4e0d\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u573a\u666f\u3002", "method": "RestoreVAR\u5229\u7528VAR\u8fdb\u884c\u5c3a\u5ea6\u7a7a\u95f4\u81ea\u56de\u5f52\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u6f5c\u5728\u7a7a\u95f4\u7ec6\u5316\u6a21\u5757\uff0c\u4f18\u5316\u4e86AiOR\u4efb\u52a1\u3002", "result": "RestoreVAR\u5728\u751f\u6210\u5f0fAiOR\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4LDM\u5feb10\u500d\u4ee5\u4e0a\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RestoreVAR\u901a\u8fc7VAR\u548c\u67b6\u6784\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86LDM\u7684\u63a8\u7406\u901f\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17210", "pdf": "https://arxiv.org/pdf/2505.17210", "abs": "https://arxiv.org/abs/2505.17210", "authors": ["Martin Villagrana", "Francisco Lopez-Tiro", "Clement Larose", "Gilberto Ochoa-Ruiz", "Christian Daul"], "title": "Assessing the generalization performance of SAM for ureteroscopy scene understanding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages, 4 figures, 2 tables, conference, MIUA25", "summary": "The segmentation of kidney stones is regarded as a critical preliminary step\nto enable the identification of urinary stone types through machine- or\ndeep-learning-based approaches. In urology, manual segmentation is considered\ntedious and impractical due to the typically large scale of image databases and\nthe continuous generation of new data. In this study, the potential of the\nSegment Anything Model (SAM) -- a state-of-the-art deep learning framework --\nis investigated for the automation of kidney stone segmentation. The\nperformance of SAM is evaluated in comparison to traditional models, including\nU-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,\nfrequently exhibit limitations in generalizing to unseen datasets. The findings\nhighlight SAM's superior adaptability and efficiency. While SAM achieves\ncomparable performance to U-Net on in-distribution data (Accuracy: 97.68 +\n3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly\nenhanced generalization capabilities on out-of-distribution data, surpassing\nall U-Net variants by margins of up to 23 percent.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Segment Anything Model (SAM)\u5728\u80be\u810f\u7ed3\u77f3\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfU-Net\u6a21\u578b\u3002", "motivation": "\u80be\u810f\u7ed3\u77f3\u5206\u5272\u662f\u8bc6\u522b\u7ed3\u77f3\u7c7b\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u624b\u52a8\u5206\u5272\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u5e93\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86SAM\u4e0e\u4f20\u7edf\u6a21\u578b\uff08U-Net\u3001Residual U-Net\u3001Attention U-Net\uff09\u5728\u80be\u810f\u7ed3\u77f3\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u6cdb\u5316\u80fd\u529b\u3002", "result": "SAM\u5728\u5206\u5e03\u5185\u6570\u636e\u4e0a\u8868\u73b0\u4e0eU-Net\u76f8\u5f53\uff08\u51c6\u786e\u738797.68\u00b13.04\uff09\uff0c\u4f46\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8eU-Net\u53d8\u4f53\uff08\u63d0\u5347\u8fbe23%\uff09\u3002", "conclusion": "SAM\u5728\u80be\u810f\u7ed3\u77f3\u5206\u5272\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\uff0c\u9002\u5408\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.17810", "pdf": "https://arxiv.org/pdf/2505.17810", "abs": "https://arxiv.org/abs/2505.17810", "authors": ["Elias J\u00e4\u00e4saari", "Ville Hyv\u00f6nen", "Matteo Ceccarello", "Teemu Roos", "Martin Aum\u00fcller"], "title": "VIBE: Vector Index Benchmark for Embeddings", "categories": ["cs.LG", "cs.IR"], "comment": "25 pages", "summary": "Approximate nearest neighbor (ANN) search is a performance-critical component\nof many machine learning pipelines. Rigorous benchmarking is essential for\nevaluating the performance of vector indexes for ANN search. However, the\ndatasets of the existing benchmarks are no longer representative of the current\napplications of ANN search. Hence, there is an urgent need for an up-to-date\nset of benchmarks. To this end, we introduce Vector Index Benchmark for\nEmbeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE\ncontains a pipeline for creating benchmark datasets using dense embedding\nmodels characteristic of modern applications, such as retrieval-augmented\ngeneration (RAG). To replicate real-world workloads, we also include\nout-of-distribution (OOD) datasets where the queries and the corpus are drawn\nfrom different distributions. We use VIBE to conduct a comprehensive evaluation\nof SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution\nand 6 out-of-distribution datasets.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86VIBE\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANN\uff09\u7b97\u6cd5\u7684\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e0d\u5177\u4ee3\u8868\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709ANN\u641c\u7d22\u57fa\u51c6\u7684\u6570\u636e\u96c6\u5df2\u65e0\u6cd5\u4ee3\u8868\u5f53\u524d\u5e94\u7528\u9700\u6c42\uff0c\u4e9f\u9700\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "\u63d0\u51faVIBE\uff0c\u901a\u8fc7\u5bc6\u96c6\u5d4c\u5165\u6a21\u578b\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\uff0c\u4ee5\u6a21\u62df\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u572812\u4e2a\u5206\u5e03\u5185\u548c6\u4e2a\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e8621\u79cdSOTA\u5411\u91cf\u7d22\u5f15\u5b9e\u73b0\u3002", "conclusion": "VIBE\u4e3aANN\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u73b0\u4ee3\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\u3002", "relevance": 40.0}}
{"id": "2505.17503", "pdf": "https://arxiv.org/pdf/2505.17503", "abs": "https://arxiv.org/abs/2505.17503", "authors": ["Minsoo Khang", "Sangjun Park", "Teakgyu Hong", "Dawoon Jung"], "title": "CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made substantial progress in recent years,\nyet evaluating their capabilities in practical Retrieval-Augmented Generation\n(RAG) scenarios remains challenging. In practical applications, LLMs must\ndemonstrate complex reasoning, refuse to answer appropriately, provide precise\ncitations, and effectively understand document layout. These capabilities are\ncrucial for advanced task handling, uncertainty awareness, maintaining\nreliability, and structural understanding. While some of the prior works\naddress these aspects individually, there is a need for a unified framework\nthat evaluates them collectively in practical RAG scenarios. To address this,\nwe present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation\nwith Complex Reasoning over Structured Documents), a benchmark designed to\nassess these key dimensions holistically. CReSt comprises 2,245 human-annotated\nexamples in English and Korean, designed to capture practical RAG scenarios\nthat require complex reasoning over structured documents. It also introduces a\ntailored evaluation methodology to comprehensively assess model performance in\nthese critical areas. Our evaluation shows that even advanced LLMs struggle to\nperform consistently across these dimensions, underscoring key areas for\nimprovement. We release CReSt to support further research and the development\nof more robust RAG systems. The dataset and code are available at:\nhttps://github.com/UpstageAI/CReSt.", "AI": {"tldr": "CReSt\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u573a\u666f\u4e2dLLM\u7efc\u5408\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u590d\u6742\u63a8\u7406\u3001\u62d2\u7edd\u56de\u7b54\u3001\u7cbe\u786e\u5f15\u7528\u548c\u6587\u6863\u5e03\u5c40\u7406\u89e3\u7b49\u5173\u952e\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLM\u5728RAG\u573a\u666f\u4e2d\u7efc\u5408\u80fd\u529b\u7684\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0cCReSt\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "CReSt\u5305\u542b2,245\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u82f1\u6587\u548c\u97e9\u6587\u793a\u4f8b\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684LLM\u5728\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4e5f\u4e0d\u7a33\u5b9a\uff0c\u63ed\u793a\u4e86\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "CReSt\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "relevance": 85.0}}
{"id": "2505.18048", "pdf": "https://arxiv.org/pdf/2505.18048", "abs": "https://arxiv.org/abs/2505.18048", "authors": ["Simon Malzard", "Nitish Mital", "Richard Walters", "Victoria Nockles", "Raghuveer Rao", "Celso M. De Melo"], "title": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios", "categories": ["cs.CV"], "comment": "19 pages, 2 images", "summary": "Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u9aa8\u9abc\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\uff08SHAR\uff09\u7684\u6570\u636e\u9000\u5316\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u9886\u5148\u6a21\u578b\u5728\u4e09\u79cd\u9000\u5316\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u53d1\u73b0\u9000\u5316\u7c7b\u578b\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u901a\u8fc7\u63d2\u503c\u65b9\u6cd5\u6539\u8fdb\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c97\u7cd9\u8def\u5f84\u7406\u8bba\u7684\u9000\u5316\u62b5\u6297\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u9000\u5316\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9aa8\u9abc\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5728NTU-RGB+D-120\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u4e86\u6570\u636e\u9000\u5316\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cdSHAR\u6a21\u578b\u5bf9\u4e09\u79cd\u9000\u5316\u7c7b\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u63d2\u503c\u65b9\u6cd5\u6539\u8fdb\u6027\u80fd\u3002", "result": "\u9000\u5316\u7c7b\u578b\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff08>40%\u5dee\u5f02\uff09\uff0c\u63d2\u503c\u65b9\u6cd5\u63d0\u5347\u6027\u80fd>40%\uff0c\u57fa\u4e8e\u7c97\u7cd9\u8def\u5f84\u7406\u8bba\u7684LogSigRNN\u6a21\u578b\u5728\u4f4e\u5e27\u7387\u4e0b\u4f18\u4e8eSoTA\u6a21\u578b\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u6570\u636e\u9000\u5316\u5bf9SHAR\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u9000\u5316\u62b5\u6297\u6a21\u578b\u3002", "relevance": 30.0}}
{"id": "2505.17826", "pdf": "https://arxiv.org/pdf/2505.17826", "abs": "https://arxiv.org/abs/2505.17826", "authors": ["Xuchen Pan", "Yanxi Chen", "Yushuo Chen", "Yuchang Sun", "Daoyuan Chen", "Wenhao Zhang", "Yuexiang Xie", "Yilun Huang", "Yilei Zhang", "Dawei Gao", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models", "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": "This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT", "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.", "AI": {"tldr": "Trinity-RFT\u662f\u4e00\u4e2a\u901a\u7528\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u7edf\u4e00\u7684\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u89e3\u8026\u8bbe\u8ba1\uff0c\u5305\u62ecRFT\u6838\u5fc3\u3001\u9ad8\u6548\u7684\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u96c6\u6210\u548c\u4f18\u5316\u7684\u6570\u636e\u7ba1\u9053\u3002", "result": "\u5c55\u793a\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u652f\u6301\u540c\u6b65/\u5f02\u6b65\u3001\u5728\u7ebf/\u79bb\u7ebf\u7b49\u591a\u79cdRFT\u6a21\u5f0f\u3002", "conclusion": "Trinity-RFT\u4e3a\u63a2\u7d22\u9ad8\u7ea7\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u3002", "relevance": 85.0}}
{"id": "2505.17505", "pdf": "https://arxiv.org/pdf/2505.17505", "abs": "https://arxiv.org/abs/2505.17505", "authors": ["Xiaohao Liu", "Xiaobo Xia", "Weixiang Zhao", "Manyi Zhang", "Xianzhi Yu", "Xiu Su", "Shuo Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved notable progress. Despite their\nsuccess, next-token prediction (NTP), the dominant method for LLM training and\ninference, is constrained in both contextual coverage and inference efficiency\ndue to its inherently sequential process. To overcome these challenges, we\npropose leap multi-token prediction~(L-MTP), an innovative token prediction\nmethod that extends the capabilities of multi-token prediction (MTP) by\nintroducing a leap-based mechanism. Unlike conventional MTP, which generates\nmultiple tokens at adjacent positions, L-MTP strategically skips over\nintermediate tokens, predicting non-sequential ones in a single forward pass.\nThis structured leap not only enhances the model's ability to capture\nlong-range dependencies but also enables a decoding strategy specially\noptimized for non-sequential leap token generation, effectively accelerating\ninference. We theoretically demonstrate the benefit of L-MTP in improving\ninference efficiency. Experiments across diverse benchmarks validate its merit\nin boosting both LLM performance and inference speed. The source code will be\npublicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL-MTP\u7684\u521b\u65b0\u591a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8df3\u8dc3\u673a\u5236\u6539\u8fdb\u4f20\u7edf\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\uff08NTP\uff09\u5728\u4e0a\u4e0b\u6587\u8986\u76d6\u548c\u63a8\u7406\u6548\u7387\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0cL-MTP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "L-MTP\u901a\u8fc7\u8df3\u8dc3\u673a\u5236\u9884\u6d4b\u975e\u8fde\u7eed\u4ee4\u724c\uff0c\u5355\u6b21\u524d\u5411\u4f20\u9012\u751f\u6210\u591a\u4e2a\u4ee4\u724c\uff0c\u4f18\u5316\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eL-MTP\u5728\u63d0\u5347LLM\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6709\u6548\u3002", "conclusion": "L-MTP\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eLLM\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "relevance": 85.0}}
{"id": "2505.18049", "pdf": "https://arxiv.org/pdf/2505.18049", "abs": "https://arxiv.org/abs/2505.18049", "authors": ["Gaole Dai", "Menghang Dong", "Rongyu Zhang", "Ruichuan An", "Shanghang Zhang", "Tiejun Huang"], "title": "SpikeGen: Generative Framework for Visual Spike Stream Processing", "categories": ["cs.CV"], "comment": null, "summary": "Neuromorphic Visual Systems, such as spike cameras, have attracted\nconsiderable attention due to their ability to capture clear textures under\ndynamic conditions. This capability effectively mitigates issues related to\nmotion and aperture blur. However, in contrast to conventional RGB modalities\nthat provide dense spatial information, these systems generate binary,\nspatially sparse frames as a trade-off for temporally rich visual streams. In\nthis context, generative models emerge as a promising solution to address the\ninherent limitations of sparse data. These models not only facilitate the\nconditional fusion of existing information from both spike and RGB modalities\nbut also enable the conditional generation based on latent priors. In this\nstudy, we introduce a robust generative processing framework named SpikeGen,\ndesigned for visual spike streams captured by spike cameras. We evaluate this\nframework across multiple tasks involving mixed spike-RGB modalities, including\nconditional image/video deblurring, dense frame reconstruction from spike\nstreams, and high-speed scene novel-view synthesis. Supported by comprehensive\nexperimental results, we demonstrate that leveraging the latent space operation\nabilities of generative models allows us to effectively address the sparsity of\nspatial information while fully exploiting the temporal richness of spike\nstreams, thereby promoting a synergistic enhancement of different visual\nmodalities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpikeGen\u7684\u751f\u6210\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u7531\u8109\u51b2\u76f8\u673a\u6355\u83b7\u7684\u89c6\u89c9\u8109\u51b2\u6d41\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u51b3\u7a00\u758f\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8109\u51b2\u89c6\u89c9\u7cfb\u7edf\uff08\u5982\u8109\u51b2\u76f8\u673a\uff09\u80fd\u6355\u6349\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u6e05\u6670\u7eb9\u7406\uff0c\u4f46\u751f\u6210\u7684\u6570\u636e\u7a7a\u95f4\u7a00\u758f\u3002\u751f\u6210\u6a21\u578b\u88ab\u63d0\u51fa\u4ee5\u89e3\u51b3\u7a00\u758f\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u878d\u5408\u8109\u51b2\u548cRGB\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51faSpikeGen\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u80fd\u529b\uff0c\u5904\u7406\u8109\u51b2\u6d41\u5e76\u878d\u5408RGB\u6a21\u6001\u4fe1\u606f\uff0c\u5e94\u7528\u4e8e\u56fe\u50cf/\u89c6\u9891\u53bb\u6a21\u7cca\u3001\u5e27\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7b49\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpikeGen\u80fd\u6709\u6548\u89e3\u51b3\u7a7a\u95f4\u4fe1\u606f\u7a00\u758f\u95ee\u9898\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u8109\u51b2\u6d41\u7684\u65f6\u95f4\u4e30\u5bcc\u6027\uff0c\u5b9e\u73b0\u4e0d\u540c\u89c6\u89c9\u6a21\u6001\u7684\u534f\u540c\u589e\u5f3a\u3002", "conclusion": "SpikeGen\u6846\u67b6\u901a\u8fc7\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8109\u51b2\u6d41\u6570\u636e\u7684\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "relevance": 30.0}}
{"id": "2505.17830", "pdf": "https://arxiv.org/pdf/2505.17830", "abs": "https://arxiv.org/abs/2505.17830", "authors": ["Nicolas Castanet", "Olivier Sigaud", "Sylvain Lamprier"], "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u03b2-VAE\u6846\u67b6\u548c\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u89e3\u51b3GCRL\u4e2d\u6f5c\u5728\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u548c\u4e0b\u6e38\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u5728\u89c6\u89c9\u73af\u5883\u4e2d\uff0cGCRL\u56e0\u9ad8\u7ef4\u7a00\u758f\u89c2\u6d4b\u9762\u4e34\u6f5c\u5728\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u4ee3\u7406\u5b66\u4e60\u53d7\u9650\u3002", "method": "DRAG\u7ed3\u5408\u03b2-VAE\u6846\u67b6\u548c\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u5229\u7528\u5bf9\u6297\u6027\u795e\u7ecf\u52a0\u6743\u5668\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u5728\u8ff7\u5bab\u548c\u673a\u5668\u4eba\u63a7\u5236\u7b49\u786c\u63a2\u7d22\u73af\u5883\u4e2d\uff0cDRAG\u663e\u8457\u63d0\u5347\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "DRAG\u4e3aGCRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u5148\u9a8c\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u6269\u5c55\u6f5c\u5728\u7a7a\u95f4\u8986\u76d6\u3002", "relevance": 60.0}}
{"id": "2505.17510", "pdf": "https://arxiv.org/pdf/2505.17510", "abs": "https://arxiv.org/abs/2505.17510", "authors": ["Marcus Ma", "Georgios Chochlakis", "Niyantha Maruthu Pandiyan", "Jesse Thomason", "Shrikanth Narayanan"], "title": "Large Language Models Do Multi-Label Classification Differently", "categories": ["cs.CL"], "comment": "18 pages, 11 figures, 6 tables", "summary": "Multi-label classification is prevalent in real-world settings, but the\nbehavior of Large Language Models (LLMs) in this setting is understudied. We\ninvestigate how autoregressive LLMs perform multi-label classification, with a\nfocus on subjective tasks, by analyzing the output distributions of the models\nin each generation step. We find that their predictive behavior reflects the\nmultiple steps in the underlying language modeling required to generate all\nrelevant labels as they tend to suppress all but one label at each step. We\nfurther observe that as model scale increases, their token distributions\nexhibit lower entropy, yet the internal ranking of the labels improves.\nFinetuning methods such as supervised finetuning and reinforcement learning\namplify this phenomenon. To further study this issue, we introduce the task of\ndistribution alignment for multi-label settings: aligning LLM-derived label\ndistributions with empirical distributions estimated from annotator responses\nin subjective tasks. We propose both zero-shot and supervised methods which\nimprove both alignment and predictive performance over existing approaches.", "AI": {"tldr": "\u7814\u7a76\u4e86\u81ea\u56de\u5f52LLMs\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u751f\u6210\u6b65\u9aa4\u4e2d\u503e\u5411\u4e8e\u6291\u5236\u591a\u4e2a\u6807\u7b7e\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u5927\u65f6\u6807\u7b7e\u6392\u540d\u6539\u5584\u3002\u63d0\u51fa\u4e86\u5206\u5e03\u5bf9\u9f50\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u591a\u6807\u7b7e\u5206\u7c7b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u89c1\uff0c\u4f46LLMs\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u4e3b\u89c2\u4efb\u52a1\u3002", "method": "\u5206\u6790LLMs\u5728\u751f\u6210\u6b65\u9aa4\u4e2d\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u7814\u7a76\u6a21\u578b\u89c4\u6a21\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5206\u5e03\u5bf9\u9f50\u4efb\u52a1\u53ca\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u89c4\u6a21\u589e\u5927\u65f6\u6807\u7b7e\u6392\u540d\u6539\u5584\uff0c\u4f46\u71b5\u964d\u4f4e\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5206\u5e03\u5bf9\u9f50\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LLMs\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u7279\u5b9a\u884c\u4e3a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6539\u8fdb\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.18051", "pdf": "https://arxiv.org/pdf/2505.18051", "abs": "https://arxiv.org/abs/2505.18051", "authors": ["Anthony Fuller", "Yousef Yassin", "Junfeng Wen", "Daniel G. Kyrollos", "Tarek Ibrahim", "James R. Green", "Evan Shelhamer"], "title": "LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Vision transformers are ever larger, more accurate, and more expensive to\ncompute. The expense is even more extreme at high resolution as the number of\ntokens grows quadratically with the image size. We turn to adaptive computation\nto cope with this cost by learning to predict where to compute. Our LookWhere\nmethod divides the computation between a low-resolution selector and a\nhigh-resolution extractor without ever processing the full high-resolution\ninput. We jointly pretrain the selector and extractor without task supervision\nby distillation from a self-supervised teacher, in effect, learning where and\nwhat to compute simultaneously. Unlike prior token reduction methods, which pay\nto save by pruning already-computed tokens, and prior token selection methods,\nwhich require complex and expensive per-task optimization, LookWhere\neconomically and accurately selects and extracts transferrable representations\nof images. We show that LookWhere excels at sparse recognition on\nhigh-resolution inputs (Traffic Signs), maintaining accuracy while reducing\nFLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks\nthat are global (ImageNet classification) or local (ADE20K segmentation),\nimproving accuracy while reducing time by 1.36x.", "AI": {"tldr": "LookWhere\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8ba1\u7b97\u51cf\u5c11Vision Transformers\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u8054\u5408\u8bad\u7ec3\u9009\u62e9\u5668\u548c\u63d0\u53d6\u5668\uff0c\u663e\u8457\u964d\u4f4eFLOPs\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u4e2d\uff0cVision Transformers\u7684\u8ba1\u7b97\u6210\u672c\u968f\u56fe\u50cf\u5c3a\u5bf8\u5e73\u65b9\u589e\u957f\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "LookWhere\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u9009\u62e9\u5668\u548c\u9ad8\u5206\u8fa8\u7387\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u84b8\u998f\u8054\u5408\u8bad\u7ec3\uff0c\u65e0\u9700\u4efb\u52a1\u76d1\u7763\u3002", "result": "\u5728\u7a00\u758f\u8bc6\u522b\u4efb\u52a1\u4e2d\u51cf\u5c11FLOPs 34\u500d\u3001\u65f6\u95f46\u500d\uff1b\u5728\u6807\u51c6\u4efb\u52a1\u4e2d\u63d0\u5347\u7cbe\u5ea6\u5e76\u51cf\u5c11\u65f6\u95f41.36\u500d\u3002", "conclusion": "LookWhere\u7ecf\u6d4e\u9ad8\u6548\u5730\u9009\u62e9\u5e76\u63d0\u53d6\u56fe\u50cf\u8868\u793a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bc6\u522b\u4efb\u52a1\u3002", "relevance": 70.0}}
{"id": "2505.17241", "pdf": "https://arxiv.org/pdf/2505.17241", "abs": "https://arxiv.org/abs/2505.17241", "authors": ["Niklas Holzner", "Sebastian Maier", "Stefan Feuerriegel"], "title": "Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis", "categories": ["cs.HC", "cs.AI"], "comment": "22 pages, 6 figures. Code and data are available at\n  https://github.com/SM2982/Meta-Analysis-LLMs-Creativity.git", "summary": "Generative artificial intelligence (GenAI) is increasingly used to support a\nwide range of human tasks, yet empirical evidence on its effect on creativity\nremains scattered. Can GenAI generate ideas that are creative? To what extent\ncan it support humans in generating ideas that are both creative and diverse?\nIn this study, we conduct a meta-analysis to evaluate the effect of GenAI on\nthe performance in creative tasks. For this, we first perform a systematic\nliterature search, based on which we identify n = 28 relevant studies (m = 8214\nparticipants) for inclusion in our meta-analysis. We then compute standardized\neffect sizes based on Hedges' g. We compare different outcomes: (i) how\ncreative GenAI is; (ii) how creative humans augmented by GenAI are; and (iii)\nthe diversity of ideas by humans augmented by GenAI. Our results show no\nsignificant difference in creative performance between GenAI and humans (g =\n-0.05), while humans collaborating with GenAI significantly outperform those\nworking without assistance (g = 0.27). However, GenAI has a significant\nnegative effect on the diversity of ideas for such collaborations between\nhumans and GenAI (g = -0.86). We further analyze heterogeneity across different\nGenAI models (e.g., GPT-3.5, GPT-4), different tasks (e.g., creative writing,\nideation, divergent thinking), and different participant populations (e.g.,\nlaypeople, business, academia). Overall, our results position GenAI as an\naugmentative tool that can support, rather than replace, human\ncreativity-particularly in tasks benefiting from ideation support.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5143\u5206\u6790\u8bc4\u4f30\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5bf9\u521b\u610f\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0GenAI\u4e0e\u4eba\u7c7b\u521b\u610f\u8868\u73b0\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u4eba\u7c7b\u4e0eGenAI\u534f\u4f5c\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u5de5\u4f5c\uff0c\u5c3d\u7ba1GenAI\u4f1a\u964d\u4f4e\u521b\u610f\u591a\u6837\u6027\u3002", "motivation": "\u63a2\u8ba8GenAI\u5728\u521b\u610f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u4eba\u7c7b\u521b\u610f\u652f\u6301\u7684\u6548\u679c\uff0c\u586b\u8865\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u6587\u732e\u641c\u7d22\u540e\u7eb3\u516528\u9879\u7814\u7a76\uff088214\u540d\u53c2\u4e0e\u8005\uff09\uff0c\u8ba1\u7b97Hedges' g\u6548\u5e94\u91cf\uff0c\u6bd4\u8f83GenAI\u521b\u610f\u6027\u3001\u4eba\u7c7b\u4e0eGenAI\u534f\u4f5c\u7684\u521b\u610f\u6027\u53ca\u521b\u610f\u591a\u6837\u6027\u3002", "result": "GenAI\u4e0e\u4eba\u7c7b\u521b\u610f\u65e0\u663e\u8457\u5dee\u5f02\uff08g=-0.05\uff09\uff0c\u4eba\u7c7b\u4e0eGenAI\u534f\u4f5c\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u5de5\u4f5c\uff08g=0.27\uff09\uff0c\u4f46GenAI\u964d\u4f4e\u521b\u610f\u591a\u6837\u6027\uff08g=-0.86\uff09\u3002", "conclusion": "GenAI\u662f\u652f\u6301\u800c\u975e\u66ff\u4ee3\u4eba\u7c7b\u521b\u610f\u7684\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u521b\u610f\u6784\u601d\u7684\u4efb\u52a1\u3002", "relevance": 40.0}}
{"id": "2505.17847", "pdf": "https://arxiv.org/pdf/2505.17847", "abs": "https://arxiv.org/abs/2505.17847", "authors": ["Hao Wang", "Licheng Pan", "Zhichao Chen", "Xu Chen", "Qingyang Dai", "Lei Wang", "Haoxuan Li", "Zhouchen Lin"], "title": "TransDF: Time-Series Forecasting Needs Transformed Label Alignment", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Training time-series forecasting models presents unique challenges in\ndesigning effective learning objectives. Existing methods predominantly utilize\nthe temporal mean squared error, which faces two critical challenges: (1) label\nautocorrelation, which leads to bias from the label sequence likelihood; (2)\nexcessive amount of tasks, which increases with the forecast horizon and\ncomplicates optimization. To address these challenges, we propose\nTransform-enhanced Direct Forecast (TransDF), which transforms the label\nsequence into decorrelated components with discriminated significance. Models\nare trained to align the most significant components, thereby effectively\nmitigating label autocorrelation and reducing task amount. Extensive\nexperiments demonstrate that TransDF achieves state-of-the-art performance and\nis compatible with various forecasting models. Code is available at\nhttps://anonymous.4open.science/r/TransDF-88CF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTransDF\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6807\u7b7e\u5e8f\u5217\u8f6c\u6362\u4e3a\u53bb\u76f8\u5173\u4e14\u91cd\u8981\u6027\u5206\u91cf\u7684\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u6807\u7b7e\u81ea\u76f8\u5173\u548c\u4efb\u52a1\u91cf\u8fc7\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u65f6\u95f4\u5747\u65b9\u8bef\u5dee\uff0c\u4f46\u9762\u4e34\u6807\u7b7e\u81ea\u76f8\u5173\u548c\u4efb\u52a1\u91cf\u8fc7\u5927\u7684\u6311\u6218\u3002", "method": "TransDF\u5c06\u6807\u7b7e\u5e8f\u5217\u8f6c\u6362\u4e3a\u53bb\u76f8\u5173\u4e14\u91cd\u8981\u6027\u5206\u91cf\u7684\u7ec4\u4ef6\uff0c\u8bad\u7ec3\u6a21\u578b\u5bf9\u9f50\u6700\u91cd\u8981\u7684\u7ec4\u4ef6\u3002", "result": "TransDF\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5bb9\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "TransDF\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u81ea\u76f8\u5173\u548c\u4efb\u52a1\u91cf\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "relevance": 50.0}}
{"id": "2505.17536", "pdf": "https://arxiv.org/pdf/2505.17536", "abs": "https://arxiv.org/abs/2505.17536", "authors": ["Kent K. Chang", "Mackenzie Hanh Cramer", "Anna Ho", "Ti Ti Nguyen", "Yilin Yuan", "David Bamman"], "title": "Multimodal Conversation Structure Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u9488\u5bf9\u591a\u6a21\u6001\u3001\u591a\u89d2\u8272\u5bf9\u8bdd\u7ed3\u6784\u7684\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u5bf9\u8bdd\u7ed3\u6784\u7406\u89e3\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u5bf9\u8bdd\u7ed3\u6784\u7406\u89e3\uff08\u5982\u89d2\u8272\u5f52\u5c5e\u548c\u5bf9\u8bdd\u7ebf\u7a0b\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u3001\u591a\u89d2\u8272\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u89d2\u8272\u5f52\u5c5e\uff08\u8bf4\u8bdd\u8005\u3001\u88ab\u63d0\u53ca\u8005\u3001\u65c1\u542c\u8005\uff09\u548c\u5bf9\u8bdd\u7ebf\u7a0b\uff08\u8bdd\u8bed\u94fe\u63a5\u4e0e\u805a\u7c7b\uff09\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u6d41\u884c\u7684\u591a\u6a21\u6001LLMs\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u97f3\u9891-\u89c6\u89c9LLM\u5728\u89d2\u8272\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u53c2\u4e0e\u8005\u533f\u540d\u5316\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u5bf9\u8bdd\u53c2\u4e0e\u8005\u6570\u91cf\u548c\u58f0\u5b66\u6e05\u6670\u5ea6\u662f\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u591a\u6a21\u6001LLMs\u5728\u5bf9\u8bdd\u7ed3\u6784\u63a8\u7406\u65b9\u9762\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "relevance": 85.0}}
{"id": "2505.18052", "pdf": "https://arxiv.org/pdf/2505.18052", "abs": "https://arxiv.org/abs/2505.18052", "authors": ["Zhihua Liu", "Lei Tong", "Xilin He", "Che Liu", "Rossella Arcucci", "Chen Jin", "Huiyu Zhou"], "title": "BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching", "categories": ["cs.CV"], "comment": null, "summary": "Existed echocardiography segmentation methods often suffer from anatomical\ninconsistency challenge caused by shape variation, partial observation and\nregion ambiguity with similar intensity across 2D echocardiographic sequences,\nresulting in false positive segmentation with anatomical defeated structures in\nchallenging low signal-to-noise ratio conditions. To provide a strong\nanatomical guarantee across different echocardiographic frames, we propose a\nnovel segmentation framework named BOTM (Bi-directional Optimal Token Matching)\nthat performs echocardiography segmentation and optimal anatomy transportation\nsimultaneously. Given paired echocardiographic images, BOTM learns to match two\nsets of discrete image tokens by finding optimal correspondences from a novel\nanatomical transportation perspective. We further extend the token matching\ninto a bi-directional cross-transport attention proxy to regulate the preserved\nanatomical consistency within the cardiac cyclic deformation in temporal\ndomain. Extensive experimental results show that BOTM can generate stable and\naccurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on\nTED), and provide a better matching interpretation with anatomical consistency\nguarantee.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOTM\u7684\u65b0\u578b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u6700\u4f18\u4ee4\u724c\u5339\u914d\u89e3\u51b3\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u4e2d\u7684\u89e3\u5256\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u65b9\u6cd5\u56e0\u5f62\u72b6\u53d8\u5316\u3001\u90e8\u5206\u89c2\u5bdf\u548c\u533a\u57df\u6a21\u7cca\u7b49\u95ee\u9898\u5bfc\u81f4\u89e3\u5256\u4e0d\u4e00\u81f4\u6027\uff0cBOTM\u65e8\u5728\u63d0\u4f9b\u8de8\u5e27\u7684\u89e3\u5256\u4e00\u81f4\u6027\u4fdd\u8bc1\u3002", "method": "BOTM\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u4f20\u8f93\u6ce8\u610f\u529b\u4ee3\u7406\u5339\u914d\u79bb\u6563\u56fe\u50cf\u4ee4\u724c\uff0c\u4ece\u89e3\u5256\u8fd0\u8f93\u89d2\u5ea6\u4f18\u5316\u5206\u5272\u548c\u89e3\u5256\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBOTM\u80fd\u751f\u6210\u7a33\u5b9a\u51c6\u786e\u7684\u5206\u5272\u7ed3\u679c\uff08\u5982CAMUS2H LV\u7684HD\u964d\u4f4e1.917\uff0cTED\u7684Dice\u63d0\u53471.9%\uff09\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u89e3\u5256\u4e00\u81f4\u6027\u89e3\u91ca\u3002", "conclusion": "BOTM\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u89e3\u5256\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "relevance": 30.0}}
{"id": "2505.17852", "pdf": "https://arxiv.org/pdf/2505.17852", "abs": "https://arxiv.org/abs/2505.17852", "authors": ["Francois Chaubard", "Mykel Kochenderfer"], "title": "Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "During inference, Recurrent Neural Networks (RNNs) scale constant in both\nFLOPs and GPU memory with increasing context length, as they compress all prior\ntokens into a fixed-size memory. In contrast, transformers scale linearly in\nFLOPs and, at best, linearly in memory during generation, since they must\nattend to all previous tokens explicitly. Despite this inference-time\nadvantage, training large RNNs on long contexts remains impractical because\nstandard optimization methods depend on Backpropagation Through Time (BPTT).\nBPTT requires retention of all intermediate activations during the forward\npass, causing memory usage to scale linearly with both context length and model\nsize. In this paper, we show that Zero-Order Optimization (ZOO) methods such as\nRandom-vector Gradient Estimation (RGE) can successfully replace BPTT to train\nRNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while\nusing orders of magnitude less memory and cost, as the model remains in\ninference mode throughout training. We further demonstrate that\nCentral-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate\nloss, inherently regularizing training and improving generalization. Our method\nmatches or outperforms BPTT across three settings: (1) overfitting, (2)\ntransduction, and (3) language modeling. Across all tasks, with sufficient\nperturbations, our models generalize as well as or better than those trained\nwith BPTT, often in fewer steps. Despite the need for more forward passes per\nstep, we can surpass BPTT wall-clock time per step using recent advancements\nsuch as FlashRNN and distributed inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff08\u5982RGE\uff09\u66ff\u4ee3BPTT\u8bad\u7ec3RNN\u7684\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8aBPTT\u7684\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3RNN\u5728\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u56e0BPTT\u5bfc\u81f4\u7684\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff08\u5982RGE\u548cCD-RGE\uff09\u66ff\u4ee3BPTT\uff0c\u907f\u514d\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b\uff0c\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u8fc7\u62df\u5408\u3001\u8f6c\u6362\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8aBPTT\u6027\u80fd\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u4e3aRNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u3002", "relevance": 75.0}}
{"id": "2505.17537", "pdf": "https://arxiv.org/pdf/2505.17537", "abs": "https://arxiv.org/abs/2505.17537", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Xueqi Cheng"], "title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often fail to recognize their knowledge\nboundaries, producing confident yet incorrect answers. In this paper, we\ninvestigate how knowledge popularity affects LLMs' ability to perceive their\nknowledge boundaries. Focusing on entity-centric factual question answering\n(QA), we quantify knowledge popularity from three perspectives: the popularity\nof entities in the question, the popularity of entities in the answer, and\nrelation popularity, defined as their co-occurrence frequency. Experiments on\nthree representative datasets containing knowledge with varying popularity show\nthat LLMs exhibit better QA performance, higher confidence, and more accurate\nperception on more popular knowledge, with relation popularity having the\nstrongest correlation. Cause knowledge popularity shows strong correlation with\nLLMs' QA performance, we propose to leverage these signals for confidence\ncalibration. This improves the accuracy of answer correctness prediction by an\naverage of 5.24% across all models and datasets. Furthermore, we explore\nprompting LLMs to estimate popularity without external corpora, which yields a\nviable alternative.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u77e5\u8bc6\u6d41\u884c\u5ea6\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc6\u522b\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6d41\u884c\u5ea6\u4e0e\u6a21\u578b\u8868\u73b0\u3001\u4fe1\u5fc3\u53ca\u8fb9\u754c\u611f\u77e5\u6b63\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u4fe1\u5fc3\u6821\u51c6\u65b9\u6cd5\u3002", "motivation": "LLMs\u5e38\u56e0\u65e0\u6cd5\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\u800c\u751f\u6210\u9519\u8bef\u4f46\u81ea\u4fe1\u7684\u7b54\u6848\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u77e5\u8bc6\u6d41\u884c\u5ea6\u5bf9LLMs\u8fb9\u754c\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u4f53\u548c\u5173\u7cfb\u6d41\u884c\u5ea6\u91cf\u5316\u77e5\u8bc6\u6d41\u884c\u5ea6\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5LLMs\u7684\u8868\u73b0\u3001\u4fe1\u5fc3\u53ca\u8fb9\u754c\u611f\u77e5\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u4fe1\u5fc3\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u6d41\u884c\u5ea6\u4e0eLLMs\u8868\u73b0\u6b63\u76f8\u5173\uff0c\u5173\u7cfb\u6d41\u884c\u5ea6\u5f71\u54cd\u6700\u5927\uff1b\u4fe1\u5fc3\u6821\u51c6\u65b9\u6cd5\u5e73\u5747\u63d0\u53475.24%\u7684\u7b54\u6848\u6b63\u786e\u6027\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u77e5\u8bc6\u6d41\u884c\u5ea6\u662f\u5f71\u54cdLLMs\u8fb9\u754c\u611f\u77e5\u7684\u5173\u952e\u56e0\u7d20\uff0c\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u6821\u51c6\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "relevance": 85.0}}
{"id": "2505.18053", "pdf": "https://arxiv.org/pdf/2505.18053", "abs": "https://arxiv.org/abs/2505.18053", "authors": ["Zherui Zhang", "Jiaxin Wu", "Changwei Wang", "Rongtao Xu", "Longzhao Huang", "Wenhao Xu", "Wenbo Xu", "Li Guo", "Shibiao Xu"], "title": "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Prompt learning as a parameter-efficient method that has been widely adopted\nto adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt\ndesign requires domain expertise and iterative optimization, soft-prompt\nmethods rely heavily on task-specific hard labels, limiting their\ngeneralization to unseen categories. Recent popular distillation-based prompt\nlearning methods improve generalization by exploiting larger teacher VLMs and\nunsupervised knowledge transfer, yet their repetitive teacher model online\ninference sacrifices the inherent training efficiency advantage of prompt\nlearning. In this paper, we propose {{\\large {\\textbf{F}}}}aster {{\\large\n{\\textbf{D}}}}istillation-{{\\large {\\textbf{B}}}}ased {{\\large\n{\\textbf{P}}}}rompt {{\\large {\\textbf{L}}}}earning (\\textbf{FDBPL}), which\naddresses these issues by sharing soft supervision contexts across multiple\ntraining stages and implementing accelerated I/O. Furthermore, FDBPL introduces\na region-aware prompt learning paradigm with dual positive-negative prompt\nspaces to fully exploit randomly cropped regions that containing multi-level\ninformation. We propose a positive-negative space mutual learning mechanism\nbased on similarity-difference learning, enabling student CLIP models to\nrecognize correct semantics while learning to reject weakly related concepts,\nthereby improving zero-shot performance. Unlike existing distillation-based\nprompt learning methods that sacrifice parameter efficiency for generalization,\nFDBPL maintains dual advantages of parameter efficiency and strong downstream\ngeneralization. Comprehensive evaluations across 11 datasets demonstrate\nsuperior performance in base-to-new generalization, cross-dataset transfer, and\nrobustness tests, achieving $2.2\\times$ faster training speed.", "AI": {"tldr": "FDBPL\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u8f6f\u76d1\u7763\u4e0a\u4e0b\u6587\u548c\u52a0\u901fI/O\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u6548\u7387\u4e0a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f15\u5165\u533a\u57df\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u548c\u6b63\u8d1f\u7a7a\u95f4\u4e92\u5b66\u673a\u5236\uff0c\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u6548\u7387\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u84b8\u998f\u65b9\u6cd5\u727a\u7272\u4e86\u53c2\u6570\u6548\u7387\u3002FDBPL\u65e8\u5728\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u9ad8\u6548\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "method": "FDBPL\u901a\u8fc7\u5171\u4eab\u8f6f\u76d1\u7763\u4e0a\u4e0b\u6587\u3001\u52a0\u901fI/O\u3001\u533a\u57df\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u548c\u6b63\u8d1f\u7a7a\u95f4\u4e92\u5b66\u673a\u5236\uff0c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u548c\u96f6\u6837\u672c\u6027\u80fd\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFDBPL\u5728\u6cdb\u5316\u3001\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.2\u500d\u3002", "conclusion": "FDBPL\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "relevance": 75.0}}
{"id": "2505.17854", "pdf": "https://arxiv.org/pdf/2505.17854", "abs": "https://arxiv.org/abs/2505.17854", "authors": ["Lukas Koller", "Tobias Ladner", "Matthias Althoff"], "title": "Out of the Shadows: Exploring a Latent Space for Neural Network Verification", "categories": ["cs.LG"], "comment": null, "summary": "Neural networks are ubiquitous. However, they are often sensitive to small\ninput changes. Hence, to prevent unexpected behavior in safety-critical\napplications, their formal verification -- a notoriously hard problem -- is\nnecessary. Many state-of-the-art verification algorithms use reachability\nanalysis or abstract interpretation to enclose the set of possible outputs of a\nneural network. Often, the verification is inconclusive due to the conservatism\nof the enclosure. To address this problem, we design a novel latent space for\nformal verification that enables the transfer of output specifications to the\ninput space for an iterative specification-driven input refinement, i.e., we\niteratively reduce the set of possible inputs to only enclose the unsafe ones.\nThe latent space is constructed from a novel view of projection-based set\nrepresentations, e.g., zonotopes, which are commonly used in reachability\nanalysis of neural networks. A projection-based set representation is a\n\"shadow\" of a higher-dimensional set -- a latent space -- that does not change\nduring a set propagation through a neural network. Hence, the input set and the\noutput enclosure are \"shadows\" of the same latent space that we can use to\ntransfer constraints. We present an efficient verification tool for neural\nnetworks that uses our iterative refinement to significantly reduce the number\nof subproblems in a branch-and-bound procedure. Using zonotopes as a set\nrepresentation, unlike many other state-of-the-art approaches, our approach can\nbe realized by only using matrix operations, which enables a significant\nspeed-up through efficient GPU acceleration. We demonstrate that our tool\nachieves competitive performance, which would place it among the top-ranking\ntools of the last neural network verification competition (VNN-COMP'24).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u8f93\u51fa\u89c4\u8303\u5230\u8f93\u5165\u7a7a\u95f4\u7684\u4f20\u9012\uff0c\u4ece\u800c\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\u96c6\u4ee5\u51cf\u5c11\u4e0d\u5b89\u5168\u8f93\u5165\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6295\u5f71\u57fa\u96c6\u5408\u8868\u793a\uff08\u5982zonotopes\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5206\u652f\u5b9a\u754c\u8fc7\u7a0b\u4e2d\u7684\u5b50\u95ee\u9898\u6570\u91cf\uff0c\u5e76\u901a\u8fc7GPU\u52a0\u901f\u5b9e\u73b0\u9ad8\u6548\u9a8c\u8bc1\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u53ef\u80fd\u56e0\u8f93\u5165\u5fae\u5c0f\u53d8\u5316\u800c\u4ea7\u751f\u610f\u5916\u884c\u4e3a\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u662f\u5fc5\u8981\u7684\u3002\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u5e38\u56e0\u4fdd\u5b88\u6027\u5bfc\u81f4\u7ed3\u679c\u4e0d\u786e\u5b9a\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u6f5c\u5728\u7a7a\u95f4\uff0c\u5c06\u8f93\u51fa\u89c4\u8303\u4f20\u9012\u5230\u8f93\u5165\u7a7a\u95f4\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff1b\u5229\u7528\u6295\u5f71\u57fa\u96c6\u5408\u8868\u793a\uff08\u5982zonotopes\uff09\uff0c\u901a\u8fc7\u77e9\u9635\u64cd\u4f5c\u548cGPU\u52a0\u901f\u5b9e\u73b0\u9ad8\u6548\u9a8c\u8bc1\u3002", "result": "\u9a8c\u8bc1\u5de5\u5177\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u53ef\u8dfb\u8eabVNN-COMP'24\u9876\u7ea7\u5de5\u5177\u4e4b\u5217\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u548c\u8fed\u4ee3\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "relevance": 70.0}}
{"id": "2505.17538", "pdf": "https://arxiv.org/pdf/2505.17538", "abs": "https://arxiv.org/abs/2505.17538", "authors": ["Leonora Vesterbacka", "Faton Rekathati", "Robin Kurtz", "Justyna Sikora", "Agnes Toftg\u00e5rd"], "title": "Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "This work presents a suite of fine-tuned Whisper models for Swedish, trained\non a dataset of unprecedented size and variability for this mid-resourced\nlanguage. As languages of smaller sizes are often underrepresented in\nmultilingual training datasets, substantial improvements in performance can be\nachieved by fine-tuning existing multilingual models, as shown in this work.\nThis work reports an overall improvement across model sizes compared to\nOpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47%\nreduction in WER comparing our best performing model to OpenAI's\nwhisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u745e\u5178\u8bed\u5fae\u8c03\u4e86Whisper\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u4e2d\u7b49\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u79cd\u5728\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u745e\u5178\u8bed\u6570\u636e\u96c6\u5bf9Whisper\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u76f8\u6bd4OpenAI\u7684Whisper\u6a21\u578b\uff0c\u5e73\u5747WER\u964d\u4f4e\u4e8647%\u3002", "conclusion": "\u5fae\u8c03\u591a\u8bed\u8a00\u6a21\u578b\u5bf9\u5c0f\u8bed\u79cd\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "relevance": 40.0}}
{"id": "2505.18060", "pdf": "https://arxiv.org/pdf/2505.18060", "abs": "https://arxiv.org/abs/2505.18060", "authors": ["Kaiyan Zhang", "Xinghui Li", "Jingyi Lu", "Kai Han"], "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline", "categories": ["cs.CV"], "comment": null, "summary": "Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u8bed\u4e49\u5bf9\u5e94\u4efb\u52a1\u7684\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u65b9\u6cd5\u5e76\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\uff0c\u603b\u7ed3\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u8bed\u4e49\u5bf9\u5e94\u4efb\u52a1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5e26\u6765\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u7efc\u8ff0\u548c\u5206\u6790\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u8be6\u7ec6\u5206\u6790\u6bcf\u79cd\u65b9\u6cd5\uff1b\u6c47\u603b\u6587\u732e\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff1b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u65b9\u6cd5\u7684\u7ec4\u4ef6\u6709\u6548\u6027\uff1b\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u57fa\u7ebf\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u53c2\u8003\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "relevance": 30.0}}
{"id": "2505.17856", "pdf": "https://arxiv.org/pdf/2505.17856", "abs": "https://arxiv.org/abs/2505.17856", "authors": ["Moule Lin", "Shuhao Guan", "Weipeng Jing", "Goetz Botterweck", "Andrea Patane"], "title": "Stochastic Weight Sharing for Bayesian Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While offering a principled framework for uncertainty quantification in deep\nlearning, the employment of Bayesian Neural Networks (BNNs) is still\nconstrained by their increased computational requirements and the convergence\ndifficulties when training very deep, state-of-the-art architectures. In this\nwork, we reinterpret weight-sharing quantization techniques from a stochastic\nperspective in the context of training and inference with Bayesian Neural\nNetworks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,\nWasserstein distance estimations, and alpha blending to encode the stochastic\nbehaviour of a BNN in a lower dimensional, soft Gaussian representation.\nThrough extensive empirical investigation, we demonstrate that our approach\nsignificantly reduces the computational overhead inherent in Bayesian learning\nby several orders of magnitude, enabling the efficient Bayesian training of\nlarge-scale models, such as ResNet-101 and Vision Transformer (VIT). On various\ncomputer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our\napproach compresses model parameters by approximately 50x and reduces model\nsize by 75, while achieving accuracy and uncertainty estimations comparable to\nthe state-of-the-art.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff08BNNs\uff09\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc72D\u81ea\u9002\u5e94\u9ad8\u65af\u5206\u5e03\u3001Wasserstein\u8ddd\u79bb\u4f30\u8ba1\u548calpha\u6df7\u5408\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u8d1d\u53f6\u65af\u8bad\u7ec3\u3002", "motivation": "\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff08BNNs\uff09\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u8ba1\u7b97\u5f00\u9500\u548c\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\u7684\u6536\u655b\u95ee\u9898\u9650\u5236\u4e86\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u75282D\u81ea\u9002\u5e94\u9ad8\u65af\u5206\u5e03\u3001Wasserstein\u8ddd\u79bb\u4f30\u8ba1\u548calpha\u6df7\u5408\u6280\u672f\uff0c\u5c06BNN\u7684\u968f\u673a\u884c\u4e3a\u7f16\u7801\u4e3a\u4f4e\u7ef4\u9ad8\u65af\u8868\u793a\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5728CIFAR10\u3001CIFAR100\u548cImageNet1k\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u53c2\u6570\u538b\u7f29\u7ea650\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1175\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17558", "pdf": "https://arxiv.org/pdf/2505.17558", "abs": "https://arxiv.org/abs/2505.17558", "authors": ["Shrey Pandit", "Ashwin Vinod", "Liu Leqi", "Ying Ding"], "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": "Code and dataset are available at https://teachingwithlies.github.io/", "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684DPO\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u5e7b\u89c9\u6837\u672c\u4f5c\u4e3a\u8d1f\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5e7b\u89c9\u68c0\u6d4b\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u5e7b\u89c9\u6837\u672c\u6539\u8fdb\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u4ece\u6613\u5230\u96be\u9010\u6b65\u8bad\u7ec3\uff0c\u7ed3\u5408\u72ec\u7acb\u4e8b\u5b9e\u6838\u67e5\u6a21\u578b\u7b5b\u9009\u6837\u672c\u3002", "result": "HaluCheck\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe24%\uff0c\u4e14\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "\u8bfe\u7a0bDPO\u65b9\u6cd5\u7ed3\u5408\u9ad8\u8d28\u91cf\u8d1f\u4f8b\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.18078", "pdf": "https://arxiv.org/pdf/2505.18078", "abs": "https://arxiv.org/abs/2505.18078", "authors": ["Junhao Chen", "Mingjin Chen", "Jianjin Xu", "Xiang Li", "Junting Dong", "Mingze Sun", "Puhua Jiang", "Hongxiang Li", "Yuhang Yang", "Hao Zhao", "Xiaoxiao Long", "Ruqi Huang"], "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation", "categories": ["cs.CV"], "comment": "Our video demos and code are available at https://DanceTog.github.io/", "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.", "AI": {"tldr": "DanceTogether\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u89d2\u8272\u4ea4\u4e92\u7684\u957f\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u6f02\u79fb\u548c\u5916\u89c2\u6e17\u6f0f\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u5728\u591a\u89d2\u8272\u4ea4\u4e92\u548c\u566a\u58f0\u63a7\u5236\u4fe1\u53f7\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0cDanceTogether\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528MaskPoseAdapter\u7ed3\u5408\u8ddf\u8e2a\u63a9\u7801\u548c\u59ff\u52bf\u70ed\u56fe\uff0c\u786e\u4fdd\u8eab\u4efd\u548c\u52a8\u4f5c\u7684\u7ed1\u5b9a\u3002\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u4e8e\u65b0\u6570\u636e\u96c6PairFS-4K\u3001HumanRob-300\u548cTogetherVideoBench\u3002", "result": "\u5728TogetherVideoBench\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7\u5fae\u8c03\u5feb\u901f\u9002\u5e94\u4eba\u673a\u4ea4\u4e92\u4efb\u52a1\u3002", "conclusion": "DanceTogether\u5c06\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4ece\u5355\u89d2\u8272\u6269\u5c55\u5230\u591a\u89d2\u8272\u4ea4\u4e92\uff0c\u4e3a\u6570\u5b57\u5236\u4f5c\u548c\u4eff\u771f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 40.0}}
{"id": "2505.17859", "pdf": "https://arxiv.org/pdf/2505.17859", "abs": "https://arxiv.org/abs/2505.17859", "authors": ["Masahiro Fujisawa", "Masaki Adachi", "Michael A. Osborne"], "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "38 pages, 7 figures", "summary": "Despite the importance of aligning language models with human preferences,\ncrowd-sourced human feedback is often noisy -- for example, preferring less\ndesirable responses -- posing a fundamental challenge to alignment. A truly\nrobust alignment objective should yield identical model parameters even under\nsevere label noise, a property known as redescending. We prove that no existing\nalignment methods satisfy this property. To address this, we propose\nH\\\"older-DPO, the first principled alignment loss with a provable redescending\nproperty, enabling estimation of the clean data distribution from noisy\nfeedback. The aligned model estimates the likelihood of clean data, providing a\ntheoretically grounded metric for dataset valuation that identifies the\nlocation and fraction of mislabels. This metric is gradient-free, enabling\nscalable and automated human feedback valuation without costly manual\nverification or clean validation dataset. H\\\"older-DPO achieves\nstate-of-the-art robust alignment performance while accurately detecting\nmislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used\nalignment datasets, revealing substantial noise levels and demonstrating that\nremoving these mislabels significantly improves alignment performance across\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faH\\\"older-DPO\uff0c\u4e00\u79cd\u5177\u6709\u53ef\u8bc1\u660e\u6297\u566a\u58f0\u7279\u6027\u7684\u5bf9\u9f50\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u4ece\u5608\u6742\u7684\u4eba\u7c7b\u53cd\u9988\u4e2d\u4f30\u8ba1\u5e72\u51c0\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u5728\u4e25\u91cd\u6807\u7b7e\u566a\u58f0\u4e0b\u4fdd\u6301\u6a21\u578b\u53c2\u6570\u4e00\u81f4\u6027\uff08redescending\u6027\u8d28\uff09\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u76ee\u6807\u3002", "method": "\u63d0\u51faH\\\"older-DPO\uff0c\u4e00\u79cd\u7406\u8bba\u652f\u6301\u7684\u5bf9\u9f50\u635f\u5931\u51fd\u6570\uff0c\u80fd\u4f30\u8ba1\u5e72\u51c0\u6570\u636e\u5206\u5e03\u5e76\u68c0\u6d4b\u9519\u8bef\u6807\u7b7e\u3002", "result": "H\\\"older-DPO\u5728\u9c81\u68d2\u5bf9\u9f50\u6027\u80fd\u4e0a\u8fbe\u5230SOTA\uff0c\u5e76\u80fd\u51c6\u786e\u68c0\u6d4b\u9519\u8bef\u6807\u7b7e\uff0c\u79fb\u9664\u8fd9\u4e9b\u6807\u7b7e\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "H\\\"older-DPO\u4e3a\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u566a\u58f0\u53cd\u9988\u5904\u7406\u3002", "relevance": 90.0}}
{"id": "2505.17565", "pdf": "https://arxiv.org/pdf/2505.17565", "abs": "https://arxiv.org/abs/2505.17565", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "title": "PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models", "categories": ["cs.CL"], "comment": null, "summary": "Improving large language models (LLMs) with self-generated data has\ndemonstrated success in tasks such as mathematical reasoning and code\ngeneration. Yet, no exploration has been made on table question answering\n(TQA), where a system answers questions based on tabular data. Addressing this\ngap is crucial for TQA, as effective self-improvement can boost performance\nwithout requiring costly or manually annotated data. In this work, we propose\nPPT, a Process-based Preference learning framework for TQA. It decomposes\nreasoning chains into discrete states, assigns scores to each state, and\nsamples contrastive steps for preference learning. Experimental results show\nthat PPT effectively improves TQA models by up to 5% on in-domain datasets and\n2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,\nthe resulting models achieve competitive results compared to more complex and\nlarger state-of-the-art TQA systems, while being five times more efficient\nduring inference.", "AI": {"tldr": "PPT\u6846\u67b6\u901a\u8fc7\u81ea\u751f\u6210\u6570\u636e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u5347\u6027\u80fd\u4e14\u9ad8\u6548\u3002", "motivation": "\u63a2\u7d22\u81ea\u751f\u6210\u6570\u636e\u5728\u8868\u683c\u95ee\u7b54\uff08TQA\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u4f4e\u6210\u672c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faPPT\u6846\u67b6\uff0c\u5206\u89e3\u63a8\u7406\u94fe\u4e3a\u79bb\u6563\u72b6\u6001\uff0c\u8bc4\u5206\u5e76\u91c7\u6837\u5bf9\u6bd4\u6b65\u9aa4\u8fdb\u884c\u504f\u597d\u5b66\u4e60\u3002", "result": "PPT\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53475%\u548c2.4%\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad85\u500d\u3002", "conclusion": "PPT\u6709\u6548\u4e14\u9ad8\u6548\u5730\u6539\u8fdb\u4e86TQA\u4efb\u52a1\uff0c\u6027\u80fd\u63a5\u8fd1\u66f4\u590d\u6742\u7684SOTA\u6a21\u578b\u3002", "relevance": 85.0}}
{"id": "2505.18079", "pdf": "https://arxiv.org/pdf/2505.18079", "abs": "https://arxiv.org/abs/2505.18079", "authors": ["Xiaoyi Zhang", "Zhaoyang Jia", "Zongyu Guo", "Jiahao Li", "Bin Li", "Houqiang Li", "Yan Lu"], "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeep Video Discovery\uff08DVD\uff09\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u4e3b\u641c\u7d22\u7b56\u7565\u5904\u7406\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u56e0\u65f6\u7a7a\u590d\u6742\u6027\u9ad8\u4e14\u95ee\u7b54\u96be\u5ea6\u5927\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709LLM\u5728\u5904\u7406\u4fe1\u606f\u5bc6\u96c6\u7684\u957f\u89c6\u9891\u65f6\u4ecd\u6709\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u4e3b\u4ee3\u7406\uff08DVD\uff09\uff0c\u5229\u7528\u591a\u7c92\u5ea6\u89c6\u9891\u6570\u636e\u5e93\u4e2d\u7684\u641c\u7d22\u5de5\u5177\uff0c\u901a\u8fc7LLM\u89c4\u5212\u3001\u5de5\u5177\u9009\u62e9\u548c\u53c2\u6570\u4f18\u5316\u8fed\u4ee3\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5728LVBench\u7b49\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "DVD\u4ee3\u7406\u901a\u8fc7\u81ea\u4e3b\u641c\u7d22\u7b56\u7565\u548cLLM\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 70.0}}
{"id": "2505.17863", "pdf": "https://arxiv.org/pdf/2505.17863", "abs": "https://arxiv.org/abs/2505.17863", "authors": ["Nicolas Zucchet", "Francesco d'Angelo", "Andrew K. Lampinen", "Stephanie C. Y. Chan"], "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Emergence is a fascinating property of large language models and neural\nnetworks more broadly: as models scale and train for longer, they sometimes\ndevelop new abilities in sudden ways. Despite initial studies, we still lack a\ncomprehensive understanding of how and when these abilities emerge. To address\nthis gap, we study the emergence over training of sparse attention, a critical\nand frequently observed attention pattern in Transformers. By combining\ntheoretical analysis of a toy model with empirical observations on small\nTransformers trained on a linear regression variant, we uncover the mechanics\ndriving sparse attention emergence and reveal that emergence timing follows\npower laws based on task structure, architecture, and optimizer choice. We\nadditionally find that repetition can greatly speed up emergence. Finally, we\nconfirm these results on a well-studied in-context associative recall task. Our\nfindings provide a simple, theoretically grounded framework for understanding\nhow data distributions and model design influence the learning dynamics behind\none form of emergence.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u5728Transformer\u4e2d\u7684\u6d8c\u73b0\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u5176\u6d8c\u73b0\u65f6\u673a\u4e0e\u4efb\u52a1\u7ed3\u6784\u3001\u67b6\u6784\u548c\u4f18\u5316\u5668\u9009\u62e9\u76f8\u5173\u7684\u5e42\u5f8b\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u91cd\u590d\u53ef\u4ee5\u52a0\u901f\u6d8c\u73b0\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6d8c\u73b0\u73b0\u8c61\uff08\u5982\u7a00\u758f\u6ce8\u610f\u529b\uff09\u7684\u673a\u5236\uff0c\u586b\u8865\u5bf9\u6d8c\u73b0\u5982\u4f55\u53ca\u4f55\u65f6\u53d1\u751f\u7684\u7406\u89e3\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u73a9\u5177\u6a21\u578b\u7684\u7406\u8bba\u5206\u6790\u548c\u5c0f\u578bTransformer\u5728\u53d8\u4f53\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u89c2\u5bdf\u3002", "result": "\u53d1\u73b0\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6d8c\u73b0\u65f6\u673a\u9075\u5faa\u5e42\u5f8b\uff0c\u4e14\u91cd\u590d\u80fd\u52a0\u901f\u6d8c\u73b0\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u7406\u8bba\u652f\u6301\u7684\u6846\u67b6\uff0c\u7406\u89e3\u6570\u636e\u5206\u5e03\u548c\u6a21\u578b\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u6d8c\u73b0\u7684\u5b66\u4e60\u52a8\u6001\u3002", "relevance": 85.0}}
{"id": "2505.17571", "pdf": "https://arxiv.org/pdf/2505.17571", "abs": "https://arxiv.org/abs/2505.17571", "authors": ["Sichun Luo", "Guanzhi Deng", "Jian Xu", "Xiaojie Zhang", "Hanxu Hou", "Linqi Song"], "title": "Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation", "categories": ["cs.CL"], "comment": null, "summary": "Personalization is a critical task in modern intelligent systems, with\napplications spanning diverse domains, including interactions with large\nlanguage models (LLMs). Recent advances in reasoning capabilities have\nsignificantly enhanced LLMs, enabling unprecedented performance in tasks such\nas mathematics and coding. However, their potential for personalization tasks\nremains underexplored.\n  In this paper, we present the first systematic evaluation of large reasoning\nmodels (LRMs) for personalization tasks. Surprisingly, despite generating more\ntokens, LRMs do not consistently outperform general-purpose LLMs, especially in\nretrieval-intensive scenarios where their advantages diminish. Our analysis\nidentifies three key limitations: divergent thinking, misalignment of response\nformats, and ineffective use of retrieved information. To address these\nchallenges, we propose Reinforced Reasoning for Personalization (\\model), a\nnovel framework that incorporates a hierarchical reasoning thought template to\nguide LRMs in generating structured outputs. Additionally, we introduce a\nreasoning process intervention method to enforce adherence to designed\nreasoning patterns, enhancing alignment. We also propose a cross-referencing\nmechanism to ensure consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing techniques.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5e76\u672a\u663e\u8457\u4f18\u4e8e\u901a\u7528LLMs\uff0c\u5e76\u63d0\u51fa\u4e86Reinforced Reasoning for Personalization\uff08RRP\uff09\u6846\u67b6\u4ee5\u89e3\u51b3LRMs\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86RRP\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5c42\u63a8\u7406\u601d\u7ef4\u6a21\u677f\u3001\u63a8\u7406\u8fc7\u7a0b\u5e72\u9884\u65b9\u6cd5\u548c\u4ea4\u53c9\u5f15\u7528\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRRP\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "RRP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LRMs\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.18087", "pdf": "https://arxiv.org/pdf/2505.18087", "abs": "https://arxiv.org/abs/2505.18087", "authors": ["Hyungyung Lee", "Geon Choi", "Jung-Oh Lee", "Hangyul Yoon", "Hyuk Gi Hong", "Edward Choi"], "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench", "AI": {"tldr": "\u63d0\u51fa\u4e86CheXStruct\u548cCXReasonBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u8bca\u65ad\u7b54\u6848\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u662f\u5426\u8fdb\u884c\u4e34\u5e8a\u6709\u610f\u4e49\u63a8\u7406\u7684\u6df1\u5165\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eMIMIC-CXR-JPG\u6570\u636e\u96c6\uff0c\u81ea\u52a8\u751f\u6210\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff08\u5982\u89e3\u5256\u533a\u57df\u5206\u5272\u3001\u8bca\u65ad\u6d4b\u91cf\u7b49\uff09\uff0c\u5e76\u6784\u5efaCXReasonBench\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u768410\u4e2aLVLM\u5728\u7ed3\u6784\u5316\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u6a21\u578b\u5728\u4e34\u5e8a\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u62bd\u8c61\u77e5\u8bc6\u4e0e\u89c6\u89c9\u89e3\u91ca\u7684\u7ed3\u5408\u3002", "relevance": 60.0}}
{"id": "2505.17866", "pdf": "https://arxiv.org/pdf/2505.17866", "abs": "https://arxiv.org/abs/2505.17866", "authors": ["Hongshu Guo", "Zeyuan Ma", "Yining Ma", "Xinglin Zhang", "Wei-Neng Chen", "Yue-Jiao Gong"], "title": "DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Designing effective black-box optimizers is hampered by limited\nproblem-specific knowledge and manual control that spans months for almost\nevery detail. In this paper, we present DesignX, the first automated algorithm\ndesign framework that generates an effective optimizer specific to a given\nblack-box optimization problem within seconds. Rooted in the first principles,\nwe identify two key sub-tasks: 1) algorithm structure generation and 2)\nhyperparameter control. To enable systematic construction, a comprehensive\nmodular algorithmic space is first built, embracing hundreds of algorithm\ncomponents collected from decades of research. We then introduce a dual-agent\nreinforcement learning system that collaborates on structural and parametric\ndesign through a novel cooperative training objective, enabling large-scale\nmeta-training across 10k diverse instances. Remarkably, through days of\nautonomous learning, the DesignX-generated optimizers continuously surpass\nhuman-crafted optimizers by orders of magnitude, either on synthetic testbed or\non realistic optimization scenarios such as Protein-docking, AutoML and UAV\npath planning. Further in-depth analysis reveals DesignX's capability to\ndiscover non-trivial algorithm patterns beyond expert intuition, which,\nconversely, provides valuable design insights for the optimization community.\nWe provide DesignX's inference code at https://github.com/MetaEvo/DesignX.", "AI": {"tldr": "DesignX\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7b97\u6cd5\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u5728\u51e0\u79d2\u5185\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u534f\u4f5c\u8bbe\u8ba1\u7ed3\u6784\u548c\u53c2\u6570\uff0c\u6027\u80fd\u8fdc\u8d85\u4eba\u5de5\u8bbe\u8ba1\u7684\u4f18\u5316\u5668\u3002", "motivation": "\u89e3\u51b3\u9ed1\u76d2\u4f18\u5316\u4e2d\u7f3a\u4e4f\u95ee\u9898\u7279\u5b9a\u77e5\u8bc6\u548c\u624b\u52a8\u63a7\u5236\u8017\u65f6\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u6a21\u5757\u5316\u7b97\u6cd5\u7a7a\u95f4\uff0c\u5f15\u5165\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u534f\u4f5c\u8bbe\u8ba1\u7ed3\u6784\u548c\u53c2\u6570\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5143\u8bad\u7ec3\u3002", "result": "DesignX\u751f\u6210\u7684\u4f18\u5316\u5668\u5728\u5408\u6210\u6d4b\u8bd5\u548c\u5b9e\u9645\u573a\u666f\uff08\u5982\u86cb\u767d\u8d28\u5bf9\u63a5\u3001AutoML\u548c\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u4f18\u5316\u5668\u3002", "conclusion": "DesignX\u80fd\u591f\u53d1\u73b0\u8d85\u8d8a\u4e13\u5bb6\u76f4\u89c9\u7684\u975e\u5e73\u51e1\u7b97\u6cd5\u6a21\u5f0f\uff0c\u4e3a\u4f18\u5316\u793e\u533a\u63d0\u4f9b\u8bbe\u8ba1\u89c1\u89e3\u3002", "relevance": 40.0}}
{"id": "2505.17601", "pdf": "https://arxiv.org/pdf/2505.17601", "abs": "https://arxiv.org/abs/2505.17601", "authors": ["Jiawei Kong", "Hao Fang", "Xiaochen Yang", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) aligns large language models (LLMs) with human\nintent by training them on labeled task-specific data. Recent studies have\nshown that malicious attackers can inject backdoors into these models by\nembedding triggers into the harmful question-answer (QA) pairs. However,\nexisting poisoning attacks face two critical limitations: (1) they are easily\ndetected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)\nembedding harmful content can undermine the model's safety alignment, resulting\nin high attack success rates (ASR) even in the absence of triggers during\ninference, thus compromising stealthiness. To address these issues, we propose\na novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of\nassociating triggers with harmful responses, our approach overfits them to a\nfixed, benign-sounding positive reply prefix using harmless QA pairs. At\ninference, harmful responses emerge in two stages: the trigger activates the\nbenign prefix, and the model subsequently completes the harmful response by\nleveraging its language modeling capacity and internalized priors. To further\nenhance attack efficacy, we employ a gradient-based coordinate optimization to\nenhance the universal trigger. Extensive experiments demonstrate that our\nmethod can effectively jailbreak backdoor various LLMs even under the detection\nof guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and\nQwen-2.5-7B judged by GPT-4o.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u201c\u5e72\u51c0\u6570\u636e\u540e\u95e8\u653b\u51fb\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u5bb3\u95ee\u7b54\u5bf9\u5c06\u89e6\u53d1\u5668\u4e0e\u826f\u6027\u524d\u7f00\u5173\u8054\uff0c\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\uff0c\u6210\u529f\u52ab\u6301LLMs\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u6613\u88ab\u5b89\u5168\u9632\u62a4\u68c0\u6d4b\u4e14\u7834\u574f\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\uff0c\u9700\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u65e0\u5bb3QA\u5bf9\u5c06\u89e6\u53d1\u5668\u4e0e\u826f\u6027\u524d\u7f00\u5173\u8054\uff0c\u5206\u4e24\u9636\u6bb5\u751f\u6210\u6709\u5bb3\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u589e\u5f3a\u89e6\u53d1\u5668\u3002", "result": "\u5728LLaMA-3-8B\u548cQwen-2.5-7B\u4e0a\u653b\u51fb\u6210\u529f\u7387\u8fbe86.67%\u548c85%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed5\u8fc7\u9632\u62a4\uff0c\u52ab\u6301LLMs\uff0c\u7a81\u663e\u5b89\u5168\u5bf9\u9f50\u7684\u8106\u5f31\u6027\u3002", "relevance": 85.0}}
{"id": "2505.18096", "pdf": "https://arxiv.org/pdf/2505.18096", "abs": "https://arxiv.org/abs/2505.18096", "authors": ["Ziqiao Peng", "Yanbo Fan", "Haoyu Wu", "Xuan Wang", "Hongyan Liu", "Jun He", "Zhaoxin Fan"], "title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted by CVPR 2025", "summary": "In face-to-face conversations, individuals need to switch between speaking\nand listening roles seamlessly. Existing 3D talking head generation models\nfocus solely on speaking or listening, neglecting the natural dynamics of\ninteractive conversation, which leads to unnatural interactions and awkward\ntransitions. To address this issue, we propose a new task -- multi-round\ndual-speaker interaction for 3D talking head generation -- which requires\nmodels to handle and generate both speaking and listening behaviors in\ncontinuous conversation. To solve this task, we introduce DualTalk, a novel\nunified framework that integrates the dynamic behaviors of speakers and\nlisteners to simulate realistic and coherent dialogue interactions. This\nframework not only synthesizes lifelike talking heads when speaking but also\ngenerates continuous and vivid non-verbal feedback when listening, effectively\ncapturing the interplay between the roles. We also create a new dataset\nfeaturing 50 hours of multi-round conversations with over 1,000 characters,\nwhere participants continuously switch between speaking and listening roles.\nExtensive experiments demonstrate that our method significantly enhances the\nnaturalness and expressiveness of 3D talking heads in dual-speaker\nconversations. We recommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/dualtalk.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u591a\u8f6e\u53cc\u8bf4\u8bdd\u8005\u4ea4\u4e92\u76843D\u8bf4\u8bdd\u5934\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86DualTalk\u6846\u67b6\uff0c\u4ee5\u6a21\u62df\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7684\u8bf4\u8bdd\u548c\u503e\u542c\u884c\u4e3a\u3002", "motivation": "\u73b0\u67093D\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u4ec5\u5173\u6ce8\u8bf4\u8bdd\u6216\u503e\u542c\uff0c\u5ffd\u7565\u4e86\u5bf9\u8bdd\u4e2d\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u5bfc\u81f4\u4e0d\u81ea\u7136\u7684\u8fc7\u6e21\u3002", "method": "\u63d0\u51fa\u4e86DualTalk\u6846\u67b6\uff0c\u6574\u5408\u8bf4\u8bdd\u548c\u503e\u542c\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b50\u5c0f\u65f6\u5bf9\u8bdd\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53cc\u8bf4\u8bdd\u8005\u5bf9\u8bdd\u4e2d3D\u8bf4\u8bdd\u5934\u7684\u81ea\u7136\u6027\u548c\u8868\u73b0\u529b\u3002", "conclusion": "DualTalk\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8bdd\u4e2d\u89d2\u8272\u5207\u6362\u7684\u95ee\u9898\uff0c\u4e3a3D\u8bf4\u8bdd\u5934\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u6a21\u62df\u3002", "relevance": 40.0}}
{"id": "2505.17309", "pdf": "https://arxiv.org/pdf/2505.17309", "abs": "https://arxiv.org/abs/2505.17309", "authors": ["Nam H. Le", "Josh Bongard"], "title": "LaSER: How Learning Can Guide the Evolution of Equations", "categories": ["cs.NE", "cs.AI", "cs.SC"], "comment": null, "summary": "Evolution and learning are two distinct yet complementary forms of\nadaptation. While evolutionary processes operate across generations via the\nselection of genotypes, learning occurs within the lifetime of an individual,\nshaping behavior through phenotypic adjustment. The Baldwin effect describes\nhow lifetime learning can improve evolutionary search without altering\ninherited structures. While this has proven effective in areas like\nneuroevolution, where gradient-based learning is often used to fine-tune\nweights or behaviors produced by evolution, it remains underexplored in systems\nthat evolve non-differentiable symbolic structures like Genetic Programming\n(GP). GP evolves explicit syntax trees that represent equations, offering\nstrong interpretability but limited generalization due to the burden of\ndiscovering both useful representations and precise mappings.\n  Here, we show for the first time that integrating a simple form of supervised\nlearning, applied at the semantic or behavioral level during evaluation, can\neffectively guide the evolution of equations in GP. To achieve this, we propose\na new GP pipeline, LaSER (Latent Semantic Evolutionary Regression), where each\nGP individual generates a semantic representation that is passed to a\nsupervised learner. The quality of the learned mapping is used to assign\nfitness, without modifying the underlying syntax tree or evolutionary process.\n  Across standard symbolic regression benchmarks, in terms of generalization\nability, LaSER significantly outperforms traditional GP and, in several cases,\nmatches or exceeds popular machine learning regressors, while preserving the\nsymbolic interpretability. By separating evolution from learning, LaSER offers\na practical route to integrating GP with modern ML workflows, and opens new\navenues for research at the intersection of evolutionary computation and\nrepresentation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fdb\u5316\u8ba1\u7b97\u548c\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5LaSER\uff0c\u7528\u4e8e\u6539\u8fdb\u9057\u4f20\u7f16\u7a0b\uff08GP\uff09\u5728\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u76d1\u7763\u5b66\u4e60\u5f15\u5165\u975e\u53ef\u5fae\u7b26\u53f7\u7ed3\u6784\uff08\u5982GP\uff09\u7684\u8fdb\u5316\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u63d0\u5347\u5176\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faLaSER\u65b9\u6cd5\uff0c\u5c06GP\u4e2a\u4f53\u7684\u8bed\u4e49\u8868\u793a\u4f20\u9012\u7ed9\u76d1\u7763\u5b66\u4e60\u5668\uff0c\u5229\u7528\u5b66\u4e60\u5668\u7684\u6620\u5c04\u8d28\u91cf\u8bc4\u4f30\u9002\u5e94\u5ea6\uff0c\u800c\u4e0d\u6539\u53d8\u8bed\u6cd5\u6811\u6216\u8fdb\u5316\u8fc7\u7a0b\u3002", "result": "LaSER\u5728\u7b26\u53f7\u56de\u5f52\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfGP\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8d8a\u6d41\u884c\u7684\u673a\u5668\u5b66\u4e60\u56de\u5f52\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "LaSER\u4e3a\u8fdb\u5316\u8ba1\u7b97\u4e0e\u8868\u793a\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.17868", "pdf": "https://arxiv.org/pdf/2505.17868", "abs": "https://arxiv.org/abs/2505.17868", "authors": ["Devan Shah", "Shlomo Fortgang", "Sofiia Druchyna", "Elad Hazan"], "title": "SpectraLDS: Provable Distillation for Linear Dynamical Systems", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We present the first provable method for identifying symmetric linear\ndynamical systems (LDS) with accuracy guarantees that are independent of the\nsystems' state dimension or effective memory. Our approach builds upon recent\nwork that represents symmetric LDSs as convolutions learnable via fixed\nspectral transformations. We show how to invert this representation, thereby\nrecovering an LDS model from its spectral transform and yielding an end-to-end\nconvex optimization procedure. This distillation preserves predictive accuracy\nwhile enabling constant-time and constant-space inference per token,\nindependent of sequence length. We evaluate our method, SpectraLDS, as a\ncomponent in sequence prediction architectures and demonstrate that accuracy is\npreserved while inference efficiency is improved on tasks such as language\nmodeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5bf9\u79f0\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\uff08LDS\uff09\uff0c\u5176\u7cbe\u5ea6\u4fdd\u8bc1\u4e0e\u7cfb\u7edf\u72b6\u6001\u7ef4\u5ea6\u6216\u6709\u6548\u5185\u5b58\u65e0\u5173\u3002\u901a\u8fc7\u56fa\u5b9a\u8c31\u53d8\u6362\u5b66\u4e60\u5377\u79ef\u8868\u793a\uff0c\u5e76\u53cd\u8f6c\u8be5\u8868\u793a\u4ee5\u6062\u590dLDS\u6a21\u578b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u51f8\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u5bf9\u79f0LDS\u7684\u9ad8\u6548\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u5347\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6548\u7387\u3002", "method": "\u5229\u7528\u56fa\u5b9a\u8c31\u53d8\u6362\u5b66\u4e60\u5377\u79ef\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u51f8\u4f18\u5316\u53cd\u8f6c\u8be5\u8868\u793a\uff0c\u6062\u590dLDS\u6a21\u578b\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\u4e2d\uff0c\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "conclusion": "SpectraLDS\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u7684\u6052\u5b9a\u65f6\u95f4\u548c\u7a7a\u95f4\u63a8\u7406\u3002", "relevance": 40.0}}
{"id": "2505.17612", "pdf": "https://arxiv.org/pdf/2505.17612", "abs": "https://arxiv.org/abs/2505.17612", "authors": ["Minki Kang", "Jongwon Jeong", "Seanie Lee", "Jaewoong Cho", "Sung Ju Hwang"], "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "categories": ["cs.CL", "cs.AI"], "comment": "preprint, v1", "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgent Distillation\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u548c\u4efb\u52a1\u89e3\u51b3\u884c\u4e3a\u8f6c\u79fb\u5230\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08sLMs\uff09\u4e2d\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u548c\u4ee3\u7801\u5de5\u5177\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u7f55\u89c1\u4e8b\u5b9e\u77e5\u8bc6\u6216\u7cbe\u786e\u8ba1\u7b97\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "1. \u5f15\u5165first-thought prefix\u63d0\u793a\u65b9\u6cd5\u63d0\u5347\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u8f68\u8ff9\u8d28\u91cf\uff1b2. \u63d0\u51faself-consistent action generation\u589e\u5f3a\u5c0f\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u516b\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c0.5B\u30011.5B\u30013B\u53c2\u6570\u7684\u5c0f\u6a21\u578b\u6027\u80fd\u4e0e1.5B\u30013B\u30017B\u53c2\u6570\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "Agent Distillation\u6846\u67b6\u4e3a\u6784\u5efa\u5b9e\u7528\u7684\u5de5\u5177\u578b\u5c0f\u6a21\u578b\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "relevance": 85.0}}
{"id": "2505.18106", "pdf": "https://arxiv.org/pdf/2505.18106", "abs": "https://arxiv.org/abs/2505.18106", "authors": ["Varun Ajith", "Anindya Pal", "Saumik Bhattacharya", "Sayantari Ghosh"], "title": "F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.LG", "eess.IV"], "comment": "11 pages, 9 figures, 2 tables, conference paper", "summary": "Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faF-ANcGAN\uff0c\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u5c11\u91cf\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7eb3\u7c73\u6750\u6599SEM\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7eb3\u7c73\u6750\u6599\u7814\u7a76\u4e2d\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7f3a\u4e4f\u9650\u5236\u4e86\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u5c11\u91cf\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u5faa\u73af\u4e00\u81f4\u6027\u751f\u6210\u5bf9\u6297\u7cfb\u7edf\uff08F-ANcGAN\uff09\uff0c\u7ed3\u5408Style U-Net\u751f\u6210\u5668\u548cU-Net\u5206\u5272\u7f51\u7edc\uff0c\u5e76\u5e94\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u6a21\u578b\u5728TiO2\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8617.65\u7684\u539f\u59cbFID\u5206\u6570\uff0c\u901a\u8fc7\u540e\u5904\u7406\u8fdb\u4e00\u6b65\u964d\u81f310.39\u3002", "conclusion": "F-ANcGAN\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u7f13\u89e3\u7eb3\u7c73\u6750\u6599\u5206\u6790\u4e2d\u7684\u6570\u636e\u77ed\u7f3a\u95ee\u9898\u3002", "relevance": 20.0}}
{"id": "2505.17869", "pdf": "https://arxiv.org/pdf/2505.17869", "abs": "https://arxiv.org/abs/2505.17869", "authors": ["Mohammad Shahverdikondori", "Mohammad Reza Badri", "Negar Kiyavash"], "title": "Best Group Identification in Multi-Objective Bandits", "categories": ["cs.LG"], "comment": null, "summary": "We introduce the Best Group Identification problem in a multi-objective\nmulti-armed bandit setting, where an agent interacts with groups of arms with\nvector-valued rewards. The performance of a group is determined by an\nefficiency vector which represents the group's best attainable rewards across\ndifferent dimensions. The objective is to identify the set of optimal groups in\nthe fixed-confidence setting. We investigate two key formulations: group Pareto\nset identification, where efficiency vectors of optimal groups are Pareto\noptimal and linear best group identification, where each reward dimension has a\nknown weight and the optimal group maximizes the weighted sum of its efficiency\nvector's entries. For both settings, we propose elimination-based algorithms,\nestablish upper bounds on their sample complexity, and derive lower bounds that\napply to any correct algorithm. Through numerical experiments, we demonstrate\nthe strong empirical performance of the proposed algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u6700\u4f18\u7ec4\u8bc6\u522b\u95ee\u9898\uff0c\u7814\u7a76\u4e86\u4e24\u79cd\u5173\u952e\u5f62\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d88\u9664\u7684\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6837\u672c\u590d\u6742\u5ea6\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u76ee\u6807\u591a\u81c2\u8001\u864e\u673a\u4e2d\u8bc6\u522b\u6700\u4f18\u7ec4\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u8bbe\u7f6e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u7ec4\u5e15\u7d2f\u6258\u96c6\u8bc6\u522b\u548c\u7ebf\u6027\u6700\u4f18\u7ec4\u8bc6\u522b\uff0c\u57fa\u4e8e\u6d88\u9664\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5f3a\u5b9e\u8bc1\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u6837\u672c\u590d\u6742\u5ea6\u7684\u7406\u8bba\u754c\u9650\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u591a\u76ee\u6807\u591a\u81c2\u8001\u864e\u673a\u4e2d\u6709\u6548\u8bc6\u522b\u6700\u4f18\u7ec4\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\u3002", "relevance": 30.0}}
{"id": "2505.17616", "pdf": "https://arxiv.org/pdf/2505.17616", "abs": "https://arxiv.org/abs/2505.17616", "authors": ["Qingyu Lu", "Liang Ding", "Siyi Cao", "Xuebo Liu", "Kanjian Zhang", "Jinxia Zhang", "Dacheng Tao"], "title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Agents powered by large language models (LLMs) have demonstrated strong\nplanning and decision-making capabilities in complex embodied environments.\nHowever, such agents often suffer from inefficiencies in multi-turn\ninteractions, frequently trapped in repetitive loops or issuing ineffective\ncommands, leading to redundant computational overhead. Instead of relying\nsolely on learning from trajectories, we take a first step toward exploring the\nearly-exit behavior for LLM-based agents. We propose two complementary\napproaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions\nduring generation, and 2. an $\\textbf{extrinsic}$ method that verifies task\ncompletion to determine when to halt an agent's trial. To evaluate early-exit\nmechanisms, we introduce two metrics: one measures the reduction of\n$\\textbf{redundant steps}$ as a positive effect, and the other evaluates\n$\\textbf{progress degradation}$ as a negative effect. Experiments with 4\ndifferent LLMs across 5 embodied environments show significant efficiency\nimprovements, with only minor drops in agent performance. We also validate a\npractical strategy where a stronger agent assists after an early-exit agent,\nachieving better performance with the same total steps. We will release our\ncode to support further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65e9\u671f\u9000\u51fa\u673a\u5236\uff08\u5185\u5728\u548c\u5916\u5728\u65b9\u6cd5\uff09\u4ee5\u63d0\u9ad8\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u6548\u7387\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u591a\u8f6e\u4ea4\u4e92\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u5e38\u9677\u5165\u91cd\u590d\u5faa\u73af\u6216\u65e0\u6548\u547d\u4ee4\uff0c\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u5f00\u9500\u3002", "method": "1. \u5185\u5728\u65b9\u6cd5\uff1a\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u9000\u51fa\u6307\u4ee4\uff1b2. \u5916\u5728\u65b9\u6cd5\uff1a\u9a8c\u8bc1\u4efb\u52a1\u5b8c\u6210\u5ea6\u4ee5\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u4ee3\u7406\u3002\u5f15\u5165\u4e24\u4e2a\u6307\u6807\uff08\u5197\u4f59\u6b65\u9aa4\u51cf\u5c11\u548c\u8fdb\u5ea6\u9000\u5316\uff09\u8bc4\u4f30\u65e9\u671f\u9000\u51fa\u673a\u5236\u3002", "result": "\u57284\u79cdLLM\u548c5\u79cd\u73af\u5883\u4e2d\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u4ec5\u5c0f\u5e45\u4e0b\u964d\u3002\u9a8c\u8bc1\u4e86\u5f3a\u4ee3\u7406\u8f85\u52a9\u7b56\u7565\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u65e9\u671f\u9000\u51fa\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u7684\u6548\u7387\uff0c\u4e14\u6027\u80fd\u635f\u5931\u53ef\u63a7\u3002", "relevance": 85.0}}
{"id": "2505.18111", "pdf": "https://arxiv.org/pdf/2505.18111", "abs": "https://arxiv.org/abs/2505.18111", "authors": ["Cheng-Yen Yang", "Hsiang-Wei Huang", "Pyong-Kun Kim", "Chien-Kai Kuo", "Jui-Wei Chang", "Kwang-Ju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking", "categories": ["cs.CV"], "comment": "Accepted by ICPR Multi-Modal Visual Pattern Recognition Workshop", "summary": "We present an effective approach for adapting the Segment Anything Model 2\n(SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the\npowerful pre-trained capabilities of SAM2 and incorporates several key\ntechniques to enhance its performance in VOT applications. By combining SAM2\nwith our proposed optimizations, we achieved a first place AUC score of 89.4 on\nthe 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the\neffectiveness of our approach. This paper details our methodology, the specific\nenhancements made to SAM2, and a comprehensive analysis of our results in the\ncontext of VOT solutions along with the multi-modality aspect of the dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Segment Anything Model 2\uff08SAM2\uff09\u9002\u914d\u5230\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\uff08VOT\uff09\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u80fd\u529b\u548c\u5173\u952e\u4f18\u5316\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528SAM2\u7684\u5f3a\u5927\u9884\u8bad\u7ec3\u80fd\u529b\uff0c\u63d0\u5347\u5176\u5728VOT\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408SAM2\u7684\u9884\u8bad\u7ec3\u80fd\u529b\uff0c\u63d0\u51fa\u591a\u9879\u4f18\u5316\u6280\u672f\uff0c\u5305\u62ec\u7279\u5b9a\u589e\u5f3a\u548c\u6027\u80fd\u63d0\u5347\u7b56\u7565\u3002", "result": "\u57282024 ICPR\u591a\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\u6311\u6218\u4e2d\uff0c\u4ee589.4\u7684AUC\u5f97\u5206\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SAM2\u5728VOT\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.17321", "pdf": "https://arxiv.org/pdf/2505.17321", "abs": "https://arxiv.org/abs/2505.17321", "authors": ["Tiago Fonseca", "Clarisse Sousa", "Ricardo Ven\u00e2ncio", "Pedro Pires", "Ricardo Severino", "Paulo Rodrigues", "Pedro Paiva", "Luis Lino Ferreira"], "title": "Control of Renewable Energy Communities using AI and Real-World Data", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "8 pages, 3 figures, 1 table, 30th IEEE International Conference on\n  Emerging Technologies and Factory Automation", "summary": "The electrification of transportation and the increased adoption of\ndecentralized renewable energy generation have added complexity to managing\nRenewable Energy Communities (RECs). Integrating Electric Vehicle (EV) charging\nwith building energy systems like heating, ventilation, air conditioning\n(HVAC), photovoltaic (PV) generation, and battery storage presents significant\nopportunities but also practical challenges. Reinforcement learning (RL),\nparticularly MultiAgent Deep Deterministic Policy Gradient (MADDPG) algorithms,\nhave shown promising results in simulation, outperforming heuristic control\nstrategies. However, translating these successes into real-world deployments\nfaces substantial challenges, including incomplete and noisy data, integration\nof heterogeneous subsystems, synchronization issues, unpredictable occupant\nbehavior, and missing critical EV state-of-charge (SoC) information. This paper\nintroduces a framework designed explicitly to handle these complexities and\nbridge the simulation to-reality gap. The framework incorporates EnergAIze, a\nMADDPG-based multi-agent control strategy, and specifically addresses\nchallenges related to real-world data collection, system integration, and user\nbehavior modeling. Preliminary results collected from a real-world operational\nREC with four residential buildings demonstrate the practical feasibility of\nour approach, achieving an average 9% reduction in daily peak demand and a 5%\ndecrease in energy costs through optimized load scheduling and EV charging\nbehaviors. These outcomes underscore the framework's effectiveness, advancing\nthe practical deployment of intelligent energy management solutions in RECs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMADDPG\u7684\u591a\u667a\u80fd\u4f53\u63a7\u5236\u6846\u67b6EnergAIze\uff0c\u7528\u4e8e\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u793e\u533a\uff08RECs\uff09\u4e2d\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u4e0e\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u96c6\u6210\u7684\u590d\u6742\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u4ea4\u901a\u7535\u6c14\u5316\u548c\u53ef\u518d\u751f\u80fd\u6e90\u7684\u666e\u53ca\uff0c\u7ba1\u7406RECs\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u6574\u5408\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u4e0e\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u65f6\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08MADDPG\uff09\u7b97\u6cd5\uff0c\u8bbe\u8ba1EnergAIze\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u4e0d\u5b8c\u6574\u3001\u5f02\u6784\u7cfb\u7edf\u96c6\u6210\u548c\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u7b49\u95ee\u9898\u3002", "result": "\u5728\u5b9e\u9645REC\u4e2d\u6d4b\u8bd5\uff0c\u6846\u67b6\u5b9e\u73b0\u4e86\u6bcf\u65e5\u5cf0\u503c\u9700\u6c42\u5e73\u5747\u964d\u4f4e9%\u548c\u80fd\u6e90\u6210\u672c\u51cf\u5c115%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63a8\u8fdb\u4e86\u667a\u80fd\u80fd\u6e90\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u5728RECs\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.17871", "pdf": "https://arxiv.org/pdf/2505.17871", "abs": "https://arxiv.org/abs/2505.17871", "authors": ["Zezhi Shao", "Yujie Li", "Fei Wang", "Chengqing Yu", "Yisong Fu", "Tangwen Qian", "Bin Xu", "Boyu Diao", "Yongjun Xu", "Xueqi Cheng"], "title": "BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models", "categories": ["cs.LG"], "comment": "Accepted by SIGKDD 2025 (Research Track)", "summary": "The advent of universal time series forecasting models has revolutionized\nzero-shot forecasting across diverse domains, yet the critical role of data\ndiversity in training these models remains underexplored. Existing large-scale\ntime series datasets often suffer from inherent biases and imbalanced\ndistributions, leading to suboptimal model performance and generalization. To\naddress this gap, we introduce BLAST, a novel pre-training corpus designed to\nenhance data diversity through a balanced sampling strategy. First, BLAST\nincorporates 321 billion observations from publicly available datasets and\nemploys a comprehensive suite of statistical metrics to characterize time\nseries patterns. Then, to facilitate pattern-oriented sampling, the data is\nimplicitly clustered using grid-based partitioning. Furthermore, by integrating\ngrid sampling and grid mixup techniques, BLAST ensures a balanced and\nrepresentative coverage of diverse patterns. Experimental results demonstrate\nthat models pre-trained on BLAST achieve state-of-the-art performance with a\nfraction of the computational resources and training tokens required by\nexisting methods. Our findings highlight the pivotal role of data diversity in\nimproving both training efficiency and model performance for the universal\nforecasting task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBLAST\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u5e73\u8861\u91c7\u6837\u7b56\u7565\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\uff0c\u63d0\u5347\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u5b58\u5728\u504f\u5dee\u548c\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u591a\u6837\u6027\u5bf9\u901a\u7528\u9884\u6d4b\u6a21\u578b\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "BLAST\u6574\u54083210\u4ebf\u89c2\u6d4b\u6570\u636e\uff0c\u4f7f\u7528\u7edf\u8ba1\u6307\u6807\u8868\u5f81\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u7f51\u683c\u5206\u533a\u805a\u7c7b\u548c\u7f51\u683c\u6df7\u5408\u6280\u672f\u5b9e\u73b0\u5e73\u8861\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eBLAST\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4ee5\u66f4\u5c11\u8ba1\u7b97\u8d44\u6e90\u548c\u8bad\u7ec3\u6807\u8bb0\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u6570\u636e\u591a\u6837\u6027\u5bf9\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "relevance": 40.0}}
{"id": "2505.17625", "pdf": "https://arxiv.org/pdf/2505.17625", "abs": "https://arxiv.org/abs/2505.17625", "authors": ["Hayato Aida", "Kosuke Takahashi", "Takahiro Omi"], "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports", "categories": ["cs.CL", "cs.CV", "68T50", "I.2"], "comment": "Accepted at IIAI AAI 2025, the 3rd International Conference on\n  Computational and Data Sciences in Economics and Finance", "summary": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u8868\u683c\u7406\u89e3\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8868\u683c\u5185\u6587\u672c\u5185\u5bb9\u548c\u5e03\u5c40\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u6587\u6863\u5e03\u5c40\u7684\u89e3\u6790\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u53d1\u5c55\uff0c\u8868\u683c\u7ed3\u6784\u7406\u89e3\u5728\u91d1\u878d\u7b49\u9886\u57df\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u5b57\u7b26\u548c\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u8868\u683c\u5185\u6587\u672c\u5185\u5bb9\u548c\u5e03\u5c40\u7279\u5f81\u4f5c\u4e3a\u8f85\u52a9\u6a21\u6001\uff0c\u589e\u5f3aLVLM\u7684\u8868\u683c\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f\u7ed3\u6784\u5316\u8f93\u5165\u5373\u53ef\u89e3\u6790\u590d\u6742\u6587\u6863\u5e03\u5c40\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001LLM\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 70.0}}
{"id": "2505.18115", "pdf": "https://arxiv.org/pdf/2505.18115", "abs": "https://arxiv.org/abs/2505.18115", "authors": ["Jacob Hansen", "Wei Lin", "Junmo Kang", "Muhammad Jehanzeb Mirza", "Hongyin Luo", "Rogerio Feris", "Alan Ritter", "James Glass", "Leonid Karlinsky"], "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion", "categories": ["cs.CV"], "comment": null, "summary": "Visual Instruction Tuning (VisIT) data, commonly available as human-assistant\nconversations with images interleaved in the human turns, are currently the\nmost widespread vehicle for aligning strong LLMs to understand visual inputs,\nconverting them to strong LMMs. While many VisIT datasets are available, most\nare constructed using ad-hoc techniques developed independently by different\ngroups. They are often poorly documented, lack reproducible code, and rely on\npaid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert\nimage metadata (labels) into VisIT instructions. This leads to high costs and\nmakes it challenging to scale, enhance quality, or generate VisIT data for new\ndatasets. In this work, we address these challenges and propose an open and\nunified recipe and approach,~\\textbf{\\method}, for converting available\nmetadata to VisIT instructions using open LLMs. Our multi-stage \\method\nfeatures an efficient framework for metadata grouping, quality control, data\nand prompt organization, and conversation sampling. We show that our approach\ncan reproduce or enhance the data quality of available VisIT datasets when\napplied to the same image data and metadata sources, improving GPT-4 generated\nVisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks\nusing open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our\napproach enables effective performance scaling - both in quantity and quality -\nby enhancing the resulting LMM performance across a wide range of benchmarks.\nWe also analyze the impact of various factors, including conversation format,\nbase model selection, and resampling strategies. Our code, which supports the\nreproduction of equal or higher-quality VisIT datasets and facilities future\nmetadata-to-VisIT data conversion for niche domains, is released at\nhttps://github.com/jacob-hansen/Instructify.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\method\u7684\u5f00\u6e90\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u56fe\u50cf\u5143\u6570\u636e\u8f6c\u6362\u4e3a\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\uff08VisIT\uff09\u6570\u636e\uff0c\u5229\u7528\u5f00\u6e90LLM\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VisIT\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u3001\u4e0d\u53ef\u6269\u5c55\u548c\u4f9d\u8d56\u95ed\u6e90\u6a21\u578bAPI\u7684\u95ee\u9898\u3002", "method": "\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u5143\u6570\u636e\u5206\u7ec4\u3001\u8d28\u91cf\u63a7\u5236\u3001\u6570\u636e\u7ec4\u7ec7\u3001\u5bf9\u8bdd\u91c7\u6837\uff0c\u4f7f\u7528\u5f00\u6e90LLM\uff08\u5982Gemma 2 27B\u548cLLaMa 3.1 70B\uff09\u3002", "result": "\u6570\u636e\u8d28\u91cf\u63d0\u53473%\u5e73\u5747\uff08\u6700\u9ad812%\uff09\uff0cLMM\u6027\u80fd\u5728\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\\method\u4e3aVisIT\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5f00\u6e90\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17872", "pdf": "https://arxiv.org/pdf/2505.17872", "abs": "https://arxiv.org/abs/2505.17872", "authors": ["Licheng Pan", "Zhichao Chen", "Haoxuan Li", "Guangyi Liu", "Zhijian Xu", "Zhaoran Liu", "Hao Wang", "Ying Wei"], "title": "Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-task forecasting has become the standard approach for time-series\nforecasting (TSF). However, we show that it suffers from an Expressiveness\nBottleneck, where predictions at different time steps share the same\nrepresentation, leading to unavoidable errors even with optimal\nrepresentations. To address this issue, we propose a two-stage framework:\nfirst, pre-train a foundation model for one-step-ahead prediction; then, adapt\nit using step-specific LoRA modules.This design enables the foundation model to\nhandle any number of forecast steps while avoiding the expressiveness\nbottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which\nemploys adaptively weighted LoRA experts to achieve partial parameter sharing\nacross steps. This approach enhances both efficiency and forecasting\nperformance by exploiting interdependencies between forecast steps. Experiments\nshow that MoLA significantly improves model expressiveness and outperforms\nstate-of-the-art time-series forecasting methods. Code is available at\nhttps://anonymous.4open.science/r/MoLA-BC92.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff08\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b+LoRA\u6a21\u5757\u9002\u914d\uff09\u6765\u89e3\u51b3\u591a\u4efb\u52a1\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f15\u5165Mixture-of-LoRA\uff08MoLA\uff09\u6a21\u578b\u4ee5\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u591a\u4efb\u52a1\u9884\u6d4b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u666e\u904d\u5b58\u5728\u8868\u8fbe\u80fd\u529b\u74f6\u9888\u95ee\u9898\uff0c\u5373\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u9884\u6d4b\u5171\u4eab\u76f8\u540c\u8868\u793a\uff0c\u5bfc\u81f4\u4e0d\u53ef\u907f\u514d\u7684\u8bef\u5dee\u3002", "method": "1. \u9884\u8bad\u7ec3\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u5355\u6b65\u9884\u6d4b\uff1b2. \u4f7f\u7528\u6b65\u9aa4\u7279\u5b9a\u7684LoRA\u6a21\u5757\u8fdb\u884c\u9002\u914d\uff1b3. \u5f15\u5165MoLA\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u7684LoRA\u4e13\u5bb6\u5b9e\u73b0\u90e8\u5206\u53c2\u6570\u5171\u4eab\u3002", "result": "MoLA\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u548cMoLA\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u8868\u8fbe\u80fd\u529b\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002", "relevance": 60.0}}
{"id": "2505.17630", "pdf": "https://arxiv.org/pdf/2505.17630", "abs": "https://arxiv.org/abs/2505.17630", "authors": ["Joakim Edin", "R\u00f3bert Csord\u00e1s", "Tuukka Ruotsalo", "Zhengxuan Wu", "Maria Maistro", "Jing Huang", "Lars Maal\u00f8e"], "title": "GIM: Improved Interpretability for Large Language Models", "categories": ["cs.CL", "cs.LG", "68T07", "I.2.0; I.2.7"], "comment": null, "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5Gradient Interaction Modifications (GIM)\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u81ea\u4fee\u590d\u73b0\u8c61\uff0c\u63d0\u9ad8\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u81ea\u4fee\u590d\u73b0\u8c61\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u4f4e\u4f30\u4e86\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5173\u952e\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\uff0c\u5f71\u54cd\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "method": "\u63d0\u51faGIM\u6280\u672f\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u8003\u8651\u81ea\u4fee\u590d\u6548\u5e94\uff0c\u6539\u8fdb\u7535\u8def\u8bc6\u522b\u548c\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGIM\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002", "conclusion": "GIM\u662f\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "relevance": 90.0}}
{"id": "2505.18129", "pdf": "https://arxiv.org/pdf/2505.18129", "abs": "https://arxiv.org/abs/2505.18129", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.", "AI": {"tldr": "V-Triune\u662f\u4e00\u4e2a\u89c6\u89c9\u4e09\u91cd\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u8054\u5408\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5b8c\u6210\u89c6\u89c9\u63a8\u7406\u548c\u611f\u77e5\u4efb\u52a1\u3002\u901a\u8fc7\u52a8\u6001IoU\u5956\u52b1\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\uff0cOrsta\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u9664\u63a8\u7406\u4efb\u52a1\u5916\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u611f\u77e5\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff09\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faV-Triune\u7cfb\u7edf\uff0c\u5305\u542b\u6837\u672c\u7ea7\u6570\u636e\u683c\u5f0f\u5316\u3001\u9a8c\u8bc1\u7ea7\u5956\u52b1\u8ba1\u7b97\u548c\u6e90\u7ea7\u6307\u6807\u76d1\u63a7\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u5f15\u5165\u52a8\u6001IoU\u5956\u52b1\u3002\u57fa\u4e8e\u5f00\u6e907B\u548c32B\u6a21\u578b\u5b9e\u73b0\u3002", "result": "Orsta\u6a21\u578b\u5728MEGA-Bench Core\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff08+2.1\u81f3+14.1\uff09\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u5e7f\u6cdb\u80fd\u529b\u3002", "conclusion": "\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9VLMs\u6709\u6548\u4e14\u53ef\u6269\u5c55\u3002", "relevance": 75.0}}
{"id": "2505.17875", "pdf": "https://arxiv.org/pdf/2505.17875", "abs": "https://arxiv.org/abs/2505.17875", "authors": ["Yan Zhong", "Xingyu Wu", "Xinping Zhao", "Li Zhang", "Xinyuan Song", "Lei Shi", "Bingbing Jiang"], "title": "Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "In practical domains, high-dimensional data are usually associated with\ndiverse semantic labels, whereas traditional feature selection methods are\ndesigned for single-label data. Moreover, existing multi-label methods\nencounter two main challenges in semi-supervised scenarios: (1). Most\nsemi-supervised methods fail to evaluate the label correlations without enough\nlabeled samples, which are the critical information of multi-label feature\nselection, making label-specific features discarded. (2). The similarity graph\nstructure directly derived from the original feature space is suboptimal for\nmulti-label problems in existing graph-based methods, leading to unreliable\nsoft labels and degraded feature selection performance. To overcome them, we\npropose a consistent sparse graph learning method for multi-label\nsemi-supervised feature selection (SGMFS), which can enhance the feature\nselection performance by maintaining space consistency and learning label\ncorrelations in semi-supervised scenarios. Specifically, for Challenge (1),\nSGMFS learns a low-dimensional and independent label subspace from the\nprojected features, which can compatibly cross multiple labels and effectively\nachieve the label correlations. For Challenge (2), instead of constructing a\nfixed similarity graph for semi-supervised learning, SGMFS thoroughly explores\nthe intrinsic structure of the data by performing sparse reconstruction of\nsamples in both the label space and the learned subspace simultaneously. In\nthis way, the similarity graph can be adaptively learned to maintain the\nconsistency between label space and the learned subspace, which can promote\npropagating proper soft labels for unlabeled samples, facilitating the ultimate\nfeature selection. An effective solution with fast convergence is designed to\noptimize the objective function. Extensive experiments validate the superiority\nof SGMFS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6807\u7b7e\u534a\u76d1\u7763\u7279\u5f81\u9009\u62e9\u7684\u4e00\u81f4\u7a00\u758f\u56fe\u5b66\u4e60\u65b9\u6cd5\uff08SGMFS\uff09\uff0c\u901a\u8fc7\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u5b66\u4e60\u6807\u7b7e\u76f8\u5173\u6027\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u9488\u5bf9\u5355\u6807\u7b7e\u6570\u636e\uff0c\u800c\u73b0\u6709\u591a\u6807\u7b7e\u65b9\u6cd5\u5728\u534a\u76d1\u7763\u573a\u666f\u4e0b\u9762\u4e34\u6807\u7b7e\u76f8\u5173\u6027\u8bc4\u4f30\u548c\u76f8\u4f3c\u56fe\u7ed3\u6784\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "SGMFS\u901a\u8fc7\u6784\u5efa\u4f4e\u7ef4\u72ec\u7acb\u6807\u7b7e\u5b50\u7a7a\u95f4\u548c\u7a00\u758f\u91cd\u6784\u6837\u672c\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u76f8\u4f3c\u56fe\uff0c\u4fdd\u6301\u6807\u7b7e\u7a7a\u95f4\u4e0e\u5b66\u4e60\u5b50\u7a7a\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SGMFS\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "SGMFS\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6807\u7b7e\u534a\u76d1\u7763\u7279\u5f81\u9009\u62e9\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "relevance": 30.0}}
{"id": "2505.17642", "pdf": "https://arxiv.org/pdf/2505.17642", "abs": "https://arxiv.org/abs/2505.17642", "authors": ["Alessandra Teresa Cignarella", "Anastasia Giachanou", "Els Lefever"], "title": "Stereotype Detection in Natural Language Processing", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Stereotypes influence social perceptions and can escalate into discrimination\nand violence. While NLP research has extensively addressed gender bias and hate\nspeech, stereotype detection remains an emerging field with significant\nsocietal implications. In this work is presented a survey of existing research,\nanalyzing definitions from psychology, sociology, and philosophy. A\nsemi-automatic literature review was performed by using Semantic Scholar. We\nretrieved and filtered over 6,000 papers (in the year range 2000-2025),\nidentifying key trends, methodologies, challenges and future directions. The\nfindings emphasize stereotype detection as a potential early-monitoring tool to\nprevent bias escalation and the rise of hate speech. Conclusions highlight the\nneed for a broader, multilingual, and intersectional approach in NLP studies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5206\u6790\u4e86\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u5b66\u548c\u54f2\u5b66\u4e2d\u7684\u5b9a\u4e49\uff0c\u5e76\u63d0\u51fa\u4e86NLP\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u523b\u677f\u5370\u8c61\u53ef\u80fd\u5bfc\u81f4\u6b67\u89c6\u548c\u66b4\u529b\uff0c\u800cNLP\u9886\u57df\u5bf9\u6027\u522b\u504f\u89c1\u548c\u4ec7\u6068\u8a00\u8bba\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u4ecd\u662f\u4e00\u4e2a\u65b0\u5174\u9886\u57df\uff0c\u5177\u6709\u91cd\u8981\u7684\u793e\u4f1a\u610f\u4e49\u3002", "method": "\u901a\u8fc7Semantic Scholar\u8fdb\u884c\u4e86\u534a\u81ea\u52a8\u6587\u732e\u7efc\u8ff0\uff0c\u68c0\u7d22\u5e76\u7b5b\u9009\u4e862000-2025\u5e74\u76846000\u591a\u7bc7\u8bba\u6587\uff0c\u5206\u6790\u4e86\u5173\u952e\u8d8b\u52bf\u3001\u65b9\u6cd5\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u53ef\u4f5c\u4e3a\u65e9\u671f\u76d1\u6d4b\u5de5\u5177\uff0c\u9632\u6b62\u504f\u89c1\u5347\u7ea7\u548c\u4ec7\u6068\u8a00\u8bba\u7684\u51fa\u73b0\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03NLP\u7814\u7a76\u9700\u8981\u66f4\u5e7f\u6cdb\u3001\u591a\u8bed\u8a00\u548c\u4ea4\u53c9\u6027\u7684\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.18132", "pdf": "https://arxiv.org/pdf/2505.18132", "abs": "https://arxiv.org/abs/2505.18132", "authors": ["Dingqing Ye", "Chao Fan", "Zhanbo Huang", "Chengwen Luo", "Jianqiang Li", "Shiqi Yu", "Xiaoming Liu"], "title": "BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models", "categories": ["cs.CV"], "comment": null, "summary": "Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBiggerGait\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff08LVM\uff09\u7684\u4e2d\u95f4\u5c42\u4e92\u8865\u6027\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b65\u6001\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LVM\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6b65\u6001\u5148\u9a8c\u77e5\u8bc6\uff0c\u5ffd\u89c6\u4e86LVM\u672c\u8eab\u7684\u591a\u5c42\u6b21\u8868\u793a\u6f5c\u529b\u3002", "method": "\u5206\u6790LVM\u5404\u5c42\u8868\u793a\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7b80\u5355\u901a\u7528\u7684BiggerGait\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86BiggerGait\u7684\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "BiggerGait\u4e3a\u6b65\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u5b9e\u7528\u7684\u57fa\u7ebf\uff0c\u65e0\u9700\u590d\u6742\u5148\u9a8c\u77e5\u8bc6\u3002", "relevance": 40.0}}
{"id": "2505.17883", "pdf": "https://arxiv.org/pdf/2505.17883", "abs": "https://arxiv.org/abs/2505.17883", "authors": ["Laines Schmalwasser", "Niklas Penzel", "Joachim Denzler", "Julia Niebling"], "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at ICML 2025, 27 pages, 20 figures, 9 tables", "summary": "Concepts such as objects, patterns, and shapes are how humans understand the\nworld. Building on this intuition, concept-based explainability methods aim to\nstudy representations learned by deep neural networks in relation to\nhuman-understandable concepts. Here, Concept Activation Vectors (CAVs) are an\nimportant tool and can identify whether a model learned a concept or not.\nHowever, the computational cost and time requirements of existing CAV\ncomputation pose a significant challenge, particularly in large-scale,\nhigh-dimensional architectures. To address this limitation, we introduce\nFastCAV, a novel approach that accelerates the extraction of CAVs by up to\n63.6x (on average 46.4x). We provide a theoretical foundation for our approach\nand give concrete assumptions under which it is equivalent to established\nSVM-based methods. Our empirical results demonstrate that CAVs calculated with\nFastCAV maintain similar performance while being more efficient and stable. In\ndownstream applications, i.e., concept-based explanation methods, we show that\nFastCAV can act as a replacement leading to equivalent insights. Hence, our\napproach enables previously infeasible investigations of deep models, which we\ndemonstrate by tracking the evolution of concepts during model training.", "AI": {"tldr": "FastCAV\u662f\u4e00\u79cd\u52a0\u901f\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08CAV\uff09\u63d0\u53d6\u7684\u65b0\u65b9\u6cd5\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb46.4\u500d\uff08\u6700\u9ad863.6\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u5b83\u652f\u6301\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u67b6\u6784\u7684\u6982\u5ff5\u5206\u6790\uff0c\u5e76\u53ef\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u4e2d\u6982\u5ff5\u7684\u52a8\u6001\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709CAV\u8ba1\u7b97\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u67b6\u6784\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u6982\u5ff5\u5206\u6790\u7684\u5e94\u7528\u3002FastCAV\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faFastCAV\uff0c\u57fa\u4e8e\u7406\u8bba\u5047\u8bbe\u52a0\u901fCAV\u63d0\u53d6\uff0c\u4e0eSVM\u65b9\u6cd5\u7b49\u6548\uff0c\u4f46\u66f4\u9ad8\u6548\u7a33\u5b9a\u3002", "result": "FastCAV\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff08\u5e73\u574746.4\u500d\uff09\uff0c\u5e76\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "FastCAV\u4e3a\u6df1\u5ea6\u6a21\u578b\u7684\u6982\u5ff5\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u652f\u6301\u65b0\u7814\u7a76\u5982\u8bad\u7ec3\u4e2d\u6982\u5ff5\u7684\u52a8\u6001\u8ddf\u8e2a\u3002", "relevance": 75.0}}
{"id": "2505.17643", "pdf": "https://arxiv.org/pdf/2505.17643", "abs": "https://arxiv.org/abs/2505.17643", "authors": ["Sara Ketabi", "Dhanesh Ramachandram"], "title": "Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conventional machine learning models, particularly tree-based approaches,\nhave demonstrated promising performance across various clinical prediction\ntasks using electronic health record (EHR) data. Despite their strengths, these\nmodels struggle with tasks that require deeper contextual understanding, such\nas predicting 30-day hospital readmission. This can be primarily due to the\nlimited semantic information available in structured EHR data. To address this\nlimitation, we propose a deep multimodal contrastive learning (CL) framework\nthat aligns the latent representations of structured EHR data with unstructured\ndischarge summary notes. It works by pulling together paired EHR and text\nembeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR\nencoder extracted from this framework significantly boosts downstream task\nperformance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission\nprediction. Such results demonstrate the effect of integrating domain knowledge\nfrom clinical notes into EHR-based pipelines, enabling more accurate and\ncontext-aware clinical decision support systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u9f50\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u548c\u975e\u7ed3\u6784\u5316\u51fa\u9662\u6458\u8981\u7684\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u6811\u6a21\u578b\uff09\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6df1\u5c42\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u4fe1\u606f\u6765\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5bf9\u9f50\u7ed3\u6784\u5316EHR\u6570\u636e\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684EHR\u7f16\u7801\u5668\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u572830\u5929\u518d\u5165\u9662\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cAUROC\u63d0\u5347\u4e864.1%\uff0c\u4f18\u4e8eXGBoost\u3002", "conclusion": "\u7ed3\u5408\u4e34\u5e8a\u6587\u672c\u7684\u9886\u57df\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u5347EHR\u7ba1\u7ebf\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "relevance": 40.0}}
{"id": "2505.18137", "pdf": "https://arxiv.org/pdf/2505.18137", "abs": "https://arxiv.org/abs/2505.18137", "authors": ["Amit Kumar Kundu", "Vaishnavi Patil", "Joseph Jaja"], "title": "Boosting Open Set Recognition Performance through Modulated Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The open set recognition (OSR) problem aims to identify test samples from\nnovel semantic classes that are not part of the training classes, a task that\nis crucial in many practical scenarios. However, existing OSR methods use a\nconstant scaling factor (the temperature) to the logits before applying a loss\nfunction, which hinders the model from exploring both ends of the spectrum in\nrepresentation learning -- from instance-level to semantic-level features. In\nthis paper, we address this problem by enabling temperature-modulated\nrepresentation learning using our novel negative cosine scheduling scheme. Our\nscheduling lets the model form a coarse decision boundary at the beginning of\ntraining by focusing on fewer neighbors, and gradually prioritizes more\nneighbors to smooth out rough edges. This gradual task switching leads to a\nricher and more generalizable representation space. While other OSR methods\nbenefit by including regularization or auxiliary negative samples, such as with\nmix-up, thereby adding a significant computational overhead, our scheme can be\nfolded into any existing OSR method with no overhead. We implement the proposed\nscheme on top of a number of baselines, using both cross-entropy and\ncontrastive loss functions as well as a few other OSR methods, and find that\nour scheme boosts both the OSR performance and the closed set performance in\nmost cases, especially on the tougher semantic shift benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e29\u5ea6\u8c03\u5236\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1f\u4f59\u5f26\u8c03\u5ea6\u65b9\u6848\u6539\u8fdb\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u4efb\u52a1\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709OSR\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u6e29\u5ea6\u7f29\u653e\u56e0\u5b50\u9650\u5236\u4e86\u8868\u793a\u5b66\u4e60\u7684\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u540c\u65f6\u63a2\u7d22\u5b9e\u4f8b\u7ea7\u548c\u8bed\u4e49\u7ea7\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u8d1f\u4f59\u5f26\u8c03\u5ea6\u65b9\u6848\uff0c\u52a8\u6001\u8c03\u6574\u6e29\u5ea6\uff0c\u4f7f\u6a21\u578b\u4ece\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u51b3\u7b56\u8fb9\u754c\u9010\u6b65\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u7ebf\u548c\u635f\u5931\u51fd\u6570\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86OSR\u548c\u95ed\u96c6\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u504f\u79fb\u57fa\u51c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709OSR\u65b9\u6cd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u793a\u5b66\u4e60\u7684\u4e30\u5bcc\u6027\u548c\u6cdb\u5316\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17899", "pdf": "https://arxiv.org/pdf/2505.17899", "abs": "https://arxiv.org/abs/2505.17899", "authors": ["Romain Mussard", "Fannia Pacheco", "Maxime Berar", "Gilles Gasso", "Paul Honeine"], "title": "Universal Domain Adaptation Benchmark for Time Series Data Representation", "categories": ["cs.LG"], "comment": null, "summary": "Deep learning models have significantly improved the ability to detect\nnovelties in time series (TS) data. This success is attributed to their strong\nrepresentation capabilities. However, due to the inherent variability in TS\ndata, these models often struggle with generalization and robustness. To\naddress this, a common approach is to perform Unsupervised Domain Adaptation,\nparticularly Universal Domain Adaptation (UniDA), to handle domain shifts and\nemerging novel classes. While extensively studied in computer vision, UniDA\nremains underexplored for TS data. This work provides a comprehensive\nimplementation and comparison of state-of-the-art TS backbones in a UniDA\nframework. We propose a reliable protocol to evaluate their robustness and\ngeneralization across different domains. The goal is to provide practitioners\nwith a framework that can be easily extended to incorporate future advancements\nin UniDA and TS architectures. Our results highlight the critical influence of\nbackbone selection in UniDA performance and enable a robustness analysis across\nvarious datasets and architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u901a\u7528\u9886\u57df\u81ea\u9002\u5e94\uff08UniDA\uff09\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540cTS\u9aa8\u5e72\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u534f\u8bae\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u53d8\u5f02\u6027\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cUniDA\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5728TS\u6570\u636e\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5b9e\u73b0\u548c\u6bd4\u8f83\u591a\u79cdTS\u9aa8\u5e72\u6a21\u578b\u5728UniDA\u6846\u67b6\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u8bc4\u4f30\u534f\u8bae\u4ee5\u5206\u6790\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\u9aa8\u5e72\u6a21\u578b\u7684\u9009\u62e9\u5bf9UniDA\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u8de8\u6570\u636e\u96c6\u548c\u67b6\u6784\u7684\u9c81\u68d2\u6027\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4fbf\u4e8e\u672a\u6765UniDA\u548cTS\u67b6\u6784\u7684\u6539\u8fdb\u3002", "relevance": 40.0}}
{"id": "2505.17654", "pdf": "https://arxiv.org/pdf/2505.17654", "abs": "https://arxiv.org/abs/2505.17654", "authors": ["Ancheng Xu", "Zhihao Yang", "Jingpeng Li", "Guanghu Yuan", "Longze Chen", "Liang Yan", "Jiehui Zhou", "Zhen Qin", "Hengyun Chang", "Hamid Alinejad-Rokny", "Bo Zheng", "Min Yang"], "title": "EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.", "AI": {"tldr": "EVADE\u662f\u4e00\u4e2a\u9488\u5bf9\u7535\u5b50\u5546\u52a1\u4e2d\u89c4\u907f\u5185\u5bb9\u68c0\u6d4b\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6587\u672c\u548c\u56fe\u50cf\u6837\u672c\uff0c\u8bc4\u4f30\u4e3b\u6d41LLMs\u548cVLMs\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u4f9d\u8d56LLMs\u548cVLMs\u68c0\u6d4b\u8fdd\u89c4\u5185\u5bb9\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5bf9\u89c4\u907f\u5185\u5bb9\uff08\u8868\u9762\u5408\u89c4\u4f46\u9690\u542b\u8fdd\u89c4\uff09\u7684\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6027\u57fa\u51c6\u3002", "method": "\u63d0\u51faEVADE\u57fa\u51c6\uff0c\u5305\u542b2,833\u6587\u672c\u548c13,961\u56fe\u50cf\u6837\u672c\uff0c\u8bbe\u8ba1\u4e24\u79cd\u4efb\u52a1\uff08Single-Violation\u548cAll-in-One\uff09\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "\u4e3b\u6d41\u6a21\u578b\u5728\u89c4\u907f\u5185\u5bb9\u68c0\u6d4b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cAll-in-One\u4efb\u52a1\u663e\u793a\u6e05\u6670\u89c4\u5219\u5b9a\u4e49\u53ef\u63d0\u5347\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "EVADE\u4e3a\u89c4\u907f\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u9996\u4e2a\u4e25\u683c\u6807\u51c6\uff0c\u63ed\u793a\u591a\u6a21\u6001\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u66f4\u5b89\u5168\u7684\u7535\u5b50\u5546\u52a1\u5185\u5bb9\u5ba1\u6838\u3002", "relevance": 75.0}}
{"id": "2505.18142", "pdf": "https://arxiv.org/pdf/2505.18142", "abs": "https://arxiv.org/abs/2505.18142", "authors": ["Junfeng Wu", "Dongliang Luo", "Weizhi Zhao", "Zhihao Xie", "Yuanhao Wang", "Junyi Li", "Xudong Xie", "Yuliang Liu", "Xiang Bai"], "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation", "categories": ["cs.CV", "cs.DB"], "comment": "Benchmark, homepagee: https://wjf5203.github.io/TokBench", "summary": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nImage tokenization has significantly advanced visual generation and multimodal\nmodeling, particularly with autoregressive models due to the modeling\nsimplicity of discrete tokens. Autoregressive models typically rely on image\ntokenizers to compress images into discrete tokens for sequential prediction,\nwhereas diffusion models often operate on continuous latent space to reduce\ncomputational costs. However, both visual compression approaches inevitably\nlose visual information, thereby limiting the upper bound of visual generation\nquality. To evaluate how these compression losses affect text and faces, the\nmost human-sensitive visual elements, we first collect and curate a collection\nof text and faces images from existing datasets, ensuring clarity and\ndiversity. For text reconstruction, we employ OCR models to assess the\nrecognition accuracy of the reconstructed text, and then we measure feature\nsimilarity between original and reconstructed faces thereby quantifying faces\nreconstruction fidelity. Our method is highly lightweight, requiring just 2GB\nmemory and 4 minutes to complete evaluations. With our benchmark, we analyze\nthe reconstruction quality of text and faces at various scales across different\nimage tokenizers and VAEs. Our results demonstrate that modern visual\ntokenizers still struggle to preserve fine-grained features, particularly at\nsmaller scales. Furthermore, we extend this evaluation framework to the video,\nconducting a comprehensive analysis of video tokenizers. Additionally, we find\nthat traditional metrics fail to accurately reflect the reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u89c6\u89c9\u5206\u8bcd\u5668\u548cVAE\u5728\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7279\u5f81\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u548c\u9762\u90e8\u91cd\u5efa\u6027\u80fd\u7684\u57fa\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u4ee3\u89c6\u89c9\u5206\u8bcd\u5668\u5728\u5c0f\u5c3a\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30\u89c6\u89c9\u538b\u7f29\u65b9\u6cd5\uff08\u5982\u5206\u8bcd\u5668\u548cVAE\uff09\u5728\u91cd\u5efa\u4eba\u7c7b\u654f\u611f\u89c6\u89c9\u5185\u5bb9\uff08\u6587\u672c\u548c\u9762\u90e8\uff09\u65f6\u7684\u6027\u80fd\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6536\u96c6\u6587\u672c\u548c\u9762\u90e8\u56fe\u50cf\uff0c\u4f7f\u7528OCR\u6a21\u578b\u8bc4\u4f30\u6587\u672c\u91cd\u5efa\u51c6\u786e\u6027\uff0c\u6d4b\u91cf\u9762\u90e8\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u663e\u793a\u73b0\u4ee3\u89c6\u89c9\u5206\u8bcd\u5668\u5728\u5c0f\u5c3a\u5ea6\u4e0a\u96be\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u4f20\u7edf\u6307\u6807\u5bf9\u6587\u672c\u548c\u9762\u90e8\u91cd\u5efa\u6027\u80fd\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "conclusion": "\u7ed3\u8bba\u662f\u73b0\u6709\u89c6\u89c9\u5206\u8bcd\u5668\u4ecd\u9700\u6539\u8fdb\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u53ef\u4f5c\u4e3a\u6709\u6548\u8865\u5145\u3002", "relevance": 70.0}}
{"id": "2505.17902", "pdf": "https://arxiv.org/pdf/2505.17902", "abs": "https://arxiv.org/abs/2505.17902", "authors": ["Ignacio Cabrera Martin", "Subhaditya Mukherjee", "Almas Baimagambetov", "Joaquin Vanschoren", "Nikolaos Polatidis"], "title": "Evolving Machine Learning: A Survey", "categories": ["cs.LG"], "comment": null, "summary": "In an era defined by rapid data evolution, traditional machine learning (ML)\nmodels often fall short in adapting to dynamic environments. Evolving Machine\nLearning (EML) has emerged as a critical paradigm, enabling continuous learning\nand adaptation in real-time data streams. This survey presents a comprehensive\nanalysis of EML, focusing on five core challenges: data drift, concept drift,\ncatastrophic forgetting, skewed learning, and network adaptation. We\nsystematically review over 120 studies, categorizing state-of-the-art methods\nacross supervised, unsupervised, and semi-supervised approaches. The survey\nexplores diverse evaluation metrics, benchmark datasets, and real-world\napplications, offering a comparative lens on the effectiveness and limitations\nof current techniques. Additionally, we highlight the growing role of adaptive\nneural architectures, meta-learning, and ensemble strategies in addressing\nevolving data complexities. By synthesizing insights from recent literature,\nthis work not only maps the current landscape of EML but also identifies\ncritical gaps and opportunities for future research. Our findings aim to guide\nresearchers and practitioners in developing robust, ethical, and scalable EML\nsystems for real-world deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u5b66\u4e60\uff08EML\uff09\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u6570\u636e\u6f02\u79fb\u3001\u6982\u5ff5\u6f02\u79fb\u7b49\u4e94\u5927\u6311\u6218\uff0c\u5e76\u603b\u7ed3\u4e86\u76d1\u7763\u3001\u65e0\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u6570\u636e\u73af\u5883\uff0cEML\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u548c\u5b9e\u65f6\u9002\u5e94\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86120\u591a\u9879\u7814\u7a76\uff0c\u5206\u7c7b\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u6280\u672f\u7684\u6709\u6548\u6027\u548c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u81ea\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3001\u5143\u5b66\u4e60\u548c\u96c6\u6210\u7b56\u7565\u7684\u4f5c\u7528\u3002", "conclusion": "\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u7a7a\u767d\u548c\u673a\u4f1a\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684EML\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "relevance": 40.0}}
{"id": "2505.17656", "pdf": "https://arxiv.org/pdf/2505.17656", "abs": "https://arxiv.org/abs/2505.17656", "authors": ["Hexiang Tan", "Fei Sun", "Sha Liu", "Du Su", "Qi Cao", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Yuanzhuo Wang", "Huawei Shen", "Xueqi Cheng"], "title": "Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs", "categories": ["cs.CL"], "comment": "Underreview in EMNLP25", "summary": "As large language models (LLMs) often generate plausible but incorrect\ncontent, error detection has become increasingly critical to ensure\ntruthfulness. However, existing detection methods often overlook a critical\nproblem we term as self-consistent error, where LLMs repeatly generate the same\nincorrect response across multiple stochastic samples. This work formally\ndefines self-consistent errors and evaluates mainstream detection methods on\nthem. Our investigation reveals two key findings: (1) Unlike inconsistent\nerrors, whose frequency diminishes significantly as LLM scale increases, the\nfrequency of self-consistent errors remains stable or even increases. (2) All\nfour types of detection methshods significantly struggle to detect\nself-consistent errors. These findings reveal critical limitations in current\ndetection methods and underscore the need for improved methods. Motivated by\nthe observation that self-consistent errors often differ across LLMs, we\npropose a simple but effective cross-model probe method that fuses hidden state\nevidence from an external verifier LLM. Our method significantly enhances\nperformance on self-consistent errors across three LLM families.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u4e2d\u81ea\u6d3d\u6027\u9519\u8bef\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u5176\u6548\u679c\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u578b\u63a2\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "LLM\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u5185\u5bb9\uff0c\u800c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u81ea\u6d3d\u6027\u9519\u8bef\uff08\u5373LLM\u5728\u591a\u8f6e\u91c7\u6837\u4e2d\u91cd\u590d\u751f\u6210\u76f8\u540c\u9519\u8bef\uff09\u3002", "method": "\u5b9a\u4e49\u4e86\u81ea\u6d3d\u6027\u9519\u8bef\uff0c\u8bc4\u4f30\u4e86\u4e3b\u6d41\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5916\u90e8\u9a8c\u8bc1\u5668LLM\u9690\u85cf\u72b6\u6001\u7684\u8de8\u6a21\u578b\u63a2\u6d4b\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u81ea\u6d3d\u6027\u9519\u8bef\u7684\u9891\u7387\u968f\u6a21\u578b\u89c4\u6a21\u7a33\u5b9a\u6216\u589e\u52a0\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5bf9\u5176\u68c0\u6d4b\u6548\u679c\u5dee\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8de8\u6a21\u578b\u63a2\u6d4b\u65b9\u6cd5\u4e3a\u89e3\u51b3\u81ea\u6d3d\u6027\u9519\u8bef\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "relevance": 85.0}}
{"id": "2505.18153", "pdf": "https://arxiv.org/pdf/2505.18153", "abs": "https://arxiv.org/abs/2505.18153", "authors": ["Savya Khosla", "Sethuraman TV", "Barnett Lee", "Alexander Schwing", "Derek Hoiem"], "title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders", "categories": ["cs.CV"], "comment": null, "summary": "We introduce the Region Encoder Network (REN), a fast and effective model for\ngenerating region-based image representations using point prompts. Recent\nmethods combine class-agnostic segmenters (e.g., SAM) with patch-based image\nencoders (e.g., DINO) to produce compact and effective region representations,\nbut they suffer from high computational cost due to the segmentation step. REN\nbypasses this bottleneck using a lightweight module that directly generates\nregion tokens, enabling 60x faster token generation with 35x less memory, while\nalso improving token quality. It uses a few cross-attention blocks that take\npoint prompts as queries and features from a patch-based image encoder as keys\nand values to produce region tokens that correspond to the prompted objects. We\ntrain REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that\nit can be extended to other encoders without dedicated training. We evaluate\nREN on semantic segmentation and retrieval tasks, where it consistently\noutperforms the original encoders in both performance and compactness, and\nmatches or exceeds SAM-based region methods while being significantly faster.\nNotably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D\nbenchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle\nchallenge. Code and models are available at: https://github.com/savya08/REN.", "AI": {"tldr": "REN\u662f\u4e00\u79cd\u5feb\u901f\u6709\u6548\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u70b9\u63d0\u793a\u751f\u6210\u57fa\u4e8e\u533a\u57df\u7684\u56fe\u50cf\u8868\u793a\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u7c7b\u65e0\u5173\u5206\u5272\u5668\u548c\u57fa\u4e8e\u8865\u4e01\u7684\u56fe\u50cf\u7f16\u7801\u5668\u751f\u6210\u533a\u57df\u8868\u793a\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002REN\u65e8\u5728\u7ed5\u8fc7\u8fd9\u4e00\u74f6\u9888\uff0c\u76f4\u63a5\u751f\u6210\u533a\u57df\u6807\u8bb0\u3002", "method": "REN\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u5757\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\uff0c\u4ee5\u70b9\u63d0\u793a\u4e3a\u67e5\u8be2\uff0c\u57fa\u4e8e\u8865\u4e01\u7684\u56fe\u50cf\u7f16\u7801\u5668\u7279\u5f81\u4e3a\u952e\u548c\u503c\uff0c\u76f4\u63a5\u751f\u6210\u533a\u57df\u6807\u8bb0\u3002", "result": "REN\u5728\u8bed\u4e49\u5206\u5272\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "REN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u533a\u57df\u8868\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u7f16\u7801\u5668\u3002", "relevance": 40.0}}
{"id": "2505.17909", "pdf": "https://arxiv.org/pdf/2505.17909", "abs": "https://arxiv.org/abs/2505.17909", "authors": ["Bram Grooten", "Farid Hasanov", "Chenxiang Zhang", "Qiao Xiao", "Boqian Wu", "Zahra Atashgahi", "Ghada Sokar", "Shiwei Liu", "Lu Yin", "Elena Mocanu", "Mykola Pechenizkiy", "Decebal Constantin Mocanu"], "title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling", "categories": ["cs.LG", "cs.AI"], "comment": "Our open-source code is available at\n  https://github.com/bramgrooten/neurotrails", "summary": "Model ensembles have long been a cornerstone for improving generalization and\nrobustness in deep learning. However, their effectiveness often comes at the\ncost of substantial computational overhead. To address this issue,\nstate-of-the-art methods aim to replicate ensemble-class performance without\nrequiring multiple independently trained networks. Unfortunately, these\nalgorithms often still demand considerable compute at inference. In response to\nthese limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head\narchitecture with dynamically evolving topology. This unexplored model-agnostic\ntraining paradigm improves ensemble performance while reducing the required\nresources. We analyze the underlying reason for its effectiveness and observe\nthat the various neural trails induced by dynamic sparsity attain a\n$\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays\nefficacy with convolutional and transformer-based architectures on computer\nvision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,\namong many others, demonstrate increased accuracy and stronger robustness in\nzero-shot generalization, while requiring significantly fewer parameters.", "AI": {"tldr": "NeuroTrails\u662f\u4e00\u79cd\u7a00\u758f\u591a\u5934\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u6f14\u5316\u62d3\u6251\u7ed3\u6784\u63d0\u5347\u96c6\u6210\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "\u63d0\u51faNeuroTrails\uff0c\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u52a8\u6001\u7a00\u758f\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6027\u5b9e\u73b0\u9884\u6d4b\u591a\u6837\u6027\u7684\u2018Goldilocks zone\u2019\u3002", "result": "\u5728ResNet-50/ImageNet\u548cLLaMA-350M/C4\u7b49\u4efb\u52a1\u4e2d\uff0cNeuroTrails\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u9700\u6c42\u3002", "conclusion": "NeuroTrails\u4e3a\u9ad8\u6548\u96c6\u6210\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u3002", "relevance": 75.0}}
{"id": "2505.17663", "pdf": "https://arxiv.org/pdf/2505.17663", "abs": "https://arxiv.org/abs/2505.17663", "authors": ["Yang Xiao", "Jiashuo Wang", "Qiancheng Xu", "Changhe Song", "Chunpu Xu", "Yi Cheng", "Wenjie Li", "Pengfei Liu"], "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present \\textsc{DynToM}, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DynToM\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u52a8\u6001\u5fc3\u667a\u72b6\u6001\u8ffd\u8e2a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u968f\u7740LLM\u5728\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u53c2\u4e0e\u589e\u52a0\uff0c\u8bc4\u4f30\u5176\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u80fd\u529b\uff0c\u5c24\u5176\u662f\u52a8\u6001\u5fc3\u667a\u72b6\u6001\u8ffd\u8e2a\u80fd\u529b\uff0c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u5fc3\u667a\u72b6\u6001\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u65f6\u95f4\u6f14\u5316\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86DynToM\u57fa\u51c6\uff0c\u901a\u8fc7\u56db\u6b65\u6846\u67b6\u751f\u6210\u4e861,100\u4e2a\u793e\u4ea4\u60c5\u5883\uff0c\u5305\u542b5,500\u4e2a\u573a\u666f\u548c78,100\u4e2a\u95ee\u9898\uff0c\u5e76\u5bf910\u4e2a\u524d\u6cbfLLM\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cLLM\u7684\u5e73\u5747\u8868\u73b0\u6bd4\u4eba\u7c7b\u4f4e44.7%\uff0c\u4e14\u5728\u8ffd\u8e2a\u548c\u63a8\u7406\u5fc3\u667a\u72b6\u6001\u53d8\u5316\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u5efa\u6a21\u52a8\u6001\u4eba\u7c7b\u5fc3\u667a\u72b6\u6001\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\u3002", "relevance": 75.0}}
{"id": "2505.17918", "pdf": "https://arxiv.org/pdf/2505.17918", "abs": "https://arxiv.org/abs/2505.17918", "authors": ["Hangting Ye", "Jinmeng Li", "He Zhao", "Dandan Guo", "Yi Chang"], "title": "LLM Meeting Decision Trees on Tabular Data", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data have been playing a vital role in diverse real-world fields,\nincluding healthcare, finance, etc. With the recent success of Large Language\nModels (LLMs), early explorations of extending LLMs to the domain of tabular\ndata have been developed. Most of these LLM-based methods typically first\nserialize tabular data into natural language descriptions, and then tune LLMs\nor directly infer on these serialized data. However, these methods suffer from\ntwo key inherent issues: (i) data perspective: existing data serialization\nmethods lack universal applicability for structured tabular data, and may pose\nprivacy risks through direct textual exposure, and (ii) model perspective: LLM\nfine-tuning methods struggle with tabular data, and in-context learning\nscalability is bottle-necked by input length constraints (suitable for few-shot\nlearning). This work explores a novel direction of integrating LLMs into\ntabular data throughough logical decision tree rules as intermediaries,\nproposes a decision tree enhancer with LLM-derived rule for tabular prediction,\nDeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied\nto full data learning setting without LLM fine-tuning. Specifically, we\nleverage the reasoning ability of LLMs to redesign an improved rule given a set\nof decision tree rules. Furthermore, we provide a calibration method for\noriginal decision trees via new generated rule by LLM, which approximates the\nerror correction vector to steer the original decision tree predictions in the\ndirection of ``errors'' reducing. Finally, extensive experiments on diverse\ntabular benchmarks show that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeLTa\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u51b3\u7b56\u6811\u89c4\u5219\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u5c06LLMs\u96c6\u6210\u5230\u8868\u683c\u6570\u636e\u4e2d\uff0c\u907f\u514d\u4e86\u6570\u636e\u5e8f\u5217\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u8868\u683c\u6570\u636e\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u5e8f\u5217\u5316\u548c\u6a21\u578b\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528LLMs\u7684\u63a8\u7406\u80fd\u529b\u6539\u8fdb\u51b3\u7b56\u6811\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u6821\u51c6\u65b9\u6cd5\u4f18\u5316\u539f\u59cb\u51b3\u7b56\u6811\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u8868\u683c\u6570\u636e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeLTa\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DeLTa\u65b9\u6cd5\u4e3aLLMs\u5728\u8868\u683c\u6570\u636e\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 75.0}}
{"id": "2505.17667", "pdf": "https://arxiv.org/pdf/2505.17667", "abs": "https://arxiv.org/abs/2505.17667", "authors": ["Fanqi Wan", "Weizhou Shen", "Shengyi Liao", "Yingcheng Shi", "Chenliang Li", "Ziyi Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQwenLong-L1\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4e0a\u4e0b\u6587\u6269\u5c55\u5c06\u77ed\u4e0a\u4e0b\u6587\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LRMs\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u4ecd\u5b58\u5728\u8bad\u7ec3\u6548\u7387\u4f4e\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u4e0a\u4e0b\u6587\u6269\u5c55\u7b56\u7565\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u9884\u70ed\u9636\u6bb5\u3001\u8bfe\u7a0b\u5f15\u5bfc\u7684\u5206\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u53ca\u96be\u5ea6\u611f\u77e5\u7684\u56de\u987e\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u4e03\u4e2a\u957f\u4e0a\u4e0b\u6587\u6587\u6863\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwenLong-L1-32B\u8868\u73b0\u4f18\u4e8eOpenAI-o3-mini\u548cQwen3-235B-A22B\uff0c\u4e0eClaude-3.7-Sonnet-Thinking\u76f8\u5f53\u3002", "conclusion": "QwenLong-L1\u6846\u67b6\u4e3a\u5f00\u53d1\u5b9e\u7528\u7684\u957f\u4e0a\u4e0b\u6587LRMs\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17919", "pdf": "https://arxiv.org/pdf/2505.17919", "abs": "https://arxiv.org/abs/2505.17919", "authors": ["Mingquan Feng", "Yifan Fu", "Tongcheng Zhang", "Yu Jiang", "Yixin Huang", "Junchi Yan"], "title": "KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches", "categories": ["cs.LG"], "comment": null, "summary": "Despite the widely recognized success of residual connections in modern\nneural networks, their design principles remain largely heuristic. This paper\nintroduces KITINet (Kinetics Theory Inspired Network), a novel architecture\nthat reinterprets feature propagation through the lens of non-equilibrium\nparticle dynamics and partial differential equation (PDE) simulation. At its\ncore, we propose a residual module that models feature updates as the\nstochastic evolution of a particle system, numerically simulated via a\ndiscretized solver for the Boltzmann transport equation (BTE). This formulation\nmimics particle collisions and energy exchange, enabling adaptive feature\nrefinement via physics-informed interactions. Additionally, we reveal that this\nmechanism induces network parameter condensation during training, where\nparameters progressively concentrate into a sparse subset of dominant channels.\nExperiments on scientific computation (PDE operator), image classification\n(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent\nimprovements over classic network baselines, with negligible increase of FLOPs.", "AI": {"tldr": "KITINet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u5e73\u8861\u7c92\u5b50\u52a8\u529b\u5b66\u548cPDE\u6a21\u62df\u7684\u65b0\u578b\u6b8b\u5dee\u8fde\u63a5\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u62df\u7c92\u5b50\u7cfb\u7edf\u7684\u968f\u673a\u6f14\u5316\u5b9e\u73b0\u81ea\u9002\u5e94\u7279\u5f81\u4f18\u5316\u3002", "motivation": "\u63a2\u7d22\u6b8b\u5dee\u8fde\u63a5\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u4ece\u975e\u5e73\u8861\u7c92\u5b50\u52a8\u529b\u5b66\u548cPDE\u6a21\u62df\u7684\u89d2\u5ea6\u91cd\u65b0\u89e3\u91ca\u7279\u5f81\u4f20\u64ad\u3002", "method": "\u63d0\u51faKITINet\uff0c\u5229\u7528Boltzmann\u8f93\u8fd0\u65b9\u7a0b\u7684\u79bb\u6563\u5316\u6c42\u89e3\u5668\u6a21\u62df\u7c92\u5b50\u78b0\u649e\u548c\u80fd\u91cf\u4ea4\u6362\uff0c\u5b9e\u73b0\u7269\u7406\u542f\u53d1\u7684\u7279\u5f81\u66f4\u65b0\u3002", "result": "\u5728\u79d1\u5b66\u8ba1\u7b97\u3001\u56fe\u50cf\u5206\u7c7b\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u7f51\u7edc\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u53ef\u5ffd\u7565\u3002", "conclusion": "KITINet\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6b8b\u5dee\u8fde\u63a5\uff0c\u540c\u65f6\u8bf1\u5bfc\u7f51\u7edc\u53c2\u6570\u7a00\u758f\u5316\u3002", "relevance": 60.0}}
{"id": "2505.17671", "pdf": "https://arxiv.org/pdf/2505.17671", "abs": "https://arxiv.org/abs/2505.17671", "authors": ["Yilun Liu", "Chunguang Zhao", "Xinhua Yang", "Hongyong Zeng", "Shimin Tao", "Weibin Meng", "Minggui He", "Chang Su", "Yan Yu", "Hongxia Ma", "Li Zhang", "Daimeng Wei", "Hao Yang"], "title": "MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis", "categories": ["cs.CL"], "comment": null, "summary": "Despite doubts on data quality, instruction synthesis has been widely applied\ninto instruction tuning (IT) of LLMs as an economic and rapid alternative.\nRecent endeavors focus on improving data quality for synthesized instruction\npairs in English and have facilitated IT of English-centric LLMs. However, data\nquality issues in multilingual synthesized instruction pairs are even more\nsevere, since the common synthesizing practice is to translate English\nsynthesized data into other languages using machine translation (MT). Besides\nthe known content errors in these English synthesized data, multilingual\nsynthesized instruction data are further exposed to defects introduced by MT\nand face insufficient localization of the target languages. In this paper, we\npropose MIDB, a Multilingual Instruction Data Booster to automatically address\nthe quality issues in multilingual synthesized data. MIDB is trained on around\n36.8k revision examples across 16 languages by human linguistic experts,\nthereby can boost the low-quality data by addressing content errors and MT\ndefects, and improving localization in these synthesized data. Both automatic\nand human evaluation indicate that not only MIDB steadily improved instruction\ndata quality in 16 languages, but also the instruction-following and\ncultural-understanding abilities of multilingual LLMs fine-tuned on\nMIDB-boosted data were significantly enhanced.", "AI": {"tldr": "MIDB\u662f\u4e00\u79cd\u591a\u8bed\u8a00\u6307\u4ee4\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u4fee\u6b63\u7ffb\u8bd1\u548c\u5185\u5bb9\u9519\u8bef\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u5408\u6210\u6307\u4ee4\u6570\u636e\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u589e\u5f3a\u591a\u8bed\u8a00LLM\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u8bed\u8a00\u5408\u6210\u6307\u4ee4\u6570\u636e\u5b58\u5728\u7ffb\u8bd1\u9519\u8bef\u548c\u672c\u5730\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cdLLM\u7684\u6307\u4ee4\u8ddf\u968f\u548c\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "method": "MIDB\u57fa\u4e8e\u4eba\u7c7b\u8bed\u8a00\u4e13\u5bb6\u4fee\u8ba2\u768436.8k\u591a\u8bed\u8a00\u793a\u4f8b\u8bad\u7ec3\uff0c\u81ea\u52a8\u4fee\u6b63\u5185\u5bb9\u9519\u8bef\u548c\u7ffb\u8bd1\u7f3a\u9677\uff0c\u5e76\u63d0\u5347\u6570\u636e\u672c\u5730\u5316\u3002", "result": "\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0cMIDB\u663e\u8457\u63d0\u5347\u4e8616\u79cd\u8bed\u8a00\u7684\u6307\u4ee4\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u589e\u5f3a\u4e86\u591a\u8bed\u8a00LLM\u7684\u6027\u80fd\u3002", "conclusion": "MIDB\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u5408\u6210\u6307\u4ee4\u6570\u636e\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u591a\u8bed\u8a00LLM\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 85.0}}
{"id": "2505.17929", "pdf": "https://arxiv.org/pdf/2505.17929", "abs": "https://arxiv.org/abs/2505.17929", "authors": ["Alexander Gabitashvili", "Philipp Kellmeyer"], "title": "Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV", "categories": ["cs.LG"], "comment": null, "summary": "Intensive care unit (ICU) is a crucial hospital department that handles\nlife-threatening cases. Nowadays machine learning (ML) is being leveraged in\nhealthcare ubiquitously. In recent years, management of ICU became one of the\nmost significant parts of the hospital functionality (largely but not only due\nto the worldwide COVID-19 pandemic). This study explores multiple ML approaches\nfor predicting LOS in ICU specifically for the patients with neurological\ndiseases based on the MIMIC-IV dataset. The evaluated models include classic ML\nalgorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and\nNeural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS\nprediction is often framed as a classification task, this study categorizes LOS\ninto three groups: less than two days, less than a week, and a week or more. As\nthe first ML-based approach targeting LOS prediction for neurological disorder\npatients, this study does not aim to outperform existing methods but rather to\nassess their effectiveness in this specific context. The findings provide\ninsights into the applicability of ML techniques for improving ICU resource\nmanagement and patient care. According to the results, Random Forest model\nproved to outperform others on static, achieving an accuracy of 0.68, a\nprecision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model\noutperformed LSTM model on time-series data with an accuracy of 0.80, a\nprecision of 0.80, a recall of 0.80 and F1-score 0.80.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9884\u6d4b\u795e\u7ecf\u75be\u75c5\u60a3\u8005ICU\u4f4f\u9662\u65f6\u95f4\uff08LOS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u7ecf\u5178\u7b97\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5728ICU\u8d44\u6e90\u7ba1\u7406\u548c\u60a3\u8005\u62a4\u7406\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u795e\u7ecf\u75be\u75c5\u60a3\u8005\u7684LOS\u9884\u6d4b\u4e2d\u3002", "method": "\u4f7f\u7528\u4e86\u7ecf\u5178ML\u7b97\u6cd5\uff08\u5982KNN\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001CatBoost\uff09\u548c\u795e\u7ecf\u7f51\u7edc\uff08LSTM\u3001BERT\u3001Temporal Fusion Transformer\uff09\uff0c\u5c06LOS\u5206\u4e3a\u4e09\u7c7b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u5728\u9759\u6001\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u73870.68\uff09\uff0cBERT\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u4f18\u4e8eLSTM\uff08\u51c6\u786e\u73870.80\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660eML\u6280\u672f\u53ef\u7528\u4e8e\u4f18\u5316ICU\u8d44\u6e90\u7ba1\u7406\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u5728\u7279\u5b9a\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u6548\u679c\u3002", "relevance": 30.0}}
{"id": "2505.17682", "pdf": "https://arxiv.org/pdf/2505.17682", "abs": "https://arxiv.org/abs/2505.17682", "authors": ["Fanjin Meng", "Jingtao Ding", "Jiahui Gong", "Chen Yang", "Hong Chen", "Zuojian Wang", "Haisheng Lu", "Yong Li"], "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples.", "AI": {"tldr": "BehaviorLM\u662f\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u89e3\u51b3LLM\u5728\u9884\u6d4b\u957f\u5c3e\u884c\u4e3a\u65f6\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u5bf9\u4e0d\u5e38\u89c1\u884c\u4e3a\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u5728\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u5e38\u89c1\u884c\u4e3a\uff0c\u5bfc\u81f4\u5bf9\u957f\u5c3e\u884c\u4e3a\u7684\u9884\u6d4b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u7b2c\u4e00\u9636\u6bb5\u5fae\u8c03LLM\u4ee5\u4fdd\u7559\u901a\u7528\u884c\u4e3a\u77e5\u8bc6\uff1b2. \u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u6837\u672c\u96be\u5ea6\u5e73\u8861\u5fae\u8c03\u6240\u6709\u884c\u4e3a\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cBehaviorLM\u80fd\u7a33\u5065\u9884\u6d4b\u5e38\u89c1\u548c\u957f\u5c3e\u884c\u4e3a\uff0c\u5e76\u6709\u6548\u5229\u7528LLM\u77e5\u8bc6\u5b9e\u73b0\u5c11\u6837\u672c\u5b66\u4e60\u3002", "conclusion": "BehaviorLM\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5fae\u8c03\u663e\u8457\u63d0\u5347LLM\u5bf9\u957f\u5c3e\u884c\u4e3a\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5e38\u89c1\u884c\u4e3a\u7684\u8868\u73b0\u3002", "relevance": 75.0}}
{"id": "2505.17096", "pdf": "https://arxiv.org/pdf/2505.17096", "abs": "https://arxiv.org/abs/2505.17096", "authors": ["Sirui Li", "Linkai Peng", "Zheyuan Zhang", "Gorkem Durak", "Ulas Bagci"], "title": "TAGS: 3D Tumor-Adaptive Guidance for SAM", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Foundation models (FMs) such as CLIP and SAM have recently shown great\npromise in image segmentation tasks, yet their adaptation to 3D medical\nimaging-particularly for pathology detection and segmentation-remains\nunderexplored. A critical challenge arises from the domain gap between natural\nimages and medical volumes: existing FMs, pre-trained on 2D data, struggle to\ncapture 3D anatomical context, limiting their utility in clinical applications\nlike tumor segmentation. To address this, we propose an adaptation framework\ncalled TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D\nmedical tasks through multi-prompt fusion. By preserving most of the\npre-trained weights, our approach enhances SAM's spatial feature extraction\nusing CLIP's semantic insights and anatomy-specific prompts. Extensive\nexperiments on three open-source tumor segmentation datasets prove that our\nmodel surpasses the state-of-the-art medical image segmentation models (+46.88%\nover nnUNet), interactive segmentation frameworks, and other established\nmedical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and\nSAM-B (at least +13% over them). This highlights the robustness and\nadaptability of our proposed framework across diverse medical segmentation\ntasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAGS\u7684\u6846\u67b6\uff0c\u5c062D\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u548cSAM\uff09\u9002\u5e94\u4e8e3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u878d\u5408\u63d0\u5347\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u548cSAM\uff09\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u57283D\u533b\u5b66\u56fe\u50cf\uff08\u5982\u80bf\u7624\u5206\u5272\uff09\u4e2d\u56e0\u9886\u57df\u5dee\u8ddd\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTAGS\u6846\u67b6\uff0c\u7ed3\u5408CLIP\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u89e3\u5256\u5b66\u63d0\u793a\uff0c\u4fdd\u7559\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u589e\u5f3aSAM\u7684\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u6e90\u80bf\u7624\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0cTAGS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982nnUNet\u548c\u5176\u4ed6\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff09\uff0c\u6027\u80fd\u63d0\u5347\u81f3\u5c1113%\u3002", "conclusion": "TAGS\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.17936", "pdf": "https://arxiv.org/pdf/2505.17936", "abs": "https://arxiv.org/abs/2505.17936", "authors": ["Sebastian Gerstner", "Hinrich Sch\u00fctze"], "title": "Understanding Gated Neurons in Transformers from Their Input-Output Functionality", "categories": ["cs.LG", "cs.CL"], "comment": "31 pages, 22 figures", "summary": "Interpretability researchers have attempted to understand MLP neurons of\nlanguage models based on both the contexts in which they activate and their\noutput weight vectors. They have paid little attention to a complementary\naspect: the interactions between input and output. For example, when neurons\ndetect a direction in the input, they might add much the same direction to the\nresidual stream (\"enrichment neurons\") or reduce its presence (\"depletion\nneurons\"). We address this aspect by examining the cosine similarity between\ninput and output weights of a neuron. We apply our method to 12 models and find\nthat enrichment neurons dominate in early-middle layers whereas later layers\ntend more towards depletion. To explain this finding, we argue that enrichment\nneurons are largely responsible for enriching concept representations, one of\nthe first steps of factual recall. Our input-output perspective is a complement\nto activation-dependent analyses and to approaches that treat input and output\nseparately.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4e2dMLP\u795e\u7ecf\u5143\u7684\u8f93\u5165\u4e0e\u8f93\u51fa\u6743\u91cd\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0\u65e9\u671f\u5c42\u503e\u5411\u4e8e\u201c\u5bcc\u96c6\u795e\u7ecf\u5143\u201d\uff0c\u800c\u540e\u671f\u5c42\u503e\u5411\u4e8e\u201c\u8d2b\u5316\u795e\u7ecf\u5143\u201d\uff0c\u5e76\u89e3\u91ca\u4e86\u8fd9\u4e00\u73b0\u8c61\u4e0e\u6982\u5ff5\u8868\u793a\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u4e0a\u4e0b\u6587\u548c\u8f93\u51fa\u6743\u91cd\uff0c\u800c\u5ffd\u7565\u4e86\u8f93\u5165\u4e0e\u8f93\u51fa\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u795e\u7ecf\u5143\u8f93\u5165\u4e0e\u8f93\u51fa\u6743\u91cd\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5206\u6790\u4e8612\u4e2a\u6a21\u578b\u4e2d\u7684\u795e\u7ecf\u5143\u884c\u4e3a\u3002", "result": "\u65e9\u671f\u5c42\u4ee5\u5bcc\u96c6\u795e\u7ecf\u5143\u4e3a\u4e3b\uff0c\u540e\u671f\u5c42\u5219\u66f4\u591a\u8d2b\u5316\u795e\u7ecf\u5143\uff0c\u8fd9\u4e0e\u6982\u5ff5\u8868\u793a\u7684\u5bcc\u96c6\u76f8\u5173\u3002", "conclusion": "\u8f93\u5165-\u8f93\u51fa\u89c6\u89d2\u662f\u5bf9\u73b0\u6709\u6fc0\u6d3b\u4f9d\u8d56\u5206\u6790\u548c\u5206\u79bb\u8f93\u5165\u8f93\u51fa\u65b9\u6cd5\u7684\u8865\u5145\u3002", "relevance": 85.0}}
{"id": "2505.17691", "pdf": "https://arxiv.org/pdf/2505.17691", "abs": "https://arxiv.org/abs/2505.17691", "authors": ["Yan Yu", "Yilun Liu", "Minggui He", "Shimin Tao", "Weibin Meng", "Xinhua Yang", "Li Zhang", "Hongxia Ma", "Chang Su", "Hao Yang", "Fuliang Li"], "title": "ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used as evaluators for open-ended\ntasks, while previous research has emphasized biases in LLM evaluations, the\nissue of non-transitivity in pairwise comparisons remains unresolved:\nnon-transitive preferences for pairwise comparisons, where evaluators prefer A\nover B, B over C, but C over A. Our results suggest that low-quality training\ndata may reduce the transitivity of preferences generated by the Evaluator LLM.\nTo address this, We propose a graph-theoretic framework to analyze and mitigate\nthis problem by modeling pairwise preferences as tournament graphs. We quantify\nnon-transitivity and introduce directed graph structural entropy to measure the\noverall clarity of preferences. Our analysis reveals significant\nnon-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting\n67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting\nlow overall clarity of preferences. To address this issue, we designed a\nfiltering strategy, ELSPR, to eliminate preference data that induces\nnon-transitivity, retaining only consistent and transitive preference data for\nmodel fine-tuning. Experiments demonstrate that models fine-tuned with filtered\ndata reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease\nstructural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely\nwith human evaluators (human agreement rate improves by 0.6% and Spearman\ncorrelation increases by 0.01).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u975e\u4f20\u9012\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6570\u636e\u63d0\u5347\u8bc4\u4f30\u7684\u4f20\u9012\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "LLM\u5728\u5f00\u653e\u4efb\u52a1\u8bc4\u4f30\u4e2d\u5b58\u5728\u975e\u4f20\u9012\u6027\u504f\u597d\u95ee\u9898\uff0c\u4f4e\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u52a0\u5267\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u56fe\u8bba\u5efa\u6a21\u504f\u597d\u5173\u7cfb\uff0c\u63d0\u51fa\u7ed3\u6784\u71b5\u91cf\u5316\u975e\u4f20\u9012\u6027\uff0c\u8bbe\u8ba1\u8fc7\u6ee4\u7b56\u7565ELSPR\u4fdd\u7559\u4e00\u81f4\u6027\u6570\u636e\u3002", "result": "\u8fc7\u6ee4\u540e\u6570\u636e\u663e\u8457\u51cf\u5c11\u975e\u4f20\u9012\u6027\uff0813.78%\uff09\u548c\u7ed3\u6784\u71b5\uff080.0879\uff09\uff0c\u63d0\u5347\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "ELSPR\u7b56\u7565\u6709\u6548\u63d0\u5347LLM\u8bc4\u4f30\u7684\u4f20\u9012\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u7b5b\u9009\u63d0\u4f9b\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17939", "pdf": "https://arxiv.org/pdf/2505.17939", "abs": "https://arxiv.org/abs/2505.17939", "authors": ["Manuel Lecha", "Andrea Cavallo", "Francesca Dominici", "Ran Levi", "Alessio Del Bue", "Elvin Isufi", "Pietro Morerio", "Claudio Battiloro"], "title": "Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) excel at learning from pairwise interactions but\noften overlook multi-way and hierarchical relationships. Topological Deep\nLearning (TDL) addresses this limitation by leveraging combinatorial\ntopological spaces. However, existing TDL models are restricted to undirected\nsettings and fail to capture the higher-order directed patterns prevalent in\nmany complex systems, e.g., brain networks, where such interactions are both\nabundant and functionally significant. To fill this gap, we introduce\nSemi-Simplicial Neural Networks (SSNs), a principled class of TDL models that\noperate on semi-simplicial sets -- combinatorial structures that encode\ndirected higher-order motifs and their directional relationships. To enhance\nscalability, we propose Routing-SSNs, which dynamically select the most\ninformative relations in a learnable manner. We prove that SSNs are strictly\nmore expressive than standard graph and TDL models. We then introduce a new\nprincipled framework for brain dynamics representation learning, grounded in\nthe ability of SSNs to provably recover topological descriptors shown to\nsuccessfully characterize brain activity. Empirically, SSNs achieve\nstate-of-the-art performance on brain dynamics classification tasks,\noutperforming the second-best model by up to 27%, and message passing GNNs by\nup to 50% in accuracy. Our results highlight the potential of principled\ntopological models for learning from structured brain data, establishing a\nunique real-world case study for TDL. We also test SSNs on standard node\nclassification and edge regression tasks, showing competitive performance. We\nwill make the code and data publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Semi-Simplicial Neural Networks (SSNs)\uff0c\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u9ad8\u9636\u6709\u5411\u5173\u7cfb\uff0c\u5e76\u5728\u8111\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ec5\u9002\u7528\u4e8e\u65e0\u5411\u573a\u666f\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u9ad8\u9636\u6709\u5411\u5173\u7cfb\uff08\u5982\u8111\u7f51\u7edc\uff09\u3002SSNs\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SSNs\u57fa\u4e8e\u534a\u5355\u7eaf\u96c6\uff08semi-simplicial sets\uff09\u8bbe\u8ba1\uff0c\u80fd\u591f\u7f16\u7801\u6709\u5411\u9ad8\u9636\u6a21\u5f0f\u53ca\u5176\u65b9\u5411\u5173\u7cfb\u3002\u4e3a\u63d0\u5347\u53ef\u6269\u5c55\u6027\uff0c\u63d0\u51fa\u4e86Routing-SSNs\uff0c\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5173\u7cfb\u3002", "result": "SSNs\u5728\u8111\u52a8\u6001\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6bd4\u6b21\u4f18\u6a21\u578b\u63d0\u534727%\uff0c\u6bd4\u6d88\u606f\u4f20\u9012GNNs\u63d0\u534750%\u3002\u5728\u6807\u51c6\u8282\u70b9\u5206\u7c7b\u548c\u8fb9\u56de\u5f52\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "SSNs\u4e3a\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c24\u5176\u5728\u5904\u7406\u7ed3\u6784\u5316\u8111\u6570\u636e\u65b9\u9762\u5c55\u793a\u4e86\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17697", "pdf": "https://arxiv.org/pdf/2505.17697", "abs": "https://arxiv.org/abs/2505.17697", "authors": ["Zekai Zhao", "Qi Liu", "Kun Zhou", "Zihan Liu", "Yifei Shao", "Zhiting Hu", "Biwei Huang"], "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u653e\u5927\u5173\u952e\u6fc0\u6d3b\u548c\u63d2\u5165\u201c\u7b49\u5f85\u201d\u6807\u8bb0\u6765\u6fc0\u53d1LLMs\u7684\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u6fc0\u6d3b\u63a7\u5236\u6280\u672f\u3002", "motivation": "\u7814\u7a76LLMs\u957f\u94fe\u63a8\u7406\u80fd\u529b\u7684\u5185\u90e8\u673a\u5236\uff0c\u907f\u514d\u9ad8\u6210\u672c\u7684\u5f3a\u5316\u5b66\u4e60\u6216\u76d1\u7763\u5fae\u8c03\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5173\u952e\u6fc0\u6d3b\u7684\u52a8\u6001\u8f68\u8ff9\uff0c\u8bbe\u8ba1\u65e0\u8bad\u7ec3\u7684\u6fc0\u6d3b\u63a7\u5236\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u6fc0\u53d1\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u6548\u7387\u4f18\u4e8e\u5168LoRA\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLMs\u7684\u957f\u94fe\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17941", "pdf": "https://arxiv.org/pdf/2505.17941", "abs": "https://arxiv.org/abs/2505.17941", "authors": ["Zigeng Chen", "Xinyin Ma", "Gongfan Fang", "Ruonan Yu", "Xinchao Wang"], "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient", "categories": ["cs.LG"], "comment": "Working in progress. Code Repo:\n  https://github.com/czg1225/VeriThinker", "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker", "AI": {"tldr": "VeriThinker\u901a\u8fc7\u8f85\u52a9\u9a8c\u8bc1\u4efb\u52a1\u538b\u7f29CoT\u63a8\u7406\u94fe\uff0c\u51cf\u5c11\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u56e0\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u63a8\u7406\u94fe\u8fc7\u957f\u3001\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8f85\u52a9\u9a8c\u8bc1\u4efb\u52a1\u5fae\u8c03LRMs\uff0c\u4f7f\u5176\u66f4\u51c6\u786e\u9a8c\u8bc1CoT\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\uff0c\u4ece\u800c\u6291\u5236\u8fc7\u5ea6\u601d\u8003\u3002", "result": "\u5728MATH500\u548cAIME25\u6570\u636e\u96c6\u4e0a\uff0c\u63a8\u7406\u94fe\u957f\u5ea6\u663e\u8457\u51cf\u5c11\uff0c\u51c6\u786e\u6027\u6709\u6240\u63d0\u5347\u3002", "conclusion": "VeriThinker\u6709\u6548\u51cf\u5c11\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u53ef\u96f6\u6837\u672c\u63a8\u5e7f\u5230\u63a8\u6d4b\u63a8\u7406\u3002", "relevance": 85.0}}
{"id": "2505.17704", "pdf": "https://arxiv.org/pdf/2505.17704", "abs": "https://arxiv.org/abs/2505.17704", "authors": ["Maria Ponomareva", "Maria Petrova", "Julia Detkova", "Oleg Serikov", "Maria Yarova"], "title": "SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus", "categories": ["cs.CL"], "comment": null, "summary": "The paper deals with elaborating different approaches to the machine\nprocessing of semantic sketches. It presents the pilot open corpus of semantic\nsketches. Different aspects of creating the sketches are discussed, as well as\nthe tasks that the sketches can help to solve. Special attention is paid to the\ncreation of the machine processing tools for the corpus. For this purpose, the\nSemSketches-2021 Shared Task was organized. The participants were given the\nanonymous sketches and a set of contexts containing the necessary predicates.\nDuring the Task, one had to assign the proper contexts to the corresponding\nsketches.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u4e49\u8349\u56fe\u7684\u673a\u5668\u5904\u7406\u65b9\u6cd5\uff0c\u4ecb\u7ecd\u4e86\u5f00\u653e\u7684\u8bed\u4e49\u8349\u56fe\u8bed\u6599\u5e93\uff0c\u5e76\u8ba8\u8bba\u4e86\u8349\u56fe\u7684\u521b\u5efa\u53ca\u5176\u5e94\u7528\u4efb\u52a1\u3002\u91cd\u70b9\u662f\u4e3a\u8bed\u6599\u5e93\u5f00\u53d1\u673a\u5668\u5904\u7406\u5de5\u5177\uff0c\u901a\u8fc7SemSketches-2021\u5171\u4eab\u4efb\u52a1\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u8bed\u4e49\u8349\u56fe\u7684\u673a\u5668\u5904\u7406\uff0c\u4ee5\u89e3\u51b3\u76f8\u5173\u4efb\u52a1\u5e76\u5f00\u53d1\u5de5\u5177\u3002", "method": "\u521b\u5efa\u5f00\u653e\u7684\u8bed\u4e49\u8349\u56fe\u8bed\u6599\u5e93\uff0c\u7ec4\u7ec7SemSketches-2021\u5171\u4eab\u4efb\u52a1\uff0c\u53c2\u4e0e\u8005\u9700\u4e3a\u533f\u540d\u8349\u56fe\u5339\u914d\u4e0a\u4e0b\u6587\u3002", "result": "\u63d0\u51fa\u4e86\u8bed\u4e49\u8349\u56fe\u7684\u5904\u7406\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u901a\u8fc7\u5171\u4eab\u4efb\u52a1\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u8bed\u4e49\u8349\u56fe\u7684\u673a\u5668\u5904\u7406\u662f\u53ef\u884c\u7684\uff0c\u5171\u4eab\u4efb\u52a1\u4e3a\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u8df5\u57fa\u7840\u3002", "relevance": 30.0}}
{"id": "2505.17389", "pdf": "https://arxiv.org/pdf/2505.17389", "abs": "https://arxiv.org/abs/2505.17389", "authors": ["Jinrong Yang", "Kexun Chen", "Zhuoling Li", "Shengkai Wu", "Yong Zhao", "Liangliang Ren", "Wenqiu Luo", "Chaohui Shang", "Meiyu Zhi", "Linfeng Gao", "Mingshan Sun", "Hui Cheng"], "title": "Bootstrapping Imitation Learning for Long-horizon Manipulation via Hierarchical Data Collection Space", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Imitation learning (IL) with human demonstrations is a promising method for\nrobotic manipulation tasks. While minimal demonstrations enable robotic action\nexecution, achieving high success rates and generalization requires high cost,\ne.g., continuously adding data or incrementally conducting human-in-loop\nprocesses with complex hardware/software systems. In this paper, we rethink the\nstate/action space of the data collection pipeline as well as the underlying\nfactors responsible for the prediction of non-robust actions. To this end, we\nintroduce a Hierarchical Data Collection Space (HD-Space) for robotic imitation\nlearning, a simple data collection scheme, endowing the model to train with\nproactive and high-quality data. Specifically, We segment the fine manipulation\ntask into multiple key atomic tasks from a high-level perspective and design\natomic state/action spaces for human demonstrations, aiming to generate robust\nIL data. We conduct empirical evaluations across two simulated and five\nreal-world long-horizon manipulation tasks and demonstrate that IL policy\ntraining with HD-Space-based data can achieve significantly enhanced policy\nperformance. HD-Space allows the use of a small amount of demonstration data to\ntrain a more powerful policy, particularly for long-horizon manipulation tasks.\nWe aim for HD-Space to offer insights into optimizing data quality and guiding\ndata scaling. project page: https://hd-space-robotics.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHD-Space\u7684\u5206\u5c42\u6570\u636e\u6536\u96c6\u65b9\u6848\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u8d28\u91cf\u63d0\u5347\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6cdb\u5316\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u6570\u636e\u6536\u96c6\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u7b56\u7565\u6027\u80fd\u3002", "method": "\u63d0\u51faHD-Space\u65b9\u6848\uff0c\u5c06\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5173\u952e\u539f\u5b50\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u539f\u5b50\u72b6\u6001/\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u751f\u6210\u9c81\u68d2\u7684\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8eHD-Space\u7684\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u6570\u636e\u3002", "conclusion": "HD-Space\u4e3a\u4f18\u5316\u6570\u636e\u8d28\u91cf\u548c\u6307\u5bfc\u6570\u636e\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.17962", "pdf": "https://arxiv.org/pdf/2505.17962", "abs": "https://arxiv.org/abs/2505.17962", "authors": ["James A. Walker", "Moein Khajehnejad", "Adeel Razi"], "title": "A Principled Bayesian Framework for Training Binary and Spiking Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "We propose a Bayesian framework for training binary and spiking neural\nnetworks that achieves state-of-the-art performance without normalisation\nlayers. Unlike commonly used surrogate gradient methods -- often heuristic and\nsensitive to hyperparameter choices -- our approach is grounded in a\nprobabilistic model of noisy binary networks, enabling fully end-to-end\ngradient-based optimisation. We introduce importance-weighted straight-through\n(IW-ST) estimators, a unified class generalising straight-through and\nrelaxation-based estimators. We characterise the bias-variance trade-off in\nthis family and derive a bias-minimising objective implemented via an auxiliary\nloss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),\na variational inference framework that uses posterior noise to train Binary and\nSpiking Neural Networks with IW-ST. This Bayesian approach minimises gradient\nbias, regularises parameters, and introduces dropout-like noise. By linking\nlow-bias conditions, vanishing gradients, and the KL term, we enable training\nof deep residual networks without normalisation. Experiments on CIFAR-10, DVS\nGesture, and SHD show our method matches or exceeds existing approaches without\nnormalisation or hand-tuned gradients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u7684\u8bad\u7ec3\u4e8c\u5143\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5f52\u4e00\u5316\u5c42\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4ee3\u7406\u68af\u5ea6\uff09\u901a\u5e38\u662f\u542f\u53d1\u5f0f\u7684\u4e14\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u800c\u8d1d\u53f6\u65af\u65b9\u6cd5\u901a\u8fc7\u6982\u7387\u6a21\u578b\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u4f18\u5316\u3002", "method": "\u5f15\u5165\u4e86\u91cd\u8981\u6027\u52a0\u6743\u76f4\u901a\uff08IW-ST\uff09\u4f30\u8ba1\u5668\uff0c\u7edf\u4e00\u4e86\u76f4\u901a\u548c\u57fa\u4e8e\u677e\u5f1b\u7684\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u8f85\u52a9\u635f\u5931\u6700\u5c0f\u5316\u504f\u5dee\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86Spiking Bayesian Neural Networks\uff08SBNNs\uff09\uff0c\u5229\u7528\u540e\u9a8c\u566a\u58f0\u8fdb\u884c\u53d8\u5206\u63a8\u7406\u3002", "result": "\u5728CIFAR-10\u3001DVS Gesture\u548cSHD\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5f52\u4e00\u5316\u6216\u624b\u52a8\u8c03\u6574\u68af\u5ea6\u5373\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8d1d\u53f6\u65af\u6846\u67b6\u4e3a\u4e8c\u5143\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4f4e\u504f\u5dee\u3001\u6b63\u5219\u5316\u548c\u566a\u58f0\u9c81\u68d2\u6027\u7684\u4f18\u52bf\u3002", "relevance": 40.0}}
{"id": "2505.17712", "pdf": "https://arxiv.org/pdf/2505.17712", "abs": "https://arxiv.org/abs/2505.17712", "authors": ["Yi Su", "Jiayi Zhang", "Shu Yang", "Xinhai Wang", "Lijie Hu", "Di Wang"], "title": "Understanding How Value Neurons Shape the Generation of Specified Values in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Rapid integration of large language models (LLMs) into societal applications\nhas intensified concerns about their alignment with universal ethical\nprinciples, as their internal value representations remain opaque despite\nbehavioral alignment advancements. Current approaches struggle to\nsystematically interpret how values are encoded in neural architectures,\nlimited by datasets that prioritize superficial judgments over mechanistic\nanalysis. We introduce ValueLocate, a mechanistic interpretability framework\ngrounded in the Schwartz Values Survey, to address this gap. Our method first\nconstructs ValueInsight, a dataset that operationalizes four dimensions of\nuniversal value through behavioral contexts in the real world. Leveraging this\ndataset, we develop a neuron identification method that calculates activation\ndifferences between opposing value aspects, enabling precise localization of\nvalue-critical neurons without relying on computationally intensive attribution\nmethods. Our proposed validation method demonstrates that targeted manipulation\nof these neurons effectively alters model value orientations, establishing\ncausal relationships between neurons and value representations. This work\nadvances the foundation for value alignment by bridging psychological value\nframeworks with neuron analysis in LLMs.", "AI": {"tldr": "ValueLocate\u662f\u4e00\u4e2a\u57fa\u4e8eSchwartz Values Survey\u7684\u673a\u5236\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u4f4d\u548c\u5206\u6790LLM\u4e2d\u4ef7\u503c\u7f16\u7801\u7684\u795e\u7ecf\u5143\uff0c\u4ece\u800c\u63d0\u5347\u4ef7\u503c\u5bf9\u9f50\u7684\u900f\u660e\u5ea6\u3002", "motivation": "\u968f\u7740LLM\u5728\u793e\u4f1a\u5e94\u7528\u4e2d\u7684\u5feb\u901f\u96c6\u6210\uff0c\u5176\u5185\u90e8\u4ef7\u503c\u8868\u793a\u7684\u4e0d\u900f\u660e\u6027\u5f15\u53d1\u4e86\u5bf9\u4ef7\u503c\u5bf9\u9f50\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u89e3\u91ca\u4ef7\u503c\u5982\u4f55\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7f16\u7801\u3002", "method": "1. \u6784\u5efaValueInsight\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u73b0\u5b9e\u884c\u4e3a\u573a\u666f\u64cd\u4f5c\u5316\u56db\u79cd\u666e\u904d\u4ef7\u503c\u7ef4\u5ea6\u30022. \u5f00\u53d1\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u6cd5\uff0c\u8ba1\u7b97\u5bf9\u7acb\u4ef7\u503c\u65b9\u9762\u7684\u6fc0\u6d3b\u5dee\u5f02\uff0c\u7cbe\u786e\u5b9a\u4f4d\u5173\u952e\u795e\u7ecf\u5143\u30023. \u9a8c\u8bc1\u901a\u8fc7\u64cd\u7eb5\u8fd9\u4e9b\u795e\u7ecf\u5143\u53ef\u6709\u6548\u6539\u53d8\u6a21\u578b\u7684\u4ef7\u503c\u53d6\u5411\u3002", "result": "ValueLocate\u80fd\u591f\u7cbe\u786e\u5b9a\u4f4d\u4ef7\u503c\u5173\u952e\u795e\u7ecf\u5143\uff0c\u5e76\u9a8c\u8bc1\u5176\u4e0e\u4ef7\u503c\u8868\u793a\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4e3a\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u5236\u89e3\u91ca\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u5fc3\u7406\u5b66\u4ef7\u503c\u6846\u67b6\u4e0eLLM\u795e\u7ecf\u5143\u5206\u6790\u7ed3\u5408\uff0c\u4e3a\u4ef7\u503c\u5bf9\u9f50\u5960\u5b9a\u4e86\u66f4\u900f\u660e\u7684\u57fa\u7840\u3002", "relevance": 85.0}}
{"id": "2505.17967", "pdf": "https://arxiv.org/pdf/2505.17967", "abs": "https://arxiv.org/abs/2505.17967", "authors": ["Ionut-Vlad Modoranu", "Mher Safaryan", "Erik Schultheis", "Dan Alistarh"], "title": "SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to reduce the memory usage of adaptive optimizers by\nconstraining learning to a lower-dimensional space. Prior work typically\nprojects gradients of linear layers using approaches based on Singular Value\nDecomposition (SVD). However, applying SVD-based procedures individually to\neach layer in large models is computationally expensive and incurs additional\nmemory costs due to storing the projection matrices. In this work, we propose a\ncomputationally efficient and conceptually simple two-step procedure to\napproximate SVD-based gradient projections into lower-dimensional spaces.\nFirst, we construct a complete orthogonal basis using predefined orthogonal\nmatrices of the Discrete Cosine Transform (DCT). Second, we adaptively select\nbasis columns based on their alignment with the gradient of each layer. Each\nprojection matrix in our method is obtained via a single matrix multiplication\nfollowed by a lightweight sorting step to identify the most relevant basis\nvectors. Due to the predefined nature of the orthogonal bases, they are\ncomputed once at the start of training. During training, we store only the\nindices of the selected columns, avoiding the need to store full projection\nmatrices for each layer. Our numerical experiments on both pre-training and\nfine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, matching the performance of costly\nSVD-based methods while achieving faster runtime and reduced memory usage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e24\u6b65\u6cd5\u6765\u8fd1\u4f3cSVD\u68af\u5ea6\u6295\u5f71\uff0c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3SVD\u5728LLM\u8bad\u7ec3\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u5b9a\u4e49\u7684DCT\u6b63\u4ea4\u57fa\u548c\u81ea\u9002\u5e94\u9009\u62e9\u57fa\u5411\u91cf\u6765\u8fd1\u4f3cSVD\u6295\u5f71\u3002", "result": "\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0eSVD\u76f8\u5f53\uff0c\u4f46\u8fd0\u884c\u66f4\u5feb\u4e14\u5185\u5b58\u5360\u7528\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u79e9\u4f18\u5316\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17733", "pdf": "https://arxiv.org/pdf/2505.17733", "abs": "https://arxiv.org/abs/2505.17733", "authors": ["Maria Petrova", "Maria Ponomareva", "Alexandra Ivoylova"], "title": "The Pilot Corpus of the English Semantic Sketches", "categories": ["cs.CL"], "comment": null, "summary": "The paper is devoted to the creation of the semantic sketches for English\nverbs. The pilot corpus consists of the English-Russian sketch pairs and is\naimed to show what kind of contrastive studies the sketches help to conduct.\nSpecial attention is paid to the cross-language differences between the\nsketches with similar semantics. Moreover, we discuss the process of building a\nsemantic sketch, and analyse the mistakes that could give insight to the\nlinguistic nature of sketches.", "AI": {"tldr": "\u8bba\u6587\u81f4\u529b\u4e8e\u4e3a\u82f1\u8bed\u52a8\u8bcd\u521b\u5efa\u8bed\u4e49\u8349\u56fe\uff0c\u901a\u8fc7\u82f1\u4fc4\u5bf9\u6bd4\u7814\u7a76\u5c55\u793a\u5176\u5e94\u7528\uff0c\u5e76\u5206\u6790\u8de8\u8bed\u8a00\u5dee\u5f02\u53ca\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002", "motivation": "\u63a2\u7d22\u82f1\u8bed\u52a8\u8bcd\u7684\u8bed\u4e49\u8349\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u7814\u7a76\u63ed\u793a\u8de8\u8bed\u8a00\u5dee\u5f02\uff0c\u4e3a\u8bed\u8a00\u5b66\u5206\u6790\u63d0\u4f9b\u5de5\u5177\u3002", "method": "\u6784\u5efa\u82f1\u4fc4\u8bed\u4e49\u8349\u56fe\u5bf9\u4f5c\u4e3a\u8bd5\u70b9\u8bed\u6599\u5e93\uff0c\u5206\u6790\u8de8\u8bed\u8a00\u5dee\u5f02\u53ca\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002", "result": "\u5c55\u793a\u4e86\u8bed\u4e49\u8349\u56fe\u5728\u5bf9\u6bd4\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7684\u8bed\u8a00\u5b66\u95ee\u9898\u3002", "conclusion": "\u8bed\u4e49\u8349\u56fe\u662f\u6709\u6548\u7684\u8bed\u8a00\u5b66\u5206\u6790\u5de5\u5177\uff0c\u4f46\u6784\u5efa\u8fc7\u7a0b\u4e2d\u9700\u6ce8\u610f\u8de8\u8bed\u8a00\u5dee\u5f02\u548c\u6f5c\u5728\u9519\u8bef\u3002", "relevance": 10.0}}
{"id": "2505.17202", "pdf": "https://arxiv.org/pdf/2505.17202", "abs": "https://arxiv.org/abs/2505.17202", "authors": ["Arnav Verma", "Kushin Mukherjee", "Christopher Potts", "Elisa Kreiss", "Judith E. Fan"], "title": "CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Data visualizations are powerful tools for communicating patterns in\nquantitative data. Yet understanding any data visualization is no small feat --\nsucceeding requires jointly making sense of visual, numerical, and linguistic\ninputs arranged in a conventionalized format one has previously learned to\nparse. Recently developed vision-language models are, in principle, promising\ncandidates for developing computational models of these cognitive operations.\nHowever, it is currently unclear to what degree these models emulate human\nbehavior on tasks that involve reasoning about data visualizations. This gap\nreflects limitations in prior work that has evaluated data visualization\nunderstanding in artificial systems using measures that differ from those\ntypically used to assess these abilities in humans. Here we evaluated eight\nvision-language models on six data visualization literacy assessments designed\nfor humans and compared model responses to those of human participants. We\nfound that these models performed worse than human participants on average, and\nthis performance gap persisted even when using relatively lenient criteria to\nassess model performance. Moreover, while relative performance across items was\nsomewhat correlated between models and humans, all models produced patterns of\nerrors that were reliably distinct from those produced by human participants.\nTaken together, these findings suggest significant opportunities for further\ndevelopment of artificial systems that might serve as useful models of how\nhumans reason about data visualizations. All code and data needed to reproduce\nthese results are available at:\nhttps://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e868\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57286\u9879\u6570\u636e\u53ef\u89c6\u5316\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u666e\u904d\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u4e14\u9519\u8bef\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u5bf9\u6570\u636e\u53ef\u89c6\u5316\u7684\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u57286\u9879\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6570\u636e\u53ef\u89c6\u5316\u6d4b\u8bd5\u4e0a\u8bc4\u4f308\u79cd\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u9519\u8bef\u6a21\u5f0f\u4e5f\u4e0d\u540c\u4e8e\u4eba\u7c7b\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u73b0\u6709\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u6570\u636e\u53ef\u89c6\u5316\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "relevance": 40.0}}
{"id": "2505.17968", "pdf": "https://arxiv.org/pdf/2505.17968", "abs": "https://arxiv.org/abs/2505.17968", "authors": ["Jiayi Geng", "Howard Chen", "Dilip Arumugam", "Thomas L. Griffiths"], "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages", "summary": "Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.", "AI": {"tldr": "LLMs struggle to reverse-engineer black-box systems from passive observations but improve with active interventions, testing edge cases, and refining beliefs.", "motivation": "To explore how LLMs can identify black-box functions, supporting future autonomous AI researchers in scientific discovery.", "method": "Evaluate LLMs on three black-box systems (Program, Formal Language, Math Equation) using passive observation and active intervention.", "result": "Active intervention improves performance by addressing overcomplication and overlooking, though it falls short of Bayesian inference.", "conclusion": "Active interventions help LLMs better reverse-engineer black-box systems, aiding their use in discovery.", "relevance": 85.0}}
{"id": "2505.17746", "pdf": "https://arxiv.org/pdf/2505.17746", "abs": "https://arxiv.org/abs/2505.17746", "authors": ["Wei Huang", "Yizhe Xiong", "Xin Ye", "Zhijie Deng", "Hui Chen", "Zijia Lin", "Guiguang Ding"], "title": "Fast Quiet-STaR: Thinking Without Thought Tokens", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance across a\nrange of natural language processing tasks. However, recent advances\ndemonstrate that further gains particularly in complex reasoning tasks require\nmore than merely scaling up model sizes or training data. One promising\ndirection is to enable models to think during the reasoning process. Recently,\nQuiet STaR significantly improves reasoning by generating token-level thought\ntraces, but incurs substantial inference overhead. In this work, we propose\nFast Quiet STaR, a more efficient reasoning framework that preserves the\nbenefits of token-level reasoning while reducing computational cost. Our method\nintroduces a curriculum learning based training strategy that gradually reduces\nthe number of thought tokens, enabling the model to internalize more abstract\nand concise reasoning processes. We further extend this approach to the\nstandard Next Token Prediction (NTP) setting through reinforcement\nlearning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates\nthe need for explicit thought token generation during inference. Experiments on\nfour benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast\nQuiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy\nunder the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an\naverage accuracy improvement of 9\\% on Mistral 7B and 5.7\\% on Qwen2.5 7B,\nwhile maintaining the same inference latency. Our code will be available at\nhttps://github.com/huangwei200012/Fast-Quiet-STaR.", "AI": {"tldr": "Fast Quiet STaR\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u4fdd\u7559\u4ee4\u724c\u7ea7\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u590d\u6742\u63a8\u7406\u4efb\u52a1\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6269\u5927\u6a21\u578b\u89c4\u6a21\u6216\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51faFast Quiet STaR\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u9010\u6b65\u51cf\u5c11\u601d\u8003\u4ee4\u724c\u6570\u91cf\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6269\u5c55\u81f3\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\uff08NTP\uff09\u8bbe\u7f6e\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFast Quiet STaR\u5728\u76f8\u540c\u63a8\u7406\u65f6\u95f4\u9884\u7b97\u4e0b\u4f18\u4e8eQuiet-STaR\uff0cFast Quiet-STaR NTP\u5728Mistral 7B\u548cQwen2.5 7B\u4e0a\u5206\u522b\u5b9e\u73b0\u4e869%\u548c5.7%\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "Fast Quiet STaR\u901a\u8fc7\u9ad8\u6548\u63a8\u7406\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "relevance": 85.0}}
{"id": "2505.17426", "pdf": "https://arxiv.org/pdf/2505.17426", "abs": "https://arxiv.org/abs/2505.17426", "authors": ["Rui Wang", "Qianguo Sun", "Tianrong Chen", "Zhiyun Zeng", "Junlong Wu", "Jiaxing Zhang"], "title": "UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "The emergence of multi-codebook neutral audio codecs such as Residual Vector\nQuantization (RVQ) and Group Vector Quantization (GVQ) has significantly\nadvanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These\ncodecs are crucial in separating semantic and acoustic information while\nefficiently harnessing semantic priors. However, since semantic and acoustic\ninformation cannot be fully aligned, a significant drawback of these methods\nwhen applied to LLM-based TTS is that large language models may have limited\naccess to comprehensive audio information. To address this limitation, we\npropose DistilCodec and UniTTS, which collectively offer the following\nadvantages: 1) This method can distill a multi-codebook audio codec into a\nsingle-codebook audio codec with 32,768 codes while achieving a near 100\\%\nutilization. 2) As DistilCodec does not employ a semantic alignment scheme, a\nlarge amount of high-quality unlabeled audio (such as audiobooks with sound\neffects, songs, etc.) can be incorporated during training, further expanding\ndata diversity and broadening its applicability. 3) Leveraging the\ncomprehensive audio information modeling of DistilCodec, we integrated three\nkey tasks into UniTTS's pre-training framework: audio modality autoregression,\ntext modality autoregression, and speech-text cross-modal autoregression. This\nallows UniTTS to accept interleaved text and speech/audio prompts while\nsubstantially preserving LLM's text capabilities. 4) UniTTS employs a\nthree-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and\nAlignment. Source code and model checkpoints are publicly available at\nhttps://github.com/IDEA-Emdoor-Lab/UniTTS and\nhttps://github.com/IDEA-Emdoor-Lab/DistilCodec.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDistilCodec\u548cUniTTS\uff0c\u89e3\u51b3\u591a\u7801\u672c\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728LLM-based TTS\u4e2d\u97f3\u9891\u4fe1\u606f\u4e0d\u5168\u9762\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u7801\u672c\u7f16\u89e3\u7801\u5668\u548c\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u6846\u67b6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u7801\u672c\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff08\u5982RVQ\u548cGVQ\uff09\u5728LLM-based TTS\u4e2d\u5b58\u5728\u8bed\u4e49\u4e0e\u58f0\u5b66\u4fe1\u606f\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u5bfc\u81f4LLM\u65e0\u6cd5\u5168\u9762\u83b7\u53d6\u97f3\u9891\u4fe1\u606f\u3002", "method": "\u63d0\u51faDistilCodec\uff08\u5c06\u591a\u7801\u672c\u7f16\u89e3\u7801\u5668\u84b8\u998f\u4e3a\u5355\u7801\u672c\uff09\u548cUniTTS\uff08\u96c6\u6210\u97f3\u9891\u6a21\u6001\u81ea\u56de\u5f52\u3001\u6587\u672c\u6a21\u6001\u81ea\u56de\u5f52\u548c\u8de8\u6a21\u6001\u81ea\u56de\u5f52\u4efb\u52a1\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff09\u3002", "result": "DistilCodec\u5b9e\u73b0\u8fd1100%\u7684\u7801\u672c\u5229\u7528\u7387\uff0cUniTTS\u652f\u6301\u6587\u672c\u4e0e\u97f3\u9891\u6df7\u5408\u8f93\u5165\u5e76\u4fdd\u7559LLM\u7684\u6587\u672c\u80fd\u529b\u3002", "conclusion": "DistilCodec\u548cUniTTS\u663e\u8457\u63d0\u5347\u4e86LLM-based TTS\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\u3002", "relevance": 60.0}}
{"id": "2505.17974", "pdf": "https://arxiv.org/pdf/2505.17974", "abs": "https://arxiv.org/abs/2505.17974", "authors": ["Viktoriia Chekalina", "Daniil Moskovskiy", "Daria Cherniuk", "Maxim Kurkin", "Andrey Kuznetsov", "Evgeny Frolov"], "title": "Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Fisher information is a fundamental concept for characterizing the\nsensitivity of parameters in neural networks. However, leveraging the full\nobserved Fisher information is too expensive for large models, so most methods\nrely on simple diagonal approximations. While efficient, this approach ignores\nparameter correlations, often resulting in reduced performance on downstream\ntasks. In this work, we mitigate these limitations and propose Generalized\nFisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that\naccounts for both diagonal and off-diagonal elements of the Fisher information\nmatrix, providing a more accurate reflection of parameter importance. To make\nthe method tractable, we introduce a scalable adaptation of the\nKronecker-factored approximation algorithm for the observed Fisher information.\nWe demonstrate the effectiveness of our method on LLM compression, showing\nimprovements over existing compression baselines. For example, at a 20\ncompression rate on the MMLU benchmark, our method outperforms FWSVD, which is\nbased on a diagonal approximation of the Fisher information, by 5 percent,\nSVD-LLM by 3 percent, and ASVD by 6 percent compression rate.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGFWSVD\u7684\u540e\u8bad\u7ec3LLM\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u8003\u8651Fisher\u4fe1\u606f\u77e9\u9635\u7684\u5bf9\u89d2\u548c\u975e\u5bf9\u89d2\u5143\u7d20\uff0c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u53c2\u6570\u91cd\u8981\u6027\u3002\u8be5\u65b9\u6cd5\u5728MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f7f\u7528Fisher\u4fe1\u606f\u77e9\u9635\u7684\u5bf9\u89d2\u8fd1\u4f3c\uff0c\u5ffd\u7565\u4e86\u53c2\u6570\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faGFWSVD\u6280\u672f\uff0c\u7ed3\u5408Kronecker\u5206\u89e3\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u9ad8\u6548\u8ba1\u7b97Fisher\u4fe1\u606f\u77e9\u9635\u7684\u5bf9\u89d2\u548c\u975e\u5bf9\u89d2\u5143\u7d20\u3002", "result": "\u572820%\u538b\u7f29\u7387\u4e0b\uff0cGFWSVD\u5728MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eFWSVD\uff085%\uff09\u3001SVD-LLM\uff083%\uff09\u548cASVD\uff086%\uff09\u3002", "conclusion": "GFWSVD\u901a\u8fc7\u66f4\u5168\u9762\u5730\u5229\u7528Fisher\u4fe1\u606f\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u538b\u7f29\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.17747", "pdf": "https://arxiv.org/pdf/2505.17747", "abs": "https://arxiv.org/abs/2505.17747", "authors": ["Maureen de Seyssel", "Jie Chi", "Skyler Seto", "Maartje ter Hoeve", "Masha Fedzechkina", "Natalie Schluter"], "title": "Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a set of training-free ABX-style discrimination tasks to\nevaluate how multilingual language models represent language identity (form)\nand semantic content (meaning). Inspired from speech processing, these\nzero-shot tasks measure whether minimal differences in representation can be\nreliably detected. This offers a flexible and interpretable alternative to\nprobing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints\nand layers, we find that language discrimination declines over training and\nbecomes concentrated in lower layers, while meaning discrimination strengthens\nover time and stabilizes in deeper layers. We then explore probing tasks,\nshowing some alignment between our metrics and linguistic learning performance.\nOur results position ABX tasks as a lightweight framework for analyzing the\nstructure of multilingual representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684ABX\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u8a00\u8eab\u4efd\u548c\u8bed\u4e49\u5185\u5bb9\u7684\u8868\u793a\uff0c\u53d1\u73b0\u8bed\u8a00\u8bc6\u522b\u80fd\u529b\u968f\u8bad\u7ec3\u4e0b\u964d\uff0c\u800c\u8bed\u4e49\u8bc6\u522b\u80fd\u529b\u589e\u5f3a\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u8a00\u8eab\u4efd\u548c\u8bed\u4e49\u5185\u5bb9\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u6790\u6846\u67b6\u3002", "method": "\u91c7\u7528ABX\u4efb\u52a1\u8bc4\u4f30XLM-R\u6a21\u578b\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u548c\u5c42\u7ea7\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u8bed\u8a00\u8bc6\u522b\u80fd\u529b\u968f\u8bad\u7ec3\u4e0b\u964d\u4e14\u96c6\u4e2d\u5728\u4f4e\u5c42\uff0c\u8bed\u4e49\u8bc6\u522b\u80fd\u529b\u589e\u5f3a\u5e76\u7a33\u5b9a\u5728\u6df1\u5c42\u3002", "conclusion": "ABX\u4efb\u52a1\u53ef\u4f5c\u4e3a\u5206\u6790\u591a\u8bed\u8a00\u8868\u793a\u7ed3\u6784\u7684\u6709\u6548\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17430", "pdf": "https://arxiv.org/pdf/2505.17430", "abs": "https://arxiv.org/abs/2505.17430", "authors": ["Yongkang Yang", "Jian Zhao", "Tengfei Yang"], "title": "SEvoBench : A C++ Framework For Evolutionary Single-Objective Optimization Benchmarking", "categories": ["cs.NE", "cs.AI", "cs.MS", "math.OC"], "comment": "9 pages, 9 figures", "summary": "We present SEvoBench, a modern C++ framework for evolutionary computation\n(EC), specifically designed to systematically benchmark evolutionary\nsingle-objective optimization algorithms. The framework features modular\nimplementations of Particle Swarm Optimization (PSO) and Differential Evolution\n(DE) algorithms, organized around three core components: (1) algorithm\nconstruction with reusable modules, (2) efficient benchmark problem suites, and\n(3) parallel experimental analysis. Experimental evaluations demonstrate the\nframework's superior performance in benchmark testing and algorithm comparison.\nCase studies further validate its capabilities in algorithm hybridization and\nparameter analysis. Compared to existing frameworks, SEvoBench demonstrates\nthree key advantages: (i) highly efficient and reusable modular implementations\nof PSO and DE algorithms, (ii) accelerated benchmarking through parallel\nexecution, and (iii) enhanced computational efficiency via SIMD (Single\nInstruction Multiple Data) vectorization for large-scale problems.", "AI": {"tldr": "SEvoBench\u662f\u4e00\u4e2a\u73b0\u4ee3C++\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u5355\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5177\u6709\u6a21\u5757\u5316\u5b9e\u73b0\u3001\u5e76\u884c\u5206\u6790\u548c\u9ad8\u6548\u8ba1\u7b97\u7684\u7279\u70b9\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u5730\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u5316\u8ba1\u7b97\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u548c\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6a21\u5757\u5316\u7b97\u6cd5\u6784\u5efa\u3001\u9ad8\u6548\u57fa\u51c6\u95ee\u9898\u96c6\u548c\u5e76\u884c\u5b9e\u9a8c\u5206\u6790\u3002\u901a\u8fc7SIMD\u5411\u91cf\u5316\u548c\u5e76\u884c\u6267\u884c\u63d0\u5347\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSEvoBench\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u7b97\u6cd5\u6bd4\u8f83\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u7b97\u6cd5\u6df7\u5408\u548c\u53c2\u6570\u5206\u6790\u3002", "conclusion": "SEvoBench\u5728\u6a21\u5757\u5316\u5b9e\u73b0\u3001\u5e76\u884c\u6267\u884c\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "relevance": 20.0}}
{"id": "2505.17987", "pdf": "https://arxiv.org/pdf/2505.17987", "abs": "https://arxiv.org/abs/2505.17987", "authors": ["Weihang You", "Hanqi Jiang", "Zishuai Liu", "Zihang Xie", "Tianming Liu", "Jin Lu", "Fei Dou"], "title": "ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real world collection of Activities of Daily Living data is challenging due\nto privacy concerns, costly deployment and labeling, and the inherent sparsity\nand imbalance of human behavior. We present ADLGen, a generative framework\nspecifically designed to synthesize realistic, event triggered, and symbolic\nsensor sequences for ambient assistive environments. ADLGen integrates a\ndecoder only Transformer with sign based symbolic temporal encoding, and a\ncontext and layout aware sampling mechanism to guide generation toward\nsemantically rich and physically plausible sensor event sequences. To enhance\nsemantic fidelity and correct structural inconsistencies, we further\nincorporate a large language model into an automatic generate evaluate refine\nloop, which verifies logical, behavioral, and temporal coherence and generates\ncorrection rules without manual intervention or environment specific tuning.\nThrough comprehensive experiments with novel evaluation metrics, ADLGen is\nshown to outperform baseline generators in statistical fidelity, semantic\nrichness, and downstream activity recognition, offering a scalable and\nprivacy-preserving solution for ADL data synthesis.", "AI": {"tldr": "ADLGen\u662f\u4e00\u4e2a\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u771f\u5b9e\u7684\u3001\u4e8b\u4ef6\u89e6\u53d1\u7684\u7b26\u53f7\u4f20\u611f\u5668\u5e8f\u5217\uff0c\u9002\u7528\u4e8e\u8f85\u52a9\u73af\u5883\u3002\u5b83\u7ed3\u5408\u4e86Transformer\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210-\u8bc4\u4f30-\u4f18\u5316\u5faa\u73af\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u6536\u96c6\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u6570\u636e\u7684\u9690\u79c1\u3001\u6210\u672c\u548c\u7a00\u758f\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89e3\u7801\u5668Transformer\u548c\u7b26\u53f7\u65f6\u95f4\u7f16\u7801\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u548c\u5e03\u5c40\u611f\u77e5\u91c7\u6837\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u4f18\u5316\u3002", "result": "ADLGen\u5728\u7edf\u8ba1\u4fdd\u771f\u5ea6\u3001\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u4e0b\u6e38\u6d3b\u52a8\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u751f\u6210\u5668\u3002", "conclusion": "ADLGen\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684ADL\u6570\u636e\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.17762", "pdf": "https://arxiv.org/pdf/2505.17762", "abs": "https://arxiv.org/abs/2505.17762", "authors": ["Ziyu Ge", "Yuhao Wu", "Daniel Wai Kit Chin", "Roy Ka-Wei Lee", "Rui Cao"], "title": "Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs", "categories": ["cs.CL", "cs.IR"], "comment": "Camera-ready for IJCAI 2025, AI and Social Good", "summary": "Large Language Models (LLMs) augmented with retrieval mechanisms have\ndemonstrated significant potential in fact-checking tasks by integrating\nexternal knowledge. However, their reliability decreases when confronted with\nconflicting evidence from sources of varying credibility. This paper presents\nthe first systematic evaluation of Retrieval-Augmented Generation (RAG) models\nfor fact-checking in the presence of conflicting evidence. To support this\nstudy, we introduce \\textbf{CONFACT} (\\textbf{Con}flicting Evidence for\n\\textbf{Fact}-Checking) (Dataset available at\nhttps://github.com/zoeyyes/CONFACT), a novel dataset comprising questions\npaired with conflicting information from various sources. Extensive experiments\nreveal critical vulnerabilities in state-of-the-art RAG methods, particularly\nin resolving conflicts stemming from differences in media source credibility.\nTo address these challenges, we investigate strategies to integrate media\nbackground information into both the retrieval and generation stages. Our\nresults show that effectively incorporating source credibility significantly\nenhances the ability of RAG models to resolve conflicting evidence and improve\nfact-checking performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u578b\u5728\u5b58\u5728\u51b2\u7a81\u8bc1\u636e\u65f6\u7684\u4e8b\u5b9e\u6838\u67e5\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCONFACT\u7684\u65b0\u6570\u636e\u96c6\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709RAG\u65b9\u6cd5\u5728\u89e3\u51b3\u5a92\u4f53\u6765\u6e90\u53ef\u4fe1\u5ea6\u5dee\u5f02\u65f6\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6574\u5408\u6765\u6e90\u53ef\u4fe1\u5ea6\u4fe1\u606f\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u7684LLMs\u5728\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u6765\u6e90\u53ef\u4fe1\u5ea6\u4e0d\u540c\u7684\u51b2\u7a81\u8bc1\u636e\u65f6\u53ef\u9760\u6027\u4e0b\u964d\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86CONFACT\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30RAG\u6a21\u578b\u5728\u51b2\u7a81\u8bc1\u636e\u4e0b\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u5728\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u6574\u5408\u5a92\u4f53\u80cc\u666f\u4fe1\u606f\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u6765\u6e90\u53ef\u4fe1\u5ea6\u5dee\u5f02\u65f6\u5b58\u5728\u663e\u8457\u6f0f\u6d1e\u3002\u6574\u5408\u6765\u6e90\u53ef\u4fe1\u5ea6\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u6765\u6e90\u53ef\u4fe1\u5ea6\u4fe1\u606f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347RAG\u6a21\u578b\u5728\u51b2\u7a81\u8bc1\u636e\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.17434", "pdf": "https://arxiv.org/pdf/2505.17434", "abs": "https://arxiv.org/abs/2505.17434", "authors": ["Guanzhou Lan", "Yuqi Yang", "Anup Teejo Mathew", "Feiping Nie", "Rong Wang", "Xuelong Li", "Federico Renda", "Bin Zhao"], "title": "Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy", "categories": ["cs.RO", "cs.AI"], "comment": "11 pages,", "summary": "Goal-conditioned dynamic manipulation is inherently challenging due to\ncomplex system dynamics and stringent task constraints, particularly in\ndeformable object scenarios characterized by high degrees of freedom and\nunderactuation. Prior methods often simplify the problem to low-speed or 2D\nsettings, limiting their applicability to real-world 3D tasks. In this work, we\nexplore 3D goal-conditioned rope manipulation as a representative challenge. To\nmitigate data scarcity, we introduce a novel simulation framework and benchmark\ngrounded in reduced-order dynamics, which enables compact state representation\nand facilitates efficient policy learning. Building on this, we propose\nDynamics Informed Diffusion Policy (DIDP), a framework that integrates\nimitation pretraining with physics-informed test-time adaptation. First, we\ndesign a diffusion policy that learns inverse dynamics within the reduced-order\nspace, enabling imitation learning to move beyond na\\\"ive data fitting and\ncapture the underlying physical structure. Second, we propose a\nphysics-informed test-time adaptation scheme that imposes kinematic boundary\nconditions and structured dynamics priors on the diffusion process, ensuring\nconsistency and reliability in manipulation execution. Extensive experiments\nvalidate the proposed approach, demonstrating strong performance in terms of\naccuracy and robustness in the learned policy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u964d\u9636\u52a8\u529b\u5b66\u7684\u4eff\u771f\u6846\u67b6\u548cDIDP\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b33D\u76ee\u6807\u5bfc\u5411\u7684\u7ef3\u7d22\u64cd\u7eb5\u95ee\u9898\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u7269\u7406\u4fe1\u606f\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u3001\u6b20\u9a71\u52a8\u7cfb\u7edf\u4e2d3D\u76ee\u6807\u5bfc\u5411\u64cd\u7eb5\u7684\u6311\u6218\uff0c\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e3D\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u964d\u9636\u52a8\u529b\u5b66\u4eff\u771f\u6846\u67b6\uff0c\u63d0\u51faDIDP\u65b9\u6cd5\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u548c\u7269\u7406\u4fe1\u606f\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "DIDP\u57283D\u7ef3\u7d22\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17988", "pdf": "https://arxiv.org/pdf/2505.17988", "abs": "https://arxiv.org/abs/2505.17988", "authors": ["Yutong Chen", "Jiandong Gao", "Ji Wu"], "title": "Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "11 figs, 3 table, preprint", "summary": "R1-style Reinforcement Learning (RL) significantly enhances Large Language\nModels' reasoning capabilities, yet the mechanism behind rule-based RL remains\nunclear. We found that small-scale SFT has significant influence on RL but\nshows poor efficiency. To explain our observations, we propose an analytical\nframework and compare the efficiency of SFT and RL by measuring sample effect.\nHypothetical analysis show that SFT efficiency is limited by training data.\nGuided by our analysis, we propose Re-distillation, a technique that fine-tunes\npretrain model through small-scale distillation from the RL-trained policy.\nExperiments on Knight & Knave and MATH datasets demonstrate re-distillation's\nsurprising efficiency: re-distilled models match RL performance with far fewer\nsamples and less computation. Empirical verification shows that sample effect\nis a good indicator of performance improvements. As a result, on K&K dataset,\nour re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT\nsamples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches\nits instruct-tuned variant without RL. Our work explains several interesting\nphenomena in R1-style RL, shedding light on the mechanisms behind its empirical\nsuccess. Code is available at: https://github.com/on1262/deep-reasoning", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRe-distillation\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u84b8\u998f\u4eceRL\u8bad\u7ec3\u7684\u7b56\u7565\u4e2d\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76R1-style\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u673a\u5236\uff0c\u53d1\u73b0\u5c0f\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bf9RL\u6709\u663e\u8457\u5f71\u54cd\u4f46\u6548\u7387\u4f4e\u4e0b\uff0c\u8bd5\u56fe\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u5e76\u6539\u8fdb\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5206\u6790\u6846\u67b6\u6bd4\u8f83SFT\u548cRL\u7684\u6837\u672c\u6548\u7387\uff0c\u57fa\u4e8e\u5206\u6790\u63d0\u51faRe-distillation\u6280\u672f\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u84b8\u998f\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728Knight & Knave\u548cMATH\u6570\u636e\u96c6\u4e0a\uff0cRe-distillation\u6280\u672f\u4ee5\u66f4\u5c11\u6837\u672c\u548c\u8ba1\u7b97\u91cf\u5339\u914d\u6216\u8d85\u8d8aRL\u6027\u80fd\u3002", "conclusion": "Re-distillation\u89e3\u91ca\u4e86R1-style RL\u7684\u6210\u529f\u673a\u5236\uff0c\u4e3a\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17767", "pdf": "https://arxiv.org/pdf/2505.17767", "abs": "https://arxiv.org/abs/2505.17767", "authors": ["Weiwen Liu", "Jiarui Qin", "Xu Huang", "Xingshan Zeng", "Yunjia Xi", "Jianghao Lin", "Chuhan Wu", "Yasheng Wang", "Lifeng Shang", "Ruiming Tang", "Defu Lian", "Yong Yu", "Weinan Zhang"], "title": "The Real Barrier to LLM Agent Usability is Agentic ROI", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) agents represent a promising shift in human-AI\ninteraction, moving beyond passive prompt-response systems to autonomous agents\ncapable of reasoning, planning, and goal-directed action. Despite the\nwidespread application in specialized, high-effort tasks like coding and\nscientific research, we highlight a critical usability gap in high-demand,\nmass-market applications. This position paper argues that the limited\nreal-world adoption of LLM agents stems not only from gaps in model\ncapabilities, but also from a fundamental tradeoff between the value an agent\ncan provide and the costs incurred during real-world use. Hence, we call for a\nshift from solely optimizing model performance to a broader, utility-driven\nperspective: evaluating agents through the lens of the overall agentic return\non investment (Agent ROI). By identifying key factors that determine Agentic\nROI--information quality, agent time, and cost--we posit a zigzag development\ntrajectory in optimizing agentic ROI: first scaling up to improve the\ninformation quality, then scaling down to minimize the time and cost. We\noutline the roadmap across different development stages to bridge the current\nusability gaps, aiming to make LLM agents truly scalable, accessible, and\neffective in real-world contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLM\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4ece\u5355\u7eaf\u4f18\u5316\u6a21\u578b\u6027\u80fd\u8f6c\u5411\u4ee5\u6548\u7528\u9a71\u52a8\u7684\u89c6\u89d2\uff0c\u63d0\u51faAgent ROI\uff08\u4ee3\u7406\u6295\u8d44\u56de\u62a5\uff09\u6982\u5ff5\uff0c\u5e76\u89c4\u5212\u4e86\u4f18\u5316\u8def\u5f84\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5927\u4f17\u5e02\u573a\u5e94\u7528\u4e2d\u5b58\u5728\u53ef\u7528\u6027\u5dee\u8ddd\uff0c\u4e3b\u8981\u6e90\u4e8e\u6a21\u578b\u80fd\u529b\u4e0d\u8db3\u4e0e\u4f7f\u7528\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51faAgent ROI\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8d28\u91cf\u3001\u4ee3\u7406\u65f6\u95f4\u548c\u6210\u672c\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\u8bc4\u4f30\u4ee3\u7406\u6548\u7528\uff0c\u5e76\u89c4\u5212\u5148\u6269\u5c55\u540e\u538b\u7f29\u7684\u4f18\u5316\u8def\u5f84\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u89c6\u89d2\u548c\u4f18\u5316\u8def\u5f84\uff0c\u65e8\u5728\u89e3\u51b3LLM\u4ee3\u7406\u7684\u73b0\u5b9e\u53ef\u7528\u6027\u95ee\u9898\u3002", "conclusion": "\u547c\u5401\u4ece\u6a21\u578b\u6027\u80fd\u4f18\u5316\u8f6c\u5411\u6548\u7528\u9a71\u52a8\u7684\u53d1\u5c55\uff0c\u4ee5\u5b9e\u73b0LLM\u4ee3\u7406\u5728\u5927\u4f17\u5e02\u573a\u7684\u771f\u6b63\u666e\u53ca\u3002", "relevance": 85.0}}
{"id": "2505.17402", "pdf": "https://arxiv.org/pdf/2505.17402", "abs": "https://arxiv.org/abs/2505.17402", "authors": ["Mahmoud Chick Zaouali", "Todd Charter", "Homayoun Najjaran"], "title": "From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": null, "summary": "High-fidelity 3D reconstruction is critical for aerial inspection tasks such\nas infrastructure monitoring, structural assessment, and environmental\nsurveying. While traditional photogrammetry techniques enable geometric\nmodeling, they lack semantic interpretability, limiting their effectiveness for\nautomated inspection workflows. Recent advances in neural rendering and 3D\nGaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but\nsimilarly lack scene-level understanding.\n  In this work, we present a UAV-based pipeline that extends Feature-3DGS for\nlanguage-guided 3D segmentation. We leverage LSeg-based feature fields with\nCLIP embeddings to generate heatmaps in response to language prompts. These are\nthresholded to produce rough segmentations, and the highest-scoring point is\nthen used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view\nrenderings. Our results highlight the strengths and limitations of various\nfeature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful\nstructure in large-scale outdoor environments. We demonstrate that this hybrid\napproach enables flexible, language-driven interaction with photorealistic 3D\nreconstructions, opening new possibilities for semantic aerial inspection and\nscene understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUAV\u76843D\u91cd\u5efa\u7ba1\u9053\uff0c\u7ed3\u5408\u8bed\u8a00\u5f15\u5bfc\u76843D\u5206\u5272\u6280\u672f\uff0c\u5229\u7528CLIP\u5d4c\u5165\u548cSAM/SAM2\u5b9e\u73b0\u8bed\u4e49\u573a\u666f\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf3D\u91cd\u5efa\u6280\u672f\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u68c0\u67e5\u7684\u6548\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u76843D\u5206\u5272\u63d0\u5347\u8bed\u4e49\u573a\u666f\u7406\u89e3\u3002", "method": "\u7ed3\u5408Feature-3DGS\u3001CLIP\u5d4c\u5165\u548cSAM/SAM2\uff0c\u901a\u8fc7\u8bed\u8a00\u63d0\u793a\u751f\u6210\u70ed\u56fe\u5e76\u8fdb\u884c2D\u5206\u5272\uff0c\u5b9e\u73b0\u8bed\u4e493D\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u7279\u5f81\u573a\u9aa8\u5e72\uff08CLIP-LSeg\u3001SAM\u3001SAM2\uff09\u5728\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u9a71\u52a8\u76843D\u4ea4\u4e92\u80fd\u529b\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u4e3a\u8bed\u4e49\u7a7a\u4e2d\u68c0\u67e5\u548c\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "relevance": 20.0}}
{"id": "2505.17989", "pdf": "https://arxiv.org/pdf/2505.17989", "abs": "https://arxiv.org/abs/2505.17989", "authors": ["Benjamin Turtel", "Danny Franklin", "Kris Skotheim", "Luke Hewitt", "Philipp Schoenegger"], "title": "Outcome-based Reinforcement Learning to Predict the Future", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has boosted math and\ncoding in large language models, yet there has been little effort to extend\nRLVR into messier, real-world domains like forecasting. One sticking point is\nthat outcome-based reinforcement learning for forecasting must learn from\nbinary, delayed, and noisy rewards, a regime where standard fine-tuning is\nbrittle. We show that outcome-only online RL on a 14B model can match\nfrontier-scale accuracy and surpass it in calibration and hypothetical\nprediction market betting by adapting two leading algorithms, Group-Relative\nPolicy Optimisation (GRPO) and ReMax, to the forecasting setting. Our\nadaptations remove per-question variance scaling in GRPO, apply\nbaseline-subtracted advantages in ReMax, hydrate training with 100k temporally\nconsistent synthetic questions, and introduce lightweight guard-rails that\npenalise gibberish, non-English responses and missing rationales, enabling a\nsingle stable pass over 110k events. Scaling ReMax to 110k questions and\nensembling seven predictions yields a 14B model that matches frontier baseline\no1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in\ncalibration (ECE = 0.042, p < 0.001). A simple trading rule turns this\ncalibration edge into \\$127 of hypothetical profit versus \\$92 for o1 (p =\n0.037). This demonstrates that refined RLVR methods can convert small-scale\nLLMs into potentially economically valuable forecasting tools, with\nimplications for scaling this to larger models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.17784", "pdf": "https://arxiv.org/pdf/2505.17784", "abs": "https://arxiv.org/abs/2505.17784", "authors": ["Lukas Edman", "Helmut Schmid", "Alexander Fraser"], "title": "EXECUTE: A Multilingual Benchmark for LLM Token Understanding", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "The CUTE benchmark showed that LLMs struggle with character understanding in\nEnglish. We extend it to more languages with diverse scripts and writing\nsystems, introducing EXECUTE. Our simplified framework allows easy expansion to\nany language. Tests across multiple LLMs reveal that challenges in other\nlanguages are not always on the character level as in English. Some languages\nshow word-level processing issues, some show no issues at all. We also examine\nsub-character tasks in Chinese, Japanese, and Korean to assess LLMs'\nunderstanding of character components.", "AI": {"tldr": "EXECUTE\u6269\u5c55\u4e86CUTE\u57fa\u51c6\uff0c\u6d4b\u8bd5LLM\u5728\u591a\u8bed\u8a00\u4e2d\u7684\u5b57\u7b26\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u8bed\u8a00\u7684\u95ee\u9898\u5c42\u6b21\u4e0d\u540c\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e2d\u65e5\u97e9\u7684\u5b50\u5b57\u7b26\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u5b57\u7b26\u7406\u89e3\u80fd\u529b\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6269\u5c55CUTE\u57fa\u51c6\u81f3\u591a\u8bed\u8a00\uff08EXECUTE\uff09\uff0c\u7b80\u5316\u6846\u67b6\u4ee5\u652f\u6301\u4efb\u610f\u8bed\u8a00\uff0c\u6d4b\u8bd5\u591a\u79cdLLM\uff0c\u5e76\u5206\u6790\u5b57\u7b26\u548c\u5b50\u5b57\u7b26\u4efb\u52a1\u3002", "result": "\u4e0d\u540c\u8bed\u8a00\u7684\u95ee\u9898\u5c42\u6b21\u4e0d\u540c\uff1a\u82f1\u8bed\u4e3a\u5b57\u7b26\u7ea7\uff0c\u5176\u4ed6\u8bed\u8a00\u53ef\u80fd\u4e3a\u8bcd\u7ea7\u6216\u65e0\u95ee\u9898\uff1b\u4e2d\u65e5\u97e9\u7684\u5b50\u5b57\u7b26\u4efb\u52a1\u63ed\u793a\u4e86LLM\u5bf9\u5b57\u7b26\u7ec4\u4ef6\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "LLM\u7684\u5b57\u7b26\u7406\u89e3\u80fd\u529b\u56e0\u8bed\u8a00\u800c\u5f02\uff0c\u9700\u9488\u5bf9\u4e0d\u540c\u8bed\u8a00\u8bbe\u8ba1\u7279\u5b9a\u8bc4\u4f30\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2505.17997", "pdf": "https://arxiv.org/pdf/2505.17997", "abs": "https://arxiv.org/abs/2505.17997", "authors": ["Jintian Shao", "Yiming Cheng", "Hongyi Huang", "Beiwen Zhang", "Zhiyu Wu", "You Shan", "Mingkai Zheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization.", "AI": {"tldr": "VAPO\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u503c\u6a21\u578b\u504f\u5dee\u3001\u5f02\u6784\u5e8f\u5217\u957f\u5ea6\u548c\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u548c\u53ef\u9760\u6027\u3002\u672c\u6587\u4ece\u7406\u8bba\u89d2\u5ea6\u63a2\u8ba8\u5176\u673a\u5236\u548c\u6f5c\u5728\u9650\u5236\u3002", "motivation": "\u63a2\u8ba8VAPO\u6846\u67b6\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u5176\u5047\u8bbe\u53ef\u80fd\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\uff0c\u4ee5\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u63a8\u7406\u4ee3\u7406\u3002", "method": "\u5206\u6790VAPO\u6846\u67b6\u4e2d\u7684\u503c\u51fd\u6570\u8fd1\u4f3c\u3001\u81ea\u9002\u5e94\u4f18\u52bf\u4f30\u8ba1\u3001\u4ee4\u724c\u7ea7\u4f18\u5316\u7684\u6700\u4f18\u6027\uff0c\u4ee5\u53ca\u63a2\u7d22\u548c\u6cdb\u5316\u7684\u6311\u6218\u3002", "result": "VAPO\u5728\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5176\u7406\u8bba\u673a\u5236\u548c\u6f5c\u5728\u9650\u5236\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002", "conclusion": "VAPO\u7684\u5b9e\u8df5\u6548\u679c\u663e\u8457\uff0c\u4f46\u7406\u8bba\u7406\u89e3\u4e0d\u8db3\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u5176\u673a\u5236\u548c\u9650\u5236\u3002", "relevance": 85.0}}
{"id": "2505.17793", "pdf": "https://arxiv.org/pdf/2505.17793", "abs": "https://arxiv.org/abs/2505.17793", "authors": ["Jianxiang Zang", "Meiling Ning", "Yongda Wei", "Shihan Dou", "Jiazheng Zhang", "Nijia Mo", "Binhong Li", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion", "categories": ["cs.CL"], "comment": null, "summary": "Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u538b\u7f29\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5931\u771f\u5206\u6790\u6539\u8fdb\u4e86\u538b\u7f29\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86LM\u80fd\u529b\u7684\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u9ad8\u5ea6\u538b\u7f29\u7684LM\u8868\u793a\u7a7a\u95f4\u4f1a\u9000\u5316\u5230\u9ad8\u5ea6\u5404\u5411\u5f02\u6027\u72b6\u6001\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u8fd9\u79cd\u2018\u538b\u7f29\u9ed1\u5ba2\u2019\u73b0\u8c61\u901a\u8fc7\u727a\u7272\u7a7a\u95f4\u5747\u5300\u6027\u5236\u9020\u9ad8\u538b\u7f29\u7387\u7684\u5047\u8c61\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u7684\u538b\u7f29\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u51e0\u4f55\u5931\u771f\u5206\u6790\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u81ea\u8bc4\u4f30\u6d41\u7a0b\u4e2d\u3002", "result": "\u6539\u8fdb\u7684\u5ea6\u91cf\u4e0eLM\u7efc\u5408\u80fd\u529b\u9ad8\u5ea6\u4e00\u81f4\uff08Spearman\u76f8\u5173\u7cfb\u6570>0.9\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u538b\u7f29\u548c\u5176\u4ed6\u57fa\u4e8e\u5185\u90e8\u7ed3\u6784\u7684\u5ea6\u91cf\u3002", "conclusion": "\u538b\u7f29\u9ed1\u5ba2\u901a\u8fc7\u7ed3\u5408\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u5931\u771f\uff0c\u663e\u8457\u589e\u5f3a\u4e86LM\u7684\u4fe1\u606f\u5b66\u89e3\u91ca\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.17472", "pdf": "https://arxiv.org/pdf/2505.17472", "abs": "https://arxiv.org/abs/2505.17472", "authors": ["Jiangjie Wu", "Lixuan Chen", "Zhenghao Li", "Xin Li", "Saban Ozturk", "Lihui Wang", "Rongpin Wang", "Hongjiang Wei", "Yuyao Zhang"], "title": "SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D\nslices is crucial for clinical diagnosis. Reliable slice-to-volume registration\n(SVR)-based motion correction and super-resolution reconstruction (SRR) methods\nare essential. Deep learning (DL) has demonstrated potential in enhancing SVR\nand SRR when compared to conventional methods. However, it requires large-scale\nexternal training datasets, which are difficult to obtain for clinical fetal\nMRI. To address this issue, we propose an unsupervised iterative SVR-SRR\nframework for isotropic HR volume reconstruction. Specifically, SVR is\nformulated as a function mapping a 2D slice and a 3D target volume to a rigid\ntransformation matrix, which aligns the slice to the underlying location in the\ntarget volume. The function is parameterized by a convolutional neural network,\nwhich is trained by minimizing the difference between the volume slicing at the\npredicted position and the input slice. In SRR, a decoding network embedded\nwithin a deep image prior framework is incorporated with a comprehensive image\ndegradation model to produce the high-resolution (HR) volume. The deep image\nprior framework offers a local consistency prior to guide the reconstruction of\nHR volumes. By performing a forward degradation model, the HR volume is\noptimized by minimizing loss between predicted slices and the observed slices.\nComprehensive experiments conducted on large-magnitude motion-corrupted\nsimulation data and clinical data demonstrate the superior performance of the\nproposed framework over state-of-the-art fetal brain reconstruction frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u8fed\u4ee3SVR-SRR\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8fd0\u52a8\u4f2a\u5f71\u76842D\u5207\u7247\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u80ce\u513f\u8111MRI\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u4e34\u5e8a\u80ce\u513fMRI\u96be\u4ee5\u83b7\u53d6\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u5207\u7247\u5230\u4f53\u79ef\u7684\u914d\u51c6\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002", "result": "\u5728\u6a21\u62df\u548c\u4e34\u5e8a\u6570\u636e\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u80ce\u513f\u8111MRI\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u5916\u90e8\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.18002", "pdf": "https://arxiv.org/pdf/2505.18002", "abs": "https://arxiv.org/abs/2505.18002", "authors": ["Di Jin", "Jingyi Cao", "Xiaobao Wang", "Bingdao Feng", "Dongxiao He", "Longbiao Wang", "Jianwu Dang"], "title": "Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Graph anomaly detection aims to identify unusual patterns in graph-based\ndata, with wide applications in fields such as web security and financial fraud\ndetection. Existing methods typically rely on contrastive learning, assuming\nthat a lower similarity between a node and its local subgraph indicates\nabnormality. However, these approaches overlook a crucial limitation: the\npresence of interfering edges invalidates this assumption, since it introduces\ndisruptive noise that compromises the contrastive learning process.\nConsequently, this limitation impairs the ability to effectively learn\nmeaningful representations of normal patterns, leading to suboptimal detection\nperformance. To address this issue, we propose a Clean-View Enhanced Graph\nAnomaly Detection framework (CVGAD), which includes a multi-scale anomaly\nawareness module to identify key sources of interference in the contrastive\nlearning process. Moreover, to mitigate bias from the one-step edge removal\nprocess, we introduce a novel progressive purification module. This module\nincrementally refines the graph by iteratively identifying and removing\ninterfering edges, thereby enhancing model performance. Extensive experiments\non five benchmark datasets validate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCVGAD\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5f02\u5e38\u611f\u77e5\u6a21\u5757\u548c\u6e10\u8fdb\u51c0\u5316\u6a21\u5757\u89e3\u51b3\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5e72\u6270\u8fb9\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f46\u5e72\u6270\u8fb9\u7684\u5b58\u5728\u4f1a\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faCVGAD\u6846\u67b6\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u5f02\u5e38\u611f\u77e5\u6a21\u5757\u548c\u6e10\u8fdb\u51c0\u5316\u6a21\u5757\uff0c\u9010\u6b65\u79fb\u9664\u5e72\u6270\u8fb9\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CVGAD\u901a\u8fc7\u89e3\u51b3\u5e72\u6270\u8fb9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.17795", "pdf": "https://arxiv.org/pdf/2505.17795", "abs": "https://arxiv.org/abs/2505.17795", "authors": ["Tazeek Bin Abdur Rakib", "Ambuj Mehrish", "Lay-Ki Soon", "Wern Han Lim", "Soujanya Poria"], "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-language-model (LLM) agents excel at reactive dialogue but struggle\nwith proactive, goal-driven interactions due to myopic decoding and costly\nplanning. We introduce DialogXpert, which leverages a frozen LLM to propose a\nsmall, high-quality set of candidate actions per turn and employs a compact\nQ-network over fixed BERT embeddings trained via temporal-difference learning\nto select optimal moves within this reduced space. By tracking the user's\nemotions, DialogXpert tailors each decision to advance the task while nurturing\na genuine, empathetic connection. Across negotiation, emotional support, and\ntutoring benchmarks, DialogXpert drives conversations to under $3$ turns with\nsuccess rates exceeding 94\\% and, with a larger LLM prior, pushes success above\n97\\% while markedly improving negotiation outcomes. This framework delivers\nreal-time, strategic, and emotionally intelligent dialogue planning at scale.\nCode available at https://github.com/declare-lab/dialogxpert/", "AI": {"tldr": "DialogXpert \u662f\u4e00\u4e2a\u57fa\u4e8e\u51bb\u7ed3 LLM \u548c\u7d27\u51d1 Q \u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u76ee\u6807\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u60c5\u611f\u667a\u80fd\u3002", "motivation": "\u5f53\u524d LLM \u4ee3\u7406\u5728\u76ee\u6807\u9a71\u52a8\u7684\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u77ed\u89c6\u7684\u89e3\u7801\u548c\u9ad8\u6210\u672c\u7684\u89c4\u5212\u3002DialogXpert \u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u51bb\u7ed3\u7684 LLM \u751f\u6210\u5019\u9009\u52a8\u4f5c\uff0c\u901a\u8fc7\u7d27\u51d1 Q \u7f51\u7edc\uff08\u57fa\u4e8e BERT \u5d4c\u5165\uff09\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u60c5\u611f\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u5728\u8c08\u5224\u3001\u60c5\u611f\u652f\u6301\u548c\u8f85\u5bfc\u4efb\u52a1\u4e2d\uff0cDialogXpert \u5728 3 \u8f6e\u5185\u5b8c\u6210\u4efb\u52a1\u7684\u6210\u529f\u7387\u8d85\u8fc7 94%\uff0c\u4f7f\u7528\u66f4\u5927 LLM \u65f6\u53ef\u8fbe 97%\u3002", "conclusion": "DialogXpert \u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u6218\u7565\u6027\u548c\u60c5\u611f\u667a\u80fd\u7684\u5bf9\u8bdd\u89c4\u5212\u3002", "relevance": 85.0}}
{"id": "2505.17484", "pdf": "https://arxiv.org/pdf/2505.17484", "abs": "https://arxiv.org/abs/2505.17484", "authors": ["Hai Jiang", "Qiongting Liu", "Yuanpin Zhou", "Jiawei Pan", "Ting Song", "Yao Lu"], "title": "Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Placenta Accreta Spectrum Disorders (PAS) pose significant risks during\npregnancy, frequently leading to postpartum hemorrhage during cesarean\ndeliveries and other severe clinical complications, with bleeding severity\ncorrelating to the degree of placental invasion. Consequently, accurate\nprenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta\nincreta (PI), and placenta percreta (PP)-is crucial. However, existing\nguidelines and methodologies predominantly focus on the presence of PAS, with\nlimited research addressing subtype recognition. Additionally, previous\nmulti-class diagnostic efforts have primarily relied on inefficient two-stage\ncascaded binary classification tasks. In this study, we propose a novel\nconvolutional neural network (CNN) architecture designed for efficient\none-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic\nresonance imaging (MRI) slices. Our model features two branches: the main\nclassification branch utilizes a residual block architecture comprising\nmultiple residual blocks, while the second branch integrates anatomical\nfeatures of the uteroplacental area and the adjacent uterine serous layer to\nenhance the model's attention during classification. Furthermore, we implement\na multitask learning strategy to leverage both branches effectively.\nExperiments conducted on a real clinical dataset demonstrate that our model\nachieves state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u80ce\u76d8\u690d\u5165\u8c31\u7cfb\u969c\u788d\uff08PAS\uff09\u53ca\u5176\u4e9a\u578b\u9ad8\u6548\u591a\u7c7b\u8bca\u65ad\u7684\u65b0\u578bCNN\u67b6\u6784\uff0c\u57fa\u4e8eMRI\u6570\u636e\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8PAS\u5b58\u5728\u6027\uff0c\u800c\u4e9a\u578b\u8bc6\u522b\u7814\u7a76\u6709\u9650\uff0c\u4e14\u591a\u91c7\u7528\u4f4e\u6548\u7684\u4e24\u9636\u6bb5\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u5206\u652fCNN\u67b6\u6784\uff0c\u4e3b\u5206\u652f\u91c7\u7528\u6b8b\u5dee\u5757\u7ed3\u6784\uff0c\u6b21\u5206\u652f\u6574\u5408\u89e3\u5256\u7279\u5f81\u4ee5\u589e\u5f3a\u5206\u7c7b\u6ce8\u610f\u529b\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aPAS\u4e9a\u578b\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4e00\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 10.0}}
{"id": "2505.18003", "pdf": "https://arxiv.org/pdf/2505.18003", "abs": "https://arxiv.org/abs/2505.18003", "authors": ["Joshua Clymer", "Jonah Weinbaum", "Robert Kirk", "Kimberly Mai", "Selena Zhang", "Xander Davies"], "title": "An Example Safety Case for Safeguards Against Misuse", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence\nthat is often difficult to connect to real-world decisions. To bridge this gap,\nwe describe an end-to-end argument (a \"safety case\") that misuse safeguards\nreduce the risk posed by an AI assistant to low levels. We first describe how a\nhypothetical developer red teams safeguards, estimating the effort required to\nevade them. Then, the developer plugs this estimate into a quantitative \"uplift\nmodel\" to determine how much barriers introduced by safeguards dissuade misuse\n(https://www.aimisusemodel.com/). This procedure provides a continuous signal\nof risk during deployment that helps the developer rapidly respond to emerging\nthreats. Finally, we describe how to tie these components together into a\nsimple safety case. Our work provides one concrete path -- though not the only\npath -- to rigorously justifying AI misuse risks are low.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5b89\u5168\u8bba\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30AI\u6ee5\u7528\u9632\u62a4\u63aa\u65bd\u7684\u6548\u679c\uff0c\u8bc1\u660e\u5176\u80fd\u5c06AI\u52a9\u624b\u5e26\u6765\u7684\u98ce\u9669\u964d\u81f3\u4f4e\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709AI\u6ee5\u7528\u9632\u62a4\u63aa\u65bd\u8bc4\u4f30\u8bc1\u636e\u5206\u6563\uff0c\u96be\u4ee5\u652f\u6301\u5b9e\u9645\u51b3\u7b56\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u98ce\u9669\u5e76\u5feb\u901f\u54cd\u5e94\u5a01\u80c1\u3002", "method": "\u5f00\u53d1\u8005\u901a\u8fc7\u7ea2\u961f\u6d4b\u8bd5\u8bc4\u4f30\u9632\u62a4\u63aa\u65bd\u7684\u89c4\u907f\u96be\u5ea6\uff0c\u5e76\u5c06\u5176\u8f93\u5165\u5b9a\u91cf\u201c\u63d0\u5347\u6a21\u578b\u201d\u4ee5\u91cf\u5316\u9632\u62a4\u63aa\u65bd\u5bf9\u6ee5\u7528\u7684\u5a01\u6151\u6548\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u90e8\u7f72\u671f\u95f4\u98ce\u9669\u7684\u8fde\u7eed\u4fe1\u53f7\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u5e94\u5bf9\u65b0\u5174\u5a01\u80c1\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e25\u683c\u8bc1\u660eAI\u6ee5\u7528\u98ce\u9669\u4f4e\u63d0\u4f9b\u4e86\u4e00\u6761\u5177\u4f53\u8def\u5f84\u3002", "relevance": 80.0}}
{"id": "2505.17813", "pdf": "https://arxiv.org/pdf/2505.17813", "abs": "https://arxiv.org/abs/2505.17813", "authors": ["Michael Hassid", "Gabriel Synnaeve", "Yossi Adi", "Roy Schwartz"], "title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Under review", "summary": "Reasoning large language models (LLMs) heavily rely on scaling test-time\ncompute to perform complex reasoning tasks by generating extensive \"thinking\"\nchains. While demonstrating impressive results, this approach incurs\nsignificant computational costs and inference time. In this work, we challenge\nthe assumption that long thinking chains results in better reasoning\ncapabilities. We first demonstrate that shorter reasoning chains within\nindividual questions are significantly more likely to yield correct answers -\nup to 34.5% more accurate than the longest chain sampled for the same question.\nBased on these results, we suggest short-m@k, a novel reasoning LLM inference\nmethod. Our method executes k independent generations in parallel and halts\ncomputation once the first m thinking processes are done. The final answer is\nchosen using majority voting among these m chains. Basic short-1@k demonstrates\nsimilar or even superior performance over standard majority voting in\nlow-compute settings - using up to 40% fewer thinking tokens. short-3@k, while\nslightly less efficient than short-1@k, consistently surpasses majority voting\nacross all compute budgets, while still being substantially faster (up to 33%\nwall time reduction). Inspired by our results, we finetune an LLM using short,\nlong, and randomly selected reasoning chains. We then observe that training on\nthe shorter ones leads to better performance. Our findings suggest rethinking\ncurrent methods of test-time compute in reasoning LLMs, emphasizing that longer\n\"thinking\" does not necessarily translate to improved performance and can,\ncounter-intuitively, lead to degraded results.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u957f\u63a8\u7406\u94fe\u63d0\u5347LLM\u6027\u80fd\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u77ed\u63a8\u7406\u94fe\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u5e76\u8bbe\u8ba1\u4e86short-m@k\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7814\u7a76\u957f\u63a8\u7406\u94fe\u662f\u5426\u771f\u7684\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u77ed\u94fe\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u4ece\u800c\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fashort-m@k\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u884c\u751f\u6210k\u6761\u63a8\u7406\u94fe\uff0c\u9009\u62e9\u524dm\u6761\u8fdb\u884c\u591a\u6570\u6295\u7968\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "result": "\u77ed\u63a8\u7406\u94fe\u6bd4\u957f\u94fe\u51c6\u786e\u7387\u63d0\u534734.5%\uff0cshort-m@k\u65b9\u6cd5\u51cf\u5c1140%\u8ba1\u7b97\u91cf\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed33%\u3002", "conclusion": "\u77ed\u63a8\u7406\u94fe\u66f4\u9ad8\u6548\uff0c\u957f\u94fe\u53ef\u80fd\u9002\u5f97\u5176\u53cd\uff0c\u5efa\u8bae\u91cd\u65b0\u601d\u8003LLM\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u5206\u914d\u7b56\u7565\u3002", "relevance": 85.0}}
{"id": "2505.17528", "pdf": "https://arxiv.org/pdf/2505.17528", "abs": "https://arxiv.org/abs/2505.17528", "authors": ["Hai Jiang", "Chushan Zheng", "Jiawei Pan", "Yuanpin Zhou", "Qiongting Liu", "Xiang Zhang", "Jun Shen", "Yao Lu"], "title": "DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Accurate assessment of metastatic burden in axillary lymph nodes\nis crucial for guiding breast cancer treatment decisions, yet conventional\nimaging modalities struggle to differentiate metastatic burden levels and\ncapture comprehensive lymph node characteristics. This study leverages\ndual-energy computed tomography (DECT) to exploit spectral-spatial information\nfor improved multi-class classification. Purpose: To develop a noninvasive\nDECT-based model classifying sentinel lymph nodes into three categories: no\nmetastasis ($N_0$), low metastatic burden ($N_{+(1-2)}$), and heavy metastatic\nburden ($N_{+(\\geq3)}$), thereby aiding therapeutic planning. Methods: We\npropose a novel space-squeeze method combining two innovations: (1) a\nchannel-wise attention mechanism to compress and recalibrate spectral-spatial\nfeatures across 11 energy levels, and (2) virtual class injection to sharpen\ninter-class boundaries and compact intra-class variations in the representation\nspace. Results: Evaluated on 227 biopsy-confirmed cases, our method achieved an\naverage test AUC of 0.86 (95% CI: 0.80-0.91) across three cross-validation\nfolds, outperforming established CNNs (VGG, ResNet, etc). The channel-wise\nattention and virtual class components individually improved AUC by 5.01% and\n5.87%, respectively, demonstrating complementary benefits. Conclusions: The\nproposed framework enhances diagnostic AUC by effectively integrating DECT's\nspectral-spatial data and mitigating class ambiguity, offering a promising tool\nfor noninvasive metastatic burden assessment in clinical practice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u80fdCT\uff08DECT\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u538b\u7f29\u548c\u865a\u62df\u7c7b\u522b\u6ce8\u5165\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u5bf9\u4e73\u817a\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u8d1f\u62c5\u7684\u591a\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u5f71\u50cf\u5b66\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u8d1f\u62c5\uff0c\u5f71\u54cd\u6cbb\u7597\u51b3\u7b56\u3002\u5229\u7528DECT\u7684\u5149\u8c31\u7a7a\u95f4\u4fe1\u606f\uff0c\u5f00\u53d1\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u5206\u7c7b\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u538b\u7f29\u5149\u8c31\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u7c7b\u522b\u6ce8\u5165\u6280\u672f\u4f18\u5316\u7c7b\u95f4\u8fb9\u754c\u548c\u7c7b\u5185\u7d27\u51d1\u6027\u3002", "result": "\u5728227\u4f8b\u6d3b\u68c0\u786e\u8ba4\u7684\u75c5\u4f8b\u4e2d\uff0c\u5e73\u5747\u6d4b\u8bd5AUC\u4e3a0.86\uff0c\u4f18\u4e8e\u73b0\u6709CNN\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6574\u5408DECT\u6570\u636e\uff0c\u51cf\u5c11\u5206\u7c7b\u6a21\u7cca\u6027\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u8bc4\u4f30\u5de5\u5177\u3002", "relevance": 10.0}}
{"id": "2505.18005", "pdf": "https://arxiv.org/pdf/2505.18005", "abs": "https://arxiv.org/abs/2505.18005", "authors": ["Sergio Calo", "Anders Jonsson", "Gergely Neu", "Ludovic Schwartz", "Javier Segovia-Aguas"], "title": "Distances for Markov chains from sample streams", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Bisimulation metrics are powerful tools for measuring similarities between\nstochastic processes, and specifically Markov chains. Recent advances have\nuncovered that bisimulation metrics are, in fact, optimal-transport distances,\nwhich has enabled the development of fast algorithms for computing such metrics\nwith provable accuracy and runtime guarantees. However, these recent methods,\nas well as all previously known methods, assume full knowledge of the\ntransition dynamics. This is often an impractical assumption in most real-world\nscenarios, where typically only sample trajectories are available. In this\nwork, we propose a stochastic optimization method that addresses this\nlimitation and estimates bisimulation metrics based on sample access, without\nrequiring explicit transition models. Our approach is derived from a new linear\nprogramming (LP) formulation of bisimulation metrics, which we solve using a\nstochastic primal-dual optimization method. We provide theoretical guarantees\non the sample complexity of the algorithm and validate its effectiveness\nthrough a series of empirical evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u7684\u968f\u673a\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u53cc\u6a21\u62df\u5ea6\u91cf\uff0c\u65e0\u9700\u663e\u5f0f\u8f6c\u79fb\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u5df2\u77e5\u8f6c\u79fb\u52a8\u6001\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u65b0\u7684\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u516c\u5f0f\uff0c\u91c7\u7528\u968f\u673a\u5bf9\u5076\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u8bbf\u95ee\u4e0b\u6709\u6548\u4f30\u8ba1\u53cc\u6a21\u62df\u5ea6\u91cf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17816", "pdf": "https://arxiv.org/pdf/2505.17816", "abs": "https://arxiv.org/abs/2505.17816", "authors": ["Hei Yi Mak", "Tan Lee"], "title": "Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong", "categories": ["cs.CL"], "comment": "Proceedings of the 2021 5th International Conference on Natural\n  Language Processing and Information Retrieval", "summary": "The majority of inhabitants in Hong Kong are able to read and write in\nstandard Chinese but use Cantonese as the primary spoken language in daily\nlife. Spoken Cantonese can be transcribed into Chinese characters, which\nconstitute the so-called written Cantonese. Written Cantonese exhibits\nsignificant lexical and grammatical differences from standard written Chinese.\nThe rise of written Cantonese is increasingly evident in the cyber world. The\ngrowing interaction between Mandarin speakers and Cantonese speakers is leading\nto a clear demand for automatic translation between Chinese and Cantonese. This\npaper describes a transformer-based neural machine translation (NMT) system for\nwritten-Chinese-to-written-Cantonese translation. Given that parallel text data\nof Chinese and Cantonese are extremely scarce, a major focus of this study is\non the effort of preparing good amount of training data for NMT. In addition to\ncollecting 28K parallel sentences from previous linguistic studies and\nscattered internet resources, we devise an effective approach to obtaining 72K\nparallel sentences by automatically extracting pairs of semantically similar\nsentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.\nWe show that leveraging highly similar sentence pairs mined from Wikipedia\nimproves translation performance in all test sets. Our system outperforms Baidu\nFanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU\nscores. Translation examples reveal that our system is able to capture\nimportant linguistic transformations between standard Chinese and spoken\nCantonese.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u7528\u4e8e\u6807\u51c6\u4e2d\u6587\u4e0e\u7ca4\u8bed\u4e66\u9762\u8bed\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u5e76\u901a\u8fc7\u6316\u6398\u7ef4\u57fa\u767e\u79d1\u4e2d\u7684\u76f8\u4f3c\u53e5\u5b50\u5bf9\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u7ca4\u8bed\u4e66\u9762\u8bed\u5728\u7f51\u7edc\u4e2d\u7684\u5174\u8d77\uff0c\u4ee5\u53ca\u666e\u901a\u8bdd\u4e0e\u7ca4\u8bed\u4f7f\u7528\u8005\u4e4b\u95f4\u7684\u4e92\u52a8\u589e\u52a0\uff0c\u5bf9\u4e2d\u6587\u4e0e\u7ca4\u8bed\u81ea\u52a8\u7ffb\u8bd1\u7684\u9700\u6c42\u65e5\u76ca\u660e\u663e\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u6536\u96c628K\u5e73\u884c\u53e5\u5bf9\u548c\u4ece\u7ef4\u57fa\u767e\u79d1\u4e2d\u81ea\u52a8\u63d0\u53d672K\u76f8\u4f3c\u53e5\u5bf9\uff0c\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u7cfb\u7edf\u5728BLEU\u5206\u6570\u4e0a\u4f18\u4e8e\u767e\u5ea6\u7ffb\u8bd1\u76846/8\u6d4b\u8bd5\u96c6\uff0c\u5e76\u80fd\u6355\u6349\u4e2d\u6587\u4e0e\u7ca4\u8bed\u95f4\u7684\u5173\u952e\u8bed\u8a00\u8f6c\u6362\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u6316\u6398\u548cTransformer\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u5230\u7ca4\u8bed\u7ffb\u8bd1\u3002", "relevance": 40.0}}
{"id": "2505.17544", "pdf": "https://arxiv.org/pdf/2505.17544", "abs": "https://arxiv.org/abs/2505.17544", "authors": ["Ruiqi Xing"], "title": "FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "15 pages, 1 figure", "summary": "Medical image segmentation faces persistent challenges due to severe class\nimbalance and the frequency-specific distribution of anatomical structures.\nMost conventional CNN-based methods operate in the spatial domain and struggle\nto capture minority class signals, often affected by frequency aliasing and\nlimited spectral selectivity. Transformer-based models, while powerful in\nmodeling global dependencies, tend to overlook critical local details necessary\nfor fine-grained segmentation. To overcome these limitations, we propose\nFreqU-FNet, a novel U-shaped segmentation architecture operating in the\nfrequency domain. Our framework incorporates a Frequency Encoder that leverages\nLow-Pass Frequency Convolution and Daubechies wavelet-based downsampling to\nextract multi-scale spectral features. To reconstruct fine spatial details, we\nintroduce a Spatial Learnable Decoder (SLD) equipped with an adaptive\nmulti-branch upsampling strategy. Furthermore, we design a frequency-aware loss\n(FAL) function to enhance minority class learning. Extensive experiments on\nmultiple medical segmentation benchmarks demonstrate that FreqU-FNet\nconsistently outperforms both CNN and Transformer baselines, particularly in\nhandling under-represented classes, by effectively exploiting discriminative\nfrequency bands.", "AI": {"tldr": "FreqU-FNet\u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u57df\u7684U\u5f62\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7\u4f4e\u9891\u5377\u79ef\u548c\u5c0f\u6ce2\u4e0b\u91c7\u6837\u63d0\u53d6\u591a\u5c3a\u5ea6\u9891\u8c31\u7279\u5f81\uff0c\u7ed3\u5408\u7a7a\u95f4\u53ef\u5b66\u4e60\u89e3\u7801\u5668\u548c\u9891\u7387\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5c11\u6570\u7c7b\u7684\u8868\u73b0\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9891\u7387\u5206\u5e03\u95ee\u9898\uff0c\u4f20\u7edfCNN\u548cTransformer\u65b9\u6cd5\u5728\u6355\u83b7\u5c11\u6570\u7c7b\u4fe1\u53f7\u548c\u5c40\u90e8\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFreqU-FNet\uff0c\u5305\u62ec\u9891\u7387\u7f16\u7801\u5668\uff08\u4f4e\u9891\u5377\u79ef\u548c\u5c0f\u6ce2\u4e0b\u91c7\u6837\uff09\u3001\u7a7a\u95f4\u53ef\u5b66\u4e60\u89e3\u7801\u5668\u548c\u9891\u7387\u611f\u77e5\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFreqU-FNet\u663e\u8457\u4f18\u4e8eCNN\u548cTransformer\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u5c11\u6570\u7c7b\u5904\u7406\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "FreqU-FNet\u901a\u8fc7\u6709\u6548\u5229\u7528\u9891\u7387\u57df\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5c40\u90e8\u7ec6\u8282\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.18017", "pdf": "https://arxiv.org/pdf/2505.18017", "abs": "https://arxiv.org/abs/2505.18017", "authors": ["Matthieu Blanke", "Yongquan Qu", "Sara Shamekh", "Pierre Gentine"], "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling", "categories": ["cs.LG"], "comment": null, "summary": "Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSplit Augmented Langevin (SAL)\u7684\u65b0\u91c7\u6837\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5f3a\u5236\u6267\u884c\u7269\u7406\u7ea6\u675f\uff0c\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u5e76\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u7cfb\u7edf\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u7269\u7406\u7ea6\u675f\u7f3a\u4e4f\u4fdd\u8bc1\u7684\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e25\u683c\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eLangevin\u52a8\u529b\u5b66\u7684\u53d8\u5206\u516c\u5f0f\uff0c\u63d0\u51faSAL\u7b97\u6cd5\uff0c\u901a\u8fc7\u53d8\u91cf\u5206\u88c2\u9010\u6b65\u5f3a\u5236\u6267\u884c\u7ea6\u675f\uff0c\u5e76\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5e94\u7528SAL\u7b97\u6cd5\uff0c\u6210\u529f\u751f\u6210\u4e86\u6ee1\u8db3\u80fd\u91cf\u548c\u8d28\u91cf\u5b88\u6052\u5b9a\u5f8b\u7684\u7269\u7406\u573a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u5b88\u6052\u91cf\u7684\u4fdd\u6301\u3002", "conclusion": "SAL\u7b97\u6cd5\u4e3a\u751f\u6210\u6a21\u578b\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u9645\u6548\u679c\u9a8c\u8bc1\u3002", "relevance": 60.0}}
{"id": "2505.17827", "pdf": "https://arxiv.org/pdf/2505.17827", "abs": "https://arxiv.org/abs/2505.17827", "authors": ["Hang Yuan", "Bin Yu", "Haotian Li", "Shijun Yang", "Christina Dan Wang", "Zhou Yu", "Xueyin Xu", "Weizhen Qi", "Kai Chen"], "title": "Not All Tokens Are What You Need In Thinking", "categories": ["cs.CL"], "comment": "11 pages, 7 figures and 3 tables", "summary": "Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit\nimpressive problem-solving capabilities but suffer from critical\ninefficiencies: high inference latency, excessive computational resource\nconsumption, and a tendency toward overthinking -- generating verbose chains of\nthought (CoT) laden with redundant tokens that contribute minimally to the\nfinal answer. To address these issues, we propose Conditional Token Selection\n(CTS), a token-level compression framework with a flexible and variable\ncompression ratio that identifies and preserves only the most essential tokens\nin CoT. CTS evaluates each token's contribution to deriving correct answers\nusing conditional importance scoring, then trains models on compressed CoT.\nExtensive experiments demonstrate that CTS effectively compresses long CoT\nwhile maintaining strong reasoning performance. Notably, on the GPQA benchmark,\nQwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with\n13.2% fewer reasoning tokens (13% training token reduction). Further reducing\ntraining tokens by 42% incurs only a marginal 5% accuracy drop while yielding a\n75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy\nin existing CoT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConditional Token Selection (CTS)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u538b\u7f29\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5197\u4f59token\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u63a8\u7406\u6a21\u578b\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u4ee5\u53ca\u8fc7\u5ea6\u601d\u8003\uff08\u751f\u6210\u5197\u4f59token\uff09\u7684\u95ee\u9898\uff0cCTS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CTS\u901a\u8fc7\u6761\u4ef6\u91cd\u8981\u6027\u8bc4\u5206\u8bc6\u522b\u5e76\u4fdd\u7559CoT\u4e2d\u6700\u5173\u952e\u7684token\uff0c\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u538b\u7f29\u540e\u7684CoT\u3002", "result": "\u5728GPQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCTS\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406token\u6570\u91cf\uff08\u6700\u591a75.8%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u63a8\u7406\u6027\u80fd\uff08\u51c6\u786e\u7387\u4ec5\u4e0b\u964d5%\uff09\u3002", "conclusion": "CTS\u8bc1\u660e\u4e86\u73b0\u6709CoT\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\uff0c\u901a\u8fc7\u538b\u7f29\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "relevance": 85.0}}
{"id": "2505.17555", "pdf": "https://arxiv.org/pdf/2505.17555", "abs": "https://arxiv.org/abs/2505.17555", "authors": ["Yuchen He", "Jianbing Lv", "Liqi Cheng", "Lingyu Meng", "Dazhen Deng", "Yingcai Wu"], "title": "ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization", "categories": ["cs.HC", "cs.CV"], "comment": "Accepted at CHI'25", "summary": "Temporal Action Localization (TAL) aims to detect the start and end\ntimestamps of actions in a video. However, the training of TAL models requires\na substantial amount of manually annotated data. Data programming is an\nefficient method to create training labels with a series of human-defined\nlabeling functions. However, its application in TAL faces difficulties of\ndefining complex actions in the context of temporal video frames. In this\npaper, we propose ProTAL, a drag-and-link video programming framework for TAL.\nProTAL enables users to define \\textbf{key events} by dragging nodes\nrepresenting body parts and objects and linking them to constrain the relations\n(direction, distance, etc.). These definitions are used to generate action\nlabels for large-scale unlabelled videos. A semi-supervised method is then\nemployed to train TAL models with such labels. We demonstrate the effectiveness\nof ProTAL through a usage scenario and a user study, providing insights into\ndesigning video programming framework.", "AI": {"tldr": "ProTAL\u662f\u4e00\u4e2a\u62d6\u62fd\u94fe\u63a5\u7684\u89c6\u9891\u7f16\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\uff08TAL\uff09\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u5173\u952e\u4e8b\u4ef6\u751f\u6210\u6807\u7b7e\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "TAL\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u6570\u636e\uff0c\u6570\u636e\u7f16\u7a0b\u65b9\u6cd5\u96be\u4ee5\u5b9a\u4e49\u590d\u6742\u52a8\u4f5c\u3002ProTAL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7528\u6237\u901a\u8fc7\u62d6\u62fd\u8282\u70b9\uff08\u8eab\u4f53\u90e8\u4f4d\u548c\u7269\u4f53\uff09\u5e76\u94fe\u63a5\u5b83\u4eec\u6765\u5b9a\u4e49\u5173\u952e\u4e8b\u4ef6\uff0c\u751f\u6210\u52a8\u4f5c\u6807\u7b7e\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "ProTAL\u901a\u8fc7\u4f7f\u7528\u573a\u666f\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ProTAL\u4e3a\u89c6\u9891\u7f16\u7a0b\u6846\u67b6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.18023", "pdf": "https://arxiv.org/pdf/2505.18023", "abs": "https://arxiv.org/abs/2505.18023", "authors": ["Duc Anh Nguyen", "Ernesto Araya", "Adalbert Fono", "Gitta Kutyniok"], "title": "Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Recent years have seen significant progress in developing spiking neural\nnetworks (SNNs) as a potential solution to the energy challenges posed by\nconventional artificial neural networks (ANNs). However, our theoretical\nunderstanding of SNNs remains relatively limited compared to the ever-growing\nbody of literature on ANNs. In this paper, we study a discrete-time model of\nSNNs based on leaky integrate-and-fire (LIF) neurons, referred to as\ndiscrete-time LIF-SNNs, a widely used framework that still lacks solid\ntheoretical foundations. We demonstrate that discrete-time LIF-SNNs with static\ninputs and outputs realize piecewise constant functions defined on polyhedral\nregions, and more importantly, we quantify the network size required to\napproximate continuous functions. Moreover, we investigate the impact of\nlatency (number of time steps) and depth (number of layers) on the complexity\nof the input space partitioning induced by discrete-time LIF-SNNs. Our analysis\nhighlights the importance of latency and contrasts these networks with ANNs\nemploying piecewise linear activation functions. Finally, we present numerical\nexperiments to support our theoretical findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u6563\u65f6\u95f4LIF-SNNs\u7684\u7406\u8bba\u57fa\u7840\uff0c\u91cf\u5316\u4e86\u7f51\u7edc\u89c4\u6a21\u5bf9\u8fde\u7eed\u51fd\u6570\u903c\u8fd1\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5ef6\u8fdf\u548c\u6df1\u5ea6\u5bf9\u8f93\u5165\u7a7a\u95f4\u5212\u5206\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1SNNs\u5728\u89e3\u51b3\u4f20\u7edfANNs\u7684\u80fd\u6e90\u6311\u6218\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u76f8\u5bf9\u8584\u5f31\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u79bb\u6563\u65f6\u95f4LIF-SNNs\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u79bb\u6563\u65f6\u95f4LIF\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u5206\u6790\u9759\u6001\u8f93\u5165\u548c\u8f93\u51fa\u4e0b\u7684\u7f51\u7edc\u884c\u4e3a\uff0c\u91cf\u5316\u7f51\u7edc\u89c4\u6a21\u5bf9\u51fd\u6570\u903c\u8fd1\u7684\u5f71\u54cd\uff0c\u5e76\u7814\u7a76\u5ef6\u8fdf\u548c\u6df1\u5ea6\u5bf9\u8f93\u5165\u7a7a\u95f4\u5212\u5206\u7684\u590d\u6742\u6027\u3002", "result": "\u79bb\u6563\u65f6\u95f4LIF-SNNs\u5b9e\u73b0\u591a\u9762\u4f53\u533a\u57df\u4e0a\u7684\u5206\u6bb5\u5e38\u6570\u51fd\u6570\uff0c\u5e76\u91cf\u5316\u4e86\u903c\u8fd1\u8fde\u7eed\u51fd\u6570\u6240\u9700\u7684\u7f51\u7edc\u89c4\u6a21\u3002\u5ef6\u8fdf\u5bf9\u8f93\u5165\u7a7a\u95f4\u5212\u5206\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5ef6\u8fdf\u5728SNNs\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "relevance": 30.0}}
{"id": "2505.17829", "pdf": "https://arxiv.org/pdf/2505.17829", "abs": "https://arxiv.org/abs/2505.17829", "authors": ["Zezhong Wang", "Xingshan Zeng", "Weiwen Liu", "Yufei Wang", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu", "Kam-Fai Wong"], "title": "Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a\npowerful capability of Large Language Models (LLMs), which can be further\nenhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.\nHowever, these methods, despite improving accuracy by allocating more\ncomputational resources during inference, often suffer from path homogenization\nand inefficient use of intermediate results. To address these limitations, we\npropose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that\nintroduces checkpoints between reasoning steps. It incorporates two key\nstrategies: (1) Answer-Clustered Search, which groups reasoning paths by their\nintermediate checkpoint answers to maintain diversity while ensuring quality,\nand (2) Checkpoint Candidate Augmentation, which leverages all intermediate\nanswers for final decision-making. Our approach effectively reduces path\nhomogenization and creates a fault-tolerant mechanism by utilizing high-quality\nintermediate results. Experimental results show that SRCA improves reasoning\naccuracy compared to existing TTS methods across various mathematical datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRCA\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u68c0\u67e5\u70b9\uff0c\u89e3\u51b3\u73b0\u6709TTS\u65b9\u6cd5\u4e2d\u7684\u8def\u5f84\u540c\u8d28\u5316\u548c\u4e2d\u95f4\u7ed3\u679c\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709TTS\u65b9\u6cd5\uff08\u5982Beam Search\u548cDVTS\uff09\u867d\u7136\u80fd\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\uff0c\u4f46\u5b58\u5728\u8def\u5f84\u540c\u8d28\u5316\u548c\u4e2d\u95f4\u7ed3\u679c\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "SRCA\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7b56\u7565\uff1a(1) Answer-Clustered Search\uff0c\u901a\u8fc7\u6309\u4e2d\u95f4\u68c0\u67e5\u70b9\u7b54\u6848\u5206\u7ec4\u4fdd\u6301\u591a\u6837\u6027\uff1b(2) Checkpoint Candidate Augmentation\uff0c\u5229\u7528\u6240\u6709\u4e2d\u95f4\u7b54\u6848\u8fdb\u884c\u6700\u7ec8\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSRCA\u5728\u591a\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709TTS\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "SRCA\u901a\u8fc7\u4f18\u5316\u4e2d\u95f4\u7ed3\u679c\u5229\u7528\u548c\u51cf\u5c11\u8def\u5f84\u540c\u8d28\u5316\uff0c\u4e3aLLM\u7684\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.18028", "pdf": "https://arxiv.org/pdf/2505.18028", "abs": "https://arxiv.org/abs/2505.18028", "authors": ["Zizhao Chen", "Yoav Artzi"], "title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "We propose KnotGym, an interactive environment for complex, spatial reasoning\nand manipulation. KnotGym includes goal-oriented rope manipulation tasks with\nvarying levels of complexity, all requiring acting from pure image\nobservations. Tasks are defined along a clear and quantifiable axis of\ncomplexity based on the number of knot crossings, creating a natural\ngeneralization test. KnotGym has a simple observation space, allowing for\nscalable development, yet it highlights core challenges in integrating acute\nperception, spatial reasoning, and grounded manipulation. We evaluate methods\nof different classes, including model-based RL, model-predictive control, and\nchain-of-thought reasoning, and illustrate the challenges KnotGym presents.\nKnotGym is available at https://github.com/lil-lab/knotgym.", "AI": {"tldr": "KnotGym\u662f\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u548c\u64cd\u4f5c\u7684\u4ea4\u4e92\u73af\u5883\uff0c\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u7ef3\u7ed3\u64cd\u4f5c\u4efb\u52a1\uff0c\u63d0\u4f9b\u53ef\u91cf\u5316\u7684\u590d\u6742\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6d4b\u8bd5\u611f\u77e5\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u64cd\u4f5c\u6574\u5408\u80fd\u529b\u7684\u6807\u51c6\u5316\u73af\u5883\uff0c\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u7ef3\u7ed3\u4ea4\u53c9\u70b9\u6570\u91cf\u7684\u4efb\u52a1\u590d\u6742\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6a21\u578b\u7684RL\u3001\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u94fe\u5f0f\u63a8\u7406\uff09\u3002", "result": "\u5c55\u793a\u4e86KnotGym\u5728\u6d4b\u8bd5\u4e0d\u540c\u65b9\u6cd5\u65f6\u7684\u6311\u6218\u6027\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "conclusion": "KnotGym\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u548c\u63a8\u52a8\u7a7a\u95f4\u63a8\u7406\u4e0e\u64cd\u4f5c\u7684\u7814\u7a76\u3002", "relevance": 40.0}}
{"id": "2505.17832", "pdf": "https://arxiv.org/pdf/2505.17832", "abs": "https://arxiv.org/abs/2505.17832", "authors": ["Giacomo Magnifico", "Eduard Barbu"], "title": "Emerging categories in scientific explanations", "categories": ["cs.CL"], "comment": "Accepted at the 3rd TRR 318 Conference: Contextualizing Explanations\n  (ContEx25), as a two-pager abstract. Will be published at BiUP (Bielefeld\n  University Press) at a later date", "summary": "Clear and effective explanations are essential for human understanding and\nknowledge dissemination. The scope of scientific research aiming to understand\nthe essence of explanations has recently expanded from the social sciences to\nmachine learning and artificial intelligence. Explanations for machine learning\ndecisions must be impactful and human-like, and there is a lack of large-scale\ndatasets focusing on human-like and human-generated explanations. This work\naims to provide such a dataset by: extracting sentences that indicate\nexplanations from scientific literature among various sources in the\nbiotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access\nsubset); providing a multi-class notation derived inductively from the data;\nevaluating annotator consensus on the emerging categories. The sentences are\norganized in an openly-available dataset, with two different classifications\n(6-class and 3-class category annotation), and the 3-class notation achieves a\n0.667 Krippendorf Alpha value.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u4eba\u7c7b\u751f\u6210\u7684\u89e3\u91ca\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4eba\u7c7b\u751f\u6210\u7684\u5927\u89c4\u6a21\u89e3\u91ca\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u51b3\u7b56\u7684\u89e3\u91ca\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u751f\u7269\u6280\u672f\u548c\u751f\u7269\u7269\u7406\u5b66\u9886\u57df\u7684\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u89e3\u91ca\u6027\u53e5\u5b50\uff0c\u5e76\u8fdb\u884c\u591a\u7c7b\u6807\u6ce8\u548c\u6ce8\u91ca\u8005\u5171\u8bc6\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b6\u7c7b\u548c3\u7c7b\u6807\u6ce8\uff0c\u5176\u4e2d3\u7c7b\u6807\u6ce8\u7684Krippendorf Alpha\u503c\u4e3a0.667\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u673a\u5668\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "relevance": 60.0}}
{"id": "2505.17582", "pdf": "https://arxiv.org/pdf/2505.17582", "abs": "https://arxiv.org/abs/2505.17582", "authors": ["Masataka Kobayashi", "Shintaro Shiba", "Quan Kong", "Norimasa Kobori", "Tsukasa Shimizu", "Shan Lu", "Takaya Yamazato"], "title": "Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras", "categories": ["eess.IV", "cs.CV", "cs.RO", "I.4.8; I.2.10; I.5.4"], "comment": "6 pages, 7 figures. To appear in IEEE Intelligent Vehicles Symposium\n  (IV) 2025", "summary": "With the growing adoption of autonomous driving, the advancement of sensor\ntechnology is crucial for ensuring safety and reliable operation. Sensor fusion\ntechniques that combine multiple sensors such as LiDAR, radar, and cameras have\nproven effective, but the integration of multiple devices increases both\nhardware complexity and cost. Therefore, developing a single sensor capable of\nperforming multiple roles is highly desirable for cost-efficient and scalable\nautonomous driving systems.\n  Event cameras have emerged as a promising solution due to their unique\ncharacteristics, including high dynamic range, low latency, and high temporal\nresolution. These features enable them to perform well in challenging lighting\nconditions, such as low-light or backlit environments. Moreover, their ability\nto detect fine-grained motion events makes them suitable for applications like\npedestrian detection and vehicle-to-infrastructure communication via visible\nlight.\n  In this study, we present a method for distance estimation using a monocular\nevent camera and a roadside LED bar. By applying a phase-only correlation\ntechnique to the event data, we achieve sub-pixel precision in detecting the\nspatial shift between two light sources. This enables accurate\ntriangulation-based distance estimation without requiring stereo vision. Field\nexperiments conducted in outdoor driving scenarios demonstrated that the\nproposed approach achieves over 90% success rate with less than 0.5-meter error\nfor distances ranging from 20 to 60 meters.\n  Future work includes extending this method to full position estimation by\nleveraging infrastructure such as smart poles equipped with LEDs, enabling\nevent-camera-based vehicles to determine their own position in real time. This\nadvancement could significantly enhance navigation accuracy, route\noptimization, and integration into intelligent transportation systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\u548c\u8def\u8fb9LED\u6761\u7684\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u4f4d\u76f8\u5173\u6280\u672f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e09\u89d2\u6d4b\u91cf\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u7684\u666e\u53ca\uff0c\u4f20\u611f\u5668\u6280\u672f\u7684\u8fdb\u6b65\u5bf9\u5b89\u5168\u548c\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u9ad8\u52a8\u6001\u8303\u56f4\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u6210\u4e3a\u591a\u89d2\u8272\u4f20\u611f\u5668\u7684\u7406\u60f3\u9009\u62e9\u3002", "method": "\u5229\u7528\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\u548c\u8def\u8fb9LED\u6761\uff0c\u901a\u8fc7\u76f8\u4f4d\u76f8\u5173\u6280\u672f\u68c0\u6d4b\u5149\u6e90\u95f4\u7684\u7a7a\u95f4\u4f4d\u79fb\uff0c\u5b9e\u73b0\u65e0\u9700\u7acb\u4f53\u89c6\u89c9\u7684\u4e09\u89d2\u6d4b\u91cf\u8ddd\u79bb\u4f30\u8ba1\u3002", "result": "\u6237\u5916\u9a7e\u9a76\u573a\u666f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u572820\u81f360\u7c73\u8303\u56f4\u5185\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u8bef\u5dee\u5c0f\u4e8e0.5\u7c73\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u81f3\u5229\u7528\u667a\u80fdLED\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u5b9e\u65f6\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u63d0\u5347\u5bfc\u822a\u7cbe\u5ea6\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u96c6\u6210\u3002", "relevance": 30.0}}
{"id": "2505.18032", "pdf": "https://arxiv.org/pdf/2505.18032", "abs": "https://arxiv.org/abs/2505.18032", "authors": ["Maximilian Mueller", "Matthias Hein"], "title": "Mahalanobis++: Improving OOD Detection via Feature Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Detecting out-of-distribution (OOD) examples is an important task for\ndeploying reliable machine learning models in safety-critial applications.\nWhile post-hoc methods based on the Mahalanobis distance applied to pre-logit\nfeatures are among the most effective for ImageNet-scale OOD detection, their\nperformance varies significantly across models. We connect this inconsistency\nto strong variations in feature norms, indicating severe violations of the\nGaussian assumption underlying the Mahalanobis distance estimation. We show\nthat simple $\\ell_2$-normalization of the features mitigates this problem\neffectively, aligning better with the premise of normally distributed data with\nshared covariance matrix. Extensive experiments on 44 models across diverse\narchitectures and pretraining schemes show that $\\ell_2$-normalization improves\nthe conventional Mahalanobis distance-based approaches significantly and\nconsistently, and outperforms other recently proposed OOD detection methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7b80\u5355\u7684\u7279\u5f81L2\u5f52\u4e00\u5316\u6539\u8fdb\u57fa\u4e8eMahalanobis\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u8303\u6570\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u6027\u80fd\u6ce2\u52a8\u95ee\u9898\uff0c\u5e76\u572844\u79cd\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u68c0\u6d4bOOD\u6837\u672c\u5bf9\u90e8\u7f72\u53ef\u9760\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8eMahalanobis\u8ddd\u79bb\u7684\u65b9\u6cd5\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4e0e\u9ad8\u65af\u5206\u5e03\u5047\u8bbe\u4e0d\u7b26\u3002", "method": "\u901a\u8fc7L2\u5f52\u4e00\u5316\u9884\u5904\u7406\u7279\u5f81\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u9ad8\u65af\u5206\u5e03\u5047\u8bbe\uff0c\u4ece\u800c\u6539\u8fdbMahalanobis\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u572844\u79cd\u4e0d\u540c\u67b6\u6784\u548c\u9884\u8bad\u7ec3\u65b9\u6848\u7684\u6a21\u578b\u4e2d\uff0cL2\u5f52\u4e00\u5316\u663e\u8457\u4e14\u4e00\u81f4\u5730\u63d0\u5347\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u8fd1\u671f\u63d0\u51fa\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "L2\u5f52\u4e00\u5316\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u63d0\u5347\u57fa\u4e8eMahalanobis\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.17833", "pdf": "https://arxiv.org/pdf/2505.17833", "abs": "https://arxiv.org/abs/2505.17833", "authors": ["Kalle Lahtinen", "Einari Vaaras", "Liisa Mustanoja", "Okko R\u00e4s\u00e4nen"], "title": "Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at Interspeech 2025, Rotterdam, The\n  Netherlands", "summary": "Study of affect in speech requires suitable data, as emotional expression and\nperception vary across languages. Until now, no corpus has existed for natural\nexpression of affect in spontaneous Finnish, existing data being acted or from\na very specific communicative setting. This paper presents the first such\ncorpus, created by annotating 12,000 utterances for emotional arousal and\nvalence, sampled from three large-scale Finnish speech corpora. To ensure\ndiverse affective expression, sample selection was conducted with an affect\nmining approach combining acoustic, cross-linguistic speech emotion, and text\nsentiment features. We compare this method to random sampling in terms of\nannotation diversity, and conduct post-hoc analyses to identify sampling\nchoices that would have maximized the diversity. As an outcome, the work\nintroduces a spontaneous Finnish affective speech corpus and informs sampling\nstrategies for affective speech corpus creation in other languages or domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u82ac\u5170\u8bed\u81ea\u7136\u60c5\u611f\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u60c5\u611f\u6316\u6398\u65b9\u6cd5\u6807\u6ce8\u4e8612,000\u4e2a\u8bed\u53e5\uff0c\u5e76\u6bd4\u8f83\u4e86\u968f\u673a\u91c7\u6837\u4e0e\u60c5\u611f\u6316\u6398\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u82ac\u5170\u8bed\u4e2d\u81ea\u7136\u60c5\u611f\u8868\u8fbe\u7684\u7f3a\u4e4f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bed\u6599\u5e93\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u60c5\u611f\u6316\u6398\u65b9\u6cd5\u7ed3\u5408\u58f0\u5b66\u3001\u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u548c\u6587\u672c\u60c5\u611f\u7279\u5f81\u8fdb\u884c\u6837\u672c\u9009\u62e9\uff0c\u5e76\u4e0e\u968f\u673a\u91c7\u6837\u5bf9\u6bd4\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u82ac\u5170\u8bed\u81ea\u7136\u60c5\u611f\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u5e76\u5206\u6790\u4e86\u91c7\u6837\u7b56\u7565\u5bf9\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u8bed\u6599\u5e93\u4e3a\u82ac\u5170\u8bed\u60c5\u611f\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5e76\u4e3a\u5176\u4ed6\u8bed\u8a00\u6216\u9886\u57df\u7684\u60c5\u611f\u8bed\u6599\u5e93\u6784\u5efa\u63d0\u4f9b\u4e86\u91c7\u6837\u7b56\u7565\u53c2\u8003\u3002", "relevance": 20.0}}
{"id": "2505.18043", "pdf": "https://arxiv.org/pdf/2505.18043", "abs": "https://arxiv.org/abs/2505.18043", "authors": ["Changyeol Lee", "Yongho Shin", "Hyung-Chan An"], "title": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach", "categories": ["cs.LG", "cs.DB", "cs.DS"], "comment": null, "summary": "Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u548c\u7ec4\u5408\u7b97\u6cd5\u4f18\u52bf\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e09\u79cd\u7248\u672c\u7684\u8fb9\u7740\u8272\u805a\u7c7b\u95ee\u9898\uff08Local ECC\u3001Global ECC\u548cRobust ECC\uff09\uff0c\u5728\u9ad8\u6548\u6027\u548c\u89e3\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf\u8fb9\u7740\u8272\u805a\u7c7b\uff08ECC\uff09\u5b58\u5728\u975e\u91cd\u53e0\u548c\u7a77\u4e3e\u805a\u7c7b\u7684\u9650\u5236\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u7ed3\u5408LP\u548c\u7ec4\u5408\u7b97\u6cd5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u548c\u89e3\u8d28\u91cf\u3002", "result": "\u7b97\u6cd5\u5728\u4e09\u79cdECC\u95ee\u9898\u4e0a\u5747\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u590d\u6742\u6027\u7406\u8bba\u7684\u4e0d\u8fd1\u4f3c\u6027\u7ed3\u679c\u548c\u79ef\u5206\u95f4\u9699\u754c\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u5e76\u56de\u7b54\u4e86\u6587\u732e\u4e2d\u7684\u4e24\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "relevance": 40.0}}
{"id": "2505.17855", "pdf": "https://arxiv.org/pdf/2505.17855", "abs": "https://arxiv.org/abs/2505.17855", "authors": ["Jingyi Sun", "Greta Warren", "Irina Shklovski", "Isabelle Augenstein"], "title": "Explaining Sources of Uncertainty in Automated Fact-Checking", "categories": ["cs.CL"], "comment": null, "summary": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.", "AI": {"tldr": "CLUE\u6846\u67b6\u901a\u8fc7\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u51b2\u7a81\u4e0e\u4e00\u81f4\u5173\u7cfb\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u767d\u76d2\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6570\u503c\u4e0d\u786e\u5b9a\u6027\u6216\u6a21\u7cca\u8868\u8fbe\uff09\u65e0\u6cd5\u89e3\u91ca\u7531\u8bc1\u636e\u51b2\u7a81\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u6237\u96be\u4ee5\u4f9d\u8d56\u8f93\u51fa\u6216\u89e3\u51b3\u5206\u6b67\u3002", "method": "CLUE\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u51b2\u7a81\u4e0e\u4e00\u81f4\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5728\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u4e0a\uff0cCLUE\u751f\u6210\u7684\u89e3\u91ca\u66f4\u5fe0\u5b9e\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u4eba\u7c7b\u8bc4\u4f30\u8ba4\u4e3a\u5176\u66f4\u6709\u5e2e\u52a9\u548c\u4fe1\u606f\u91cf\u3002", "conclusion": "CLUE\u901a\u8fc7\u5c06\u4e0d\u786e\u5b9a\u6027\u4e0e\u8bc1\u636e\u51b2\u7a81\u660e\u786e\u5173\u8054\uff0c\u4e3a\u4e8b\u5b9e\u6838\u67e5\u7b49\u4efb\u52a1\u63d0\u4f9b\u5b9e\u7528\u652f\u6301\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u590d\u6742\u63a8\u7406\u4efb\u52a1\u3002", "relevance": 85.0}}
{"id": "2505.17479", "pdf": "https://arxiv.org/pdf/2505.17479", "abs": "https://arxiv.org/abs/2505.17479", "authors": ["Olivier Toubia", "George Z. Gui", "Tianyi Peng", "Daniel J. Merlau", "Ang Li", "Haozhe Chen"], "title": "Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions", "categories": ["cs.CY", "cs.AI", "cs.HC", "econ.EM"], "comment": "Also available at SSRN:\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5265253", "summary": "LLM-based digital twin simulation, where large language models are used to\nemulate individual human behavior, holds great promise for research in AI,\nsocial science, and digital experimentation. However, progress in this area has\nbeen hindered by the scarcity of real, individual-level datasets that are both\nlarge and publicly available. This lack of high-quality ground truth limits\nboth the development and validation of digital twin methodologies. To address\nthis gap, we introduce a large-scale, public dataset designed to capture a rich\nand holistic view of individual human behavior. We survey a representative\nsample of $N = 2,058$ participants (average 2.42 hours per person) in the US\nacross four waves with 500 questions in total, covering a comprehensive battery\nof demographic, psychological, economic, personality, and cognitive measures,\nas well as replications of behavioral economics experiments and a pricing\nsurvey. The final wave repeats tasks from earlier waves to establish a\ntest-retest accuracy baseline. Initial analyses suggest the data are of high\nquality and show promise for constructing digital twins that predict human\nbehavior well at the individual and aggregate levels. By making the full\ndataset publicly available, we aim to establish a valuable testbed for the\ndevelopment and benchmarking of LLM-based persona simulations. Beyond LLM\napplications, due to its unique breadth and scale the dataset also enables\nbroad social science research, including studies of cross-construct\ncorrelations and heterogeneous treatment effects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\uff0c\u7528\u4e8eLLM\u6570\u5b57\u5b6a\u751f\u6a21\u62df\uff0c\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u771f\u5b9e\u4e2a\u4f53\u884c\u4e3a\u6570\u636e\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dLLM\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u7684\u771f\u5b9e\u4e2a\u4f53\u884c\u4e3a\u6570\u636e\u800c\u53d7\u9650\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u56db\u8f6e\u8c03\u67e5\u6536\u96c6\u4e862,058\u540d\u7f8e\u56fd\u53c2\u4e0e\u8005\u7684\u7efc\u5408\u6570\u636e\uff0c\u6db5\u76d6\u4eba\u53e3\u7edf\u8ba1\u3001\u5fc3\u7406\u3001\u7ecf\u6d4e\u7b49\u591a\u7ef4\u5ea6\u4fe1\u606f\u3002", "result": "\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u521d\u6b65\u5206\u6790\u663e\u793a\u53ef\u7528\u4e8e\u6784\u5efa\u9884\u6d4b\u4e2a\u4f53\u548c\u7fa4\u4f53\u884c\u4e3a\u7684\u6570\u5b57\u5b6a\u751f\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3aLLM\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u540c\u65f6\u652f\u6301\u5e7f\u6cdb\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002", "relevance": 70.0}}
{"id": "2505.18044", "pdf": "https://arxiv.org/pdf/2505.18044", "abs": "https://arxiv.org/abs/2505.18044", "authors": ["Zhishuai Liu", "Pan Xu"], "title": "Linear Mixture Distributionally Robust Markov Decision Processes", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": "26 pages, 7 figures", "summary": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u6df7\u5408DRMDP\u6846\u67b6\uff0c\u901a\u8fc7\u56f4\u7ed5\u6df7\u5408\u6743\u91cd\u53c2\u6570\u5b9a\u4e49\u4e0d\u786e\u5b9a\u6027\u96c6\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u7cbe\u7ec6\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u72b6\u6001\u8f6c\u79fb\u4e0d\u540c\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u8bbe\u8ba1\u66f4\u7cbe\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u96c6\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u6df7\u5408DRMDP\u6846\u67b6\uff0c\u57fa\u4e8e\u6df7\u5408\u6743\u91cd\u53c2\u6570\u5b9a\u4e49\u4e0d\u786e\u5b9a\u6027\u96c6\uff0c\u5e76\u8bbe\u8ba1\u5143\u7b97\u6cd5\u8fdb\u884c\u9c81\u68d2\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728\u603b\u53d8\u5dee\u3001KL\u548c\u03c7\u00b2\u6563\u5ea6\u4e0b\u5206\u6790\u4e86\u6837\u672c\u590d\u6742\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7ebf\u6027\u6df7\u5408DRMDP\u7684\u7edf\u8ba1\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "\u7ebf\u6027\u6df7\u5408DRMDP\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c24\u5176\u5728\u6df7\u5408\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u5b58\u5728\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "relevance": 40.0}}
{"id": "2505.17870", "pdf": "https://arxiv.org/pdf/2505.17870", "abs": "https://arxiv.org/abs/2505.17870", "authors": ["Shaina Raza", "Rizwan Qureshi", "Marcelo Lotif", "Aman Chadha", "Deval Pandya", "Christos Emmanouilidis"], "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods", "categories": ["cs.CL"], "comment": null, "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5fae\u8c03AI\u6a21\u578b\u4e8e\u6807\u8bb0\u7684\u865a\u5047\u4fe1\u606f\u4e0a\uff0c\u4ee5\u201c\u514d\u75ab\u201d\u6a21\u578b\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u7c7b\u4f3c\u751f\u7269\u514d\u75ab\u673a\u5236\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\u5e38\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u590d\u5236\u9519\u8bef\u4fe1\u606f\uff0c\u9700\u4e00\u79cd\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u8bc6\u522b\u548c\u62d2\u7edd\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u5728\u5fae\u8c03\u9636\u6bb5\u5b9a\u671f\u6ce8\u5165\u5c11\u91cf\u6807\u8bb0\u7684\u865a\u5047\u4fe1\u606f\u4f5c\u4e3a\u201c\u75ab\u82d7\u201d\uff0c\u589e\u5f3a\u6a21\u578b\u8bc6\u522b\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u201c\u514d\u75ab\u201d\u6a21\u578b\u751f\u6210\u7684\u9519\u8bef\u4fe1\u606f\u663e\u8457\u5c11\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347AI\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e3b\u52a8\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4f26\u7406\u4fdd\u969c\u63aa\u65bd\u3002", "relevance": 85.0}}
{"id": "2505.17480", "pdf": "https://arxiv.org/pdf/2505.17480", "abs": "https://arxiv.org/abs/2505.17480", "authors": ["Faruk Alpay"], "title": "Alpay Algebra II: Identity as Fixed-Point Emergence in Categorical Data", "categories": ["math.CT", "cs.AI", "cs.LO", "18C10, 18D05, 03B70, 03G30", "F.4.1; I.2.3; F.3.2; F.1.1"], "comment": "13 pages, no figures. Sequel to Alpay Algebra: A Universal Structural\n  Foundation (arXiv:2505.15344). Defines identity as a categorical fixed point\n  in the Alpay Algebra system. All content is self-contained", "summary": "In this second installment of the Alpay Algebra framework, I formally define\nidentity as a fixed point that emerges through categorical recursion. Building\nupon the transfinite operator $\\varphi^\\infty$, I characterize identity as the\nuniversal solution to a self-referential functorial equation over a small\ncartesian closed category. I prove the existence and uniqueness of such\nidentity-fixed-points via ordinal-indexed iteration, and interpret their\nconvergence through internal categorical limits. Functors, adjunctions, and\nmorphisms are reconstructed as dynamic traces of evolving states governed by\n$\\varphi$, reframing identity not as a static label but as a stabilized\nprocess. Through formal theorems and symbolic flows, I show how these fixed\npoints encode symbolic memory, recursive coherence, and semantic invariance.\nThis paper positions identity as a mathematical structure that arises from\nwithin the logic of change itself computable, convergent, and categorically\nintrinsic.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8303\u7574\u9012\u5f52\u5b9a\u4e49\u4e86\u8eab\u4efd\u4f5c\u4e3a\u56fa\u5b9a\u70b9\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b58\u5728\u6027\u4e0e\u552f\u4e00\u6027\uff0c\u91cd\u6784\u4e86\u51fd\u5b50\u3001\u4f34\u968f\u548c\u6001\u5c04\u7684\u52a8\u6001\u8fc7\u7a0b\u3002", "motivation": "\u63a2\u8ba8\u8eab\u4efd\u4f5c\u4e3a\u52a8\u6001\u8fc7\u7a0b\u800c\u975e\u9759\u6001\u6807\u7b7e\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u63ed\u793a\u5176\u5728\u7b26\u53f7\u8bb0\u5fc6\u548c\u9012\u5f52\u4e00\u81f4\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u5e8f\u6570\u7d22\u5f15\u8fed\u4ee3\u548c\u5185\u90e8\u8303\u7574\u6781\u9650\uff0c\u901a\u8fc7\u81ea\u53cd\u51fd\u5b50\u65b9\u7a0b\u5b9a\u4e49\u8eab\u4efd\u56fa\u5b9a\u70b9\u3002", "result": "\u8bc1\u660e\u4e86\u8eab\u4efd\u56fa\u5b9a\u70b9\u7684\u5b58\u5728\u6027\u4e0e\u552f\u4e00\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5982\u4f55\u7f16\u7801\u7b26\u53f7\u8bb0\u5fc6\u548c\u8bed\u4e49\u4e0d\u53d8\u6027\u3002", "conclusion": "\u8eab\u4efd\u662f\u4e00\u79cd\u53ef\u8ba1\u7b97\u3001\u6536\u655b\u4e14\u8303\u7574\u5185\u751f\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u6e90\u4e8e\u53d8\u5316\u903b\u8f91\u672c\u8eab\u3002", "relevance": 20.0}}
{"id": "2505.18046", "pdf": "https://arxiv.org/pdf/2505.18046", "abs": "https://arxiv.org/abs/2505.18046", "authors": ["Yizhou Xu", "Florent Krzakala", "Lenka Zdeborov\u00e1"], "title": "Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "comment": null, "summary": "The Restricted Boltzmann Machine (RBM) is one of the simplest generative\nneural networks capable of learning input distributions. Despite its\nsimplicity, the analysis of its performance in learning from the training data\nis only well understood in cases that essentially reduce to singular value\ndecomposition of the data. Here, we consider the limit of a large dimension of\nthe input space and a constant number of hidden units. In this limit, we\nsimplify the standard RBM training objective into a form that is equivalent to\nthe multi-index model with non-separable regularization. This opens a path to\nanalyze training of the RBM using methods that are established for multi-index\nmodels, such as Approximate Message Passing (AMP) and its state evolution, and\nthe analysis of Gradient Descent (GD) via the dynamical mean-field theory. We\nthen give rigorous asymptotics of the training dynamics of RBM on data\ngenerated by the spiked covariance model as a prototype of a structure suitable\nfor unsupervised learning. We show in particular that RBM reaches the optimal\ncomputational weak recovery threshold, aligning with the BBP transition, in the\nspiked covariance model.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a\uff08RBM\uff09\u5728\u5927\u8f93\u5165\u7a7a\u95f4\u7ef4\u5ea6\u548c\u5c0f\u9690\u85cf\u5355\u5143\u6570\u60c5\u51b5\u4e0b\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5c06\u5176\u7b80\u5316\u4e3a\u591a\u7d22\u5f15\u6a21\u578b\uff0c\u5e76\u5229\u7528AMP\u548cGD\u65b9\u6cd5\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u7814\u7a76RBM\u5728\u7279\u5b9a\u6781\u9650\u4e0b\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u586b\u8865\u5176\u5728\u975eSVD\u60c5\u51b5\u4e0b\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5c06RBM\u8bad\u7ec3\u76ee\u6807\u7b80\u5316\u4e3a\u591a\u7d22\u5f15\u6a21\u578b\uff0c\u5229\u7528AMP\u3001\u72b6\u6001\u6f14\u5316\u548cGD\u7684\u52a8\u529b\u5b66\u5e73\u5747\u573a\u7406\u8bba\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u5c16\u5cf0\u534f\u65b9\u5dee\u6a21\u578b\u4e2d\uff0cRBM\u8fbe\u5230\u6700\u4f18\u8ba1\u7b97\u5f31\u6062\u590d\u9608\u503c\uff0c\u4e0eBBP\u8f6c\u53d8\u4e00\u81f4\u3002", "conclusion": "RBM\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4e0e\u591a\u7d22\u5f15\u6a21\u578b\u76f8\u4f3c\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u4e3a\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "relevance": 30.0}}
{"id": "2505.17873", "pdf": "https://arxiv.org/pdf/2505.17873", "abs": "https://arxiv.org/abs/2505.17873", "authors": ["Wanhao Liu", "Zonglin Yang", "Jue Wang", "Lidong Bing", "Di Zhang", "Dongzhan Zhou", "Yuqiang Li", "Houqiang Li", "Erik Cambria", "Wanli Ouyang"], "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u9a8c\u5f15\u5bfc\u7684\u5047\u8bbe\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u53cd\u9988\u4f18\u5316\u5047\u8bbe\u4f18\u5148\u7ea7\uff0c\u4f18\u4e8e\u4f20\u7edf\u9884\u5b9e\u9a8c\u6392\u5e8f\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u79d1\u5b66\u7814\u7a76\u4e2d\u6e7f\u5b9e\u9a8c\u6210\u672c\u9ad8\u3001\u901a\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u5229\u7528\u5b9e\u9a8c\u53cd\u9988\u4f18\u5316\u5047\u8bbe\u6392\u5e8f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9886\u57df\u5047\u8bbe\u7684\u6a21\u62df\u5668\uff0c\u5f00\u53d1\u4f2a\u5b9e\u9a8c\u5f15\u5bfc\u7684\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u6a21\u62df\u53cd\u9988\u4f18\u5316\u5047\u8bbe\u4f18\u5148\u7ea7\u3002", "result": "\u5728124\u4e2a\u5316\u5b66\u5047\u8bbe\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u4f18\u4e8e\u9884\u5b9e\u9a8c\u57fa\u51c6\u548c\u5f3a\u6d88\u878d\u5b9e\u9a8c\u3002", "conclusion": "\u5b9e\u9a8c\u5f15\u5bfc\u7684\u6392\u5e8f\u65b9\u6cd5\u6709\u6548\uff0c\u6a21\u62df\u5668\u4e3a\u81ea\u7136\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 70.0}}
{"id": "2505.17644", "pdf": "https://arxiv.org/pdf/2505.17644", "abs": "https://arxiv.org/abs/2505.17644", "authors": ["Taoran Zheng", "Xing Li", "Yan Yang", "Xiang Gu", "Zongben Xu", "Jian Sun"], "title": "Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image reconstruction from measurement data is a vital but challenging\ninverse problem. Deep learning approaches have achieved promising results, but\noften requires paired measurement and high-quality images, which is typically\nsimulated through a forward model, i.e., retrospective reconstruction. However,\ntraining on simulated pairs commonly leads to performance degradation on real\nprospective data due to the retrospective-to-prospective gap caused by\nincomplete imaging knowledge in simulation. To address this challenge, this\npaper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT),\na novel dynamic optimal transport framework with optimality in the sense of\npreserving consistency with imaging physics in transport, that conceptualizes\nreconstruction as finding a dynamic transport path. KIDOT learns from unpaired\ndata by modeling reconstruction as a continuous evolution path from\nmeasurements to images, guided by an imaging knowledge-informed cost function\nand transport equation. This dynamic and knowledge-aware approach enhances\nrobustness and better leverages unpaired data while respecting acquisition\nphysics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic\noptimal transport, ensuring its mathematical rationale and solution existence.\nExtensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKIDOT\u7684\u52a8\u6001\u6700\u4f18\u4f20\u8f93\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\uff0c\u901a\u8fc7\u5b66\u4e60\u672a\u914d\u5bf9\u6570\u636e\u5e76\u5229\u7528\u6210\u50cf\u7269\u7406\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u6570\u636e\u4e0e\u5b9e\u9645\u6570\u636e\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u9006\u95ee\u9898\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u62df\u7684\u914d\u5bf9\u6570\u636e\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\u3002KIDOT\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u6700\u4f18\u4f20\u8f93\u548c\u6210\u50cf\u7269\u7406\u77e5\u8bc6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KIDOT\u5c06\u91cd\u5efa\u5efa\u6a21\u4e3a\u4ece\u6d4b\u91cf\u5230\u56fe\u50cf\u7684\u52a8\u6001\u4f20\u8f93\u8def\u5f84\uff0c\u5229\u7528\u6210\u50cf\u77e5\u8bc6\u6307\u5bfc\u7684\u6210\u672c\u51fd\u6570\u548c\u4f20\u8f93\u65b9\u7a0b\uff0c\u5b66\u4e60\u672a\u914d\u5bf9\u6570\u636e\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728MRI\u548cCT\u91cd\u5efa\u5b9e\u9a8c\u4e2d\uff0cKIDOT\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "KIDOT\u901a\u8fc7\u52a8\u6001\u6700\u4f18\u4f20\u8f93\u548c\u6210\u50cf\u77e5\u8bc6\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "relevance": 40.0}}
{"id": "2505.18064", "pdf": "https://arxiv.org/pdf/2505.18064", "abs": "https://arxiv.org/abs/2505.18064", "authors": ["Victor Boone"], "title": "Asymptotically optimal regret in communicating Markov decision processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this paper, we present a learning algorithm that achieves asymptotically\noptimal regret for Markov decision processes in average reward under a\ncommunicating assumption. That is, given a communicating Markov decision\nprocess $M$, our algorithm has regret $K(M) \\log(T) + \\mathrm{o}(\\log(T))$\nwhere $T$ is the number of learning steps and $K(M)$ is the best possible\nconstant. This algorithm works by explicitly tracking the constant $K(M)$ to\nlearn optimally, then balances the trade-off between exploration (playing\nsub-optimally to gain information), co-exploration (playing optimally to gain\ninformation) and exploitation (playing optimally to score maximally). We\nfurther show that the function $K(M)$ is discontinuous, which is a consequence\nchallenge for our approach. To that end, we describe a regularization mechanism\nto estimate $K(M)$ with arbitrary precision from empirical data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5e73\u5747\u5956\u52b1\u4e0b\u5bf9\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u5b9e\u73b0\u6e10\u8fd1\u6700\u4f18\u9057\u61be\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u63a2\u7d22\u3001\u5171\u540c\u63a2\u7d22\u548c\u5229\u7528\u6765\u4f18\u5316\u5b66\u4e60\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728MDP\u4e2d\u5b9e\u73b0\u6700\u4f18\u9057\u61be\uff0c\u89e3\u51b3\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5904\u7406\u51fd\u6570K(M)\u7684\u4e0d\u8fde\u7eed\u6027\u3002", "method": "\u7b97\u6cd5\u901a\u8fc7\u663e\u5f0f\u8ddf\u8e2aK(M)\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u7ed3\u5408\u63a2\u7d22\u3001\u5171\u540c\u63a2\u7d22\u548c\u5229\u7528\u7684\u6743\u8861\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u673a\u5236\u4f30\u8ba1K(M)\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86K(M)log(T)+o(log(T))\u7684\u9057\u61be\uff0c\u5e76\u8bc1\u660e\u4e86K(M)\u7684\u4e0d\u8fde\u7eed\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728MDP\u4e2d\u5b9e\u73b0\u4e86\u6e10\u8fd1\u6700\u4f18\u9057\u61be\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u673a\u5236\u89e3\u51b3\u4e86K(M)\u4f30\u8ba1\u7684\u6311\u6218\u3002", "relevance": 40.0}}
{"id": "2505.17894", "pdf": "https://arxiv.org/pdf/2505.17894", "abs": "https://arxiv.org/abs/2505.17894", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motaism Hamed", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.", "AI": {"tldr": "Mutarjim\u662f\u4e00\u4e2a\u7d27\u51d1\u4f46\u5f3a\u5927\u7684\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u53cc\u5411\u7ffb\u8bd1\u6a21\u578b\uff0c\u57fa\u4e8eKuwain-1.5B\u5f00\u53d1\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\uff0c\u6027\u80fd\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002\u540c\u65f6\u53d1\u5e03\u4e86Tarjama-25\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u5728\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u9700\u6c42\u4e0a\u66f4\u5177\u4f18\u52bf\u3002Mutarjim\u65e8\u5728\u8bc1\u660e\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u3002", "method": "\u57fa\u4e8eKuwain-1.5B\uff0c\u91c7\u7528\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u8bed\u6599\u5e93\u3002", "result": "Mutarjim\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u5e76\u5728Tarjama-25\u4e0a\u5b9e\u73b0\u82f1\u8bed-\u963f\u62c9\u4f2f\u8bed\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u5c0f\u578b\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "relevance": 40.0}}
{"id": "2505.17659", "pdf": "https://arxiv.org/pdf/2505.17659", "abs": "https://arxiv.org/abs/2505.17659", "authors": ["Xiaolong Tang", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Safe and feasible trajectory planning is essential for real-world autonomous\ndriving systems. However, existing learning-based planning methods often rely\non expert demonstrations, which not only lack explicit safety awareness but\nalso risk inheriting unsafe behaviors such as speeding from suboptimal human\ndriving data. Inspired by the success of large language models, we propose\nPlan-R1, a novel two-stage trajectory planning framework that formulates\ntrajectory planning as a sequential prediction task, guided by explicit\nplanning principles such as safety, comfort, and traffic rule compliance. In\nthe first stage, we train an autoregressive trajectory predictor via next\nmotion token prediction on expert data. In the second stage, we design\nrule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the\nmodel using Group Relative Policy Optimization (GRPO), a reinforcement learning\nstrategy, to align its predictions with these planning principles. Experiments\non the nuPlan benchmark demonstrate that our Plan-R1 significantly improves\nplanning safety and feasibility, achieving state-of-the-art performance.", "AI": {"tldr": "Plan-R1\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u8f68\u8ff9\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u660e\u786e\u7684\u89c4\u5212\u539f\u5219\uff08\u5982\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\uff09\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u6f14\u793a\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u5b89\u5168\u610f\u8bc6\uff0c\u53ef\u80fd\u7ee7\u627f\u4e0d\u5b89\u5168\u884c\u4e3a\uff08\u5982\u8d85\u901f\uff09\u3002Plan-R1\u65e8\u5728\u901a\u8fc7\u660e\u786e\u7684\u89c4\u5212\u539f\u5219\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u7b2c\u4e00\u9636\u6bb5\uff1a\u5728\u4e13\u5bb6\u6570\u636e\u4e0a\u901a\u8fc7\u4e0b\u4e00\u4e2a\u8fd0\u52a8\u6807\u8bb0\u9884\u6d4b\u8bad\u7ec3\u81ea\u56de\u5f52\u8f68\u8ff9\u9884\u6d4b\u5668\u30022. \u7b2c\u4e8c\u9636\u6bb5\uff1a\u8bbe\u8ba1\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\uff08\u5982\u907f\u78b0\u3001\u9650\u901f\uff09\uff0c\u5e76\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlan-R1\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Plan-R1\u901a\u8fc7\u7ed3\u5408\u8f68\u8ff9\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u548c\u53ef\u884c\u7684\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u3002", "relevance": 30.0}}
{"id": "2505.18065", "pdf": "https://arxiv.org/pdf/2505.18065", "abs": "https://arxiv.org/abs/2505.18065", "authors": ["Zeen Song", "Wenwen Qiang", "Siyu Zhao", "Changwen Zheng", "Gang Hua"], "title": "Reward Model Generalization for Compute-Aware Test-Time Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "External test-time reasoning enhances large language models (LLMs) by\ndecoupling generation and selection. At inference time, the model generates\nmultiple reasoning paths, and an auxiliary process reward model (PRM) is used\nto score and select the best one. A central challenge in this setting is\ntest-time compute optimality (TCO), i.e., how to maximize answer accuracy under\na fixed inference budget. In this work, we establish a theoretical framework to\nanalyze how the generalization error of the PRM affects compute efficiency and\nreasoning performance. Leveraging PAC-Bayes theory, we derive generalization\nbounds and show that a lower generalization error of PRM leads to fewer samples\nrequired to find correct answers. Motivated by this analysis, we propose\nCompute-Aware Tree Search (CATS), an actor-critic framework that dynamically\ncontrols search behavior. The actor outputs sampling hyperparameters based on\nreward distributions and sparsity statistics, while the critic estimates their\nutility to guide budget allocation. Experiments on the MATH and AIME benchmarks\nwith various LLMs and PRMs demonstrate that CATS consistently outperforms other\nexternal TTS methods, validating our theoretical predictions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5916\u90e8\u6d4b\u8bd5\u65f6\u95f4\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u751f\u6210\u548c\u9009\u62e9\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u4f7f\u7528\u8f85\u52a9\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u8bc4\u5206\u5e76\u9009\u62e9\u6700\u4f73\u63a8\u7406\u8def\u5f84\uff0c\u91cd\u70b9\u89e3\u51b3\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u6700\u4f18\u6027\uff08TCO\uff09\u95ee\u9898\u3002\u7406\u8bba\u5206\u6790\u8868\u660ePRM\u7684\u6cdb\u5316\u8bef\u5dee\u5f71\u54cd\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u63a7\u5236\u641c\u7d22\u884c\u4e3a\u7684CATS\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347LLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\uff0c\u89e3\u51b3\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u6700\u4f18\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faCompute-Aware Tree Search\uff08CATS\uff09\u6846\u67b6\uff0c\u7ed3\u5408PAC-Bayes\u7406\u8bba\u5206\u6790PRM\u6cdb\u5316\u8bef\u5dee\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u52a8\u6001\u63a7\u5236\u641c\u7d22\u884c\u4e3a\u3002", "result": "\u5728MATH\u548cAIME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCATS\u4f18\u4e8e\u5176\u4ed6\u5916\u90e8\u6d4b\u8bd5\u65f6\u95f4\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0cPRM\u6cdb\u5316\u8bef\u5dee\u7684\u964d\u4f4e\u53ef\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0cCATS\u6846\u67b6\u6709\u6548\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2505.17923", "pdf": "https://arxiv.org/pdf/2505.17923", "abs": "https://arxiv.org/abs/2505.17923", "authors": ["Yuekun Yao", "Yupei Du", "Dawei Zhu", "Michael Hahn", "Alexander Koller"], "title": "Language models can learn implicit multi-hop reasoning, but only if they have lots of training data", "categories": ["cs.CL"], "comment": null, "summary": "Implicit reasoning is the ability of a language model to solve multi-hop\nreasoning tasks in a single forward pass, without chain of thought. We\ninvestigate this capability using GPT2-style language models trained from\nscratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that\nwhile such models can indeed learn implicit $k$-hop reasoning, the required\ntraining data grows exponentially in $k$, and the required number of\ntransformer layers grows linearly in $k$. We offer a theoretical explanation\nfor why this depth growth is necessary. We further find that the data\nrequirement can be mitigated, but not eliminated, through curriculum learning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b8c\u6210\u591a\u8df3\u63a8\u7406\u7684\u80fd\u529b\uff08\u9690\u5f0f\u63a8\u7406\uff09\uff0c\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u6df1\u5ea6\u968f\u63a8\u7406\u6b65\u6570\u5448\u6307\u6570\u548c\u7ebf\u6027\u589e\u957f\uff0c\u5e76\u63d0\u51fa\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u4e0d\u4f9d\u8d56\u94fe\u5f0f\u601d\u7ef4\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u591a\u8df3\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5206\u6790\u5176\u8bad\u7ec3\u9700\u6c42\u548c\u6a21\u578b\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528GPT2\u98ce\u683c\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u63a7\u5236\u7684\u591a\u8df3\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u4ece\u5934\u8bad\u7ec3\uff0c\u7814\u7a76\u6570\u636e\u91cf\u548c\u6a21\u578b\u6df1\u5ea6\u5bf9\u9690\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u80fd\u5b66\u4e60\u9690\u5f0f\u63a8\u7406\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u968f\u63a8\u7406\u6b65\u6570\u6307\u6570\u589e\u957f\uff0c\u6a21\u578b\u6df1\u5ea6\u9700\u6c42\u7ebf\u6027\u589e\u957f\uff1b\u8bfe\u7a0b\u5b66\u4e60\u53ef\u7f13\u89e3\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u9690\u5f0f\u63a8\u7406\u80fd\u529b\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u6df1\u5c42\u6a21\u578b\uff0c\u8bfe\u7a0b\u5b66\u4e60\u662f\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 75.0}}
{"id": "2505.17683", "pdf": "https://arxiv.org/pdf/2505.17683", "abs": "https://arxiv.org/abs/2505.17683", "authors": ["Dan Yuan", "Yi Feng", "Ziyun Tang"], "title": "Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages,6 figures and 3 tables", "summary": "Intraventricular hemorrhage (IVH) is a severe neurological complication among\npremature infants, necessitating early and accurate detection from brain\nultrasound (US) images to improve clinical outcomes. While recent deep learning\nmethods offer promise for computer-aided diagnosis, challenges remain in\ncapturing both local spatial details and global contextual dependencies\ncritical for segmenting brain anatomies. In this work, we propose an enhanced\nResidual U-Net architecture incorporating two complementary attention\nmechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse\nAttention Layer (SAL). The CBAM improves the model's ability to refine spatial\nand channel-wise features, while the SAL introduces a dual-branch design,\nsparse attention filters out low-confidence query-key pairs to suppress noise,\nand dense attention ensures comprehensive information propagation. Extensive\nexperiments on the Brain US dataset demonstrate that our method achieves\nstate-of-the-art segmentation performance, with a Dice score of 89.04% and IoU\nof 81.84% for ventricle region segmentation. These results highlight the\neffectiveness of integrating spatial refinement and attention sparsity for\nrobust brain anatomy detection. Code is available at:\nhttps://github.com/DanYuan001/BrainImgSegment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CBAM\u548cSAL\u7684\u589e\u5f3a\u578bResidual U-Net\u67b6\u6784\uff0c\u7528\u4e8e\u65e9\u4ea7\u513f\u8111\u90e8\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u8111\u5ba4\u51fa\u8840\u5206\u5272\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u65e9\u4ea7\u513f\u8111\u5ba4\u51fa\u8840\uff08IVH\uff09\u7684\u65e9\u671f\u51c6\u786e\u68c0\u6d4b\u5bf9\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6355\u6349\u5c40\u90e8\u7a7a\u95f4\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\uff08CBAM\uff09\u548c\u7a00\u758f\u6ce8\u610f\u529b\u5c42\uff08SAL\uff09\u7684Residual U-Net\u67b6\u6784\uff0cCBAM\u4f18\u5316\u7a7a\u95f4\u548c\u901a\u9053\u7279\u5f81\uff0cSAL\u901a\u8fc7\u7a00\u758f\u548c\u5bc6\u96c6\u6ce8\u610f\u529b\u5206\u652f\u6291\u5236\u566a\u58f0\u5e76\u786e\u4fdd\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u5728\u8111\u90e8\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8111\u5ba4\u533a\u57df\u5206\u5272\u4e2d\u8fbe\u5230Dice\u5206\u657089.04%\u548cIoU 81.84%\uff0c\u6027\u80fd\u9886\u5148\u3002", "conclusion": "\u7ed3\u5408\u7a7a\u95f4\u7ec6\u5316\u548c\u6ce8\u610f\u529b\u7a00\u758f\u6027\u5bf9\u7a33\u5065\u7684\u8111\u90e8\u89e3\u5256\u68c0\u6d4b\u6709\u6548\u3002", "relevance": 20.0}}
{"id": "2505.17490", "pdf": "https://arxiv.org/pdf/2505.17490", "abs": "https://arxiv.org/abs/2505.17490", "authors": ["Haotian Liu", "Yuchuang Tong", "Zhengtao Zhang"], "title": "DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In physical Human-Robot Collaboration (pHRC), accurate human intent\nestimation and rational human-robot role allocation are crucial for safe and\nefficient assistance. Existing methods that rely on short-term motion data for\nintention estimation lack multi-step prediction capabilities, hindering their\nability to sense intent changes and adjust human-robot assignments\nautonomously, resulting in potential discrepancies. To address these issues, we\npropose a Dual Transformer-based Robot Trajectron (DTRT) featuring a\nhierarchical architecture, which harnesses human-guided motion and force data\nto rapidly capture human intent changes, enabling accurate trajectory\npredictions and dynamic robot behavior adjustments for effective collaboration.\nSpecifically, human intent estimation in DTRT uses two Transformer-based\nConditional Variational Autoencoders (CVAEs), incorporating robot motion data\nin obstacle-free case with human-guided trajectory and force for obstacle\navoidance. Additionally, Differential Cooperative Game Theory (DCGT) is\nemployed to synthesize predictions based on human-applied forces, ensuring\nrobot behavior align with human intention. Compared to state-of-the-art (SOTA)\nmethods, DTRT incorporates human dynamics into long-term prediction, providing\nan accurate understanding of intention and enabling rational role allocation,\nachieving robot autonomy and maneuverability. Experiments demonstrate DTRT's\naccurate intent estimation and superior collaboration performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ccTransformer\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff08DTRT\uff09\uff0c\u7528\u4e8e\u7269\u7406\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4eba\u4f53\u610f\u56fe\u4f30\u8ba1\u548c\u52a8\u6001\u89d2\u8272\u5206\u914d\uff0c\u7ed3\u5408\u4e86Transformer\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u77ed\u671f\u8fd0\u52a8\u6570\u636e\uff0c\u7f3a\u4e4f\u591a\u6b65\u9884\u6d4b\u80fd\u529b\uff0c\u5bfc\u81f4\u610f\u56fe\u53d8\u5316\u611f\u77e5\u4e0d\u8db3\u548c\u89d2\u8272\u5206\u914d\u4e0d\u51c6\u786e\u3002DTRT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u534f\u4f5c\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "DTRT\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u7ed3\u5408Transformer-based CVAE\u548c\u5dee\u5206\u5408\u4f5c\u535a\u5f08\u7406\u8bba\uff08DCGT\uff09\uff0c\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u548c\u529b\u6570\u636e\u5b9e\u73b0\u610f\u56fe\u4f30\u8ba1\u548c\u52a8\u6001\u884c\u4e3a\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDTRT\u5728\u610f\u56fe\u4f30\u8ba1\u548c\u534f\u4f5c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u957f\u671f\u9884\u6d4b\u548c\u52a8\u6001\u89d2\u8272\u5206\u914d\u3002", "conclusion": "DTRT\u901a\u8fc7\u7ed3\u5408Transformer\u548cDCGT\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u610f\u56fe\u611f\u77e5\u548c\u81ea\u4e3b\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 30.0}}
{"id": "2505.18069", "pdf": "https://arxiv.org/pdf/2505.18069", "abs": "https://arxiv.org/abs/2505.18069", "authors": ["David Koplow", "Tomaso Poggio", "Liu Ziyin"], "title": "Emergence of Hebbian Dynamics in Regularized Non-Local Learners", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Stochastic Gradient Descent (SGD) has emerged as a remarkably effective\nlearning algorithm, underpinning nearly all state-of-the-art machine learning\nmodels, from large language models to autonomous vehicles. Despite its\npractical success, SGD appears fundamentally distinct from biological learning\nmechanisms. It is widely believed that the biological brain can not implement\ngradient descent because it is nonlocal, and we have found little (if any)\nexperimental evidence for it. In contrast, the brain is widely thought to learn\nvia local Hebbian learning principles, which have been seen as incompatible\nwith gradient descent. In this paper, we establish a theoretical and empirical\nconnection between the learning signals of neural networks trained using SGD\nwith weight decay and those trained with Hebbian learning near convergence. We\nshow that SGD with regularization can appear to learn according to a Hebbian\nrule, and SGD with injected noise according to an anti-Hebbian rule. We also\nprovide empirical evidence that Hebbian learning properties can emerge in a\nnetwork with weight decay from virtually any learning rule--even random ones.\nThese results may bridge a long-standing gap between artificial and biological\nlearning, revealing Hebbian properties as an epiphenomenon of deeper\noptimization principles and cautioning against interpreting their presence in\nneural data as evidence against more complex hetero-synaptic mechanisms.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86SGD\u4e0eHebbian\u5b66\u4e60\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u53d1\u73b0SGD\u5728\u6b63\u5219\u5316\u6761\u4ef6\u4e0b\u53ef\u4ee5\u8868\u73b0\u51fa\u7c7b\u4f3cHebbian\u5b66\u4e60\u7684\u7279\u6027\u3002", "motivation": "\u7814\u7a76SGD\u4e0e\u751f\u7269\u5b66\u4e60\u673a\u5236\uff08\u5982Hebbian\u5b66\u4e60\uff09\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\uff0c\u4ee5\u5f25\u5408\u4eba\u5de5\u4e0e\u751f\u7269\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u6bd4\u8f83SGD\uff08\u542b\u6743\u91cd\u8870\u51cf\u548c\u566a\u58f0\u6ce8\u5165\uff09\u4e0eHebbian\u5b66\u4e60\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "SGD\u5728\u6b63\u5219\u5316\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7c7b\u4f3cHebbian\u5b66\u4e60\u7684\u7279\u6027\uff0c\u800c\u566a\u58f0\u6ce8\u5165\u5219\u8868\u73b0\u51fa\u53cdHebbian\u7279\u6027\u3002", "conclusion": "Hebbian\u7279\u6027\u53ef\u80fd\u662f\u4f18\u5316\u539f\u5219\u7684\u526f\u4ea7\u54c1\uff0c\u9700\u8c28\u614e\u89e3\u91ca\u5176\u5728\u795e\u7ecf\u6570\u636e\u4e2d\u7684\u5b58\u5728\u3002", "relevance": 40.0}}
{"id": "2505.17950", "pdf": "https://arxiv.org/pdf/2505.17950", "abs": "https://arxiv.org/abs/2505.17950", "authors": ["Tom Bleckmann", "Paul Tschisgale"], "title": "Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models", "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "comment": null, "summary": "Recent advancements in Natural Language Processing (NLP) have facilitated the\nanalysis of student-generated language products in learning analytics (LA),\nparticularly through the use of NLP embedding models. Yet when it comes to\nscience-related language, symbolic expressions such as equations and formulas\nintroduce challenges that current embedding models struggle to address.\nExisting studies and applications often either overlook these challenges or\nremove symbolic expressions altogether, potentially leading to biased findings\nand diminished performance of LA applications. This study therefore explores\nhow contemporary embedding models differ in their capability to process and\ninterpret science-related symbolic expressions. To this end, various embedding\nmodels are evaluated using physics-specific symbolic expressions drawn from\nauthentic student responses, with performance assessed via two approaches:\nsimilarity-based analyses and integration into a machine learning pipeline. Our\nfindings reveal significant differences in model performance, with OpenAI's\nGPT-text-embedding-3-large outperforming all other examined models, though its\nadvantage over other models was moderate rather than decisive. Beyond\nperformance, additional factors such as cost, regulatory compliance, and model\ntransparency are discussed as key considerations for model selection. Overall,\nthis study underscores the importance for LA researchers and practitioners of\ncarefully selecting NLP embedding models when working with science-related\nlanguage products that include symbolic expressions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540cNLP\u5d4c\u5165\u6a21\u578b\u5728\u5904\u7406\u79d1\u5b66\u76f8\u5173\u7b26\u53f7\u8868\u8fbe\u5f0f\u65f6\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53d1\u73b0GPT-text-embedding-3-large\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4f18\u52bf\u6709\u9650\uff0c\u5e76\u5f3a\u8c03\u4e86\u6a21\u578b\u9009\u62e9\u65f6\u9700\u8003\u8651\u6210\u672c\u3001\u5408\u89c4\u6027\u548c\u900f\u660e\u5ea6\u7b49\u56e0\u7d20\u3002", "motivation": "\u79d1\u5b66\u76f8\u5173\u8bed\u8a00\u4e2d\u7684\u7b26\u53f7\u8868\u8fbe\u5f0f\uff08\u5982\u516c\u5f0f\uff09\u5bf9\u73b0\u6709NLP\u5d4c\u5165\u6a21\u578b\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u89c6\u6216\u79fb\u9664\u8fd9\u4e9b\u8868\u8fbe\u5f0f\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u89c1\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u76f8\u4f3c\u6027\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u8bc4\u4f30\u591a\u79cd\u5d4c\u5165\u6a21\u578b\u5728\u7269\u7406\u76f8\u5173\u7b26\u53f7\u8868\u8fbe\u5f0f\u4e0a\u7684\u6027\u80fd\u3002", "result": "GPT-text-embedding-3-large\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4f18\u52bf\u4e0d\u663e\u8457\uff1b\u6a21\u578b\u9009\u62e9\u8fd8\u9700\u8003\u8651\u6210\u672c\u3001\u5408\u89c4\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u5728\u6d89\u53ca\u79d1\u5b66\u7b26\u53f7\u8868\u8fbe\u5f0f\u7684\u8bed\u8a00\u5206\u6790\u4e2d\uff0c\u9700\u8c28\u614e\u9009\u62e9NLP\u5d4c\u5165\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.17491", "pdf": "https://arxiv.org/pdf/2505.17491", "abs": "https://arxiv.org/abs/2505.17491", "authors": ["Reza Marzban", "Hamed Abiri", "Raphael Pestourie", "Ali Adibi"], "title": "HiLAB: A Hybrid Inverse-Design Framework", "categories": ["physics.optics", "cs.AI", "physics.app-ph"], "comment": "19 pages, 7 figures", "summary": "HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based\npartial optimizations, and Bayesian optimization) is a new paradigm for inverse\ndesign of nanophotonic structures. Combining early-terminated topological\noptimization (TO) with a Vision Transformer-based variational autoencoder (VAE)\nand a Bayesian search, HiLAB addresses multi-functional device design by\ngenerating diverse freeform configurations at reduced simulation costs.\nShortened adjoint-driven TO runs, coupled with randomized physical parameters,\nproduce robust initial structures. These structures are compressed into a\ncompact latent space by the VAE, enabling Bayesian optimization to co-optimize\ngeometry and physical hyperparameters. Crucially, the trained VAE can be reused\nfor alternative objectives or constraints by adjusting only the acquisition\nfunction. Compared to conventional TO pipelines prone to local optima, HiLAB\nsystematically explores near-global optima with considerably fewer\nelectromagnetic simulations. Even after accounting for training overhead, the\ntotal number of full simulations decreases by over an order of magnitude,\naccelerating the discovery of fabrication-friendly devices. Demonstrating its\nefficacy, HiLAB is used to design an achromatic beam deflector for red, green,\nand blue wavelengths, achieving balanced diffraction efficiencies of ~25% while\nmitigating chromatic aberrations-a performance surpassing existing\ndemonstrations. Overall, HiLAB provides a flexible platform for robust,\nmulti-parameter photonic designs and rapid adaptation to next-generation\nnanophotonic challenges.", "AI": {"tldr": "HiLAB\u662f\u4e00\u79cd\u7528\u4e8e\u7eb3\u7c73\u5149\u5b50\u7ed3\u6784\u9006\u5411\u8bbe\u8ba1\u7684\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u62d3\u6251\u4f18\u5316\u3001\u89c6\u89c9Transformer\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u8d1d\u53f6\u65af\u641c\u7d22\uff0c\u5b9e\u73b0\u591a\u529f\u80fd\u8bbe\u5907\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u53c2\u6570\u5149\u5b50\u8bbe\u8ba1\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u65e9\u671f\u7ec8\u6b62\u7684\u62d3\u6251\u4f18\u5316\u3001\u89c6\u89c9Transformer\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u8d1d\u53f6\u65af\u641c\u7d22\uff0c\u538b\u7f29\u7ed3\u6784\u5230\u6f5c\u5728\u7a7a\u95f4\u5e76\u4f18\u5316\u51e0\u4f55\u4e0e\u7269\u7406\u8d85\u53c2\u6570\u3002", "result": "\u8bbe\u8ba1\u51fa\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6d88\u8272\u5dee\u5149\u675f\u504f\u8f6c\u5668\uff0c\u884d\u5c04\u6548\u7387\u8fbe25%\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "HiLAB\u4e3a\u7eb3\u7c73\u5149\u5b50\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5149\u5b50\u5b66\u6311\u6218\u3002", "relevance": 30.0}}
{"id": "2505.18080", "pdf": "https://arxiv.org/pdf/2505.18080", "abs": "https://arxiv.org/abs/2505.18080", "authors": ["Chunlin Gong", "Yin Wang", "Jingru Li", "Hanleran Zhang"], "title": "AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 11 figures", "summary": "This paper presents AFD-STA Net, a neural framework integrating adaptive\nfiltering and spatiotemporal dynamics learning for predicting high-dimensional\nchaotic systems governed by partial differential equations. The architecture\ncombines: 1) An adaptive exponential smoothing module with position-aware decay\ncoefficients for robust attractor reconstruction, 2) Parallel attention\nmechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated\nfusion of multiscale features, and 4) Deep projection networks with\ndimension-scaling capabilities. Numerical experiments on nonlinear PDE systems\ndemonstrate the model's effectiveness in maintaining prediction accuracy under\nboth smooth and strongly chaotic regimes while exhibiting noise tolerance\nthrough adaptive filtering. Component ablation studies confirm critical\ncontributions from each module, particularly highlighting the essential role of\nspatiotemporal attention in learning complex dynamical interactions. The\nframework shows promising potential for real-world applications requiring\nsimultaneous handling of measurement uncertainties and high-dimensional\nnonlinear dynamics.", "AI": {"tldr": "AFD-STA Net\u662f\u4e00\u4e2a\u7ed3\u5408\u81ea\u9002\u5e94\u6ee4\u6ce2\u548c\u65f6\u7a7a\u52a8\u6001\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u9884\u6d4b\u7cbe\u5ea6\u548c\u566a\u58f0\u5bb9\u5fcd\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u6307\u6570\u5e73\u6ed1\u3001\u5e76\u884c\u6ce8\u610f\u529b\u673a\u5236\u3001\u52a8\u6001\u95e8\u63a7\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u6295\u5f71\u7f51\u7edc\u3002", "result": "\u5728\u975e\u7ebf\u6027PDE\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u566a\u58f0\u5bb9\u5fcd\u6027\u3002", "conclusion": "\u6846\u67b6\u5728\u540c\u65f6\u5904\u7406\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u548c\u9ad8\u7ef4\u975e\u7ebf\u6027\u52a8\u6001\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17952", "pdf": "https://arxiv.org/pdf/2505.17952", "abs": "https://arxiv.org/abs/2505.17952", "authors": ["Che Liu", "Haozhe Wang", "Jiazhen Pan", "Zhongwei Wan", "Yong Dai", "Fangzhen Lin", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.", "AI": {"tldr": "AlphaMed is a medical LLM that achieves state-of-the-art performance through reinforcement learning (RL) with rule-based rewards, bypassing supervised fine-tuning (SFT) and chain-of-thought (CoT) data. It outperforms larger models on medical QA benchmarks and highlights dataset informativeness as key to reasoning.", "motivation": "To enable interpretable decision-making and improve performance in LLMs for clinical applications without relying on costly SFT or CoT data.", "method": "Uses minimalist rule-based RL on public multiple-choice QA datasets, avoiding SFT or distilled CoT data.", "result": "Achieves state-of-the-art results on six medical QA benchmarks, surpassing larger or closed-source models.", "conclusion": "Dataset informativeness drives reasoning performance, and minimalist RL on informative data can induce reasoning without CoT supervision. Current benchmarks need more challenge.", "relevance": 90.0}}
{"id": "2505.18081", "pdf": "https://arxiv.org/pdf/2505.18081", "abs": "https://arxiv.org/abs/2505.18081", "authors": ["Adam D. Cobb", "Susmit Jha"], "title": "Backpropagation-Free Metropolis-Adjusted Langevin Algorithm", "categories": ["cs.LG", "cs.AI"], "comment": "19 Pages, 8 Figures", "summary": "Recent work on backpropagation-free learning has shown that it is possible to\nuse forward-mode automatic differentiation (AD) to perform optimization on\ndifferentiable models. Forward-mode AD requires sampling a tangent vector for\neach forward pass of a model. The result is the model evaluation with the\ndirectional derivative along the tangent. In this paper, we illustrate how the\nsampling of this tangent vector can be incorporated into the proposal mechanism\nfor the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the\nfirst to introduce a backpropagation-free gradient-based Markov chain Monte\nCarlo (MCMC) algorithm. We also extend to a novel backpropagation-free\nposition-specific preconditioned forward-mode MALA that leverages Hessian\ninformation. Overall, we propose four new algorithms: Forward MALA; Line\nForward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward\nMALA. We highlight the reduced computational cost of the forward-mode samplers\nand show that forward-mode is competitive with the original MALA, while even\noutperforming it depending on the probabilistic model. We include Bayesian\ninference results on a range of probabilistic models, including hierarchical\ndistributions and Bayesian neural networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u524d\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\uff08AD\uff09\u4f18\u5316\u53ef\u5fae\u5206\u6a21\u578b\uff0c\u5e76\u6269\u5c55\u4e86\u56db\u79cd\u65b0\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7d22\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06\u524d\u5411\u6a21\u5f0fAD\u4e0eMetropolis-Adjusted Langevin Algorithm\uff08MALA\uff09\u7ed3\u5408\uff0c\u63d0\u51fa\u56db\u79cd\u65b0\u7b97\u6cd5\uff1aForward MALA\u3001Line Forward MALA\u3001Pre-conditioned Forward MALA\u548cPre-conditioned Line Forward MALA\u3002", "result": "\u524d\u5411\u6a21\u5f0f\u91c7\u6837\u5668\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u6027\u80fd\u4e0e\u539f\u59cbMALA\u76f8\u5f53\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u6982\u7387\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u524d\u5411\u6a21\u5f0fAD\u5728MCMC\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u5e73\u8861\u65b9\u9762\u3002", "relevance": 40.0}}
{"id": "2505.17964", "pdf": "https://arxiv.org/pdf/2505.17964", "abs": "https://arxiv.org/abs/2505.17964", "authors": ["Jiashun Jin", "Tracy Ke", "Bingcheng Sui", "Zhenggang Wang"], "title": "Counting Cycles with Deepseek", "categories": ["cs.CL"], "comment": null, "summary": "Despite recent progress, AI still struggles on advanced mathematics. We\nconsider a difficult open problem: How to derive a Computationally Efficient\nEquivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not\nhave known general solutions, and requires delicate combinatorics and tedious\ncalculations. Such a task is hard to accomplish by humans but is an ideal\nexample where AI can be very helpful. We solve the problem by combining a novel\napproach we propose and the powerful coding skills of AI. Our results use\ndelicate graph theory and contain new formulas for general cases that have not\nbeen discovered before. We find that, while AI is unable to solve the problem\nall by itself, it is able to solve it if we provide it with a clear strategy, a\nstep-by-step guidance and carefully written prompts. For simplicity, we focus\nour study on DeepSeek-R1 but we also investigate other AI approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65b0\u9896\u65b9\u6cd5\u4e0eAI\u7f16\u7801\u6280\u80fd\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u9ad8\u6548\u7b49\u6548\u5f62\u5f0f\uff08CEEF\uff09\u7684\u96be\u9898\uff0c\u5c55\u793a\u4e86AI\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "AI\u5728\u9ad8\u7ea7\u6570\u5b66\u95ee\u9898\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u9700\u8981\u590d\u6742\u7ec4\u5408\u548c\u8ba1\u7b97\u7684CEEF\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u7b56\u7565\u4e0eAI\u80fd\u529b\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u7ed3\u5408AI\u7684\u7f16\u7801\u80fd\u529b\uff0c\u901a\u8fc7\u660e\u786e\u7684\u7b56\u7565\u3001\u9010\u6b65\u6307\u5bfc\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u89e3\u51b3CEEF\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u4e86\u672a\u89e3\u51b3\u7684\u4e00\u822c\u60c5\u51b5\u4e0b\u7684\u65b0\u516c\u5f0f\uff0c\u5c55\u793a\u4e86AI\u5728\u4eba\u7c7b\u6307\u5bfc\u4e0b\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\u3002", "conclusion": "AI\u867d\u65e0\u6cd5\u72ec\u7acb\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\uff0c\u4f46\u5728\u4eba\u7c7b\u7b56\u7565\u548c\u6307\u5bfc\u4e0b\u80fd\u6709\u6548\u5b8c\u6210\u4efb\u52a1\uff0c\u4e3aAI\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.18082", "pdf": "https://arxiv.org/pdf/2505.18082", "abs": "https://arxiv.org/abs/2505.18082", "authors": ["Georgios Kementzidis", "Erin Wong", "John Nicholson", "Ruichen Xu", "Yuefan Deng"], "title": "An Iterative Framework for Generative Backmapping of Coarse Grained Proteins", "categories": ["cs.LG"], "comment": "17 pages, 8 figures. For associated code repositories, see: CGVAE:\n  https://github.com/wwang2/CoarseGrainingVAE GenZProT:\n  https://github.com/learningmatter-mit/GenZProt See also arXiv:2201.12176 and\n  arXiv:2303.01569 for related methods", "summary": "The techniques of data-driven backmapping from coarse-grained (CG) to\nfine-grained (FG) representation often struggle with accuracy, unstable\ntraining, and physical realism, especially when applied to complex systems such\nas proteins. In this work, we introduce a novel iterative framework by using\nconditional Variational Autoencoders and graph-based neural networks,\nspecifically designed to tackle the challenges associated with such large-scale\nbiomolecules. Our method enables stepwise refinement from CG beads to full\natomistic details. We outline the theory of iterative generative backmapping\nand demonstrate via numerical experiments the advantages of multistep schemes\nby applying them to proteins of vastly different structures with very coarse\nrepresentations. This multistep approach not only improves the accuracy of\nreconstructions but also makes the training process more computationally\nefficient for proteins with ultra-CG representations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u91cd\u5efa\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u91cd\u5efa\u4e2d\u51c6\u786e\u6027\u4f4e\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u7269\u7406\u771f\u5b9e\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u8fed\u4ee3\u6846\u67b6\uff0c\u9010\u6b65\u4ece\u7c97\u7c92\u5ea6\u5230\u5168\u539f\u5b50\u7ec6\u8282\u91cd\u5efa\u3002", "result": "\u591a\u6b65\u65b9\u6cd5\u63d0\u9ad8\u4e86\u91cd\u5efa\u51c6\u786e\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8d85\u7c97\u7c92\u5ea6\u86cb\u767d\u8d28\u8bad\u7ec3\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fed\u4ee3\u751f\u6210\u5f0f\u53cd\u5411\u6620\u5c04\u65b9\u6cd5\u5728\u590d\u6742\u751f\u7269\u5206\u5b50\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.17978", "pdf": "https://arxiv.org/pdf/2505.17978", "abs": "https://arxiv.org/abs/2505.17978", "authors": ["Rui Cao", "Zifeng Ding", "Zhijiang Guo", "Michael Schlichtkrull", "Andreas Vlachos"], "title": "AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web", "categories": ["cs.CL"], "comment": null, "summary": "Textual claims are often accompanied by images to enhance their credibility\nand spread on social media, but this also raises concerns about the spread of\nmisinformation. Existing datasets for automated verification of image-text\nclaims remain limited, as they often consist of synthetic claims and lack\nevidence annotations to capture the reasoning behind the verdict. In this work,\nwe introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text\nclaims. Each claim is annotated with question-answer (QA) pairs containing\nevidence from the web, reflecting a decomposed reasoning regarding the verdict.\nWe mitigate common challenges in fact-checking datasets such as contextual\ndependence, temporal leakage, and evidence insufficiency, via claim\nnormalization, temporally constrained evidence annotation, and a two-stage\nsufficiency check. We assess the consistency of the annotation in AVerImaTeC\nvia inter-annotator studies, achieving a $\\kappa=0.742$ on verdicts and\n$74.7\\%$ consistency on QA pairs. We also propose a novel evaluation method for\nevidence retrieval and conduct extensive experiments to establish baselines for\nverifying image-text claims using open-web evidence.", "AI": {"tldr": "AVerImaTeC\u662f\u4e00\u4e2a\u5305\u542b1,297\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf-\u6587\u672c\u58f0\u660e\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u9a8c\u8bc1\uff0c\u5e26\u6709\u8bc1\u636e\u6ce8\u91ca\u548c\u5206\u89e3\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u5408\u6210\u6570\u636e\u4e14\u7f3a\u4e4f\u8bc1\u636e\u6ce8\u91ca\uff0c\u65e0\u6cd5\u652f\u6301\u81ea\u52a8\u5316\u9a8c\u8bc1\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u58f0\u660e\u89c4\u8303\u5316\u3001\u65f6\u95f4\u7ea6\u675f\u8bc1\u636e\u6ce8\u91ca\u548c\u4e24\u9636\u6bb5\u5145\u5206\u6027\u68c0\u67e5\uff0c\u6784\u5efaAVerImaTeC\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u6807\u6ce8\u4e00\u81f4\u6027\u8f83\u9ad8\uff08\u03ba=0.742\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc1\u636e\u68c0\u7d22\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "AVerImaTeC\u4e3a\u56fe\u50cf-\u6587\u672c\u58f0\u660e\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002", "relevance": 30.0}}
{"id": "2505.17860", "pdf": "https://arxiv.org/pdf/2505.17860", "abs": "https://arxiv.org/abs/2505.17860", "authors": ["Wenning Xu", "Shiyu Fan", "Paul Henderson", "Edmond S. L. Ho"], "title": "Multi-Person Interaction Generation from Two-Person Motion Priors", "categories": ["cs.GR", "cs.CV", "cs.LG", "I.3.7"], "comment": "SIGGRAPH 2025 Conference Papers", "summary": "Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u591a\u4eba\u4ea4\u4e92\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u7684\u53cc\u4eba\u8fd0\u52a8\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u89e3\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u4eba\u4ea4\u4e92\u3002", "motivation": "\u591a\u4eba\u4ea4\u4e92\u5efa\u6a21\u662f\u4e00\u4e2a\u8f83\u5c11\u63a2\u7d22\u7684\u9886\u57df\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u591a\u4eba\u4ea4\u4e92\u3002", "method": "\u5c06\u591a\u4eba\u4ea4\u4e92\u5206\u89e3\u4e3a\u56fe\u7ed3\u6784\u4e2d\u7684\u53cc\u4eba\u4ea4\u4e92\uff08Pairwise Interaction Graph\uff09\uff0c\u5e76\u5728\u6269\u6563\u91c7\u6837\u4e2d\u5f15\u5165\u56fe\u76f8\u5173\u5f15\u5bfc\u9879\u4ee5\u51cf\u5c11\u751f\u6210\u4e2d\u7684\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u4f2a\u5f71\u548c\u751f\u6210\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u6269\u6563\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u591a\u4eba\u4ea4\u4e92\u7684\u751f\u6210\u3002", "relevance": 40.0}}
{"id": "2505.17498", "pdf": "https://arxiv.org/pdf/2505.17498", "abs": "https://arxiv.org/abs/2505.17498", "authors": ["Marco Brandizi", "Carlos Bobed", "Luca Garulli", "Arn\u00e9 de Klerk", "Keywan Hassani-Pak"], "title": "Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark based on the rdf2pg Framework and Plant Biology Data", "categories": ["cs.DB", "cs.AI"], "comment": "19 pages", "summary": "Linked Data and labelled property graphs (LPG) are two data management\napproaches with complementary strengths and weaknesses, making their\nintegration beneficial for sharing datasets and supporting software ecosystems.\nIn this paper, we introduce rdf2pg, an extensible framework for mapping RDF\ndata to semantically equivalent LPG formats and data-bases. Utilising this\nframework, we perform a comparative analysis of three popular graph databases -\nVirtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages\nSPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments\nunderline the strengths and limitations of these graph database technologies.\nAdditionally, we highlight the potential of rdf2pg as a versatile tool for\nenabling polyglot access to knowledge graphs, aligning with established\nstandards of Linked Data and the Semantic Web.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86rdf2pg\u6846\u67b6\uff0c\u7528\u4e8e\u5c06RDF\u6570\u636e\u6620\u5c04\u5230LPG\u683c\u5f0f\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u56fe\u6570\u636e\u5e93\u7684\u6027\u80fd\u3002", "motivation": "\u7ed3\u5408Linked Data\u548cLPG\u7684\u4f18\u52bf\uff0c\u4fc3\u8fdb\u6570\u636e\u96c6\u5171\u4eab\u548c\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u652f\u6301\u3002", "method": "\u5f00\u53d1rdf2pg\u6846\u67b6\uff0c\u8fdb\u884c\u56fe\u6570\u636e\u5e93\uff08Virtuoso\u3001Neo4j\u3001ArcadeDB\uff09\u548c\u67e5\u8be2\u8bed\u8a00\uff08SPARQL\u3001Cypher\u3001Gremlin\uff09\u7684\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u8fd9\u4e9b\u56fe\u6570\u636e\u5e93\u6280\u672f\u7684\u4f18\u7f3a\u70b9\uff0c\u5c55\u793a\u4e86rdf2pg\u4f5c\u4e3a\u591a\u8bed\u8a00\u77e5\u8bc6\u56fe\u8c31\u8bbf\u95ee\u5de5\u5177\u7684\u6f5c\u529b\u3002", "conclusion": "rdf2pg\u6846\u67b6\u7b26\u5408Linked Data\u548c\u8bed\u4e49\u7f51\u6807\u51c6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "relevance": 10.0}}
{"id": "2505.18083", "pdf": "https://arxiv.org/pdf/2505.18083", "abs": "https://arxiv.org/abs/2505.18083", "authors": ["Quentin Clark", "Florian Shkurti"], "title": "What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?", "categories": ["cs.LG", "cs.RO"], "comment": "9 Pages", "summary": "In planning, stitching is an ability of algorithms to piece together\nsub-trajectories of data they are trained on to generate new and diverse\nbehaviours. While stitching is historically a strength of offline reinforcement\nlearning, recent generative behavioural cloning (BC) methods have also shown\nproficiency at stitching. However, the main factors behind this are poorly\nunderstood, hindering the development of new algorithms that can reliably\nstitch. Focusing on diffusion planners trained via BC, we find two properties\nare needed to compose: \\emph{positional equivariance} and \\emph{local\nreceptiveness}. We use these two properties to explain architecture, data, and\ninference choices in existing generative BC methods based on diffusion\nplanning, including replanning frequency, data augmentation, and data scaling.\nExperimental comparisions show that (1) while locality is more important than\npositional equivariance in creating a diffusion planner capable of composition,\nboth are crucial (2) enabling these properties through relatively simple\narchitecture choices can be competitive with more computationally expensive\nmethods such as replanning or scaling data, and (3) simple inpainting-based\nguidance can guide architecturally compositional models to enable\ngeneralization in goal-conditioned settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u65b9\u6cd5\u4e2d\u7684\u201cstitching\u201d\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u5c5e\u6027\uff08\u4f4d\u7f6e\u7b49\u53d8\u6027\u548c\u5c40\u90e8\u611f\u53d7\u6027\uff09\u6765\u89e3\u91ca\u6269\u6563\u89c4\u5212\u5668\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5c5e\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u751f\u6210\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u4e2d\u201cstitching\u201d\u80fd\u529b\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u4ee5\u63a8\u52a8\u80fd\u591f\u53ef\u9760\u5b9e\u73b0\u201cstitching\u201d\u7684\u65b0\u7b97\u6cd5\u5f00\u53d1\u3002", "method": "\u7814\u7a76\u6269\u6563\u89c4\u5212\u5668\u5728\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4f4d\u7f6e\u7b49\u53d8\u6027\u548c\u5c40\u90e8\u611f\u53d7\u6027\u5bf9\u201cstitching\u201d\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u67b6\u6784\u3001\u6570\u636e\u548c\u63a8\u7406\u9009\u62e9\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c40\u90e8\u611f\u53d7\u6027\u6bd4\u4f4d\u7f6e\u7b49\u53d8\u6027\u66f4\u91cd\u8981\uff0c\u4f46\u4e24\u8005\u5747\u5173\u952e\uff1b\u901a\u8fc7\u7b80\u5355\u7684\u67b6\u6784\u9009\u62e9\u53ef\u4ee5\u5ab2\u7f8e\u8ba1\u7b97\u6602\u8d35\u7684\u65b9\u6cd5\uff1b\u57fa\u4e8e\u4fee\u590d\u7684\u5f15\u5bfc\u80fd\u63d0\u5347\u76ee\u6807\u6761\u4ef6\u8bbe\u7f6e\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4f4d\u7f6e\u7b49\u53d8\u6027\u548c\u5c40\u90e8\u611f\u53d7\u6027\u662f\u5b9e\u73b0\u201cstitching\u201d\u7684\u5173\u952e\u5c5e\u6027\uff0c\u7b80\u5355\u7684\u67b6\u6784\u6539\u8fdb\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "relevance": 70.0}}
{"id": "2505.17998", "pdf": "https://arxiv.org/pdf/2505.17998", "abs": "https://arxiv.org/abs/2505.17998", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "TRACE for Tracking the Emergence of Semantic Representations in Transformers", "categories": ["cs.CL"], "comment": null, "summary": "Modern transformer models exhibit phase transitions during training, distinct\nshifts from memorisation to abstraction, but the mechanisms underlying these\ntransitions remain poorly understood. Prior work has often focused on endpoint\nrepresentations or isolated signals like curvature or mutual information,\ntypically in symbolic or arithmetic domains, overlooking the emergence of\nlinguistic structure. We introduce TRACE (Tracking Representation Abstraction\nand Compositional Emergence), a diagnostic framework combining geometric,\ninformational, and linguistic signals to detect phase transitions in\nTransformer-based LMs. TRACE leverages a frame-semantic data generation method,\nABSynth, that produces annotated synthetic corpora with controllable\ncomplexity, lexical distributions, and structural entropy, while being fully\nannotated with linguistic categories, enabling precise analysis of abstraction\nemergence. Experiments reveal that (i) phase transitions align with clear\nintersections between curvature collapse and dimension stabilisation; (ii)\nthese geometric shifts coincide with emerging syntactic and semantic accuracy;\n(iii) abstraction patterns persist across architectural variants, with\ncomponents like feedforward networks affecting optimisation stability rather\nthan fundamentally altering trajectories. This work advances our understanding\nof how linguistic abstractions emerge in LMs, offering insights into model\ninterpretability, training efficiency, and compositional generalisation that\ncould inform more principled approaches to LM development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTRACE\u6846\u67b6\uff0c\u7ed3\u5408\u51e0\u4f55\u3001\u4fe1\u606f\u548c\u8bed\u8a00\u4fe1\u53f7\u68c0\u6d4bTransformer\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bad\u7ec3\u9636\u6bb5\u8f6c\u53d8\uff0c\u63ed\u793a\u62bd\u8c61\u80fd\u529b\u4e0e\u51e0\u4f55\u53d8\u5316\u7684\u5173\u8054\u3002", "motivation": "\u7406\u89e3Transformer\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u4ece\u8bb0\u5fc6\u5230\u62bd\u8c61\u7684\u8f6c\u53d8\u673a\u5236\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5bf9\u8bed\u8a00\u7ed3\u6784\u6d8c\u73b0\u7684\u5ffd\u89c6\u3002", "method": "\u5f15\u5165TRACE\u6846\u67b6\u548cABSynth\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u5408\u6210\u8bed\u6599\u5206\u6790\u62bd\u8c61\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u9636\u6bb5\u8f6c\u53d8\u4e0e\u51e0\u4f55\u53d8\u5316\uff08\u5982\u66f2\u7387\u5d29\u6e83\u548c\u7ef4\u5ea6\u7a33\u5b9a\uff09\u53ca\u8bed\u8a00\u80fd\u529b\u6d8c\u73b0\u76f8\u5173\uff0c\u4e14\u4e0d\u540c\u67b6\u6784\u4e2d\u62bd\u8c61\u6a21\u5f0f\u4e00\u81f4\u3002", "conclusion": "TRACE\u4e3a\u8bed\u8a00\u6a21\u578b\u62bd\u8c61\u80fd\u529b\u7684\u7406\u89e3\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u7ec4\u5408\u6cdb\u5316\u7684\u6539\u8fdb\u3002", "relevance": 90.0}}
{"id": "2505.17500", "pdf": "https://arxiv.org/pdf/2505.17500", "abs": "https://arxiv.org/abs/2505.17500", "authors": ["Vladimir Baulin", "Austin Cook", "Daniel Friedman", "Janna Lumiruusu", "Andrew Pashea", "Shagor Rahman", "Benedikt Waldeck"], "title": "The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes", "categories": ["cond-mat.soft", "cs.AI"], "comment": null, "summary": "The prevailing model for disseminating scientific knowledge relies on\nindividual publications dispersed across numerous journals and archives. This\nlegacy system is ill suited to the recent exponential proliferation of\npublications, contributing to insurmountable information overload, issues\nsurrounding reproducibility and retractions. We introduce the Discovery Engine,\na framework to address these challenges by transforming an array of\ndisconnected literature into a unified, computationally tractable\nrepresentation of a scientific domain. Central to our approach is the\nLLM-driven distillation of publications into structured \"knowledge artifacts,\"\ninstances of a universal conceptual schema, complete with verifiable links to\nsource evidence. These artifacts are then encoded into a high-dimensional\nConceptual Tensor. This tensor serves as the primary, compressed representation\nof the synthesized field, where its labeled modes index scientific components\n(concepts, methods, parameters, relations) and its entries quantify their\ninterdependencies. The Discovery Engine allows dynamic \"unrolling\" of this\ntensor into human-interpretable views, such as explicit knowledge graphs (the\nCNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI\nagents operate directly on the graph using abstract mathematical and learned\noperations to navigate the knowledge landscape, identify non-obvious\nconnections, pinpoint gaps, and assist researchers in generating novel\nknowledge artifacts (hypotheses, designs). By converting literature into a\nstructured tensor and enabling agent-based interaction with this compact\nrepresentation, the Discovery Engine offers a new paradigm for AI-augmented\nscientific inquiry and accelerated discovery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiscovery Engine\u6846\u67b6\uff0c\u5229\u7528LLM\u5c06\u5206\u6563\u7684\u79d1\u5b66\u6587\u732e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\uff0c\u652f\u6301AI\u9a71\u52a8\u7684\u79d1\u5b66\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u79d1\u5b66\u6587\u732e\u5206\u6563\u3001\u4fe1\u606f\u8fc7\u8f7d\u53ca\u53ef\u91cd\u590d\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u8ba1\u7b97\u5316\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eLLM\u5c06\u6587\u732e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5355\u5143\uff08Conceptual Tensor\uff09\uff0c\u652f\u6301\u52a8\u6001\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u6216\u8bed\u4e49\u5411\u91cf\u7a7a\u95f4\u3002", "result": "\u5b9e\u73b0\u79d1\u5b66\u9886\u57df\u7684\u538b\u7f29\u8868\u793a\uff0c\u652f\u6301AI\u4ee3\u7406\u76f4\u63a5\u64cd\u4f5c\u77e5\u8bc6\u56fe\u8c31\uff0c\u53d1\u73b0\u6f5c\u5728\u8054\u7cfb\u548c\u751f\u6210\u65b0\u5047\u8bbe\u3002", "conclusion": "Discovery Engine\u4e3aAI\u589e\u5f3a\u7684\u79d1\u5b66\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "relevance": 60.0}}
{"id": "2505.18088", "pdf": "https://arxiv.org/pdf/2505.18088", "abs": "https://arxiv.org/abs/2505.18088", "authors": ["Andrea Giuseppe Di Francesco", "Maria Sofia Bucarelli", "Franco Maria Nardini", "Raffaele Perego", "Nicola Tonellotto", "Fabrizio Silvestri"], "title": "Early-Exit Graph Neural Networks", "categories": ["cs.LG"], "comment": "37 pages, 14 figures", "summary": "Early-exit mechanisms allow deep neural networks to halt inference as soon as\nclassification confidence is high enough, adaptively trading depth for\nconfidence, and thereby cutting latency and energy on easy inputs while\nretaining full-depth accuracy for harder ones. Similarly, adding early exit\nmechanisms to Graph Neural Networks (GNNs), the go-to models for\ngraph-structured data, allows for dynamic trading depth for confidence on\nsimple graphs while maintaining full-depth accuracy on harder and more complex\ngraphs to capture intricate relationships. Although early exits have proven\neffective across various deep learning domains, their potential within GNNs in\nscenarios that require deep architectures while resisting over-smoothing and\nover-squashing remains largely unexplored. We unlock that potential by first\nintroducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose\nsymmetry-based inductive biases mitigate these issues and yield stable\nintermediate representations that can be useful to allow early exiting in GNNs.\nBuilding on this backbone, we present Early-Exit Graph Neural Networks\n(EEGNNs), which append confidence-aware exit heads that allow on-the-fly\ntermination of propagation based on each node or the entire graph. Experiments\nshow that EEGNNs preserve robust performance as depth grows and deliver\ncompetitive accuracy on heterophilic and long-range benchmarks, matching\nattention-based and asynchronous message-passing models while substantially\nreducing computation and latency. We plan to release the code to reproduce our\nexperiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GNN\u4e2d\u5f15\u5165\u65e9\u671f\u9000\u51fa\u673a\u5236\u7684\u65b9\u6cd5\uff08EEGNNs\uff09\uff0c\u901a\u8fc7\u5bf9\u79f0-\u53cd\u5bf9\u79f0GNN\uff08SAS-GNN\uff09\u89e3\u51b3\u6df1\u5ea6\u67b6\u6784\u4e2d\u7684\u8fc7\u5e73\u6ed1\u548c\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u548c\u5ef6\u8fdf\u3002", "motivation": "\u63a2\u7d22\u5728GNN\u4e2d\u5e94\u7528\u65e9\u671f\u9000\u51fa\u673a\u5236\u7684\u6f5c\u529b\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u67b6\u6784\u4e2d\u7684\u8fc7\u5e73\u6ed1\u548c\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "1. \u5f15\u5165SAS-GNN\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u5f52\u7eb3\u504f\u7f6e\u7a33\u5b9a\u4e2d\u95f4\u8868\u793a\uff1b2. \u5728SAS-GNN\u57fa\u7840\u4e0a\u6784\u5efaEEGNNs\uff0c\u6dfb\u52a0\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u9000\u51fa\u5934\uff0c\u52a8\u6001\u7ec8\u6b62\u4f20\u64ad\u3002", "result": "EEGNNs\u5728\u5f02\u8d28\u6027\u548c\u957f\u8ddd\u79bb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u548c\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u548c\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "EEGNNs\u901a\u8fc7\u65e9\u671f\u9000\u51fa\u673a\u5236\u5728GNN\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u6df1\u5ea6GNN\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 60.0}}
{"id": "2505.18011", "pdf": "https://arxiv.org/pdf/2505.18011", "abs": "https://arxiv.org/abs/2505.18011", "authors": ["Prince Kumar", "Rudra Murthy", "Riyaz Bhat", "Danish Contractor"], "title": "Training with Pseudo-Code for Instruction Following", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Despite the rapid progress in the capabilities of Large Language Models\n(LLMs), they continue to have difficulty following relatively simple,\nunambiguous instructions, especially when compositions are involved. In this\npaper, we take inspiration from recent work that suggests that models may\nfollow instructions better when they are expressed in pseudo-code. However,\nwriting pseudo-code programs can be tedious and using few-shot demonstrations\nto craft code representations for use in inference can be unnatural for\nnon-expert users of LLMs. To overcome these limitations, we propose fine-tuning\nLLMs with instruction-tuning data that additionally includes instructions\nre-expressed in pseudo-code along with the final response. We evaluate models\ntrained using our method on $11$ publicly available benchmarks comprising of\ntasks related to instruction-following, mathematics, and common-sense\nreasoning. We conduct rigorous experiments with $5$ different models and find\nthat not only do models follow instructions better when trained with\npseudo-code, they also retain their capabilities on the other tasks related to\nmathematical and common sense reasoning. Specifically, we observe a relative\ngain of $3$--$19$% on instruction-following benchmark, and an average gain of\nupto 14% across all tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5fae\u8c03LLMs\uff0c\u5c06\u6307\u4ee4\u91cd\u65b0\u8868\u8fbe\u4e3a\u4f2a\u4ee3\u7801\u4ee5\u63d0\u9ad8\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u80fd\u529b\u5feb\u901f\u63d0\u5347\uff0c\u4f46\u5728\u9075\u5faa\u7b80\u5355\u3001\u660e\u786e\u6307\u4ee4\uff08\u5c24\u5176\u662f\u590d\u5408\u6307\u4ee4\uff09\u65f6\u4ecd\u6709\u56f0\u96be\u3002\u4f2a\u4ee3\u7801\u53ef\u80fd\u6539\u5584\u6307\u4ee4\u9075\u5faa\uff0c\u4f46\u7f16\u5199\u4f2a\u4ee3\u7801\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u4e0d\u53cb\u597d\u3002", "method": "\u5fae\u8c03LLMs\uff0c\u5728\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u4e2d\u52a0\u5165\u4f2a\u4ee3\u7801\u8868\u8fbe\u7684\u6307\u4ee4\u53ca\u5176\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u572811\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f2a\u4ee3\u7801\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e0a\u76f8\u5bf9\u63d0\u53473-19%\uff0c\u6240\u6709\u4efb\u52a1\u5e73\u5747\u63d0\u534714%\u3002", "conclusion": "\u4f2a\u4ee3\u7801\u8f85\u52a9\u7684\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5176\u4ed6\u4efb\u52a1\u8868\u73b0\u3002", "relevance": 85.0}}
{"id": "2505.18091", "pdf": "https://arxiv.org/pdf/2505.18091", "abs": "https://arxiv.org/abs/2505.18091", "authors": ["Xinran Gu", "Kaifeng Lyu", "Jiazheng Li", "Jingzhao Zhang"], "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are typically trained on data mixtures: most\ndata come from web scrapes, while a small portion is curated from high-quality\nsources with dense domain-specific knowledge. In this paper, we show that when\ntraining LLMs on such data mixtures, knowledge acquisition from knowledge-dense\ndatasets, unlike training exclusively on knowledge-dense data\n(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit\nphase transitions with respect to the mixing ratio and model size. Through\ncontrolled experiments on a synthetic biography dataset mixed with web-scraped\ndata, we demonstrate that: (1) as we increase the model size to a critical\nvalue, the model suddenly transitions from memorizing very few to most of the\nbiographies; (2) below a critical mixing ratio, the model memorizes almost\nnothing even with extensive training, but beyond this threshold, it rapidly\nmemorizes more biographies. We attribute these phase transitions to a capacity\nallocation phenomenon: a model with bounded capacity must act like a knapsack\nproblem solver to minimize the overall test loss, and the optimal allocation\nacross datasets can change discontinuously as the model size or mixing ratio\nvaries. We formalize this intuition in an information-theoretic framework and\nreveal that these phase transitions are predictable, with the critical mixing\nratio following a power-law relationship with the model size. Our findings\nhighlight a concrete case where a good mixing recipe for large models may not\nbe optimal for small models, and vice versa.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6df7\u5408\u6570\u636e\u4e0a\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u77e5\u8bc6\u83b7\u53d6\u7684\u9636\u6bb5\u6027\u53d8\u5316\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u6df7\u5408\u6bd4\u4f8b\u5bf9\u77e5\u8bc6\u8bb0\u5fc6\u7684\u4e34\u754c\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5728\u6df7\u5408\u6570\u636e\uff08\u7f51\u7edc\u6293\u53d6\u6570\u636e\u4e0e\u9ad8\u8d28\u91cf\u77e5\u8bc6\u5bc6\u96c6\u6570\u636e\uff09\u4e0a\u8bad\u7ec3LLMs\u65f6\uff0c\u77e5\u8bc6\u83b7\u53d6\u7684\u975e\u7ebf\u6027\u884c\u4e3a\u53ca\u5176\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5408\u6210\u4f20\u8bb0\u6570\u636e\u96c6\u4e0e\u7f51\u7edc\u6570\u636e\u7684\u6df7\u5408\u5b9e\u9a8c\uff0c\u5206\u6790\u6a21\u578b\u5927\u5c0f\u548c\u6df7\u5408\u6bd4\u4f8b\u5bf9\u77e5\u8bc6\u8bb0\u5fc6\u7684\u5f71\u54cd\uff0c\u5e76\u7528\u4fe1\u606f\u8bba\u6846\u67b6\u89e3\u91ca\u73b0\u8c61\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5927\u5c0f\u548c\u6df7\u5408\u6bd4\u4f8b\u5b58\u5728\u4e34\u754c\u70b9\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8bb0\u5fc6\u7684\u7a81\u7136\u53d8\u5316\uff1b\u4e34\u754c\u6df7\u5408\u6bd4\u4f8b\u4e0e\u6a21\u578b\u5927\u5c0f\u5448\u5e42\u5f8b\u5173\u7cfb\u3002", "conclusion": "\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u6a21\u578b\u5927\u5c0f\u548c\u6df7\u5408\u6bd4\u4f8b\u7684\u4e34\u754c\u6548\u5e94\u9700\u88ab\u8003\u8651\uff0c\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\u7684\u6700\u4f18\u6df7\u5408\u7b56\u7565\u53ef\u80fd\u4e0d\u540c\u3002", "relevance": 85.0}}
{"id": "2505.18040", "pdf": "https://arxiv.org/pdf/2505.18040", "abs": "https://arxiv.org/abs/2505.18040", "authors": ["Minxue Niu", "Emily Mower Provost"], "title": "Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition", "categories": ["cs.CL"], "comment": null, "summary": "The ability to handle various emotion labels without dedicated training is\ncrucial for building adaptable Emotion Recognition (ER) systems. Conventional\nER models rely on training using fixed label sets and struggle to generalize\nbeyond them. On the other hand, Large Language Models (LLMs) have shown strong\nzero-shot ER performance across diverse label spaces, but their scale limits\ntheir use on edge devices. In this work, we propose a contrastive distillation\nframework that transfers rich emotional knowledge from LLMs into a compact\nmodel without the use of human annotations. We use GPT-4 to generate\ndescriptive emotion annotations, offering rich supervision beyond fixed label\nsets. By aligning text samples with emotion descriptors in a shared embedding\nspace, our method enables zero-shot prediction on different emotion classes,\ngranularity, and label schema. The distilled model is effective across multiple\ndatasets and label spaces, outperforming strong baselines of similar size and\napproaching GPT-4's zero-shot performance, while being over 10,000 times\nsmaller.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u7684\u4e30\u5bcc\u60c5\u611f\u77e5\u8bc6\u8fc1\u79fb\u5230\u7d27\u51d1\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u60c5\u611f\u8bc6\u522b\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u4f9d\u8d56\u56fa\u5b9a\u6807\u7b7e\u96c6\u8bad\u7ec3\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u56e0\u89c4\u6a21\u8fc7\u5927\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u4f7f\u7528GPT-4\u751f\u6210\u60c5\u611f\u63cf\u8ff0\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6587\u672c\u6837\u672c\u4e0e\u60c5\u611f\u63cf\u8ff0\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9884\u6d4b\u3002", "result": "\u84b8\u998f\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6807\u7b7e\u7a7a\u95f4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u7f29\u5c0f\u4e8610,000\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 70.0}}
{"id": "2505.17912", "pdf": "https://arxiv.org/pdf/2505.17912", "abs": "https://arxiv.org/abs/2505.17912", "authors": ["Luohong Wu", "Matthias Seibold", "Nicola A. Cavalcanti", "Giuseppe Loggia", "Lisa Reissner", "Bastian Sigrist", "Jonas Hein", "Lilian Calvet", "Arnd Vieh\u00f6fer", "Philipp F\u00fcrnstahl"], "title": "UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Bone surface reconstruction plays a critical role in\ncomputer-assisted orthopedic surgery. Compared to traditional imaging\nmodalities such as CT and MRI, ultrasound offers a radiation-free,\ncost-effective, and portable alternative. Continuous bone surface\nreconstruction can be employed for many clinical applications. However, due to\nthe inherent limitations of ultrasound imaging, B-mode ultrasound typically\ncapture only partial bone surfaces. Existing reconstruction methods struggle\nwith such incomplete data, leading to artifacts and increased reconstruction\nerrors. Effective techniques for accurately reconstructing thin and open bone\nsurfaces from real-world 3D ultrasound volumes remain lacking. Methods: We\npropose UltraBoneUDF, a self-supervised framework designed for reconstructing\nopen bone surfaces from ultrasound using neural Unsigned Distance Functions. To\nenhance reconstruction quality, we introduce a novel global feature extractor\nthat effectively fuses ultrasound-specific image characteristics. Additionally,\nwe present a novel loss function based on local tangent plane optimization that\nsubstantially improves surface reconstruction quality. UltraBoneUDF and\nbaseline models are extensively evaluated on four open-source datasets.\nResults: Qualitative results highlight the limitations of the state-of-the-art\nmethods for open bone surface reconstruction and demonstrate the effectiveness\nof UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms\ncompeting methods across all evaluated datasets for both open and closed bone\nsurface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the\nUltraBones100k dataset (39.6\\% improvement compared to the SOTA), 0.23 mm on\nthe OpenBoneCT dataset (69.3\\% improvement), 0.18 mm on the ClosedBoneCT\ndataset (70.2\\% improvement), and 0.05 mm on the Prostate dataset (55.3\\%\nimprovement).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6UltraBoneUDF\uff0c\u7528\u4e8e\u4ece\u8d85\u58f0\u6570\u636e\u4e2d\u91cd\u5efa\u5f00\u653e\u7684\u9aa8\u8868\u9762\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u65e0\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8d85\u58f0\u6210\u50cf\u53ea\u80fd\u6355\u6349\u90e8\u5206\u9aa8\u8868\u9762\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e0d\u5b8c\u6574\u6570\u636e\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u51c6\u786e\u91cd\u5efa\u5f00\u653e\u9aa8\u8868\u9762\u7684\u6280\u672f\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u6846\u67b6UltraBoneUDF\uff0c\u7ed3\u5408\u5168\u5c40\u7279\u5f81\u63d0\u53d6\u5668\u548c\u57fa\u4e8e\u5c40\u90e8\u5207\u5e73\u9762\u4f18\u5316\u7684\u65b0\u635f\u5931\u51fd\u6570\u3002", "result": "UltraBoneUDF\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747Chamfer\u8ddd\u79bb\u8bef\u5dee\u5927\u5e45\u964d\u4f4e\uff08\u6700\u9ad8\u63d0\u534770.2%\uff09\u3002", "conclusion": "UltraBoneUDF\u4e3a\u5f00\u653e\u9aa8\u8868\u9762\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "relevance": 10.0}}
{"id": "2505.18097", "pdf": "https://arxiv.org/pdf/2505.18097", "abs": "https://arxiv.org/abs/2505.18097", "authors": ["Chun Tong Lei", "Zhongliang Guo", "Hon Chung Lee", "Minh Quoc Duong", "Chun Pong Lau"], "title": "Towards more transferable adversarial attack in black-box manner", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u66ff\u4ee3\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u3002", "motivation": "\u4f20\u7edf\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u4f18\u5316\u6846\u67b6\u800c\u975e\u66ff\u4ee3\u6a21\u578b\u67b6\u6784\uff0c\u800c\u6269\u6563\u6a21\u578b\u867d\u63d0\u5347\u8fc1\u79fb\u6027\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u4f5c\u8005\u5047\u8bbe\u7c7b\u4f3c\u6269\u6563\u6a21\u578b\u5f52\u7eb3\u504f\u7f6e\u7684\u6a21\u578b\u7ed3\u5408\u9002\u5f53\u635f\u5931\u51fd\u6570\u53ef\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u548c\u66ff\u4ee3\u6a21\u578b\uff0c\u5229\u7528\u5206\u7c7b\u5668\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u4f9d\u8d56\u5206\u7c7b\u5668\u5206\u6570\uff0c\u5c06\u81ea\u7136\u6570\u636e\u5206\u5e03\u77e5\u8bc6\u878d\u5165\u5bf9\u6297\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u578b\u67b6\u6784\u7684\u8fc1\u79fb\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6269\u6563\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5047\u8bbe\uff0c\u8868\u660e\u65e0\u9700\u4f9d\u8d56\u6269\u6563\u6a21\u578b\u5373\u53ef\u9ad8\u6548\u5b9e\u73b0\u9ad8\u8fc1\u79fb\u6027\u3002", "relevance": 75.0}}
{"id": "2505.18056", "pdf": "https://arxiv.org/pdf/2505.18056", "abs": "https://arxiv.org/abs/2505.18056", "authors": ["Wei-Ling Hsu", "Yu-Chien Tang", "An-Zi Yen"], "title": "MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Online learning enhances educational accessibility, offering students the\nflexibility to learn anytime, anywhere. However, a key limitation is the lack\nof immediate, personalized feedback, particularly in helping students correct\nerrors in math problem-solving. Several studies have investigated the\napplications of large language models (LLMs) in educational contexts. In this\npaper, we explore the capabilities of LLMs to assess students' math\nproblem-solving processes and provide adaptive feedback. The MathEDU dataset is\nintroduced, comprising authentic student solutions annotated with teacher\nfeedback. We evaluate the model's ability to support personalized learning in\ntwo scenarios: one where the model has access to students' prior answer\nhistories, and another simulating a cold-start context. Experimental results\nshow that the fine-tuned model performs well in identifying correctness.\nHowever, the model still faces challenges in generating detailed feedback for\npedagogical purposes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u5b66\u751f\u6570\u5b66\u89e3\u9898\u8fc7\u7a0b\u5e76\u63d0\u4f9b\u81ea\u9002\u5e94\u53cd\u9988\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u8bc6\u522b\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u751f\u6210\u8be6\u7ec6\u6559\u5b66\u53cd\u9988\u65b9\u9762\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u5728\u7ebf\u6559\u80b2\u7f3a\u4e4f\u5373\u65f6\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b66\u89e3\u9898\u4e2d\uff0cLLMs\u7684\u5e94\u7528\u53ef\u4ee5\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u5f15\u5165MathEDU\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff1a\u6709\u5b66\u751f\u5386\u53f2\u7b54\u6848\u548c\u65e0\u5386\u53f2\u7b54\u6848\uff08\u51b7\u542f\u52a8\uff09\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u8bc6\u522b\u7b54\u6848\u6b63\u786e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u751f\u6210\u8be6\u7ec6\u6559\u5b66\u53cd\u9988\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "LLMs\u5728\u6570\u5b66\u6559\u80b2\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u53cd\u9988\u3002", "relevance": 50.0}}
{"id": "2505.17915", "pdf": "https://arxiv.org/pdf/2505.17915", "abs": "https://arxiv.org/abs/2505.17915", "authors": ["Lynn Karam", "Yipei Wang", "Veeru Kasivisvanathan", "Mirabela Rusu", "Yipeng Hu", "Shaheer U. Saeed"], "title": "Promptable cancer segmentation using minimal expert-curated data", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Automated segmentation of cancer on medical images can aid targeted\ndiagnostic and therapeutic procedures. However, its adoption is limited by the\nhigh cost of expert annotations required for training and inter-observer\nvariability in datasets. While weakly-supervised methods mitigate some\nchallenges, using binary histology labels for training as opposed to requiring\nfull segmentation, they require large paired datasets of histology and images,\nwhich are difficult to curate. Similarly, promptable segmentation aims to allow\nsegmentation with no re-training for new tasks at inference, however, existing\nmodels perform poorly on pathological regions, again necessitating large\ndatasets for training. In this work we propose a novel approach for promptable\nsegmentation requiring only 24 fully-segmented images, supplemented by 8\nweakly-labelled images, for training. Curating this minimal data to a high\nstandard is relatively feasible and thus issues with the cost and variability\nof obtaining labels can be mitigated. By leveraging two classifiers, one\nweakly-supervised and one fully-supervised, our method refines segmentation\nthrough a guided search process initiated by a single-point prompt. Our\napproach outperforms existing promptable segmentation methods, and performs\ncomparably with fully-supervised methods, for the task of prostate cancer\nsegmentation, while using substantially less annotated data (up to 100X less).\nThis enables promptable segmentation with very minimal labelled data, such that\nthe labels can be curated to a very high standard.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u63d0\u793a\u5206\u5272\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u901a\u5e38\u4f9d\u8d56\u5927\u91cf\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u6807\u6ce8\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u5f31\u76d1\u7763\u65b9\u6cd5\u867d\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u4f46\u4ecd\u9700\u5927\u91cf\u914d\u5bf9\u6570\u636e\u3002\u73b0\u6709\u53ef\u63d0\u793a\u5206\u5272\u65b9\u6cd5\u5728\u75c5\u7406\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u4ecd\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u7ed3\u5408\u5f31\u76d1\u7763\u548c\u5168\u76d1\u7763\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5355\u70b9\u63d0\u793a\u5f15\u5bfc\u641c\u7d22\u8fc7\u7a0b\uff0c\u4ec5\u970024\u5f20\u5168\u6807\u6ce8\u56fe\u50cf\u548c8\u5f20\u5f31\u6807\u6ce8\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4ec5\u4f7f\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff08\u6700\u591a\u51cf\u5c11100\u500d\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u53ef\u63d0\u793a\u5206\u5272\u65b9\u6cd5\uff0c\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6570\u636e\u9700\u6c42\uff0c\u4f7f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6210\u4e3a\u53ef\u80fd\uff0c\u63a8\u52a8\u4e86\u53ef\u63d0\u793a\u5206\u5272\u7684\u5b9e\u9645\u5e94\u7528\u3002", "relevance": 40.0}}
{"id": "2505.17511", "pdf": "https://arxiv.org/pdf/2505.17511", "abs": "https://arxiv.org/abs/2505.17511", "authors": ["Aditya Gautam"], "title": "Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification", "categories": ["cs.MA", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "The rapid proliferation of misinformation in digital media demands solutions\nthat go beyond isolated Large Language Model(LLM) or AI Agent based detection\nmethods. This paper introduces a novel multi-agent framework that covers the\ncomplete misinformation lifecycle: classification, detection, correction, and\nsource verification to deliver more transparent and reliable outcomes. In\ncontrast to single-agent or monolithic architectures, our approach employs five\nspecialized agents: an Indexer agent for dynamically maintaining trusted\nrepositories, a Classifier agent for labeling misinformation types, an\nExtractor agent for evidence based retrieval and ranking, a Corrector agent for\ngenerating fact-based correction and a Verification agent for validating\noutputs and tracking source credibility. Each agent can be individually\nevaluated and optimized, ensuring scalability and adaptability as new types of\nmisinformation and data sources emerge. By decomposing the misinformation\nlifecycle into specialized agents - our framework enhances scalability,\nmodularity, and explainability. This paper proposes a high-level system\noverview, agent design with emphasis on transparency, evidence-based outputs,\nand source provenance to support robust misinformation detection and correction\nat scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u865a\u5047\u4fe1\u606f\u7684\u5168\u751f\u547d\u5468\u671f\uff08\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u4fee\u6b63\u548c\u6765\u6e90\u9a8c\u8bc1\uff09\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u63d0\u5347\u900f\u660e\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6570\u5b57\u5a92\u4f53\u4e2d\u865a\u5047\u4fe1\u606f\u7684\u5feb\u901f\u4f20\u64ad\u9700\u8981\u8d85\u8d8a\u5355\u4e00LLM\u6216AI\u4ee3\u7406\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u4f9b\u66f4\u900f\u660e\u548c\u53ef\u9760\u7684\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u4e94\u4e2a\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\uff08\u7d22\u5f15\u5668\u3001\u5206\u7c7b\u5668\u3001\u63d0\u53d6\u5668\u3001\u4fee\u6b63\u5668\u548c\u9a8c\u8bc1\u5668\uff09\u5206\u522b\u5904\u7406\u865a\u5047\u4fe1\u606f\u7684\u4e0d\u540c\u73af\u8282\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u865a\u5047\u4fe1\u606f\u751f\u547d\u5468\u671f\u4e3a\u4e13\u95e8\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u865a\u5047\u4fe1\u606f\u7684\u68c0\u6d4b\u548c\u4fee\u6b63\uff0c\u540c\u65f6\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u8bc1\u636e\u57fa\u7840\u3002", "relevance": 70.0}}
{"id": "2505.18101", "pdf": "https://arxiv.org/pdf/2505.18101", "abs": "https://arxiv.org/abs/2505.18101", "authors": ["Congren Dai", "Huichi Zhou", "Jiahao Huang", "Zhenxuan Zhang", "Fanwen Wang", "Guang Yang", "Fei Ye"], "title": "Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning", "categories": ["cs.LG"], "comment": null, "summary": "Online Continual Learning (OCL) presents a complex learning environment in\nwhich new data arrives in a batch-to-batch online format, and the risk of\ncatastrophic forgetting can significantly impair model efficacy. In this study,\nwe address OCL by introducing an innovative memory framework that incorporates\na short-term memory system to retain dynamic information and a long-term memory\nsystem to archive enduring knowledge. Specifically, the long-term memory system\ncomprises a collection of sub-memory buffers, each linked to a cluster\nprototype and designed to retain data samples from distinct categories. We\npropose a novel $K$-means-based sample selection method to identify cluster\nprototypes for each encountered category. To safeguard essential and critical\nsamples, we introduce a novel memory optimisation strategy that selectively\nretains samples in the appropriate sub-memory buffer by evaluating each cluster\nprototype against incoming samples through an optimal transportation mechanism.\nThis approach specifically promotes each sub-memory buffer to retain data\nsamples that exhibit significant discrepancies from the corresponding cluster\nprototype, thereby ensuring the preservation of semantically rich information.\nIn addition, we propose a novel Divide-and-Conquer (DAC) approach that\nformulates the memory updating as an optimisation problem and divides it into\nseveral subproblems. As a result, the proposed DAC approach can solve these\nsubproblems separately and thus can significantly reduce computations of the\nproposed memory updating process. We conduct a series of experiments across\nstandard and imbalanced learning settings, and the empirical findings indicate\nthat the proposed memory framework achieves state-of-the-art performance in\nboth learning contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\uff08OCL\uff09\uff0c\u901a\u8fc7\u77ed\u65f6\u548c\u957f\u65f6\u8bb0\u5fc6\u7cfb\u7edf\u7ed3\u5408K-means\u6837\u672c\u9009\u62e9\u4e0e\u4f18\u5316\u8fd0\u8f93\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u548c\u6301\u4e45\u8bb0\u5fc6\u7cfb\u7edf\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002", "method": "\u7ed3\u5408\u77ed\u65f6\u548c\u957f\u65f6\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4f7f\u7528K-means\u9009\u62e9\u6837\u672c\uff0c\u4f18\u5316\u8fd0\u8f93\u673a\u5236\u4fdd\u7559\u5173\u952e\u6837\u672c\uff0c\u5e76\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u4f18\u5316\u5185\u5b58\u66f4\u65b0\u3002", "result": "\u5728\u6807\u51c6\u548c\u4e0d\u5e73\u8861\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bb0\u5fc6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86OCL\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002", "relevance": 40.0}}
{"id": "2505.18071", "pdf": "https://arxiv.org/pdf/2505.18071", "abs": "https://arxiv.org/abs/2505.18071", "authors": ["Jia-Nan Li", "Jian Guan", "Wei Wu", "Rui Yan"], "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning\\textemdash the ability to\nderive general rules from incomplete evidence, remains underexplored. This\npaper investigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose \\textsc{AlignXplore}, a model that leverages extended reasoning chains\nto enable systematic preference inference from behavioral signals in users'\ninteraction histories. We develop \\textsc{AlignXplore} by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that \\textsc{AlignXplore}\nachieves substantial improvements over the backbone model by an average of\n11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong\ngeneralization ability across different input formats and downstream models.\nFurther analyses establish best practices for preference inference learning\nthrough systematic comparison of reward modeling strategies, while revealing\nthe emergence of human-like inductive reasoning patterns during training.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u4e2a\u6027\u5316\u504f\u597d\u63a8\u7406\u4e2d\u7684\u6269\u5c55\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86AlignXplore\u6a21\u578b\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u51b7\u542f\u52a8\u8bad\u7ec3\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5f52\u7eb3\u63a8\u7406\uff08\u5982\u4e2a\u6027\u5316\u504f\u597d\u63a8\u7406\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u5f53\u524dLLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u6355\u6349\u591a\u6837\u5316\u7528\u6237\u504f\u597d\u65f6\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAlignXplore\u6a21\u578b\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u51b7\u542f\u52a8\u8bad\u7ec3\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6269\u5c55\u63a8\u7406\u94fe\u4ece\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u4e2d\u63a8\u65ad\u504f\u597d\u3002", "result": "AlignXplore\u5728\u9886\u57df\u5185\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534711.05%\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u8f93\u5165\u683c\u5f0f\u548c\u4e0b\u6e38\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u504f\u597d\u63a8\u7406\u5b66\u4e60\u63d0\u4f9b\u4e86\u6700\u4f73\u5b9e\u8df5\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5f52\u7eb3\u63a8\u7406\u6a21\u5f0f\u7684\u51fa\u73b0\u3002", "relevance": 85.0}}
{"id": "2505.17966", "pdf": "https://arxiv.org/pdf/2505.17966", "abs": "https://arxiv.org/abs/2505.17966", "authors": ["Frederik Nolte", "Bernhard Sch\u00f6lkopf", "Ingmar Posner"], "title": "Is Single-View Mesh Reconstruction Ready for Robotics?", "categories": ["cs.RO", "cs.CV", "I.4.5; I.4.8; I.2.9; I.2.10"], "comment": "20 pages, 17 figures", "summary": "This paper evaluates single-view mesh reconstruction models for creating\ndigital twin environments in robot manipulation. Recent advances in computer\nvision for 3D reconstruction from single viewpoints present a potential\nbreakthrough for efficiently creating virtual replicas of physical environments\nfor robotics contexts. However, their suitability for physics simulations and\nrobotics applications remains unexplored. We establish benchmarking criteria\nfor 3D reconstruction in robotics contexts, including handling typical inputs,\nproducing collision-free and stable reconstructions, managing occlusions, and\nmeeting computational constraints. Our empirical evaluation using realistic\nrobotics datasets shows that despite success on computer vision benchmarks,\nexisting approaches fail to meet robotics-specific requirements. We\nquantitively examine limitations of single-view reconstruction for practical\nrobotics implementation, in contrast to prior work that focuses on multi-view\napproaches. Our findings highlight critical gaps between computer vision\nadvances and robotics needs, guiding future research at this intersection.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5355\u89c6\u89d2\u7f51\u683c\u91cd\u5efa\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u521b\u5efa\u6570\u5b57\u5b6a\u751f\u73af\u5883\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u867d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\u4e0a\u6210\u529f\uff0c\u4f46\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u7279\u5b9a\u9700\u6c42\u3002", "motivation": "\u7814\u7a76\u5355\u89c6\u89d23D\u91cd\u5efa\u6a21\u578b\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u586b\u8865\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u673a\u5668\u4eba\u9700\u6c42\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u5efa\u7acb\u673a\u5668\u4eba\u573a\u666f\u4e0b\u76843D\u91cd\u5efa\u57fa\u51c6\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7279\u5b9a\u9700\u6c42\uff08\u5982\u78b0\u649e\u81ea\u7531\u3001\u7a33\u5b9a\u6027\u3001\u906e\u6321\u5904\u7406\u7b49\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u8fdb\u5c55\u4e0e\u673a\u5668\u4eba\u9700\u6c42\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6307\u5bfc\u3002", "relevance": 30.0}}
{"id": "2505.18102", "pdf": "https://arxiv.org/pdf/2505.18102", "abs": "https://arxiv.org/abs/2505.18102", "authors": ["Takashi Ishida", "Thanawat Lodkaew", "Ikko Yamane"], "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "comment": null, "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u6b62LLM\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u6c61\u67d3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5316\u6b63\u786e\u7b54\u6848\u5e76\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u8d85\u8fc7\u8d1d\u53f6\u65af\u51c6\u786e\u7387\u6765\u8bc6\u522b\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u89e3\u51b3\u516c\u5f00LLM\u57fa\u51c6\u6d4b\u8bd5\u65f6\u53ef\u80fd\u5bfc\u81f4\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u907f\u514d\u6d4b\u8bd5\u96c6\u8fc7\u62df\u5408\u548c\u5355\u70b9\u4fe1\u4efb\u98ce\u9669\u3002", "method": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6ce8\u5165\u968f\u673a\u6027\uff0c\u51c6\u5907\u591a\u4e2a\u903b\u8f91\u6b63\u786e\u7684\u7b54\u6848\uff0c\u4ec5\u9009\u62e9\u4e00\u4e2a\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u8d1d\u53f6\u65af\u51c6\u786e\u7387\u4e0a\u9650\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u68c0\u6d4b\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u9632\u6b62\u6570\u636e\u6c61\u67d3\uff0c\u540c\u65f6\u4fdd\u6301\u516c\u5f00\u8bc4\u4f30LLM\u7684\u80fd\u529b\u3002", "relevance": 85.0}}
{"id": "2505.18092", "pdf": "https://arxiv.org/pdf/2505.18092", "abs": "https://arxiv.org/abs/2505.18092", "authors": ["Weizhou Shen", "Chenliang Li", "Fanqi Wan", "Shengyi Liao", "Shaopeng Lai", "Bo Zhang", "Yingcheng Shi", "Yuning Wu", "Gang Fu", "Zhansheng Li", "Bin Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "QwenLong-CPRS: Towards $\\infty$-LLMs with Dynamic Context Optimization", "categories": ["cs.CL"], "comment": null, "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59$\\times$ context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.", "AI": {"tldr": "QwenLong-CPRS\u662f\u4e00\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u4f18\u5316\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u673a\u5236\u63d0\u5347LLM\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5f15\u5165\u52a8\u6001\u4e0a\u4e0b\u6587\u4f18\u5316\u673a\u5236\uff0c\u5305\u62ec\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u7684\u538b\u7f29\u3001\u53cc\u5411\u63a8\u7406\u5c42\u3001\u4ee4\u724c\u8bc4\u5224\u673a\u5236\u548c\u7a97\u53e3\u5e76\u884c\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b021.59\u500d\u4e0a\u4e0b\u6587\u538b\u7f29\u548c19.15\u70b9\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "QwenLong-CPRS\u4e3a\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u67b6\u6784\u3002", "relevance": 85.0}}
{"id": "2505.17971", "pdf": "https://arxiv.org/pdf/2505.17971", "abs": "https://arxiv.org/abs/2505.17971", "authors": ["Danial Khan", "Zohaib Salahuddin", "Yumeng Zhang", "Sheng Kuang", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Rachel Cavill", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Adrian Galiana-Bordera", "Paula Jimenez Gomez", "Luis Marti-Bonmati", "Philippe Lambin"], "title": "Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present a fully automated, anatomically guided deep learning pipeline for\nprostate cancer (PCa) risk stratification using routine MRI. The pipeline\nintegrates three key components: an nnU-Net module for segmenting the prostate\ngland and its zones on axial T2-weighted MRI; a classification module based on\nthe UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with\noptional anatomical priors and clinical data; and a VAE-GAN framework for\ngenerating counterfactual heatmaps that localize decision-driving image\nregions. The system was developed using 1,500 PI-CAI cases for segmentation and\n617 biparametric MRIs with metadata from the CHAIMELEON challenge for\nclassification (split into 70% training, 10% validation, and 20% testing).\nSegmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),\nand 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69\nto 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,\ncomposite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.\nCounterfactual heatmaps reliably highlighted lesions within segmented regions,\nenhancing model interpretability. In a prospective multi-center in-silico trial\nwith 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to\n0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case\nby 40%. These results demonstrate that anatomy-aware foundation models with\ncounterfactual explainability can enable accurate, interpretable, and efficient\nPCa risk assessment, supporting their potential use as virtual biopsies in\nclinical practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u524d\u5217\u817a\u764c\u98ce\u9669\u5206\u5c42\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86nnU-Net\u5206\u5272\u3001Swin Transformer\u5206\u7c7b\u548cVAE-GAN\u751f\u6210\u53cd\u4e8b\u5b9e\u70ed\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u5347\u524d\u5217\u817a\u764cMRI\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002", "method": "1. nnU-Net\u5206\u5272\u524d\u5217\u817a\u53ca\u5176\u533a\u57df\uff1b2. Swin Transformer\u5206\u7c7b\u6a21\u578b\uff1b3. VAE-GAN\u751f\u6210\u53cd\u4e8b\u5b9e\u70ed\u56fe\u3002", "result": "\u5206\u5272Dice\u5206\u65700.92-0.95\uff0c\u5206\u7c7bAUC\u63d0\u5347\u81f30.79\uff0c\u4e34\u5e8a\u6d4b\u8bd5\u4e2d\u8bca\u65ad\u51c6\u786e\u6027\u4ece0.72\u63d0\u5347\u81f30.77\u3002", "conclusion": "\u89e3\u5256\u5b66\u611f\u77e5\u7684\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6027\uff0c\u6709\u671b\u6210\u4e3a\u4e34\u5e8a\u865a\u62df\u6d3b\u68c0\u5de5\u5177\u3002", "relevance": 30.0}}
{"id": "2505.18113", "pdf": "https://arxiv.org/pdf/2505.18113", "abs": "https://arxiv.org/abs/2505.18113", "authors": ["Halyun Jeong", "Jack Xin", "Penghang Yin"], "title": "Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Training quantized neural networks requires addressing the non-differentiable\nand discrete nature of the underlying optimization problem. To tackle this\nchallenge, the straight-through estimator (STE) has become the most widely\nadopted heuristic, allowing backpropagation through discrete operations by\nintroducing surrogate gradients. However, its theoretical properties remain\nlargely unexplored, with few existing works simplifying the analysis by\nassuming an infinite amount of training data. In contrast, this work presents\nthe first finite-sample analysis of STE in the context of neural network\nquantization. Our theoretical results highlight the critical role of sample\nsize in the success of STE, a key insight absent from existing studies.\nSpecifically, by analyzing the quantization-aware training of a two-layer\nneural network with binary weights and activations, we derive the sample\ncomplexity bound in terms of the data dimensionality that guarantees the\nconvergence of STE-based optimization to the global minimum. Moreover, in the\npresence of label noises, we uncover an intriguing recurrence property of\nSTE-gradient method, where the iterate repeatedly escape from and return to the\noptimal binary weights. Our analysis leverages tools from compressed sensing\nand dynamical systems theory.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9STE\u5728\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u4e2d\u7684\u6709\u9650\u6837\u672c\u6027\u8d28\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6837\u672c\u91cf\u5bf9STE\u6210\u529f\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63a8\u5bfc\u4e86\u6536\u655b\u5230\u5168\u5c40\u6700\u5c0f\u503c\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "motivation": "\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u975e\u53ef\u5fae\u6027\u548c\u79bb\u6563\u6027\u662f\u4e00\u4e2a\u6311\u6218\uff0cSTE\u4f5c\u4e3a\u5e7f\u6cdb\u4f7f\u7528\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5176\u7406\u8bba\u6027\u8d28\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5177\u6709\u4e8c\u503c\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u5229\u7528\u538b\u7f29\u611f\u77e5\u548c\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\u5de5\u5177\uff0c\u63a8\u5bfc\u4e86STE\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6837\u672c\u91cf\u662fSTE\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5728\u6807\u7b7e\u566a\u58f0\u5b58\u5728\u65f6\u53d1\u73b0\u4e86STE\u68af\u5ea6\u65b9\u6cd5\u7684\u5faa\u73af\u7279\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aSTE\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u9996\u4e2a\u7ed3\u679c\uff0c\u5f3a\u8c03\u4e86\u6837\u672c\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86STE\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u72ec\u7279\u884c\u4e3a\u3002", "relevance": 50.0}}
{"id": "2505.18098", "pdf": "https://arxiv.org/pdf/2505.18098", "abs": "https://arxiv.org/abs/2505.18098", "authors": ["Joey Hong", "Anca Dragan", "Sergey Levine"], "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u6761\u4ef6\u4ef7\u503c\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6307\u5bfcLLM\u5728\u591a\u8f6e\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86RL\u5fae\u8c03\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\uff08\u5982\u8c08\u5224\u548c\u8bf4\u670d\uff09\u9700\u8981\u957f\u65f6\u63a8\u7406\u548c\u89c4\u5212\uff0c\u4f46RL\u5fae\u8c03\u5b58\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u5927\u578bLLM\u4e0d\u652f\u6301\u6b64\u7c7b\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u4ef7\u503c\u51fd\u6570\u9884\u6d4b\u4efb\u52a1\u7ed3\u679c\uff0c\u6307\u5bfcLLM\u4ee3\u7406\u8bc4\u4f30\u591a\u6b65\u63a8\u7406\uff0c\u800c\u975e\u5b8c\u6574\u52a8\u4f5c\uff0c\u4ee5\u8f7b\u91cf\u5316\u6a21\u5757\u652f\u6301\u51b3\u7b56\u3002", "result": "\u5728\u5de5\u5177\u4f7f\u7528\u3001\u793e\u4ea4\u63a8\u7406\u548c\u5bf9\u8bdd\u7b49\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8eRL\u5fae\u8c03\u548c\u63d0\u793a\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u5728\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2505.18116", "pdf": "https://arxiv.org/pdf/2505.18116", "abs": "https://arxiv.org/abs/2505.18116", "authors": ["Huayu Chen", "Kaiwen Zheng", "Qinsheng Zhang", "Ganqu Cui", "Yin Cui", "Haotian Ye", "Tsung-Yi Lin", "Ming-Yu Liu", "Jun Zhu", "Haoxiang Wang"], "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNegative-aware Fine-Tuning (NFT)\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u751f\u6210\u7684\u8d1f\u9762\u53cd\u9988\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u65e0\u9700\u5916\u90e8\u6559\u5e08\u3002\u5b9e\u9a8c\u8868\u660e\uff0cNFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u751a\u81f3\u4e0e\u9886\u5148\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u6311\u6218\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728LLM\u81ea\u6211\u6539\u8fdb\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u63a2\u7d22\u76d1\u7763\u5b66\u4e60\uff08SL\uff09\u5728\u65e0\u5916\u90e8\u6559\u5e08\u60c5\u51b5\u4e0b\u901a\u8fc7\u8d1f\u9762\u53cd\u9988\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faNFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21LLM\u751f\u6210\u7684\u8d1f\u9762\u7b54\u6848\u6784\u5efa\u9690\u5f0f\u8d1f\u9762\u7b56\u7565\uff0c\u76f4\u63a5\u4f18\u5316\u6240\u6709LLM\u751f\u6210\u7ed3\u679c\u3002", "result": "\u57287B\u548c32B\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cNFT\u663e\u8457\u4f18\u4e8e\u4f20\u7edfSL\u65b9\u6cd5\uff0c\u4e0eGRPO\u548cDAPO\u7b49RL\u7b97\u6cd5\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "conclusion": "NFT\u548cGRPO\u5728\u4e25\u683c\u7b56\u7565\u8bad\u7ec3\u4e0b\u7b49\u4ef7\uff0c\u586b\u8865\u4e86SL\u548cRL\u5728\u4e8c\u5143\u53cd\u9988\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u7406\u8bba\u5dee\u8ddd\u3002", "relevance": 85.0}}
{"id": "2505.18105", "pdf": "https://arxiv.org/pdf/2505.18105", "abs": "https://arxiv.org/abs/2505.18105", "authors": ["Lisheng Huang", "Yichen Liu", "Jinhao Jiang", "Rongxiang Zhang", "Jiahao Yan", "Junyi Li", "Wayne Xin Zhao"], "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework", "categories": ["cs.CL"], "comment": "LLM, Complex Search Benchmark", "summary": "Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch", "AI": {"tldr": "ManuSearch\u662f\u4e00\u4e2a\u900f\u660e\u3001\u6a21\u5757\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u4e3aLLMs\u63d0\u4f9b\u6df1\u5ea6\u641c\u7d22\u80fd\u529b\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u5206\u89e3\u641c\u7d22\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5728ORION\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7f51\u7edc\u7684LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u591a\u4e3a\u5c01\u95ed\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\u3002ManuSearch\u65e8\u5728\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u63a8\u52a8\u53ef\u590d\u73b0\u7684\u6df1\u5ea6\u641c\u7d22\u7814\u7a76\u3002", "method": "ManuSearch\u7531\u4e09\u4e2a\u667a\u80fd\u4f53\u7ec4\u6210\uff1a\u89e3\u51b3\u65b9\u6848\u89c4\u5212\u667a\u80fd\u4f53\u3001\u4e92\u8054\u7f51\u641c\u7d22\u667a\u80fd\u4f53\u548c\u7ed3\u6784\u5316\u7f51\u9875\u9605\u8bfb\u667a\u80fd\u4f53\u3002ORION\u57fa\u51c6\u6d4b\u8bd5\u7528\u4e8e\u8bc4\u4f30\u5f00\u653e\u7f51\u7edc\u63a8\u7406\u80fd\u529b\u3002", "result": "ManuSearch\u5728ORION\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\uff0c\u751a\u81f3\u8d85\u8fc7\u95ed\u6e90\u7cfb\u7edf\u3002", "conclusion": "ManuSearch\u4e3a\u5f00\u653e\u6df1\u5ea6\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7814\u7a76\u8def\u5f84\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u548c\u4ee3\u7801\u3002", "relevance": 85.0}}
{"id": "2505.18125", "pdf": "https://arxiv.org/pdf/2505.18125", "abs": "https://arxiv.org/abs/2505.18125", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "AI": {"tldr": "TabSTAR\u662f\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u8bed\u4e49\u76ee\u6807\u611f\u77e5\u8868\u793a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u548c\u76ee\u6807\u4ee4\u724c\u8f93\u5165\uff0c\u5b9e\u73b0\u4e86\u5728\u5e26\u6709\u6587\u672c\u7279\u5f81\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u8868\u683c\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cTabSTAR\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u63d0\u5347\u8868\u683c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "TabSTAR\u91c7\u7528\u65e0\u6570\u636e\u96c6\u7279\u5b9a\u53c2\u6570\u7684\u67b6\u6784\uff0c\u89e3\u51bb\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u5e76\u8f93\u5165\u76ee\u6807\u4ee4\u724c\uff0c\u4ee5\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u5d4c\u5165\u8868\u793a\u3002", "result": "TabSTAR\u5728\u5e26\u6709\u6587\u672c\u7279\u5f81\u7684\u5206\u7c7b\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u6570\u636e\u96c6\u6570\u91cf\u6269\u5c55\u89c4\u5f8b\u3002", "conclusion": "TabSTAR\u4e3a\u8868\u683c\u6570\u636e\u7684\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u8fdb\u4e00\u6b65\u6027\u80fd\u63d0\u5347\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.18110", "pdf": "https://arxiv.org/pdf/2505.18110", "abs": "https://arxiv.org/abs/2505.18110", "authors": ["Zinuo Li", "Xian Zhang", "Yongxin Guo", "Mohammed Bennamoun", "Farid Boussaid", "Girish Dwivedi", "Luqi Gong", "Qiuhong Ke"], "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM", "categories": ["cs.CL"], "comment": null, "summary": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.", "AI": {"tldr": "TriSense\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u97f3\u9891\u548c\u8bed\u97f3\u6a21\u6001\u5b9e\u73b0\u89c6\u9891\u65f6\u5e8f\u7406\u89e3\uff0c\u5e76\u5f15\u5165Query-Based Connector\u52a8\u6001\u8c03\u6574\u6a21\u6001\u6743\u91cd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u878d\u5408\u548c\u89e3\u91ca\u97f3\u9891\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u89c6\u9891\u65f6\u5e8f\u7406\u89e3\u7684\u5168\u9762\u6027\u3002", "method": "\u63d0\u51faTriSense\u6a21\u578b\uff0c\u4f7f\u7528Query-Based Connector\u52a8\u6001\u8c03\u6574\u6a21\u6001\u6743\u91cd\uff0c\u5e76\u6784\u5efaTriSense-2M\u6570\u636e\u96c6\u652f\u6301\u591a\u6a21\u6001\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86TriSense\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u89c6\u9891\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "TriSense\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.18058", "pdf": "https://arxiv.org/pdf/2505.18058", "abs": "https://arxiv.org/abs/2505.18058", "authors": ["Yumeng Zhang", "Zohaib Salahuddin", "Danial Khan", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "title": "A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer", "categories": ["eess.IV", "cs.CV"], "comment": "22 pages, 8 figures", "summary": "Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u591a\u4e2d\u5fc3\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u7c7b\u76f4\u80a0\u764cMRI\u4e2d\u7684EVI\u548cMFI\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9891\u7387\u57df\u534f\u8c03\u548c\u591a\u89c6\u56fe\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u76f4\u80a0\u764cMRI\u4e2dEVI\u548cMFI\u89c6\u89c9\u8bc4\u4f30\u7684\u4e3b\u89c2\u6027\u548c\u673a\u6784\u95f4\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528331\u4f8bMRI\u6570\u636e\uff0c\u901a\u8fc7TotalSegmentator\u63d0\u53d6\u76f4\u80a0\u533a\u57df\uff0c\u8bad\u7ec3\u81ea\u76d1\u7763\u9891\u7387\u57df\u534f\u8c03\u7ba1\u9053\uff0c\u6bd4\u8f83\u56db\u79cd\u5206\u7c7b\u5668\uff08ResNet50\u3001SeResNet\u3001UMedPT\u3001UMedPT_LR\uff09\u3002", "result": "UMedPT_LR\u5728EVI\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08AUC=0.82\uff09\uff0cUMedPT\u5728MFI\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff08AUC=0.77\uff09\uff0c\u5747\u8d85\u8d8a\u57fa\u51c6\u65b9\u6cd5\u3002\u9891\u7387\u57df\u534f\u8c03\u5bf9MFI\u5206\u7c7b\u6709\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7279\u5f81\u3001\u534f\u8c03\u548c\u591a\u89c6\u56fe\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f4\u80a0MRI\u7684\u8bca\u65ad\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.18126", "pdf": "https://arxiv.org/pdf/2505.18126", "abs": "https://arxiv.org/abs/2505.18126", "authors": ["Lorenz Wolf", "Robert Kirk", "Mirco Musolesi"], "title": "Reward Model Overoptimisation in Iterated RLHF", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "20 pages, 17 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) is a widely used method for\naligning large language models with human preferences. However, RLHF often\nsuffers from reward model overoptimisation, in which models overfit to the\nreward function, resulting in non-generalisable policies that exploit the\nidiosyncrasies and peculiarities of the reward function. A common mitigation is\niterated RLHF, in which reward models are repeatedly retrained with updated\nhuman feedback and policies are re-optimised. Despite its increasing adoption,\nthe dynamics of overoptimisation in this setting remain poorly understood. In\nthis work, we present the first comprehensive study of overoptimisation in\niterated RLHF. We systematically analyse key design choices - how reward model\ntraining data is transferred across iterations, which reward function is used\nfor optimisation, and how policies are initialised. Using the controlled\nAlpacaFarm benchmark, we observe that overoptimisation tends to decrease over\nsuccessive iterations, as reward models increasingly approximate ground-truth\npreferences. However, performance gains diminish over time, and while\nreinitialising from the base policy is robust, it limits optimisation\nflexibility. Other initialisation strategies often fail to recover from early\noveroptimisation. These findings offer actionable insights for building more\nstable and generalisable RLHF pipelines.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8fed\u4ee3RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u5bf9\u8fc7\u4f18\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdbRLHF\u7ba1\u9053\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "RLHF\u5728\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u65f6\u5b58\u5728\u5956\u52b1\u6a21\u578b\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u8fed\u4ee3RLHF\u867d\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u52a8\u6001\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7AlpacaFarm\u57fa\u51c6\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4f20\u9012\u3001\u5956\u52b1\u51fd\u6570\u9009\u62e9\u548c\u7b56\u7565\u521d\u59cb\u5316\u7b49\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u8fc7\u4f18\u5316\u968f\u8fed\u4ee3\u51cf\u5c11\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u9012\u51cf\uff1b\u4ece\u57fa\u7840\u7b56\u7565\u91cd\u65b0\u521d\u59cb\u5316\u7a33\u5065\u4f46\u7075\u6d3b\u6027\u53d7\u9650\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u7a33\u5b9a\u548c\u901a\u7528\u7684RLHF\u7ba1\u9053\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "relevance": 90.0}}
{"id": "2505.18122", "pdf": "https://arxiv.org/pdf/2505.18122", "abs": "https://arxiv.org/abs/2505.18122", "authors": ["Poojah Ganesan", "Rajat Aayush Jha", "Dan Roth", "Vivek Gupta"], "title": "UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have greatly improved\nText-to-SQL performance for single-table queries. But, it remains challenging\nin multi-table databases due to complex schema and relational operations.\nExisting methods often struggle with retrieving the right tables and columns,\ngenerating accurate JOINs and UNIONs, and generalizing across diverse schemas.\nTo address these issues, we introduce UNJOIN, a two-stage framework that\ndecouples the retrieval of schema elements from SQL logic generation. In the\nfirst stage, we merge the column names of all tables in the database into a\nsingle-table representation by prefixing each column with its table name. This\nallows the model to focus purely on accurate retrieval without being distracted\nby the need to write complex SQL logic. In the second stage, the SQL query is\ngenerated on this simplified schema and mapped back to the original schema by\nreconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and\nBIRD datasets show that UNJOIN matches or exceeds the state-of-the-art\nbaselines. UNJOIN uses only schema information, which does not require data\naccess or fine-tuning, making it scalable and adaptable across databases.", "AI": {"tldr": "UNJOIN\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8868\u6570\u636e\u5e93\u4e2d\u7684Text-to-SQL\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u5f0f\u5143\u7d20\u68c0\u7d22\u548cSQL\u903b\u8f91\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u8868\u6570\u636e\u5e93\u4e2d\u7684Text-to-SQL\u4efb\u52a1\u56e0\u590d\u6742\u6a21\u5f0f\u548c\u5173\u7cfb\u64cd\u4f5c\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u7d22\u8868\u548c\u5217\u3001\u751f\u6210JOINs\u548cUNIONs\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "UNJOIN\u5c06\u591a\u8868\u6a21\u5f0f\u7b80\u5316\u4e3a\u5355\u8868\u8868\u793a\uff0c\u5206\u4e24\u9636\u6bb5\u5904\u7406\uff1a\u5148\u68c0\u7d22\u6a21\u5f0f\u5143\u7d20\uff0c\u518d\u751f\u6210SQL\u903b\u8f91\u5e76\u6620\u5c04\u56de\u539f\u6a21\u5f0f\u3002", "result": "\u5728SPIDER\u548cBIRD\u6570\u636e\u96c6\u4e0a\uff0cUNJOIN\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u6570\u636e\u8bbf\u95ee\u6216\u5fae\u8c03\u3002", "conclusion": "UNJOIN\u901a\u8fc7\u7b80\u5316\u6a21\u5f0f\u8868\u793a\u548c\u5206\u9636\u6bb5\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8868Text-to-SQL\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "relevance": 70.0}}
{"id": "2505.18131", "pdf": "https://arxiv.org/pdf/2505.18131", "abs": "https://arxiv.org/abs/2505.18131", "authors": ["Jonas A. Actor", "Graham Harper", "Ben Southworth", "Eric C. Cyr"], "title": "Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement", "categories": ["cs.LG", "cs.AI", "68T99", "I.2.6"], "comment": "20 pages, 3 figures, 3 tables", "summary": "Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,\nused in a variety of modern deep learning frameworks. However, recently\nKolmogorov-Arnold Networks (KANs) have become increasingly popular due to their\nsuccess on a range of problems, particularly for scientific machine learning\ntasks. In this paper, we exploit the relationship between KANs and multichannel\nMLPs to gain structural insight into how to train MLPs faster. We demonstrate\nthe KAN basis (1) provides geometric localized support, and (2) acts as a\npreconditioned descent in the ReLU basis, overall resulting in expedited\ntraining and improved accuracy. Our results show the equivalence between\nfree-knot spline KAN architectures, and a class of MLPs that are refined\ngeometrically along the channel dimension of each weight tensor. We exploit\nthis structural equivalence to define a hierarchical refinement scheme that\ndramatically accelerates training of the multi-channel MLP architecture. We\nshow further accuracy improvements can be had by allowing the $1$D locations of\nthe spline knots to be trained simultaneously with the weights. These advances\nare demonstrated on a range of benchmark examples for regression and scientific\nmachine learning.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Kolmogorov-Arnold Networks (KANs)\u4e0e\u591a\u901a\u9053MLPs\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKAN\u7684\u51e0\u4f55\u5c40\u90e8\u652f\u6301\u548c\u9884\u6761\u4ef6\u4e0b\u964d\u65b9\u6cd5\uff0c\u663e\u8457\u52a0\u901f\u4e86MLPs\u7684\u8bad\u7ec3\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76KANs\u4e0eMLPs\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u4ee5\u6539\u8fdbMLPs\u7684\u8bad\u7ec3\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528KAN\u7684\u51e0\u4f55\u5c40\u90e8\u652f\u6301\u548c\u9884\u6761\u4ef6\u4e0b\u964d\u7279\u6027\uff0c\u5b9a\u4e49\u4e86\u4e00\u79cd\u5206\u5c42\u7ec6\u5316\u65b9\u6848\uff0c\u5e76\u8bad\u7ec31D\u6837\u6761\u8282\u70b9\u4f4d\u7f6e\u3002", "result": "\u5728\u56de\u5f52\u548c\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u663e\u8457\u52a0\u901f\u4e86MLPs\u7684\u8bad\u7ec3\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "KANs\u7684\u7ed3\u6784\u7279\u6027\u4e3aMLPs\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "relevance": 40.0}}
{"id": "2505.18128", "pdf": "https://arxiv.org/pdf/2505.18128", "abs": "https://arxiv.org/abs/2505.18128", "authors": ["Chau Minh Pham", "Jenna Russell", "Dzung Pham", "Mohit Iyyer"], "title": "Frankentext: Stitching random text fragments into long-form narratives", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFrankentexts\u7684\u65b0\u578b\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u8981\u6c42LLMs\u5728\u5927\u90e8\u5206\u5185\u5bb9\uff08\u598290%\uff09\u5fc5\u987b\u76f4\u63a5\u590d\u5236\u4eba\u7c7b\u6587\u672c\u7684\u6761\u4ef6\u4e0b\u751f\u6210\u8fde\u8d2f\u53d9\u4e8b\u3002Gemini-2.5-Pro\u8868\u73b0\u4f18\u5f02\uff0c81%\u7684\u751f\u6210\u6587\u672c\u8fde\u8d2f\u4e14100%\u7b26\u5408\u63d0\u793a\uff0c\u4f4659%\u88ab\u8bef\u5224\u4e3a\u4eba\u7c7b\u5199\u4f5c\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6781\u7aef\u7ea6\u675f\u4e0b\u7684\u53ef\u63a7\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793aAI\u6587\u672c\u68c0\u6d4b\u5668\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u9009\u62e9\u5e76\u7ec4\u5408\u4eba\u7c7b\u6587\u672c\u7247\u6bb5\u751f\u6210\u521d\u7a3f\uff0c\u968f\u540e\u8fed\u4ee3\u4fee\u8ba2\u4ee5\u4fdd\u6301\u6307\u5b9a\u590d\u5236\u6bd4\u4f8b\u3002", "result": "Gemini-2.5-Pro\u751f\u6210\u7684\u6587\u672c81%\u8fde\u8d2f\u4e14100%\u76f8\u5173\uff0c\u4f4659%\u88ab\u68c0\u6d4b\u5668\u8bef\u5224\u4e3a\u4eba\u7c7b\u5199\u4f5c\u3002", "conclusion": "Frankentexts\u4e0d\u4ec5\u662f\u4e00\u9879\u6311\u6218\u6027\u4efb\u52a1\uff0c\u8fd8\u4e3aAI\u6587\u672c\u68c0\u6d4b\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u5199\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 85.0}}
{"id": "2505.18107", "pdf": "https://arxiv.org/pdf/2505.18107", "abs": "https://arxiv.org/abs/2505.18107", "authors": ["Yichi Zhang", "Zhihao Duan", "Yuning Huang", "Fengqing Zhu"], "title": "Accelerating Learned Image Compression Through Modeling Neural Training Dynamics", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to TMLR", "summary": "As learned image compression (LIC) methods become increasingly\ncomputationally demanding, enhancing their training efficiency is crucial. This\npaper takes a step forward in accelerating the training of LIC methods by\nmodeling the neural training dynamics. We first propose a Sensitivity-aware\nTrue and Dummy Embedding Training mechanism (STDET) that clusters LIC model\nparameters into few separate modes where parameters are expressed as affine\ntransformations of reference parameters within the same mode. By further\nutilizing the stable intra-mode correlations throughout training and parameter\nsensitivities, we gradually embed non-reference parameters, reducing the number\nof trainable parameters. Additionally, we incorporate a Sampling-then-Moving\nAverage (SMA) technique, interpolating sampled weights from stochastic gradient\ndescent (SGD) training to obtain the moving average weights, ensuring smooth\ntemporal behavior and minimizing training state variances. Overall, our method\nsignificantly reduces training space dimensions and the number of trainable\nparameters without sacrificing model performance, thus accelerating model\nconvergence. We also provide a theoretical analysis on the Noisy quadratic\nmodel, showing that the proposed method achieves a lower training variance than\nstandard SGD. Our approach offers valuable insights for further developing\nefficient training methods for LICs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTDET\u7684\u673a\u5236\uff0c\u901a\u8fc7\u805a\u7c7bLIC\u6a21\u578b\u53c2\u6570\u4e3a\u5c11\u6570\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u53c2\u6570\u654f\u611f\u6027\u548c\u7a33\u5b9a\u7684\u6a21\u5f0f\u5185\u76f8\u5173\u6027\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u7ed3\u5408SMA\u6280\u672f\u52a0\u901f\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\uff08LIC\uff09\u65b9\u6cd5\u7684\u8ba1\u7b97\u9700\u6c42\u589e\u52a0\uff0c\u63d0\u9ad8\u5176\u8bad\u7ec3\u6548\u7387\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faSTDET\u673a\u5236\uff0c\u805a\u7c7b\u53c2\u6570\u4e3a\u6a21\u5f0f\uff0c\u5229\u7528\u53c2\u6570\u654f\u611f\u6027\u548c\u6a21\u5f0f\u5185\u76f8\u5173\u6027\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\uff1b\u7ed3\u5408SMA\u6280\u672f\u5e73\u6ed1\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u7a7a\u95f4\u7ef4\u5ea6\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\uff0c\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLIC\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u5176\u8bad\u7ec3\u65b9\u5dee\u4f4e\u4e8e\u6807\u51c6SGD\u3002", "relevance": 40.0}}
{"id": "2505.17568", "pdf": "https://arxiv.org/pdf/2505.17568", "abs": "https://arxiv.org/abs/2505.17568", "authors": ["Zifan Peng", "Yule Liu", "Zhen Sun", "Mingchen Li", "Zeren Luo", "Jingyi Zheng", "Wenhan Dong", "Xinlei He", "Xuechao Wang", "Yingjie Xue", "Shengmin Xu", "Xinyi Huang"], "title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Audio Language Models (ALMs) have made significant progress recently. These\nmodels integrate the audio modality directly into the model, rather than\nconverting speech into text and inputting text to Large Language Models (LLMs).\nWhile jailbreak attacks on LLMs have been extensively studied, the security of\nALMs with audio modalities remains largely unexplored. Currently, there is a\nlack of an adversarial audio dataset and a unified framework specifically\ndesigned to evaluate and compare attacks and ALMs. In this paper, we present\nJALMBench, the \\textit{first} comprehensive benchmark to assess the safety of\nALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200\ntext samples and 51,381 audio samples with over 268 hours. It supports 12\nmainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and\n5 defense methods. Using JALMBench, we provide an in-depth analysis of attack\nefficiency, topic sensitivity, voice diversity, and attack representations.\nAdditionally, we explore mitigation strategies for the attacks at both the\nprompt level and the response level.", "AI": {"tldr": "JALMBench\u662f\u9996\u4e2a\u9488\u5bf9\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08ALMs\uff09\u5b89\u5168\u6027\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6570\u636e\u96c6\u548c\u591a\u79cd\u653b\u51fb\u4e0e\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30ALMs\u7684\u8d8a\u72f1\u653b\u51fb\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u7684\u8d8a\u72f1\u653b\u51fb\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46ALMs\u7684\u5b89\u5168\u6027\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faJALMBench\u57fa\u51c6\uff0c\u5305\u542b2200\u4e2a\u6587\u672c\u6837\u672c\u548c51381\u4e2a\u97f3\u9891\u6837\u672c\uff0c\u652f\u630112\u79cdALMs\u30018\u79cd\u653b\u51fb\u65b9\u6cd5\u548c5\u79cd\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7JALMBench\u5206\u6790\u4e86\u653b\u51fb\u6548\u7387\u3001\u4e3b\u9898\u654f\u611f\u6027\u3001\u8bed\u97f3\u591a\u6837\u6027\u53ca\u653b\u51fb\u8868\u793a\uff0c\u5e76\u63a2\u7d22\u4e86\u63d0\u793a\u548c\u54cd\u5e94\u5c42\u9762\u7684\u7f13\u89e3\u7b56\u7565\u3002", "conclusion": "JALMBench\u586b\u8865\u4e86ALMs\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u65b9\u5411\u3002", "relevance": 60.0}}
{"id": "2505.18150", "pdf": "https://arxiv.org/pdf/2505.18150", "abs": "https://arxiv.org/abs/2505.18150", "authors": ["Nic Fishman", "Gokul Gowri", "Peng Yin", "Jonathan Gootenberg", "Omar Abudayyeh"], "title": "Generative Distribution Embeddings", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "comment": null, "summary": "Many real-world problems require reasoning across multiple scales, demanding\nmodels which operate not on single data points, but on entire distributions. We\nintroduce generative distribution embeddings (GDE), a framework that lifts\nautoencoders to the space of distributions. In GDEs, an encoder acts on sets of\nsamples, and the decoder is replaced by a generator which aims to match the\ninput distribution. This framework enables learning representations of\ndistributions by coupling conditional generative models with encoder networks\nwhich satisfy a criterion we call distributional invariance. We show that GDEs\nlearn predictive sufficient statistics embedded in the Wasserstein space, such\nthat latent GDE distances approximately recover the $W_2$ distance, and latent\ninterpolation approximately recovers optimal transport trajectories for\nGaussian and Gaussian mixture distributions. We systematically benchmark GDEs\nagainst existing approaches on synthetic datasets, demonstrating consistently\nstronger performance. We then apply GDEs to six key problems in computational\nbiology: learning representations of cell populations from lineage-tracing data\n(150K cells), predicting perturbation effects on single-cell transcriptomes (1M\ncells), predicting perturbation effects on cellular phenotypes (20M single-cell\nimages), modeling tissue-specific DNA methylation patterns (253M sequences),\ndesigning synthetic yeast promoters (34M sequences), and spatiotemporal\nmodeling of viral protein sequences (1M sequences).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5206\u5e03\u5d4c\u5165\uff08GDE\uff09\u6846\u67b6\uff0c\u5c06\u81ea\u7f16\u7801\u5668\u6269\u5c55\u5230\u5206\u5e03\u7a7a\u95f4\uff0c\u901a\u8fc7\u5b66\u4e60\u5206\u5e03\u7684\u8868\u793a\uff0c\u5728\u591a\u4e2a\u8ba1\u7b97\u751f\u7269\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u95ee\u9898\u9700\u8981\u8de8\u5c3a\u5ea6\u63a8\u7406\uff0c\u4f20\u7edf\u6a21\u578b\u4ec5\u5904\u7406\u5355\u70b9\u6570\u636e\uff0c\u800cGDE\u6846\u67b6\u901a\u8fc7\u5206\u5e03\u5d4c\u5165\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GDE\u6846\u67b6\u7ed3\u5408\u6761\u4ef6\u751f\u6210\u6a21\u578b\u548c\u6ee1\u8db3\u5206\u5e03\u4e0d\u53d8\u6027\u51c6\u5219\u7684\u7f16\u7801\u5668\u7f51\u7edc\uff0c\u5b66\u4e60\u5206\u5e03\u7684\u8868\u793a\u3002", "result": "GDE\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u516d\u4e2a\u8ba1\u7b97\u751f\u7269\u5b66\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GDE\u6846\u67b6\u4e3a\u5206\u5e03\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.18136", "pdf": "https://arxiv.org/pdf/2505.18136", "abs": "https://arxiv.org/abs/2505.18136", "authors": ["Mykola Trokhymovych", "Lydia Pintscher", "Ricardo Baeza-Yates", "Diego Saez-Trumper"], "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a next-generation vandalism detection system for Wikidata, one\nof the largest open-source structured knowledge bases on the Web. Wikidata is\nhighly complex: its items incorporate an ever-expanding universe of factual\ntriples and multilingual texts. While edits can alter both structured and\ntextual content, our approach converts all edits into a single space using a\nmethod we call Graph2Text. This allows for evaluating all content changes for\npotential vandalism using a single multilingual language model. This unified\napproach improves coverage and simplifies maintenance. Experiments demonstrate\nthat our solution outperforms the current production system. Additionally, we\nare releasing the code under an open license along with a large dataset of\nvarious human-generated knowledge alterations, enabling further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eWikidata\u7684\u4e0b\u4e00\u4ee3\u7834\u574f\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7Graph2Text\u65b9\u6cd5\u5c06\u6240\u6709\u7f16\u8f91\u8f6c\u6362\u4e3a\u7edf\u4e00\u7a7a\u95f4\uff0c\u5229\u7528\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5185\u5bb9\u53d8\u66f4\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "Wikidata\u662f\u4e00\u4e2a\u590d\u6742\u7684\u5f00\u653e\u77e5\u8bc6\u5e93\uff0c\u7f16\u8f91\u53ef\u80fd\u7834\u574f\u5176\u7ed3\u6784\u5316\u6216\u6587\u672c\u5185\u5bb9\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u7834\u574f\u884c\u4e3a\u3002", "method": "\u91c7\u7528Graph2Text\u65b9\u6cd5\u5c06\u6240\u6709\u7f16\u8f91\u8f6c\u6362\u4e3a\u7edf\u4e00\u7a7a\u95f4\uff0c\u4f7f\u7528\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5185\u5bb9\u53d8\u66f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u751f\u4ea7\u7cfb\u7edf\uff0c\u5e76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8986\u76d6\u8303\u56f4\u5e76\u7b80\u5316\u4e86\u7ef4\u62a4\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002", "relevance": 30.0}}
{"id": "2505.17589", "pdf": "https://arxiv.org/pdf/2505.17589", "abs": "https://arxiv.org/abs/2505.17589", "authors": ["Zhihao Du", "Changfeng Gao", "Yuxuan Wang", "Fan Yu", "Tianyu Zhao", "Hao Wang", "Xiang Lv", "Hui Wang", "Xian Shi", "Keyu An", "Guanrou Yang", "Yabin Li", "Yanni Chen", "Zhifu Gao", "Qian Chen", "Yue Gu", "Mengzhe Chen", "Yafeng Chen", "Shiliang Zhang", "Wen Wang", "Jieping Ye"], "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Preprint, work in progress", "summary": "In our prior works, we introduced a scalable streaming speech synthesis\nmodel, CosyVoice 2, which integrates a large language model (LLM) and a\nchunk-aware flow matching (FM) model, and achieves low-latency bi-streaming\nspeech synthesis and human-parity quality. Despite these advancements,\nCosyVoice 2 exhibits limitations in language coverage, domain diversity, data\nvolume, text formats, and post-training techniques. In this paper, we present\nCosyVoice 3, an improved model designed for zero-shot multilingual speech\nsynthesis in the wild, surpassing its predecessor in content consistency,\nspeaker similarity, and prosody naturalness. Key features of CosyVoice 3\ninclude: 1) A novel speech tokenizer to improve prosody naturalness, developed\nvia supervised multi-task training, including automatic speech recognition,\nspeech emotion recognition, language identification, audio event detection, and\nspeaker analysis. 2) A new differentiable reward model for post-training\napplicable not only to CosyVoice 3 but also to other LLM-based speech synthesis\nmodels. 3) Dataset Size Scaling: Training data is expanded from ten thousand\nhours to one million hours, encompassing 9 languages and 18 Chinese dialects\nacross various domains and text formats. 4) Model Size Scaling: Model\nparameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced\nperformance on our multilingual benchmark due to the larger model capacity.\nThese advancements contribute significantly to the progress of speech synthesis\nin the wild. We encourage readers to listen to the demo at\nhttps://funaudiollm.github.io/cosyvoice3.", "AI": {"tldr": "CosyVoice 3\u662fCosyVoice 2\u7684\u6539\u8fdb\u7248\uff0c\u4e13\u6ce8\u4e8e\u96f6\u6837\u672c\u591a\u8bed\u8a00\u8bed\u97f3\u5408\u6210\uff0c\u901a\u8fc7\u65b0\u8bed\u97f3\u5206\u8bcd\u5668\u3001\u53ef\u5fae\u5206\u5956\u52b1\u6a21\u578b\u3001\u6570\u636e\u6269\u5c55\u548c\u6a21\u578b\u89c4\u6a21\u63d0\u5347\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u4e00\u81f4\u6027\u3001\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u548c\u97f5\u5f8b\u81ea\u7136\u5ea6\u3002", "motivation": "\u89e3\u51b3CosyVoice 2\u5728\u8bed\u8a00\u8986\u76d6\u3001\u9886\u57df\u591a\u6837\u6027\u3001\u6570\u636e\u91cf\u548c\u6587\u672c\u683c\u5f0f\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u8fdb\u6b65\u3002", "method": "1) \u5f00\u53d1\u591a\u4efb\u52a1\u8bad\u7ec3\u7684\u8bed\u97f3\u5206\u8bcd\u5668\uff1b2) \u5f15\u5165\u53ef\u5fae\u5206\u5956\u52b1\u6a21\u578b\uff1b3) \u6269\u5c55\u8bad\u7ec3\u6570\u636e\u81f3\u767e\u4e07\u5c0f\u65f6\uff1b4) \u589e\u52a0\u6a21\u578b\u53c2\u6570\u81f315\u4ebf\u3002", "result": "\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5185\u5bb9\u4e00\u81f4\u6027\u3001\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u548c\u97f5\u5f8b\u81ea\u7136\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CosyVoice 3\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u5408\u6210\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u6280\u672f\u652f\u6301\u3002", "relevance": 60.0}}
{"id": "2411.17411", "pdf": "https://arxiv.org/pdf/2411.17411", "abs": "https://arxiv.org/abs/2411.17411", "authors": ["Takaaki Fujita"], "title": "Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond", "categories": ["cs.AI", "cs.DM", "cs.LG", "math.CO", "03B52"], "comment": "255 pages. 11 figures. Published as a book in 2024. Publisher: Biblio\n  Publishing. ISBN: 978-1-59973-812-3", "summary": "To better handle real-world uncertainty, concepts such as fuzzy sets,\nneutrosophic sets, rough sets, and soft sets have been introduced. For example,\nneutrosophic sets, which simultaneously represent truth, indeterminacy, and\nfalsehood, have proven to be valuable tools for modeling uncertainty in complex\nsystems. These set concepts are increasingly studied in graphized forms, and\ngeneralized graph concepts now encompass well-known structures such as\nhypergraphs and superhypergraphs. Furthermore, hyperconcepts and\nsuperhyperconcepts are being actively researched in areas beyond graph theory.\n  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough\nsets, soft sets, and plithogenic sets), uncertain graphs, and hyper and\nsuperhyper concepts are active areas of research with significant mathematical\nand practical implications. Recognizing their importance, this paper explores\nnew graph and set concepts, as well as hyper and superhyper concepts, as\ndetailed in the \"Results\" section of \"The Structure of the Paper.\"\nAdditionally, this work aims to consolidate recent findings, providing a\nsurvey-like resource to inform and engage readers.\n  For instance, we extend several graph concepts by introducing Neutrosophic\nOversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard\nReal Set. This paper defines a variety of concepts with the goal of inspiring\nnew ideas and serving as a valuable resource for researchers in their academic\npursuits.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6a21\u7cca\u96c6\u3001\u4e2d\u667a\u96c6\u7b49\u4e0d\u786e\u5b9a\u96c6\u5408\u5728\u56fe\u8bba\u4e2d\u7684\u6269\u5c55\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u56fe\u6982\u5ff5\uff08\u5982\u4e2d\u667a\u8d85\u96c6\u3001\u5b50\u96c6\u7b49\uff09\uff0c\u5e76\u6574\u5408\u4e86\u8fd1\u671f\u7814\u7a76\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7814\u7a76\u6a21\u7cca\u96c6\u3001\u4e2d\u667a\u96c6\u7b49\u5728\u56fe\u8bba\u4e2d\u7684\u6269\u5c55\u53ca\u5176\u6570\u5b66\u548c\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e2d\u667a\u8d85\u96c6\u3001\u4e2d\u667a\u5b50\u96c6\u7b49\u65b0\u6982\u5ff5\uff0c\u6269\u5c55\u56fe\u8bba\u4e2d\u7684\u4e0d\u786e\u5b9a\u96c6\u5408\u7406\u8bba\uff0c\u5e76\u6574\u5408\u8fd1\u671f\u7814\u7a76\u6210\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u591a\u79cd\u65b0\u7684\u56fe\u6982\u5ff5\uff08\u5982\u4e2d\u667a\u8d85\u96c6\u3001\u5b50\u96c6\u7b49\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u7814\u7a76\u7684\u7efc\u8ff0\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0d\u786e\u5b9a\u96c6\u5408\u548c\u56fe\u8bba\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8d44\u6e90\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8005\u6709\u542f\u53d1\u4f5c\u7528\u3002", "relevance": 30.0}}
{"id": "2505.18148", "pdf": "https://arxiv.org/pdf/2505.18148", "abs": "https://arxiv.org/abs/2505.18148", "authors": ["Owen Bianchi", "Mathew J. Koretsky", "Maya Willey", "Chelsea X. Alvarado", "Tanay Nayak", "Adi Asija", "Nicole Kuznetsov", "Mike A. Nalls", "Faraz Faghri", "Daniel Khashabi"], "title": "Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9ec4\u91d1\u4e0a\u4e0b\u6587\u957f\u5ea6\u5bf9LLM\u5728\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8f83\u77ed\u7684\u9ec4\u91d1\u4e0a\u4e0b\u6587\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u5e76\u589e\u52a0\u4f4d\u7f6e\u654f\u611f\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u2018\u5927\u6d77\u635e\u9488\u2019\u4efb\u52a1\u4e2d\u9ec4\u91d1\u4e0a\u4e0b\u6587\u957f\u5ea6\u5f71\u54cd\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u9ec4\u91d1\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5b9e\u9a8c\u6db5\u76d6\u4e09\u4e2a\u9886\u57df\u548c\u4e03\u79cd\u5148\u8fdbLLM\u3002", "result": "\u8f83\u77ed\u7684\u9ec4\u91d1\u4e0a\u4e0b\u6587\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u653e\u5927\u4e86\u4f4d\u7f6e\u654f\u611f\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684LLM\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u660e\u786e\u6307\u5bfc\u3002", "relevance": 85.0}}
{"id": "2505.18151", "pdf": "https://arxiv.org/pdf/2505.18151", "abs": "https://arxiv.org/abs/2505.18151", "authors": ["Zizhang Li", "Hong-Xing Yu", "Wei Liu", "Yin Yang", "Charles Herrmann", "Gordon Wetzstein", "Jiajun Wu"], "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "The first two authors contributed equally. Project website:\n  https://kyleleey.github.io/WonderPlay/", "summary": "WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/", "AI": {"tldr": "WonderPlay\u662f\u4e00\u4e2a\u7ed3\u5408\u7269\u7406\u6a21\u62df\u4e0e\u89c6\u9891\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u52a8\u4f5c\u6761\u4ef6\u7684\u52a8\u60013D\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u521a\u4f53\u6216\u7b80\u5355\u5f39\u6027\u52a8\u529b\u5b66\uff0c\u800cWonderPlay\u65e8\u5728\u901a\u8fc7\u6df7\u5408\u751f\u6210\u6a21\u62df\u5668\u5408\u6210\u66f4\u5e7f\u6cdb\u76843D\u52a8\u6001\u573a\u666f\u3002", "method": "\u91c7\u7528\u6df7\u5408\u751f\u6210\u6a21\u62df\u5668\uff0c\u5148\u7528\u7269\u7406\u6c42\u89e3\u5668\u6a21\u62df\u7c97\u75653D\u52a8\u6001\uff0c\u518d\u901a\u8fc7\u89c6\u9891\u751f\u6210\u5668\u751f\u6210\u66f4\u7cbe\u7ec6\u7684\u89c6\u9891\uff0c\u6700\u540e\u7528\u89c6\u9891\u66f4\u65b0\u6a21\u62df\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWonderPlay\u80fd\u5904\u7406\u591a\u79cd\u52a8\u6001\u573a\u666f\uff08\u5982\u5e03\u6599\u3001\u6c99\u5b50\u3001\u96ea\u3001\u6db2\u4f53\u7b49\uff09\uff0c\u4ec5\u9700\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u3002", "conclusion": "WonderPlay\u7ed3\u5408\u4e86\u7269\u7406\u6a21\u62df\u7684\u7cbe\u786e\u6027\u548c\u6269\u6563\u89c6\u9891\u751f\u6210\u7684\u8868\u73b0\u529b\uff0c\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u7528\u6237\u63a7\u5236\u3002", "relevance": 40.0}}
{"id": "2505.17030", "pdf": "https://arxiv.org/pdf/2505.17030", "abs": "https://arxiv.org/abs/2505.17030", "authors": ["Jingzhi Hu", "Geoffrey Ye Li"], "title": "Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Future networks are envisioned to connect massive artificial intelligence\n(AI) agents, enabling their extensive collaboration on diverse tasks. Compared\nto traditional entities, these agents naturally suit the semantic communication\n(SC), which can significantly enhance the bandwidth efficiency. Nevertheless,\nSC requires the knowledge among agents to be aligned, while agents have\ndistinct expert knowledge for their individual tasks in practice. In this\npaper, we propose a distillation-enabled knowledge alignment protocol (DeKAP),\nwhich distills the expert knowledge of each agent into parameter-efficient\nlow-rank matrices, allocates them across the network, and allows agents to\nsimultaneously maintain aligned knowledge for multiple tasks. We formulate the\njoint minimization of alignment loss, communication overhead, and storage cost\nas a large-scale integer linear programming problem and develop a highly\nefficient greedy algorithm. From computer simulation, the DeKAP establishes\nknowledge alignment with the lowest communication and computation resources\ncompared to conventional approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u84b8\u998f\u9a71\u52a8\u7684\u77e5\u8bc6\u5bf9\u9f50\u534f\u8bae\uff08DeKAP\uff09\uff0c\u901a\u8fc7\u5c06\u4e13\u5bb6\u77e5\u8bc6\u84b8\u998f\u4e3a\u4f4e\u79e9\u77e9\u9635\u5e76\u5206\u914d\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u77e5\u8bc6\u5bf9\u9f50\uff0c\u4f18\u5316\u901a\u4fe1\u548c\u5b58\u50a8\u5f00\u9500\u3002", "motivation": "\u672a\u6765\u7f51\u7edc\u9700\u8981\u8fde\u63a5\u5927\u91cfAI\u4ee3\u7406\u4ee5\u5b9e\u73b0\u534f\u4f5c\uff0c\u4f46\u4ee3\u7406\u95f4\u7684\u77e5\u8bc6\u5dee\u5f02\u963b\u788d\u4e86\u8bed\u4e49\u901a\u4fe1\uff08SC\uff09\u7684\u6548\u7387\u3002DeKAP\u65e8\u5728\u89e3\u51b3\u77e5\u8bc6\u5bf9\u9f50\u95ee\u9898\u3002", "method": "DeKAP\u5c06\u4e13\u5bb6\u77e5\u8bc6\u84b8\u998f\u4e3a\u53c2\u6570\u9ad8\u6548\u7684\u4f4e\u79e9\u77e9\u9635\uff0c\u5206\u914d\u81f3\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u548c\u8d2a\u5fc3\u7b97\u6cd5\u4f18\u5316\u5bf9\u9f50\u635f\u5931\u3001\u901a\u4fe1\u5f00\u9500\u548c\u5b58\u50a8\u6210\u672c\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0cDeKAP\u5728\u901a\u4fe1\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u5bf9\u9f50\u3002", "conclusion": "DeKAP\u4e3a\u591a\u4ee3\u7406\u7f51\u7edc\u4e2d\u7684\u77e5\u8bc6\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8bed\u4e49\u901a\u4fe1\u573a\u666f\u3002", "relevance": 60.0}}
{"id": "2505.18149", "pdf": "https://arxiv.org/pdf/2505.18149", "abs": "https://arxiv.org/abs/2505.18149", "authors": ["Aradhye Agarwal", "Ayan Sengupta", "Tanmoy Chakraborty"], "title": "First Finish Search: Efficient Test-Time Scaling in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches $n$ independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves $82.23\\%$ accuracy on the AIME datasets, a $15\\%$\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFirst Finish Search (FFS)\u7684\u65e0\u8bad\u7ec3\u5e76\u884c\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u5728\u63a8\u7406\u65f6\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002FFS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4f9d\u8d56\u957f\u89e3\u7801\u8def\u5f84\u6216\u5927\u91cf\u6837\u672c\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002\u4f5c\u8005\u53d1\u73b0\u77ed\u8def\u5f84\u66f4\u53ef\u80fd\u6b63\u786e\uff0c\u56e0\u6b64\u63d0\u51faFFS\u4ee5\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "method": "FFS\u662f\u4e00\u79cd\u65e0\u8bad\u7ec3\u5e76\u884c\u89e3\u7801\u7b56\u7565\uff0c\u542f\u52a8\u591a\u4e2a\u72ec\u7acb\u6837\u672c\u5e76\u5728\u4efb\u4e00\u5b8c\u6210\u65f6\u8fd4\u56de\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86FFS\u4e0e\u5176\u4ed6\u89e3\u7801\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "FFS\u5728DeepSeek-R1\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8682.23%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534715%\uff0c\u63a5\u8fd1OpenAI o4-mini\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u652f\u6301\u77ed\u8def\u5f84\u7684\u6b63\u786e\u6027\u3002", "conclusion": "FFS\u5c55\u793a\u4e86\u7b80\u5355\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u4e3a\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2505.17032", "pdf": "https://arxiv.org/pdf/2505.17032", "abs": "https://arxiv.org/abs/2505.17032", "authors": ["Jiequn Han", "Arnulf Jentzen", "Weinan E"], "title": "A brief review of the Deep BSDE method for solving high-dimensional partial differential equations", "categories": ["math.NA", "cs.CE", "cs.LG", "cs.NA"], "comment": null, "summary": "High-dimensional partial differential equations (PDEs) pose significant\nchallenges for numerical computation due to the curse of dimensionality, which\nlimits the applicability of traditional mesh-based methods. Since 2017, the\nDeep BSDE method has introduced deep learning techniques that enable the\neffective solution of nonlinear PDEs in very high dimensions. This innovation\nhas sparked considerable interest in using neural networks for high-dimensional\nPDEs, making it an active area of research. In this short review, we briefly\nsketch the Deep BSDE method, its subsequent developments, and future directions\nfor the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u56de\u987e\u4e86Deep BSDE\u65b9\u6cd5\u53ca\u5176\u5728\u9ad8\u7ef4\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u540e\u7eed\u53d1\u5c55\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u9ad8\u7ef4PDE\u7684\u4f20\u7edf\u6570\u503c\u8ba1\u7b97\u65b9\u6cd5\u53d7\u9650\u4e8e\u7ef4\u5ea6\u707e\u96be\uff0cDeep BSDE\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u6fc0\u53d1\u4e86\u7814\u7a76\u5174\u8da3\u3002", "method": "\u8bba\u6587\u7b80\u8981\u6982\u8ff0\u4e86Deep BSDE\u65b9\u6cd5\u53ca\u5176\u540e\u7eed\u6539\u8fdb\u3002", "result": "Deep BSDE\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u9ad8\u7ef4\u975e\u7ebf\u6027PDE\u7684\u6c42\u89e3\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "Deep BSDE\u65b9\u6cd5\u4e3a\u9ad8\u7ef4PDE\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u4ecd\u6709\u53d1\u5c55\u6f5c\u529b\u3002", "relevance": 20.0}}
{"id": "2505.18152", "pdf": "https://arxiv.org/pdf/2505.18152", "abs": "https://arxiv.org/abs/2505.18152", "authors": ["Wafa Alghallabi", "Ritesh Thawkar", "Sara Ghaboura", "Ketan More", "Omkar Thawakar", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs", "categories": ["cs.CL"], "comment": "Github:https://github.com/mbzuai-oryx/FannOrFlop,\n  Dataset:https://huggingface.co/datasets/omkarthawakar/FannOrFlop", "summary": "Arabic poetry stands as one of the most sophisticated and culturally embedded\nforms of expression in the Arabic language, known for its layered meanings,\nstylistic diversity, and deep historical continuity. Although large language\nmodels (LLMs) have demonstrated strong performance across languages and tasks,\ntheir ability to understand Arabic poetry remains largely unexplored. In this\nwork, we introduce `Fann or Flop`, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in twelve historical eras, covering 21\ncore poetic genres and a variety of metrical forms, from classical structures\nto contemporary free verse. The benchmark comprises a curated corpus of poems\nwith explanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM is in understanding\nclassical Arabic through the Arabic poetry. Unlike surface-level tasks, this\ndomain demands deeper interpretive reasoning and cultural sensitivity. Our\nevaluation of state-of-the-art LLMs shows that most models struggle with poetic\nunderstanding despite strong results on standard Arabic benchmarks. We release\n`Fann or Flop` along with the evaluation suite as an open-source resource to\nenable rigorous evaluation and advancement for Arabic language models. Code is\navailable at: https://github.com/mbzuai-oryx/FannOrFlop.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u963f\u62c9\u4f2f\u8bd7\u6b4c\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6`Fann or Flop`\uff0c\u8986\u76d612\u4e2a\u5386\u53f2\u65f6\u671f\u548c21\u79cd\u8bd7\u6b4c\u7c7b\u578b\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709LLM\u5728\u8bd7\u6b4c\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bd7\u6b4c\u662f\u963f\u62c9\u4f2f\u8bed\u8a00\u4e2d\u590d\u6742\u4e14\u6587\u5316\u6df1\u539a\u7684\u8868\u8fbe\u5f62\u5f0f\uff0c\u4f46LLM\u5bf9\u5176\u7406\u89e3\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa`Fann or Flop`\u57fa\u51c6\uff0c\u5305\u542b\u8bd7\u6b4c\u8bed\u6599\u5e93\u548c\u8bc4\u4f30\u8bed\u4e49\u7406\u89e3\u3001\u9690\u55bb\u89e3\u91ca\u3001\u97f5\u5f8b\u610f\u8bc6\u53ca\u6587\u5316\u80cc\u666f\u7684\u4efb\u52a1\u3002", "result": "\u73b0\u6709LLM\u5728\u8bd7\u6b4c\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u5c3d\u7ba1\u5728\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8bd7\u6b4c\u7406\u89e3\u662f\u8bc4\u4f30LLM\u5bf9\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\u7406\u89e3\u80fd\u529b\u7684\u5f3a\u6709\u529b\u6307\u6807\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "relevance": 60.0}}
{"id": "2505.18154", "pdf": "https://arxiv.org/pdf/2505.18154", "abs": "https://arxiv.org/abs/2505.18154", "authors": ["Ya Wu", "Qiang Sheng", "Danding Wang", "Guang Yang", "Yifan Sun", "Zhengjia Wang", "Yuyan Bu", "Juan Cao"], "title": "The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas", "categories": ["cs.CL", "cs.CY"], "comment": "25 pages, 8 figures", "summary": "Ethical decision-making is a critical aspect of human judgment, and the\ngrowing use of LLMs in decision-support systems necessitates a rigorous\nevaluation of their moral reasoning capabilities. However, existing assessments\nprimarily rely on single-step evaluations, failing to capture how models adapt\nto evolving ethical challenges. Addressing this gap, we introduce the\nMulti-step Moral Dilemmas (MMDs), the first dataset specifically constructed to\nevaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.\nThis framework enables a fine-grained, dynamic analysis of how LLMs adjust\ntheir moral reasoning across escalating dilemmas. Our evaluation of nine widely\nused LLMs reveals that their value preferences shift significantly as dilemmas\nprogress, indicating that models recalibrate moral judgments based on scenario\ncomplexity. Furthermore, pairwise value comparisons demonstrate that while LLMs\noften prioritize the value of care, this value can sometimes be superseded by\nfairness in certain contexts, highlighting the dynamic and context-dependent\nnature of LLM ethical reasoning. Our findings call for a shift toward dynamic,\ncontext-aware evaluation paradigms, paving the way for more human-aligned and\nvalue-sensitive development of LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Multi-step Moral Dilemmas (MMDs)\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u52a8\u6001\u8bc4\u4f30LLMs\u5728\u590d\u6742\u9053\u5fb7\u56f0\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u7684\u4ef7\u503c\u504f\u597d\u4f1a\u968f\u60c5\u5883\u53d8\u5316\u800c\u8c03\u6574\uff0c\u547c\u5401\u91c7\u7528\u52a8\u6001\u8bc4\u4f30\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u591a\u4e3a\u5355\u6b65\uff0c\u65e0\u6cd5\u6355\u6349LLMs\u5728\u52a8\u6001\u9053\u5fb7\u6311\u6218\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u9700\u5f00\u53d1\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3,302\u4e2a\u4e94\u9636\u6bb5\u9053\u5fb7\u56f0\u5883\u7684MMDs\u6570\u636e\u96c6\uff0c\u5bf9\u4e5d\u79cd\u4e3b\u6d41LLMs\u8fdb\u884c\u52a8\u6001\u5206\u6790\u3002", "result": "LLMs\u7684\u4ef7\u503c\u504f\u597d\u968f\u56f0\u5883\u590d\u6742\u5316\u663e\u8457\u53d8\u5316\uff0c\u4f18\u5148\u8003\u8651\u5173\u6000\u4ef7\u503c\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u5883\u4e0b\u516c\u5e73\u4ef7\u503c\u53ef\u80fd\u8d85\u8d8a\u5173\u6000\u3002", "conclusion": "\u9700\u8f6c\u5411\u52a8\u6001\u3001\u60c5\u5883\u611f\u77e5\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ee5\u4fc3\u8fdbLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u66f4\u4e00\u81f4\u7684\u53d1\u5c55\u3002", "relevance": 85.0}}
{"id": "2505.17046", "pdf": "https://arxiv.org/pdf/2505.17046", "abs": "https://arxiv.org/abs/2505.17046", "authors": ["Lucas Arenstein", "Martin Mikkelsen", "Michael Kastoryano"], "title": "Fast and Flexible Quantum-Inspired Differential Equation Solvers with Data Integration", "categories": ["math.NA", "cs.LG", "cs.NA", "quant-ph"], "comment": null, "summary": "Accurately solving high-dimensional partial differential equations (PDEs)\nremains a central challenge in computational mathematics. Traditional numerical\nmethods, while effective in low-dimensional settings or on coarse grids, often\nstruggle to deliver the precision required in practical applications. Recent\nmachine learning-based approaches offer flexibility but frequently fall short\nin terms of accuracy and reliability, particularly in industrial contexts. In\nthis work, we explore a quantum-inspired method based on quantized tensor\ntrains (QTT), enabling efficient and accurate solutions to PDEs in a variety of\nchallenging scenarios. Through several representative examples, we demonstrate\nthat the QTT approach can achieve logarithmic scaling in both memory and\ncomputational cost for linear and nonlinear PDEs. Additionally, we introduce a\nnovel technique for data-driven learning within the quantum-inspired framework,\ncombining the adaptability of neural networks with enhanced accuracy and\nreduced training time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u542f\u53d1\u7684\u91cf\u5316\u5f20\u91cf\u94fe\uff08QTT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u6c42\u89e3\u9ad8\u7ef4\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u7684\u5bf9\u6570\u7f29\u653e\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u9ad8\u7ef4PDEs\u6c42\u89e3\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002QTT\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u542f\u53d1\u7684\u91cf\u5316\u5f20\u91cf\u94fe\uff08QTT\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u9ad8\u6c42\u89e3PDEs\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "QTT\u65b9\u6cd5\u5728\u591a\u79cd\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027PDEs\u4e2d\u5b9e\u73b0\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u7684\u5bf9\u6570\u7f29\u653e\uff0c\u5e76\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "QTT\u65b9\u6cd5\u4e3a\u9ad8\u7ef4PDEs\u6c42\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u9002\u5e94\u6027\u548c\u91cf\u5b50\u542f\u53d1\u7684\u4f18\u52bf\u3002", "relevance": 30.0}}
{"id": "2505.17623", "pdf": "https://arxiv.org/pdf/2505.17623", "abs": "https://arxiv.org/abs/2505.17623", "authors": ["Ali Rahimi", "Babak H. Khalaj", "Mohammad Ali Maddah-Ali"], "title": "\\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG", "cs.PF"], "comment": null, "summary": "Verifiable computing (VC) has gained prominence in decentralized machine\nlearning systems, where resource-intensive tasks like deep neural network (DNN)\ninference are offloaded to external participants due to blockchain limitations.\nThis creates a need to verify the correctness of outsourced computations\nwithout re-execution. We propose \\texttt{Range-Arithmetic}, a novel framework\nfor efficient and verifiable DNN inference that transforms non-arithmetic\noperations, such as rounding after fixed-point matrix multiplication and ReLU,\ninto arithmetic steps verifiable using sum-check protocols and concatenated\nrange proofs. Our approach avoids the complexity of Boolean encoding,\nhigh-degree polynomials, and large lookup tables while remaining compatible\nwith finite-field-based proof systems. Experimental results show that our\nmethod not only matches the performance of existing approaches, but also\nreduces the computational cost of verifying the results, the computational\neffort required from the untrusted party performing the DNN inference, and the\ncommunication overhead between the two sides.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRange-Arithmetic\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u53ef\u9a8c\u8bc1\u7684DNN\u63a8\u7406\uff0c\u901a\u8fc7\u5c06\u975e\u7b97\u672f\u64cd\u4f5c\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u7684\u7b97\u672f\u6b65\u9aa4\uff0c\u964d\u4f4e\u4e86\u9a8c\u8bc1\u7684\u8ba1\u7b97\u6210\u672c\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u8d44\u6e90\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982DNN\u63a8\u7406\uff09\u5e38\u88ab\u5916\u5305\u7ed9\u5916\u90e8\u53c2\u4e0e\u8005\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u800c\u4e0d\u91cd\u65b0\u6267\u884c\u3002", "method": "\u901a\u8fc7\u5c06\u975e\u7b97\u672f\u64cd\u4f5c\uff08\u5982\u56fa\u5b9a\u70b9\u77e9\u9635\u4e58\u6cd5\u540e\u7684\u820d\u5165\u548cReLU\uff09\u8f6c\u6362\u4e3a\u7b97\u672f\u6b65\u9aa4\uff0c\u5229\u7528\u548c\u68c0\u67e5\u534f\u8bae\u548c\u8303\u56f4\u8bc1\u660e\u5b9e\u73b0\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u8fd8\u964d\u4f4e\u4e86\u9a8c\u8bc1\u7684\u8ba1\u7b97\u6210\u672c\u3001\u5916\u5305\u65b9\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "Range-Arithmetic\u6846\u67b6\u4e3a\u9ad8\u6548\u4e14\u53ef\u9a8c\u8bc1\u7684DNN\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6709\u9650\u57df\u8bc1\u660e\u7cfb\u7edf\u3002", "relevance": 40.0}}
{"id": "2505.17041", "pdf": "https://arxiv.org/pdf/2505.17041", "abs": "https://arxiv.org/abs/2505.17041", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo"], "title": "Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing", "categories": ["cs.CY", "cs.CL", "cs.HC"], "comment": "31 pages, 16 figures", "summary": "Generative Artificial Intelligence is transforming how English as a foreign\nlanguage students write. Still, little is known about how students manipulate\ntext generated by generative AI during the writing process. This study\ninvestigates how EFL secondary school students integrate and modify\nAI-generated text when completing an expository writing task. The study\nemployed an exploratory mixed-methods design. Screen recordings were collected\nfrom 29 Hong Kong secondary school students who attended an AI-assisted writing\nworkshop and recorded their screens while using generative AI to write an\narticle. Content analysis with hierarchical coding and thematic analysis with a\nmultiple case study approach were adopted to analyze the recordings. 15 types\nof AI-generated text edits across seven categories were identified from the\nrecordings. Notably, AI-initiated edits from iOS and Google Docs emerged as\nunanticipated sources of AI-generated text. A thematic analysis revealed four\npatterns of students' editing behaviors based on planning and drafting\ndirection: planning with top-down drafting and revising; top-down drafting and\nrevising without planning; planning with bottom-up drafting and revising; and\nbottom-up drafting and revising without planning. Network graphs illustrate\ncases of each pattern, demonstrating that students' interactions with\nAI-generated text involve more complex cognitive processes than simple text\ninsertion. The findings challenge assumptions about students' passive,\nsimplistic use of generative AI tools and have implications for developing\nexplicit instructional approaches to teaching AI-generated text editing\nstrategies in the AFL writing pedagogy.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8EFL\u4e2d\u5b66\u751f\u5982\u4f55\u4fee\u6539AI\u751f\u6210\u6587\u672c\uff0c\u53d1\u73b0\u7f16\u8f91\u884c\u4e3a\u590d\u6742\uff0c\u6311\u6218\u4e86\u5b66\u751f\u88ab\u52a8\u4f7f\u7528AI\u7684\u5047\u8bbe\u3002", "motivation": "\u4e86\u89e3EFL\u5b66\u751f\u5982\u4f55\u6574\u5408\u548c\u4fee\u6539AI\u751f\u6210\u6587\u672c\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u5206\u679029\u540d\u5b66\u751f\u7684\u5c4f\u5e55\u5f55\u50cf\uff0c\u4f7f\u7528\u5185\u5bb9\u5206\u6790\u548c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa15\u79cd\u7f16\u8f91\u884c\u4e3a\uff0c\u53d1\u73b0\u56db\u79cd\u7f16\u8f91\u6a21\u5f0f\uff0c\u663e\u793a\u5b66\u751f\u4e0eAI\u6587\u672c\u7684\u4e92\u52a8\u590d\u6742\u3002", "conclusion": "\u5b66\u751f\u4f7f\u7528AI\u7684\u884c\u4e3a\u6bd4\u9884\u60f3\u7684\u66f4\u590d\u6742\uff0c\u9700\u5f00\u53d1\u660e\u786e\u7684\u6559\u5b66\u7b56\u7565\u3002", "relevance": 30.0}}
{"id": "2505.17629", "pdf": "https://arxiv.org/pdf/2505.17629", "abs": "https://arxiv.org/abs/2505.17629", "authors": ["Yuheng Lu", "Qian Yu", "Hongru Wang", "Zeming Liu", "Wei Su", "Yanping Liu", "Yuhang Guo", "Maocheng Liang", "Yunhong Wang", "Haifeng Wang"], "title": "TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Graphical User Interface (GUI) agents, which autonomously operate on digital\ninterfaces through natural language instructions, hold transformative potential\nfor accessibility, automation, and user experience. A critical aspect of their\nfunctionality is grounding - the ability to map linguistic intents to visual\nand structural interface elements. However, existing GUI agents often struggle\nto adapt to the dynamic and interconnected nature of real-world digital\nenvironments, where tasks frequently span multiple platforms and applications\nwhile also being impacted by version updates. To address this, we introduce\nTransBench, the first benchmark designed to systematically evaluate and enhance\nthe transferability of GUI agents across three key dimensions: cross-version\ntransferability (adapting to version updates), cross-platform transferability\n(generalizing across platforms like iOS, Android, and Web), and\ncross-application transferability (handling tasks spanning functionally\ndistinct apps). TransBench includes 15 app categories with diverse\nfunctionalities, capturing essential pages across versions and platforms to\nenable robust evaluation. Our experiments demonstrate significant improvements\nin grounding accuracy, showcasing the practical utility of GUI agents in\ndynamic, real-world environments. Our code and data will be publicly available\nat Github.", "AI": {"tldr": "TransBench\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30GUI\u4ee3\u7406\u5728\u8de8\u7248\u672c\u3001\u8de8\u5e73\u53f0\u548c\u8de8\u5e94\u7528\u7a0b\u5e8f\u7684\u8f6c\u79fb\u80fd\u529b\uff0c\u63d0\u5347\u5176\u5728\u52a8\u6001\u6570\u5b57\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u3001\u4e92\u8054\u7684\u6570\u5b57\u73af\u5883\uff0cTransBench\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165TransBench\u57fa\u51c6\uff0c\u5305\u542b15\u4e2a\u5e94\u7528\u7c7b\u522b\uff0c\u8bc4\u4f30\u8de8\u7248\u672c\u3001\u8de8\u5e73\u53f0\u548c\u8de8\u5e94\u7528\u7a0b\u5e8f\u7684\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGUI\u4ee3\u7406\u7684grounding\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "TransBench\u4e3aGUI\u4ee3\u7406\u5728\u771f\u5b9e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "relevance": 40.0}}
{"id": "2505.17631", "pdf": "https://arxiv.org/pdf/2505.17631", "abs": "https://arxiv.org/abs/2505.17631", "authors": ["Jiahui Gong", "Jingtao Ding", "Fanjin Meng", "Chen Yang", "Hong Chen", "Zuojian Wang", "Haisheng Lu", "Yong Li"], "title": "BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling", "categories": ["cs.IR", "cs.AI"], "comment": "22 pages, 8 figures, 5 tables", "summary": "In recent years, foundational models have revolutionized the fields of\nlanguage and vision, demonstrating remarkable abilities in understanding and\ngenerating complex data; however, similar advances in user behavior modeling\nhave been limited, largely due to the complexity of behavioral data and the\nchallenges involved in capturing intricate temporal and contextual\nrelationships in user activities. To address this, we propose BehaveGPT, a\nfoundational model designed specifically for large-scale user behavior\nprediction. Leveraging transformer-based architecture and a novel pretraining\nparadigm, BehaveGPT is trained on vast user behavior datasets, allowing it to\nlearn complex behavior patterns and support a range of downstream tasks,\nincluding next behavior prediction, long-term generation, and cross-domain\nadaptation. Our approach introduces the DRO-based pretraining paradigm tailored\nfor user behavior data, which improves model generalization and transferability\nby equitably modeling both head and tail behaviors. Extensive experiments on\nreal-world datasets demonstrate that BehaveGPT outperforms state-of-the-art\nbaselines, achieving more than a 10% improvement in macro and weighted recall,\nshowcasing its ability to effectively capture and predict user behavior.\nFurthermore, we measure the scaling law in the user behavior domain for the\nfirst time on the Honor dataset, providing insights into how model performance\nscales with increased data and parameter sizes.", "AI": {"tldr": "BehaveGPT\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u5927\u89c4\u6a21\u7528\u6237\u884c\u4e3a\u9884\u6d4b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\uff08DRO-based\uff09\u548c\u5927\u91cf\u884c\u4e3a\u6570\u636e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u7684\u8fdb\u5c55\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u884c\u4e3a\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u6355\u6349\u65f6\u95f4\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\u548cDRO-based\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u8bad\u7ec3\u4e8e\u5927\u89c4\u6a21\u7528\u6237\u884c\u4e3a\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b8f\u53ec\u56de\u7387\u548c\u52a0\u6743\u53ec\u56de\u7387\u63d0\u5347\u8d85\u8fc710%\uff0c\u5e76\u9996\u6b21\u5728\u7528\u6237\u884c\u4e3a\u9886\u57df\u6d4b\u91cf\u4e86\u6269\u5c55\u89c4\u5f8b\u3002", "conclusion": "BehaveGPT\u80fd\u6709\u6548\u6355\u6349\u548c\u9884\u6d4b\u7528\u6237\u884c\u4e3a\uff0c\u4e3a\u884c\u4e3a\u5efa\u6a21\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u89c1\u89e3\u3002", "relevance": 70.0}}
{"id": "2505.17632", "pdf": "https://arxiv.org/pdf/2505.17632", "abs": "https://arxiv.org/abs/2505.17632", "authors": ["Mohammad Kasra Habib", "Daniel Graziotin", "Stefan Wagner"], "title": "ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Requirements elicitation and specification remains a labor-intensive, manual\nprocess prone to inconsistencies and gaps, presenting a significant challenge\nin modern software engineering. Emerging studies underscore the potential of\nemploying large language models (LLMs) for automated requirements generation to\nsupport requirements elicitation and specification; however, it remains unclear\nhow to implement this effectively. In this work, we introduce ReqBrain, an\nAl-assisted tool that employs a fine-tuned LLM to generate authentic and\nadequate software requirements. Software engineers can engage with ReqBrain\nthrough chat-based sessions to automatically generate software requirements and\ncategorize them by type. We curated a high-quality dataset of ISO\n29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine\nthe most effective base model for ReqBrain. The top-performing model,\nZephyr-7b-beta, achieved 89.30\\% Fl using the BERT score and a FRUGAL score of\n91.20 in generating authentic and adequate requirements. Human evaluations\nfurther confirmed ReqBrain's effectiveness in generating requirements. Our\nfindings suggest that generative Al, when fine-tuned, has the potential to\nimprove requirements elicitation and specification, paving the way for future\nextensions into areas such as defect identification, test case generation, and\nagile user story creation.", "AI": {"tldr": "ReqBrain\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fae\u8c03LLM\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u9700\u6c42\uff0c\u901a\u8fc7\u804a\u5929\u4f1a\u8bdd\u652f\u6301\u5de5\u7a0b\u5e08\u751f\u6210\u548c\u5206\u7c7b\u9700\u6c42\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u9700\u6c42\u83b7\u53d6\u548c\u89c4\u8303\u4ecd\u4f9d\u8d56\u4eba\u5de5\uff0c\u6613\u51fa\u73b0\u4e0d\u4e00\u81f4\u548c\u9057\u6f0f\u3002LLM\u6709\u6f5c\u529b\u81ea\u52a8\u5316\u6b64\u8fc7\u7a0b\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5b9e\u73b0\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u76847B\u53c2\u6570LLM\uff08\u5982Zephyr-7b-beta\uff09\uff0c\u57fa\u4e8eISO 29148\u6570\u636e\u96c6\u751f\u6210\u9700\u6c42\uff0c\u5e76\u901a\u8fc7BERT\u548cFRUGAL\u8bc4\u5206\u8bc4\u4f30\u3002", "result": "Zephyr-7b-beta\u5728BERT\u548cFRUGAL\u8bc4\u5206\u4e2d\u5206\u522b\u8fbe\u523089.30%\u548c91.20\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u5fae\u8c03\u7684\u751f\u6210\u5f0fAI\u53ef\u6539\u8fdb\u9700\u6c42\u83b7\u53d6\u548c\u89c4\u8303\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u7f3a\u9677\u8bc6\u522b\u3001\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u7b49\u9886\u57df\u3002", "relevance": 60.0}}
{"id": "2505.17088", "pdf": "https://arxiv.org/pdf/2505.17088", "abs": "https://arxiv.org/abs/2505.17088", "authors": ["Ahmed Adel Attia", "Dorottya Demszky", "Jing Liu", "Carol Espy-Wilson"], "title": "From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Recent progress in speech recognition has relied on models trained on vast\namounts of labeled data. However, classroom Automatic Speech Recognition (ASR)\nfaces the real-world challenge of abundant weak transcripts paired with only a\nsmall amount of accurate, gold-standard data. In such low-resource settings,\nhigh transcription costs make re-transcription impractical. To address this, we\nask: what is the best approach when abundant inexpensive weak transcripts\ncoexist with limited gold-standard data, as is the case for classroom speech\ndata? We propose Weakly Supervised Pretraining (WSP), a two-step process where\nmodels are first pretrained on weak transcripts in a supervised manner, and\nthen fine-tuned on accurate data. Our results, based on both synthetic and real\nweak transcripts, show that WSP outperforms alternative methods, establishing\nit as an effective training methodology for low-resource ASR in real-world\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\uff08WSP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u8d44\u6e90\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4e2d\u5f31\u8f6c\u5f55\u672c\u4e0e\u5c11\u91cf\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u5171\u5b58\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8bfe\u5802ASR\u4e2d\u5f31\u8f6c\u5f55\u672c\u4e0e\u5c11\u91cf\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u5171\u5b58\u7684\u6311\u6218\uff0c\u907f\u514d\u9ad8\u6210\u672c\u7684\u91cd\u65b0\u8f6c\u5f55\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u5728\u5f31\u8f6c\u5f55\u672c\u4e0a\u8fdb\u884c\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u4e0a\u5fae\u8c03\u3002", "result": "WSP\u5728\u5408\u6210\u548c\u771f\u5b9e\u5f31\u8f6c\u5f55\u672c\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "WSP\u662f\u4f4e\u8d44\u6e90ASR\u4e2d\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.17093", "pdf": "https://arxiv.org/pdf/2505.17093", "abs": "https://arxiv.org/abs/2505.17093", "authors": ["Yejin Lee", "Jaehoon Kang", "Kyuhong Shim"], "title": "Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "In this paper, we propose a novel framework to control voice style in\nprompt-based, controllable text-to-speech systems by leveraging textual\npersonas as voice style prompts. We present two persona rewriting strategies to\ntransform generic persona descriptions into speech-oriented prompts, enabling\nfine-grained manipulation of prosodic attributes such as pitch, emotion, and\nspeaking rate. Experimental results demonstrate that our methods enhance the\nnaturalness, clarity, and consistency of synthesized speech. Finally, we\nanalyze implicit social biases introduced by LLM-based rewriting, with a focus\non gender. We underscore voice style as a crucial factor for persona-driven AI\ndialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u89d2\u8272\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236\u6587\u672c\u5230\u8bed\u97f3\u7cfb\u7edf\u4e2d\u7684\u8bed\u97f3\u98ce\u683c\uff0c\u5e76\u901a\u8fc7\u89d2\u8272\u91cd\u5199\u7b56\u7565\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u63d0\u5347\u4e86\u8bed\u97f3\u5408\u6210\u7684\u81ea\u7136\u6027\u548c\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5206\u6790\u4e86LLM\u91cd\u5199\u5f15\u5165\u7684\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6587\u672c\u89d2\u8272\u63a7\u5236\u8bed\u97f3\u98ce\u683c\uff0c\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u5173\u6ce8\u793e\u4f1a\u504f\u89c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u89d2\u8272\u91cd\u5199\u7b56\u7565\uff0c\u5c06\u901a\u7528\u89d2\u8272\u63cf\u8ff0\u8f6c\u5316\u4e3a\u8bed\u97f3\u5bfc\u5411\u63d0\u793a\uff0c\u4ee5\u64cd\u7eb5\u97f5\u5f8b\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u63d0\u5347\u4e86\u8bed\u97f3\u5408\u6210\u7684\u81ea\u7136\u6027\u3001\u6e05\u6670\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86LLM\u91cd\u5199\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u8bed\u97f3\u98ce\u683c\u662f\u89d2\u8272\u9a71\u52a8AI\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u5173\u6ce8\u5176\u793e\u4f1a\u5f71\u54cd\u3002", "relevance": 40.0}}
{"id": "2505.17109", "pdf": "https://arxiv.org/pdf/2505.17109", "abs": "https://arxiv.org/abs/2505.17109", "authors": ["Alfonso de Gregorio"], "title": "Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities", "categories": ["cs.CR", "cs.CL"], "comment": "8 pages, no figures", "summary": "Open-weight general-purpose AI (GPAI) models offer significant benefits but\nalso introduce substantial cybersecurity risks, as demonstrated by the\noffensive capabilities of models like DeepSeek-R1 in evaluations such as\nMITRE's OCCULT. These publicly available models empower a wider range of actors\nto automate and scale cyberattacks, challenging traditional defence paradigms\nand regulatory approaches. This paper analyzes the specific threats --\nincluding accelerated malware development and enhanced social engineering --\nmagnified by open-weight AI release. We critically assess current regulations,\nnotably the EU AI Act and the GPAI Code of Practice, identifying significant\ngaps stemming from the loss of control inherent in open distribution, which\nrenders many standard security mitigations ineffective. We propose a path\nforward focusing on evaluating and controlling specific high-risk capabilities\nrather than entire models, advocating for pragmatic policy interpretations for\nopen-weight systems, promoting defensive AI innovation, and fostering\ninternational collaboration on standards and cyber threat intelligence (CTI)\nsharing to ensure security without unduly stifling open technological progress.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f00\u6e90\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08GPAI\uff09\u6a21\u578b\u5e26\u6765\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6cd5\u89c4\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u9ad8\u98ce\u9669\u80fd\u529b\u7684\u8bc4\u4f30\u4e0e\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u5f00\u6e90GPAI\u6a21\u578b\uff08\u5982DeepSeek-R1\uff09\u5728\u5e26\u6765\u4fbf\u5229\u7684\u540c\u65f6\uff0c\u4e5f\u52a0\u5267\u4e86\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff08\u5982\u81ea\u52a8\u5316\u653b\u51fb\u3001\u6076\u610f\u8f6f\u4ef6\u5f00\u53d1\uff09\uff0c\u73b0\u6709\u6cd5\u89c4\uff08\u5982\u6b27\u76dfAI\u6cd5\u6848\uff09\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f00\u6e90GPAI\u7684\u5a01\u80c1\uff08\u5982\u52a0\u901f\u6076\u610f\u8f6f\u4ef6\u5f00\u53d1\u548c\u793e\u4ea4\u5de5\u7a0b\u653b\u51fb\uff09\uff0c\u8bc4\u4f30\u73b0\u6709\u6cd5\u89c4\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u9ad8\u98ce\u9669\u80fd\u529b\u7684\u63a7\u5236\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u6cd5\u89c4\u5b58\u5728\u91cd\u5927\u6f0f\u6d1e\uff0c\u63d0\u51fa\u901a\u8fc7\u8bc4\u4f30\u9ad8\u98ce\u9669\u80fd\u529b\u3001\u63a8\u52a8\u9632\u5fa1\u6027AI\u521b\u65b0\u548c\u56fd\u9645\u5408\u4f5c\u6765\u5e73\u8861\u5b89\u5168\u4e0e\u6280\u672f\u8fdb\u6b65\u3002", "conclusion": "\u5efa\u8bae\u805a\u7126\u9ad8\u98ce\u9669\u80fd\u529b\u800c\u975e\u6574\u4e2a\u6a21\u578b\uff0c\u63a8\u52a8\u653f\u7b56\u8c03\u6574\u548c\u56fd\u9645\u534f\u4f5c\uff0c\u4ee5\u5728\u4e0d\u963b\u788d\u6280\u672f\u8fdb\u6b65\u7684\u524d\u63d0\u4e0b\u4fdd\u969c\u5b89\u5168\u3002", "relevance": 40.0}}
{"id": "2505.17077", "pdf": "https://arxiv.org/pdf/2505.17077", "abs": "https://arxiv.org/abs/2505.17077", "authors": ["Upasana Sarmah", "Parthajit Borah", "D. K. Bhattacharyya"], "title": "Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Applications over the Web primarily rely on the HTTP protocol to transmit web\npages to and from systems. There are a variety of application layer protocols,\nbut among all, HTTP is the most targeted because of its versatility and ease of\nintegration with online services. The attackers leverage the fact that by\ndefault no detection system blocks any HTTP traffic. Thus, by exploiting such\ncharacteristics of the protocol, attacks are launched against web applications.\nHTTP flooding attacks are one such attack in the application layer of the OSI\nmodel. In this paper, a method for the detection of such an attack is proposed.\nThe heart of the detection method is an incremental feature subset selection\nmethod based on mutual information and correlation. INFS-MICC helps in\nidentifying a subset of highly relevant and independent feature subset so as to\ndetect HTTP Flooding attacks with best possible classification performance in\nnear-real time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u548c\u76f8\u5173\u6027\u7684\u589e\u91cf\u7279\u5f81\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff08INFS-MICC\uff09\uff0c\u7528\u4e8e\u68c0\u6d4bHTTP\u6d2a\u6c34\u653b\u51fb\u3002", "motivation": "HTTP\u534f\u8bae\u56e0\u5176\u901a\u7528\u6027\u548c\u6613\u96c6\u6210\u6027\u6210\u4e3a\u653b\u51fb\u76ee\u6807\uff0c\u800c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u6ca1\u6709\u68c0\u6d4b\u7cfb\u7edf\u4f1a\u963b\u6b62HTTP\u6d41\u91cf\uff0c\u5bfc\u81f4\u653b\u51fb\u9891\u53d1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3HTTP\u6d2a\u6c34\u653b\u51fb\u7684\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e92\u4fe1\u606f\u548c\u76f8\u5173\u6027\u7684\u589e\u91cf\u7279\u5f81\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff08INFS-MICC\uff09\uff0c\u4ee5\u8bc6\u522b\u9ad8\u5ea6\u76f8\u5173\u4e14\u72ec\u7acb\u7684\u7279\u5f81\u5b50\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u4f73\u5206\u7c7b\u6027\u80fd\u5728\u8fd1\u5b9e\u65f6\u6761\u4ef6\u4e0b\u68c0\u6d4bHTTP\u6d2a\u6c34\u653b\u51fb\u3002", "conclusion": "INFS-MICC\u4e3aHTTP\u6d2a\u6c34\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u68c0\u6d4b\u624b\u6bb5\u3002", "relevance": 10.0}}
{"id": "2505.17092", "pdf": "https://arxiv.org/pdf/2505.17092", "abs": "https://arxiv.org/abs/2505.17092", "authors": ["Matthew Jagielski", "Daniel Escudero", "Rahul Rachuri", "Peter Scholl"], "title": "Covert Attacks on Machine Learning Training in Passively Secure MPC", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Secure multiparty computation (MPC) allows data owners to train machine\nlearning models on combined data while keeping the underlying training data\nprivate. The MPC threat model either considers an adversary who passively\ncorrupts some parties without affecting their overall behavior, or an adversary\nwho actively modifies the behavior of corrupt parties. It has been argued that\nin some settings, active security is not a major concern, partly because of the\npotential risk of reputation loss if a party is detected cheating.\n  In this work we show explicit, simple, and effective attacks that an active\nadversary can run on existing passively secure MPC training protocols, while\nkeeping essentially zero risk of the attack being detected. The attacks we show\ncan compromise both the integrity and privacy of the model, including attacks\nreconstructing exact training data. Our results challenge the belief that a\nthreat model that does not include malicious behavior by the involved parties\nmay be reasonable in the context of PPML, motivating the use of actively secure\nprotocols for training.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5728\u88ab\u52a8\u5b89\u5168\u7684\u591a\u65b9\u8ba1\u7b97\uff08MPC\uff09\u8bad\u7ec3\u534f\u8bae\u4e2d\uff0c\u4e3b\u52a8\u653b\u51fb\u8005\u53ef\u4ee5\u5b9e\u65bd\u7b80\u5355\u6709\u6548\u7684\u653b\u51fb\uff0c\u5a01\u80c1\u6a21\u578b\u5b8c\u6574\u6027\u548c\u9690\u79c1\uff0c\u6311\u6218\u4e86PPML\u4e2d\u5ffd\u7565\u6076\u610f\u884c\u4e3a\u7684\u5408\u7406\u6027\u3002", "motivation": "\u63a2\u8ba8\u88ab\u52a8\u5b89\u5168MPC\u8bad\u7ec3\u534f\u8bae\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u63ed\u793a\u4e3b\u52a8\u653b\u51fb\u8005\u53ef\u80fd\u5e26\u6765\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u5e76\u6f14\u793a\u4e86\u9488\u5bf9\u88ab\u52a8\u5b89\u5168MPC\u8bad\u7ec3\u534f\u8bae\u7684\u5177\u4f53\u653b\u51fb\u65b9\u6cd5\uff0c\u5305\u62ec\u6a21\u578b\u5b8c\u6574\u6027\u548c\u9690\u79c1\u7834\u574f\u3002", "result": "\u653b\u51fb\u6210\u529f\u7834\u574f\u4e86\u6a21\u578b\u7684\u5b8c\u6574\u6027\u548c\u9690\u79c1\uff0c\u751a\u81f3\u80fd\u91cd\u6784\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u51e0\u4e4e\u65e0\u6cd5\u88ab\u68c0\u6d4b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728PPML\u4e2d\u4f7f\u7528\u4e3b\u52a8\u5b89\u5168\u534f\u8bae\u7684\u5fc5\u8981\u6027\uff0c\u6311\u6218\u4e86\u5ffd\u7565\u6076\u610f\u884c\u4e3a\u7684\u5a01\u80c1\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.17094", "pdf": "https://arxiv.org/pdf/2505.17094", "abs": "https://arxiv.org/abs/2505.17094", "authors": ["Hemanth Ravipati"], "title": "Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Neuromorphic computing, inspired by the human brain's neural architecture, is\nrevolutionizing artificial intelligence and edge computing with its low-power,\nadaptive, and event-driven designs. However, these unique characteristics\nintroduce novel cybersecurity risks. This paper proposes Neuromorphic Mimicry\nAttacks (NMAs), a groundbreaking class of threats that exploit the\nprobabilistic and non-deterministic nature of neuromorphic chips to execute\ncovert intrusions. By mimicking legitimate neural activity through techniques\nsuch as synaptic weight tampering and sensory input poisoning, NMAs evade\ntraditional intrusion detection systems, posing risks to applications such as\nautonomous vehicles, smart medical implants, and IoT networks. This research\ndevelops a theoretical framework for NMAs, evaluates their impact using a\nsimulated neuromorphic chip dataset, and proposes countermeasures, including\nneural-specific anomaly detection and secure synaptic learning protocols. The\nfindings underscore the critical need for tailored cybersecurity measures to\nprotect brain-inspired computing, offering a pioneering exploration of this\nemerging threat landscape.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u2014\u2014\u795e\u7ecf\u5f62\u6001\u6a21\u4eff\u653b\u51fb\uff08NMAs\uff09\uff0c\u5229\u7528\u795e\u7ecf\u5f62\u6001\u82af\u7247\u7684\u6982\u7387\u6027\u548c\u975e\u786e\u5b9a\u6027\u7279\u6027\u8fdb\u884c\u9690\u853d\u5165\u4fb5\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u56e0\u5176\u4f4e\u529f\u8017\u548c\u81ea\u9002\u5e94\u7279\u6027\u5728AI\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u72ec\u7279\u7684\u67b6\u6784\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u82af\u7247\u6570\u636e\u96c6\uff0c\u7814\u7a76NMAs\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u795e\u7ecf\u7279\u5f02\u6027\u5f02\u5e38\u68c0\u6d4b\u548c\u5b89\u5168\u7a81\u89e6\u5b66\u4e60\u534f\u8bae\u7b49\u5bf9\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0NMAs\u80fd\u591f\u89c4\u907f\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u3001\u667a\u80fd\u533b\u7597\u690d\u5165\u548c\u7269\u8054\u7f51\u7f51\u7edc\u7b49\u5e94\u7528\u6784\u6210\u5a01\u80c1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5b9a\u5236\u7f51\u7edc\u5b89\u5168\u63aa\u65bd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u9996\u6b21\u63a2\u7d22\u4e86\u8fd9\u4e00\u65b0\u5174\u5a01\u80c1\u9886\u57df\u3002", "relevance": 30.0}}
{"id": "2505.17717", "pdf": "https://arxiv.org/pdf/2505.17717", "abs": "https://arxiv.org/abs/2505.17717", "authors": ["Akira Tanimoto"], "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u6297\u635f\u5931\u51fd\u6570\u89e3\u51b3\u56e0\u679c\u63a8\u7406\u4e2d\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u6743\u91cd\u4e0d\u7a33\u5b9a\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u548c\u6743\u91cd\u6b63\u5219\u5316\u3002", "motivation": "\u56e0\u679c\u63a8\u7406\u4e2d\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982IPW\uff09\u56e0\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u6781\u7aef\u6743\u91cd\u4e0d\u7a33\u5b9a\u800c\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5904\u7406\u503e\u5411\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u52a0\u6743Rademacher\u590d\u6742\u5ea6\u8fdb\u884c\u6743\u91cd\u6b63\u5219\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u503e\u5411\u6a21\u7cca\u6027\u548c\u7edf\u8ba1\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56e0\u679c\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "relevance": 40.0}}
{"id": "2505.17374", "pdf": "https://arxiv.org/pdf/2505.17374", "abs": "https://arxiv.org/abs/2505.17374", "authors": ["Seon Gyeom Kim", "Jae Young Choi", "Ryan Rossi", "Eunyee Koh", "Tak Yeon Lee"], "title": "Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts", "categories": ["cs.HC", "cs.CL"], "comment": "This paper has been accepted to IEEE PacificVis 2025", "summary": "The field of Multimodal Large Language Models (MLLMs) has made remarkable\nprogress in visual understanding tasks, presenting a vast opportunity to\npredict the perceptual and emotional impact of charts. However, it also raises\nconcerns, as many applications of LLMs are based on overgeneralized assumptions\nfrom a few examples, lacking sufficient validation of their performance and\neffectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising\n36 charts, evaluated by crowdsourced workers for their impact on seven\nexperiential factors. Using the dataset as ground truth, we evaluated\ncapabilities of state-of-the-art MLLMs on two tasks: direct prediction and\npairwise comparison of charts. Our findings imply that MLLMs are not as\nsensitive as human evaluators when assessing individual charts, but are\naccurate and reliable in pairwise comparisons.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Chart-to-Experience\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u8868\u611f\u77e5\u548c\u60c5\u611f\u5f71\u54cd\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0MLLMs\u5728\u76f4\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u4e0d\u5982\u4eba\u7c7b\u654f\u611f\uff0c\u4f46\u5728\u6210\u5bf9\u6bd4\u8f83\u4e2d\u8868\u73b0\u51c6\u786e\u53ef\u9760\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u6027\u80fd\u548c\u6709\u6548\u6027\u7f3a\u4e4f\u5145\u5206\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u57fa\u51c6\u6570\u636e\u96c6\u8bc4\u4f30MLLMs\u5728\u56fe\u8868\u611f\u77e5\u548c\u60c5\u611f\u5f71\u54cd\u9884\u6d4b\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165Chart-to-Experience\u6570\u636e\u96c6\uff08\u5305\u542b36\u4e2a\u56fe\u8868\uff0c\u7531\u4f17\u5305\u5de5\u4eba\u8bc4\u4f30\u5176\u4e03\u4e2a\u4f53\u9a8c\u56e0\u7d20\u7684\u5f71\u54cd\uff09\uff0c\u5e76\u8bc4\u4f30MLLMs\u5728\u76f4\u63a5\u9884\u6d4b\u548c\u6210\u5bf9\u6bd4\u8f83\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "MLLMs\u5728\u76f4\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u4e0d\u5982\u4eba\u7c7b\u654f\u611f\uff0c\u4f46\u5728\u6210\u5bf9\u6bd4\u8f83\u4efb\u52a1\u4e2d\u8868\u73b0\u51c6\u786e\u53ef\u9760\u3002", "conclusion": "MLLMs\u5728\u56fe\u8868\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4f46\u5728\u6210\u5bf9\u6bd4\u8f83\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u6539\u8fdb\u3002", "relevance": 60.0}}
{"id": "2505.17410", "pdf": "https://arxiv.org/pdf/2505.17410", "abs": "https://arxiv.org/abs/2505.17410", "authors": ["Natsuo Yamashita", "Masaaki Yamamoto", "Hiroaki Kokubo", "Yohei Kawaguchi"], "title": "LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Generative error correction (GER) with large language models (LLMs) has\nemerged as an effective post-processing approach to improve automatic speech\nrecognition (ASR) performance. However, it often struggles with rare or\ndomain-specific words due to limited training data. Furthermore, existing\nLLM-based GER approaches primarily rely on textual information, neglecting\nphonetic cues, which leads to over-correction. To address these issues, we\npropose a novel LLM-based GER approach that targets rare words and incorporates\nphonetic information. First, we generate synthetic data to contain rare words\nfor fine-tuning the GER model. Second, we integrate ASR's N-best hypotheses\nalong with phonetic context to mitigate over-correction. Experimental results\nshow that our method not only improves the correction of rare words but also\nreduces the WER and CER across both English and Japanese datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u97f3\u4fe1\u606f\u7684LLM\u751f\u6210\u5f0f\u9519\u8bef\u6821\u6b63\u65b9\u6cd5\uff0c\u9488\u5bf9\u7f55\u89c1\u8bcd\u548c\u8fc7\u6821\u6b63\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u548cN-best\u5047\u8bbe\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u751f\u6210\u5f0f\u9519\u8bef\u6821\u6b63\u65b9\u6cd5\u5728\u7f55\u89c1\u8bcd\u548c\u8bed\u97f3\u4fe1\u606f\u5229\u7528\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "1. \u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u5305\u542b\u7f55\u89c1\u8bcd\uff1b2. \u7ed3\u5408ASR\u7684N-best\u5047\u8bbe\u548c\u8bed\u97f3\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u82f1\u8bed\u548c\u65e5\u8bed\u6570\u636e\u96c6\u4e0a\u964d\u4f4e\u4e86WER\u548cCER\uff0c\u5e76\u63d0\u5347\u7f55\u89c1\u8bcd\u6821\u6b63\u6548\u679c\u3002", "conclusion": "\u7ed3\u5408\u8bed\u97f3\u4fe1\u606f\u7684LLM\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347ASR\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.17791", "pdf": "https://arxiv.org/pdf/2505.17791", "abs": "https://arxiv.org/abs/2505.17791", "authors": ["Luca Fehlings", "Bojian Zhang", "Paolo Gibertini", "Martin A. Nicholson", "Erika Covi", "Fernando M. Quintana"], "title": "Bruno: Backpropagation Running Undersampled for Novel device Optimization", "categories": ["cs.NE", "cs.AI"], "comment": "12 pages, 3 pages supplementary material", "summary": "Recent efforts to improve the efficiency of neuromorphic and machine learning\nsystems have focused on the development of application-specific integrated\ncircuits (ASICs), which provide hardware specialized for the deployment of\nneural networks, leading to potential gains in efficiency and performance.\nThese systems typically feature an architecture that goes beyond the von\nNeumann architecture employed in general-purpose hardware such as GPUs. Neural\nnetworks developed for this specialised hardware then need to take into account\nthe specifics of the hardware platform, which requires novel training\nalgorithms and accurate models of the hardware, since they cannot be abstracted\nas a general-purpose computing platform. In this work, we present a bottom-up\napproach to train neural networks for hardware based on spiking neurons and\nsynapses built on ferroelectric capacitor (FeCap) and Resistive switching\nnon-volatile devices (RRAM) respectively. In contrast to the more common\napproach of designing hardware to fit existing abstract neuron or synapse\nmodels, this approach starts with compact models of the physical device to\nmodel the computational primitive of the neurons. Based on these models, a\ntraining algorithm is developed that can reliably backpropagate through these\nphysical models, even when applying common hardware limitations, such as\nstochasticity, variability, and low bit precision. The training algorithm is\nthen tested on a spatio-temporal dataset with a network composed of quantized\nsynapses based on RRAM and ferroelectric leaky integrate-and-fire (FeLIF)\nneurons. The performance of the network is compared with different networks\ncomposed of LIF neurons. The results of the experiments show the potential\nadvantage of using BRUNO to train networks with FeLIF neurons, by achieving a\nreduction in both time and memory for detecting spatio-temporal patterns with\nquantized synapses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u8bad\u7ec3\u57fa\u4e8e\u94c1\u7535\u7535\u5bb9\u5668\uff08FeCap\uff09\u548c\u7535\u963b\u5f00\u5173\u975e\u6613\u5931\u6027\u5668\u4ef6\uff08RRAM\uff09\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u7269\u7406\u8bbe\u5907\u6a21\u578b\u5f00\u53d1\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5e76\u5728\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u795e\u7ecf\u5f62\u6001\u548c\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u901a\u8fc7\u4e13\u7528\u786c\u4ef6\uff08\u5982ASICs\uff09\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u90e8\u7f72\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u786c\u4ef6\u7279\u6027\u8bbe\u8ba1\u65b0\u7684\u8bad\u7ec3\u7b97\u6cd5\u3002", "method": "\u57fa\u4e8e\u7269\u7406\u8bbe\u5907\u6a21\u578b\u5f00\u53d1\u8bad\u7ec3\u7b97\u6cd5\uff0c\u652f\u6301\u53cd\u5411\u4f20\u64ad\uff0c\u5e76\u8003\u8651\u786c\u4ef6\u9650\u5236\uff08\u5982\u968f\u673a\u6027\u3001\u53d8\u5f02\u6027\u548c\u4f4e\u6bd4\u7279\u7cbe\u5ea6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528FeLIF\u795e\u7ecf\u5143\u548cRRAM\u91cf\u5316\u7a81\u89e6\u7684\u7f51\u7edc\u5728\u65f6\u7a7a\u6a21\u5f0f\u68c0\u6d4b\u4e2d\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "BRUNO\u8bad\u7ec3\u65b9\u6cd5\u5728\u4e13\u7528\u786c\u4ef6\u4e0a\u5177\u6709\u6f5c\u5728\u4f18\u52bf\uff0c\u4e3a\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "relevance": 40.0}}
{"id": "2505.17417", "pdf": "https://arxiv.org/pdf/2505.17417", "abs": "https://arxiv.org/abs/2505.17417", "authors": ["Alan Dao", "Dinh Bach Vu", "Huy Hoang Ha", "Tuan Le Duc Anh", "Shreyas Gopal", "Yue Heng Yeo", "Warren Keng Hoong Low", "Eng Siong Chng", "Jia Qi Yip"], "title": "Speechless: Speech Instruction Training Without Speech for Low Resource Languages", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "This paper was accepted by INTERSPEECH 2025", "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed5\u8fc7TTS\u76f4\u63a5\u5728\u8bed\u4e49\u8868\u793a\u5c42\u9762\u751f\u6210\u8bed\u97f3\u6307\u4ee4\u6570\u636e\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u52a9\u624b\u8bad\u7ec3\u3002", "motivation": "\u8bed\u97f3\u52a9\u624b\u9700\u8981\u5927\u91cf\u8bed\u97f3\u6307\u4ee4\u6570\u636e\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684TTS\u6a21\u578b\uff0c\u5bfc\u81f4\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u5728\u8bed\u4e49\u8868\u793a\u5c42\u9762\u505c\u6b62\u5408\u6210\uff0c\u5c06\u5176\u4e0e\u9884\u8bad\u7ec3\u7684Whisper\u7f16\u7801\u5668\u5bf9\u9f50\uff0c\u4f7fLLM\u80fd\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u8bed\u97f3\u6307\u4ee4\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6784\u5efa\u8bed\u97f3\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u52a9\u624b\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 75.0}}
{"id": "2505.17203", "pdf": "https://arxiv.org/pdf/2505.17203", "abs": "https://arxiv.org/abs/2505.17203", "authors": ["Yi Zhang", "Elynn Chen", "Yujun Yan"], "title": "Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift", "categories": ["stat.ME", "cs.LG", "stat.AP"], "comment": null, "summary": "We study contextual dynamic pricing when a target market can leverage K\nauxiliary markets -- offline logs or concurrent streams -- whose mean utilities\ndiffer by a structured preference shift. We propose Cross-Market Transfer\nDynamic Pricing (CM-TDP), the first algorithm that provably handles such\nmodel-shift transfer and delivers minimax-optimal regret for both linear and\nnon-parametric utility models.\n  For linear utilities of dimension d, where the difference between source- and\ntarget-task coefficients is $s_{0}$-sparse, CM-TDP attains regret\n$\\tilde{O}((d*K^{-1}+s_{0})\\log T)$. For nonlinear demand residing in a\nreproducing kernel Hilbert space with effective dimension $\\alpha$, complexity\n$\\beta$ and task-similarity parameter $H$, the regret becomes\n$\\tilde{O}\\!(K^{-2\\alpha\\beta/(2\\alpha\\beta+1)}T^{1/(2\\alpha\\beta+1)} +\nH^{2/(2\\alpha+1)}T^{1/(2\\alpha+1)})$, matching information-theoretic lower\nbounds up to logarithmic factors. The RKHS bound is the first of its kind for\ntransfer pricing and is of independent interest.\n  Extensive simulations show up to 50% lower cumulative regret and 5 times\nfaster learning relative to single-market pricing baselines. By bridging\ntransfer learning, robust aggregation, and revenue optimization, CM-TDP moves\ntoward pricing systems that transfer faster, price smarter.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.17204", "pdf": "https://arxiv.org/pdf/2505.17204", "abs": "https://arxiv.org/abs/2505.17204", "authors": ["Pilhwa Lee", "Jayshawn Cooper"], "title": "Liouville PDE-based sliced-Wasserstein flow for fair regression", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO", "stat.TH"], "comment": "11 pages, 3 figures", "summary": "The sliced Wasserstein flow (SWF), a nonparametric and implicit generative\ngradient flow, is applied to fair regression. We have improved the SWF in a few\naspects. First, the stochastic diffusive term from the Fokker-Planck\nequation-based Monte Carlo is transformed to Liouville partial differential\nequation (PDE)-based transport with density estimation, however, without the\ndiffusive term. Now, the computation of the Wasserstein barycenter is\napproximated by the SWF barycenter with the prescription of Kantorovich\npotentials for the induced gradient flow to generate its samples. These two\nefforts improve the convergence in training and testing SWF and SWF barycenters\nwith reduced variance. Applying the generative SWF barycenter for fair\nregression demonstrates competent profiles in the accuracy-fairness Pareto\ncurves.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5207\u7247Wasserstein\u6d41\uff08SWF\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u516c\u5e73\u56de\u5f52\uff0c\u901a\u8fc7\u4f18\u5316\u68af\u5ea6\u6d41\u548c\u51cf\u5c11\u65b9\u5dee\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u8fdbSWF\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u516c\u5e73\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6536\u655b\u6027\u548c\u65b9\u5dee\u51cf\u5c11\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u5c06Fokker-Planck\u65b9\u7a0b\u7684\u968f\u673a\u6269\u6563\u9879\u8f6c\u6362\u4e3a\u57fa\u4e8eLiouville PDE\u7684\u4f20\u8f93\uff0c\u5e76\u7ed3\u5408\u5bc6\u5ea6\u4f30\u8ba1\uff0c\u6539\u8fdb\u4e86SWF\u3002\u540c\u65f6\uff0c\u5229\u7528Kantorovich\u52bf\u80fd\u751f\u6210\u68af\u5ea6\u6d41\u6837\u672c\uff0c\u8fd1\u4f3c\u8ba1\u7b97Wasserstein\u91cd\u5fc3\u3002", "result": "\u6539\u8fdb\u540e\u7684SWF\u5728\u516c\u5e73\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u7684Pareto\u66f2\u7ebf\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u6539\u8fdb\u7684SWF\u65b9\u6cd5\u5728\u516c\u5e73\u56de\u5f52\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u63d0\u5347\u6536\u655b\u6027\u548c\u51cf\u5c11\u65b9\u5dee\u65b9\u9762\u3002", "relevance": 40.0}}
{"id": "2505.17207", "pdf": "https://arxiv.org/pdf/2505.17207", "abs": "https://arxiv.org/abs/2505.17207", "authors": ["Adeep Hande", "Kishorekumar Sundararajan", "Sardar Hamidian", "Ferhan Ture"], "title": "Content Moderation in TV Search: Balancing Policy Compliance, Relevance, and User Experience", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted at SIGIR 2025 Industry Track. 5 pages, 1 figure, 2 tables.\n  DOI: 10.1145/3726302.3731962", "summary": "Millions of people rely on search functionality to find and explore content\non entertainment platforms. Modern search systems use a combination of\ncandidate generation and ranking approaches, with advanced methods leveraging\ndeep learning and LLM-based techniques to retrieve, generate, and categorize\nsearch results. Despite these advancements, search algorithms can still surface\ninappropriate or irrelevant content due to factors like model unpredictability,\nmetadata errors, or overlooked design flaws. Such issues can misalign with\nproduct goals and user expectations, potentially harming user trust and\nbusiness outcomes. In this work, we introduce an additional monitoring layer\nusing Large Language Models (LLMs) to enhance content moderation. This\nadditional layer flags content if the user did not intend to search for it.\nThis approach serves as a baseline for product quality assurance, with\ncollected feedback used to refine the initial retrieval mechanisms of the\nsearch model, ensuring a safer and more reliable user experience.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u76d1\u63a7\u5c42\uff0c\u7528\u4e8e\u589e\u5f3a\u5185\u5bb9\u5ba1\u6838\uff0c\u63d0\u5347\u641c\u7d22\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u4ee3\u641c\u7d22\u7cfb\u7edf\u53ef\u80fd\u56e0\u6a21\u578b\u4e0d\u53ef\u9884\u6d4b\u6027\u6216\u8bbe\u8ba1\u7f3a\u9677\u800c\u5c55\u793a\u4e0d\u9002\u5f53\u5185\u5bb9\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002", "method": "\u5f15\u5165LLM\u4f5c\u4e3a\u989d\u5916\u76d1\u63a7\u5c42\uff0c\u6807\u8bb0\u7528\u6237\u65e0\u610f\u641c\u7d22\u7684\u5185\u5bb9\uff0c\u5e76\u53cd\u9988\u4ee5\u4f18\u5316\u521d\u59cb\u68c0\u7d22\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u641c\u7d22\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "LLM\u76d1\u63a7\u5c42\u53ef\u4f5c\u4e3a\u4ea7\u54c1\u8d28\u91cf\u4fdd\u969c\u7684\u57fa\u7ebf\uff0c\u4f18\u5316\u641c\u7d22\u4f53\u9a8c\u3002", "relevance": 60.0}}
{"id": "2505.17834", "pdf": "https://arxiv.org/pdf/2505.17834", "abs": "https://arxiv.org/abs/2505.17834", "authors": ["Shy-el Cohen", "Yoni Choukroun", "Eliya Nachmani"], "title": "Hybrid Mamba-Transformer Decoder for Error-Correcting Codes", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "comment": null, "summary": "We introduce a novel deep learning method for decoding error correction codes\nbased on the Mamba architecture, enhanced with Transformer layers. Our approach\nproposes a hybrid decoder that leverages Mamba's efficient sequential modeling\nwhile maintaining the global context capabilities of Transformers. To further\nimprove performance, we design a novel layer-wise masking strategy applied to\neach Mamba layer, allowing selective attention to relevant code features at\ndifferent depths. Additionally, we introduce a progressive layer-wise loss,\nsupervising the network at intermediate stages and promoting robust feature\nextraction throughout the decoding process. Comprehensive experiments across a\nrange of linear codes demonstrate that our method significantly outperforms\nTransformer-only decoders and standard Mamba models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u548cTransformer\u5c42\u7684\u6df7\u5408\u89e3\u7801\u5668\uff0c\u7528\u4e8e\u7ea0\u9519\u7801\u7684\u89e3\u7801\uff0c\u901a\u8fc7\u5c42\u95f4\u63a9\u7801\u7b56\u7565\u548c\u6e10\u8fdb\u5f0f\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7ed3\u5408Mamba\u7684\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548cTransformer\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u7ea0\u9519\u7801\u89e3\u7801\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u89e3\u7801\u5668\uff0c\u7ed3\u5408Mamba\u548cTransformer\u5c42\uff0c\u5e76\u5f15\u5165\u5c42\u95f4\u63a9\u7801\u7b56\u7565\u548c\u6e10\u8fdb\u5f0f\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u79cd\u7ebf\u6027\u7801\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u7eafTransformer\u89e3\u7801\u5668\u548c\u6807\u51c6Mamba\u6a21\u578b\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u548c\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u7ea0\u9519\u7801\u89e3\u7801\u7684\u6027\u80fd\u3002", "relevance": 75.0}}
{"id": "2505.17283", "pdf": "https://arxiv.org/pdf/2505.17283", "abs": "https://arxiv.org/abs/2505.17283", "authors": ["Prateek Jaiswal", "Esmaeil Keyvanshokooh", "Junyu Cao"], "title": "Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.AP"], "comment": null, "summary": "Randomized clinical trials often require large patient cohorts before drawing\ndefinitive conclusions, yet abundant observational data from parallel studies\nremains underutilized due to confounding and hidden biases. To bridge this gap,\nwe propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical\napproach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a\nsparse set of reliable measured covariates and combines them with key hidden\ncovariates to form a reduced context. By initializing Thompson Sampling (LinTS)\npriors with DDL-estimated means and variances on these measured features --\nwhile keeping uninformative priors on hidden features -- DWTS effectively\nharnesses confounded observational data to kick-start adaptive clinical trials.\nEvaluated on both a purely synthetic environment and a virtual environment\ncreated using real cardiovascular risk dataset, DWTS consistently achieves\nlower cumulative regret than standard LinTS, showing how offline causal\ninsights from observational data can improve trial efficiency and support more\npersonalized treatment decisions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Deconfounded Warm-Start Thompson Sampling (DWTS)\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c2\u5bdf\u6570\u636e\u548c\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u901a\u8fc7\u53bb\u504fLASSO\u548cThompson\u91c7\u6837\u63d0\u5347\u8bd5\u9a8c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u968f\u673a\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u89c2\u5bdf\u6570\u636e\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5229\u7528\u53bb\u504f\u65b9\u6cd5\u51cf\u5c11\u6df7\u6742\u504f\u5dee\uff0c\u63d0\u5347\u8bd5\u9a8c\u6548\u7387\u3002", "method": "\u4f7f\u7528Doubly Debiased LASSO (DDL)\u8bc6\u522b\u53ef\u9760\u534f\u53d8\u91cf\uff0c\u7ed3\u5408Thompson Sampling (LinTS)\u521d\u59cb\u5316\u5148\u9a8c\uff0c\u5f62\u6210DWTS\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5fc3\u8840\u7ba1\u98ce\u9669\u6570\u636e\u73af\u5883\u4e2d\uff0cDWTS\u6bd4\u6807\u51c6LinTS\u8868\u73b0\u66f4\u597d\uff0c\u7d2f\u79ef\u9057\u61be\u66f4\u4f4e\u3002", "conclusion": "DWTS\u901a\u8fc7\u7ed3\u5408\u89c2\u5bdf\u6570\u636e\u548c\u56e0\u679c\u63a8\u65ad\uff0c\u63d0\u5347\u4e86\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6548\u7387\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u51b3\u7b56\u3002", "relevance": 30.0}}
{"id": "2505.17841", "pdf": "https://arxiv.org/pdf/2505.17841", "abs": "https://arxiv.org/abs/2505.17841", "authors": ["Wiebke Hutiri", "Mircea Cimpoi", "Morgan Scheuerman", "Victoria Matthews", "Alice Xiang"], "title": "TEDI: Trustworthy and Ethical Dataset Indicators to Analyze and Compare Dataset Documentation", "categories": ["cs.CY", "cs.AI", "eess.AS"], "comment": null, "summary": "Dataset transparency is a key enabler of responsible AI, but insights into\nmultimodal dataset attributes that impact trustworthy and ethical aspects of AI\napplications remain scarce and are difficult to compare across datasets. To\naddress this challenge, we introduce Trustworthy and Ethical Dataset Indicators\n(TEDI) that facilitate the systematic, empirical analysis of dataset\ndocumentation. TEDI encompasses 143 fine-grained indicators that characterize\ntrustworthy and ethical attributes of multimodal datasets and their collection\nprocesses. The indicators are framed to extract verifiable information from\ndataset documentation. Using TEDI, we manually annotated and analyzed over 100\nmultimodal datasets that include human voices. We further annotated data\nsourcing, size, and modality details to gain insights into the factors that\nshape trustworthy and ethical dimensions across datasets. We find that only a\nselect few datasets have documented attributes and practices pertaining to\nconsent, privacy, and harmful content indicators. The extent to which these and\nother ethical indicators are addressed varies based on the data collection\nmethod, with documentation of datasets collected via crowdsourced and direct\ncollection approaches being more likely to mention them. Scraping dominates\nscale at the cost of ethical indicators, but is not the only viable collection\nmethod. Our approach and empirical insights contribute to increasing dataset\ntransparency along trustworthy and ethical dimensions and pave the way for\nautomating the tedious task of extracting information from dataset\ndocumentation in future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TEDI\uff08Trustworthy and Ethical Dataset Indicators\uff09\uff0c\u5305\u542b143\u4e2a\u7ec6\u7c92\u5ea6\u6307\u6807\uff0c\u7528\u4e8e\u5206\u6790\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u900f\u660e\u5ea6\u548c\u4f26\u7406\u5c5e\u6027\u3002\u901a\u8fc7\u624b\u52a8\u6807\u6ce8100\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6570\u636e\u96c6\u5728\u4f26\u7406\u6307\u6807\uff08\u5982\u540c\u610f\u3001\u9690\u79c1\uff09\u4e0a\u7f3a\u4e4f\u6587\u6863\u5316\uff0c\u4e14\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff08\u5982\u4f17\u5305\u3001\u76f4\u63a5\u6536\u96c6\uff09\u5f71\u54cd\u4f26\u7406\u6307\u6807\u7684\u63d0\u53ca\u9891\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u900f\u660e\u5ea6\u548c\u4f26\u7406\u5c5e\u6027\u5bf9\u8d1f\u8d23\u4efbAI\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u5206\u6790\u5de5\u5177\u548c\u8de8\u6570\u636e\u96c6\u7684\u53ef\u6bd4\u6027\u3002", "method": "\u63d0\u51faTEDI\u6846\u67b6\uff0c\u5305\u542b143\u4e2a\u6307\u6807\uff0c\u624b\u52a8\u6807\u6ce8100\u591a\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5206\u6790\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u5bf9\u4f26\u7406\u6307\u6807\u7684\u5f71\u54cd\u3002", "result": "\u5927\u591a\u6570\u6570\u636e\u96c6\u5728\u4f26\u7406\u6307\u6807\u4e0a\u7f3a\u4e4f\u6587\u6863\u5316\uff0c\u4f17\u5305\u548c\u76f4\u63a5\u6536\u96c6\u65b9\u6cd5\u66f4\u53ef\u80fd\u63d0\u53ca\u4f26\u7406\u6307\u6807\u3002", "conclusion": "TEDI\u4e3a\u6570\u636e\u96c6\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5206\u6790\u5de5\u5177\uff0c\u672a\u6765\u53ef\u81ea\u52a8\u5316\u4fe1\u606f\u63d0\u53d6\u3002", "relevance": 40.0}}
{"id": "2505.17288", "pdf": "https://arxiv.org/pdf/2505.17288", "abs": "https://arxiv.org/abs/2505.17288", "authors": ["Seamus Somerstep", "Vinod Raman", "Unique Subedi", "Yuekai Sun"], "title": "Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Using the bit string generation problem as a case study, we theoretically\ncompare two standard methods for adapting large language models to new tasks.\nThe first, referred to as supervised fine-tuning, involves training a new next\ntoken predictor on good generations. The second method, Best-of-N, trains a\nreward model to select good responses from a collection generated by an\nunaltered base model. If the learning setting is realizable, we find that\nsupervised fine-tuning outperforms BoN through a better dependence on the\nresponse length in its rate of convergence. If realizability fails, then\ndepending on the failure mode, BoN can enjoy a better rate of convergence in\neither n or a rate of convergence with better dependence on the response\nlength.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u5230\u65b0\u4efb\u52a1\u7684\u65b9\u6cd5\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548cBest-of-N\uff08BoN\uff09\u3002\u5728\u53ef\u5b9e\u73b0\u6027\u6761\u4ef6\u4e0b\uff0cSFT\u8868\u73b0\u66f4\u597d\uff1b\u5426\u5219\uff0cBoN\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u66f4\u6709\u6548\u5730\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u5230\u65b0\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e24\u79cd\u5e38\u89c1\u65b9\u6cd5\u7684\u4f18\u52a3\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e24\u79cd\u65b9\u6cd5\uff08SFT\u548cBoN\uff09\u5728\u6bd4\u7279\u4e32\u751f\u6210\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u8003\u8651\u53ef\u5b9e\u73b0\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728\u53ef\u5b9e\u73b0\u6027\u6761\u4ef6\u4e0b\uff0cSFT\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff1b\u5426\u5219\uff0cBoN\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u9009\u62e9\u9002\u5e94\u65b9\u6cd5\u9700\u8003\u8651\u4efb\u52a1\u7684\u53ef\u5b9e\u73b0\u6027\uff0cSFT\u548cBoN\u5404\u6709\u4f18\u52bf\u3002", "relevance": 85.0}}
{"id": "2505.17291", "pdf": "https://arxiv.org/pdf/2505.17291", "abs": "https://arxiv.org/abs/2505.17291", "authors": ["Linus Bleistein", "Aur\u00e9lien Bellet", "Julie Josse"], "title": "Optimal Transport with Heterogeneously Missing Data", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We consider the problem of solving the optimal transport problem between two\nempirical distributions with missing values. Our main assumption is that the\ndata is missing completely at random (MCAR), but we allow for heterogeneous\nmissingness probabilities across features and across the two distributions. As\na first contribution, we show that the Wasserstein distance between empirical\nGaussian distributions and linear Monge maps between arbitrary distributions\ncan be debiased without significantly affecting the sample complexity.\nSecondly, we show that entropic regularized optimal transport can be estimated\nefficiently and consistently using iterative singular value thresholding\n(ISVT). We propose a validation set-free hyperparameter selection strategy for\nISVT that leverages our estimator of the Bures-Wasserstein distance, which\ncould be of independent interest in general matrix completion problems.\nFinally, we validate our findings on a wide range of numerical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6709\u7f3a\u5931\u503c\u7684\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u53bb\u504fWasserstein\u8ddd\u79bb\u548c\u9ad8\u6548\u4f30\u8ba1\u71b5\u6b63\u5219\u5316\u6700\u4f18\u4f20\u8f93\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7f3a\u5931\u5b8c\u5168\u968f\u673a\uff08MCAR\uff09\u4f46\u6982\u7387\u5f02\u6784\u7684\u6761\u4ef6\u4e0b\u3002", "method": "1. \u53bb\u504fWasserstein\u8ddd\u79bb\u548c\u7ebf\u6027Monge\u6620\u5c04\uff1b2. \u4f7f\u7528\u8fed\u4ee3\u5947\u5f02\u503c\u9608\u503c\uff08ISVT\uff09\u9ad8\u6548\u4f30\u8ba1\u71b5\u6b63\u5219\u5316\u6700\u4f18\u4f20\u8f93\uff1b3. \u63d0\u51fa\u65e0\u9a8c\u8bc1\u96c6\u7684\u8d85\u53c2\u6570\u9009\u62e9\u7b56\u7565\u3002", "result": "\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0d\u53d7\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u5728\u6570\u503c\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7f3a\u5931\u6570\u636e\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u4e86\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.17519", "pdf": "https://arxiv.org/pdf/2505.17519", "abs": "https://arxiv.org/abs/2505.17519", "authors": ["Wenhan Chang", "Tianqing Zhu", "Yu Zhao", "Shuangyong Song", "Ping Xiong", "Wanlei Zhou", "Yongxiang Li"], "title": "Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": "25 pages, 4 figures", "summary": "In the era of rapid generative AI development, interactions between humans\nand large language models face significant misusing risks. Previous research\nhas primarily focused on black-box scenarios using human-guided prompts and\nwhite-box scenarios leveraging gradient-based LLM generation methods,\nneglecting the possibility that LLMs can act not only as victim models, but\nalso as attacker models to harm other models. We proposes a novel jailbreaking\nmethod inspired by the Chain-of-Thought mechanism, where the attacker model\nuses mission transfer to conceal harmful user intent in dialogue and generates\nchained narrative lures to stimulate the reasoning capabilities of victim\nmodels, leading to successful jailbreaking. To enhance the attack success rate,\nwe introduce a helper model that performs random narrative optimization on the\nnarrative lures during multi-turn dialogues while ensuring alignment with the\noriginal intent, enabling the optimized lures to bypass the safety barriers of\nvictim models effectively. Our experiments reveal that models with weaker\nsafety mechanisms exhibit stronger attack capabilities, demonstrating that\nmodels can not only be exploited, but also help harm others. By incorporating\ntoxicity scores, we employ third-party models to evaluate the harmfulness of\nvictim models' responses to jailbreaking attempts. The study shows that using\nrefusal keywords as an evaluation metric for attack success rates is\nsignificantly flawed because it does not assess whether the responses guide\nharmful questions, while toxicity scores measure the harm of generated content\nwith more precision and its alignment with harmful questions. Our approach\ndemonstrates outstanding performance, uncovering latent vulnerabilities in LLMs\nand providing data-driven feedback to optimize LLM safety mechanisms. We also\ndiscuss two defensive strategies to offer guidance on improving defense\nmechanisms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u601d\u7ef4\u94fe\u673a\u5236\u7684\u65b0\u578b\u8d8a\u72f1\u65b9\u6cd5\uff0c\u653b\u51fb\u6a21\u578b\u901a\u8fc7\u4efb\u52a1\u8f6c\u79fb\u9690\u85cf\u6709\u5bb3\u610f\u56fe\uff0c\u751f\u6210\u94fe\u5f0f\u53d9\u4e8b\u8bf1\u9975\u6fc0\u53d1\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6210\u529f\u5b9e\u73b0\u8d8a\u72f1\u3002\u5f15\u5165\u8f85\u52a9\u6a21\u578b\u4f18\u5316\u8bf1\u9975\uff0c\u5b9e\u9a8c\u8868\u660e\u5b89\u5168\u6027\u8f83\u5f31\u7684\u6a21\u578b\u653b\u51fb\u80fd\u529b\u66f4\u5f3a\uff0c\u6bd2\u6027\u8bc4\u5206\u66f4\u7cbe\u51c6\u8bc4\u4f30\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\u4e2d\u7684\u6ee5\u7528\u98ce\u9669\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u6a21\u578b\u4e0d\u4ec5\u4f5c\u4e3a\u53d7\u5bb3\u8005\u4e5f\u53ef\u80fd\u6210\u4e3a\u653b\u51fb\u8005\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u8d8a\u72f1\u65b9\u6cd5\uff0c\u7ed3\u5408\u4efb\u52a1\u8f6c\u79fb\u548c\u94fe\u5f0f\u53d9\u4e8b\u8bf1\u9975\uff0c\u5f15\u5165\u8f85\u52a9\u6a21\u578b\u4f18\u5316\u8bf1\u9975\u4ee5\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5b89\u5168\u6027\u5f31\u7684\u6a21\u578b\u653b\u51fb\u80fd\u529b\u66f4\u5f3a\uff0c\u6bd2\u6027\u8bc4\u5206\u6bd4\u62d2\u7edd\u5173\u952e\u8bcd\u66f4\u7cbe\u51c6\u8bc4\u4f30\u653b\u51fb\u6548\u679c\uff0c\u63ed\u793a\u4e86LLM\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "conclusion": "\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4f18\u5316LLM\u5b89\u5168\u673a\u5236\u63d0\u4f9b\u6570\u636e\u652f\u6301\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e24\u79cd\u9632\u5fa1\u7b56\u7565\u3002", "relevance": 85.0}}
{"id": "2505.17300", "pdf": "https://arxiv.org/pdf/2505.17300", "abs": "https://arxiv.org/abs/2505.17300", "authors": ["Selina Carter", "Arun K Kuchibhotla"], "title": "Statistical Inference for Online Algorithms", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "comment": "Although SGD is the most commonly mentioned method in machine\n  learning, our simulations show that the performance of SGD is highly\n  sensitive to the choice of tuning parameters of the algorithm. We could not\n  find a simple remedy that improves performance and also makes the asymptotic\n  properties manageable. We hope that our article acts as a word of caution to\n  anyone using online algorithms blindly", "summary": "Construction of confidence intervals and hypothesis tests for functionals\nbased on asymptotically normal estimators is a classical topic in statistical\ninference. The simplest and in many cases optimal inference procedure is the\nWald interval or the likelihood ratio test, both of which require an estimator\nand an estimate of the asymptotic variance of the estimator. Estimators\nobtained from online/sequential algorithms forces one to consider the\ncomputational aspects of the inference problem, i.e., one cannot access all of\nthe data as many times as needed. Several works on this topic explored the\nonline estimation of asymptotic variance. In this article, we propose\ncomputationally efficient, rate-optimal, and asymptotically valid confidence\nregions based on the output of online algorithms {\\em without} estimating the\nasymptotic variance. As a special case, this implies inference from any\nalgorithm that yields an asymptotically normal estimator. We focus our efforts\non stochastic gradient descent with Polyak averaging to understand the\npractical performance of the proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u7b97\u6cd5\u8f93\u51fa\u7684\u8ba1\u7b97\u9ad8\u6548\u3001\u901f\u7387\u6700\u4f18\u4e14\u6e10\u8fd1\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u57df\u65b9\u6cd5\uff0c\u65e0\u9700\u4f30\u8ba1\u6e10\u8fd1\u65b9\u5dee\u3002", "motivation": "\u4f20\u7edf\u63a8\u65ad\u65b9\u6cd5\uff08\u5982Wald\u533a\u95f4\u6216\u4f3c\u7136\u6bd4\u68c0\u9a8c\uff09\u9700\u8981\u4f30\u8ba1\u6e10\u8fd1\u65b9\u5dee\uff0c\u800c\u5728\u7ebf/\u987a\u5e8f\u7b97\u6cd5\u9650\u5236\u4e86\u6570\u636e\u7684\u591a\u6b21\u8bbf\u95ee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4f30\u8ba1\u6e10\u8fd1\u65b9\u5dee\u7684\u9ad8\u6548\u63a8\u65ad\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u7b97\u6cd5\u8f93\u51fa\u7684\u7f6e\u4fe1\u533a\u57df\u6784\u9020\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u5e26\u6709Polyak\u5e73\u5747\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u5b9e\u9645\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u3001\u901f\u7387\u6700\u4f18\u4e14\u6e10\u8fd1\u6709\u6548\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u751f\u6210\u6e10\u8fd1\u6b63\u6001\u4f30\u8ba1\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u7ebf\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4f30\u8ba1\u6e10\u8fd1\u65b9\u5dee\u7684\u63a8\u65ad\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2505.17308", "pdf": "https://arxiv.org/pdf/2505.17308", "abs": "https://arxiv.org/abs/2505.17308", "authors": ["Philipp Pilar", "Markus Heinonen", "Niklas Wahlstr\u00f6m"], "title": "Repulsive Ensembles for Bayesian Inference in Physics-informed Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Physics-informed neural networks (PINNs) have proven an effective tool for\nsolving differential equations, in particular when considering non-standard or\nill-posed settings. When inferring solutions and parameters of the differential\nequation from data, uncertainty estimates are preferable to point estimates, as\nthey give an idea about the accuracy of the solution. In this work, we consider\nthe inverse problem and employ repulsive ensembles of PINNs (RE-PINN) for\nobtaining such estimates. The repulsion is implemented by adding a particular\nrepulsive term to the loss function, which has the property that the ensemble\npredictions correspond to the true Bayesian posterior in the limit of infinite\nensemble members. Where possible, we compare the ensemble predictions to Monte\nCarlo baselines. Whereas the standard ensemble tends to collapse to\nmaximum-a-posteriori solutions, the repulsive ensemble produces significantly\nmore accurate uncertainty estimates and exhibits higher sample diversity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u65a5\u6027\u96c6\u6210\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08RE-PINN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u53cd\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u5728\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6bd4\u70b9\u4f30\u8ba1\u66f4\u6709\u4ef7\u503c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6807\u51c6\u96c6\u6210\uff09\u5bb9\u6613\u5d29\u6e83\u4e3a\u6700\u5927\u540e\u9a8c\u89e3\u3002", "method": "\u901a\u8fc7\u5411\u635f\u5931\u51fd\u6570\u6dfb\u52a0\u6392\u65a5\u9879\uff0c\u6784\u5efa\u6392\u65a5\u6027\u96c6\u6210PINN\uff08RE-PINN\uff09\uff0c\u7406\u8bba\u4e0a\u5728\u65e0\u9650\u96c6\u6210\u6210\u5458\u65f6\u80fd\u903c\u8fd1\u771f\u5b9e\u8d1d\u53f6\u65af\u540e\u9a8c\u3002", "result": "RE-PINN\u6bd4\u6807\u51c6\u96c6\u6210\u65b9\u6cd5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u66f4\u9ad8\u7684\u6837\u672c\u591a\u6837\u6027\u3002", "conclusion": "\u6392\u65a5\u6027\u96c6\u6210\u65b9\u6cd5\u5728\u5fae\u5206\u65b9\u7a0b\u53cd\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17598", "pdf": "https://arxiv.org/pdf/2505.17598", "abs": "https://arxiv.org/abs/2505.17598", "authors": ["Linbao Li", "Yannan Liu", "Daojing He", "Yu Li"], "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArrAttack\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u9632\u5fa1\u6027LLMs\uff0c\u80fd\u591f\u751f\u6210\u7ed5\u8fc7\u591a\u79cd\u9632\u5fa1\u63aa\u65bd\u7684\u9c81\u68d2\u8d8a\u72f1\u63d0\u793a\u3002", "motivation": "\u7814\u7a76\u8d8a\u72f1\u653b\u51fb\u5bf9LLMs\u5b89\u5168\u5bf9\u9f50\u7684\u5a01\u80c1\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u653b\u51fb\u7b56\u7565\u65e0\u6cd5\u5e94\u5bf9\u5feb\u901f\u53d1\u5c55\u7684\u9632\u5fa1\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86ArrAttack\u65b9\u6cd5\uff0c\u5305\u62ec\u4e00\u4e2a\u901a\u7528\u9c81\u68d2\u6027\u5224\u65ad\u6a21\u578b\u548c\u4e00\u4e2a\u9ad8\u6548\u7684\u8d8a\u72f1\u63d0\u793a\u751f\u6210\u5668\u3002", "result": "ArrAttack\u5728\u591a\u79cd\u9632\u5fa1\u63aa\u65bd\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u7b56\u7565\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9GPT-4\u548cClaude-3\u7b49\u6a21\u578b\u7684\u5f3a\u8fc1\u79fb\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u8d8a\u72f1\u653b\u51fb\u4e0e\u9632\u5fa1\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u751f\u6210\u9c81\u68d2\u8d8a\u72f1\u63d0\u793a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.17877", "pdf": "https://arxiv.org/pdf/2505.17877", "abs": "https://arxiv.org/abs/2505.17877", "authors": ["Fran\u00e7ois Derrida", "Shahar Lutati", "Eliya Nachmani"], "title": "Toward Optimal ANC: Establishing Mutual Information Lower Bound", "categories": ["cs.IT", "cs.AI", "cs.LG", "cs.SD", "eess.AS", "math.IT"], "comment": null, "summary": "Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic\ndisturbances by generating anti-noise signals that destructively interfere with\nthe original noise in real time. Although recent deep learning-based ANC\nalgorithms have set new performance benchmarks, there remains a shortage of\ntheoretical limits to rigorously assess their improvements. To address this, we\nderive a unified lower bound on cancellation performance composed of two\ncomponents. The first component is information-theoretic: it links residual\nerror power to the fraction of disturbance entropy captured by the anti-noise\nsignal, thereby quantifying limits imposed by information-processing capacity.\nThe second component is support-based: it measures the irreducible error\narising in frequency bands that the cancellation path cannot address,\nreflecting fundamental physical constraints. By taking the maximum of these two\nterms, our bound establishes a theoretical ceiling on the Normalized Mean\nSquared Error (NMSE) attainable by any ANC algorithm. We validate its tightness\nempirically on the NOISEX dataset under varying reverberation times,\ndemonstrating robustness across diverse acoustic conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e0b\u754c\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e3b\u52a8\u566a\u58f0\u6d88\u9664\uff08ANC\uff09\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4fe1\u606f\u8bba\u548c\u7269\u7406\u7ea6\u675f\u4e24\u90e8\u5206\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728ANC\u7b97\u6cd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6027\u80fd\u6781\u9650\u6765\u4e25\u683c\u8bc4\u4f30\u5176\u6539\u8fdb\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4fe1\u606f\u8bba\uff08\u6b8b\u4f59\u8bef\u5dee\u529f\u7387\u4e0e\u6297\u566a\u58f0\u4fe1\u53f7\u6355\u83b7\u7684\u5e72\u6270\u71b5\u7684\u5173\u7cfb\uff09\u548c\u7269\u7406\u7ea6\u675f\uff08\u4e0d\u53ef\u6d88\u9664\u7684\u9891\u5e26\u8bef\u5dee\uff09\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u4e0b\u754c\u3002", "result": "\u5728NOISEX\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u4e0b\u754c\u7684\u7d27\u5bc6\u5ea6\uff0c\u8868\u660e\u5176\u5728\u591a\u79cd\u58f0\u5b66\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7406\u8bba\u754c\u9650\u4e3a\u4efb\u4f55ANC\u7b97\u6cd5\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u4e0a\u9650\uff0c\u5e76\u53ef\u7528\u4e8e\u8bc4\u4f30\u7b97\u6cd5\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "relevance": 20.0}}
{"id": "2505.17895", "pdf": "https://arxiv.org/pdf/2505.17895", "abs": "https://arxiv.org/abs/2505.17895", "authors": ["Dan A. Calian", "Gregory Farquhar", "Iurii Kemaev", "Luisa M. Zintgraf", "Matteo Hessel", "Jeremy Shar", "Junhyuk Oh", "Andr\u00e1s Gy\u00f6rgy", "Tom Schaul", "Jeffrey Dean", "Hado van Hasselt", "David Silver"], "title": "DataRater: Meta-Learned Dataset Curation", "categories": ["stat.ML", "cs.AI", "cs.LG", "I.2.6"], "comment": null, "summary": "The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDataRater\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u8bc4\u4f30\u8bad\u7ec3\u6570\u636e\u7684\u4ef7\u503c\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4f9d\u8d56\u4eba\u5de5\u8c03\u6574\u6216\u542f\u53d1\u5f0f\u8fc7\u6ee4\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u7ec6\u5316\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5143\u68af\u5ea6\uff08meta-gradients\uff09\u8fdb\u884c\u5143\u5b66\u4e60\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u6570\u636e\u70b9\u7684\u8bad\u7ec3\u4ef7\u503c\uff0c\u5e76\u57fa\u4e8e\u6b64\u8fc7\u6ee4\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDataRater\u80fd\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "conclusion": "DataRater\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7cbe\u7ec6\u5316\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u8c03\u6574\u6216\u542f\u53d1\u5f0f\u8fc7\u6ee4\u3002", "relevance": 85.0}}
{"id": "2505.17329", "pdf": "https://arxiv.org/pdf/2505.17329", "abs": "https://arxiv.org/abs/2505.17329", "authors": ["Hossein Adeli", "Minni Sun", "Nikolaus Kriegeskorte"], "title": "Transformer brain encoders explain human high-level visual responses", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "A major goal of neuroscience is to understand brain computations during\nvisual processing in naturalistic settings. A dominant approach is to use\nimage-computable deep neural networks trained with different task objectives as\na basis for linear encoding models. However, in addition to requiring tuning a\nlarge number of parameters, the linear encoding approach ignores the structure\nof the feature maps both in the brain and the models. Recently proposed\nalternatives have focused on decomposing the linear mapping to spatial and\nfeature components but focus on finding static receptive fields for units that\nare applicable only in early visual areas. In this work, we employ the\nattention mechanism used in the transformer architecture to study how\nretinotopic visual features can be dynamically routed to category-selective\nareas in high-level visual processing. We show that this computational motif is\nsignificantly more powerful than alternative methods in predicting brain\nactivity during natural scene viewing, across different feature basis models\nand modalities. We also show that this approach is inherently more\ninterpretable, without the need to create importance maps, by interpreting the\nattention routing signal for different high-level categorical areas. Our\napproach proposes a mechanistic model of how visual information from\nretinotopic maps can be routed based on the relevance of the input content to\ndifferent category-selective regions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8def\u7531\u89c6\u89c9\u7279\u5f81\u5230\u9ad8\u7ea7\u89c6\u89c9\u5904\u7406\u533a\u57df\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u7406\u89e3\u81ea\u7136\u573a\u666f\u4e0b\u89c6\u89c9\u5904\u7406\u4e2d\u7684\u5927\u8111\u8ba1\u7b97\uff0c\u73b0\u6709\u7ebf\u6027\u7f16\u7801\u6a21\u578b\u5ffd\u7565\u4e86\u7279\u5f81\u56fe\u7684\u7ed3\u6784\uff0c\u4e14\u53c2\u6570\u591a\u3002", "method": "\u5229\u7528Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u8def\u7531\u89c6\u89c9\u7279\u5f81\uff0c\u9884\u6d4b\u5927\u8111\u6d3b\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u5927\u8111\u6d3b\u52a8\u65f6\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u91cd\u8981\u6027\u56fe\u5373\u53ef\u89e3\u91ca\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5bb9\u76f8\u5173\u6027\u7684\u89c6\u89c9\u4fe1\u606f\u8def\u7531\u673a\u5236\u6a21\u578b\u3002", "relevance": 40.0}}
{"id": "2505.17928", "pdf": "https://arxiv.org/pdf/2505.17928", "abs": "https://arxiv.org/abs/2505.17928", "authors": ["Junyi Lu", "Lili Jiang", "Xiaojia Li", "Jianbing Fang", "Fengjun Zhang", "Li Yang", "Chun Zuo"], "title": "Towards Practical Defect-Focused Automated Code Review", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Forty-Second International Conference on Machine Learning\n  (ICML 2025)", "summary": "The complexity of code reviews has driven efforts to automate review\ncomments, but prior approaches oversimplify this task by treating it as\nsnippet-level code-to-text generation and relying on text similarity metrics\nlike BLEU for evaluation. These methods overlook repository context, real-world\nmerge request evaluation, and defect detection, limiting their practicality. To\naddress these issues, we explore the full automation pipeline within the online\nrecommendation service of a company with nearly 400 million daily active users,\nanalyzing industry-grade C++ codebases comprising hundreds of thousands of\nlines of code. We identify four key challenges: 1) capturing relevant context,\n2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and\n4) integrating human workflows. To tackle these, we propose 1) code slicing\nalgorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a\nfiltering mechanism for FAR reduction, and 4) a novel prompt design for better\nhuman interaction. Our approach, validated on real-world merge requests from\nhistorical fault reports, achieves a 2x improvement over standard LLMs and a\n10x gain over previous baselines. While the presented results focus on C++, the\nunderlying framework design leverages language-agnostic principles (e.g.,\nAST-based analysis), suggesting potential for broader applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4ed3\u5e93\u4e0a\u4e0b\u6587\u3001\u771f\u5b9e\u5408\u5e76\u8bf7\u6c42\u8bc4\u4f30\u548c\u7f3a\u9677\u68c0\u6d4b\u7684\u95ee\u9898\u3002\u901a\u8fc7\u4ee3\u7801\u5207\u7247\u3001\u591a\u89d2\u8272LLM\u6846\u67b6\u3001\u8fc7\u6ee4\u673a\u5236\u548c\u63d0\u793a\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u4e0a\u4e0b\u6587\u6355\u83b7\u3001\u7f3a\u9677\u68c0\u6d4b\u548c\u4eba\u7c7b\u5de5\u4f5c\u6d41\u96c6\u6210\u3002", "method": "1) \u4ee3\u7801\u5207\u7247\u7b97\u6cd5\u63d0\u53d6\u4e0a\u4e0b\u6587\uff1b2) \u591a\u89d2\u8272LLM\u6846\u67b6\u63d0\u5347\u5173\u952e\u7f3a\u9677\u5305\u542b\uff1b3) \u8fc7\u6ee4\u673a\u5236\u964d\u4f4e\u8bef\u62a5\u7387\uff1b4) \u65b0\u9896\u63d0\u793a\u8bbe\u8ba1\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u5728\u771f\u5b9e\u5408\u5e76\u8bf7\u6c42\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u6bd4\u6807\u51c6LLM\u63d0\u53472\u500d\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534710\u500d\u3002", "conclusion": "\u6846\u67b6\u8bbe\u8ba1\u57fa\u4e8e\u8bed\u8a00\u65e0\u5173\u539f\u5219\uff08\u5982AST\u5206\u6790\uff09\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6f5c\u529b\u3002", "relevance": 60.0}}
{"id": "2505.17381", "pdf": "https://arxiv.org/pdf/2505.17381", "abs": "https://arxiv.org/abs/2505.17381", "authors": ["Mitsumasa Nakajima", "Kohki Shibahara", "Kohei Ikeda", "Akira Kawai", "Masaya Notomi", "Yutaka Miyamoto", "Toshikazu Hashimoto"], "title": "Programmable Photonic Unitary Processor Enables Parametrized Differentiable Long-Haul Spatial Division Multiplexed Transmission", "categories": ["physics.optics", "cs.LG", "physics.app-ph"], "comment": null, "summary": "The explosive growth of global data traffic demands scalable and\nenergy-efficient optical communication systems. Spatial division multiplexing\n(SDM) using multicore or multimode fibers is a promising solution to overcome\nthe capacity limit of single-mode fibers. However, long-haul SDM transmission\nfaces significant challenges due to modal dispersion, which imposes heavy\ncomputational loads on digital signal processing (DSP) for signal equalization.\nHere, we propose parameterized SDM transmission, where programmable photonic\nunitary processors are installed at intermediate nodes. Instead of relying on\nconventional digital equalization only on the receiver side, our approach\nenables direct optimization of the SDM transmission channel itself by the\nprogrammable unitary processor, which reduces digital post-processing loads. We\nintroduce a gradient-based optimization algorithm using a differentiable SDM\ntransmission model to determine the optimal unitary transformation. As a key\nenabler, we first implemented telecom-grade programmable photonic unitary\nprocessor, achieving a low-loss (2.1 dB fiber-to-fiber), wideband (full\nC-band), polarization-independent, and high-fidelity (R2>96% across the C-band)\noperation. We experimentally demonstrate 1300-km transmission using a\nthree-mode fiber, achieving strong agreement between simulation and experiment.\nThe optimized photonic processor significantly reduces modal dispersion and\npost-processing complexity. Our results establish a scalable framework for\nintegrating photonic computation into the optical layer, enabling more\nefficient, high-capacity optical networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684\u7a7a\u95f4\u5206\u590d\u7528\uff08SDM\uff09\u4f20\u8f93\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u7f16\u7a0b\u5149\u5b50\u9149\u5904\u7406\u5668\u4f18\u5316\u4f20\u8f93\u901a\u9053\uff0c\u51cf\u5c11\u6570\u5b57\u540e\u5904\u7406\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3\u957f\u8ddd\u79bbSDM\u4f20\u8f93\u4e2d\u6a21\u6001\u8272\u6563\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53ef\u7f16\u7a0b\u5149\u5b50\u9149\u5904\u7406\u5668\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u7b97\u6cd5\u4f18\u5316SDM\u4f20\u8f93\u901a\u9053\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e861300\u516c\u91cc\u4e09\u6a21\u5149\u7ea4\u4f20\u8f93\uff0c\u663e\u8457\u964d\u4f4e\u6a21\u6001\u8272\u6563\u548c\u540e\u5904\u7406\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u3001\u9ad8\u5bb9\u91cf\u5149\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002", "relevance": 10.0}}
{"id": "2505.17938", "pdf": "https://arxiv.org/pdf/2505.17938", "abs": "https://arxiv.org/abs/2505.17938", "authors": ["Tianyou Li", "Haijun Zou", "Jiayuan Wu", "Zaiwen Wen"], "title": "LMask: Learn to Solve Constrained Routing Problems with Lazy Masking", "categories": ["math.OC", "cs.AI", "cs.LG", "90C27, 68T20"], "comment": null, "summary": "Routing problems are canonical combinatorial optimization tasks with\nwide-ranging applications in logistics, transportation, and supply chain\nmanagement. However, solving these problems becomes significantly more\nchallenging when complex constraints are involved. In this paper, we propose\nLMask, a novel learning framework that utilizes dynamic masking to generate\nhigh-quality feasible solutions for constrained routing problems. LMask\nintroduces the LazyMask decoding method, which lazily refines feasibility masks\nwith the backtracking mechanism. In addition, it employs the refinement\nintensity embedding to encode the search trace into the model, mitigating\nrepresentation ambiguities induced by backtracking. To further reduce sampling\ncost, LMask sets a backtracking budget during decoding, while constraint\nviolations are penalized in the loss function during training to counteract\ninfeasibility caused by this budget. We provide theoretical guarantees for the\nvalidity and probabilistic optimality of our approach. Extensive experiments on\nthe traveling salesman problem with time windows (TSPTW) and TSP with draft\nlimits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility\nrates and solution quality, outperforming existing neural methods.", "AI": {"tldr": "LMask\u662f\u4e00\u4e2a\u5229\u7528\u52a8\u6001\u63a9\u7801\u89e3\u51b3\u7ea6\u675f\u8def\u7531\u95ee\u9898\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7LazyMask\u89e3\u7801\u548c\u56de\u6eaf\u673a\u5236\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u884c\u89e3\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u7ea6\u675f\u4e0b\u7684\u8def\u7531\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u53ef\u884c\u89e3\u3002", "method": "\u5f15\u5165LazyMask\u89e3\u7801\u548c\u56de\u6eaf\u673a\u5236\uff0c\u4f7f\u7528\u7ec6\u5316\u5f3a\u5ea6\u5d4c\u5165\u7f16\u7801\u641c\u7d22\u8f68\u8ff9\uff0c\u8bbe\u7f6e\u56de\u6eaf\u9884\u7b97\u5e76\u60e9\u7f5a\u7ea6\u675f\u8fdd\u53cd\u3002", "result": "\u5728TSPTW\u548cTSPDL\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53ef\u884c\u7387\u548c\u89e3\u8d28\u91cf\u3002", "conclusion": "LMask\u5728\u7ea6\u675f\u8def\u7531\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u795e\u7ecf\u65b9\u6cd5\u3002", "relevance": 40.0}}
{"id": "2505.17443", "pdf": "https://arxiv.org/pdf/2505.17443", "abs": "https://arxiv.org/abs/2505.17443", "authors": ["Elfarouk Harb", "Yousef Yassin", "Chandra Chekuri"], "title": "Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "We study the problem of minimizing or maximizing the average value $ f(S)/|S|\n$ of a submodular or supermodular set function $ f: 2^V \\to \\mathbb{R} $ over\nnon-empty subsets $ S \\subseteq V $. This generalizes classical problems such\nas Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular\nFunction Minimization (SFM). Motivated by recent applications, we introduce two\nbroad formulations: Unrestricted Sparsest Submodular Set (USSS) and\nUnrestricted Densest Supermodular Set (UDSS), which allow for negative and\nnon-monotone functions.\n  We show that DSS, SFM, USSS, UDSS, and the Minimum Norm Point (MNP) problem\nare equivalent under strongly polynomial-time reductions, enabling algorithmic\ncrossover. In particular, viewing these through the lens of the MNP in the base\npolyhedron, we connect Fujishige's theory with dense decomposition, and show\nthat both Fujishige-Wolfe's algorithm and the heuristic \\textsc{SuperGreedy++}\nact as universal solvers for all these problems, including sub-modular function\nminimization.\n  Theoretically, we explain why \\textsc{SuperGreedy++} is effective beyond DSS,\nincluding for tasks like submodular minimization and minimum $ s $-$ t $ cut.\nEmpirically, we test several solvers, including the Fujishige-Wolfe algorithm\non over 400 experiments across seven problem types and large-scale\nreal/synthetic datasets. Surprisingly, general-purpose convex and flow-based\nmethods outperform task-specific baselines, demonstrating that with the right\nframing, general optimization techniques can be both scalable and\nstate-of-the-art for submodular and supermodular ratio problems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5b50\u6a21\u548c\u8d85\u6a21\u96c6\u5408\u51fd\u6570\u7684\u5e73\u5747\u503c\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u95ee\u9898USSS\u548cUDSS\uff0c\u5e76\u5c55\u793a\u4e86\u5b83\u4eec\u4e0e\u7ecf\u5178\u95ee\u9898\u7684\u7b49\u4ef7\u6027\u3002\u901a\u8fc7MNP\u95ee\u9898\u89c6\u89d2\uff0c\u8fde\u63a5\u4e86Fujishige\u7684\u7406\u8bba\u4e0e\u7a20\u5bc6\u5206\u89e3\uff0c\u8bc1\u660e\u4e86\u901a\u7528\u6c42\u89e3\u5668\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u901a\u7528\u4f18\u5316\u65b9\u6cd5\u5728\u5b50\u6a21\u548c\u8d85\u6a21\u6bd4\u4f8b\u95ee\u9898\u4e0a\u5177\u6709\u4f18\u8d8a\u6027\u3002", "motivation": "\u7814\u7a76\u5b50\u6a21\u548c\u8d85\u6a21\u96c6\u5408\u51fd\u6570\u7684\u5e73\u5747\u503c\u4f18\u5316\u95ee\u9898\uff0c\u6269\u5c55\u7ecf\u5178\u95ee\u9898\u5982DSG\u548cSFM\uff0c\u5e76\u89e3\u51b3\u65b0\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165USSS\u548cUDSS\u95ee\u9898\uff0c\u901a\u8fc7MNP\u95ee\u9898\u89c6\u89d2\u8fde\u63a5Fujishige\u7406\u8bba\u4e0e\u7a20\u5bc6\u5206\u89e3\uff0c\u63d0\u51fa\u901a\u7528\u6c42\u89e3\u5668\u5982Fujishige-Wolfe\u7b97\u6cd5\u548cSuperGreedy++\u3002", "result": "\u8bc1\u660e\u4e86DSS\u3001SFM\u3001USSS\u3001UDSS\u548cMNP\u95ee\u9898\u7684\u7b49\u4ef7\u6027\uff0c\u901a\u7528\u6c42\u89e3\u5668\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u7528\u4f18\u5316\u65b9\u6cd5\u5728\u5b50\u6a21\u548c\u8d85\u6a21\u6bd4\u4f8b\u95ee\u9898\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u548c\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u57fa\u7ebf\u3002", "relevance": 40.0}}
{"id": "2505.17961", "pdf": "https://arxiv.org/pdf/2505.17961", "abs": "https://arxiv.org/abs/2505.17961", "authors": ["Khellaf R\u00e9mi", "Bellet Aur\u00e9lien", "Josse Julie"], "title": "Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation", "categories": ["stat.ME", "cs.AI", "math.ST", "stat.AP", "stat.TH"], "comment": null, "summary": "Causal inference typically assumes centralized access to individual-level\ndata. Yet, in practice, data are often decentralized across multiple sites,\nmaking centralization infeasible due to privacy, logistical, or legal\nconstraints. We address this by estimating the Average Treatment Effect (ATE)\nfrom decentralized observational data using federated learning, which enables\ninference through the exchange of aggregate statistics rather than\nindividual-level data. We propose a novel method to estimate propensity scores\nin a (non-)parametric manner by computing a federated weighted average of local\nscores, using two theoretically grounded weighting schemes -- Membership\nWeights (MW) and Density Ratio Weights (DW) -- that balance communication\nefficiency and model flexibility. These federated scores are then used to\nconstruct two ATE estimators: the Federated Inverse Propensity Weighting\nestimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis\nmethods, which fail when any site violates positivity, our approach leverages\nheterogeneity in treatment assignment across sites to improve overlap. We show\nthat Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample\nsizes, treatment mechanisms, and covariate distributions, with theoretical\nanalysis and experiments on simulated and real-world data highlighting their\nstrengths and limitations relative to meta-analysis and related methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u6362\u805a\u5408\u7edf\u8ba1\u91cf\u800c\u975e\u4e2a\u4f53\u6570\u636e\uff0c\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u3002\u65b9\u6cd5\u5305\u62ec\u4e24\u79cd\u52a0\u6743\u65b9\u6848\uff08MW\u548cDW\uff09\u548c\u4e24\u79cdATE\u4f30\u8ba1\u5668\uff08Fed-IPW\u548cFed-AIPW\uff09\uff0c\u5728\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u5143\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u73af\u5883\u4e0b\u56e0\u679c\u63a8\u65ad\u7684\u6311\u6218\uff0c\u907f\u514d\u4e2a\u4f53\u6570\u636e\u5171\u4eab\u5e26\u6765\u7684\u9690\u79c1\u548c\u5408\u89c4\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u79cd\u52a0\u6743\u65b9\u6848\uff08MW\u548cDW\uff09\u548c\u4e24\u79cdATE\u4f30\u8ba1\u5668\uff08Fed-IPW\u548cFed-AIPW\uff09\u3002", "result": "Fed-IPW\u548cFed-AIPW\u5728\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f18\u4e8e\u5143\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u6570\u636e\u73af\u5883\u3002", "relevance": 40.0}}
{"id": "2505.17468", "pdf": "https://arxiv.org/pdf/2505.17468", "abs": "https://arxiv.org/abs/2505.17468", "authors": ["Miruna Oprescu", "Brian M Cho", "Nathan Kallus"], "title": "Efficient Adaptive Experimentation with Non-Compliance", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": "26 pages, 3 figures", "summary": "We study the problem of estimating the average treatment effect (ATE) in\nadaptive experiments where treatment can only be encouraged--rather than\ndirectly assigned--via a binary instrumental variable. Building on\nsemiparametric efficiency theory, we derive the efficiency bound for ATE\nestimation under arbitrary, history-dependent instrument-assignment policies,\nand show it is minimized by a variance-aware allocation rule that balances\noutcome noise and compliance variability. Leveraging this insight, we introduce\nAMRIV--an \\textbf{A}daptive, \\textbf{M}ultiply-\\textbf{R}obust estimator for\n\\textbf{I}nstrumental-\\textbf{V}ariable settings with variance-optimal\nassignment. AMRIV pairs (i) an online policy that adaptively approximates the\noptimal allocation with (ii) a sequential, influence-function-based estimator\nthat attains the semiparametric efficiency bound while retaining\nmultiply-robust consistency. We establish asymptotic normality, explicit\nconvergence rates, and anytime-valid asymptotic confidence sequences that\nenable sequential inference. Finally, we demonstrate the practical\neffectiveness of our approach through empirical studies, showing that adaptive\ninstrument assignment, when combined with the AMRIV estimator, yields improved\nefficiency and robustness compared to existing baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u591a\u91cd\u7a33\u5065\u7684\u4f30\u8ba1\u5668AMRIV\uff0c\u7528\u4e8e\u5728\u5de5\u5177\u53d8\u91cf\u8bbe\u7f6e\u4e2d\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\uff0c\u5e76\u901a\u8fc7\u65b9\u5dee\u6700\u4f18\u5206\u914d\u7b56\u7565\u63d0\u9ad8\u6548\u7387\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u81ea\u9002\u5e94\u5b9e\u9a8c\u4e2d\u901a\u8fc7\u5de5\u5177\u53d8\u91cf\u95f4\u63a5\u9f13\u52b1\u5904\u7406\u800c\u975e\u76f4\u63a5\u5206\u914d\u5904\u7406\u65f6\uff0c\u5982\u4f55\u9ad8\u6548\u4e14\u7a33\u5065\u5730\u4f30\u8ba1ATE\u3002", "method": "\u57fa\u4e8e\u534a\u53c2\u6570\u6548\u7387\u7406\u8bba\uff0c\u63a8\u5bfcATE\u4f30\u8ba1\u7684\u6548\u7387\u8fb9\u754c\uff0c\u5e76\u8bbe\u8ba1AMRIV\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u5728\u7ebf\u7b56\u7565\u548c\u5e8f\u5217\u4f30\u8ba1\u5668\uff0c\u5b9e\u73b0\u65b9\u5dee\u6700\u4f18\u5206\u914d\u548c\u591a\u91cd\u7a33\u5065\u6027\u3002", "result": "AMRIV\u5728\u6548\u7387\u548c\u7a33\u5065\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AMRIV\u7ed3\u5408\u81ea\u9002\u5e94\u5de5\u5177\u5206\u914d\u548c\u591a\u91cd\u7a33\u5065\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86ATE\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2505.17506", "pdf": "https://arxiv.org/pdf/2505.17506", "abs": "https://arxiv.org/abs/2505.17506", "authors": ["Kihyuk Hong", "Ambuj Tewari"], "title": "Offline Constrained Reinforcement Learning under Partial Data Coverage", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study offline constrained reinforcement learning (RL) with general\nfunction approximation. We aim to learn a policy from a pre-collected dataset\nthat maximizes the expected discounted cumulative reward for a primary reward\nsignal while ensuring that expected discounted returns for multiple auxiliary\nreward signals are above predefined thresholds. Existing algorithms either\nrequire fully exploratory data, are computationally inefficient, or depend on\nan additional auxiliary function classes to obtain an $\\epsilon$-optimal policy\nwith sample complexity $O(\\epsilon^{-2})$. In this paper, we propose an\noracle-efficient primal-dual algorithm based on a linear programming (LP)\nformulation, achieving $O(\\epsilon^{-2})$ sample complexity under partial data\ncoverage. By introducing a realizability assumption, our approach ensures that\nall saddle points of the Lagrangian are optimal, removing the need for\nregularization that complicated prior analyses. Through Lagrangian\ndecomposition, our method extracts policies without requiring knowledge of the\ndata-generating distribution, enhancing practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u5bf9\u5076\u7b97\u6cd5\uff0c\u7528\u4e8e\u79bb\u7ebf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u90e8\u5206\u6570\u636e\u8986\u76d6\u4e0b\u5b9e\u73b0\u6837\u672c\u590d\u6742\u5ea6O(\u03f5\u207b\u00b2)\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u9700\u8981\u5b8c\u5168\u63a2\u7d22\u6570\u636e\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u6216\u4f9d\u8d56\u989d\u5916\u8f85\u52a9\u51fd\u6570\u7c7b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u5bf9\u5076\u7b97\u6cd5\uff0c\u5f15\u5165\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u786e\u4fdd\u62c9\u683c\u6717\u65e5\u978d\u70b9\u6700\u4f18\u3002", "result": "\u5728\u90e8\u5206\u6570\u636e\u8986\u76d6\u4e0b\u5b9e\u73b0O(\u03f5\u207b\u00b2)\u6837\u672c\u590d\u6742\u5ea6\uff0c\u65e0\u9700\u6b63\u5219\u5316\u6216\u6570\u636e\u751f\u6210\u5206\u5e03\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u79bb\u7ebf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002", "relevance": 75.0}}
{"id": "2505.17530", "pdf": "https://arxiv.org/pdf/2505.17530", "abs": "https://arxiv.org/abs/2505.17530", "authors": ["Vendi Ardianto Nugroho", "Byung Moo Lee"], "title": "GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": "Submitted to IEEE. The code implementation repository:\n  https://github.com/ardiantovn/gpsbeam", "summary": "Millimeter-wave (mmWave) communication enables high data rates for\ncellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam\nmanagement remains challenging due to significant path loss and the dynamic\nmobility of UAVs, which can destabilize the UAV-base station (BS) link. This\nresearch presents a GPS-aided deep learning (DL) model that simultaneously\npredicts current and future optimal beams for UAV mmWave communications,\nmaintaining a Top-1 prediction accuracy exceeding 70% and an average power loss\nbelow 0.6 dB across all prediction steps. These outcomes stem from a proposed\ndata set splitting method ensuring balanced label distribution, paired with a\nGPS preprocessing technique that extracts key positional features, and a DL\narchitecture that maps sequential position data to beam index predictions. The\nmodel reduces overhead by approximately 93% (requiring the training of 2 ~ 3\nbeams instead of 32 beams) with 95% beam prediction accuracy guarantees, and\nensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPS\u8f85\u52a9\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u65e0\u4eba\u673a\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u6700\u4f18\u6ce2\u675f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u9500\u5e76\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u65e0\u4eba\u673a\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\uff0c\u6ce2\u675f\u7ba1\u7406\u56e0\u9ad8\u8def\u5f84\u635f\u8017\u548c\u52a8\u6001\u79fb\u52a8\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6570\u636e\u96c6\u5206\u5272\u65b9\u6cd5\u5e73\u8861\u6807\u7b7e\u5206\u5e03\uff0c\u7ed3\u5408GPS\u9884\u5904\u7406\u63d0\u53d6\u5173\u952e\u4f4d\u7f6e\u7279\u5f81\uff0c\u8bbe\u8ba1\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5c06\u5e8f\u5217\u4f4d\u7f6e\u6570\u636e\u6620\u5c04\u5230\u6ce2\u675f\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728Top-1\u9884\u6d4b\u7cbe\u5ea6\u8d85\u8fc770%\uff0c\u5e73\u5747\u529f\u7387\u635f\u8017\u4f4e\u4e8e0.6 dB\uff0c\u5f00\u9500\u51cf\u5c11\u7ea693%\uff0c\u9884\u6d4b\u7cbe\u5ea6\u8fbe95%\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u65e0\u4eba\u673a\u6beb\u7c73\u6ce2\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u6ce2\u675f\u7ba1\u7406\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.18006", "pdf": "https://arxiv.org/pdf/2505.18006", "abs": "https://arxiv.org/abs/2505.18006", "authors": ["Gizem Gultekin-Varkonyi"], "title": "AI Literacy for Legal AI Systems: A practical approach", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.IR"], "comment": "Forthcoming in Iustum Aequum Salutare (2025) vol.21", "summary": "Legal AI systems are increasingly being adopted by judicial and legal system\ndeployers and providers worldwide to support a range of applications. While\nthey offer potential benefits such as reducing bias, increasing efficiency, and\nimproving accountability, they also pose significant risks, requiring a careful\nbalance between opportunities, and legal and ethical development and\ndeployment. AI literacy, as a legal requirement under the EU AI Act and a\ncritical enabler of ethical AI for deployers and providers, could be a tool to\nachieve this. The article introduces the term \"legal AI systems\" and then\nanalyzes the concept of AI literacy and the benefits and risks associated with\nthese systems. This analysis is linked to a broader AI-L concept for\norganizations that deal with legal AI systems. The outcome of the article, a\nroadmap questionnaire as a practical tool for developers and providers to\nassess risks, benefits, and stakeholder concerns, could be useful in meeting\nsocietal and regulatory expectations for legal AI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6cd5\u5f8bAI\u7cfb\u7edf\u7684\u5229\u5f0a\uff0c\u63d0\u51fa\u4e86AI\u7d20\u517b\u7684\u6982\u5ff5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u98ce\u9669\u8bc4\u4f30\u95ee\u5377\u4f5c\u4e3a\u5b9e\u7528\u5de5\u5177\u3002", "motivation": "\u7814\u7a76\u6cd5\u5f8bAI\u7cfb\u7edf\u7684\u6f5c\u5728\u76ca\u5904\uff08\u5982\u51cf\u5c11\u504f\u89c1\u3001\u63d0\u9ad8\u6548\u7387\uff09\u4e0e\u98ce\u9669\uff08\u5982\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\uff09\uff0c\u5e76\u63d0\u51faAI\u7d20\u517b\u4f5c\u4e3a\u5e73\u8861\u5de5\u5177\u3002", "method": "\u5f15\u5165\u201c\u6cd5\u5f8bAI\u7cfb\u7edf\u201d\u672f\u8bed\uff0c\u5206\u6790AI\u7d20\u517b\u6982\u5ff5\u53ca\u5176\u76f8\u5173\u5229\u5f0a\uff0c\u63d0\u51faAI-L\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u98ce\u9669\u8bc4\u4f30\u95ee\u5377\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u98ce\u9669\u8bc4\u4f30\u95ee\u5377\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u63d0\u4f9b\u8005\u6ee1\u8db3\u793e\u4f1a\u548c\u76d1\u7ba1\u5bf9\u6cd5\u5f8bAI\u7684\u671f\u671b\u3002", "conclusion": "AI\u7d20\u517b\u548c\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u662f\u5e73\u8861\u6cd5\u5f8bAI\u7cfb\u7edf\u673a\u4f1a\u4e0e\u98ce\u9669\u7684\u5173\u952e\u3002", "relevance": 40.0}}
{"id": "2505.17592", "pdf": "https://arxiv.org/pdf/2505.17592", "abs": "https://arxiv.org/abs/2505.17592", "authors": ["Tijmen de Haan", "Yuan-Sen Ting", "Tirthankar Ghosal", "Tuan Dung Nguyen", "Alberto Accomazzi", "Emily Herron", "Vanessa Lama", "Rui Pan", "Azton Wells", "Nesar Ramachandra"], "title": "AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model", "categories": ["astro-ph.IM", "cs.LG"], "comment": null, "summary": "General-purpose large language models, despite their broad capabilities,\noften struggle with specialized domain knowledge, a limitation particularly\npronounced in more accessible, lower-parameter versions. This gap hinders their\ndeployment as effective agents in demanding fields such as astronomy. Building\non our prior work with AstroSage-8B, this study introduces AstroSage-70B, a\nsignificantly larger and more advanced domain-specialized natural-language AI\nassistant. It is designed for research and education across astronomy,\nastrophysics, space science, astroparticle physics, cosmology, and astronomical\ninstrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B\nunderwent extensive continued pre-training on a vast corpus of astronomical\nliterature, followed by supervised fine-tuning and model merging. Beyond its\n70-billion parameter scale, this model incorporates refined datasets,\njudiciously chosen learning hyperparameters, and improved training procedures,\nachieving state-of-the-art performance on complex astronomical tasks. Notably,\nwe integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to\neither answer the user query immediately, or first emit a human-readable\nthought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425\nquestions from literature withheld during training -- AstroSage-70B achieves\nstate-of-the-art performance. It surpasses all other tested open-weight and\nproprietary models, including leading systems like o3, Gemini-2.5-Pro,\nClaude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two\norders of magnitude higher. This work demonstrates that domain specialization,\nwhen applied to large-scale models, can enable them to outperform generalist\ncounterparts in specialized knowledge areas like astronomy, thereby advancing\nthe frontier of AI capabilities in the field.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AstroSage-70B\uff0c\u4e00\u4e2a\u4e13\u4e3a\u5929\u6587\u5b66\u9886\u57df\u8bbe\u8ba1\u768470B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u4e13\u4e1a\u5316\u8bad\u7ec3\u5728\u590d\u6742\u5929\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5929\u6587\u5b66\uff09\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002AstroSage-70B\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eLlama-3.1-70B\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u6a21\u578b\u5408\u5e76\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728AstroMLab-1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u6709\u5f00\u653e\u548c\u4e13\u6709\u6a21\u578b\uff0c\u5305\u62ecGemini-2.5-Pro\u7b49\u3002", "conclusion": "\u9886\u57df\u4e13\u4e1a\u5316\u80fd\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6027\u80fd\uff0c\u63a8\u52a8AI\u5728\u5929\u6587\u5b66\u7684\u5e94\u7528\u3002", "relevance": 85.0}}
{"id": "2505.18018", "pdf": "https://arxiv.org/pdf/2505.18018", "abs": "https://arxiv.org/abs/2505.18018", "authors": ["Lijiang Liu", "Junyu Shi", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Shugen Ma", "Qiang Nie"], "title": "ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Current exoskeleton control methods often face challenges in delivering\npersonalized treatment. Standardized walking gaits can lead to patient\ndiscomfort or even injury. Therefore, personalized gait is essential for the\neffectiveness of exoskeleton robots, as it directly impacts their adaptability,\ncomfort, and rehabilitation outcomes for individual users. To enable\npersonalized treatment in exoskeleton-assisted therapy and related\napplications, accurate recognition of personal gait is crucial for implementing\ntailored gait control. The key challenge in gait recognition lies in\neffectively capturing individual differences in subtle gait features caused by\njoint synergy, such as step frequency and step length. To tackle this issue, we\npropose a novel approach, which uses Multi-Scale Global Dense Graph\nConvolutional Networks (GCN) in the spatial domain to identify latent joint\nsynergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics\nLearning module to effectively capture the periodic characteristics of gait in\nthe temporal domain. To support our individual gait recognition task, we have\nconstructed a comprehensive gait dataset that ensures both completeness and\nreliability. Our experimental results demonstrate that our method achieves an\nimpressive accuracy of 94.34% on this dataset, surpassing the current\nstate-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of\nour approach to enhance personalized gait control in exoskeleton-assisted\ntherapy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u5168\u5c40\u5bc6\u96c6\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u6b65\u6001\u8bc6\u522b\uff0c\u4ee5\u63d0\u9ad8\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u6cbb\u7597\u6548\u679c\u3002", "motivation": "\u6807\u51c6\u5316\u6b65\u6001\u53ef\u80fd\u5bfc\u81f4\u60a3\u8005\u4e0d\u9002\u6216\u53d7\u4f24\uff0c\u56e0\u6b64\u4e2a\u6027\u5316\u6b65\u6001\u8bc6\u522b\u5bf9\u4e8e\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u548c\u5eb7\u590d\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u591a\u5c3a\u5ea6\u5168\u5c40\u5bc6\u96c6GCN\u8bc6\u522b\u6f5c\u5728\u5173\u8282\u534f\u540c\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u6b65\u6001\u975e\u7ebf\u6027\u5468\u671f\u6027\u52a8\u6001\u5b66\u4e60\u6a21\u5757\u6355\u6349\u65f6\u95f4\u57df\u6b65\u6001\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u8fbe\u523094.34%\uff0c\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e863.77%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u6b65\u6001\u63a7\u5236\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u5916\u9aa8\u9abc\u8f85\u52a9\u6cbb\u7597\u7684\u6548\u679c\u3002", "relevance": 20.0}}
{"id": "2505.18019", "pdf": "https://arxiv.org/pdf/2505.18019", "abs": "https://arxiv.org/abs/2505.18019", "authors": ["Rashmi Gupta", "Aditya K Gupta", "Aarav Jain", "Avinash C Pandey", "Atul Gupta"], "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 12 figures, Accepted in EASE 2025\n  https://conf.researchr.org/details/ease-2025/ease-2025-ai-models---data/11/LLM-assisted-web-application-functional-requirements-generation-A-case-study-of-fou", "summary": "Like any other discipline, Large Language Models (LLMs) have significantly\nimpacted software engineering by helping developers generate the required\nartifacts across various phases of software development. This paper presents a\ncase study comparing the performance of popular LLMs GPT, Claude, Gemini, and\nDeepSeek in generating functional specifications that include use cases,\nbusiness rules, and collaborative workflows for a web application, the Mess\nManagement System. The study evaluated the quality of LLM generated use cases,\nbusiness rules, and collaborative workflows in terms of their syntactic and\nsemantic correctness, consistency, non ambiguity, and completeness compared to\nthe reference specifications against the zero-shot prompted problem statement.\nOur results suggested that all four LLMs can specify syntactically and\nsemantically correct, mostly non-ambiguous artifacts. Still, they may be\ninconsistent at times and may differ significantly in the completeness of the\ngenerated specification. Claude and Gemini generated all the reference use\ncases, with Claude achieving the most complete but somewhat redundant use case\nspecifications. Similar results were obtained for specifying workflows.\nHowever, all four LLMs struggled to generate relevant Business Rules, with\nDeepSeek generating the most reference rules but with less completeness.\nOverall, Claude generated more complete specification artifacts, while Gemini\nwas more precise in the specifications it generated.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u4e86GPT\u3001Claude\u3001Gemini\u548cDeepSeek\u5728\u751f\u6210\u529f\u80fd\u89c4\u683c\u8bf4\u660e\u4e66\u65f6\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u8bed\u6cd5\u3001\u8bed\u4e49\u3001\u4e00\u81f4\u6027\u3001\u975e\u6b67\u4e49\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u7684\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u751f\u6210\u529f\u80fd\u89c4\u683c\u8bf4\u660e\u4e66\u7684\u80fd\u529b\uff0c\u4ee5\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u5b8c\u6210\u5f00\u53d1\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\u95ee\u9898\u9648\u8ff0\uff0c\u8bc4\u4f30\u56db\u79cdLLMs\u751f\u6210\u7684\u7528\u4f8b\u3001\u4e1a\u52a1\u89c4\u5219\u548c\u534f\u4f5c\u5de5\u4f5c\u6d41\u7684\u8d28\u91cf\u3002", "result": "Claude\u751f\u6210\u7684\u89c4\u683c\u6700\u5b8c\u6574\u4f46\u5197\u4f59\uff0cGemini\u66f4\u7cbe\u786e\uff0c\u6240\u6709LLMs\u5728\u4e1a\u52a1\u89c4\u5219\u751f\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "LLMs\u80fd\u751f\u6210\u8bed\u6cd5\u548c\u8bed\u4e49\u6b63\u786e\u7684\u89c4\u683c\uff0c\u4f46\u5728\u4e00\u81f4\u6027\u548c\u5b8c\u6574\u6027\u4e0a\u5b58\u5728\u5dee\u5f02\uff0cClaude\u548cGemini\u8868\u73b0\u8f83\u597d\u3002", "relevance": 60.0}}
{"id": "2505.18066", "pdf": "https://arxiv.org/pdf/2505.18066", "abs": "https://arxiv.org/abs/2505.18066", "authors": ["Min Hun Lee", "Martyn Zhe Yu Tok"], "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "ACM FAccT 2025", "summary": "Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8ddd\u79bb\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u5728AI\u4efb\u52a1\u59d4\u6258\u4e2d\u7684\u6548\u7528\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u8868\u793a\u53ef\u89c6\u5316\u8fd9\u4e9b\u8bc4\u5206\uff0c\u4ee5\u63d0\u5347\u4eba\u673a\u51b3\u7b56\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u6982\u7387\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u548c\u7528\u6237\u5bf9AI\u7684\u9002\u5f53\u4f9d\u8d56\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u4fc3\u8fdb\u4eba\u7c7b\u5bf9AI\u7684\u9002\u5f53\u4f9d\u8d56\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u8ddd\u79bb\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u53ca\u5176\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u51b3\u7b56\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u7269\u7406\u4e2d\u98ce\u5eb7\u590d\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\uff0819\u540d\u5065\u5eb7\u4e13\u4e1a\u4eba\u5458\u548c10\u540d\u533b\u5b66\u751f\uff09\u6bd4\u8f83\u4e86\u57fa\u4e8e\u8ddd\u79bb\u548c\u6982\u7387\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u5bf9\u7528\u6237\u4f9d\u8d56AI\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u4e8e\u8ddd\u79bb\u7684\u8bc4\u5206\u5728\u8bc6\u522b\u4e0d\u786e\u5b9a\u6848\u4f8b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7528\u6237\u51b3\u7b56\u51c6\u786e\u7387\u63d0\u9ad8\u4e868.20%\uff0c\u9519\u8bef\u66f4\u6539\u7387\u964d\u4f4e\u4e867.14%\uff08p<0.01\uff09\u3002", "conclusion": "\u57fa\u4e8e\u8ddd\u79bb\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u80fd\u6709\u6548\u63d0\u5347\u51b3\u7b56\u51c6\u786e\u6027\u548c\u7528\u6237\u5bf9AI\u7684\u9002\u5f53\u4f9d\u8d56\uff0c\u4f46\u4eba\u673a\u534f\u4f5c\u51b3\u7b56\u4ecd\u9762\u4e34\u6311\u6218\u3002", "relevance": 65.0}}
{"id": "2505.17703", "pdf": "https://arxiv.org/pdf/2505.17703", "abs": "https://arxiv.org/abs/2505.17703", "authors": ["Andr\u00e9 Silva", "Gustav Thor\u00e9n", "Martin Monperrus"], "title": "Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces", "categories": ["cs.PL", "cs.LG", "cs.SE"], "comment": null, "summary": "Automatic program repair seeks to generate correct code from buggy programs,\nwith most approaches searching the correct program in a discrete, symbolic\nspace of source code tokens. This symbolic search is fundamentally limited by\nits inability to directly reason about program behavior. We introduce\nGradient-Based Program Repair (GBPR), a new paradigm that reframes program\nrepair as continuous optimization in a differentiable numerical program space.\nOur core insight is to compile symbolic programs into differentiable numerical\nrepresentations, enabling search in the numerical program space directly guided\nby program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of\n1,466 buggy symbolic RASP programs and their respective numerical\nrepresentations. Our experiments demonstrate that GBPR can effectively repair\nbuggy symbolic programs by gradient-based optimization in the numerical program\nspace, with convincing repair trajectories. To our knowledge, we are the first\nto state program repair as continuous optimization in a numerical program\nspace. Our work establishes a new direction for program repair research,\nbridging two rich worlds: continuous optimization and program behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff08GBPR\uff09\uff0c\u5c06\u7a0b\u5e8f\u4fee\u590d\u95ee\u9898\u8f6c\u5316\u4e3a\u5728\u53ef\u5fae\u5206\u6570\u503c\u7a0b\u5e8f\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7f16\u8bd1\u7b26\u53f7\u7a0b\u5e8f\u4e3a\u53ef\u5fae\u5206\u6570\u503c\u8868\u793a\uff0c\u76f4\u63a5\u6307\u5bfc\u7a0b\u5e8f\u884c\u4e3a\u7684\u641c\u7d22\u3002", "motivation": "\u4f20\u7edf\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u63a8\u7406\u7a0b\u5e8f\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u7a0b\u5e8f\u4fee\u590d\u7684\u6548\u679c\u3002", "method": "\u5c06\u7b26\u53f7\u7a0b\u5e8f\u7f16\u8bd1\u4e3a\u53ef\u5fae\u5206\u6570\u503c\u8868\u793a\uff0c\u5728\u6570\u503c\u7a0b\u5e8f\u7a7a\u95f4\u4e2d\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u8fdb\u884c\u4fee\u590d\u3002", "result": "GBPR\u5728RaspBugs\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u4fee\u590d\u4e86\u9519\u8bef\u7a0b\u5e8f\uff0c\u5c55\u793a\u4e86\u4fee\u590d\u8f68\u8ff9\u3002", "conclusion": "GBPR\u4e3a\u7a0b\u5e8f\u4fee\u590d\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u8fde\u63a5\u4e86\u8fde\u7eed\u4f18\u5316\u4e0e\u7a0b\u5e8f\u884c\u4e3a\u4e24\u4e2a\u9886\u57df\u3002", "relevance": 30.0}}
{"id": "2505.17756", "pdf": "https://arxiv.org/pdf/2505.17756", "abs": "https://arxiv.org/abs/2505.17756", "authors": ["M. Emre Sahin", "Edoardo Altamura", "Oscar Wallis", "Stephen P. Wood", "Anton Dekusar", "Declan A. Millar", "Takashi Imamichi", "Atsushi Matsuo", "Stefano Mensa"], "title": "Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators", "categories": ["quant-ph", "cs.ET", "cs.LG", "physics.comp-ph"], "comment": "6 pages, 1 figure. Qiskit Machine Learning is open-source and\n  available at https://github.com/qiskit-community/qiskit-machine-learning", "summary": "We present Qiskit Machine Learning (ML), a high-level Python library that\ncombines elements of quantum computing with traditional machine learning. The\nAPI abstracts Qiskit's primitives to facilitate interactions with classical\nsimulators and quantum hardware. Qiskit ML started as a proof-of-concept code\nin 2019 and has since been developed to be a modular, intuitive tool for\nnon-specialist users while allowing extensibility and fine-tuning controls for\nquantum computational scientists and developers. The library is available as a\npublic, open-source tool and is distributed under the Apache version 2.0\nlicense.", "AI": {"tldr": "Qiskit ML\u662f\u4e00\u4e2a\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u9ad8\u5c42Python\u5e93\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u6a21\u5757\u5316\u5de5\u5177\uff0c\u540c\u65f6\u652f\u6301\u91cf\u5b50\u8ba1\u7b97\u79d1\u5b66\u5bb6\u6269\u5c55\u548c\u5fae\u8c03\u3002", "motivation": "\u5c06\u91cf\u5b50\u8ba1\u7b97\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7ed3\u5408\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u6613\u7528\u5de5\u5177\uff0c\u540c\u65f6\u6ee1\u8db3\u4e13\u4e1a\u5f00\u53d1\u8005\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u62bd\u8c61Qiskit\u539f\u8bed\uff0c\u8bbe\u8ba1\u6a21\u5757\u5316\u3001\u76f4\u89c2\u7684API\uff0c\u652f\u6301\u7ecf\u5178\u6a21\u62df\u5668\u548c\u91cf\u5b50\u786c\u4ef6\u4ea4\u4e92\u3002", "result": "\u5f00\u53d1\u51fa\u5f00\u6e90\u5de5\u5177Qiskit ML\uff0c\u652f\u6301\u975e\u4e13\u4e1a\u7528\u6237\u548c\u4e13\u4e1a\u5f00\u53d1\u8005\u4f7f\u7528\u3002", "conclusion": "Qiskit ML\u6210\u529f\u5b9e\u73b0\u4e86\u91cf\u5b50\u8ba1\u7b97\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u4e3a\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 10.0}}
{"id": "2505.17789", "pdf": "https://arxiv.org/pdf/2505.17789", "abs": "https://arxiv.org/abs/2505.17789", "authors": ["Florian Kalinke", "Shakeel Gavioli-Akilagun"], "title": "Optimal Online Change Detection via Random Fourier Features", "categories": ["stat.ML", "cs.LG", "68W27 (Primary) 62G10, 46E22 (Secondary)", "G.3; I.2.6"], "comment": null, "summary": "This article studies the problem of online non-parametric change point\ndetection in multivariate data streams. We approach the problem through the\nlens of kernel-based two-sample testing and introduce a sequential testing\nprocedure based on random Fourier features, running with logarithmic time\ncomplexity per observation and with overall logarithmic space complexity. The\nalgorithm has two advantages compared to the state of the art. First, our\napproach is genuinely online, and no access to training data known to be from\nthe pre-change distribution is necessary. Second, the algorithm does not\nrequire the user to specify a window parameter over which local tests are to be\ncalculated. We prove strong theoretical guarantees on the algorithm's\nperformance, including information-theoretic bounds demonstrating that the\ndetection delay is optimal in the minimax sense. Numerical studies on real and\nsynthetic data show that our algorithm is competitive with respect to the state\nof the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u65b9\u6cd5\u7684\u5728\u7ebf\u975e\u53c2\u6570\u53d8\u70b9\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5177\u6709\u5bf9\u6570\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u6216\u7a97\u53e3\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u7814\u7a76\u591a\u53d8\u91cf\u6570\u636e\u6d41\u4e2d\u7684\u5728\u7ebf\u975e\u53c2\u6570\u53d8\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6838\u65b9\u6cd5\u7684\u53cc\u6837\u672c\u68c0\u9a8c\uff0c\u5229\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u8bbe\u8ba1\u5e8f\u5217\u6d4b\u8bd5\u8fc7\u7a0b\uff0c\u5177\u6709\u5bf9\u6570\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u6570\u503c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u8fbe\u5230\u4fe1\u606f\u8bba\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u5728\u7ebf\u53d8\u70b9\u68c0\u6d4b\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "relevance": 30.0}}
{"id": "2505.17819", "pdf": "https://arxiv.org/pdf/2505.17819", "abs": "https://arxiv.org/abs/2505.17819", "authors": ["J\u00fcrgen D\u00f6lz", "Jolanda Weygandt"], "title": "Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Spectral clustering is a popular unsupervised learning technique which is\nable to partition unlabelled data into disjoint clusters of distinct shapes.\nHowever, the data under consideration are often experimental data, implying\nthat the data is subject to measurement errors and measurements may even be\nlost or invalid. These uncertainties in the corrupted input data induce\ncorresponding uncertainties in the resulting clusters, and the clusterings thus\nbecome unreliable.\n  Modelling the uncertainties as random processes, we discuss a mathematical\nframework based on random set theory for the computational Monte Carlo\napproximation of statistically expected clusterings in case of corrupted, i.e.,\nperturbed, incomplete, and possibly even additional, data. We propose several\ncomputationally accessible quantities of interest and analyze their consistency\nin the infinite data point and infinite Monte Carlo sample limit. Numerical\nexperiments are provided to illustrate and compare the proposed quantities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u96c6\u7406\u8bba\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u56e0\u6570\u636e\u635f\u574f\uff08\u5982\u6270\u52a8\u3001\u4e0d\u5b8c\u6574\u6216\u989d\u5916\u6570\u636e\uff09\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u805a\u7c7b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u8ba1\u7b97\u7edf\u8ba1\u671f\u671b\u805a\u7c7b\u3002", "motivation": "\u7531\u4e8e\u5b9e\u9a8c\u6570\u636e\u5e38\u53d7\u6d4b\u91cf\u8bef\u5dee\u6216\u6570\u636e\u4e22\u5931\u5f71\u54cd\uff0c\u4f20\u7edf\u8c31\u805a\u7c7b\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u4e0b\u7684\u7ed3\u679c\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u5e76\u5904\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u968f\u673a\u96c6\u7406\u8bba\u5efa\u6a21\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u8499\u7279\u5361\u6d1b\u8fd1\u4f3c\u65b9\u6cd5\u8ba1\u7b97\u7edf\u8ba1\u671f\u671b\u805a\u7c7b\uff0c\u5e76\u5206\u6790\u5176\u5728\u5927\u6570\u636e\u70b9\u548c\u65e0\u9650\u8499\u7279\u5361\u6d1b\u6837\u672c\u4e0b\u7684\u6536\u655b\u6027\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u611f\u5174\u8da3\u91cf\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u4e0b\u7684\u805a\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u5de5\u5177\u3002", "relevance": 20.0}}
{"id": "2505.17823", "pdf": "https://arxiv.org/pdf/2505.17823", "abs": "https://arxiv.org/abs/2505.17823", "authors": ["Gerardo Roa-Dabike", "Trevor J. Cox", "Jon P. Barker", "Michael A. Akeroyd", "Scott Bannister", "Bruno Fazenda", "Jennifer Firth", "Simone Graetzer", "Alinka Greasley", "Rebecca R. Vos", "William M. Whitmer"], "title": "Source Separation of Small Classical Ensembles: Challenges and Opportunities", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "5 pages, 4 figures, 2 tables, submitted to WASSPA 2025", "summary": "Musical (MSS) source separation of western popular music using non-causal\ndeep learning can be very effective. In contrast, MSS for classical music is an\nunsolved problem. Classical ensembles are harder to separate than popular music\nbecause of issues such as the inherent greater variation in the music; the\nsparsity of recordings with ground truth for supervised training; and greater\nambiguity between instruments. The Cadenza project has been exploring MSS for\nclassical music. This is being done so music can be remixed to improve\nlistening experiences for people with hearing loss. To enable the work, a new\ndatabase of synthesized woodwind ensembles was created to overcome instrumental\nimbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used\nwith each model being trained to extract a string or woodwind instrument.\nConvTasNet was chosen because it enabled both causal and non-causal approaches\nto be tested. Non-causal approaches have dominated MSS work and are useful for\nrecorded music, but for live music or processing on hearing aids, causal signal\nprocessing is needed. The MSS performance was evaluated on the two small\ndatasets (Bach10 and URMP) of real instrument recordings where the ground-truth\nis available. The performances of the causal and non-causal systems were\nsimilar. Comparing the average Signal-to-Distortion (SDR) of the synthesized\nvalidation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation\nset (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized\nand recorded data is a problem. Future work needs to either gather more real\nrecordings that can be used for training, or to improve the realism and\ndiversity of the synthesized recordings to reduce the mismatch...", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53e4\u5178\u97f3\u4e50\u7684\u97f3\u6e90\u5206\u79bb\u95ee\u9898\uff0c\u4f7f\u7528ConvTasNet\u6a21\u578b\u8fdb\u884c\u56e0\u679c\u548c\u975e\u56e0\u679c\u65b9\u6cd5\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53e4\u5178\u97f3\u4e50\u97f3\u6e90\u5206\u79bb\u7684\u96be\u9898\uff0c\u4ee5\u6539\u5584\u542c\u529b\u969c\u788d\u8005\u7684\u542c\u89c9\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528ConvTasNet\u6a21\u578b\uff0c\u5206\u522b\u8bad\u7ec3\u56e0\u679c\u548c\u975e\u56e0\u679c\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u56e0\u679c\u548c\u975e\u56e0\u679c\u65b9\u6cd5\u6027\u80fd\u76f8\u8fd1\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u663e\u8457\u4e0d\u5339\u914d\uff08SDR\u5dee\u5f02\u5927\uff09\u3002", "conclusion": "\u672a\u6765\u9700\u6536\u96c6\u66f4\u591a\u771f\u5b9e\u6570\u636e\u6216\u6539\u8fdb\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002", "relevance": 30.0}}
{"id": "2505.18120", "pdf": "https://arxiv.org/pdf/2505.18120", "abs": "https://arxiv.org/abs/2505.18120", "authors": ["Jiongran Wu", "Jiahao Liu", "Dongsheng Li", "Guangping Zhang", "Mingzhe Han", "Hansu Gu", "Peng Zhang", "Li Shang", "Tun Lu", "Ning Gu"], "title": "Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": "11 pages, under review", "summary": "Large language models (LLMs) have demonstrated exceptional performance in\nunderstanding and generating semantic patterns, making them promising\ncandidates for sequential recommendation tasks. However, when combined with\nconventional recommendation models (CRMs), LLMs often face challenges related\nto high inference costs and static knowledge transfer methods. In this paper,\nwe propose a novel mutual distillation framework, LLMD4Rec, that fosters\ndynamic and bidirectional knowledge exchange between LLM-centric and CRM-based\nrecommendation systems. Unlike traditional unidirectional distillation methods,\nLLMD4Rec enables iterative optimization by alternately refining both models,\nenhancing the semantic understanding of CRMs and enriching LLMs with\ncollaborative signals from user-item interactions. By leveraging sample-wise\nadaptive weighting and aligning output distributions, our approach eliminates\nthe need for additional parameters while ensuring effective knowledge transfer.\nExtensive experiments on real-world datasets demonstrate that LLMD4Rec\nsignificantly improves recommendation accuracy across multiple benchmarks\nwithout increasing inference costs. This method provides a scalable and\nefficient solution for combining the strengths of both LLMs and CRMs in\nsequential recommendation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\u6846\u67b6LLMD4Rec\uff0c\u7ed3\u5408LLM\u548c\u4f20\u7edf\u63a8\u8350\u6a21\u578b\uff0c\u52a8\u6001\u4f18\u5316\u4e24\u8005\uff0c\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "motivation": "LLM\u4e0e\u4f20\u7edf\u63a8\u8350\u6a21\u578b\u7ed3\u5408\u65f6\u9762\u4e34\u9ad8\u63a8\u7406\u6210\u672c\u548c\u9759\u6001\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff0c\u9700\u8981\u52a8\u6001\u53cc\u5411\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u5411\u84b8\u998f\u6846\u67b6LLMD4Rec\uff0c\u901a\u8fc7\u6837\u672c\u81ea\u9002\u5e94\u52a0\u6743\u548c\u8f93\u51fa\u5206\u5e03\u5bf9\u9f50\uff0c\u5b9e\u73b0\u65e0\u989d\u5916\u53c2\u6570\u7684\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "conclusion": "LLMD4Rec\u4e3a\u7ed3\u5408LLM\u4e0e\u4f20\u7edf\u63a8\u8350\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 75.0}}
{"id": "2505.17836", "pdf": "https://arxiv.org/pdf/2505.17836", "abs": "https://arxiv.org/abs/2505.17836", "authors": ["Anna Van Elst", "Igor Colin", "Stephan Cl\u00e9men\u00e7on"], "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/t)$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGoRank\u7684\u9c81\u68d2\u6392\u540d\u4f30\u8ba1\u7b97\u6cd5\u548cGoTrim\u7684\u4fee\u526a\u5747\u503c\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u901a\u4fe1\u56fe\u4e2d\u7684\u6076\u610f\u8282\u70b9\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5747\u503c\u7684gossip\u7b97\u6cd5\u6613\u53d7\u6076\u610f\u8282\u70b9\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u5206\u5e03\u5f0f\u4f30\u8ba1\u7b97\u6cd5\u3002", "method": "\u63d0\u51faGoRank\u7b97\u6cd5\u8fdb\u884c\u9c81\u68d2\u6392\u540d\u4f30\u8ba1\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1GoTrim\u7b97\u6cd5\u7528\u4e8e\u4fee\u526a\u5747\u503c\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eGoRank\u548cGoTrim\u5206\u522b\u5177\u6709O(1/t)\u548cO(log(t)/t)\u7684\u6536\u655b\u901f\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GoRank\u548cGoTrim\u7b97\u6cd5\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u80fd\u6709\u6548\u62b5\u6297\u6076\u610f\u8282\u70b9\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002", "relevance": 30.0}}
{"id": "2505.17838", "pdf": "https://arxiv.org/pdf/2505.17838", "abs": "https://arxiv.org/abs/2505.17838", "authors": ["Abhiti Mishra", "Yash Patel", "Ambuj Tewari"], "title": "Continuum Transformers Perform In-Context Learning by Operator Gradient Descent", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Transformers robustly exhibit the ability to perform in-context learning,\nwhereby their predictive accuracy on a task can increase not by parameter\nupdates but merely with the placement of training samples in their context\nwindows. Recent works have shown that transformers achieve this by implementing\ngradient descent in their forward passes. Such results, however, are restricted\nto standard transformer architectures, which handle finite-dimensional inputs.\nIn the space of PDE surrogate modeling, a generalization of transformers to\nhandle infinite-dimensional function inputs, known as \"continuum transformers,\"\nhas been proposed and similarly observed to exhibit in-context learning.\nDespite impressive empirical performance, such in-context learning has yet to\nbe theoretically characterized. We herein demonstrate that continuum\ntransformers perform in-context operator learning by performing gradient\ndescent in an operator RKHS. We demonstrate this using novel proof strategies\nthat leverage a generalized representer theorem for Hilbert spaces and gradient\nflows over the space of functionals of a Hilbert space. We additionally show\nthe operator learned in context is the Bayes Optimal Predictor in the infinite\ndepth limit of the transformer. We then provide empirical validations of this\noptimality result and demonstrate that the parameters under which such gradient\ndescent is performed are recovered through the continuum transformer training.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u8fde\u7eed\u53d8\u6362\u5668\uff08continuum transformers\uff09\u901a\u8fc7\u5728\u5176\u524d\u5411\u4f20\u64ad\u4e2d\u6267\u884c\u68af\u5ea6\u4e0b\u964d\u6765\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u65e0\u9650\u6df1\u5ea6\u9650\u5236\u4e0b\u5b66\u4e60\u5230\u7684\u662f\u8d1d\u53f6\u65af\u6700\u4f18\u9884\u6d4b\u5668\u3002", "motivation": "\u7814\u7a76\u8fde\u7eed\u53d8\u6362\u5668\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u7406\u8bba\u7279\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u6807\u51c6\u53d8\u6362\u5668\u7684\u7a7a\u767d\u3002", "method": "\u5229\u7528\u5e7f\u4e49\u8868\u793a\u5b9a\u7406\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e0a\u7684\u68af\u5ea6\u6d41\uff0c\u8bc1\u660e\u8fde\u7eed\u53d8\u6362\u5668\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u5728\u7b97\u5b50RKHS\u4e2d\u5b66\u4e60\u3002", "result": "\u8fde\u7eed\u53d8\u6362\u5668\u5728\u65e0\u9650\u6df1\u5ea6\u9650\u5236\u4e0b\u5b66\u4e60\u5230\u7684\u662f\u8d1d\u53f6\u65af\u6700\u4f18\u9884\u6d4b\u5668\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6700\u4f18\u6027\u3002", "conclusion": "\u8fde\u7eed\u53d8\u6362\u5668\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u7406\u8bba\u4e0a\u7684\u6700\u4f18\u6027\uff0c\u4e14\u5176\u8bad\u7ec3\u53c2\u6570\u80fd\u591f\u6062\u590d\u68af\u5ea6\u4e0b\u964d\u7684\u6761\u4ef6\u3002", "relevance": 85.0}}
{"id": "2505.17907", "pdf": "https://arxiv.org/pdf/2505.17907", "abs": "https://arxiv.org/abs/2505.17907", "authors": ["Ka Long Keith Ho", "Yoshinari Takeishi", "Junichi Takeuchi"], "title": "Function Forms of Simple ReLU Networks with Random Hidden Weights", "categories": ["stat.ML", "cs.LG"], "comment": "21 pages, 1 figure, 1 table", "summary": "We investigate the function space dynamics of a two-layer ReLU neural network\nin the infinite-width limit, highlighting the Fisher information matrix (FIM)'s\nrole in steering learning. Extending seminal works on approximate\neigendecomposition of the FIM, we derive the asymptotic behavior of basis\nfunctions ($f_v(x) = X^{\\top} v $) for four groups of approximate eigenvectors,\nshowing their convergence to distinct function forms. These functions,\nprioritized by gradient descent, exhibit FIM-induced inner products that\napproximate orthogonality in the function space, forging a novel connection\nbetween parameter and function spaces. Simulations validate the accuracy of\nthese theoretical approximations, confirming their practical relevance. By\nrefining the function space inner product's role, we advance the theoretical\nframework for ReLU networks, illuminating their optimization and expressivity.\nOverall, this work offers a robust foundation for understanding wide neural\nnetworks and enhances insights into scalable deep learning architectures,\npaving the way for improved design and analysis of neural networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\u4e24\u5c42ReLU\u795e\u7ecf\u7f51\u7edc\u7684\u529f\u80fd\u7a7a\u95f4\u52a8\u529b\u5b66\uff0c\u91cd\u70b9\u5206\u6790\u4e86Fisher\u4fe1\u606f\u77e9\u9635\uff08FIM\uff09\u5728\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002\u901a\u8fc7\u6269\u5c55FIM\u8fd1\u4f3c\u7279\u5f81\u5206\u89e3\u7684\u7814\u7a76\uff0c\u63a8\u5bfc\u4e86\u56db\u7ec4\u8fd1\u4f3c\u7279\u5f81\u5411\u91cf\u7684\u57fa\u51fd\u6570\u7684\u6e10\u8fdb\u884c\u4e3a\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6536\u655b\u6027\u3002", "motivation": "\u63a2\u7d22\u5bbd\u795e\u7ecf\u7f51\u7edc\u7684\u529f\u80fd\u7a7a\u95f4\u52a8\u529b\u5b66\uff0c\u63ed\u793aFIM\u5728\u4f18\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u7406\u8bba\u5206\u6790FIM\u7684\u8fd1\u4f3c\u7279\u5f81\u5206\u89e3\uff0c\u63a8\u5bfc\u57fa\u51fd\u6570\u7684\u6e10\u8fdb\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u57fa\u51fd\u6570\u5728\u529f\u80fd\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u8fd1\u4f3c\u6b63\u4ea4\u6027\uff0c\u63ed\u793a\u4e86\u53c2\u6570\u7a7a\u95f4\u4e0e\u529f\u80fd\u7a7a\u95f4\u7684\u65b0\u8054\u7cfb\u3002\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u5bbd\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u3002", "relevance": 40.0}}
{"id": "2505.17914", "pdf": "https://arxiv.org/pdf/2505.17914", "abs": "https://arxiv.org/abs/2505.17914", "authors": ["Nayoung Kim", "Seongsu Kim", "Sungsoo Ahn"], "title": "Flexible MOF Generation with Torsion-Aware Flow Matching", "categories": ["q-bio.BM", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Designing metal-organic frameworks (MOFs) with novel chemistries is a\nlong-standing challenge due to their large combinatorial space and the complex\n3D arrangements of building blocks. While recent deep generative models have\nenabled scalable MOF generation, they assume (1) a fixed set of building blocks\nand (2) known ground-truth local block-wise 3D coordinates. However, this\nlimits their ability to (1) design novel MOFs and (2) generate the structure\nusing novel building blocks. We propose a two-stage de novo MOF generation\nframework that overcomes these limitations by modeling both chemical and\ngeometric degrees of freedom. First, we train a SMILES-based autoregressive\nmodel to generate novel metal and organic building blocks, paired with\ncheminformatics for 3D structure initialization. Second, we introduce a\nflow-matching model that predicts translations, rotations, and torsional angles\nto assemble flexible blocks into valid 3D frameworks. Our experiments\ndemonstrate improved reconstruction accuracy, the generation of valid, novel,\nand unique MOFs, and the ability of our model to create novel building blocks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u65b0\u578b\u91d1\u5c5e\u6709\u673a\u6846\u67b6\uff08MOF\uff09\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5316\u5b66\u548c\u51e0\u4f55\u81ea\u7531\u5ea6\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728MOF\u8bbe\u8ba1\u4e2d\u5047\u8bbe\u56fa\u5b9a\u7684\u6784\u5efa\u5757\u548c\u5df2\u77e5\u76843D\u5750\u6807\uff0c\u9650\u5236\u4e86\u751f\u6210\u65b0\u9896MOF\u548c\u6784\u5efa\u5757\u7684\u80fd\u529b\u3002", "method": "1. \u4f7f\u7528\u57fa\u4e8eSMILES\u7684\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u65b0\u578b\u91d1\u5c5e\u548c\u6709\u673a\u6784\u5efa\u5757\uff1b2. \u5f15\u5165\u6d41\u5339\u914d\u6a21\u578b\u9884\u6d4b\u5e73\u79fb\u3001\u65cb\u8f6c\u548c\u626d\u8f6c\u89d2\uff0c\u7ec4\u88c5\u6210\u6709\u65483D\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u91cd\u5efa\u51c6\u786e\u6027\uff0c\u751f\u6210\u4e86\u6709\u6548\u3001\u65b0\u9896\u4e14\u72ec\u7279\u7684MOF\uff0c\u5e76\u80fd\u521b\u5efa\u65b0\u578b\u6784\u5efa\u5757\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aMOF\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 30.0}}
{"id": "2505.17917", "pdf": "https://arxiv.org/pdf/2505.17917", "abs": "https://arxiv.org/abs/2505.17917", "authors": ["Xingyu Li", "Qing Liu", "Tony Jiang", "Hong Amy Xia", "Brian P. Hobbs", "Peng Wei"], "title": "M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We propose a novel method, termed the M-learner, for estimating heterogeneous\nindirect and total treatment effects and identifying relevant subgroups within\na mediation framework. The procedure comprises four key steps. First, we\ncompute individual-level conditional average indirect/total treatment effect\nSecond, we construct a distance matrix based on pairwise differences. Third, we\napply tSNE to project this matrix into a low-dimensional Euclidean space,\nfollowed by K-means clustering to identify subgroup structures. Finally, we\ncalibrate and refine the clusters using a threshold-based procedure to\ndetermine the optimal configuration. To the best of our knowledge, this is the\nfirst approach specifically designed to capture treatment effect heterogeneity\nin the presence of mediation. Experimental results validate the robustness and\neffectiveness of the proposed framework. Application to the real-world Jobs II\ndataset highlights the broad adaptability and potential applicability of our\nmethod.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-learner\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5f02\u8d28\u95f4\u63a5\u548c\u603b\u5904\u7406\u6548\u5e94\uff0c\u5e76\u5728\u4e2d\u4ecb\u6846\u67b6\u5185\u8bc6\u522b\u76f8\u5173\u5b50\u7ec4\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u4e2d\u4ecb\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u6355\u6349\u5904\u7406\u6548\u5e94\u5f02\u8d28\u6027\u7684\u95ee\u9898\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "method": "\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a\u8ba1\u7b97\u4e2a\u4f53\u7ea7\u6761\u4ef6\u5e73\u5747\u95f4\u63a5/\u603b\u5904\u7406\u6548\u5e94\uff0c\u6784\u5efa\u8ddd\u79bb\u77e9\u9635\uff0c\u5e94\u7528tSNE\u964d\u7ef4\u548cK-means\u805a\u7c7b\uff0c\u6700\u540e\u901a\u8fc7\u9608\u503c\u7a0b\u5e8f\u6821\u51c6\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0cJobs II\u6570\u636e\u96c6\u7684\u5e94\u7528\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u5e94\u6027\u3002", "conclusion": "M-learner\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e2d\u4ecb\u6846\u67b6\u4e0b\u5904\u7406\u6548\u5e94\u5f02\u8d28\u6027\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "relevance": 30.0}}
{"id": "2505.17932", "pdf": "https://arxiv.org/pdf/2505.17932", "abs": "https://arxiv.org/abs/2505.17932", "authors": ["Umberto Casti", "Sandro Zampieri", "Fabio Pasqualetti"], "title": "Selection Mechanisms for Sequence Modeling using Linear State Space Models", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "9 pages, 5 figures", "summary": "Recent advancements in language modeling tasks have been driven by\narchitectures such as Transformers and, more recently, by Selective State Space\nModels (SSMs). In this paper, we introduce an alternative selection mechanism\ninspired by control theory methodologies. Specifically, we propose a novel\nresidual generator for selection, drawing an analogy to fault detection\nstrategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes\nLinear Time-Varying (LTV) systems, our approach combines multiple LTI systems,\npreserving their beneficial properties during training while achieving\ncomparable selectivity. To evaluate the effectiveness of the proposed\narchitecture, we test its performance on synthetic tasks. While these tasks are\nnot inherently critical, they serve as benchmarks to test the selectivity\nproperties of different cores architecture. This work highlights the potential\nof integrating theoretical insights with experimental advancements, offering a\ncomplementary perspective to deep learning innovations at the intersection of\ncontrol theory and machine learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684\u9009\u62e9\u673a\u5236\uff0c\u7528\u4e8e\u6539\u8fdb\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u63a7\u5236\u7406\u8bba\u7684\u65b9\u6cd5\uff08\u5982\u6545\u969c\u68c0\u6d4b\u7b56\u7565\uff09\u5e94\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u5347\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b8b\u5dee\u751f\u6210\u5668\u9009\u62e9\u673a\u5236\uff0c\u7ed3\u5408\u591a\u4e2a\u7ebf\u6027\u65f6\u4e0d\u53d8\uff08LTI\uff09\u7cfb\u7edf\uff0c\u4fdd\u7559\u5176\u8bad\u7ec3\u4f18\u52bf\u5e76\u5b9e\u73b0\u9009\u62e9\u6027\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u7684\u9009\u62e9\u6027\u6027\u80fd\uff0c\u8868\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08\u5982Mamba\uff09\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u63a7\u5236\u7406\u8bba\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u521b\u65b0\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "relevance": 85.0}}
{"id": "2505.17958", "pdf": "https://arxiv.org/pdf/2505.17958", "abs": "https://arxiv.org/abs/2505.17958", "authors": ["Vittorio Erba", "Emanuele Troiani", "Lenka Zdeborov\u00e1", "Florent Krzakala"], "title": "The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We study the high-dimensional asymptotics of empirical risk minimization\n(ERM) in over-parametrized two-layer neural networks with quadratic activations\ntrained on synthetic data. We derive sharp asymptotics for both training and\ntest errors by mapping the $\\ell_2$-regularized learning problem to a convex\nmatrix sensing task with nuclear norm penalization. This reveals that capacity\ncontrol in such networks emerges from a low-rank structure in the learned\nfeature maps. Our results characterize the global minima of the loss and yield\nprecise generalization thresholds, showing how the width of the target function\ngoverns learnability. This analysis bridges and extends ideas from spin-glass\nmethods, matrix factorization, and convex optimization and emphasizes the deep\nlink between low-rank matrix sensing and learning in quadratic neural networks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "relevance": 1.0}}
{"id": "2505.17965", "pdf": "https://arxiv.org/pdf/2505.17965", "abs": "https://arxiv.org/abs/2505.17965", "authors": ["Daniel Cortild", "Lucas Ketels", "Juan Peypouquet", "Guillaume Garrigos"], "title": "New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "57 pages, 10 figures. Under review", "summary": "The analysis of Stochastic Gradient Descent (SGD) often relies on making some\nassumption on the variance of the stochastic gradients, which is usually not\nsatisfied or difficult to verify in practice. This paper contributes to a\nrecent line of works which attempt to provide guarantees without making any\nvariance assumption, leveraging only the (strong) convexity and smoothness of\nthe loss functions. In this context, we prove new theoretical bounds derived\nfrom the monotonicity of a simple Lyapunov energy, improving the current\nstate-of-the-art and extending their validity to larger step-sizes. Our\ntheoretical analysis is backed by a Performance Estimation Problem analysis,\nwhich allows us to claim that, empirically, the bias term in our bounds is\ntight within our framework.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7684\u7406\u8bba\u754c\u9650\uff0c\u65e0\u9700\u5047\u8bbe\u68af\u5ea6\u65b9\u5dee\uff0c\u4ec5\u5229\u7528\u635f\u5931\u51fd\u6570\u7684\u51f8\u6027\u548c\u5e73\u6ed1\u6027\uff0c\u5e76\u901a\u8fc7Lyapunov\u80fd\u91cf\u5355\u8c03\u6027\u8bc1\u660e\u3002", "motivation": "\u73b0\u6709SGD\u5206\u6790\u901a\u5e38\u4f9d\u8d56\u96be\u4ee5\u9a8c\u8bc1\u7684\u68af\u5ea6\u65b9\u5dee\u5047\u8bbe\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u65e0\u9700\u6b64\u7c7b\u5047\u8bbe\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5229\u7528Lyapunov\u80fd\u91cf\u5355\u8c03\u6027\u63a8\u5bfc\u65b0\u7406\u8bba\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u6027\u80fd\u4f30\u8ba1\u95ee\u9898\u5206\u6790\u9a8c\u8bc1\u5176\u7d27\u81f4\u6027\u3002", "result": "\u7406\u8bba\u754c\u9650\u4f18\u4e8e\u73b0\u6709\u6210\u679c\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u6b65\u957f\uff0c\u4e14\u504f\u5dee\u9879\u5728\u6846\u67b6\u5185\u7d27\u81f4\u3002", "conclusion": "\u672c\u6587\u4e3aSGD\u63d0\u4f9b\u4e86\u66f4\u666e\u9002\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u573a\u666f\u3002", "relevance": 40.0}}
{"id": "2505.17999", "pdf": "https://arxiv.org/pdf/2505.17999", "abs": "https://arxiv.org/abs/2505.17999", "authors": ["Honghao Li", "Yiwen Zhang", "Yi Zhang", "Lei Sang", "Jieming Zhu"], "title": "Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction", "categories": ["cs.IR", "cs.LG"], "comment": "KDD'25 accepted", "summary": "Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)\nprediction tasks due to its simplicity, effectiveness, and ability to capture\nfeature interactions without additional parameters. However, the underlying\nreasons for its effectiveness remain unclear. In this paper, we revisit HP from\nthe perspective of Quadratic Neural Networks (QNN), which leverage quadratic\ninteraction terms to model complex feature relationships. We further reveal\nQNN's ability to expand the feature space and provide smooth nonlinear\napproximations without relying on activation functions. Meanwhile, we find that\ntraditional post-activation does not further improve the performance of the\nQNN. Instead, mid-activation is a more suitable alternative. Through\ntheoretical analysis and empirical evaluation of 25 QNN neuron formats, we\nidentify a good-performing variant and make further enhancements on it.\nSpecifically, we propose the Multi-Head Khatri-Rao Product as a superior\nalternative to HP and a Self-Ensemble Loss with dynamic ensemble capability\nwithin the same network to enhance computational efficiency and performance.\nUltimately, we propose a novel neuron format, QNN-alpha, which is tailored for\nCTR prediction tasks. Experimental results show that QNN-alpha achieves new\nstate-of-the-art performance on six public datasets while maintaining low\ninference latency, good scalability, and excellent compatibility. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/QNN.", "AI": {"tldr": "\u672c\u6587\u4ece\u4e8c\u6b21\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u7684\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6Hadamard\u4e58\u79ef\uff08HP\uff09\uff0c\u63ed\u793a\u5176\u901a\u8fc7\u4e8c\u6b21\u4ea4\u4e92\u9879\u5efa\u6a21\u7279\u5f81\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684QNN-alpha\u795e\u7ecf\u5143\u683c\u5f0f\uff0c\u5728CTR\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22HP\u5728CTR\u9884\u6d4b\u4e2d\u6709\u6548\u6027\u80cc\u540e\u7684\u539f\u56e0\uff0c\u5e76\u57fa\u4e8eQNN\u63d0\u51fa\u66f4\u4f18\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c25\u79cdQNN\u795e\u7ecf\u5143\u683c\u5f0f\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63d0\u51faMulti-Head Khatri-Rao Product\u548cSelf-Ensemble Loss\uff0c\u6700\u7ec8\u8bbe\u8ba1QNN-alpha\u3002", "result": "QNN-alpha\u5728\u516d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u826f\u597d\u517c\u5bb9\u6027\u3002", "conclusion": "QNN-alpha\u662fCTR\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u6027\u80fd\u548c\u6548\u7387\u3002", "relevance": 40.0}}
{"id": "2505.18000", "pdf": "https://arxiv.org/pdf/2505.18000", "abs": "https://arxiv.org/abs/2505.18000", "authors": ["Valentin Kilian", "Stefano Cortinovis", "Fran\u00e7ois Caron"], "title": "Anytime-valid, Bayes-assisted,Prediction-Powered Inference", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Given a large pool of unlabelled data and a smaller amount of labels,\nprediction-powered inference (PPI) leverages machine learning predictions to\nincrease the statistical efficiency of standard confidence interval procedures\nbased solely on labelled data, while preserving their fixed-time validity.\n  In this paper, we extend the PPI framework to the sequential setting, where\nlabelled and unlabelled datasets grow over time.\n  Exploiting Ville's inequality and the method of mixtures, we propose\nprediction-powered confidence sequence procedures that are valid uniformly over\ntime and naturally accommodate prior knowledge on the quality of the\npredictions to further boost efficiency.\n  We carefully illustrate the design choices behind our method and demonstrate\nits effectiveness in real and synthetic examples.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u9884\u6d4b\u9a71\u52a8\u63a8\u7406\uff08PPI\uff09\u6846\u67b6\u81f3\u5e8f\u5217\u8bbe\u7f6e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVille\u4e0d\u7b49\u5f0f\u548c\u6df7\u5408\u65b9\u6cd5\u7684\u7f6e\u4fe1\u5e8f\u5217\u7a0b\u5e8f\uff0c\u63d0\u5347\u4e86\u7edf\u8ba1\u6548\u7387\u5e76\u4fdd\u6301\u56fa\u5b9a\u65f6\u95f4\u6709\u6548\u6027\u3002", "motivation": "\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u63d0\u5347\u7edf\u8ba1\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u56fa\u5b9a\u65f6\u95f4\u6709\u6548\u6027\u3002", "method": "\u6269\u5c55PPI\u6846\u67b6\u81f3\u5e8f\u5217\u8bbe\u7f6e\uff0c\u5229\u7528Ville\u4e0d\u7b49\u5f0f\u548c\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\u7f6e\u4fe1\u5e8f\u5217\u7a0b\u5e8f\uff0c\u5e76\u6574\u5408\u9884\u6d4b\u8d28\u91cf\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u7edf\u8ba1\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e8f\u5217\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 40.0}}
{"id": "2505.18008", "pdf": "https://arxiv.org/pdf/2505.18008", "abs": "https://arxiv.org/abs/2505.18008", "authors": ["Thomas Oliver de Jong", "Khemraj Shukla", "Mircea Lazar"], "title": "Deep Operator Neural Network Model Predictive Control", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "In this paper, we consider the design of model predictive control (MPC)\nalgorithms based on deep operator neural networks (DeepONets). These neural\nnetworks are capable of accurately approximating real and complex valued\nsolutions of continuous time nonlinear systems without relying on recurrent\narchitectures. The DeepONet architecture is made up of two feedforward neural\nnetworks: the branch network, which encodes the input function space, and the\ntrunk network, which represents dependencies on temporal variables or initial\nconditions. Utilizing the original DeepONet architecture as a predictor within\nMPC for Multi Input Multi Output (MIMO) systems requires multiple branch\nnetworks, to generate multi output predictions, one for each input. Moreover,\nto predict multiple time steps into the future, the network has to be evaluated\nmultiple times. Motivated by this, we introduce a multi step DeepONet\n(MS-DeepONet) architecture that computes in one shot multi step predictions of\nsystem outputs from multi step input sequences, which is better suited for MPC.\nWe prove that the MS DeepONet is a universal approximator in terms of multi\nstep sequence prediction. Additionally, we develop automated hyper parameter\nselection strategies and implement MPC frameworks using both the standard\nDeepONet and the proposed MS DeepONet architectures in PyTorch. The\nimplementation is publicly available on GitHub. Simulation results demonstrate\nthat MS-DeepONet consistently outperforms the standard DeepONet in learning and\npredictive control tasks across several nonlinear benchmark systems: the van\nder Pol oscillator, the quadruple tank process, and a cart pendulum unstable\nsystem, where it successfully learns and executes multiple swing up and\nstabilization policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeepONet\u7684\u591a\u6b65\u9884\u6d4b\u6a21\u578b\uff08MS-DeepONet\uff09\uff0c\u7528\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u6807\u51c6DeepONet\u5728MPC\u4e2d\u9700\u8981\u591a\u6b21\u8bc4\u4f30\u4ee5\u5b9e\u73b0\u591a\u6b65\u9884\u6d4b\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51faMS-DeepONet\u4ee5\u4e00\u6b21\u6027\u5b8c\u6210\u591a\u6b65\u9884\u6d4b\uff0c\u66f4\u9002\u5408MPC\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86MS-DeepONet\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u652f\u7f51\u7edc\u548c\u4e3b\u5e72\u7f51\u7edc\u5b9e\u73b0\u591a\u6b65\u8f93\u5165\u5e8f\u5217\u5230\u591a\u6b65\u8f93\u51fa\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u8d85\u53c2\u6570\u9009\u62e9\u7b56\u7565\u3002", "result": "MS-DeepONet\u5728\u591a\u4e2a\u975e\u7ebf\u6027\u7cfb\u7edf\uff08\u5982van der Pol\u632f\u8361\u5668\u3001\u56db\u6c34\u7bb1\u8fc7\u7a0b\u548c\u4e0d\u7a33\u5b9a\u5012\u7acb\u6446\u7cfb\u7edf\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6DeepONet\u3002", "conclusion": "MS-DeepONet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6b65\u9884\u6d4b\u67b6\u6784\uff0c\u9002\u7528\u4e8eMPC\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002", "relevance": 30.0}}
{"id": "2505.18077", "pdf": "https://arxiv.org/pdf/2505.18077", "abs": "https://arxiv.org/abs/2505.18077", "authors": ["Daniel F. Villarraga", "Ricardo A. Daziano"], "title": "Bayesian Deep Learning for Discrete Choice", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.AP"], "comment": null, "summary": "Discrete choice models (DCMs) are used to analyze individual decision-making\nin contexts such as transportation choices, political elections, and consumer\npreferences. DCMs play a central role in applied econometrics by enabling\ninference on key economic variables, such as marginal rates of substitution,\nrather than focusing solely on predicting choices on new unlabeled data.\nHowever, while traditional DCMs offer high interpretability and support for\npoint and interval estimation of economic quantities, these models often\nunderperform in predictive tasks compared to deep learning (DL) models. Despite\ntheir predictive advantages, DL models remain largely underutilized in discrete\nchoice due to concerns about their lack of interpretability, unstable parameter\nestimates, and the absence of established methods for uncertainty\nquantification. Here, we introduce a deep learning model architecture\nspecifically designed to integrate with approximate Bayesian inference methods,\nsuch as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model\ncollapses to behaviorally informed hypotheses when data is limited, mitigating\noverfitting and instability in underspecified settings while retaining the\nflexibility to capture complex nonlinear relationships when sufficient data is\navailable. We demonstrate our approach using SGLD through a Monte Carlo\nsimulation study, evaluating both predictive metrics--such as out-of-sample\nbalanced accuracy--and inferential metrics--such as empirical coverage for\nmarginal rates of substitution interval estimates. Additionally, we present\nresults from two empirical case studies: one using revealed mode choice data in\nNYC, and the other based on the widely used Swiss train choice stated\npreference data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u4e0d\u8db3\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u9009\u62e9\u6a21\u578b\uff08DCMs\uff09\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u9884\u6d4b\u80fd\u529b\u5f3a\u4f46\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u6027\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\uff08\u5982SGLD\uff09\uff0c\u5728\u6570\u636e\u6709\u9650\u65f6\u56de\u5f52\u884c\u4e3a\u5047\u8bbe\uff0c\u6570\u636e\u5145\u8db3\u65f6\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u5b9e\u8bc1\u6848\u4f8b\uff08\u5982\u7ebd\u7ea6\u4ea4\u901a\u6a21\u5f0f\u9009\u62e9\u6570\u636e\uff09\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u9884\u6d4b\u548c\u63a8\u65ad\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4fdd\u6301\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u79bb\u6563\u9009\u62e9\u5206\u6790\u3002", "relevance": 30.0}}
{"id": "2505.18118", "pdf": "https://arxiv.org/pdf/2505.18118", "abs": "https://arxiv.org/abs/2505.18118", "authors": ["Aidan Gleich", "Eric Laber", "Alexander Volfovsky"], "title": "Scalable Policy Maximization Under Network Interference", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Many interventions, such as vaccines in clinical trials or coupons in online\nmarketplaces, must be assigned sequentially without full knowledge of their\neffects. Multi-armed bandit algorithms have proven successful in such settings.\nHowever, standard independence assumptions fail when the treatment status of\none individual impacts the outcomes of others, a phenomenon known as\ninterference. We study optimal-policy learning under interference on a dynamic\nnetwork. Existing approaches to this problem require repeated observations of\nthe same fixed network and struggle to scale in sample size beyond as few as\nfifteen connected units -- both limit applications. We show that under common\nassumptions on the structure of interference, rewards become linear. This\nenables us to develop a scalable Thompson sampling algorithm that maximizes\npolicy impact when a new $n$-node network is observed each round. We prove a\nBayesian regret bound that is sublinear in $n$ and the number of rounds.\nSimulation experiments show that our algorithm learns quickly and outperforms\nexisting methods. The results close a key scalability gap between causal\ninference methods for interference and practical bandit algorithms, enabling\npolicy optimization in large-scale networked systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u52a8\u6001\u7f51\u7edc\u5e72\u6270\u4e0b\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u7684Thompson\u91c7\u6837\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6837\u672c\u89c4\u6a21\u548c\u7f51\u7edc\u89c4\u6a21\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u5728\u5e72\u6270\uff08\u4e2a\u4f53\u95f4\u76f8\u4e92\u5f71\u54cd\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "method": "\u5728\u5e72\u6270\u7ed3\u6784\u5047\u8bbe\u4e0b\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u4e3a\u7ebf\u6027\u5f62\u5f0f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684Thompson\u91c7\u6837\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u6837\u672c\u89c4\u6a21\u548c\u7f51\u7edc\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u5176\u5b66\u4e60\u901f\u5ea6\u5feb\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u586b\u8865\u4e86\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u4e0e\u5b9e\u9645\u8001\u864e\u673a\u7b97\u6cd5\u4e4b\u95f4\u7684\u53ef\u6269\u5c55\u6027\u5dee\u8ddd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\u3002", "relevance": 40.0}}
