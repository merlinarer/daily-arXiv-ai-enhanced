<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.CV](#cs.CV) [Total: 162]
- [cs.AI](#cs.AI) [Total: 138]
- [cs.LG](#cs.LG) [Total: 170]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
*Jingfeng Chen,Raghuveer Thirukovalluru,Junlin Wang,Kaiwei Luo,Bhuwan Dhingra*

Main category: cs.CL

Relevance: 90.0

TL;DR: 论文提出了一种自监督的偏好优化方法ACPO，通过利用原子一致性信号（多个随机响应中事实的一致性）来提升LLMs的事实准确性，无需依赖外部模型或知识库。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs产生事实性幻觉的问题，传统方法依赖外部模型或知识库，ACPO旨在提供一种无需外部监督的解决方案。

Method: ACPO利用原子一致性信号识别高质量和低质量的数据对，用于模型对齐，无需外部监督。

Result: ACPO在LongFact和BioGen数据集上比监督基线FactAlign高出1.95分，展示了其有效性。

Conclusion: ACPO是一种高效、可扩展的方法，能够显著提升LLMs的事实可靠性。

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [2] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
*An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jing Zhou,Jingren Zhou,Junyang Lin,Kai Dang,Keqin Bao,Kexin Yang,Le Yu,Lianghao Deng,Mei Li,Mingfeng Xue,Mingze Li,Pei Zhang,Peng Wang,Qin Zhu,Rui Men,Ruize Gao,Shixuan Liu,Shuang Luo,Tianhao Li,Tianyi Tang,Wenbiao Yin,Xingzhang Ren,Xinyu Wang,Xinyu Zhang,Xuancheng Ren,Yang Fan,Yang Su,Yichang Zhang,Yinger Zhang,Yu Wan,Yuqiong Liu,Zekun Wang,Zeyu Cui,Zhenru Zhang,Zhipeng Zhou,Zihan Qiu*

Main category: cs.CL

Relevance: 90.0

TL;DR: Qwen3是Qwen模型家族的最新版本，包含密集和MoE架构的LLM，参数规模从0.6B到235B。创新包括动态思维模式切换和思维预算机制，显著提升了多语言能力和性能。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的性能、效率和多语言能力，同时解决推理和快速响应之间的动态切换问题。

Method: 结合思维模式和非思维模式的统一框架，引入思维预算机制，优化计算资源分配，并通过知识迁移减少小规模模型的资源需求。

Result: 在代码生成、数学推理等任务上达到SOTA，支持119种语言，性能优于前代和同类模型。

Conclusion: Qwen3通过创新设计和高效资源利用，在多语言和复杂任务中表现出色，且开源以促进社区研究。

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [3] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
*Jingfeng Chen,Raghuveer Thirukovalluru,Junlin Wang,Kaiwei Luo,Bhuwan Dhingra*

Main category: cs.CL

Relevance: 90.0

TL;DR: 论文提出了一种名为ACPO的自监督偏好优化方法，用于提升大语言模型的事实准确性，无需依赖外部监督或知识库。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生看似合理但错误的事实幻觉，现有方法依赖强大模型或外部知识库，成本高且不可靠。ACPO旨在解决这一问题。

Method: ACPO利用原子一致性信号（即多个随机响应中个体事实的一致性）识别高质量和低质量数据对，进行自监督模型对齐。

Result: ACPO在LongFact和BioGen数据集上比监督基线FactAlign高出1.95分，展示了其有效性。

Conclusion: ACPO提供了一种高效、可扩展的方法，显著提升模型的事实可靠性，且无需外部资源。

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [4] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
*An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jing Zhou,Jingren Zhou,Junyang Lin,Kai Dang,Keqin Bao,Kexin Yang,Le Yu,Lianghao Deng,Mei Li,Mingfeng Xue,Mingze Li,Pei Zhang,Peng Wang,Qin Zhu,Rui Men,Ruize Gao,Shixuan Liu,Shuang Luo,Tianhao Li,Tianyi Tang,Wenbiao Yin,Xingzhang Ren,Xinyu Wang,Xinyu Zhang,Xuancheng Ren,Yang Fan,Yang Su,Yichang Zhang,Yinger Zhang,Yu Wan,Yuqiong Liu,Zekun Wang,Zeyu Cui,Zhenru Zhang,Zhipeng Zhou,Zihan Qiu*

Main category: cs.CL

Relevance: 90.0

TL;DR: Qwen3是Qwen模型家族的最新版本，包含密集和MoE架构的大语言模型，参数规模从0.6到2350亿。创新点包括动态切换思维模式和非思维模式、思维预算机制，以及通过知识迁移减少小模型训练资源。在多语言支持、推理任务和性能上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的性能、效率和多语言能力，同时解决模型切换和资源分配问题。

Method: 采用密集和MoE架构，引入动态思维模式切换和思维预算机制，利用知识迁移优化小模型训练。

Result: 在多语言支持（扩展到119种语言）、代码生成、数学推理等任务上表现优异，性能超越前代和同类模型。

Conclusion: Qwen3通过创新设计和高效资源利用，实现了高性能和多语言支持，为社区提供了开源模型。

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [5] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文重新评估了人类与大型语言模型（LLMs）在语言理解任务中的表现，发现人类表现被高估，而LLMs能力被低估。实验显示，限制重读后人类准确率下降，低于Falcon-180B-Chat和GPT-4，而GPT-o1表现完美。研究还揭示了人类和模型在特定查询上的共同挑战，并指出LLM评估中实验设计和编码实践的重要性。


<details>
  <summary>Details</summary>
Motivation: 重新验证人类与LLMs在语言理解任务中的表现差异，挑战现有研究中人类表现优于LLMs的假设。

Method: 采用预注册研究设计，比较人类在允许重读和限制重读条件下的表现，并与多个LLMs（如Falcon-180B-Chat、GPT-4、GPT-o1）进行对比。使用Llama-2-70B对数概率、模型响应重编码和语法评分进行额外分析。

Result: 限制重读后人类准确率降至73%，低于Falcon-180B-Chat（76%）和GPT-4（81%），GPT-o1达到100%。人类和模型在涉及互惠动作的查询中表现较差。

Conclusion: LLMs的语言理解能力被低估，实验设计和编码实践对评估结果有重要影响。当前模型在语言理解上未必弱于人类。

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [6] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
*Brandon Smith,Mohamed Reda Bouadjenek,Tahsin Alamgir Kheya,Phillip Dawson,Sunil Aryal*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了12种LLM生成的文本相似性、多样性和伦理表现，发现同一模型生成的文本相似度高，不同模型间差异显著，且部分模型在伦理表现上更优。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成文本的相似性、多样性和伦理问题，为模型开发和伦理评估提供依据。

Method: 使用5,000个多样化提示生成约300万文本，比较12种LLM（包括开源和专有模型）的输出。

Result: 同一LLM生成的文本相似度高；不同模型间输出差异显著（如GPT-4更独特）；部分模型在性别平衡和减少偏见上表现更好。

Conclusion: 研究揭示了LLM输出的行为和多样性，为未来开发和伦理评估提供了新视角。

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [7] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
*Sophie Zhang,Zhiming Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出CEC-Zero，一种基于强化学习的框架，使LLM能够自主学习和纠正中文拼写错误，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在中文拼写校正中的可靠性和泛化性问题。

Method: 结合强化学习与LLM的生成能力，自主学习和纠正错误，无需标注数据或辅助模型。

Result: 实验显示RL增强的LLM在准确性和跨域泛化性上表现优异，适用于实际中文NLP应用。

Conclusion: CEC-Zero为LLM在中文文本校正中的可靠性优化提供了可扩展方案，并为自改进语言模型建立了新范式。

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [8] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: InForage是一个基于强化学习的框架，通过动态检索增强LLMs的推理能力，优于传统静态检索方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态检索方法在复杂任务中的不足，利用动态检索适应模糊、多步或变化的信息需求。

Method: 基于信息觅食理论（IFT），提出InForage框架，通过强化学习奖励中间检索质量，实现动态信息搜索。

Result: 在通用问答、多跳推理和实时网络问答任务中表现优于基线方法。

Conclusion: InForage能构建鲁棒、自适应且高效的推理代理。

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [9] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
*Jingcheng Niu,Xingdi Yuan,Tong Wang,Hamidreza Saghir,Amir H. Abdi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文发现了一种名为“上下文牵引”的现象，即语言模型（LMs）会显著提高输入提示中已出现过的token的logits或概率，即使这些token是随机的。作者提出了一种基于可微分掩码的方法来识别与这一现象相关的注意力头（“牵引头”），并发现关闭这些头可以显著减弱上下文牵引效应。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在输入提示中被无关上下文信息分散注意力的机制，为理解模型行为提供新的视角。

Method: 通过统计分析和可微分掩码方法识别与上下文牵引现象相关的注意力头（牵引头），并通过实验验证关闭这些头对模型输出的影响。

Result: 发现上下文牵引是一种机制性现象，其强度受语义因素调节；关闭牵引头可显著减弱这一效应。

Conclusion: 上下文牵引现象的发现及其机制分析为解决语言模型注意力分散问题提供了关键步骤。

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [10] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

Relevance: 85.0

TL;DR: PT-MoE框架通过结合矩阵分解和MoE路由，在参数高效微调中实现了跨任务一致性和泛化能力，显著提升了QA和数学任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法存在效率与性能不匹配的问题，PT-MoE旨在通过模块化设计解决这一问题。

Method: 结合矩阵分解和MoE路由，提出PT-MoE框架，实现高效参数共享和动态适应。

Result: 在17个数据集上，PT-MoE在QA和数学任务中性能最优，参数使用比LoRA少25%。

Conclusion: PT-MoE展示了跨任务一致性和泛化能力，为未来PEFT方法提供了新思路。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [11] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
*Abdullah Mushtaq,Imran Taj,Rafay Naeem,Ibrahim Ghaznavi,Junaid Qadir*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了WorldView-Bench，一个评估LLMs全球文化包容性（GCI）的基准，通过分析其适应多元世界观的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs训练和评估框架偏向西方中心主义，导致文化同质化，缺乏全球多元性。

Method: 基于Multiplex Worldview理论，设计两种干预策略：上下文实现和多代理系统实现的多路复用LLMs。

Result: 多代理系统实现的多路复用LLMs显著提高了视角分布熵（PDS）至94%，并改善了文化平衡和情感倾向。

Conclusion: 多路复用感知的AI评估有助于减少LLMs中的文化偏见，推动更包容和伦理对齐的AI系统。

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


### [12] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文反驳了LLMs在理解简单英语语句上表现不如人类的观点，通过实验证明人类表现被高估，而LLM能力被低估。GPT-4o等模型在某些条件下表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 重新评估LLMs在语言理解任务中的表现，挑战现有关于LLM能力不足的假设。

Method: 采用预注册研究设计，比较人类在允许重读和限制重读条件下的表现，并与Falcon-180B-Chat、GPT-4等模型对比。

Result: 人类在限制重读条件下准确率下降至73%，低于Falcon-180B-Chat（76%）和GPT-4（81%）。GPT-4o表现完美。

Conclusion: 实验设计和方法对LLM评估至关重要，当前模型在语言理解上未必弱于人类。

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [13] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
*Brandon Smith,Mohamed Reda Bouadjenek,Tahsin Alamgir Kheya,Phillip Dawson,Sunil Aryal*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究了12种大型语言模型（LLM）在生成文本时的相似性、多样性和伦理表现，发现不同模型在输出风格、词汇选择和伦理标准上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成文本的相似性、多样性及伦理影响，为模型开发和伦理评估提供依据。

Method: 使用5,000个提示生成约300万文本，覆盖12种LLM（包括专有和开源模型），分析其输出相似性、多样性和伦理表现。

Result: 发现同一LLM的输出相似性高于人类文本；不同模型（如WizardLM-2-8x22b和GPT-4）在多样性和风格上差异显著；部分模型在性别平衡和偏见减少上表现更好。

Conclusion: 研究揭示了LLM输出的行为多样性，为未来模型开发和伦理评估提供了重要参考。

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [14] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
*Sophie Zhang,Zhiming Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: CEC-Zero是一种基于强化学习的框架，使LLM能够自主学习和纠正中文拼写错误，无需外部监督，显著提升了准确性和跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在中文拼写纠错（CSC）中表现出色，但其可靠性和泛化能力仍有不足。本文旨在通过强化学习优化LLM，解决这些问题。

Method: 提出CEC-Zero框架，结合强化学习与LLM的生成能力，无需标注数据或辅助模型，实现自主错误策略学习。

Result: 实验表明，RL增强的LLM在中文拼写纠错中达到行业可用准确性，并具有优越的跨领域泛化能力。

Conclusion: CEC-Zero为中文NLP应用的可靠性优化提供了可扩展方案，并为自改进语言模型建立了新范式。

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [15] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: InForage是一个基于强化学习的框架，用于动态检索增强的LLM推理，通过迭代检索和整合信息提升复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态检索方法在复杂任务中的不足，利用动态检索增强LLM的能力。

Method: 提出InForage框架，基于信息觅食理论，通过强化学习优化检索行为，并构建人类引导的数据集进行训练。

Result: 在通用问答、多跳推理和实时网络问答任务中表现优于基线方法。

Conclusion: InForage能构建高效、自适应的推理代理，适用于复杂信息需求场景。

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [16] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
*Jingcheng Niu,Xingdi Yuan,Tong Wang,Hamidreza Saghir,Amir H. Abdi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文发现语言模型存在一种新现象——上下文牵引（contextual entrainment），即模型会显著提高输入提示中已出现过的token的logit值，即使这些token与问题无关。研究发现这一现象受语义因素影响，并通过可微分掩码方法识别了相关的注意力头（entrainment heads）。关闭这些头可以显著减弱上下文牵引效应。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在处理输入提示时如何被无关上下文信息分散注意力，以揭示其内部机制并提出缓解方法。

Method: 通过统计分析验证上下文牵引现象，并使用可微分掩码方法识别对应的注意力头（entrainment heads）。

Result: 发现上下文牵引是一种机制性现象，受语义因素影响；关闭相关注意力头可显著减弱该效应。

Conclusion: 上下文牵引现象的发现为语言模型的机制分析和注意力分散问题提供了新视角。

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [17] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

Relevance: 85.0

TL;DR: PT-MoE框架结合矩阵分解和MoE路由，提升了参数高效微调的性能，在QA和数学任务中表现优异，同时减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法存在效率与性能不匹配的问题，PT-MoE旨在通过矩阵分解和MoE路由解决这些问题。

Method: 提出PT-MoE框架，结合矩阵分解和MoE路由，实现参数共享和动态适应。

Result: 在17个数据集上，PT-MoE在QA和数学任务中均优于现有方法，参数减少25%。

Conclusion: PT-MoE展示了跨任务一致性和泛化能力，为未来PEFT方法提供了新思路。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [18] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
*Abdullah Mushtaq,Imran Taj,Rafay Naeem,Ibrahim Ghaznavi,Junaid Qadir*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出WorldView-Bench，用于评估LLMs的全球文化包容性（GCI），通过多视角生成式评估减少文化偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs训练和评估框架偏向西方中心主义，导致文化同质化，缺乏全球多样性。

Method: 基于Multiplex Worldview理论，提出两种干预策略：上下文嵌入多视角和多代理系统协作生成。

Result: 多代理系统将视角分布熵从13%提升至94%，情感倾向转为67.7%正面，文化平衡性增强。

Conclusion: 多视角评估方法可有效减少LLMs的文化偏见，推动更包容的AI发展。

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


### [19] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文探讨了语言模型（如GPT-4）是否能理解和生成自然语言的元语言判断，并验证了其是否能捕捉形式与功能之间的微妙关系。通过实验发现，GPT-4在信息结构和可接受性任务中表现出可靠的元语言能力，且与人类行为一致。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能够理解和生成自然语言的元语言判断，并验证其是否能捕捉形式与功能之间的微妙关系，以评估模型的深度语言理解能力。

Method: 通过实验设计，使用GPT-4在零样本条件下完成信息结构和可接受性任务，并分析其表现。研究还通过操纵信息结构验证因果关系。

Result: GPT-4在信息结构和可接受性任务中表现出可靠的元语言能力，且与人类行为一致。研究还发现信息结构对可接受性评分的因果影响。

Conclusion: GPT-4能够捕捉自然语言中的微妙关系，表明其在语言理解方面的潜力，值得进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [20] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文探讨了语言模型（如GPT-4）是否能理解和生成可靠的元语言判断，并验证了其在信息结构和句法关系上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能够捕捉人类语言中形式与功能的微妙关系，尤其是信息结构与长距离依赖（LDD）构造之间的关系。

Method: 通过零样本任务测试GPT-4在信息结构和LDD构造上的表现，并验证其与人类判断的一致性。

Result: GPT-4展示了可靠的元语言能力，能够复制人类在信息结构和LDD构造上的交互作用，并验证了信息结构对LDD构造的因果影响。

Conclusion: GPT-4与自然英语之间存在紧密关系，信息结构与句法之间的关系值得进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [21] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

Relevance: 60.0

TL;DR: S-DAT是一个基于大语言模型的多语言框架，用于自动化评估发散思维（DT），解决了传统创造力评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统创造力评估方法耗时、语言依赖性强且主观，限制了其可扩展性和跨文化适用性。S-DAT旨在通过大语言模型和多语言嵌入技术解决这些问题。

Method: 利用大语言模型和多语言嵌入计算语义距离，作为DT的语言无关代理指标。在11种语言中评估其表现。

Result: S-DAT在多语言环境中表现出稳健和一致的评分，与其他DT测量方法具有收敛效度，并能区分收敛思维。

Conclusion: S-DAT为全球范围内的创造力研究提供了更公平、更全面的评估工具。

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [22] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

Relevance: 50.0

TL;DR: QEDACVC提出了一种基于量子计算的编码器-解码器架构，用于多语言机器翻译，在OPUS数据集上达到82%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在多语言机器翻译中的应用，以替代传统的经典计算方法。

Method: 采用量子编码器-解码器架构，结合量子卷积、量子池化、量子变分电路和量子注意力机制，在量子硬件上实现。

Result: 在英语、法语、德语和印地语的多语言翻译任务中，准确率达到82%。

Conclusion: QEDACVC展示了量子计算在多语言机器翻译中的潜力，为未来研究提供了新方向。

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [23] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

Relevance: 40.0

TL;DR: 研究利用作者验证技术量化AI在学术写作中的辅助作用，强调透明度和学生发展，通过改进的方法有效区分学生与AI生成文本。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作在教育中的普及，需要透明且可解释的方法来量化AI辅助，以支持学术诚信和学生发展。

Method: 研究分为三个阶段：数据集选择与扩展、作者验证方法开发、系统评估。采用改进的特征向量差异方法构建学生写作特征。

Result: 改进的分类器能有效识别风格差异，量化人机协作，并为教育者提供透明工具。

Conclusion: 该研究推动了作者验证技术，为AI时代的学术写作提供了实用见解。

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [24] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

Relevance: 40.0

TL;DR: 比较静态和动态叙事教育游戏对学习动机的影响，发现动态叙事能提升参与度，但需平衡教学目标和叙事动态性。


<details>
  <summary>Details</summary>
Motivation: 探索动态叙事对学习动机的影响，填补AI驱动动态叙事在教育游戏中的研究空白。

Method: 比较两种版本的Academical游戏：静态分支叙事和动态序列叙事。

Result: 动态叙事提升玩家参与度，但需平衡教学目标和叙事动态性。

Conclusion: AI驱动的动态叙事在教育游戏中具有潜力，但需进一步优化设计。

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [25] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

Relevance: 40.0

TL;DR: S-DAT是一个基于大语言模型的多语言框架，用于自动化评估发散思维（DT），克服了传统创造力评估的局限。


<details>
  <summary>Details</summary>
Motivation: 传统创造力评估方法耗时长、语言依赖性强且主观，难以大规模应用和跨文化研究。

Method: 利用大语言模型和多语言嵌入计算语义距离，作为DT的语言无关代理指标。

Result: 在11种语言中验证了S-DAT的稳健性和一致性，并展示了与其他DT测量的收敛效度。

Conclusion: S-DAT为全球范围的创造力研究提供了更公平、全面的工具。

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [26] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
*Jiin Park,Misuk Kim*

Main category: cs.CL

Relevance: 40.0

TL;DR: 提出了一种多语言、可扩展、无监督的跨领域方面检测框架，用于多语言和多领域评论数据的多标签标注。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究局限于特定领域和语言，或依赖大规模标注数据的监督学习方法的不足。

Method: 通过聚类提取方面类别候选，使用负采样生成方面感知嵌入向量，并微调预训练语言模型评估自动生成标签的有效性。

Result: 模型表现优异，自动生成标签质量与人工标注相当，框架在一致性和可扩展性上优于公开大语言模型。

Conclusion: 该框架克服了监督方法的限制，适用于多语言、多领域环境，未来将探索自动评论摘要和AI代理集成。

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [27] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种基于量子计算的编码器-解码器架构QEDACVC，用于多语言机器翻译，相比传统计算模型，其准确率达到82%。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在多语言机器翻译中的应用，以替代传统的基于经典计算的语言模型。

Method: QEDACVC采用量子编码器-解码器架构，结合量子卷积、量子池化、量子变分电路和量子注意力机制，在量子硬件上运行。

Result: 在OPUS数据集上训练后，QEDACVC在英语、法语、德语和印地语的多语言翻译任务中达到82%的准确率。

Conclusion: 量子计算在多语言机器翻译中具有潜力，QEDACVC为这一领域提供了新的研究方向。

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [28] [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
*Zekun Wu,Seonglae Cho,Umar Mohammed,Cristian Munoz,Kleyton Costa,Xin Guan,Theo King,Ze Wang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CR

Relevance: 40.0

TL;DR: LibVulnWatch是一个基于图的框架，用于评估开源AI库的风险，包括安全、许可和维护等方面。


<details>
  <summary>Details</summary>
Motivation: 开源AI库在现代AI系统中至关重要，但其风险（如安全漏洞、许可问题）未被充分研究。

Method: 使用LangGraph构建有向无环图，协调多个专业代理从可信来源提取、验证和量化风险。

Result: 评估了20个常用库，覆盖88%的OpenSSF Scorecard检查，发现每个库最多19个额外风险。

Conclusion: LibVulnWatch为AI治理提供了可扩展、透明的供应链风险评估工具。

Abstract: Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.

</details>


### [29] [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
*Dominic Zaun Eu Jones*

Main category: econ.GN

Relevance: 40.0

TL;DR: Ornithologist是一个弱监督文本分类系统，用于测量央行文本的鹰派和鸽派倾向，通过结合人类编写的决策树和大型语言模型提高透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发一个透明、可解释且适用于非专家的文本分类系统，减少幻觉风险，并能够轻松扩展到其他文本源。

Method: 使用“分类学引导推理”方法，结合人类编写的决策树和大型语言模型。

Result: Ornithologist对RBA通信的鹰派和鸽派测量结果包含对未来现金利率路径和市场预期的信息。

Conclusion: Ornithologist是一种高效、透明的文本分类工具，适用于央行文本分析，并具有扩展潜力。

Abstract: I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.

</details>


### [30] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

Relevance: 40.0

TL;DR: 研究探讨了如何利用作者验证（AV）技术量化AI在学术写作中的辅助作用，旨在提升透明度和学生发展。通过数据集扩展和方法开发，研究展示了AV分类器在识别AI生成文本和测量人机协作方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作在教育中的普及，量化AI辅助并促进透明度和学生发展成为重要挑战。

Method: 研究分为三个阶段：数据集选择与扩展、AV方法开发、系统评估。使用了三个数据集，包括公共数据集和学生写作数据，开发了改进的Feature Vector Difference AV方法。

Result: AV分类器能有效识别AI生成文本与学生写作的差异，并在测量人机协作方面表现良好。

Conclusion: 该研究推进了AV技术，为AI时代的学术写作提供了透明工具和洞察。

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [31] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
*Jiin Park,Misuk Kim*

Main category: cs.CL

Relevance: 40.0

TL;DR: 提出了一种多语言、可扩展、无监督的跨领域方面检测框架，用于多领域多语言评论数据的多标签标注。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于特定领域和语言，或依赖监督学习需要大规模标注数据。

Method: 通过聚类提取方面类别候选，使用负采样生成方面感知嵌入向量，并微调预训练语言模型评估自动生成标签。

Result: 自动生成的标签质量高，适用于训练，且在一致性和可扩展性上优于公开大语言模型。

Conclusion: 该框架克服了监督方法的限制，适用于多语言多领域环境，未来将探索自动评论摘要和AI代理集成。

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [32] [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
*Zekun Wu,Seonglae Cho,Umar Mohammed,Cristian Munoz,Kleyton Costa,Xin Guan,Theo King,Ze Wang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CR

Relevance: 40.0

TL;DR: LibVulnWatch是一个基于图的评估框架，用于分析开源AI库的安全、许可和维护等风险，生成可复现的评分并公开监控。


<details>
  <summary>Details</summary>
Motivation: 开源AI库在现代AI系统中至关重要，但其在安全、许可和维护等方面的风险未被充分研究，需要一种系统化的评估方法。

Method: 基于LangGraph构建的有向无环图框架，协调多个专业代理从可信源提取、验证和量化风险。

Result: 应用于20个常用库，覆盖88%的OpenSSF Scorecard检查，发现每库多达19个额外风险，包括RCE漏洞和许可问题。

Conclusion: LibVulnWatch通过可验证的指标推动AI治理，提供了一种可扩展、透明的供应链风险评估机制。

Abstract: Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.

</details>


### [33] [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
*Dominic Zaun Eu Jones*

Main category: econ.GN

Relevance: 40.0

TL;DR: Ornithologist是一个弱监督文本分类系统，用于测量央行文本的鹰派和鸽派倾向，通过结合人类编写的决策树和大型语言模型提高透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发一种透明且易于非专家理解的文本分类系统，减少幻觉风险，并适用于其他文本分类问题。

Method: 使用“分类学引导推理”方法，结合人类编写的决策树和大型语言模型进行弱监督分类。

Result: Ornithologist对RBA通信的鹰派和鸽派倾向测量能够预测未来现金利率路径和市场预期。

Conclusion: Ornithologist提供了一种透明、可解释且易于扩展的文本分类方法，适用于央行文本分析及其他领域。

Abstract: I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.

</details>


### [34] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

Relevance: 30.0

TL;DR: 比较静态与动态叙事在教育游戏中对学习动机的影响，发现动态叙事能提升参与度，但也面临教学与叙事平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索动态叙事对学习动机的影响，填补AI驱动动态叙事在教育游戏中的研究空白。

Method: 比较静态分支叙事与动态序列化叙事的两种版本教育游戏，分析玩家参与度与选择多样性。

Result: 动态叙事提升玩家参与度，但需平衡教学目标和叙事动态性。

Conclusion: AI驱动的动态叙事在教育游戏中具有潜力，但需进一步优化设计。

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [35] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
*Ulrich Frank,Pierre Maier*

Main category: cs.CL

Relevance: 10.0

TL;DR: 本文介绍了一种新型UML建模工具的设计、实现和应用，相比传统工具有显著进步，支持类图和对象图的集成以及对象执行，适用于教学和研究。


<details>
  <summary>Details</summary>
Motivation: 开发一种更先进的UML建模工具，以支持软件架构的集成和教学需求，同时展示长期国际研究项目的成果。

Method: 设计和实现了一种新型UML建模工具，支持类图与对象图的集成及对象执行功能。

Result: 该工具不仅支持新的软件架构设计，还提供了教学上的优势，增强了学生的学习体验。

Conclusion: 该项目展示了长期研究如何产生有价值的副产品，同时推动了UML建模工具的进步。

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


### [36] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
*Ulrich Frank,Pierre Maier*

Main category: cs.CL

Relevance: 10.0

TL;DR: 本文介绍了一种新型UML建模工具的设计、实现和应用，相比传统工具有显著进步，支持类图和对象图的集成以及对象执行。


<details>
  <summary>Details</summary>
Motivation: 旨在通过集成软件与对象模型，推动新型软件架构的发展，并提升教学体验。

Method: 设计并实现了一种支持类图和对象图集成及对象执行的UML建模工具。

Result: 该工具不仅适用于新型软件架构的开发，还特别适合教学用途，为学生提供了丰富的学习体验。

Conclusion: 该项目展示了长期国际研究如何产生有价值的成果，即使这些成果是其他工作的副产品。

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [37] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/abs/2505.08854)
*Yuping Wang,Shuo Xing,Cui Can,Renjie Li,Hongyuan Hua,Kexin Tian,Zhaobin Mo,Xiangbo Gao,Keshu Wu,Sulong Zhou,Hengxu You,Juntong Peng,Junge Zhang,Zehao Wang,Rui Song,Mingxuan Yan,Walter Zimmer,Xingcheng Zhou,Peiran Li,Zhaohan Lu,Chia-Ju Chen,Yue Huang,Ryan A. Rossi,Lichao Sun,Hongkai Yu,Zhiwen Fan,Frank Hao Yang,Yuhao Kang,Ross Greer,Chenxi Liu,Eun Hak Lee,Xuan Di,Xinyue Ye,Liu Ren,Alois Knoll,Xiaopeng Li,Shuiwang Ji,Masayoshi Tomizuka,Marco Pavone,Tianbao Yang,Jing Du,Ming-Hsuan Yang,Hua Wei,Ziran Wang,Yang Zhou,Jiachen Li,Zhengzhong Tu*

Main category: cs.CV

Relevance: 70.0

TL;DR: 该综述探讨了生成式AI在自动驾驶领域的应用，包括生成模型原理、前沿应用及挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何推动自动驾驶技术发展，特别是实现L5级完全自动驾驶。

Method: 综述了生成模型（如VAEs、GANs、Diffusion Models、LLMs）的原理及其在自动驾驶中的应用，包括数据生成、决策制定等。

Result: 总结了生成式AI在自动驾驶中的潜力与挑战，如泛化能力、安全性、伦理问题等。

Conclusion: 生成式AI为自动驾驶提供了新机遇，但仍需解决技术与社会层面的问题。

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [38] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
*Yangyi Chen,Hao Peng,Tong Zhang,Heng Ji*

Main category: cs.CV

Relevance: 70.0

TL;DR: PRIOR是一种视觉-语言预训练方法，通过差异加权优先处理与图像相关的标记，减少噪声拟合和幻觉风险，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准的视觉-语言模型预训练中，模型通过下一标记预测（NTP）最大化标题的条件概率，但仅少数标记与视觉内容直接相关，导致噪声拟合和幻觉风险增加。

Method: PRIOR利用重要性采样框架，通过文本参考LLM对标记进行加权，调整训练损失，优先处理与图像相关的标记。

Result: 在多个视觉-语言基准测试中，PRIOR相比NTP分别实现了19%和8%的平均相对改进，并显示出更好的扩展性。

Conclusion: PRIOR通过差异加权有效提升了视觉-语言模型的性能和扩展潜力。

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [39] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/abs/2505.08854)
*Yuping Wang,Shuo Xing,Cui Can,Renjie Li,Hongyuan Hua,Kexin Tian,Zhaobin Mo,Xiangbo Gao,Keshu Wu,Sulong Zhou,Hengxu You,Juntong Peng,Junge Zhang,Zehao Wang,Rui Song,Mingxuan Yan,Walter Zimmer,Xingcheng Zhou,Peiran Li,Zhaohan Lu,Chia-Ju Chen,Yue Huang,Ryan A. Rossi,Lichao Sun,Hongkai Yu,Zhiwen Fan,Frank Hao Yang,Yuhao Kang,Ross Greer,Chenxi Liu,Eun Hak Lee,Xuan Di,Xinyue Ye,Liu Ren,Alois Knoll,Xiaopeng Li,Shuiwang Ji,Masayoshi Tomizuka,Marco Pavone,Tianbao Yang,Jing Du,Ming-Hsuan Yang,Hua Wei,Ziran Wang,Yang Zhou,Jiachen Li,Zhengzhong Tu*

Main category: cs.CV

Relevance: 70.0

TL;DR: 该论文综述了生成式人工智能（GenAI）在自动驾驶领域的应用，涵盖了生成模型（如VAEs、GANs、Diffusion Models和LLMs）的原理及其在图像、LiDAR、轨迹等生成任务中的前沿应用，同时探讨了实际应用中的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索GenAI如何推动自动驾驶技术（尤其是Level 5）的发展，解决其在可靠性、安全性和泛化性等方面的挑战。

Method: 通过文献综述，系统梳理了生成模型的原理及其在自动驾驶中的应用，并分类讨论了实际应用场景（如合成数据、端到端驾驶策略等）。

Result: 总结了GenAI在自动驾驶中的前沿应用，并提出了未来研究方向，包括理论保证、信任指标和社会技术影响等。

Conclusion: GenAI为自动驾驶提供了新的可能性，但仍需解决泛化性、安全性和伦理等问题，未来研究应聚焦于理论与实践的平衡。

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [40] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
*Yangyi Chen,Hao Peng,Tong Zhang,Heng Ji*

Main category: cs.CV

Relevance: 70.0

TL;DR: PRIOR是一种改进的视觉语言预训练方法，通过差异加权图像相关标记来减少噪声和幻觉风险，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 标准视觉语言模型（LVLMs）在预训练中通过下一标记预测（NTP）最大化标题的联合概率，但仅少数标记与视觉内容直接相关，导致模型拟合噪声并增加幻觉风险。

Method: PRIOR利用重要性采样框架，通过文本参考LLM对标记进行加权，调整NTP损失。实验包括带和不带视觉编码器的LVLMs。

Result: 在多个视觉语言基准测试中，PRIOR相比NTP分别实现了19%和8%的相对改进，并展示了更好的扩展性。

Conclusion: PRIOR通过差异化加权图像相关标记，显著提升了LVLMs的性能和扩展潜力。

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [41] [Variational Visual Question Answering](https://arxiv.org/abs/2505.09591)
*Tobias Jan Wieczorek,Nathalie Daun,Mohammad Emtiyaz Khan,Marcus Rohrbach*

Main category: cs.CV

Relevance: 70.0

TL;DR: 提出了一种基于变分学习的VQA方法（IVON），用于提升多模态模型的可靠性，特别是在OOD场景下，显著改善了校准和弃权能力，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在VQA任务中存在可靠性问题（如过度自信和校准不足），尤其是在OOD场景下，而现有研究多集中于单模态模型。

Method: 采用变分算法IVON替代AdamW进行视觉语言模型的微调，生成模型参数的后验分布。

Result: 实验表明，该方法显著降低了校准误差（减少50%以上），并提高了覆盖率（4% vs. SOTA），在分布偏移下表现更优（覆盖率提升8%）。

Conclusion: 变分学习是提升多模态模型可靠性的有效方法。

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [42] [AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection](https://arxiv.org/abs/2505.09155)
*Yichen Shi,Zhuofu Tao,Yuhao Gao,Li Huang,Hongyang Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.CV

Relevance: 65.0

TL;DR: 论文提出了一种基于分割的新型网络检测机制，用于解决多模态大语言模型（MLLMs）在电路图理解上的局限性，并扩展了AMSnet数据集为AMSnet 2.0。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在电路图理解上表现不佳，主要由于缺乏高质量的训练数据和现有方法的局限性。

Method: 提出了一种基于分割的网络检测机制，具有高鲁棒性，并恢复了位置信息以实现电路图的数字重建。同时扩展了AMSnet数据集。

Result: 创建了AMSnet 2.0，包含2,686个电路图及相关数据，显著优于原AMSnet的792个电路图。

Conclusion: 新方法提高了电路图理解的鲁棒性和准确性，扩展的数据集为未来研究提供了更好的资源。

Abstract: Current multimodal large language models (MLLMs) struggle to understand
circuit schematics due to their limited recognition capabilities. This could be
attributed to the lack of high-quality schematic-netlist training data.
Existing work such as AMSnet applies schematic parsing to generate netlists.
However, these methods rely on hard-coded heuristics and are difficult to apply
to complex or noisy schematics in this paper. We therefore propose a novel net
detection mechanism based on segmentation with high robustness. The proposed
method also recovers positional information, allowing digital reconstruction of
schematics. We then expand AMSnet dataset with schematic images from various
sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with
schematic images, Spectre-formatted netlists, OpenAccess digital schematics,
and positional information for circuit components and nets, whereas AMSnet only
includes 792 circuits with SPICE netlists but no digital schematics.

</details>


### [43] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/abs/2505.08817)
*Camilo Carvajal Reyes,Joaquín Fontbona,Felipe Tobar*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种基于外部多模态模型的SFW采样器，用于防止基于分数的生成模型（SBM）生成不安全内容，无需微调，且对图像质量影响较小。


<details>
  <summary>Details</summary>
Motivation: 尽管SBM在图像合成中表现优异，但其可能生成不安全内容（如暴力或非自愿裸露）。现有方法依赖模型自身知识且需微调，本文探索利用外部源确保安全输出。

Method: 提出SFW采样器，通过条件轨迹校正步骤，利用多模态模型（如CLIP）引导样本远离不期望区域，并支持用户自定义NSFW类别。

Result: 在Stable Diffusion上的实验表明，SFW采样器有效减少不安全内容生成，且对图像质量影响较小。

Conclusion: SFW采样器适用于对齐的SBM模型，展示了利用模型无关条件预防不良图像的潜力。

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [44] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/abs/2505.08961)
*Yancheng Wang,Nebojsa Jojic,Yingzhen Yang*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种新颖的注意力模块DCS-Attention，通过可微分通道选择优化注意力权重，结合信息瓶颈原理提升特征提取能力，显著提高了行人重识别任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制在计算注意力权重时未考虑通道信息的选择性，DCS-Attention通过可微分通道选择和信息瓶颈原理，优化特征提取过程。

Method: 设计了DCS-Attention模块，支持固定或可学习的主干网络（DCS-FB和DCS-DNAS），并推导了信息瓶颈损失的可变分上界，通过SGD优化。

Result: 在多个行人重识别基准测试中，DCS-Attention显著提升了DNN的预测准确性。

Conclusion: DCS-Attention通过选择最具信息量的通道，有效提升了特征提取能力，适用于行人重识别任务。

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [45] [Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning](https://arxiv.org/abs/2505.09118)
*Dayong Liang,Changmeng Zheng,Zhiyuan Wen,Yi Cai,Xiao-Yong Wei,Qing Li*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种增强视觉语言模型（VLMs）交互推理能力的框架ISGR，通过双流图构建、交互查询和长期记忆强化学习，显著提升了复杂场景理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 传统场景图方法局限于空间关系，难以处理复杂交互。本文旨在解决传统方法生成无关关系集和缺乏持久记忆的问题。

Method: 1. 双流图构建器结合空间关系提取和交互感知标注；2. 交互查询激活VLM功能知识；3. 长期记忆强化学习策略。

Result: 在交互密集的推理基准上显著优于基线方法，尤其在复杂场景理解任务中表现突出。

Conclusion: ISGR框架有效提升了VLMs的交互推理能力，为复杂场景理解提供了新思路。

Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting
vision-language models' (VLMs) ability to reason about complex interactions in
visual scenes. This paper addresses two key challenges: (1) conventional
detection-to-construction methods produce unfocused, contextually irrelevant
relationship sets, and (2) existing approaches fail to form persistent memories
for generalizing interaction reasoning to new scenes. We propose
Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances
VLMs' interactional reasoning through three complementary components. First,
our dual-stream graph constructor combines SAM-powered spatial relation
extraction with interaction-aware captioning to generate functionally salient
scene graphs with spatial grounding. Second, we employ targeted interaction
queries to activate VLMs' latent knowledge of object functionalities,
converting passive recognition into active reasoning about how objects work
together. Finally, we introduce a lone-term memory reinforcement learning
strategy with a specialized interaction-focused reward function that transforms
transient patterns into long-term reasoning heuristics. Extensive experiments
demonstrate that our approach significantly outperforms baseline methods on
interaction-heavy reasoning benchmarks, with particularly strong improvements
on complex scene understanding tasks. The source code can be accessed at
https://github.com/open_upon_acceptance.

</details>


### [46] [Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance](https://arxiv.org/abs/2505.09123)
*Guoying Liang,Su Yang*

Main category: cs.CV

Relevance: 60.0

TL;DR: 该研究提出了一种利用Segment Anything Model (SAM)进行伪装目标检测(COD)的新框架，通过点提示提升SAM的性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在伪装目标检测中表现不佳的问题，提出一种无需从头设计专业模型的方法。

Method: 开发了Promotion Point Targeting Network (PPT-net)和关键点选择(KPS)算法，通过多尺度特征预测和对比性点提示提升SAM。

Result: 在3个数据集和6个指标上优于现有方法，证明了SAM在COD任务中的潜力。

Conclusion: 利用现成的大模型（如SAM）进行COD任务，不仅性能优越，还能简化问题。

Abstract: Big model has emerged as a new research paradigm that can be applied to
various down-stream tasks with only minor effort for domain adaption.
Correspondingly, this study tackles Camouflaged Object Detection (COD)
leveraging the Segment Anything Model (SAM). The previous studies declared that
SAM is not workable for COD but this study reveals that SAM works if promoted
properly, for which we devise a new framework to render point promotions:
First, we develop the Promotion Point Targeting Network (PPT-net) to leverage
multi-scale features in predicting the probabilities of camouflaged objects'
presences at given candidate points over the image. Then, we develop a key
point selection (KPS) algorithm to deploy both positive and negative point
promotions contrastively to SAM to guide the segmentation. It is the first work
to facilitate big model for COD and achieves plausible results experimentally
over the existing methods on 3 data sets under 6 metrics. This study
demonstrates an off-the-shelf methodology for COD by leveraging SAM, which
gains advantage over designing professional models from scratch, not only in
performance, but also in turning the problem to a less challenging task, that
is, seeking informative but not exactly precise promotions.

</details>


### [47] [Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models](https://arxiv.org/abs/2505.09139)
*Lucas Choi,Ross Greer*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种基于对比类对齐分数（CCAS）的自动提示优化方法，用于提升视觉语言模型（VLM）在目标检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型因提示词表述不同导致的性能波动问题。

Method: 利用大语言模型生成多样提示候选，通过CCAS筛选最优提示，无需额外训练或标注数据。

Result: 在挑战性目标类别上验证了方法有效性，显著提升了检测精度。

Conclusion: 提供了一种可扩展且模型无关的自动提示优化方案，替代人工提示工程。

Abstract: Vision-language models (VLMs) offer flexible object detection through natural
language prompts but suffer from performance variability depending on prompt
phrasing. In this paper, we introduce a method for automated prompt refinement
using a novel metric called the Contrastive Class Alignment Score (CCAS), which
ranks prompts based on their semantic alignment with a target object class
while penalizing similarity to confounding classes. Our method generates
diverse prompt candidates via a large language model and filters them through
CCAS, computed using prompt embeddings from a sentence transformer. We evaluate
our approach on challenging object categories, demonstrating that our automatic
selection of high-precision prompts improves object detection accuracy without
the need for additional model training or labeled data. This scalable and
model-agnostic pipeline offers a principled alternative to manual prompt
engineering for VLM-based detection systems.

</details>


### [48] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/abs/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种基于单张正常图像提示（OneNIP）的方法，用于增强统一异常检测的性能，并通过监督细化器提升像素级异常分割。


<details>
  <summary>Details</summary>
Motivation: 现有基于自注意力变换器的无监督重建网络在多类异常检测中表现优异，但存在对正常和异常特征完美重建的问题，且低分辨率潜在空间导致异常分割不准确。

Method: 提出OneNIP方法，通过单张正常图像提示重建正常特征并恢复异常特征；引入监督细化器，利用真实正常和合成异常图像回归重建误差。

Result: OneNIP在MVTec、BTAD和VisA三个工业异常检测基准上优于先前方法。

Conclusion: OneNIP通过简单有效的方法提升了统一异常检测的性能和像素级分割准确性。

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [49] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/abs/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种纯视觉基础模型MetaUAS，用于通用视觉异常分割，无需依赖语言模型或特定异常检测数据集。通过统一异常分割与变化分割，利用合成数据集训练，实现了高效且训练自由的异常分割。


<details>
  <summary>Details</summary>
Motivation: 视觉表示与语言无关，现有方法依赖视觉-语言模型和手动设计的文本提示，限制了通用性。探索纯视觉模型在异常分割中的潜力。

Method: 提出MetaUAS框架，利用合成图像对训练，通过软特征对齐模块处理几何变化，实现单图像提示的通用异常分割。

Result: MetaUAS在零样本、少样本甚至全样本异常分割任务中显著优于现有方法。

Conclusion: 纯视觉模型在异常分割中具有潜力，MetaUAS为通用异常分割提供了高效且无需训练的解决方案。

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [50] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/abs/2505.09358)
*Bingxin Ke,Kevin Qu,Tianfu Wang,Nando Metzger,Shengyu Huang,Bo Li,Anton Obukhov,Konrad Schindler*

Main category: cs.CV

Relevance: 60.0

TL;DR: Marigold是一种基于预训练潜在扩散模型（如Stable Diffusion）的条件生成模型，通过微调协议将其知识迁移到密集图像分析任务中，如深度估计和表面法线预测，实现了零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺场景下，预训练模型的质量对迁移学习至关重要。文本到图像生成模型（如潜在扩散模型）展示了强大的视觉理解能力，但尚未充分用于密集图像分析任务。

Method: 提出Marigold模型家族和微调协议，利用预训练的潜在扩散模型（如Stable Diffusion），通过最小架构修改和小规模合成数据集训练，适应密集图像分析任务。

Result: Marigold在单GPU上几天内完成训练，并在零样本泛化中达到最先进性能。

Conclusion: Marigold展示了从生成模型中提取知识用于密集视觉任务的潜力，为数据稀缺场景提供了高效解决方案。

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [51] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/abs/2505.09466)
*Xi Chen,Shiyang Zhou,Muqi Huang,Jiaxu Feng,Yun Xiong,Kun Zhou,Biao Yang,Yuhui Zhang,Huishuai Bao,Sijia Peng,Chuan Li,Feng Shi*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种新的2D语义感知位置编码方法（SaPE²），用于改进视觉Transformer中的位置编码，提升模型在计算机视觉任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的位置编码方法（如绝对和相对位置编码）主要针对1D线性位置关系，无法有效捕捉图像块之间的语义感知位置关系，限制了模型的泛化能力和翻译等变性。

Method: 提出SaPE²方法，动态调整位置表示，利用局部内容而非固定线性位置关系或空间坐标，增强语义感知。

Result: SaPE²提升了模型在不同图像分辨率和尺度下的泛化能力，改善了翻译等变性，并更好地聚合了视觉相似但空间遥远的图像块特征。

Conclusion: SaPE²通过结合位置编码和感知相似性，显著提升了视觉Transformer在计算机视觉任务中的性能。

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [52] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568)
*Jiuhai Chen,Zhiyang Xu,Xichen Pan,Yushi Hu,Can Qin,Tom Goldstein,Lifu Huang,Tianyi Zhou,Saining Xie,Silvio Savarese,Le Xue,Caiming Xiong,Ran Xu*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种基于扩散Transformer的统一多模态模型BLIP3-o，结合图像理解和生成任务，通过创新的架构设计和训练策略，实现了高效训练和高质量生成。


<details>
  <summary>Details</summary>
Motivation: 探索统一多模态框架中图像生成的最佳模型架构和训练方法，利用自回归和扩散模型的潜力。

Method: 采用扩散Transformer生成CLIP图像特征，提出分阶段预训练策略（先理解后生成），并构建高质量指令调优数据集BLIP3o-60k。

Result: BLIP3-o在图像理解和生成任务中表现优异，训练效率高且生成质量提升。

Conclusion: BLIP3-o为统一多模态模型提供了创新设计和训练方法，开源资源促进未来研究。

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [53] [Variational Visual Question Answering](https://arxiv.org/abs/2505.09591)
*Tobias Jan Wieczorek,Nathalie Daun,Mohammad Emtiyaz Khan,Marcus Rohrbach*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种基于变分学习的多模态模型可靠性提升方法，通过IVON算法优化视觉问答（VQA）模型的校准和拒绝性能。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在视觉问答中存在可靠性问题，尤其是在分布外（OOD）场景下，模型可能过度自信且校准不足。目前针对单模态模型的研究较多，但多模态模型的相关工作较少。

Method: 采用变分算法IVON替代传统的AdamW优化器，生成模型参数的后验分布，以提升模型的校准能力和拒绝性能。

Result: 实验表明，该方法显著降低了预期校准误差（超过50%），并在固定风险下提高了覆盖率（4% vs. SOTA），在分布偏移情况下性能提升更明显（8% vs. SOTA）。

Conclusion: 变分学习是提升多模态模型可靠性的有效方法。

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [54] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/abs/2505.08961)
*Yancheng Wang,Nebojsa Jojic,Yingzhen Yang*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种名为DCS-Attention的新型注意力模块，通过可微分通道选择优化注意力权重计算，结合信息瓶颈原理提升特征提取能力，显著提升了行人重识别任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制在计算注意力权重时未考虑通道选择，DCS-Attention通过可微分方式选择信息丰富的通道，结合信息瓶颈原理优化特征提取。

Method: 设计了DCS-Attention模块，支持固定或可学习（DNAS）网络骨干，推导了信息瓶颈损失的可变分上界，并将其融入训练损失中。

Result: 在多个行人重识别基准测试中，DCS-Attention显著提升了DNN的预测准确性，证明了其在学习判别性特征方面的有效性。

Conclusion: DCS-Attention通过可微分通道选择和信息瓶颈优化，为行人重识别任务提供了高性能解决方案。

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [55] [Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models](https://arxiv.org/abs/2505.09139)
*Lucas Choi,Ross Greer*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种自动优化视觉语言模型（VLM）提示的方法，通过对比类对齐分数（CCAS）筛选提示，提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）的性能受提示措辞影响较大，手动优化提示耗时且不高效。本文旨在通过自动化的方式解决这一问题。

Method: 使用大语言模型生成多样提示候选，通过CCAS（基于句子嵌入的语义对齐度量）筛选最优提示。

Result: 在挑战性目标类别上验证了方法的有效性，自动选择的提示显著提升了检测精度，无需额外训练或标注数据。

Conclusion: 该方法为VLM检测系统提供了一种可扩展且模型无关的提示优化方案。

Abstract: Vision-language models (VLMs) offer flexible object detection through natural
language prompts but suffer from performance variability depending on prompt
phrasing. In this paper, we introduce a method for automated prompt refinement
using a novel metric called the Contrastive Class Alignment Score (CCAS), which
ranks prompts based on their semantic alignment with a target object class
while penalizing similarity to confounding classes. Our method generates
diverse prompt candidates via a large language model and filters them through
CCAS, computed using prompt embeddings from a sentence transformer. We evaluate
our approach on challenging object categories, demonstrating that our automatic
selection of high-precision prompts improves object detection accuracy without
the need for additional model training or labeled data. This scalable and
model-agnostic pipeline offers a principled alternative to manual prompt
engineering for VLM-based detection systems.

</details>


### [56] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/abs/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种名为OneNIP的方法，通过仅使用一张正常图像提示，实现了高效且泛化能力强的统一异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于自注意力变换器的无监督重建网络在多类异常检测中表现优异，但可能因目标特征的高一致性而无法区分正常与异常特征，且低分辨率潜在空间的重建导致异常分割不准确。

Method: 提出OneNIP方法，仅需一张正常图像提示即可重建正常特征并恢复异常特征；此外，引入监督细化器，利用真实正常和合成异常图像回归重建误差，提升像素级异常分割。

Result: OneNIP在MVTec、BTAD和VisA三个工业异常检测基准上优于先前方法。

Conclusion: OneNIP通过简单有效的方法显著提升了统一异常检测的性能和泛化能力。

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [57] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/abs/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种基于纯视觉基础模型的通用异常分割方法MetaUAS，通过统一异常分割与变化分割，利用合成数据集训练，无需依赖语言模型或特定异常数据集。


<details>
  <summary>Details</summary>
Motivation: 探索纯视觉基础模型替代视觉-语言模型的可能性，解决视觉表示与语言无关的问题。

Method: 提出MetaUAS框架，利用合成图像对训练，通过软特征对齐模块处理几何变化。

Result: MetaUAS在零样本、少样本甚至全样本异常分割任务中显著优于现有方法。

Conclusion: MetaUAS首次实现了不依赖语言模型的通用异常分割，且仅需一张正常图像提示即可高效工作。

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [58] [FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models](https://arxiv.org/abs/2505.09415)
*Hongyang Wang,Yichen Shi,Zhuofu Tao,Yuhao Gao,Liepiao Zhang,Xun Lin,Jun Feng,Xiaochen Yuan,Zitong Yu,Xiaochun Cao*

Main category: cs.CV

Relevance: 60.0

TL;DR: FaceShield是一个多模态大语言模型（MLLM），专为面部反欺骗（FAS）任务设计，结合了预训练和监督微调数据集，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FAS方法缺乏可解释性，而多模态大语言模型在视觉任务中表现出强大的感知和推理能力，但尚未有针对FAS的通用MLLM和数据集。

Method: 采用欺骗感知视觉感知（SAVP）结合原始图像和先验知识辅助信息，并使用提示引导的视觉标记掩码（PVTM）策略提升泛化能力。

Result: 在三个基准数据集上，FaceShield在粗粒度分类、细粒度分类、推理和攻击定位四个FAS任务中显著优于现有深度学习模型和通用MLLM。

Conclusion: FaceShield为FAS任务提供了通用且可解释的解决方案，并计划发布相关数据集和代码。

Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems
from presentation attacks. Previous methods approached this task as a
classification problem, lacking interpretability and reasoning behind the
predicted results. Recently, multimodal large language models (MLLMs) have
shown strong capabilities in perception, reasoning, and decision-making in
visual tasks. However, there is currently no universal and comprehensive MLLM
and dataset specifically designed for FAS task. To address this gap, we propose
FaceShield, a MLLM for FAS, along with the corresponding pre-training and
supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.
FaceShield is capable of determining the authenticity of faces, identifying
types of spoofing attacks, providing reasoning for its judgments, and detecting
attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that
incorporates both the original image and auxiliary information based on prior
knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to
random mask vision tokens, thereby improving the model's generalization
ability. We conducted extensive experiments on three benchmark datasets,
demonstrating that FaceShield significantly outperforms previous deep learning
models and general MLLMs on four FAS tasks, i.e., coarse-grained
classification, fine-grained classification, reasoning, and attack
localization. Our instruction datasets, protocols, and codes will be released
soon.

</details>


### [59] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/abs/2505.09466)
*Xi Chen,Shiyang Zhou,Muqi Huang,Jiaxu Feng,Yun Xiong,Kun Zhou,Biao Yang,Yuhui Zhang,Huishuai Bao,Sijia Peng,Chuan Li,Feng Shi*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种新的2D语义感知位置编码方法（SaPE²），用于改进视觉Transformer中的位置编码，提升模型在图像任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法主要基于1D线性关系，无法有效捕捉图像块间的语义关系，限制了模型的泛化能力和翻译等变性。

Method: 提出SaPE²，通过动态调整位置表示，利用局部内容而非固定线性关系或空间坐标，增强语义感知。

Result: SaPE²提升了模型在不同图像分辨率和尺度上的泛化能力，改善了翻译等变性，并更好地聚合了视觉相似但空间遥远的图像块特征。

Conclusion: SaPE²通过结合位置编码与感知相似性，显著提升了视觉Transformer在计算机视觉任务中的性能。

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [60] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568)
*Jiuhai Chen,Zhiyang Xu,Xichen Pan,Yushi Hu,Can Qin,Tom Goldstein,Lifu Huang,Tianyi Zhou,Saining Xie,Silvio Savarese,Le Xue,Caiming Xiong,Ran Xu*

Main category: cs.CV

Relevance: 60.0

TL;DR: 该论文提出了一种基于扩散变换器的统一多模态模型BLIP3-o，结合了图像理解和生成能力，通过创新的模型设计、训练策略和高质量数据集，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索统一图像理解和生成的最佳模型架构与训练方法，利用自回归和扩散模型的潜力，提升生成质量和训练效率。

Method: 采用扩散变换器生成语义丰富的CLIP图像特征，提出分阶段预训练策略（先图像理解后图像生成），并构建高质量指令调优数据集BLIP3o-60k。

Result: BLIP3-o在图像理解和生成任务中表现优异，训练效率高且生成质量提升。

Conclusion: BLIP3-o为统一多模态模型提供了创新解决方案，开源模型和数据集促进未来研究。

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [61] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/abs/2505.09329)
*Jiarun Liu,Hong-Yu Zhou,Weijian Huang,Hao Yang,Dongning Song,Tao Tan,Yong Liang,Shanshan Wang*

Main category: cs.CV

Relevance: 50.0

TL;DR: 论文探讨了医学视觉基础模型的规模化行为，提出了BioVFM-21M数据集和BioVFM模型，发现规模化对性能有益但效果因任务而异。


<details>
  <summary>Details</summary>
Motivation: 医学图像与自然数据差异显著，但医学领域规模化行为的研究不足，需探索关键因素以开发医学视觉基础模型。

Method: 通过自监督学习，研究模型大小、训练算法、数据规模和成像模态的规模化行为，并引入BioVFM-21M数据集。

Result: BioVFM模型在12个医学基准测试中优于现有模型，规模化有益但效果因任务而异。

Conclusion: 规模化有助于性能提升，但任务特性、数据多样性、预训练方法和计算效率仍需重点考虑。

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [62] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/abs/2505.08814)
*Wenkai Li,Xiaoqi Li,Yingjie Mao,Yishun Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 本文研究了四种DNN覆盖指标（主要功能、边界、层次和结构覆盖）与模型深度、配置信息之间的关系，并通过实验验证了这些关系。


<details>
  <summary>Details</summary>
Motivation: 随着DNN的广泛应用，其安全性测试变得重要，但目前缺乏对不同覆盖指标之间关系的实证研究。

Method: 选择了LeNet、VGG和ResNet等不同架构的DNN模型，以及5至54层的10个模型，进行覆盖指标的实验比较。

Result: 揭示了模型深度、配置信息与覆盖指标之间的关系，并探讨了修改后的决策/条件覆盖与数据集大小的关系。

Conclusion: 提出了三个未来研究方向，以进一步推动DNN模型的安全性测试。

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [63] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/abs/2505.08833)
*Qingyi Wang,Yuebing Liang,Yunhan Zheng,Kaiyuan Xu,Jinhua Zhao,Shenhao Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出了一种基于Stable Diffusion和ControlNet的生成模型，用于根据土地利用描述生成高保真卫星图像，并通过OpenStreetMap数据增强生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成大规模、现实且实用的城市规划设计，因此需要一种能够结合语言提示和结构化数据的生成模型。

Method: 扩展Stable Diffusion模型，结合ControlNet，利用OpenStreetMap的结构化数据生成卫星图像，并通过语言提示和控制图像调整生成效果。

Result: 模型在三个美国城市的数据上表现出色，生成多样且现实的城市场景，FID和KID得分高，且用户评估显示生成图像优于真实图像。

Conclusion: 该研究为可控城市图像生成设定了基准，展示了生成AI在城市规划和公众参与中的潜力。

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [64] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/abs/2505.08909)
*Deliang Wei,Peng Chen,Haobo Xu,Jiale Yao,Fang Li,Tieyong Zeng*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种名为CoCo的保守性可扩展去噪器，用于解决泊松逆问题中的挑战，通过结合哈密顿正则化和谱正则化，证明了其全局收敛性，并在实验中表现优于相关方法。


<details>
  <summary>Details</summary>
Motivation: 解决泊松逆问题中传统PnP方法因强凸性或平滑性假设及非扩展性去噪器限制而无法有效工作的问题。

Method: 提出CoCo去噪器，结合哈密顿正则化和谱正则化训练策略，证明其为弱凸函数的近端算子，建立全局收敛性。

Result: 实验表明，该方法在视觉质量和定量指标上优于相关方法。

Conclusion: CoCo去噪器为泊松逆问题提供了一种有效的解决方案，具有理论和实验优势。

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [65] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
*Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji*

Main category: cs.CV

Relevance: 40.0

TL;DR: Maya是一个开源的多语言视觉语言模型（VLM），旨在解决现有VLM在低资源语言和文化多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在多语言和文化多样性任务上表现不佳，尤其是在低资源语言中。

Method: 1）基于LLaVA预训练数据集构建了八种语言的多语言图像-文本数据集；2）开发了支持这些语言的多语言图像-文本模型。

Result: Maya提升了视觉语言任务中的文化和语言理解能力。

Conclusion: Maya为多语言和文化多样性任务提供了有效的解决方案。

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [66] [Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking](https://arxiv.org/abs/2505.08999)
*Wei-Long Tian,Peng Gao,Xiao Liu,Long Xu,Hamido Fujita,Hanan Aljuai,Mao-Li Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种自适应元梯度对抗攻击方法（AMGA），用于揭示视觉跟踪器的安全漏洞，通过多模型集成和元学习策略提升对抗样本的迁移性和攻击效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的安全问题影响了视觉跟踪方法在现实场景中的可靠应用，需通过有效的对抗攻击揭示其漏洞。

Method: AMGA结合多模型集成、元学习、动量机制和高斯平滑，随机选择模型并构建多样化跟踪场景，迭代优化梯度方向。

Result: 在OTB2015、LaSOT和GOT-10k等数据集上，AMGA显著提升了对抗样本的攻击性能、迁移性和欺骗性。

Conclusion: AMGA有效缩小了白盒与黑盒对抗攻击的差距，在现实场景中具有潜在应用价值。

Abstract: In recent years, visual tracking methods based on convolutional neural
networks and Transformers have achieved remarkable performance and have been
successfully applied in fields such as autonomous driving. However, the
numerous security issues exposed by deep learning models have gradually
affected the reliable application of visual tracking methods in real-world
scenarios. Therefore, how to reveal the security vulnerabilities of existing
visual trackers through effective adversarial attacks has become a critical
problem that needs to be addressed. To this end, we propose an adaptive
meta-gradient adversarial attack (AMGA) method for visual tracking. This method
integrates multi-model ensembles and meta-learning strategies, combining
momentum mechanisms and Gaussian smoothing, which can significantly enhance the
transferability and attack effectiveness of adversarial examples. AMGA randomly
selects models from a large model repository, constructs diverse tracking
scenarios, and iteratively performs both white- and black-box adversarial
attacks in each scenario, optimizing the gradient directions of each model.
This paradigm minimizes the gap between white- and black-box adversarial
attacks, thus achieving excellent attack performance in black-box scenarios.
Extensive experimental results on large-scale datasets such as OTB2015, LaSOT,
and GOT-10k demonstrate that AMGA significantly improves the attack
performance, transferability, and deception of adversarial examples. Codes and
data are available at https://github.com/pgao-lab/AMGA.

</details>


### [67] [TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation](https://arxiv.org/abs/2505.09140)
*Zechao Guan,Feng Yan,Shuai Du,Lin Ma,Qingshan Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: TopoDiT-3D是一种拓扑感知的扩散Transformer模型，通过瓶颈结构和持久同调提取全局拓扑信息，显著提升了3D点云生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注局部特征提取，忽略了全局拓扑信息（如空洞），这对形状一致性和复杂几何捕捉至关重要。

Method: 提出TopoDiT-3D，利用Perceiver Resampler设计瓶颈结构，整合持久同调提取的拓扑信息，并自适应过滤冗余局部特征。

Result: 实验表明，TopoDiT-3D在视觉质量、多样性和训练效率上优于现有模型。

Conclusion: 全局拓扑信息与局部特征学习的结合对3D点云生成至关重要。

Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly
improved 3D point cloud generation. However, existing methods primarily focus
on local feature extraction while overlooking global topological information,
such as voids, which are crucial for maintaining shape consistency and
capturing complex geometries. To address this limitation, we propose
TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure
for 3D point cloud generation. Specifically, we design the bottleneck structure
utilizing Perceiver Resampler, which not only offers a mode to integrate
topological information extracted through persistent homology into feature
learning, but also adaptively filters out redundant local features to improve
training efficiency. Experimental results demonstrate that TopoDiT-3D
outperforms state-of-the-art models in visual quality, diversity, and training
efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich
topological information for 3D point cloud generation and its synergy with
conventional local feature learning. Videos and code are available at
https://github.com/Zechao-Guan/TopoDiT-3D.

</details>


### [68] [AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection](https://arxiv.org/abs/2505.09155)
*Yichen Shi,Zhuofu Tao,Yuhao Gao,Li Huang,Hongyang Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于分割的新型网络检测机制，用于解决多模态大语言模型（MLLMs）在理解电路原理图时的局限性，并扩展了AMSnet数据集为AMSnet 2.0。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）在理解电路原理图时表现不佳，主要原因是缺乏高质量的电路原理图-网表训练数据。现有方法如AMSnet依赖于硬编码启发式方法，难以处理复杂或噪声较多的原理图。

Method: 提出了一种基于分割的新型网络检测机制，具有高鲁棒性，并能恢复位置信息以实现原理图的数字重建。同时扩展了AMSnet数据集，创建了AMSnet 2.0。

Result: AMSnet 2.0包含2,686个电路，包括原理图图像、Spectre格式的网表、OpenAccess数字原理图以及电路组件和网络的位置信息，而原始AMSnet仅包含792个电路且无数字原理图。

Conclusion: 该方法显著提升了MLLMs对电路原理图的理解能力，并通过扩展数据集为未来研究提供了更丰富的资源。

Abstract: Current multimodal large language models (MLLMs) struggle to understand
circuit schematics due to their limited recognition capabilities. This could be
attributed to the lack of high-quality schematic-netlist training data.
Existing work such as AMSnet applies schematic parsing to generate netlists.
However, these methods rely on hard-coded heuristics and are difficult to apply
to complex or noisy schematics in this paper. We therefore propose a novel net
detection mechanism based on segmentation with high robustness. The proposed
method also recovers positional information, allowing digital reconstruction of
schematics. We then expand AMSnet dataset with schematic images from various
sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with
schematic images, Spectre-formatted netlists, OpenAccess digital schematics,
and positional information for circuit components and nets, whereas AMSnet only
includes 792 circuits with SPICE netlists but no digital schematics.

</details>


### [69] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/abs/2505.09178)
*Yitao Zhu,Yuan Yin,Zhenrong Shen,Zihao Zhao,Haiyu Song,Sheng Wang,Dinggang Shen,Qian Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: UniCAD是一个基于预训练视觉基础模型的多任务计算机辅助诊断（CAD）统一架构，通过低秩适应策略和模块化设计，实现了高效和可扩展的医疗图像诊断。


<details>
  <summary>Details</summary>
Motivation: 解决医疗影像领域缺乏开源CAD平台以及预训练视觉模型在医疗图像任务中资源密集的问题。

Method: 采用低秩适应策略和模块化设计，结合冻结的基础模型与可插拔专家模块。

Result: 在12个医疗数据集上表现优于现有方法，同时仅需0.17%的可训练参数。

Conclusion: UniCAD为医疗影像研究提供了一个高效、可扩展的开源平台。

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


### [70] [Zero-shot Quantization: A Comprehensive Survey](https://arxiv.org/abs/2505.09188)
*Minjun Kim,Jaehyeon Choi,Jongkeun Lee,Wonjin Cho,U Kang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文是对零样本量化（ZSQ）方法的全面综述，探讨了无需真实数据即可实现模型量化的技术，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法依赖训练数据，但在隐私、安全或法规限制下不实用，因此需要零样本量化方法。

Method: 论文首先定义了ZSQ问题，分类了现有方法（基于数据生成策略），并分析了其动机、核心思想和关键点。

Result: 总结了ZSQ的当前进展，并指出了其局限性。

Conclusion: 提出了未来研究方向，以推动ZSQ领域的发展。

Abstract: Network quantization has proven to be a powerful approach to reduce the
memory and computational demands of deep learning models for deployment on
resource-constrained devices. However, traditional quantization methods often
rely on access to training data, which is impractical in many real-world
scenarios due to privacy, security, or regulatory constraints. Zero-shot
Quantization (ZSQ) emerges as a promising solution, achieving quantization
without requiring any real data. In this paper, we provide a comprehensive
overview of ZSQ methods and their recent advancements. First, we provide a
formal definition of the ZSQ problem and highlight the key challenges. Then, we
categorize the existing ZSQ methods into classes based on data generation
strategies, and analyze their motivations, core ideas, and key takeaways.
Lastly, we suggest future research directions to address the remaining
limitations and advance the field of ZSQ. To the best of our knowledge, this
paper is the first in-depth survey on ZSQ.

</details>


### [71] [Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping](https://arxiv.org/abs/2505.09252)
*Yinuo Wang,Yue Zeng,Kai Chen,Cai Meng,Chao Pan,Zhouping Tang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文评估了多模态大语言模型（MLLMs）在颅内出血（ICH）分类任务中的表现，发现传统深度学习模型在性能上优于MLLMs，但MLLMs在交互性和可解释性方面有潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索MLLMs在医学影像分析中的应用，尤其是在ICH分类任务中的表现，以解决传统方法在低对比度和模糊边界上的挑战。

Method: 使用RSNA提供的192个NCCT数据集，比较了GPT-4o、Gemini 2.0 Flash等MLLMs与ResNet50、Vision Transformer等传统深度学习模型的表现。通过精心设计的提示词引导MLLMs完成任务。

Result: 传统深度学习模型在ICH二分类和亚型分类任务中均优于MLLMs。Gemini 2.0 Flash在亚型分类中的宏平均精确率为0.41，F1分数为0.31。

Conclusion: MLLMs在交互性和可解释性方面有优势，但性能不及传统深度学习模型。未来需优化MLLMs以提升其在三维医学影像处理中的表现。

Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes
on non-contrast computed tomography is critical for prognosis prediction and
therapeutic decision-making, yet remains challenging due to low contrast and
blurring boundaries. This study evaluates the performance of zero-shot
multi-modal large language models (MLLMs) compared to traditional deep learning
methods in ICH binary classification and subtyping. Methods: We utilized a
dataset provided by RSNA, comprising 192 NCCT volumes. The study compares
various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,
with conventional deep learning models, including ResNet50 and Vision
Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such
as ICH presence, subtype classification, localization, and volume estimation.
Results: The results indicate that in the ICH binary classification task,
traditional deep learning models outperform MLLMs comprehensively. For subtype
classification, MLLMs also exhibit inferior performance compared to traditional
deep learning models, with Gemini 2.0 Flash achieving an macro-averaged
precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While
MLLMs excel in interactive capabilities, their overall accuracy in ICH
subtyping is inferior to deep networks. However, MLLMs enhance interpretability
through language interactions, indicating potential in medical imaging
analysis. Future efforts will focus on model refinement and developing more
precise MLLMs to improve performance in three-dimensional medical image
processing.

</details>


### [72] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/abs/2505.09263)
*Guan Gui,Bin-Bin Gao,Jun Liu,Chengjie Wang,Yunsheng Wu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为AnoGen的少样本异常生成方法，通过扩散模型生成真实多样的异常样本，提升异常检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 工业检测中异常样本稀缺，现有方法生成的异常与真实异常语义差距大，导致检测性能不佳。

Method: 分三阶段：1) 学习异常分布并嵌入知识；2) 用嵌入和边界框指导扩散模型生成异常；3) 提出弱监督异常检测方法训练模型。

Result: 在MVTec数据集上，DRAEM和DesTSeg的AU-PR指标分别提升了5.8%和1.5%。

Conclusion: AnoGen方法能有效生成真实异常，显著提升异常检测性能。

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [73] [Recent Advances in Medical Imaging Segmentation: A Survey](https://arxiv.org/abs/2505.09274)
*Fares Bougourzi,Abdenour Hadid*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文综述了医学图像分割领域的最新进展，重点探讨了生成式AI、少样本学习、基础模型和通用模型等方法，旨在解决数据可访问性、标注复杂性和领域适应等挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临数据可访问性、标注复杂性和领域适应等挑战，现有模型资源密集且依赖领域专业知识，需要更高效和通用的解决方案。

Method: 论文综述了生成式AI、少样本学习、基础模型和通用模型等方法在医学图像分割中的应用。

Result: 这些方法为解决医学图像分割的挑战提供了有前景的解决方案，但仍存在局限性和未解决的问题。

Conclusion: 未来研究应关注提升分割模型的实用性和可访问性，同时维护了一个GitHub仓库以持续跟踪该领域的最新进展。

Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements
in diagnosis, treatment planning, and patient care. Among its various tasks,
segmentation remains one of the most challenging problem due to factors such as
data accessibility, annotation complexity, structural variability, variation in
medical imaging modalities, and privacy constraints. Despite recent progress,
achieving robust generalization and domain adaptation remains a significant
hurdle, particularly given the resource-intensive nature of some proposed
models and their reliance on domain expertise. This survey explores
cutting-edge advancements in medical image segmentation, focusing on
methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and
Universal Models. These approaches offer promising solutions to longstanding
challenges. We provide a comprehensive overview of the theoretical foundations,
state-of-the-art techniques, and recent applications of these methods. Finally,
we discuss inherent limitations, unresolved issues, and future research
directions aimed at enhancing the practicality and accessibility of
segmentation models in medical imaging. We are maintaining a
\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub
Repository} to continue tracking and updating innovations in this field.

</details>


### [74] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/abs/2505.09329)
*Jiarun Liu,Hong-Yu Zhou,Weijian Huang,Hao Yang,Dongning Song,Tao Tan,Yong Liang,Shanshan Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文探讨了医学视觉基础模型的扩展行为，提出了BioVFM-21M数据集和BioVFM模型，发现扩展对性能有益但受任务特性、数据多样性等因素影响。


<details>
  <summary>Details</summary>
Motivation: 医学图像与自然数据差异显著，但医学领域扩展行为的研究不足，需探索关键因素以开发可扩展的医学视觉基础模型。

Method: 通过自监督学习，研究模型大小、训练算法、数据规模和成像模态的扩展行为，并引入BioVFM-21M数据集。

Result: BioVFM模型在12个医学基准测试中优于现有技术，扩展对性能有益但受任务特性等因素影响。

Conclusion: 扩展有助于性能提升，但任务特性、数据多样性、预训练方法和计算效率仍需重点考虑。

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [75] [Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2505.09336)
*Muzammil Behzad*

Main category: cs.CV

Relevance: 40.0

TL;DR: MultiviewVLM是一个视觉语言模型，用于从3D/4D数据中无监督对比多视角学习面部情绪表示。通过伪标签和联合嵌入空间实现多视角对齐，并采用对比学习策略和梯度友好损失函数优化模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决多视角面部情绪表示学习中的无监督对齐问题，提升模型的判别性和适应性。

Method: 结合伪标签和联合嵌入空间实现多视角对齐，提出对比学习策略和梯度友好损失函数，优化分布式训练。

Result: MultiviewVLM在实验中表现优于现有方法，且易于适应实际应用。

Conclusion: MultiviewVLM为多视角情绪表示学习提供了高效且可扩展的解决方案。

Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed
for unsupervised contrastive multiview representation learning of facial
emotions from 3D/4D data. Our architecture integrates pseudo-labels derived
from generated textual prompts to guide implicit alignment of emotional
semantics. To capture shared information across multi-views, we propose a joint
embedding space that aligns multiview representations without requiring
explicit supervision. We further enhance the discriminability of our model
through a novel multiview contrastive learning strategy that leverages stable
positive-negative pair sampling. A gradient-friendly loss function is
introduced to promote smoother and more stable convergence, and the model is
optimized for distributed training to ensure scalability. Extensive experiments
demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and
can be easily adapted to various real-world applications with minimal
modifications.

</details>


### [76] [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/abs/2505.09372)
*Siyuan Yan,Xieji Li,Ming Hu,Yiwen Jiang,Zhen Yu,Zongyuan Ge*

Main category: cs.CV

Relevance: 40.0

TL;DR: MAKE是一个多视角知识增强的视觉语言预训练框架，用于零样本皮肤病任务，通过分解临床叙述、细粒度对齐和诊断引导加权，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 皮肤病诊断需要结合视觉特征和临床知识，但现有视觉语言预训练方法受限于文本长度和非结构化文本。

Method: 1. 多视角对比学习策略，通过LLM分解临床叙述；2. 细粒度对齐机制，连接子标题与诊断相关图像特征；3. 诊断引导加权方案，优先处理临床重要子标题。

Result: 在403,563个皮肤病图像-文本对上预训练后，MAKE在零样本皮肤病分类、概念标注和跨模态检索任务中显著优于现有VLP模型。

Conclusion: MAKE通过多视角知识增强和细粒度对齐，提升了皮肤病诊断的零样本性能。

Abstract: Dermatological diagnosis represents a complex multimodal challenge that
requires integrating visual features with specialized clinical knowledge. While
vision-language pretraining (VLP) has advanced medical AI, its effectiveness in
dermatology is limited by text length constraints and the lack of structured
texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced
vision-language pretraining framework for zero-shot dermatological tasks.
Recognizing that comprehensive dermatological descriptions require multiple
knowledge aspects that exceed standard text constraints, our framework
introduces: (1) a multi-aspect contrastive learning strategy that decomposes
clinical narratives into knowledge-enhanced sub-texts through large language
models, (2) a fine-grained alignment mechanism that connects subcaptions with
diagnostically relevant image features, and (3) a diagnosis-guided weighting
scheme that adaptively prioritizes different sub-captions based on clinical
significance prior. Through pretraining on 403,563 dermatological image-text
pairs collected from education resources, MAKE significantly outperforms
state-of-the-art VLP models on eight datasets across zero-shot skin disease
classification, concept annotation, and cross-modal retrieval tasks. Our code
will be made publicly available at https: //github.com/SiyuanYan1/MAKE.

</details>


### [77] [Text-driven Motion Generation: Overview, Challenges and Directions](https://arxiv.org/abs/2505.09379)
*Ali Rida Sahili,Najett Neji,Hedi Tabia*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该综述回顾了文本驱动运动生成的现代方法，分类为VAE、扩散和混合模型，并探讨了数据集、评估方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 提供灵活、直观的运动生成方式，适用于虚拟现实、游戏等领域。

Method: 分类方法包括架构（VAE、扩散、混合）和运动表示（离散、连续）。

Result: 总结了当前领域进展、挑战和未来方向。

Conclusion: 为语言驱动运动合成的研究者提供有价值的起点。

Abstract: Text-driven motion generation offers a powerful and intuitive way to create
human movements directly from natural language. By removing the need for
predefined motion inputs, it provides a flexible and accessible approach to
controlling animated characters. This makes it especially useful in areas like
virtual reality, gaming, human-computer interaction, and robotics. In this
review, we first revisit the traditional perspective on motion synthesis, where
models focused on predicting future poses from observed initial sequences,
often conditioned on action labels. We then provide a comprehensive and
structured survey of modern text-to-motion generation approaches, categorizing
them from two complementary perspectives: (i) architectural, dividing methods
into VAE-based, diffusion-based, and hybrid models; and (ii) motion
representation, distinguishing between discrete and continuous motion
generation strategies. In addition, we explore the most widely used datasets,
evaluation methods, and recent benchmarks that have shaped progress in this
area. With this survey, we aim to capture where the field currently stands,
bring attention to its key challenges and limitations, and highlight promising
directions for future exploration. We hope this work offers a valuable starting
point for researchers and practitioners working to push the boundaries of
language-driven human motion synthesis.

</details>


### [78] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/abs/2505.09435)
*Yili He,Yan Zhu,Peiyao Fu,Ruijie Yang,Tianyi Chen,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: Endo-CLIP是一个自监督框架，通过三阶段（清洗、调整、统一）优化CLIP模型，显著提升内窥镜图像分析的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析面临非信息背景图像、复杂医学术语和多病变描述模糊等挑战，需要改进预训练方法。

Method: Endo-CLIP采用三阶段框架：清洗（去除背景）、调整（利用LLM提取临床属性）、统一（患者级跨注意力解决多病变模糊性）。

Result: 实验表明，Endo-CLIP在零样本和少样本息肉检测与分类中显著优于现有方法。

Conclusion: Endo-CLIP为更准确和临床相关的内窥镜分析提供了新途径。

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [79] [Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos](https://arxiv.org/abs/2505.09455)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于Transformer的编码器-解码器模型，通过引入去噪序列转换任务，结合游戏状态信息，提升了足球视频中时空动作检测（STAD）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有STAD方法在高召回率、低精度要求下缺乏上下文理解，导致大量误报。论文旨在通过游戏级推理和序列去噪解决这一问题。

Method: 使用Transformer模型处理噪声预测序列和游戏状态信息，建模长时间上下文和团队动态。

Result: 方法在低置信度区间提升了精度和召回率，实现了更可靠的事件提取。

Conclusion: 结合游戏语言（战术规律和球员依赖）的序列去噪方法能有效补充现有基于像素的STAD方法。

Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.

</details>


### [80] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/abs/2505.09498)
*Bo Zhang,Shuo Li,Runhe Tian,Yang Yang,Jixin Tang,Jinhao Zhou,Lin Ma*

Main category: cs.CV

Relevance: 40.0

TL;DR: Flash-VL 2B是一种针对实时应用的视觉语言模型优化方法，通过架构增强和高效计算策略，在保持准确性的同时实现超低延迟和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限环境和实时应用需求，优化视觉语言模型的延迟和吞吐量。

Method: 采用定制架构选择、令牌压缩机制、数据整理、训练方案及新颖的图像处理技术（隐式语义拼接）。

Result: 在11个标准VLM基准测试中，Flash-VL 2B在速度和准确性上均达到最优。

Conclusion: Flash-VL 2B是资源受限和大规模实时应用的有前景解决方案。

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [81] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/abs/2505.08819)
*Asahi Miyazaki,Tsuyoshi Okita*

Main category: eess.IV

Relevance: 40.0

TL;DR: 本文提出了一种新的掩码模式Mesh Mask，用于改进SparK模型在自监督学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 研究掩码模式对图像预训练性能的影响，以提升自监督学习的效果。

Method: 提出Mesh Mask-ed SparK模型，通过新的掩码模式优化图像掩码预训练。

Result: 实验表明，Mesh Mask模式显著提升了SparK模型的性能。

Conclusion: Mesh Mask是一种有效的掩码模式，可提升自监督学习模型的性能。

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [82] [Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation](https://arxiv.org/abs/2505.09564)
*Anne-Marie Rickmann,Stephanie L. Thorn,Shawn S. Ahn,Supum Lee,Selen Uman,Taras Lysyy,Rachel Burns,Nicole Guerrera,Francis G. Spinale,Jason A. Burdick,Albert J. Sinusas,James S. Duncan*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文探讨了利用基础模型为猪心脏CT生成伪标签，并通过自训练方法迭代优化标签，无需手动标注猪数据。


<details>
  <summary>Details</summary>
Motivation: 由于猪与人类在心脏解剖和生理上的相似性，猪模型在临床前研究中常用，但物种差异导致模型直接迁移困难。基础模型在人类数据上表现良好，但其在猪数据上的适用性尚未充分研究。

Method: 提出一种自训练方法，利用基础模型生成伪标签，并通过迭代更新优化分割质量，无需手动标注猪数据。

Result: 自训练过程不仅提高了分割准确性，还平滑了连续帧间的时间不一致性。

Conclusion: 结果表明该方法有效，但仍有改进空间，如采用更复杂的自训练策略或探索其他基础模型和成像技术。

Abstract: Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.

</details>


### [83] [Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning](https://arxiv.org/abs/2505.08837)
*Muhammad Saqib,Dipkumar Mehta,Fnu Yashu,Shubham Malhotra*

Main category: cs.CR

Relevance: 40.0

TL;DR: 本文提出了一种基于强化学习（RL）的动态安全策略管理框架，用于优化云环境（如AWS）的安全性。通过深度Q网络和近端策略优化算法，该框架能够动态调整防火墙规则和IAM策略，显著提升了入侵检测率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 静态安全策略无法应对云环境中动态变化的威胁和弹性资源需求，因此需要一种自适应的方法来优化安全策略管理。

Method: 采用深度强化学习算法（如深度Q网络和近端策略优化），利用云遥测数据（如AWS Cloud Trail日志、网络流量数据和威胁情报）动态调整安全策略。

Result: 实验结果表明，该框架的入侵检测率（92%）显著高于静态策略（82%），并将事件检测和响应时间减少了58%。

Conclusion: 自适应强化学习方法在提升云安全策略管理方面具有显著效果，同时保持了高合规性和资源效率。

Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is
complex and dynamic. Static security policies have become inadequate as threats
evolve and cloud resources exhibit elasticity [1]. This paper addresses the
limitations of static policies by proposing a security policy management
framework that uses reinforcement learning (RL) to adapt dynamically.
Specifically, we employ deep reinforcement learning algorithms, including deep
Q Networks and proximal policy optimization, enabling the learning and
continuous adjustment of controls such as firewall rules and Identity and
Access Management (IAM) policies. The proposed RL based solution leverages
cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat
intelligence feeds) to continuously refine security policies, maximizing threat
mitigation, and compliance while minimizing resource impact. Experimental
results demonstrate that our adaptive RL based framework significantly
outperforms static policies, achieving higher intrusion detection rates (92%
compared to 82% for static policies) and substantially reducing incident
detection and response times by 58%. In addition, it maintains high conformity
with security requirements and efficient resource usage. These findings
validate the effectiveness of adaptive reinforcement learning approaches in
improving cloud security policy management.

</details>


### [84] [UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing](https://arxiv.org/abs/2505.09615)
*Yung-Hsuan Lai,Janek Ebbers,Yu-Chiang Frank Wang,François Germain,Michael Jeffrey Jones,Moitreya Chatterjee*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为UWAV的新方法，用于解决音频-视觉视频解析（AVVP）任务中的弱监督学习问题，通过引入不确定性加权和特征混合正则化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: AVVP任务需要定位单模态和多模态事件，但标注成本高，限制了方法的可扩展性。现有弱监督方法在生成伪标签时忽略了段间依赖性和标签预测偏差。

Method: 提出UWAV方法，通过不确定性加权伪标签和特征混合正则化，优化模型训练。

Result: 在多个指标和数据集上，UWAV优于现有最先进方法。

Conclusion: UWAV通过改进伪标签生成和训练正则化，有效提升了AVVP任务的性能。

Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.

</details>


### [85] [IntrinsicEdit: Precise generative image manipulation in intrinsic space](https://arxiv.org/abs/2505.08889)
*Linjie Lyu,Valentin Deschaintre,Yannick Hold-Geoffroy,Miloš Hašan,Jae Shin Yoon,Thomas Leimkühler,Christian Theobalt,Iliyan Georgiev*

Main category: cs.GR

Relevance: 40.0

TL;DR: 论文提出了一种基于RGB-X扩散框架的通用生成工作流，支持像素级精确编辑，解决了身份保持和通道纠缠问题，无需额外数据或微调。


<details>
  <summary>Details</summary>
Motivation: 现有生成扩散模型的图像编辑接口缺乏精确控制，且通常仅适用于单一任务。

Method: 采用RGB-X扩散框架，结合精确扩散反演和通道解缠技术，实现像素级精确编辑。

Result: 在复杂图像的多种编辑任务中（如颜色调整、对象插入/移除、全局光照调整）表现优异。

Conclusion: 该方法在无需额外数据或微调的情况下，实现了多功能、高精度的图像编辑。

Abstract: Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.

</details>


### [86] [Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery](https://arxiv.org/abs/2505.08932)
*Mohammad Wasil,Ahmad Drak,Brennan Penfold,Ludovico Scarton,Maximilian Johenneken,Alexander Asteroth,Sebastian Houben*

Main category: cs.RO

Relevance: 40.0

TL;DR: 论文提出了一种基于Segment Anything Model (SAM)的参数高效微调方法，用于无人机森林监测中的地面物体分割。


<details>
  <summary>Details</summary>
Motivation: 森林地面物体分割因自然多变性和模糊标注而具有挑战性，需要一种高效且适应性强的解决方案。

Method: 采用参数高效微调（PEFT）方法，调整SAM的掩码解码器以自动分割森林地面物体，比较了不同PEFT方法的性能。

Result: 基于适配器的PEFT方法在mIoU上表现最佳，而LoRA因其轻量级特性适用于资源受限的无人机平台。

Conclusion: SAM结合PEFT方法在森林监测任务中表现出色，为资源受限平台提供了高效解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.

</details>


### [87] [Multi-step manipulation task and motion planning guided by video demonstration](https://arxiv.org/abs/2505.08949)
*Kateryna Zorina,David Kovar,Mederic Fourmy,Florent Lamiraux,Nicolas Mansard,Justin Carpentier,Josef Sivic,Vladimir Petrik*

Main category: cs.RO

Relevance: 40.0

TL;DR: 本文提出了一种基于教学视频的多步任务与运动规划方法，扩展了RRT规划器，结合视频中的接触状态和3D物体位姿，解决了具有顺序依赖性的任务。


<details>
  <summary>Details</summary>
Motivation: 利用教学视频解决机器人中复杂的多步任务与运动规划问题，尤其是具有顺序依赖性的任务。

Method: 扩展RRT规划器，结合视频提取的接触状态和3D物体位姿，设计新基准任务验证方法。

Result: 在多个机器人平台上验证了方法的有效性，并开发了轨迹优化方法以实现真实机器人上的无缝执行。

Conclusion: 视频引导的规划方法在多步任务中表现优异，并展示了良好的泛化能力。

Abstract: This work aims to leverage instructional video to solve complex multi-step
task-and-motion planning tasks in robotics. Towards this goal, we propose an
extension of the well-established Rapidly-Exploring Random Tree (RRT) planner,
which simultaneously grows multiple trees around grasp and release states
extracted from the guiding video. Our key novelty lies in combining contact
states and 3D object poses extracted from the guiding video with a traditional
planning algorithm that allows us to solve tasks with sequential dependencies,
for example, if an object needs to be placed at a specific location to be
grasped later. We also investigate the generalization capabilities of our
approach to go beyond the scene depicted in the instructional video. To
demonstrate the benefits of the proposed video-guided planning approach, we
design a new benchmark with three challenging tasks: (I) 3D re-arrangement of
multiple objects between a table and a shelf, (ii) multi-step transfer of an
object through a tunnel, and (iii) transferring objects using a tray similar to
a waiter transfers dishes. We demonstrate the effectiveness of our planning
algorithm on several robots, including the Franka Emika Panda and the KUKA KMR
iiwa. For a seamless transfer of the obtained plans to the real robot, we
develop a trajectory refinement approach formulated as an optimal control
problem (OCP).

</details>


### [88] [Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ](https://arxiv.org/abs/2505.08990)
*Andrew C. Freeman*

Main category: cs.MM

Relevance: 40.0

TL;DR: 提出了基于Media Over QUIC Transport协议的实时内容审核扩展，用于直播流媒体，仅移除违规内容片段，并支持透明分发分析任务。


<details>
  <summary>Details</summary>
Motivation: 直播流媒体内容审核需求增长，但实时性要求高，现有技术难以满足。

Method: 扩展Media Over QUIC Transport协议，实现实时内容审核，支持透明分发分析任务。

Result: 系统在光敏观众场景下测试，仅增加一个GOP延迟。

Conclusion: 该方案有效解决了直播流媒体的实时内容审核问题。

Abstract: Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.

</details>


### [89] [TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving](https://arxiv.org/abs/2505.09315)
*Xuefeng Jiang,Yuan Ma,Pengxiang Li,Leimeng Xu,Xin Wen,Kun Zhan,Zhongpu Xia,Peng Jia,XianPeng Lang,Sheng Sun*

Main category: cs.RO

Relevance: 40.0

TL;DR: TransDiffuser是一种基于扩散模型的端到端自动驾驶轨迹规划模型，通过多模态条件输入和表示解相关优化机制生成高质量多样化轨迹。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型的潜力扩展到现代自动驾驶系统，解决轨迹规划中的模式崩溃问题。

Method: 采用编码器-解码器架构，利用多模态条件输入和表示解相关优化机制。

Result: 在NAVSIM基准测试中达到PDMS 94.85，超越现有方法。

Conclusion: TransDiffuser展示了扩散模型在自动驾驶轨迹规划中的有效性。

Abstract: In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.

</details>


### [90] [APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression](https://arxiv.org/abs/2505.09356)
*Srinivas Ravuri,Yuan Xu,Martin Ludwig Zehetner,Ketan Motlag,Sahin Albayrak*

Main category: cs.RO

Relevance: 40.0

TL;DR: APR-Transformer是一种基于Transformer的模型，用于预测3D位置和方向，适用于图像或LiDAR数据，在GNSS缺失环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决GNSS缺失环境下初始姿态不准确导致的定位问题。

Method: 提出APR-Transformer，利用深度神经网络预测绝对姿态（3D位置和方向）。

Result: 在Radar Oxford Robot-Car、DeepLoc和自定义数据集上达到SOTA性能，并在实时自动驾驶测试中验证了可靠性。

Conclusion: APR-Transformer在复杂环境中具有实用性和高效性。

Abstract: Precise initialization plays a critical role in the performance of
localization algorithms, especially in the context of robotics, autonomous
driving, and computer vision. Poor localization accuracy is often a consequence
of inaccurate initial poses, particularly noticeable in GNSS-denied
environments where GPS signals are primarily relied upon for initialization.
Recent advances in leveraging deep neural networks for pose regression have led
to significant improvements in both accuracy and robustness, especially in
estimating complex spatial relationships and orientations. In this paper, we
introduce APR-Transformer, a model architecture inspired by state-of-the-art
methods, which predicts absolute pose (3D position and 3D orientation) using
either image or LiDAR data. We demonstrate that our proposed method achieves
state-of-the-art performance on established benchmark datasets such as the
Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our
experiments to include our custom complex APR-BeIntelli dataset. Additionally,
we validate the reliability of our approach in GNSS-denied environments by
deploying the model in real-time on an autonomous test vehicle. This showcases
the practical feasibility and effectiveness of our approach. The source code is
available at:https://github.com/GT-ARC/APR-Transformer.

</details>


### [91] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/abs/2505.08817)
*Camilo Carvajal Reyes,Joaquín Fontbona,Felipe Tobar*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于外部多模态模型的SFW采样器，用于防止基于分数的生成模型（SBMs）生成不安全内容，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: SBMs在图像合成中表现卓越，但容易被诱导生成不安全内容。现有方法依赖模型自身知识且需微调，本文探索利用外部资源确保输出安全。

Method: 采用条件轨迹校正步骤，利用多模态模型（如CLIP）引导样本远离不安全区域，支持用户自定义NSFW类别。

Result: 在Stable Diffusion上验证，SFW采样器有效减少不安全内容生成，对图像质量影响较小。

Conclusion: SFW采样器适用于对齐的SBMs，展示了模型无关条件在防止不良图像中的潜力。

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [92] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/abs/2505.08833)
*Qingyi Wang,Yuebing Liang,Yunhan Zheng,Kaiyuan Xu,Jinhua Zhao,Shenhao Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出了一种基于Stable Diffusion和ControlNet的生成模型，用于根据土地利用描述生成高保真卫星图像，并通过OpenStreetMap数据增强生成多样性。模型在三个美国城市的数据上表现优异，评估显示其生成图像质量高且实用。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成大规模且实用的城市设计，因此论文旨在利用生成式AI技术提升城市规划和设计的自动化水平。

Method: 采用Stable Diffusion模型结合ControlNet，利用OpenStreetMap的结构化土地利用数据生成条件卫星图像，并通过语言提示和控制图像优化生成质量。

Result: 模型在FID和KID评分上表现优异，生成的图像被城市规划者和公众认为更符合设计需求且优于真实图像。

Conclusion: 该研究为可控城市图像生成设定了基准，展示了生成式AI在城市规划和公众参与中的潜力。

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [93] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
*Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文介绍了Maya，一个开源的多语言视觉语言模型（VLM），旨在解决现有VLM在低资源语言和多元文化背景下的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（VLMs）在学术基准上表现优异，但在低资源语言和多元文化背景下表现不佳，因此需要开发支持多语言的VLM以提升其通用性。

Method: 基于LLaVA预训练数据集，构建了一个包含八种语言的多语言图像-文本预训练数据集，并开发了支持这些语言的多语言图像-文本模型。

Result: Maya模型在视觉语言任务中提升了文化和语言理解能力，支持八种语言。

Conclusion: Maya是一个有效的多语言VLM，填补了现有模型在低资源语言和多元文化背景下的空白。

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [94] [Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking](https://arxiv.org/abs/2505.08999)
*Wei-Long Tian,Peng Gao,Xiao Liu,Long Xu,Hamido Fujita,Hanan Aljuai,Mao-Li Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种自适应元梯度对抗攻击方法（AMGA），用于揭示视觉跟踪器的安全漏洞，通过多模型集成和元学习策略显著提升对抗样本的迁移性和攻击效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的安全问题影响了视觉跟踪方法在实际场景中的可靠应用，需要揭示其安全漏洞。

Method: 结合多模型集成、元学习、动量机制和高斯平滑，随机选择模型并构建多样化跟踪场景，进行白盒和黑盒对抗攻击。

Result: 在OTB2015、LaSOT和GOT-10k等数据集上，AMGA显著提升了攻击性能、迁移性和欺骗性。

Conclusion: AMGA通过优化梯度方向，缩小了白盒和黑盒攻击之间的差距，在视觉跟踪领域表现出色。

Abstract: In recent years, visual tracking methods based on convolutional neural
networks and Transformers have achieved remarkable performance and have been
successfully applied in fields such as autonomous driving. However, the
numerous security issues exposed by deep learning models have gradually
affected the reliable application of visual tracking methods in real-world
scenarios. Therefore, how to reveal the security vulnerabilities of existing
visual trackers through effective adversarial attacks has become a critical
problem that needs to be addressed. To this end, we propose an adaptive
meta-gradient adversarial attack (AMGA) method for visual tracking. This method
integrates multi-model ensembles and meta-learning strategies, combining
momentum mechanisms and Gaussian smoothing, which can significantly enhance the
transferability and attack effectiveness of adversarial examples. AMGA randomly
selects models from a large model repository, constructs diverse tracking
scenarios, and iteratively performs both white- and black-box adversarial
attacks in each scenario, optimizing the gradient directions of each model.
This paradigm minimizes the gap between white- and black-box adversarial
attacks, thus achieving excellent attack performance in black-box scenarios.
Extensive experimental results on large-scale datasets such as OTB2015, LaSOT,
and GOT-10k demonstrate that AMGA significantly improves the attack
performance, transferability, and deception of adversarial examples. Codes and
data are available at https://github.com/pgao-lab/AMGA.

</details>


### [95] [Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning](https://arxiv.org/abs/2505.09118)
*Dayong Liang,Changmeng Zheng,Zhiyuan Wen,Yi Cai,Xiao-Yong Wei,Qing Li*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种增强视觉语言模型（VLMs）交互推理能力的框架ISGR，通过双流图构建、交互查询和长期记忆强化学习，显著提升了复杂场景理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 传统场景图主要关注空间关系，限制了VLMs对复杂交互的推理能力。本文旨在解决检测到构建方法生成无关关系集和现有方法缺乏持久记忆的问题。

Method: ISGR框架包含三个部分：双流图构建器（结合空间关系提取和交互感知标注）、交互查询激活VLMs功能知识、长期记忆强化学习策略。

Result: 实验表明，ISGR在交互密集的推理基准上显著优于基线方法，尤其在复杂场景理解任务中表现突出。

Conclusion: ISGR通过增强交互推理能力，显著提升了VLMs在复杂场景中的表现。

Abstract: Traditional scene graphs primarily focus on spatial relationships, limiting
vision-language models' (VLMs) ability to reason about complex interactions in
visual scenes. This paper addresses two key challenges: (1) conventional
detection-to-construction methods produce unfocused, contextually irrelevant
relationship sets, and (2) existing approaches fail to form persistent memories
for generalizing interaction reasoning to new scenes. We propose
Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances
VLMs' interactional reasoning through three complementary components. First,
our dual-stream graph constructor combines SAM-powered spatial relation
extraction with interaction-aware captioning to generate functionally salient
scene graphs with spatial grounding. Second, we employ targeted interaction
queries to activate VLMs' latent knowledge of object functionalities,
converting passive recognition into active reasoning about how objects work
together. Finally, we introduce a lone-term memory reinforcement learning
strategy with a specialized interaction-focused reward function that transforms
transient patterns into long-term reasoning heuristics. Extensive experiments
demonstrate that our approach significantly outperforms baseline methods on
interaction-heavy reasoning benchmarks, with particularly strong improvements
on complex scene understanding tasks. The source code can be accessed at
https://github.com/open_upon_acceptance.

</details>


### [96] [Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance](https://arxiv.org/abs/2505.09123)
*Guoying Liang,Su Yang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该研究提出了一种利用Segment Anything Model (SAM)进行伪装目标检测(COD)的新框架，通过点提示提升SAM的性能，并在实验中取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究认为SAM不适用于COD，但本研究旨在证明通过适当的提示提升，SAM可以有效地用于COD任务。

Method: 开发了Promotion Point Targeting Network (PPT-net)用于预测伪装目标的存在概率，并设计了关键点选择(KPS)算法，通过正负点提示对比引导SAM进行分割。

Result: 在3个数据集和6个指标上，该方法取得了优于现有方法的结果。

Conclusion: 该研究表明，通过利用SAM进行COD任务，不仅性能优于专业模型，还能将问题简化为寻找信息性而非精确的提示。

Abstract: Big model has emerged as a new research paradigm that can be applied to
various down-stream tasks with only minor effort for domain adaption.
Correspondingly, this study tackles Camouflaged Object Detection (COD)
leveraging the Segment Anything Model (SAM). The previous studies declared that
SAM is not workable for COD but this study reveals that SAM works if promoted
properly, for which we devise a new framework to render point promotions:
First, we develop the Promotion Point Targeting Network (PPT-net) to leverage
multi-scale features in predicting the probabilities of camouflaged objects'
presences at given candidate points over the image. Then, we develop a key
point selection (KPS) algorithm to deploy both positive and negative point
promotions contrastively to SAM to guide the segmentation. It is the first work
to facilitate big model for COD and achieves plausible results experimentally
over the existing methods on 3 data sets under 6 metrics. This study
demonstrates an off-the-shelf methodology for COD by leveraging SAM, which
gains advantage over designing professional models from scratch, not only in
performance, but also in turning the problem to a less challenging task, that
is, seeking informative but not exactly precise promotions.

</details>


### [97] [TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation](https://arxiv.org/abs/2505.09140)
*Zechao Guan,Feng Yan,Shuai Du,Lin Ma,Qingshan Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为TopoDiT-3D的拓扑感知扩散Transformer模型，用于3D点云生成，通过结合全局拓扑信息改进了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer模型在3D点云生成中主要关注局部特征提取，而忽略了全局拓扑信息（如空洞），这影响了形状一致性和复杂几何的捕捉。

Method: 提出TopoDiT-3D，采用瓶颈结构和Perceiver Resampler，将持久同调提取的拓扑信息融入特征学习，并自适应过滤冗余局部特征以提高训练效率。

Result: 实验表明TopoDiT-3D在视觉质量、多样性和训练效率上优于现有模型，并验证了拓扑信息与局部特征学习的协同作用。

Conclusion: TopoDiT-3D证明了全局拓扑信息对3D点云生成的重要性，并展示了其与局部特征学习的互补性。

Abstract: Recent advancements in Diffusion Transformer (DiT) models have significantly
improved 3D point cloud generation. However, existing methods primarily focus
on local feature extraction while overlooking global topological information,
such as voids, which are crucial for maintaining shape consistency and
capturing complex geometries. To address this limitation, we propose
TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure
for 3D point cloud generation. Specifically, we design the bottleneck structure
utilizing Perceiver Resampler, which not only offers a mode to integrate
topological information extracted through persistent homology into feature
learning, but also adaptively filters out redundant local features to improve
training efficiency. Experimental results demonstrate that TopoDiT-3D
outperforms state-of-the-art models in visual quality, diversity, and training
efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich
topological information for 3D point cloud generation and its synergy with
conventional local feature learning. Videos and code are available at
https://github.com/Zechao-Guan/TopoDiT-3D.

</details>


### [98] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/abs/2505.09178)
*Yitao Zhu,Yuan Yin,Zhenrong Shen,Zihao Zhao,Haiyu Song,Sheng Wang,Dinggang Shen,Qian Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: UniCAD是一个基于预训练视觉基础模型的多任务计算机辅助诊断（CAD）统一架构，通过低秩适配和模块化设计实现高效、可扩展的医疗图像处理。


<details>
  <summary>Details</summary>
Motivation: 解决医疗影像领域缺乏开源CAD平台以及预训练视觉模型在医疗图像任务中资源密集的问题。

Method: 采用低秩适配策略和模块化架构，结合冻结的基础模型与可插拔的专家模块。

Result: 在12个医疗数据集上表现优于现有方法，同时仅需0.17%的可训练参数。

Conclusion: UniCAD为医疗影像研究提供了一个高效、可扩展的开源平台。

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


### [99] [Zero-shot Quantization: A Comprehensive Survey](https://arxiv.org/abs/2505.09188)
*Minjun Kim,Jaehyeon Choi,Jongkeun Lee,Wonjin Cho,U Kang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 本文综述了零样本量化（ZSQ）方法，这是一种无需真实数据即可实现深度学习模型量化的技术，解决了传统量化方法依赖训练数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法需要访问训练数据，但在隐私、安全或监管限制下不切实际。ZSQ提供了一种无需真实数据的解决方案，填补了这一空白。

Method: 文章首先定义了ZSQ问题并指出关键挑战，然后根据数据生成策略对现有ZSQ方法进行分类，分析其动机、核心思想和关键要点。

Result: 综述了ZSQ的最新进展，并提出了未来研究方向以解决现有局限性。

Conclusion: 本文是首个关于ZSQ的深度综述，为未来研究提供了重要参考。

Abstract: Network quantization has proven to be a powerful approach to reduce the
memory and computational demands of deep learning models for deployment on
resource-constrained devices. However, traditional quantization methods often
rely on access to training data, which is impractical in many real-world
scenarios due to privacy, security, or regulatory constraints. Zero-shot
Quantization (ZSQ) emerges as a promising solution, achieving quantization
without requiring any real data. In this paper, we provide a comprehensive
overview of ZSQ methods and their recent advancements. First, we provide a
formal definition of the ZSQ problem and highlight the key challenges. Then, we
categorize the existing ZSQ methods into classes based on data generation
strategies, and analyze their motivations, core ideas, and key takeaways.
Lastly, we suggest future research directions to address the remaining
limitations and advance the field of ZSQ. To the best of our knowledge, this
paper is the first in-depth survey on ZSQ.

</details>


### [100] [Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping](https://arxiv.org/abs/2505.09252)
*Yinuo Wang,Yue Zeng,Kai Chen,Cai Meng,Chao Pan,Zhouping Tang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该研究比较了多模态大语言模型（MLLMs）与传统深度学习模型在颅内出血（ICH）分类任务中的表现，发现传统模型在准确性和子类型分类上优于MLLMs，但MLLMs在交互性和可解释性方面有潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估MLLMs在医学影像分析中的表现，尤其是在低对比度和模糊边界挑战下的ICH分类任务。

Method: 使用RSNA提供的192个NCCT数据集，比较了GPT-4o、Gemini 2.0 Flash、Claude 3.5 Sonnet V2等MLLMs与ResNet50、Vision Transformer等传统模型的表现。

Result: 传统模型在ICH二元分类和子类型分类任务中全面优于MLLMs，Gemini 2.0 Flash的宏平均精度为0.41，F1分数为0.31。

Conclusion: MLLMs在交互性方面表现优异，但在准确性上不及传统模型，未来需优化模型以提高在三维医学影像处理中的性能。

Abstract: Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes
on non-contrast computed tomography is critical for prognosis prediction and
therapeutic decision-making, yet remains challenging due to low contrast and
blurring boundaries. This study evaluates the performance of zero-shot
multi-modal large language models (MLLMs) compared to traditional deep learning
methods in ICH binary classification and subtyping. Methods: We utilized a
dataset provided by RSNA, comprising 192 NCCT volumes. The study compares
various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,
with conventional deep learning models, including ResNet50 and Vision
Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such
as ICH presence, subtype classification, localization, and volume estimation.
Results: The results indicate that in the ICH binary classification task,
traditional deep learning models outperform MLLMs comprehensively. For subtype
classification, MLLMs also exhibit inferior performance compared to traditional
deep learning models, with Gemini 2.0 Flash achieving an macro-averaged
precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While
MLLMs excel in interactive capabilities, their overall accuracy in ICH
subtyping is inferior to deep networks. However, MLLMs enhance interpretability
through language interactions, indicating potential in medical imaging
analysis. Future efforts will focus on model refinement and developing more
precise MLLMs to improve performance in three-dimensional medical image
processing.

</details>


### [101] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/abs/2505.09263)
*Guan Gui,Bin-Bin Gao,Jun Liu,Chengjie Wang,Yunsheng Wu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为AnoGen的少样本异常生成方法，通过扩散模型生成真实多样的异常样本，以提升工业检测中异常检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 工业检测中异常样本稀缺，现有方法生成的异常与真实异常存在语义差距，导致检测性能不佳。

Method: 分三阶段：1) 基于少量真实异常学习异常分布并嵌入知识；2) 利用嵌入和边界框指导扩散模型生成异常；3) 提出弱监督异常检测方法训练模型。

Result: 在MVTec数据集上，DRAEM和DesTSeg的AU-PR指标分别提升了5.8%和1.5%。

Conclusion: AnoGen方法能有效生成真实异常，显著提升异常检测性能。

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [102] [Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2505.09336)
*Muzammil Behzad*

Main category: cs.CV

Relevance: 40.0

TL;DR: MultiviewVLM是一个视觉语言模型，用于从3D/4D数据中无监督对比多视图表示学习面部情绪。通过伪标签和联合嵌入空间实现多视图语义对齐，并结合对比学习策略和梯度友好损失函数提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决多视图面部情绪表示学习中的无监督对齐问题，通过结合视觉和语言模态提升模型性能。

Method: 模型利用伪标签和联合嵌入空间进行多视图对齐，采用对比学习策略和梯度友好损失函数优化训练。

Result: MultiviewVLM在实验中表现优于现有方法，且易于适应实际应用。

Conclusion: MultiviewVLM为多视图情绪表示学习提供了一种高效且可扩展的解决方案。

Abstract: In this paper, we introduce MultiviewVLM, a vision-language model designed
for unsupervised contrastive multiview representation learning of facial
emotions from 3D/4D data. Our architecture integrates pseudo-labels derived
from generated textual prompts to guide implicit alignment of emotional
semantics. To capture shared information across multi-views, we propose a joint
embedding space that aligns multiview representations without requiring
explicit supervision. We further enhance the discriminability of our model
through a novel multiview contrastive learning strategy that leverages stable
positive-negative pair sampling. A gradient-friendly loss function is
introduced to promote smoother and more stable convergence, and the model is
optimized for distributed training to ensure scalability. Extensive experiments
demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and
can be easily adapted to various real-world applications with minimal
modifications.

</details>


### [103] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/abs/2505.09358)
*Bingxin Ke,Kevin Qu,Tianfu Wang,Nando Metzger,Shengyu Huang,Bo Li,Anton Obukhov,Konrad Schindler*

Main category: cs.CV

Relevance: 40.0

TL;DR: Marigold是一种基于预训练潜在扩散模型（如Stable Diffusion）的条件生成模型家族，通过微调协议将其知识迁移到密集图像分析任务中，如深度估计和表面法线预测。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的情况下，预训练模型的质量对迁移学习至关重要。本文旨在利用文本到图像生成模型的视觉理解能力，将其知识迁移到密集图像分析任务中。

Method: 提出Marigold模型家族和微调协议，通过最小化预训练潜在扩散模型的架构修改，利用小规模合成数据集在单GPU上进行训练。

Result: Marigold在零样本泛化任务中表现出色，达到了最先进的性能。

Conclusion: Marigold展示了从生成模型中提取知识并迁移到密集图像分析任务的有效性，为数据稀缺场景提供了新解决方案。

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [104] [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/abs/2505.09372)
*Siyuan Yan,Xieji Li,Ming Hu,Yiwen Jiang,Zhen Yu,Zongyuan Ge*

Main category: cs.CV

Relevance: 40.0

TL;DR: MAKE是一个多视角知识增强的视觉语言预训练框架，用于零样本皮肤病任务，通过分解临床叙述、细粒度对齐和诊断引导加权，显著优于现有VLP模型。


<details>
  <summary>Details</summary>
Motivation: 皮肤病诊断需要整合视觉特征和临床知识，但现有视觉语言预训练（VLP）方法受限于文本长度和缺乏结构化文本。

Method: 1. 多视角对比学习策略分解临床叙述；2. 细粒度对齐机制连接子标题与图像特征；3. 诊断引导加权方案优先临床重要子标题。

Result: 在403,563个皮肤病图像-文本对上预训练后，MAKE在8个数据集上零样本任务表现显著优于现有VLP模型。

Conclusion: MAKE通过多视角知识增强和细粒度对齐，提升了皮肤病诊断的零样本能力。

Abstract: Dermatological diagnosis represents a complex multimodal challenge that
requires integrating visual features with specialized clinical knowledge. While
vision-language pretraining (VLP) has advanced medical AI, its effectiveness in
dermatology is limited by text length constraints and the lack of structured
texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced
vision-language pretraining framework for zero-shot dermatological tasks.
Recognizing that comprehensive dermatological descriptions require multiple
knowledge aspects that exceed standard text constraints, our framework
introduces: (1) a multi-aspect contrastive learning strategy that decomposes
clinical narratives into knowledge-enhanced sub-texts through large language
models, (2) a fine-grained alignment mechanism that connects subcaptions with
diagnostically relevant image features, and (3) a diagnosis-guided weighting
scheme that adaptively prioritizes different sub-captions based on clinical
significance prior. Through pretraining on 403,563 dermatological image-text
pairs collected from education resources, MAKE significantly outperforms
state-of-the-art VLP models on eight datasets across zero-shot skin disease
classification, concept annotation, and cross-modal retrieval tasks. Our code
will be made publicly available at https: //github.com/SiyuanYan1/MAKE.

</details>


### [105] [Text-driven Motion Generation: Overview, Challenges and Directions](https://arxiv.org/abs/2505.09379)
*Ali Rida Sahili,Najett Neji,Hedi Tabia*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文综述了文本驱动运动生成领域，探讨了传统与现代方法，分类了架构和运动表示，并总结了数据集、评估方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 提供灵活、直观的文本到运动生成方法，推动虚拟现实、游戏、人机交互和机器人等领域的发展。

Method: 分类方法包括架构（VAE、扩散、混合模型）和运动表示（离散与连续生成策略），并总结数据集和评估方法。

Result: 全面概述了当前文本驱动运动生成的研究现状、挑战和未来方向。

Conclusion: 该领域仍有挑战，但前景广阔，为研究者提供了有价值的起点。

Abstract: Text-driven motion generation offers a powerful and intuitive way to create
human movements directly from natural language. By removing the need for
predefined motion inputs, it provides a flexible and accessible approach to
controlling animated characters. This makes it especially useful in areas like
virtual reality, gaming, human-computer interaction, and robotics. In this
review, we first revisit the traditional perspective on motion synthesis, where
models focused on predicting future poses from observed initial sequences,
often conditioned on action labels. We then provide a comprehensive and
structured survey of modern text-to-motion generation approaches, categorizing
them from two complementary perspectives: (i) architectural, dividing methods
into VAE-based, diffusion-based, and hybrid models; and (ii) motion
representation, distinguishing between discrete and continuous motion
generation strategies. In addition, we explore the most widely used datasets,
evaluation methods, and recent benchmarks that have shaped progress in this
area. With this survey, we aim to capture where the field currently stands,
bring attention to its key challenges and limitations, and highlight promising
directions for future exploration. We hope this work offers a valuable starting
point for researchers and practitioners working to push the boundaries of
language-driven human motion synthesis.

</details>


### [106] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/abs/2505.09385)
*Xiaoyang Yu,Xiaoming Wu,Xin Wang,Dongrun Li,Ming Yang,Peng Cheng*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种名为FedSaaS的联邦语义分割框架，通过类样本和对比损失解决异构问题中的类一致性表示问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理异构问题（如域偏移）时忽略了语义空间中的细粒度类关系，导致类表示模糊。

Method: 引入类样本作为局部和全局类表示的准则，服务器端建模类原型监督客户端全局分支，客户端通过对抗机制协调全局和局部分支，并使用多级对比损失增强一致性。

Result: 在多个驾驶场景分割数据集上表现优于现有方法，显著提高了平均分割精度。

Conclusion: FedSaaS有效解决了类一致性表示问题，提升了联邦语义分割的性能。

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [107] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/abs/2505.09435)
*Yili He,Yan Zhu,Peiyao Fu,Ruijie Yang,Tianyi Chen,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: Endo-CLIP是一个针对内窥镜图像分析的自监督框架，通过三阶段（清洗、调整、统一）优化CLIP，显著提升了零样本和少样本息肉检测与分类的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析面临非信息背景、复杂医学术语和多病变描述模糊等挑战，需要改进预训练方法。

Method: Endo-CLIP采用三阶段框架：清洗（去除背景）、调整（利用LLM提取临床属性）、统一（患者级跨注意力解决多息肉模糊性）。

Result: Endo-CLIP在零样本和少样本任务中显著优于现有预训练方法。

Conclusion: Endo-CLIP为更准确和临床相关的内窥镜分析铺平了道路。

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [108] [Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos](https://arxiv.org/abs/2505.09455)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于Transformer的编码器-解码器模型，通过结合游戏状态信息和序列去噪任务，提升了足球视频中时空动作检测（STAD）的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有STAD方法在高召回率、低精度场景下（如足球分析）因缺乏上下文理解而产生大量误报，需通过更广泛的序列和游戏状态信息解决。

Method: 使用Transformer模型处理噪声预测序列和游戏状态信息，通过建模长时间上下文和团队动态，生成去噪后的动作序列。

Result: 方法在低置信度场景下提升了STAD的精度和召回率，使事件提取更可靠。

Conclusion: 通过结合游戏状态和序列去噪，模型能更好地利用足球的战术规律和球员依赖关系，改进STAD性能。

Abstract: State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.

</details>


### [109] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/abs/2505.09498)
*Bo Zhang,Shuo Li,Runhe Tian,Yang Yang,Jixin Tang,Jinhao Zhou,Lin Ma*

Main category: cs.CV

Relevance: 40.0

TL;DR: Flash-VL 2B是一种优化视觉语言模型（VLM）的新方法，专注于实时应用，通过架构增强和计算策略实现低延迟和高吞吐，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限环境和实时应用需求，研究如何在保持性能的同时优化VLM的效率和速度。

Method: 采用定制架构、令牌压缩、数据筛选、训练方案和新型图像处理技术（隐式语义拼接）。

Result: 在11个标准VLM基准测试中，Flash-VL 2B在速度和准确性上均达到最优。

Conclusion: Flash-VL 2B为资源受限和大规模实时应用提供了高效解决方案。

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [110] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/abs/2505.08819)
*Asahi Miyazaki,Tsuyoshi Okita*

Main category: eess.IV

Relevance: 40.0

TL;DR: 本文提出了一种新的掩码模式（Mesh Mask）用于SparK模型，研究了掩码模式对预训练性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索掩码模式在自监督学习中的作用，以提升SparK模型的性能。

Method: 提出Mesh Mask掩码模式，并在SparK模型中进行实验验证。

Result: 展示了Mesh Mask对预训练性能的具体影响。

Conclusion: Mesh Mask是一种有效的掩码模式，可提升SparK模型的性能。

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [111] [Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation](https://arxiv.org/abs/2505.09564)
*Anne-Marie Rickmann,Stephanie L. Thorn,Shawn S. Ahn,Supum Lee,Selen Uman,Taras Lysyy,Rachel Burns,Nicole Guerrera,Francis G. Spinale,Jason A. Burdick,Albert J. Sinusas,James S. Duncan*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文探讨了如何利用基础模型为猪心脏CT生成伪标签，并通过自训练方法迭代优化这些标签，无需手动标注数据。


<details>
  <summary>Details</summary>
Motivation: 由于猪与人类在解剖和生理上的相似性，猪模型在临床前成像中常用，但物种差异导致模型直接迁移困难。基础模型在人类数据上表现良好，但在猪数据上的适用性尚未充分研究。

Method: 提出了一种简单的自训练方法，利用基础模型生成伪标签，并通过迭代更新优化分割质量，无需手动标注猪数据。

Result: 自训练过程不仅提高了分割准确性，还平滑了连续帧之间的时间不一致性。

Conclusion: 尽管结果令人鼓舞，但仍可通过更复杂的自训练策略和探索其他基础模型及成像技术进一步改进。

Abstract: Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.

</details>


### [112] [Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning](https://arxiv.org/abs/2505.08837)
*Muhammad Saqib,Dipkumar Mehta,Fnu Yashu,Shubham Malhotra*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了一种基于强化学习（RL）的动态安全策略管理框架，用于优化云环境（如AWS）的安全策略，显著提升了入侵检测率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 静态安全策略在动态云环境中已显不足，需要一种能够自适应调整的方法以应对不断变化的威胁和资源弹性。

Method: 使用深度强化学习算法（如深度Q网络和近端策略优化），结合云遥测数据（如AWS Cloud Trail日志、网络流量数据）动态调整防火墙规则和IAM策略。

Result: 实验表明，该框架的入侵检测率（92%）显著高于静态策略（82%），并将事件检测和响应时间减少了58%，同时保持高合规性和资源效率。

Conclusion: 自适应强化学习方法在云安全策略管理中具有显著优势，能够有效提升安全性和效率。

Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is
complex and dynamic. Static security policies have become inadequate as threats
evolve and cloud resources exhibit elasticity [1]. This paper addresses the
limitations of static policies by proposing a security policy management
framework that uses reinforcement learning (RL) to adapt dynamically.
Specifically, we employ deep reinforcement learning algorithms, including deep
Q Networks and proximal policy optimization, enabling the learning and
continuous adjustment of controls such as firewall rules and Identity and
Access Management (IAM) policies. The proposed RL based solution leverages
cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat
intelligence feeds) to continuously refine security policies, maximizing threat
mitigation, and compliance while minimizing resource impact. Experimental
results demonstrate that our adaptive RL based framework significantly
outperforms static policies, achieving higher intrusion detection rates (92%
compared to 82% for static policies) and substantially reducing incident
detection and response times by 58%. In addition, it maintains high conformity
with security requirements and efficient resource usage. These findings
validate the effectiveness of adaptive reinforcement learning approaches in
improving cloud security policy management.

</details>


### [113] [Don't Forget your Inverse DDIM for Image Editing](https://arxiv.org/abs/2505.09571)
*Guillermo Gomez-Trenado,Pablo Mesejo,Oscar Cordón,Stéphane Lathuilière*

Main category: cs.CV

Relevance: 40.0

TL;DR: SAGE是一种基于预训练扩散模型的新型图像编辑技术，通过自注意力机制优化图像重建，显著提升了编辑效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像编辑方法计算成本高或重建效果差的问题。

Method: 基于DDIM算法，利用扩散U-Net的自注意力层设计新的引导机制，优化图像重建。

Result: 在定量和定性评估中表现优异，用户研究中47名用户全部偏好SAGE，且在10项定量分析中7项排名第一。

Conclusion: SAGE通过自注意力机制显著提升了图像编辑的效率和质量。

Abstract: The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.

</details>


### [114] [UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing](https://arxiv.org/abs/2505.09615)
*Yung-Hsuan Lai,Janek Ebbers,Yu-Chiang Frank Wang,François Germain,Michael Jeffrey Jones,Moitreya Chatterjee*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为UWAV的新方法，用于解决音频-视觉视频解析（AVVP）任务中的弱监督学习问题，通过引入不确定性加权和特征混合正则化提升性能。


<details>
  <summary>Details</summary>
Motivation: AVVP任务需要定位单模态和多模态事件，但标注成本高，限制了方法的可扩展性。现有方法生成伪标签时缺乏段间依赖性和存在偏差，影响了性能。

Method: 提出UWAV方法，引入不确定性加权伪标签和特征混合正则化，优化弱监督训练。

Result: 在多个指标和数据集上，UWAV优于现有方法，证明了其有效性和泛化能力。

Conclusion: UWAV通过改进伪标签生成和训练正则化，显著提升了AVVP任务的性能。

Abstract: Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.

</details>


### [115] [IntrinsicEdit: Precise generative image manipulation in intrinsic space](https://arxiv.org/abs/2505.08889)
*Linjie Lyu,Valentin Deschaintre,Yannick Hold-Geoffroy,Miloš Hašan,Jae Shin Yoon,Thomas Leimkühler,Christian Theobalt,Iliyan Georgiev*

Main category: cs.GR

Relevance: 40.0

TL;DR: 论文提出了一种基于RGB-X扩散框架的通用生成工作流，用于实现像素级精确的图像编辑，解决了身份保持和通道纠缠问题，无需额外数据或微调。


<details>
  <summary>Details</summary>
Motivation: 现有生成扩散模型在图像编辑中缺乏精确控制，且通常仅适用于单一任务。

Method: 利用RGB-X扩散框架，结合精确扩散反演和分离通道操作，实现像素级精确编辑。

Result: 在复杂图像上展示了最先进的性能，包括颜色纹理调整、对象插入移除、全局光照调整等任务。

Conclusion: 该方法为图像编辑提供了高效、精确的解决方案，适用于多种任务。

Abstract: Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.

</details>


### [116] [Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery](https://arxiv.org/abs/2505.08932)
*Mohammad Wasil,Ahmad Drak,Brennan Penfold,Ludovico Scarton,Maximilian Johenneken,Alexander Asteroth,Sebastian Houben*

Main category: cs.RO

Relevance: 40.0

TL;DR: 该论文将Segment Anything Model（SAM）适配用于森林地面物体分割，采用参数高效微调（PEFT）方法，在保持原模型权重固定的同时微调少量参数，实现了自动分割。


<details>
  <summary>Details</summary>
Motivation: 解决森林地面物体分割的挑战，如高自然变异性和模糊标注。

Method: 使用PEFT方法微调SAM的掩码解码器，生成对应数据类别的掩码，无需手动提示。

Result: 基于适配器的PEFT方法获得最高mIoU，LoRA方法则提供轻量级替代方案。

Conclusion: PEFT方法在森林地面分割中表现优异，LoRA适合资源受限平台。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.

</details>


### [117] [TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving](https://arxiv.org/abs/2505.09315)
*Xuefeng Jiang,Yuan Ma,Pengxiang Li,Leimeng Xu,Xin Wen,Kun Zhan,Zhongpu Xia,Peng Jia,XianPeng Lang,Sheng Sun*

Main category: cs.RO

Relevance: 40.0

TL;DR: TransDiffuser是一种基于扩散模型的生成式轨迹规划模型，用于端到端自动驾驶，通过多模态表示去相关优化机制解决模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型的能力应用于现代自动驾驶系统，以生成高质量且多样化的轨迹。

Method: 提出TransDiffuser，一种基于编码器-解码器的生成模型，通过多模态条件输入和去相关优化机制训练。

Result: 在NAVSIM基准测试中达到PDMS 94.85，超越现有方法。

Conclusion: TransDiffuser在自动驾驶轨迹规划中表现出色，无需基于锚点的先验轨迹。

Abstract: In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.

</details>


### [118] [DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images](https://arxiv.org/abs/2505.09334)
*Sadman Sakib Alif,Nasim Anzum Promise,Fiaz Al Abid,Aniqua Nusrat Zereen*

Main category: eess.IV

Relevance: 40.0

TL;DR: 论文提出了一种基于知识蒸馏的轻量级模型（DCSNet），用于肺癌检测，结合可解释AI技术提升透明度，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗图像分析中的高计算成本和缺乏透明度问题，以促进AI在医疗领域的应用。

Method: 使用知识蒸馏技术，将大型CNN模型（如ResNet50）的知识转移到轻量级学生模型DCSNet，并结合可解释AI技术。

Result: DCSNet在资源受限环境下保持高诊断性能，同时提升了模型透明度。

Conclusion: 该方法为医疗领域提供了一种高效、透明的AI诊断工具。

Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.

</details>


### [119] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种基于定向图神经网络（DGNN）的在线行为监测系统，用于分类火车司机的警觉状态，结合面部和骨骼特征达到最高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统铁路安全系统（如死机开关）功能有限，需更先进的在线监测技术以提高安全性。

Method: 使用定制DGNN模型，比较三种特征配置（骨骼、面部、组合）进行状态分类。

Result: 组合特征在三分类模型中准确率最高（80.88%），二分类警觉检测准确率超过99%。

Conclusion: 该研究通过视觉技术提升了铁路安全监测能力，并首次引入模拟病理条件数据集。

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [120] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/abs/2505.08808)
*Anqing Jiang,Jinhao Chai,Yu Gao,Yiru Wang,Yuwen Heng,Zhigang Sun,Hao Sun,Zezhong Zhao,Li Sun,Jian Zhou,Lijuan Zhu,Shugong Xu,Hao Zhao*

Main category: cs.CV

Relevance: 30.0

TL;DR: 本文提出了一种优化的稀疏表示方法用于高清地图构建，通过专用网络架构、稀疏-密集分割辅助任务和去噪模块，显著提升了性能，超越了密集方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏表示在高清地图构建中效率更高，但现有方法因缺乏针对性设计而性能不足。本文旨在通过系统性改进，使稀疏方法超越密集方法。

Method: 引入专用网络架构优化稀疏特征提取，设计稀疏-密集分割辅助任务，并利用物理先验指导的去噪模块。

Result: 在nuScenes数据集上达到SOTA性能，SparseMeXt-Tiny的mAP为55.5%（32 fps），SparseMeXt-Base为65.2%，SparseMeXt-Large为68.9%（20 fps）。

Conclusion: 稀疏方法在高清地图构建中具有巨大潜力，能够重新定义效率与性能的权衡。

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [121] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/abs/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种结合自监督训练和多列CNN的模型，用于解决人群计数中的标注数据需求高和场景复杂性挑战；同时提出了一种基于VGG19的时空模型用于人群异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决人群计数中深度模型对大量标注数据的依赖以及场景复杂性（如遮挡、非均匀密度等）问题，同时提升人群异常检测的准确性和泛化能力。

Method: 1. 人群计数：结合自监督训练和多列CNN（MCNN），学习多层次特征；2. 异常检测：基于VGG19的时空模型，结合CNN提取空间特征，LSTM提取时序特征，并使用密集残差块优化性能。

Result: 在ShanghaiTech、UCFQNRF数据集上验证了人群计数模型的有效性（MAE和MSE指标）；在Hockey Fight和SCVD数据集上，异常检测模型优于现有方法。

Conclusion: 提出的方法在人群计数和异常检测任务中表现出色，解决了数据依赖和场景复杂性挑战。

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [122] [2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition](https://arxiv.org/abs/2505.09073)
*J. Brennan Peace,Shuowen Hu,Benjamin S. Riggan*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种新颖的领域自适应框架，通过联合注意力映射和熵正则化损失，提升2D面部图像与3D点云数据之间的相关性，以解决面部识别中因姿态差异导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 面部识别在姿态差异较大时性能下降，现有方法难以解决这一问题。

Method: 使用联合注意力映射和熵正则化损失，增强2D与3D表示之间的相关性。

Result: 在FaceScape和ARL-VTF数据集上表现优于竞争方法，姿态不变性显著提升。

Conclusion: 提出的框架有效提升了面部识别在姿态差异下的性能。

Abstract: Despite recent advances in facial recognition, there remains a fundamental
issue concerning degradations in performance due to substantial perspective
(pose) differences between enrollment and query (probe) imagery. Therefore, we
propose a novel domain adaptive framework to facilitate improved performances
across large discrepancies in pose by enabling image-based (2D) representations
to infer properties of inherently pose invariant point cloud (3D)
representations. Specifically, our proposed framework achieves better pose
invariance by using (1) a shared (joint) attention mapping to emphasize common
patterns that are most correlated between 2D facial images and 3D facial data
and (2) a joint entropy regularizing loss to promote better
consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D
and 3D representations$\unicode{x2014}$by leveraging both attention maps. This
framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms
competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)
TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and
1.57$\unicode{x0025}$, respectively.

</details>


### [123] [OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions](https://arxiv.org/abs/2505.09092)
*Yuhang Wang,Abdulaziz Alhuraish,Shengming Yuan,Hao Zhou*

Main category: cs.CV

Relevance: 30.0

TL;DR: OpenLKA是一个开放的大规模数据集，用于评估和改进车道保持辅助系统（LKA），包含400小时的驾驶数据，覆盖多种复杂场景。


<details>
  <summary>Details</summary>
Motivation: 由于专有系统和数据访问限制，LKA在现实世界中的性能研究不足，OpenLKA旨在填补这一空白。

Method: 通过收集50多种车型的驾驶数据，结合CAN总线流、高清视频、Openpilot输出和视觉语言模型生成的注释，构建多模态数据集。

Result: OpenLKA提供了一个全面的平台，用于评估LKA系统性能、识别安全关键场景，并评估道路基础设施对自动驾驶的适应性。

Conclusion: OpenLKA为LKA系统的研究和改进提供了宝贵的资源，并推动了自动驾驶技术的发展。

Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.

</details>


### [124] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/abs/2505.09129)
*Wei Meng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于颜色特征的轻量级异常检测框架，用于资源受限和数据敏感条件下的战术监控视频，通过无监督KMeans聚类和RGB通道直方图建模实现异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在高风险安全任务中面临挑战，特别是在无标签和数据不可利用的视频环境中。本文旨在快速识别和解释潜在威胁事件。

Method: 融合无监督KMeans聚类和RGB通道直方图建模，检测关键帧中的结构异常和颜色突变信号。

Result: 实验成功识别了与高能光源、目标存在和反射干扰相关的高度异常帧，展示了方法的部署能力和战术解释价值。

Conclusion: 颜色特征作为低语义战场信号载体具有重要意义，未来将结合图神经网络和时间建模进一步扩展战场智能感知能力。

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [125] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/abs/2505.09168)
*Jianlin Sun,Xiaolin Fang,Juwei Guan,Dongdong Gui,Teqi Wang,Tongxin Zhu*

Main category: cs.CV

Relevance: 30.0

TL;DR: DRRNet提出了一种四阶段架构，通过全局与局部特征融合解决伪装目标检测中的边缘细节丢失和背景干扰问题。


<details>
  <summary>Details</summary>
Motivation: 伪装目标检测中，目标与背景在颜色、纹理和形状上的高度相似性导致现有方法难以平衡全局语义信息和局部细节。

Method: DRRNet采用四阶段流程，包括全局特征提取、局部细节补充、双表征融合和反向细化模块。

Result: 实验表明，DRRNet在基准数据集上显著优于现有方法。

Conclusion: DRRNet通过多尺度特征融合和反向细化，有效提升了伪装目标检测的性能。

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [126] [PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement](https://arxiv.org/abs/2505.09196)
*Tong Li,Lizhi Wang,Hansen Feng,Lin Zhu,Hua Huang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种称为参数动态进化（PDE）的方法，通过模拟基因突变和重组来解决低光图像增强中静态参数导致的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现，某些参数重置为随机值反而能提升图像增强性能，这种现象被称为基因效应。静态参数限制了模型性能，因此需要动态调整参数以适应不同图像。

Method: 提出参数动态进化（PDE），使用参数正交生成技术模拟基因重组和突变，动态调整参数。

Result: 实验验证了PDE的有效性，能够显著提升低光图像增强的性能。

Conclusion: PDE通过动态参数调整解决了基因效应问题，为图像增强任务提供了新思路。

Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational
photography, aiming to improve illumination, reduce noise, and enhance image
quality. While recent advancements focus on designing increasingly complex
neural network models, we observe a peculiar phenomenon: resetting certain
parameters to random values unexpectedly improves enhancement performance for
some images. Drawing inspiration from biological genes, we term this phenomenon
the gene effect. The gene effect limits enhancement performance, as even random
parameters can sometimes outperform learned ones, preventing models from fully
utilizing their capacity. In this paper, we investigate the reason and propose
a solution. Based on our observations, we attribute the gene effect to static
parameters, analogous to how fixed genetic configurations become maladaptive
when environments change. Inspired by biological evolution, where adaptation to
new environments relies on gene mutation and recombination, we propose
parameter dynamic evolution (PDE) to adapt to different images and mitigate the
gene effect. PDE employs a parameter orthogonal generation technique and the
corresponding generated parameters to simulate gene recombination and gene
mutation, separately. Experiments validate the effectiveness of our techniques.
The code will be released to the public.

</details>


### [127] [Test-Time Augmentation for Pose-invariant Face Recognition](https://arxiv.org/abs/2505.09256)
*Jaemin Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

Relevance: 30.0

TL;DR: Pose-TTA是一种无需额外训练的方法，通过在推理阶段对齐人脸姿态提升人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要重新训练和测试的问题，提出一种无需额外训练的推理阶段姿态对齐方法。

Method: 使用肖像动画器将源图像身份转移到驱动图像的姿态中，生成匹配的侧脸图像，并提出加权特征聚合策略以减少合成数据的失真和偏差。

Result: 在多种数据集和预训练模型上，Pose-TTA显著提升了推理性能。

Conclusion: Pose-TTA无需重新训练即可集成到现有流程中，是一种高效的人脸识别增强方法。

Abstract: The goal of this paper is to enhance face recognition performance by
augmenting head poses during the testing phase. Existing methods often rely on
training on frontalised images or learning pose-invariant representations, yet
both approaches typically require re-training and testing for each dataset,
involving a substantial amount of effort. In contrast, this study proposes
Pose-TTA, a novel approach that aligns faces at inference time without
additional training. To achieve this, we employ a portrait animator that
transfers the source image identity into the pose of a driving image. Instead
of frontalising a side-profile face -- which can introduce distortion --
Pose-TTA generates matching side-profile images for comparison, thereby
reducing identity information loss. Furthermore, we propose a weighted feature
aggregation strategy to address any distortions or biases arising from the
synthetic data, thus enhancing the reliability of the augmented images.
Extensive experiments on diverse datasets and with various pre-trained face
recognition models demonstrate that Pose-TTA consistently improves inference
performance. Moreover, our method is straightforward to integrate into existing
face recognition pipelines, as it requires no retraining or fine-tuning of the
underlying recognition models.

</details>


### [128] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/abs/2505.09324)
*Lakshya Gupta,Imran N. Junejo*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种基于2D高斯泼溅的区域兴趣（ROI）神经视频压缩模型，显著提升了编码速度，适用于实时视频应用。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器依赖手工特征，而神经视频编解码器（NVC）通过端到端学习提供更高压缩效率，但其高计算需求限制了实时应用。本文旨在解决这一问题。

Method: 采用2D高斯泼溅技术，结合内容感知初始化策略和帧间冗余减少机制，显著提升编码速度。

Result: 编码时间比之前基于高斯泼溅的图像编解码器快88%，首次实现高斯泼溅在神经视频编解码中的应用。

Conclusion: 该方法为神经视频编解码提供了高效的实时解决方案，具有广泛的应用潜力。

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [129] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/abs/2505.09368)
*Jenny Schmalfuss,Victor Oei,Lukas Mehl,Madlen Bartsch,Shashank Agnihotri,Margret Keuper,Andrés Bruhn*

Main category: cs.CV

Relevance: 30.0

TL;DR: RobustSpring是一个专注于评估光学流、场景流和立体视觉模型对图像损坏（如噪声、雨等）鲁棒性的新数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注模型准确性，而忽略了对真实世界扰动的鲁棒性。RobustSpring旨在填补这一空白。

Method: 通过在高分辨率Spring数据集上应用20种不同的图像损坏（如噪声、模糊、天气失真等），生成20,000张损坏图像，并设计新的鲁棒性度量标准。

Result: 实验表明，准确性高的模型不一定鲁棒，且鲁棒性因损坏类型而异。

Conclusion: RobustSpring将鲁棒性作为首要目标，旨在推动兼具准确性和鲁棒性的模型发展。

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [130] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/abs/2505.09380)
*Qinghui Liu,Jon Nesvold,Hanna Raaum,Elakkyen Murugesu,Martin Røvang,Bradley J Maclntosh,Atle Bjørnerud,Karoline Skogen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文介绍了一个名为NeoMedSys的放射学软件平台，用于高效部署和优化AI模型，并在真实临床环境中评估其效果，重点关注颅内出血检测模型VIOLA-AI的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决AI工具在放射学临床部署中的挑战，通过NeoMedSys平台实现AI模型的快速迭代和优化。

Method: NeoMedSys整合了部署、测试和优化AI模型的工具，结合医疗图像查看器和注释系统，在挪威最大的急诊科进行临床案例研究。

Result: VIOLA-AI模型的诊断准确性显著提升，灵敏度从79.2%提高到90.3%，特异性从80.7%提高到89.3%，AUC从0.873提升到0.949。

Conclusion: NeoMedSys平台通过实时反馈和迭代优化显著提升了AI模型的性能，证明了其在临床环境中的价值。

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [131] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/abs/2505.09385)
*Xiaoyang Yu,Xiaoming Wu,Xin Wang,Dongrun Li,Ming Yang,Peng Cheng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为FedSaaS的联邦语义分割框架，通过类原型和对抗机制解决类一致性问题，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理异构问题时忽略了语义空间中的细粒度类别关系，导致类表示模糊。

Method: 引入类原型作为本地和全局类表示的准则，利用对抗机制和多级对比损失确保一致性。

Result: 在多个驾驶场景分割数据集上表现优于现有方法，显著提高了平均分割精度。

Conclusion: FedSaaS有效解决了类一致性问题，提升了联邦语义分割的性能。

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [132] [FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling](https://arxiv.org/abs/2505.09406)
*Yue Wen,Liang Song,Yijia Liu,Siting Zhu,Yanzi Miao,Lijun Han,Hesheng Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: FreeDriveRF提出了一种仅使用连续RGB图像重建动态驾驶场景的方法，无需姿态输入，通过语义监督和光流约束优化动态建模。


<details>
  <summary>Details</summary>
Motivation: 动态场景重建对自动驾驶至关重要，但现有方法依赖精确姿态输入和多传感器数据，增加了系统复杂性。

Method: 通过语义监督解耦动态和静态部分，引入光流约束的动态对象渲染一致性损失，并结合动态流优化姿态估计。

Result: 在KITTI和Waymo数据集上表现出色，优于现有方法。

Conclusion: FreeDriveRF为动态驾驶场景重建提供了一种高效且无需姿态输入的解决方案。

Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

</details>


### [133] [Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians](https://arxiv.org/abs/2505.09413)
*Ma Changfeng,Bi Ran,Guo Jie,Wang Chongjun,Guo Yanwen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种新的点云渲染方法，通过预测2D高斯分布实现，适用于稀疏点云，无需额外细化。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖类别先验、密集点云或额外细化，限制了泛化能力。

Method: 使用两个相同模块的架构，通过点云信息初始化高斯分布，并用分裂解码器细化。

Result: 在多个数据集上实现SOTA性能，泛化能力强。

Conclusion: 该方法在稀疏点云上表现优越，无需额外细化。

Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

</details>


### [134] [FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models](https://arxiv.org/abs/2505.09415)
*Hongyang Wang,Yichen Shi,Zhuofu Tao,Yuhao Gao,Liepiao Zhang,Xun Lin,Jun Feng,Xiaochen Yuan,Zitong Yu,Xiaochun Cao*

Main category: cs.CV

Relevance: 30.0

TL;DR: FaceShield是一种多模态大型语言模型（MLLM），专为面部反欺骗（FAS）任务设计，结合了预训练和微调数据集，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FAS方法缺乏可解释性，且缺乏专门设计的MLLM和数据集，因此提出FaceShield填补这一空白。

Method: 采用欺骗感知视觉感知（SAVP）和提示引导的视觉令牌掩码（PVTM）策略，结合原始图像和先验知识辅助信息。

Result: 在三个基准数据集上，FaceShield在粗粒度分类、细粒度分类、推理和攻击定位任务中显著优于现有方法。

Conclusion: FaceShield为FAS任务提供了通用且全面的解决方案，具有高可解释性和性能。

Abstract: Face anti-spoofing (FAS) is crucial for protecting facial recognition systems
from presentation attacks. Previous methods approached this task as a
classification problem, lacking interpretability and reasoning behind the
predicted results. Recently, multimodal large language models (MLLMs) have
shown strong capabilities in perception, reasoning, and decision-making in
visual tasks. However, there is currently no universal and comprehensive MLLM
and dataset specifically designed for FAS task. To address this gap, we propose
FaceShield, a MLLM for FAS, along with the corresponding pre-training and
supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.
FaceShield is capable of determining the authenticity of faces, identifying
types of spoofing attacks, providing reasoning for its judgments, and detecting
attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that
incorporates both the original image and auxiliary information based on prior
knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to
random mask vision tokens, thereby improving the model's generalization
ability. We conducted extensive experiments on three benchmark datasets,
demonstrating that FaceShield significantly outperforms previous deep learning
models and general MLLMs on four FAS tasks, i.e., coarse-grained
classification, fine-grained classification, reasoning, and attack
localization. Our instruction datasets, protocols, and codes will be released
soon.

</details>


### [135] [MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2505.09422)
*Xiangyuan Peng,Yu Wang,Miao Tang,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

Relevance: 30.0

TL;DR: MoRAL提出了一种基于运动感知的多帧4D雷达与LiDAR融合框架，用于鲁棒的3D物体检测，通过补偿雷达点云帧间错位并利用动态信息，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态融合方法中忽视雷达点云帧间错位及未充分利用4D雷达动态信息的问题。

Method: 设计了运动感知雷达编码器（MRE）补偿错位，并通过运动注意力门控融合（MAGF）模块整合雷达动态特征引导LiDAR特征聚焦动态物体。

Result: 在VoD数据集上表现优异，整体区域mAP达73.30%，驾驶走廊达88.68%，行人和骑行者的检测性能也显著提升。

Conclusion: MoRAL通过运动感知和动态特征融合，显著提升了3D物体检测的鲁棒性和准确性。

Abstract: Reliable autonomous driving systems require accurate detection of traffic
participants. To this end, multi-modal fusion has emerged as an effective
strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame
radar point clouds have demonstrated the effectiveness in bridging the point
density gap. However, they often neglect radar point clouds' inter-frame
misalignment caused by object movement during accumulation and do not fully
exploit the object dynamic information from 4D radar. In this paper, we propose
MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for
robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is
designed to compensate for inter-frame radar misalignment from moving objects.
Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion
features to guide LiDAR features to focus on dynamic foreground objects.
Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL
outperforms existing methods, achieving the highest mAP of 73.30% in the entire
area and 88.68% in the driving corridor. Notably, our method also achieves the
best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in
the driving corridor.

</details>


### [136] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/abs/2505.09433)
*Jiahao Zhu,Kang You,Dandan Ding,Zhan Ma*

Main category: cs.CV

Relevance: 30.0

TL;DR: SerLiC是一个基于序列化的神经压缩框架，用于高效压缩LiDAR点云中的反射率数据，利用Mamba模型实现快速处理和高压缩率。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云中的反射率属性对下游任务至关重要，但在神经压缩方法中尚未充分探索。

Method: 通过扫描顺序序列化将3D点云转化为1D序列，结合Mamba模型的双并行化方案进行高效建模。

Result: SerLiC实现了2倍以上的体积压缩，比现有方法减少22%的压缩比特，且仅需2%的参数。轻量版达到>10 fps。

Conclusion: SerLiC在LiDAR反射率压缩中表现出色，适合实际应用。

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [137] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/abs/2505.09450)
*Yuelin Zhang,Qingpeng Ding,Long Lei,Yongxuan Feng,Raymond Shing-Yan Tang,Shing Shin Cheng*

Main category: cs.CV

Relevance: 30.0

TL;DR: MrTrack是一种基于Mamba的针头追踪器，用于超声引导下的细针穿刺活检，通过全局上下文提取和时序提示检索提升追踪精度和效率。


<details>
  <summary>Details</summary>
Motivation: 超声引导下的细针穿刺活检需要应对快速往复运动的针头追踪问题，现有方法缺乏有效的解决方案。

Method: 提出Mamba-based register机制，包括register extractor和retriever，结合自监督的diversify loss防止特征崩溃。

Result: 在自动和手动穿刺数据集上，MrTrack在精度、鲁棒性和推理效率上优于现有方法。

Conclusion: MrTrack为解决快速运动针头追踪问题提供了高效且鲁棒的解决方案。

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.

</details>


### [138] [Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2505.09484)
*Yingjie Ma,Xun Lin,Zitong Yu,Xin Liu,Xiaochen Yuan,Weicheng Xie,Linlin Shen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种多模态去噪和对齐框架（MMDA），通过CLIP的零样本泛化能力提升跨模态对齐的泛化性能，并在多个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态人脸防伪（FAS）方法因模态特定偏差和域偏移而泛化能力不足，MMDA旨在解决这些问题。

Method: MMDA框架包含MD2A模块（减少域和模态噪声）、RS2对齐策略（利用CLIP对齐多域数据）和U-DSA模块（增强表示适应性）。

Result: 在四个基准数据集上，MMDA在跨域泛化和多模态检测精度上优于现有方法。

Conclusion: MMDA通过去噪和对齐机制显著提升了多模态FAS的泛化能力和表示能力。

Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.

</details>


### [139] [Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems](https://arxiv.org/abs/2505.09528)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文提出了一种结合共形预测和近似后验采样的方法，用于在不知道真实图像的情况下，构建全参考图像质量（FRIQ）指标的置信界限。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用（如医学成像）中，评估恢复图像与真实图像的接近程度至关重要，但直接计算FRIQ指标因缺乏真实图像而困难。

Method: 结合共形预测和近似后验采样，构建FRIQ指标的置信界限。

Result: 在图像去噪和加速磁共振成像（MRI）问题上验证了方法的有效性。

Conclusion: 该方法能够为FRIQ指标提供统计保证的界限，适用于安全关键的应用场景。

Abstract: In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.

</details>


### [140] [Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes](https://arxiv.org/abs/2505.09562)
*Nicola Marinello,Simen Cassiman,Jonas Heylen,Marc Proesmans,Luc Van Gool*

Main category: cs.CV

Relevance: 30.0

TL;DR: 本文提出了一种用于3D全景场景补全的新框架，扩展了现有的3D语义场景补全模型，并引入了对象模块和全景模块。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要完整的环境地图以规划和行动，而3D全景场景补全任务目前尚未充分探索。

Method: 通过对象模块和全景模块扩展现有3D语义场景补全模型，利用占用基准中的注释学习对象形状。

Result: 提出的框架能够有效补全3D场景中的遮挡区域并区分同类对象实例。

Conclusion: 该方法为自动驾驶中的路径规划和决策提供了更全面的环境表示。

Abstract: Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .

</details>


### [141] [Don't Forget your Inverse DDIM for Image Editing](https://arxiv.org/abs/2505.09571)
*Guillermo Gomez-Trenado,Pablo Mesejo,Oscar Cordón,Stéphane Lathuilière*

Main category: cs.CV

Relevance: 30.0

TL;DR: SAGE是一种基于预训练扩散模型的新型图像编辑技术，通过自注意力机制优化DDIM算法，实现高效图像编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像编辑方法计算成本高或重建效果差的问题。

Method: 利用扩散U-Net的自注意力层设计新的引导机制，基于反向DDIM过程的注意力图计算重建目标。

Result: 在定量和定性评估中表现优异，用户研究中47名用户均偏好SAGE。

Conclusion: SAGE有效解决了图像编辑中的关键挑战，性能优于其他方法。

Abstract: The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.

</details>


### [142] [LightLab: Controlling Light Sources in Images with Diffusion Models](https://arxiv.org/abs/2505.09608)
*Nadav Magar,Amir Hertz,Eric Tabellion,Yael Pritch,Alex Rav-Acha,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于扩散模型的细粒度光照控制方法，通过微调和合成数据实现精确的光照编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖多视图输入，要么无法提供明确的光照控制，因此需要一种更灵活且可控的方法。

Method: 利用扩散模型在少量真实照片和合成数据上进行微调，结合光的线性特性合成光照变化图像对。

Result: 模型能精确控制光照强度和颜色，用户偏好测试显示其优于现有方法。

Conclusion: 该方法在光照编辑任务中表现出色，提供了更高的可控性和效果。

Abstract: We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.

</details>


### [143] [DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images](https://arxiv.org/abs/2505.09334)
*Sadman Sakib Alif,Nasim Anzum Promise,Fiaz Al Abid,Aniqua Nusrat Zereen*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于知识蒸馏的方法，结合可解释AI技术，用于肺癌检测，旨在解决计算资源需求和模型透明度问题。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测和准确诊断对提高生存率至关重要，但现有深度学习模型计算成本高且缺乏透明度，限制了其在资源受限环境中的应用。

Method: 使用八种CNN（如ResNet50、EfficientNetB0等）作为教师模型，训练轻量级学生模型DCSNet，并结合可解释AI技术。

Result: 提出的方法在资源受限环境下实现了高诊断性能，同时增强了模型透明度。

Conclusion: 知识蒸馏和可解释AI的结合为医疗领域AI工具的推广提供了可行方案。

Abstract: Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.

</details>


### [144] [Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net](https://arxiv.org/abs/2505.09521)
*Dongyi He,Shiyang Li,Bin Jiang,He Yan*

Main category: eess.IV

Relevance: 30.0

TL;DR: 提出了一种轻量级的EEG到fMRI生成器Spec2VolCAMU-Net，通过多方向时间-频率卷积注意力编码器和Vision-Mamba U-Net解码器，实现了高效的长程空间建模和状态重建。


<details>
  <summary>Details</summary>
Motivation: 高分辨率fMRI成本高且难以获取，而EEG广泛可用，因此开发一种从EEG生成fMRI的方法可以显著提升神经影像的可及性。

Method: 结合多方向时间-频率卷积注意力编码器和Vision-Mamba U-Net解码器，使用混合SSI-MSE损失进行端到端训练。

Result: 在三个公共基准测试中实现了最先进的保真度，SSIM和PSNR均有显著提升。

Conclusion: Spec2VolCAMU-Net轻量且高效，适用于临床和研究中的实时应用。

Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.

</details>


### [145] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/abs/2505.08808)
*Anqing Jiang,Jinhao Chai,Yu Gao,Yiru Wang,Yuwen Heng,Zhigang Sun,Hao Sun,Zezhong Zhao,Li Sun,Jian Zhou,Lijuan Zhu,Shugong Xu,Hao Zhao*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种优化的稀疏表示方法，用于高效的高清地图构建，通过专用网络架构、稀疏-密集分割辅助任务和物理先验去噪模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏表示方法在在线高清地图构建中表现不佳，缺乏针对性设计，作者旨在通过系统改进使其超越密集方法。

Method: 引入专用网络架构、稀疏-密集分割辅助任务和物理先验去噪模块。

Result: 在nuScenes数据集上达到SOTA性能，SparseMeXt-Large实现68.9% mAP，超过20 fps。

Conclusion: 稀疏方法潜力巨大，挑战了传统依赖密集表示的观念，重新定义了效率与性能的权衡。

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [146] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/abs/2505.08814)
*Wenkai Li,Xiaoqi Li,Yingjie Mao,Yishun Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文研究了深度神经网络（DNNs）的四种覆盖率指标（主要功能、边界、层次和结构覆盖率）与模型深度、配置信息之间的关系，并通过实验验证了这些关系。


<details>
  <summary>Details</summary>
Motivation: 随着DNNs的广泛应用，其安全性测试变得尤为重要，但目前缺乏对不同覆盖率指标之间关系的实证研究。

Method: 选择了LeNet、VGG和ResNet等不同架构的DNNs，以及5到54层不同深度的10个模型，通过实验比较不同深度、配置信息与覆盖率指标的关系。

Result: 实验揭示了模型深度和配置信息与覆盖率指标之间的关系，并探讨了修改后的决策/条件覆盖率与数据集大小的关系。

Conclusion: 论文提出了三个未来研究方向，以进一步推动DNNs安全性测试的发展。

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [147] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/abs/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种结合自监督训练和多列CNN的方法用于人群计数，以及一种基于VGG19和LSTM的时空模型用于异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决人群计数中数据标注成本高的问题，以及异常检测中复杂场景的挑战。

Method: 人群计数采用自监督训练和多列CNN；异常检测结合VGG19和LSTM提取时空特征。

Result: 在ShanghaiTech和UCFQNRF数据集上表现优于现有方法；异常检测模型在Hockey Fight和SCVD数据集上表现优异。

Conclusion: 提出的方法在人群计数和异常检测中均有效，解决了数据标注和复杂场景的挑战。

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [148] [Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety](https://arxiv.org/abs/2505.08882)
*Ali Almakhluk,Uthman Baroudi,Yasser El-Alfy*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种基于深度学习的系统，用于实时检测和分类道路损坏（如坑洞和裂缝），并通过云服务通知相关方，以提高交通安全。


<details>
  <summary>Details</summary>
Motivation: 道路损坏是交通事故的常见原因，现有系统缺乏实时性和全面性，因此需要一种更高效的解决方案。

Method: 系统结合了Raspberry Pi、摄像头模块、深度学习模型和云服务，实时检测、分类道路损坏并发送警告。

Result: 系统能有效检测和分类道路损坏，并通过云服务通知相关方，减少潜在事故。

Conclusion: 该系统通过实时监测和预警，显著提升了道路安全性。

Abstract: This study aims to improve transportation safety, especially traffic safety.
Road damage anomalies such as potholes and cracks have emerged as a significant
and recurring cause for accidents. To tackle this problem and improve road
safety, a comprehensive system has been developed to detect potholes, cracks
(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit
this data to the cloud for appropriate action by authorities. The system also
broadcasts warning signals to nearby vehicles warning them if a severe anomaly
is detected on the road. Moreover, the system can count road anomalies in
real-time. It is emulated through the utilization of Raspberry Pi, a camera
module, deep learning model, laptop, and cloud service. Deploying this
innovative solution aims to proactively enhance road safety by notifying
relevant authorities and drivers about the presence of potholes and cracks to
take actions, thereby mitigating potential accidents arising from this
prevalent road hazard leading to safer road conditions for the whole community.

</details>


### [149] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/abs/2505.08909)
*Deliang Wei,Peng Chen,Haobo Xu,Jiale Yao,Fang Li,Tieyong Zeng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为CoCo的保守性去噪器，用于解决Poisson逆问题中的非扩张性问题，通过结合Hamiltonian正则化和谱正则化，证明了其全局收敛性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统PnP方法在Poisson逆问题中因非扩张性假设而受限，影响了去噪性能。

Method: 提出CoCo去噪器，结合Hamiltonian和谱正则化训练策略，证明其为弱凸函数的近端算子。

Result: 实验表明，该方法在视觉质量和定量指标上优于相关方法。

Conclusion: CoCo去噪器解决了传统方法的局限性，并在实际应用中表现出色。

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [150] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/abs/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种多模态深度学习框架，结合CGM时间序列数据、人口统计/微生物组数据和餐前食物图像，以提高热量估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确估计热量摄入对管理2型糖尿病至关重要，但现有方法（如CGM）因个体和餐食差异而受限。

Method: 采用基于注意力的编码和卷积特征提取处理食物图像，多层感知器处理CGM和微生物组数据，并通过后期融合策略进行联合推理。

Result: 在40多名参与者的数据集上，模型的热量估计误差（RMSRE）为0.2544，比基线模型提升50%以上。

Conclusion: 多模态传感技术有望提升慢性病管理的自动化饮食评估工具。

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [151] [2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition](https://arxiv.org/abs/2505.09073)
*J. Brennan Peace,Shuowen Hu,Benjamin S. Riggan*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种新颖的域自适应框架，通过联合注意力映射和联合熵正则化损失，提升2D面部图像与3D面部数据之间的相关性，从而改善大姿态差异下的面部识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决面部识别中因姿态差异导致的性能下降问题。

Method: 使用共享注意力映射和联合熵正则化损失，增强2D与3D表示之间的相关性。

Result: 在FaceScape和ARL-VTF数据集上表现优于竞争方法，姿态差异大时识别性能显著提升。

Conclusion: 提出的框架有效提升了姿态不变性，适用于面部识别任务。

Abstract: Despite recent advances in facial recognition, there remains a fundamental
issue concerning degradations in performance due to substantial perspective
(pose) differences between enrollment and query (probe) imagery. Therefore, we
propose a novel domain adaptive framework to facilitate improved performances
across large discrepancies in pose by enabling image-based (2D) representations
to infer properties of inherently pose invariant point cloud (3D)
representations. Specifically, our proposed framework achieves better pose
invariance by using (1) a shared (joint) attention mapping to emphasize common
patterns that are most correlated between 2D facial images and 3D facial data
and (2) a joint entropy regularizing loss to promote better
consistency$\unicode{x2014}$enhancing correlations among the intersecting 2D
and 3D representations$\unicode{x2014}$by leveraging both attention maps. This
framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms
competitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)
TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and
1.57$\unicode{x0025}$, respectively.

</details>


### [152] [OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions](https://arxiv.org/abs/2505.09092)
*Yuhang Wang,Abdulaziz Alhuraish,Shengming Yuan,Hao Zhou*

Main category: cs.CV

Relevance: 30.0

TL;DR: OpenLKA是一个开放的大规模数据集，用于评估和改进车道保持辅助系统（LKA），包含400小时的驾驶数据和多模态信息。


<details>
  <summary>Details</summary>
Motivation: 由于专有系统和数据访问限制，LKA在现实世界中的性能研究不足，OpenLKA旨在填补这一空白。

Method: 数据集通过道路测试和社区贡献收集，包含CAN总线数据、视频、Openpilot输出和场景注释。

Result: OpenLKA提供了一个全面的平台，用于评估LKA性能、识别安全关键场景和评估道路基础设施。

Conclusion: OpenLKA为LKA研究和自动驾驶基础设施评估提供了重要资源。

Abstract: Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.

</details>


### [153] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/abs/2505.09129)
*Wei Meng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于颜色特征的轻量级异常检测框架，用于资源受限和数据敏感环境下的监控视频异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在高风险安全任务中面临挑战，尤其是在无标签和数据不可利用的视频环境中。

Method: 融合无监督KMeans聚类和RGB通道直方图建模，检测关键帧中的结构异常和颜色突变信号。

Result: 成功识别了与高能光源、目标存在和反射干扰相关的高度异常帧，展示了方法的实战价值。

Conclusion: 颜色特征在战场信号传递中具有重要性，未来将结合图神经网络和时间建模进一步扩展能力。

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [154] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/abs/2505.09168)
*Jianlin Sun,Xiaolin Fang,Juwei Guan,Dongdong Gui,Teqi Wang,Tongxin Zhu*

Main category: cs.CV

Relevance: 30.0

TL;DR: DRRNet提出了一种四阶段架构，通过全局与局部特征融合解决伪装物体检测中的边缘细节丢失和背景干扰问题。


<details>
  <summary>Details</summary>
Motivation: 伪装物体检测中目标与背景的相似性导致现有方法在全局语义和局部特征之间难以平衡，导致细节丢失或背景干扰。

Method: DRRNet采用四阶段流程，包括全局上下文特征提取、局部细节提取、双表征融合和反向细化模块。

Result: 实验表明DRRNet在基准数据集上显著优于现有方法。

Conclusion: DRRNet通过多尺度特征融合和反向细化有效提升了伪装物体检测的性能。

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [155] [PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement](https://arxiv.org/abs/2505.09196)
*Tong Li,Lizhi Wang,Hansen Feng,Lin Zhu,Hua Huang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种称为参数动态演化（PDE）的方法，用于解决低光照图像增强（LLIE）中静态参数导致的“基因效应”问题，通过模拟基因重组和突变来动态调整参数。


<details>
  <summary>Details</summary>
Motivation: 研究发现，在低光照图像增强任务中，某些参数重置为随机值反而能提升性能，这种现象被称为“基因效应”，限制了模型性能。受生物基因演化的启发，作者旨在解决这一问题。

Method: 提出参数动态演化（PDE）方法，通过参数正交生成技术模拟基因重组和突变，动态调整参数以适应不同图像。

Result: 实验验证了PDE方法的有效性，能够显著提升低光照图像增强的性能。

Conclusion: PDE方法通过动态调整参数解决了“基因效应”问题，为低光照图像增强任务提供了新思路。

Abstract: Low-light image enhancement (LLIE) is a fundamental task in computational
photography, aiming to improve illumination, reduce noise, and enhance image
quality. While recent advancements focus on designing increasingly complex
neural network models, we observe a peculiar phenomenon: resetting certain
parameters to random values unexpectedly improves enhancement performance for
some images. Drawing inspiration from biological genes, we term this phenomenon
the gene effect. The gene effect limits enhancement performance, as even random
parameters can sometimes outperform learned ones, preventing models from fully
utilizing their capacity. In this paper, we investigate the reason and propose
a solution. Based on our observations, we attribute the gene effect to static
parameters, analogous to how fixed genetic configurations become maladaptive
when environments change. Inspired by biological evolution, where adaptation to
new environments relies on gene mutation and recombination, we propose
parameter dynamic evolution (PDE) to adapt to different images and mitigate the
gene effect. PDE employs a parameter orthogonal generation technique and the
corresponding generated parameters to simulate gene recombination and gene
mutation, separately. Experiments validate the effectiveness of our techniques.
The code will be released to the public.

</details>


### [156] [Test-Time Augmentation for Pose-invariant Face Recognition](https://arxiv.org/abs/2505.09256)
*Jaemin Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种无需额外训练的测试时姿态增强方法Pose-TTA，通过生成匹配的侧脸图像提升人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要针对每个数据集重新训练或测试，耗时耗力。Pose-TTA旨在在推理阶段对齐人脸姿态，避免额外训练。

Method: 使用肖像动画器将源图像身份转移到驱动图像的姿态中，生成匹配的侧脸图像，并采用加权特征聚合策略减少合成数据的偏差。

Result: 在多种数据集和预训练模型上，Pose-TTA显著提升了推理性能。

Conclusion: Pose-TTA无需重新训练或微调，易于集成到现有流程中，提升了人脸识别的可靠性。

Abstract: The goal of this paper is to enhance face recognition performance by
augmenting head poses during the testing phase. Existing methods often rely on
training on frontalised images or learning pose-invariant representations, yet
both approaches typically require re-training and testing for each dataset,
involving a substantial amount of effort. In contrast, this study proposes
Pose-TTA, a novel approach that aligns faces at inference time without
additional training. To achieve this, we employ a portrait animator that
transfers the source image identity into the pose of a driving image. Instead
of frontalising a side-profile face -- which can introduce distortion --
Pose-TTA generates matching side-profile images for comparison, thereby
reducing identity information loss. Furthermore, we propose a weighted feature
aggregation strategy to address any distortions or biases arising from the
synthetic data, thus enhancing the reliability of the augmented images.
Extensive experiments on diverse datasets and with various pre-trained face
recognition models demonstrate that Pose-TTA consistently improves inference
performance. Moreover, our method is straightforward to integrate into existing
face recognition pipelines, as it requires no retraining or fine-tuning of the
underlying recognition models.

</details>


### [157] [Recent Advances in Medical Imaging Segmentation: A Survey](https://arxiv.org/abs/2505.09274)
*Fares Bougourzi,Abdenour Hadid*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文综述了医学图像分割领域的最新进展，重点探讨了生成式AI、少样本学习、基础模型和通用模型等方法，并总结了理论、技术和应用。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临数据可访问性、标注复杂性、结构变异性等挑战，现有模型在泛化和领域适应方面仍有不足。

Method: 综述了生成式AI、少样本学习、基础模型和通用模型等方法。

Result: 总结了这些方法在医学图像分割中的应用及其潜力。

Conclusion: 讨论了当前局限性和未来研究方向，旨在提升分割模型的实用性和可访问性。

Abstract: Medical imaging is a cornerstone of modern healthcare, driving advancements
in diagnosis, treatment planning, and patient care. Among its various tasks,
segmentation remains one of the most challenging problem due to factors such as
data accessibility, annotation complexity, structural variability, variation in
medical imaging modalities, and privacy constraints. Despite recent progress,
achieving robust generalization and domain adaptation remains a significant
hurdle, particularly given the resource-intensive nature of some proposed
models and their reliance on domain expertise. This survey explores
cutting-edge advancements in medical image segmentation, focusing on
methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and
Universal Models. These approaches offer promising solutions to longstanding
challenges. We provide a comprehensive overview of the theoretical foundations,
state-of-the-art techniques, and recent applications of these methods. Finally,
we discuss inherent limitations, unresolved issues, and future research
directions aimed at enhancing the practicality and accessibility of
segmentation models in medical imaging. We are maintaining a
\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub
Repository} to continue tracking and updating innovations in this field.

</details>


### [158] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/abs/2505.09324)
*Lakshya Gupta,Imran N. Junejo*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于2D高斯泼溅的区域兴趣（ROI）神经视频压缩模型，显著提升了编码速度，适用于实时视频应用。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器依赖手工特征，而神经视频编解码器（NVC）通过学习内容感知压缩策略提供更高效率，但计算需求限制了实时应用。本文旨在解决这一问题。

Method: 利用2D高斯泼溅技术，结合内容感知初始化策略和帧间冗余减少机制，设计了一种高效的视频编解码方案。

Result: 编码时间比之前基于高斯泼溅的图像编解码器快88%，首次实现了高斯泼溅在神经视频编解码中的应用。

Conclusion: 该方法为神经视频编解码提供了实时可行的解决方案，具有广泛的应用潜力。

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [159] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/abs/2505.09380)
*Qinghui Liu,Jon Nesvold,Hanna Raaum,Elakkyen Murugesu,Martin Røvang,Bradley J Maclntosh,Atle Bjørnerud,Karoline Skogen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文介绍了一个名为NeoMedSys的放射学软件平台，用于高效部署和优化AI模型，并通过实际临床测试验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 解决AI工具在放射学临床部署中的挑战，提升AI模型在颅内出血检测中的性能。

Method: NeoMedSys整合了AI模型部署、测试和优化工具，结合医疗图像查看器和注释系统，通过实际病例评估模型性能。

Result: 模型经过迭代优化后，敏感性和特异性显著提升，AUC达到0.949。

Conclusion: NeoMedSys通过实时反馈和模型优化显著提升了AI模型的诊断性能。

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [160] [FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling](https://arxiv.org/abs/2505.09406)
*Yue Wen,Liang Song,Yijia Liu,Siting Zhu,Yanzi Miao,Lijun Han,Hesheng Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: FreeDriveRF提出了一种仅需RGB图像输入的动态场景重建方法，通过语义监督解耦动态与静态部分，并利用光流约束动态建模，提升了自动驾驶场景重建的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要精确感知动态场景，现有方法依赖多传感器和精确位姿输入，增加了系统复杂性。FreeDriveRF旨在仅用RGB图像实现高质量动态场景重建。

Method: 通过语义监督解耦动态与静态部分，引入光流约束的动态对象渲染一致性损失，并利用动态流优化位姿估计，提升重建稳定性。

Result: 在KITTI和Waymo数据集上验证了方法的优越性能，尤其在动态场景建模方面表现突出。

Conclusion: FreeDriveRF为自动驾驶提供了一种高效且不依赖多传感器的动态场景重建解决方案。

Abstract: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

</details>


### [161] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/abs/2505.09433)
*Jiahao Zhu,Kang You,Dandan Ding,Zhan Ma*

Main category: cs.CV

Relevance: 30.0

TL;DR: SerLiC是一种基于序列化的神经压缩框架，用于高效压缩LiDAR点云的反射率数据，利用Mamba模型实现快速处理和高压缩率。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云中的反射率属性在下游任务中至关重要，但在神经压缩方法中尚未充分探索。

Method: 通过扫描顺序序列化将3D LiDAR点云转换为1D序列，结合Mamba模型的双并行化方案进行高效序列建模。

Result: SerLiC实现了超过2倍的体积压缩，性能优于现有方法，且轻量级版本在实时应用中表现优异。

Conclusion: SerLiC为LiDAR反射率数据的高效压缩提供了创新解决方案，适用于实际应用。

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [162] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/abs/2505.09450)
*Yuelin Zhang,Qingpeng Ding,Long Lei,Yongxuan Feng,Raymond Shing-Yan Tang,Shing Shin Cheng*

Main category: cs.CV

Relevance: 30.0

TL;DR: MrTrack是一种基于Mamba的超声引导细针抽吸活检针追踪器，通过Mamba机制提取全局上下文并存储时间线索，解决了快速往复运动中的追踪问题。


<details>
  <summary>Details</summary>
Motivation: 超声引导细针抽吸活检中，快速往复运动导致现有追踪器失效，需要一种新的追踪方法。

Method: 提出Mamba-based register机制，包括提取器和检索器，结合自监督损失函数防止特征崩溃。

Result: 在自动和手动抽吸数据集上，MrTrack在准确性、鲁棒性和推理效率上优于现有方法。

Conclusion: MrTrack为解决快速运动中的追踪问题提供了高效且可靠的解决方案。

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.

</details>


### [163] [Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2505.09484)
*Yingjie Ma,Xun Lin,Zitong Yu,Xin Liu,Xiaochen Yuan,Weicheng Xie,Linlin Shen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为MMDA的多模态去噪和对齐框架，通过CLIP的零样本泛化能力，显著提升了跨模态对齐的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态FAS方法因模态特定偏差和域偏移而泛化能力不足，需改进。

Method: 采用MD2A模块减少域和模态噪声，RS2策略对齐多域数据，U-DSA模块增强表示适应性。

Result: 在四个基准数据集上，MMDA框架在跨域泛化和多模态检测精度上优于现有方法。

Conclusion: MMDA框架有效提升了多模态FAS的泛化能力和检测精度。

Abstract: Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.

</details>


### [164] [Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems](https://arxiv.org/abs/2505.09528)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种结合共形预测和近似后验采样的方法，用于在不知道真实图像的情况下，构建全参考图像质量（FRIQ）指标的置信边界，确保其以用户指定的误差概率成立。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用（如医学成像）中，评估恢复图像与真实图像的质量差异至关重要，但由于真实图像未知，直接计算FRIQ指标具有挑战性。

Method: 结合共形预测和近似后验采样，构建FRIQ指标的置信边界。

Result: 在图像去噪和加速磁共振成像（MRI）问题上验证了方法的有效性。

Conclusion: 该方法为FRIQ评估提供了理论保证，适用于安全关键场景。

Abstract: In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.

</details>


### [165] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/abs/2505.09529)
*Mohamed Moustafa,Joseph Lemley,Peter Corcoran*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该研究利用事件相机和CNN模型，从面部事件流中无接触重建心率信号，展示了事件相机在远程心率监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索事件相机在生理信号监测中的应用，尤其是心率信号的无接触提取，以验证其高动态范围和高时间分辨率的优势。

Method: 使用监督学习的CNN模型，从事件流中提取心率信号，并与传统相机帧的基线模型进行对比。

Result: 事件相机模型在60和120 FPS下表现优于传统相机（RMSE分别为2.54和2.13 bpm），验证了事件相机的高效性。

Conclusion: 事件相机在心率监测中具有潜力，尤其是在高帧率下表现更优。

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


### [166] [LightLab: Controlling Light Sources in Images with Diffusion Models](https://arxiv.org/abs/2505.09608)
*Nadav Magar,Amir Hertz,Eric Tabellion,Yael Pritch,Alex Rav-Acha,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于扩散模型的细粒度光照控制方法，通过微调扩散模型实现精确的光照编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视图输入或缺乏显式光照控制，难以实现精确的光照编辑。

Method: 利用真实和合成图像对微调扩散模型，结合光的线性特性生成可控光照变化。

Result: 方法在用户偏好测试中优于现有方法，实现了精确的光照强度和颜色控制。

Conclusion: 该方法为图像光照编辑提供了高效且可控的解决方案。

Abstract: We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.

</details>


### [167] [Multi-step manipulation task and motion planning guided by video demonstration](https://arxiv.org/abs/2505.08949)
*Kateryna Zorina,David Kovar,Mederic Fourmy,Florent Lamiraux,Nicolas Mansard,Justin Carpentier,Josef Sivic,Vladimir Petrik*

Main category: cs.RO

Relevance: 30.0

TL;DR: 该研究提出了一种基于教学视频的多步任务与运动规划方法，扩展了RRT规划器，结合视频中的接触状态和3D物体位姿，解决了具有顺序依赖性的任务。


<details>
  <summary>Details</summary>
Motivation: 利用教学视频解决机器人复杂多步任务与运动规划问题，尤其是具有顺序依赖性的任务。

Method: 扩展RRT规划器，结合视频提取的接触状态和3D物体位姿，设计新基准任务验证方法。

Result: 在多个机器人平台上验证了方法的有效性，并开发了轨迹优化方法以实现真实机器人应用。

Conclusion: 视频引导的规划方法在多步任务中表现优异，并能推广到视频未直接展示的场景。

Abstract: This work aims to leverage instructional video to solve complex multi-step
task-and-motion planning tasks in robotics. Towards this goal, we propose an
extension of the well-established Rapidly-Exploring Random Tree (RRT) planner,
which simultaneously grows multiple trees around grasp and release states
extracted from the guiding video. Our key novelty lies in combining contact
states and 3D object poses extracted from the guiding video with a traditional
planning algorithm that allows us to solve tasks with sequential dependencies,
for example, if an object needs to be placed at a specific location to be
grasped later. We also investigate the generalization capabilities of our
approach to go beyond the scene depicted in the instructional video. To
demonstrate the benefits of the proposed video-guided planning approach, we
design a new benchmark with three challenging tasks: (I) 3D re-arrangement of
multiple objects between a table and a shelf, (ii) multi-step transfer of an
object through a tunnel, and (iii) transferring objects using a tray similar to
a waiter transfers dishes. We demonstrate the effectiveness of our planning
algorithm on several robots, including the Franka Emika Panda and the KUKA KMR
iiwa. For a seamless transfer of the obtained plans to the real robot, we
develop a trajectory refinement approach formulated as an optimal control
problem (OCP).

</details>


### [168] [Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ](https://arxiv.org/abs/2505.08990)
*Andrew C. Freeman*

Main category: cs.MM

Relevance: 30.0

TL;DR: 该论文提出了一种基于Media Over QUIC Transport协议的实时内容审核方法，用于一对一多视频直播流，仅删除违规内容片段，同时支持客户端分布式分析。


<details>
  <summary>Details</summary>
Motivation: 随着直播的普及，实时内容审核需求增加，但现有方法在延迟和灵活性上存在不足。

Method: 扩展Media Over QUIC Transport协议，支持实时内容审核，并允许客户端分布式分析。

Result: 系统在光敏性观众场景下测试，仅增加一个GOP（图像组）的延迟。

Conclusion: 该方法有效平衡了实时内容审核的延迟和灵活性。

Abstract: Live video streaming is increasingly popular on social media platforms. With
the growth of live streaming comes an increased need for robust content
moderation to remove dangerous, illegal, or otherwise objectionable content.
Whereas video on demand distribution enables offline content analysis, live
streaming imposes restrictions on latency for both analysis and distribution.
In this paper, we present extensions to the in-progress Media Over QUIC
Transport protocol that enable real-time content moderation in one-to-many
video live streams. Importantly, our solution removes only the video segments
that contain objectionable content, allowing playback resumption as soon as the
stream conforms to content policies again. Content analysis tasks may be
transparently distributed to arbitrary client devices. We implement and
evaluate our system in the context of light strobe removal for photosensitive
viewers, finding that streaming clients experience an increased latency of only
one group-of-pictures duration.

</details>


### [169] [Neural BRDF Importance Sampling by Reparameterization](https://arxiv.org/abs/2505.08998)
*Liwen Wu,Sai Bi,Zexiang Xu,Hao Tan,Kai Zhang,Fujun Luan,Haolin Lu,Ravi Ramamoorthi*

Main category: cs.GR

Relevance: 30.0

TL;DR: 本文提出了一种基于重参数化的神经BRDF重要性采样方法，提高了渲染效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 神经BRDF在物理渲染中提升真实感，但其重要性采样仍具挑战性。

Method: 通过重参数化将分布学习任务转化为BRDF积分替换问题，避免了可逆网络和多步推理的限制。

Result: 该方法在神经BRDF渲染中实现了最佳方差减少，同时保持高推理速度。

Conclusion: 重参数化方法为神经BRDF重要性采样提供了更高效和灵活的解决方案。

Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged
as popular material representations for enhancing realism in physically-based
rendering. Yet their importance sampling remains a significant challenge. In
this paper, we introduce a reparameterization-based formulation of neural BRDF
importance sampling that seamlessly integrates into the standard rendering
pipeline with precise generation of BRDF samples. The reparameterization-based
formulation transfers the distribution learning task to a problem of
identifying BRDF integral substitutions. In contrast to previous methods that
rely on invertible networks and multi-step inference to reconstruct BRDF
distributions, our model removes these constraints, which offers greater
flexibility and efficiency. Our variance and performance analysis demonstrates
that our reparameterization method achieves the best variance reduction in
neural BRDF renderings while maintaining high inference speeds compared to
existing baselines.

</details>


### [170] [FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis](https://arxiv.org/abs/2505.09109)
*Yuxing Chen,Bowen Xiao,He Wang*

Main category: cs.RO

Relevance: 30.0

TL;DR: 论文提出了一种合成服装数据集的方法，用于机器人服装折叠任务，通过关键点生成几何模板和纹理，并结合模仿学习训练策略。KG-DAgger方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于服装的可变形性，为机器人服装操作任务生成高质量数据具有挑战性。本文旨在通过合成数据解决这一问题。

Method: 构建基于关键点的几何服装模板，生成纹理模式，并通过模仿学习训练折叠策略。提出KG-DAgger方法以增强鲁棒性。

Result: KG-DAgger将真实世界成功率提升25%，最终模型在15K轨迹训练后达到75%的成功率。

Conclusion: 提出的框架在仿真和真实环境中均验证了有效性。

Abstract: Due to the deformability of garments, generating a large amount of
high-quality data for robotic garment manipulation tasks is highly challenging.
In this paper, we present a synthetic garment dataset that can be used for
robotic garment folding. We begin by constructing geometric garment templates
based on keypoints and applying generative models to generate realistic texture
patterns. Leveraging these keypoint annotations, we generate folding
demonstrations in simulation and train folding policies via closed-loop
imitation learning. To improve robustness, we propose KG-DAgger, which uses a
keypoint-based strategy to generate demonstration data for recovering from
failures. KG-DAgger significantly improves the model performance, boosting the
real-world success rate by 25\%. After training with 15K trajectories (about 2M
image-action pairs), the model achieves a 75\% success rate in the real world.
Experiments in both simulation and real-world settings validate the
effectiveness of our proposed framework.

</details>


### [171] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/abs/2505.09193)
*Wei Jiang,Junru Li,Kai Zhang,Li Zhang*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种双向视频压缩框架BiECVC，通过多样化上下文建模和自适应门控机制，显著提升了性能，首次在多个测试数据集上超越VTM 13.2。


<details>
  <summary>Details</summary>
Motivation: 现有双向视频压缩方法在上下文提取和动态适应方面表现不足，导致性能落后于单向方法。

Method: BiECVC结合了局部和非局部上下文建模，采用线性注意力机制和双向上下文门控技术。

Result: BiECVC在随机访问配置下比VTM 13.2节省了13.4%和15.7%的比特率。

Conclusion: BiECVC是首个在所有标准测试数据集上超越VTM 13.2的学习型视频编解码器。

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead.To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>


### [172] [APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression](https://arxiv.org/abs/2505.09356)
*Srinivas Ravuri,Yuan Xu,Martin Ludwig Zehetner,Ketan Motlag,Sahin Albayrak*

Main category: cs.RO

Relevance: 30.0

TL;DR: APR-Transformer是一种基于Transformer的模型，用于通过图像或LiDAR数据预测绝对姿态（3D位置和方向），在GNSS信号缺失的环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在机器人、自动驾驶和计算机视觉中，初始姿态的准确性对定位算法至关重要。传统方法在GNSS信号缺失时表现不佳，因此需要更鲁棒的解决方案。

Method: 提出APR-Transformer，利用深度神经网络进行姿态回归，并在多个基准数据集（如Radar Oxford Robot-Car和DeepLoc）上验证其性能。

Result: APR-Transformer在基准数据集上达到最先进性能，并在GNSS信号缺失的实际环境中验证了其可靠性。

Conclusion: APR-Transformer在姿态估计任务中表现出色，适用于实际应用场景。

Abstract: Precise initialization plays a critical role in the performance of
localization algorithms, especially in the context of robotics, autonomous
driving, and computer vision. Poor localization accuracy is often a consequence
of inaccurate initial poses, particularly noticeable in GNSS-denied
environments where GPS signals are primarily relied upon for initialization.
Recent advances in leveraging deep neural networks for pose regression have led
to significant improvements in both accuracy and robustness, especially in
estimating complex spatial relationships and orientations. In this paper, we
introduce APR-Transformer, a model architecture inspired by state-of-the-art
methods, which predicts absolute pose (3D position and 3D orientation) using
either image or LiDAR data. We demonstrate that our proposed method achieves
state-of-the-art performance on established benchmark datasets such as the
Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our
experiments to include our custom complex APR-BeIntelli dataset. Additionally,
we validate the reliability of our approach in GNSS-denied environments by
deploying the model in real-time on an autonomous test vehicle. This showcases
the practical feasibility and effectiveness of our approach. The source code is
available at:https://github.com/GT-ARC/APR-Transformer.

</details>


### [173] [Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net](https://arxiv.org/abs/2505.09521)
*Dongyi He,Shiyang Li,Bin Jiang,He Yan*

Main category: eess.IV

Relevance: 30.0

TL;DR: 提出了一种轻量级的EEG-to-fMRI生成器Spec2VolCAMU-Net，结合多方向时间-频率卷积注意力编码器和Vision-Mamba U-Net解码器，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率fMRI成本高且难以获取，而EEG广泛可用。现有方法在捕捉跨通道时间-频率线索或计算效率上存在不足。

Method: 采用多方向时间-频率卷积注意力编码器和Vision-Mamba U-Net解码器，结合SSI-MSE损失进行端到端训练。

Result: 在三个公开数据集上取得最佳SSIM和PSNR分数，重建质量显著提升。

Conclusion: Spec2VolCAMU-Net轻量高效，适用于临床和研究的实时应用。

Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.

</details>


### [174] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/abs/2505.08801)
*Md. Sakib Hassan Chowdhury,Md. Hafiz Ahamed,Bishowjit Paul,Sarafat Hussain Abhi,Abu Bakar Siddique,Md. Robius Sany*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种基于骨骼模型的OptiGait-LGBM模型，用于在复杂户外环境中进行步态识别，解决了现有方法在非约束条件下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频步态识别系统在非约束条件下性能下降，缺乏同时解决户外环境、光照变化等挑战的数据集。

Method: 通过骨骼关节标志点提取数据，构建非序列数据集，开发OptiGait-LGBM分类模型。

Result: 在准确性、内存使用和训练时间上优于Random Forest和CatBoost等集成方法。

Conclusion: 该方法为现实场景提供了一种低成本、内存高效的步态识别解决方案。

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [175] [TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian](https://arxiv.org/abs/2505.08811)
*Shijie Lian,Ziyi Zhang,Laurence Tianruo Yang and,Mengyu Ren,Debin Liu,Hua Li*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种名为TUGS的方法，用于水下3D场景重建，通过轻量化的张量化高斯模型和物理基础的自适应介质估计模块，解决了水下环境中光传播与物体几何的复杂交互问题。


<details>
  <summary>Details</summary>
Motivation: 水下3D场景重建对机器人感知和导航至关重要，但现有方法难以准确建模光传播、水介质和物体表面的复杂交互，且训练和渲染成本高。

Method: TUGS采用轻量化的张量化高阶高斯模型和物理基础的自适应介质估计模块（AME），模拟水下环境中的光衰减和背散射效应。

Result: 相比其他基于NeRF和GS的水下方法，TUGS能以更快的渲染速度和更低的内存占用生成高质量水下图像。

Conclusion: TUGS通过有限的参数实现了高质量重建，特别适用于内存受限的水下无人机应用。

Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications

</details>


### [176] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/abs/2505.08886)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种结合图像处理和机器学习（神经模糊和殖民竞争方法）的AI诊断工具，用于皮肤癌早期检测，准确率达94%。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率上升，公众意识不足且临床专家短缺，亟需先进的诊断辅助工具。

Method: 融合图像处理技术和机器学习算法（神经模糊和殖民竞争方法），应用于ISIC数据库的皮肤镜图像。

Result: 在560张图像的数据集上达到94%的准确率。

Conclusion: 该方法在皮肤癌早期检测中具有潜力，可显著提升诊断效果。

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [177] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/abs/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该论文提出了一种多模态深度学习框架，结合CGM时间序列数据、人口统计/微生物组数据和餐前食物图像，以提高热量估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确估计热量摄入对管理2型糖尿病至关重要，但现有方法（如连续血糖监测）因个体和餐食差异而效果有限。

Method: 使用基于注意力的编码和卷积特征提取处理食物图像，多层感知器处理CGM和微生物组数据，采用晚期融合策略进行联合推理。

Result: 在40多名参与者的数据集上，模型的热量估计均方根相对误差（RMSRE）为0.2544，比基线模型提高了50%以上。

Conclusion: 多模态感知技术有望改善慢性病管理的自动化饮食评估工具。

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [178] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/abs/2505.09529)
*Mohamed Moustafa,Joseph Lemley,Peter Corcoran*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该研究利用事件相机和CNN模型，从面部事件流中无接触重建心脏脉搏信号，展示了事件相机在远程心率监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索事件相机在生理信号监测中的应用，尤其是无接触心率监测，以利用其高动态范围和时间分辨率的优势。

Method: 使用监督式CNN模型，从事件流中提取心脏信号，并评估心率计算的准确性。

Result: 事件相机模型在60和120 FPS下表现优于30 FPS的传统相机，RMSE分别为2.54和2.13 bpm。

Conclusion: 事件相机在心率监测中具有潜力，尤其是在高帧率下表现更优。

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


### [179] [Neural BRDF Importance Sampling by Reparameterization](https://arxiv.org/abs/2505.08998)
*Liwen Wu,Sai Bi,Zexiang Xu,Hao Tan,Kai Zhang,Fujun Luan,Haolin Lu,Ravi Ramamoorthi*

Main category: cs.GR

Relevance: 20.0

TL;DR: 提出了一种基于重参数化的神经BRDF重要性采样方法，提高了渲染效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 神经BRDF在物理渲染中提升真实感，但其重要性采样仍具挑战性。

Method: 通过重参数化将分布学习任务转化为BRDF积分替换问题，避免依赖可逆网络和多步推理。

Result: 方差和性能分析表明，该方法在神经BRDF渲染中实现了最佳方差减少，同时保持高推理速度。

Conclusion: 重参数化方法为神经BRDF重要性采样提供了更高效和灵活的解决方案。

Abstract: Neural bidirectional reflectance distribution functions (BRDFs) have emerged
as popular material representations for enhancing realism in physically-based
rendering. Yet their importance sampling remains a significant challenge. In
this paper, we introduce a reparameterization-based formulation of neural BRDF
importance sampling that seamlessly integrates into the standard rendering
pipeline with precise generation of BRDF samples. The reparameterization-based
formulation transfers the distribution learning task to a problem of
identifying BRDF integral substitutions. In contrast to previous methods that
rely on invertible networks and multi-step inference to reconstruct BRDF
distributions, our model removes these constraints, which offers greater
flexibility and efficiency. Our variance and performance analysis demonstrates
that our reparameterization method achieves the best variance reduction in
neural BRDF renderings while maintaining high inference speeds compared to
existing baselines.

</details>


### [180] [FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis](https://arxiv.org/abs/2505.09109)
*Yuxing Chen,Bowen Xiao,He Wang*

Main category: cs.RO

Relevance: 20.0

TL;DR: 提出了一种用于机器人衣物折叠任务的合成数据集生成方法，结合关键点标注和生成模型，并通过KG-DAgger提升模型鲁棒性，最终在真实场景中达到75%的成功率。


<details>
  <summary>Details</summary>
Motivation: 由于衣物的可变形性，为机器人衣物操作任务生成大量高质量数据具有挑战性。

Method: 构建基于关键点的几何衣物模板，应用生成模型生成纹理图案，通过闭环模仿学习训练折叠策略，并提出KG-DAgger增强鲁棒性。

Result: 模型在真实场景中成功率达到75%，KG-DAgger将成功率提升25%。

Conclusion: 提出的框架在仿真和真实场景中均验证了有效性。

Abstract: Due to the deformability of garments, generating a large amount of
high-quality data for robotic garment manipulation tasks is highly challenging.
In this paper, we present a synthetic garment dataset that can be used for
robotic garment folding. We begin by constructing geometric garment templates
based on keypoints and applying generative models to generate realistic texture
patterns. Leveraging these keypoint annotations, we generate folding
demonstrations in simulation and train folding policies via closed-loop
imitation learning. To improve robustness, we propose KG-DAgger, which uses a
keypoint-based strategy to generate demonstration data for recovering from
failures. KG-DAgger significantly improves the model performance, boosting the
real-world success rate by 25\%. After training with 15K trajectories (about 2M
image-action pairs), the model achieves a 75\% success rate in the real world.
Experiments in both simulation and real-world settings validate the
effectiveness of our proposed framework.

</details>


### [181] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/abs/2505.09193)
*Wei Jiang,Junru Li,Kai Zhang,Li Zhang*

Main category: eess.IV

Relevance: 20.0

TL;DR: BiECVC是一种双向视频压缩框架，通过多样化上下文建模和自适应门控机制，显著提升了性能，超越了VTM 13.2 RA。


<details>
  <summary>Details</summary>
Motivation: 现有双向视频压缩（BVC）方法在提取多样化和准确上下文方面能力有限，且缺乏对快速运动或遮挡的适应性。

Method: BiECVC结合了局部和非局部上下文建模，采用线性注意力机制和双向上下文门控动态过滤信息。

Result: BiECVC在RA配置下比特率降低了13.4%和15.7%，性能优于VTM 13.2。

Conclusion: BiECVC是首个在所有标准测试数据集上超越VTM 13.2 RA的学习视频编解码器。

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead.To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>


### [182] [Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis](https://arxiv.org/abs/2505.09323)
*Pengli Zhu,Yingji Fu,Nanguang Chen,Anqi Qiu*

Main category: eess.IV

Relevance: 20.0

TL;DR: 提出了一种名为Q-CATN的新方法，用于从灵活的q空间采样中合成多壳高角度分辨率DWI数据，利用结构MRI数据，并通过协作注意力机制动态调整内部表示。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在固定采样方案下的局限性，提升DWI数据合成的灵活性和准确性。

Method: 采用协作注意力机制提取多模态互补信息，并引入任务特定约束以保持解剖保真度。

Result: 在HCP数据集上，Q-CATN在参数图和纤维束估计上优于现有方法，同时保留细节。

Conclusion: Q-CATN是一种有潜力的工具，适用于临床和研究应用。

Abstract: This study, we propose a novel Q-space Guided Collaborative Attention
Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI
(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly
acquired structural MRI data. Q-CATN employs a collaborative attention
mechanism to effectively extract complementary information from multiple
modalities and dynamically adjust its internal representations based on
flexible q-space information, eliminating the need for fixed sampling schemes.
Additionally, we introduce a range of task-specific constraints to preserve
anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic
relationships between directional DWI signal distributions and q-space.
Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate
that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,
and QGAN, in estimating parameter maps and fiber tracts both quantitatively and
qualitatively, while preserving fine-grained details. Notably, its ability to
accommodate flexible q-space sampling highlights its potential as a promising
toolkit for clinical and research applications. Our code is available at
https://github.com/Idea89560041/Q-CATN.

</details>


### [183] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该研究提出了一种基于行为监控的系统，使用定制的有向图神经网络（DGNN）对火车司机状态进行分类，结合面部和骨骼特征实现最高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决火车司机疲劳问题，提升铁路安全。

Method: 使用DGNN分类司机状态，比较不同特征配置（骨骼、面部及组合）。

Result: 组合特征在三分类模型中准确率达80.88%，二分类中超过99%。

Conclusion: 结合视觉技术的在线监控系统可提升铁路安全。

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [184] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/abs/2505.08801)
*Md. Sakib Hassan Chowdhury,Md. Hafiz Ahamed,Bishowjit Paul,Sarafat Hussain Abhi,Abu Bakar Siddique,Md. Robius Sany*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种基于骨骼模型的OptiGait-LGBM模型，用于在复杂室外环境中进行步态识别，解决了现有方法在真实场景中性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 解决视频步态识别在真实无约束环境中的性能下降问题，如光照变化、非重叠摄像头视角等挑战。

Method: 使用骨骼模型提取关节标志点，生成数值数据集，并开发OptiGait-LGBM分类模型。

Result: 提出的方法在准确性、内存使用和训练时间上优于随机森林和CatBoost等集成技术。

Conclusion: OptiGait-LGBM为真实场景提供了一种低成本、内存高效的步态识别解决方案。

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [185] [TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian](https://arxiv.org/abs/2505.08811)
*Shijie Lian,Ziyi Zhang,Laurence Tianruo Yang and,Mengyu Ren,Debin Liu,Hua Li*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种名为TUGS的方法，用于水下3D场景重建，通过轻量化的张量化高阶高斯模型和物理驱动的自适应介质估计模块，解决了水下环境中光线传播与物体几何交互的建模问题，同时减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 水下3D场景重建对机器人感知和导航至关重要，但现有方法难以准确建模光线传播、水体介质和物体表面的复杂交互，且训练和渲染成本高昂。

Method: TUGS采用轻量化的张量化高阶高斯模型和物理驱动的自适应介质估计模块（AME），模拟水下环境中的光线衰减和背散射效应。

Result: 实验表明，TUGS在真实水下数据集上能以更快的渲染速度和更低的内存占用实现高质量重建，适用于内存受限的水下无人机应用。

Conclusion: TUGS通过高效建模和参数优化，显著提升了水下3D场景重建的质量和实用性。

Abstract: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications

</details>


### [186] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/abs/2505.08886)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种结合图像处理和机器学习的方法（神经模糊和殖民竞争算法），用于皮肤癌诊断，准确率达94%。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率上升，公众意识不足且临床专业知识短缺，亟需先进的诊断辅助工具。

Method: 采用神经模糊和殖民竞争算法，结合图像处理技术，应用于ISIC数据库的皮肤镜图像。

Result: 在560张图像的数据集上达到94%的准确率。

Conclusion: 该方法有助于临床医生早期发现黑色素瘤，对皮肤癌诊断有重要意义。

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [187] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/abs/2505.09306)
*Thijs L van der Plas,Stephen Law,Michael JO Pocock*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种基于Resnet的模型，利用卫星图像预测英国蝴蝶物种的存在，并开发了一种对比正则化损失方法以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展生物多样性监测方法的需求增长，利用遥感数据直接预测多物种存在成为研究热点。

Method: 优化Resnet模型预测多物种存在，并开发了一种针对概率标签的对比正则化损失方法。

Result: 模型在物种多样性高的地区表现优于基线，对比正则化方法提高了预测准确性。

Conclusion: 新数据集和对比正则化方法为遥感数据预测生物多样性提供了有效工具。

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [188] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/abs/2505.09368)
*Jenny Schmalfuss,Victor Oei,Lukas Mehl,Madlen Bartsch,Shashank Agnihotri,Margret Keuper,Andrés Bruhn*

Main category: cs.CV

Relevance: 20.0

TL;DR: RobustSpring是一个专注于评估光学流、场景流和立体视觉模型对图像损坏（如噪声、雨等）鲁棒性的新数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注模型准确性，而忽略了模型对现实世界图像损坏的鲁棒性。RobustSpring旨在填补这一空白。

Method: 通过在高分辨率Spring数据集上应用20种不同的图像损坏（如噪声、模糊、天气失真等），生成20,000张损坏图像，并设计新的鲁棒性指标。

Result: 实验表明，准确性高的模型不一定鲁棒，且鲁棒性因损坏类型而异。

Conclusion: RobustSpring将鲁棒性作为首要目标，旨在推动兼具准确性和鲁棒性的模型发展。

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [189] [Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians](https://arxiv.org/abs/2505.09413)
*Ma Changfeng,Bi Ran,Guo Jie,Wang Chongjun,Guo Yanwen*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种新的点云渲染方法，通过预测2D高斯分布来实现，无需依赖类别先验或密集点云，且能直接泛化到不同类别的点云。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖类别先验或密集点云，限制了泛化能力和效率。

Method: 使用两个相同模块的整个补丁架构，通过点云信息初始化高斯分布，并通过分裂解码器细化结果。

Result: 在多个数据集上实现了SOTA性能，且能直接泛化到稀疏点云。

Conclusion: 该方法高效且泛化能力强，适用于稀疏点云渲染。

Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

</details>


### [190] [MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2505.09422)
*Xiangyuan Peng,Yu Wang,Miao Tang,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

Relevance: 20.0

TL;DR: MoRAL是一个运动感知的多帧4D雷达与LiDAR融合框架，用于鲁棒的3D目标检测，通过补偿雷达点云的运动偏差并利用动态信息，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中雷达点云帧间不对齐问题，并充分利用4D雷达的动态信息以提高自动驾驶系统的可靠性。

Method: 提出运动感知雷达编码器（MRE）补偿运动偏差，以及运动注意力门控融合（MAGF）模块整合雷达动态信息指导LiDAR特征。

Result: 在VoD数据集上表现优异，整体区域mAP达73.30%，驾驶走廊达88.68%，行人和骑行者检测AP也显著提升。

Conclusion: MoRAL通过运动感知和动态信息融合显著提升了3D目标检测性能，尤其在动态对象检测上表现突出。

Abstract: Reliable autonomous driving systems require accurate detection of traffic
participants. To this end, multi-modal fusion has emerged as an effective
strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame
radar point clouds have demonstrated the effectiveness in bridging the point
density gap. However, they often neglect radar point clouds' inter-frame
misalignment caused by object movement during accumulation and do not fully
exploit the object dynamic information from 4D radar. In this paper, we propose
MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for
robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is
designed to compensate for inter-frame radar misalignment from moving objects.
Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion
features to guide LiDAR features to focus on dynamic foreground objects.
Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL
outperforms existing methods, achieving the highest mAP of 73.30% in the entire
area and 88.68% in the driving corridor. Notably, our method also achieves the
best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in
the driving corridor.

</details>


### [191] [Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes](https://arxiv.org/abs/2505.09562)
*Nicola Marinello,Simen Cassiman,Jonas Heylen,Marc Proesmans,Luc Van Gool*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该论文提出了一种用于3D全景场景补全的新框架，扩展了现有的3D语义场景补全模型，并引入了对象模块和全景模块。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要完整的环境地图以规划和行动，而3D全景场景补全任务目前尚未充分探索。

Method: 提出了一种框架，结合对象模块和全景模块，可轻松集成现有的3D占用和场景补全方法。

Result: 该方法利用占用基准中的标注，将个体对象形状学习为一个可微分问题。

Conclusion: 该框架为3D全景场景补全任务提供了新的解决方案，并开源了代码。

Abstract: Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .

</details>


### [192] [Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis](https://arxiv.org/abs/2505.09323)
*Pengli Zhu,Yingji Fu,Nanguang Chen,Anqi Qiu*

Main category: eess.IV

Relevance: 20.0

TL;DR: 提出了一种名为Q-CATN的新方法，用于从灵活的q空间采样中合成多壳高角度分辨率DWI，利用结构MRI数据，并通过协作注意力机制动态调整内部表示。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在灵活q空间采样下合成DWI时的局限性，同时保持解剖保真度。

Method: 采用协作注意力机制提取多模态互补信息，并引入任务特定约束以保持DWI的解剖保真度。

Result: 在HCP数据集上，Q-CATN在参数图和纤维束估计上优于现有方法，并保留了细节。

Conclusion: Q-CATN是一种有潜力的工具，适用于临床和研究应用。

Abstract: This study, we propose a novel Q-space Guided Collaborative Attention
Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI
(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly
acquired structural MRI data. Q-CATN employs a collaborative attention
mechanism to effectively extract complementary information from multiple
modalities and dynamically adjust its internal representations based on
flexible q-space information, eliminating the need for fixed sampling schemes.
Additionally, we introduce a range of task-specific constraints to preserve
anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic
relationships between directional DWI signal distributions and q-space.
Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate
that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,
and QGAN, in estimating parameter maps and fiber tracts both quantitatively and
qualitatively, while preserving fine-grained details. Notably, its ability to
accommodate flexible q-space sampling highlights its potential as a promising
toolkit for clinical and research applications. Our code is available at
https://github.com/Idea89560041/Q-CATN.

</details>


### [193] [Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety](https://arxiv.org/abs/2505.08882)
*Ali Almakhluk,Uthman Baroudi,Yasser El-Alfy*

Main category: cs.CV

Relevance: 10.0

TL;DR: 该研究开发了一个基于深度学习的系统，用于实时检测和分类道路损坏（如坑洞和裂缝），并通过云服务通知相关部门和附近车辆，以提高交通安全。


<details>
  <summary>Details</summary>
Motivation: 道路损坏（如坑洞和裂缝）是交通事故的常见原因，亟需一种主动检测和预警系统来提升道路安全。

Method: 系统使用树莓派、摄像头模块、深度学习模型和云服务，实时检测、分类道路损坏并传输数据。

Result: 系统能够实时检测和分类道路损坏，并向相关部门和附近车辆发出警告。

Conclusion: 该解决方案通过主动预警和通知，有望减少因道路损坏引发的交通事故，提升整体道路安全。

Abstract: This study aims to improve transportation safety, especially traffic safety.
Road damage anomalies such as potholes and cracks have emerged as a significant
and recurring cause for accidents. To tackle this problem and improve road
safety, a comprehensive system has been developed to detect potholes, cracks
(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit
this data to the cloud for appropriate action by authorities. The system also
broadcasts warning signals to nearby vehicles warning them if a severe anomaly
is detected on the road. Moreover, the system can count road anomalies in
real-time. It is emulated through the utilization of Raspberry Pi, a camera
module, deep learning model, laptop, and cloud service. Deploying this
innovative solution aims to proactively enhance road safety by notifying
relevant authorities and drivers about the presence of potholes and cracks to
take actions, thereby mitigating potential accidents arising from this
prevalent road hazard leading to safer road conditions for the whole community.

</details>


### [194] [A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures](https://arxiv.org/abs/2505.09251)
*Vineetha Joy,Aditya Anand,Nidhi,Anshuman Kumar,Amit Sethi,Hema Singh*

Main category: cs.CV

Relevance: 10.0

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）的替代模型，用于加速多层超表面雷达吸收结构（RAS）的电磁响应预测，显著减少了计算时间和设计空间探索。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖全波仿真工具进行电磁设计和优化，计算量大且耗时，因此需要一种更高效的替代方案。

Method: 采用基于CNN的架构和Huber损失函数来预测RAS的反射特性。

Result: 模型在1000次训练周期内实现了99.9%的余弦相似度和0.001的均方误差，显著减少了计算时间并保持高预测精度。

Conclusion: 提出的替代模型在保持高精度的同时大幅提升了计算效率，适用于超表面RAS的设计优化。

Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for
applications like stealth technology, electromagnetic (EM) shielding, etc. due
to their capability to achieve frequency selective absorption characteristics
with minimal thickness and reduced weight penalty. However, the conventional
approach for the EM design and optimization of these structures relies on
forward simulations, using full wave simulation tools, to predict the
electromagnetic (EM) response of candidate meta atoms. This process is
computationally intensive, extremely time consuming and requires exploration of
large design spaces. To overcome this challenge, we propose a surrogate model
that significantly accelerates the prediction of EM responses of multi-layered
metasurface-based RAS. A convolutional neural network (CNN) based architecture
with Huber loss function has been employed to estimate the reflection
characteristics of the RAS model. The proposed model achieved a cosine
similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of
training. The efficiency of the model has been established via full wave
simulations as well as experiment where it demonstrated significant reduction
in computational time while maintaining high predictive accuracy.

</details>


### [195] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/abs/2505.09306)
*Thijs L van der Plas,Stephen Law,Michael JO Pocock*

Main category: cs.CV

Relevance: 10.0

TL;DR: 论文提出了一种新的数据集和方法，用于从卫星图像预测蝴蝶物种存在，并通过对比正则化损失提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着对生物多样性监测方法的需求增加，利用遥感数据直接预测多物种存在成为研究热点。本文旨在通过优化模型和提出新方法，提高从卫星图像预测物种多样性的准确性。

Method: 使用Resnet-based模型从4波段卫星图像预测多物种存在，并开发了一种针对概率标签的软监督对比正则化损失。

Result: 模型在物种多样性高的地区表现优于基线，提出的正则化方法进一步提高了预测准确性。

Conclusion: 新数据集和对比正则化方法为从遥感数据准确预测生物多样性提供了有效工具。

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [196] [Total Variation-Based Image Decomposition and Denoising for Microscopy Images](https://arxiv.org/abs/2505.08843)
*Marco Corrias,Giada Franceschi,Michele Riva,Alberto Tampieri,Karin Föttinger,Ulrike Diebold,Thomas Pock,Cesare Franchini*

Main category: eess.IV

Relevance: 10.0

TL;DR: 该论文提出了一种基于总变分（TV）的显微镜图像分解与去噪方法，评估了TV-L1、Huber-ROF和TGV-L1在不同案例中的表现。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像常受噪声和干扰信号影响，现代去噪和恢复方法需求增加。

Method: 通过提取并去除图像中的干扰信号或直接去噪，评估了TV-L1、Huber-ROF和TGV-L1的性能。

Result: Huber-ROF表现最灵活，TGV-L1最适合去噪，方法适用于多种显微镜技术。

Conclusion: 该方法在显微镜图像处理中具有广泛适用性，代码已开源。

Abstract: Experimentally acquired microscopy images are unavoidably affected by the
presence of noise and other unwanted signals, which degrade their quality and
might hide relevant features. With the recent increase in image acquisition
rate, modern denoising and restoration solutions become necessary. This study
focuses on image decomposition and denoising of microscopy images through a
workflow based on total variation (TV), addressing images obtained from various
microscopy techniques, including atomic force microscopy (AFM), scanning
tunneling microscopy (STM), and scanning electron microscopy (SEM). Our
approach consists in restoring an image by extracting its unwanted signal
components and subtracting them from the raw one, or by denoising it. We
evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving
this goal in distinct study cases. Huber-ROF proved to be the most flexible
one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a
wider applicability of this method in microscopy, restricted not only to STM,
AFM, and SEM images. The Python code used for this study is publicly available
as part of AiSurf. It is designed to be integrated into experimental workflows
for image acquisition or can be used to denoise previously acquired images.

</details>


### [197] [A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures](https://arxiv.org/abs/2505.09251)
*Vineetha Joy,Aditya Anand,Nidhi,Anshuman Kumar,Amit Sethi,Hema Singh*

Main category: cs.CV

Relevance: 10.0

TL;DR: 论文提出了一种基于卷积神经网络（CNN）的代理模型，用于快速预测多层超表面雷达吸收结构（RAS）的电磁响应，显著减少了计算时间和设计空间探索的需求。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于全波仿真工具进行电磁设计和优化，计算量大且耗时。

Method: 采用基于Huber损失函数的CNN架构来预测RAS的反射特性。

Result: 模型在1000次训练周期内实现了99.9%的余弦相似度和0.001的均方误差，显著减少了计算时间。

Conclusion: 该代理模型在保持高预测精度的同时，大幅提升了计算效率。

Abstract: Metasurface-based radar absorbing structures (RAS) are highly preferred for
applications like stealth technology, electromagnetic (EM) shielding, etc. due
to their capability to achieve frequency selective absorption characteristics
with minimal thickness and reduced weight penalty. However, the conventional
approach for the EM design and optimization of these structures relies on
forward simulations, using full wave simulation tools, to predict the
electromagnetic (EM) response of candidate meta atoms. This process is
computationally intensive, extremely time consuming and requires exploration of
large design spaces. To overcome this challenge, we propose a surrogate model
that significantly accelerates the prediction of EM responses of multi-layered
metasurface-based RAS. A convolutional neural network (CNN) based architecture
with Huber loss function has been employed to estimate the reflection
characteristics of the RAS model. The proposed model achieved a cosine
similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of
training. The efficiency of the model has been established via full wave
simulations as well as experiment where it demonstrated significant reduction
in computational time while maintaining high predictive accuracy.

</details>


### [198] [Total Variation-Based Image Decomposition and Denoising for Microscopy Images](https://arxiv.org/abs/2505.08843)
*Marco Corrias,Giada Franceschi,Michele Riva,Alberto Tampieri,Karin Föttinger,Ulrike Diebold,Thomas Pock,Cesare Franchini*

Main category: eess.IV

Relevance: 10.0

TL;DR: 该论文提出了一种基于总变分（TV）的显微镜图像分解与去噪方法，评估了TV-L1、Huber-ROF和TGV-L1在不同显微镜技术中的表现。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像常受噪声干扰，影响质量，需要现代去噪和恢复方法。

Method: 通过提取并去除图像中的噪声成分或直接去噪，评估了TV-L1、Huber-ROF和TGV-L1的性能。

Result: Huber-ROF灵活性最高，TGV-L1最适合去噪，方法适用于多种显微镜技术。

Conclusion: 该方法在显微镜图像处理中具有广泛适用性，代码已开源。

Abstract: Experimentally acquired microscopy images are unavoidably affected by the
presence of noise and other unwanted signals, which degrade their quality and
might hide relevant features. With the recent increase in image acquisition
rate, modern denoising and restoration solutions become necessary. This study
focuses on image decomposition and denoising of microscopy images through a
workflow based on total variation (TV), addressing images obtained from various
microscopy techniques, including atomic force microscopy (AFM), scanning
tunneling microscopy (STM), and scanning electron microscopy (SEM). Our
approach consists in restoring an image by extracting its unwanted signal
components and subtracting them from the raw one, or by denoising it. We
evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving
this goal in distinct study cases. Huber-ROF proved to be the most flexible
one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a
wider applicability of this method in microscopy, restricted not only to STM,
AFM, and SEM images. The Python code used for this study is publicly available
as part of AiSurf. It is designed to be integrated into experimental workflows
for image acquisition or can be used to denoise previously acquired images.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [199] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

Relevance: 90.0

TL;DR: 论文提出了一种隐私保护的语言模型对齐方法，结合差分隐私（DP）与直接偏好优化（DPO）和强化学习人类反馈（RLHF），在中等隐私预算下性能提升15%。


<details>
  <summary>Details</summary>
Motivation: 语言模型对齐需处理敏感数据，现有方法在隐私保护与性能间存在不足。

Method: 提出新算法（如DP-AdamW），结合DP与DPO/RLHF，分析隐私预算、模型性能与计算需求的关系。

Result: DP-AdamW+DPO在中等隐私预算下性能提升15%，优于现有方法。

Conclusion: 该框架为隐私保护对齐提供了高效解决方案，并优化了性能与隐私的权衡。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


### [200] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
*Michael Majurski,Cynthia Matuszek*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了一种自动化构建基于事实的合成数据模型评估方法，利用语言模型自动生成领域特定知识的评估问题，与人工构建的基准相关性高。


<details>
  <summary>Details</summary>
Motivation: 人工构建评估基准的效率低且难以覆盖所有领域，需要自动化方法。

Method: 利用语言模型基于文档自动生成评估问题，支持多选和开放式问题。

Result: 合成数据基准与人工构建基准的相关性高（Spearman 0.96，Pearson 0.79），发现Gemma3模型表现优异。

Conclusion: 自动化评估方法高效且可靠，适用于大规模模型评估。

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [201] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种元提示方法，通过联合优化人类心理预期与LLM神经处理的相似性，生成复杂任务的流畅文本。采用代理强化学习技术，通过上下文学习让LLM作为法官（LLMaaJ）教导另一个LLM生成内容。实验显示，人类内容审阅者的预期与AI生成内容在53.8%的情况下完全对齐。


<details>
  <summary>Details</summary>
Motivation: 解决人类心理预期与LLM生成内容之间的对齐问题，提升内容质量。

Method: 采用元提示和代理强化学习（LLMaaJ），通过上下文学习优化文本生成。

Result: 人类预期与AI生成内容在53.8%的情况下完全对齐，内容质量显著提升。

Conclusion: 该方法有效解决了Theory of Mind对齐问题，并在实际应用中验证了其效果。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [202] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
*Adarsh Kumar,Hwiyoon Kim,Jawahar Sai Nathani,Neil Roy*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文探讨了如何通过结合链式思维（CoT）与检索增强生成（RAG），以及自一致性和自验证策略，减少大语言模型（LLMs）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂任务中生成错误或无关信息（幻觉）是其应用的主要限制，现有方法如CoT未能完全解决此问题。

Method: 结合CoT与RAG，引入自一致性和自验证策略，利用外部知识源并让模型自我验证输出。

Result: 比较了基线LLM、CoT、CoT+RAG、自一致性和自验证方法，发现结合多种策略能有效减少幻觉并保持流畅性和推理深度。

Conclusion: 综合使用CoT、RAG、自一致性和自验证是最稳健的方法，可显著减少幻觉并提高事实准确性。

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [203] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/abs/2505.09114)
*Minh Hoang Nguyen,Linh Le Pham Van,Thommen George Karimpanal,Sunil Gupta,Hung Le*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种基于反事实推理的决策变换器（CRDT），通过生成和利用反事实经验，提升了在未见场景中的决策能力，优于传统DT方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，离线数据集通常缺乏高质量和全面的数据，且存在次优行为，这限制了决策变换器（DT）的性能。

Method: 提出CRDT框架，利用反事实推理生成额外经验，增强DT的泛化能力，无需修改架构。

Result: 在Atari和D4RL基准测试中，CRDT在数据有限和动态变化场景下表现优于传统DT方法，具备轨迹拼接能力。

Conclusion: 反事实推理能显著提升强化学习代理的性能和泛化能力。

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [204] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/abs/2505.09289)
*Pedro M. P. Curvo,Mara Dragomir,Salvador Torpes,Mohammadmahdi Rahimi*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究评估并扩展了Piatti等人的GovSim框架，验证了大型语言模型（如GPT-4-turbo）在资源分享场景中的合作决策能力，并探讨了通用化原则的影响。研究还扩展了框架的适用性，测试了不同架构和规模的模型，并引入了新的场景和语言设置。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证大型语言模型在复杂合作任务中的表现，并扩展GovSim框架的适用性，以提供更多关于LLM适应性的见解。

Method: 通过复制关键实验并引入新模型（如DeepSeek-V3和GPT-4o-mini）和新场景（如异构多智能体环境和日语指令场景），评估模型的合作行为。

Result: 大型模型（如GPT-4-turbo）在有无通用化原则下均能实现可持续合作，而小型模型则依赖该原则。研究还发现高性能模型可以影响低性能模型的行为。

Conclusion: GovSim框架适用于新模型、场景和语言，为LLM在复杂合作任务中的适应性提供了重要见解，对开发高效合作AI系统有潜在贡献。

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [205] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/abs/2505.09341)
*Evžen Wybitul*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种基于用户凭证和风险分类的访问控制框架，以解决AI系统的双重用途问题，同时平衡模型效用与安全性。


<details>
  <summary>Details</summary>
Motivation: AI系统面临双重用途问题，即同一请求可能无害或有害，取决于请求者和目的。仅基于内容决策会导致误拒合法请求或放行有害请求。

Method: 提出基于已验证用户凭证（如机构隶属关系）和风险分类器（如高级病毒学）的访问控制框架。采用小型门控专家模块集成到生成模型中的理论方法，通过梯度路由训练实现高效风险检测。

Result: 框架初步实现了对AI能力的细粒度治理，验证用户可获取专业知识，而对手被阻止，解决了双重用途问题。

Conclusion: 该上下文方法平衡了模型效用与安全性，为AI能力的细粒度治理迈出了第一步。

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [206] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/abs/2505.09396)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了LLM代理在博弈论环境中模拟人类战略推理的能力，比较了三种代理设计，发现人类认知结构能提升LLM代理的表现，但复杂性与人类相似性关系非线性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代理是否能够复制人类战略推理，特别是在博弈论背景下，以理解代理设计的复杂性与人类行为对齐的关系。

Method: 评估了三种代理设计：简单博弈论模型、无结构LLM代理模型和LLM与传统代理框架的集成模型，使用猜测游戏作为测试平台，并引入混淆游戏场景评估泛化能力。

Result: 人类启发的认知结构能提升LLM代理与人类战略行为的对齐，但复杂性与人类相似性关系非线性，且受限于LLM基础能力。

Conclusion: LLM代理的设计复杂性需与基础模型能力匹配，简单的架构增强存在局限性。

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [207] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
*Anthony GX-Chen,Dongyan Lin,Mandana Samiei,Doina Precup,Blake A. Richards,Rob Fergus,Kenneth Marino*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了语言模型（LMs）在探索和推断因果关系时的能力，发现LMs对常见的析取因果关系表现良好，但对合取关系存在系统性偏差（“析取偏差”）。这种偏差与人类成年人类似，表明LMs可能从训练数据中继承了深层次的推理启发式。作者提出了一种测试时采样方法，显著减少了这种偏差。


<details>
  <summary>Details</summary>
Motivation: 探究LMs是否具备探索和理解世界因果结构的能力，以及是否存在系统性偏差，从而影响其科学推理的严谨性。

Method: 使用发展心理学中的“Blicket Test”范式，评估LMs在不同因果关系中的表现，并提出一种测试时采样方法以减少偏差。

Result: LMs在析取因果关系中表现可靠，但在合取关系中存在系统性偏差。这种偏差与人类成年人类似，且随着任务复杂性增加而加剧。提出的采样方法显著减少了偏差。

Conclusion: LMs在因果推理中存在与人类类似的偏差，但通过特定方法可以改善。这表明LMs的推理能力受训练数据影响，未来需进一步优化其科学推理能力。

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


### [208] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

Relevance: 85.0

TL;DR: 该论文提出了一种隐私保护的语言模型对齐方法，结合差分隐私（DP）与直接偏好优化（DPO）和强化学习人类反馈（RLHF），在中等隐私预算下显著提升对齐质量。


<details>
  <summary>Details</summary>
Motivation: 语言模型对齐需要处理敏感用户数据，现有方法在隐私保护方面表现有限，因此需要更高效的隐私保护对齐算法。

Method: 提出新的隐私保护对齐算法（如DP-AdamW），并将其应用于DPO和RLHF框架，通过实验验证其性能。

Result: 在中等隐私预算（ε=2-5）下，DP-AdamW结合DPO将对齐质量提升15%，优于现有方法。

Conclusion: 该研究为隐私保护与模型对齐的权衡提供了实用指南，展示了新算法在性能上的优势。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


### [209] [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Haim Permuter,Flavio P. Calmon*

Main category: cs.CR

Relevance: 85.0

TL;DR: 本文分析了在单次设置下的大型语言模型（LLM）文本水印技术，探讨了水印检测能力与生成文本质量之间的权衡，并提出了一种最优耦合和随机化策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本质量的提升，水印技术成为区分人类与机器生成内容的关键工具。本文旨在优化水印设计，平衡检测能力与文本质量。

Method: 通过假设检验和侧信息分析，提出了基于LLM词汇表随机分区的最优耦合策略，并在最小熵约束下验证了其最优性。

Result: 提供了检测率的闭式表达式，并通过数值实验验证了该方案在合成数据和实际LLM水印中的优越性。

Conclusion: 该研究为LLM水印设计提供了理论支持，并在实际应用中展示了其有效性。

Abstract: Large-language models (LLMs) are now able to produce text that is, in many
cases, seemingly indistinguishable from human-generated content. This has
fueled the development of watermarks that imprint a ``signal'' in LLM-generated
text with minimal perturbation of an LLM's output. This paper provides an
analysis of text watermarking in a one-shot setting. Through the lens of
hypothesis testing with side information, we formulate and analyze the
fundamental trade-off between watermark detection power and distortion in
generated textual quality. We argue that a key component in watermark design is
generating a coupling between the side information shared with the watermark
detector and a random partition of the LLM vocabulary. Our analysis identifies
the optimal coupling and randomization strategy under the worst-case LLM
next-token distribution that satisfies a min-entropy constraint. We provide a
closed-form expression of the resulting detection rate under the proposed
scheme and quantify the cost in a max-min sense. Finally, we provide an array
of numerical results, comparing the proposed scheme with the theoretical
optimum and existing schemes, in both synthetic data and LLM watermarking. Our
code is available at https://github.com/Carol-Long/CC_Watermark

</details>


### [210] [ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor](https://arxiv.org/abs/2505.09142)
*Seungbeom Choi,Jeonghoe Goo,Eunjoo Jeon,Mingyu Yang,Minsung Jang*

Main category: cs.DC

Relevance: 85.0

TL;DR: ELIS是一个用于大型语言模型（LLM）的服务系统，采用迭代最短剩余时间优先（ISRTF）调度器，通过预测剩余令牌数优化推理任务调度，减少平均任务完成时间19.6%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统采用先到先服务调度策略，容易导致“队头阻塞”问题，需通过预测推理时间优化调度。

Method: 训练基于BGE模型的响应长度预测器，设计ISRTF调度策略，并在Kubernetes上实现云原生调度系统。

Result: 实验表明，ISRTF将平均任务完成时间减少19.6%。

Conclusion: ELIS通过智能调度显著提升LLM推理效率。

Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring
an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to
efficiently manage inference tasks with the shortest remaining tokens. Current
LLM serving systems often employ a first-come-first-served scheduling strategy,
which can lead to the "head-of-line blocking" problem. To overcome this
limitation, it is necessary to predict LLM inference times and apply a shortest
job first scheduling strategy. However, due to the auto-regressive nature of
LLMs, predicting the inference latency is challenging. ELIS addresses this
challenge by training a response length predictor for LLMs using the BGE model,
an encoder-based state-of-the-art model. Additionally, we have devised the
ISRTF scheduling strategy, an optimization of shortest remaining time first
tailored to existing LLM iteration batching. To evaluate our work in an
industrial setting, we simulate streams of requests based on our study of
real-world user LLM serving trace records. Furthermore, we implemented ELIS as
a cloud-native scheduler system on Kubernetes to evaluate its performance in
production environments. Our experimental results demonstrate that ISRTF
reduces the average job completion time by up to 19.6%.

</details>


### [211] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343)
*Chenggang Zhao,Chengqi Deng,Chong Ruan,Damai Dai,Huazuo Gao,Jiashi Li,Liyue Zhang,Panpan Huang,Shangyan Zhou,Shirong Ma,Wenfeng Liang,Ying He,Yuqing Wang,Yuxuan Liu,Y. X. Wei*

Main category: cs.DC

Relevance: 85.0

TL;DR: DeepSeek-V3通过硬件与模型协同设计解决了LLM扩展中的硬件限制，提出了MLA、MoE、FP8混合精度训练和多平面网络拓扑等创新技术。


<details>
  <summary>Details</summary>
Motivation: 当前硬件架构在内存容量、计算效率和互连带宽方面限制了LLM的扩展，需要硬件与模型协同设计来解决这些问题。

Method: 采用Multi-head Latent Attention (MLA)提升内存效率，Mixture of Experts (MoE)优化计算-通信权衡，FP8混合精度训练充分发挥硬件潜力，多平面网络拓扑减少集群级网络开销。

Result: DeepSeek-V3在2,048个NVIDIA H800 GPU上实现了高效的大规模训练和推理。

Conclusion: 硬件与模型协同设计对满足AI工作负载需求至关重要，为下一代AI系统创新提供了实用蓝图。

Abstract: The rapid scaling of large language models (LLMs) has unveiled critical
limitations in current hardware architectures, including constraints in memory
capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,
trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model
co-design can effectively address these challenges, enabling cost-efficient
training and inference at scale. This paper presents an in-depth analysis of
the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting
key innovations such as Multi-head Latent Attention (MLA) for enhanced memory
efficiency, Mixture of Experts (MoE) architectures for optimized
computation-communication trade-offs, FP8 mixed-precision training to unlock
the full potential of hardware capabilities, and a Multi-Plane Network Topology
to minimize cluster-level network overhead. Building on the hardware
bottlenecks encountered during DeepSeek-V3's development, we engage in a
broader discussion with academic and industry peers on potential future
hardware directions, including precise low-precision computation units,
scale-up and scale-out convergence, and innovations in low-latency
communication fabrics. These insights underscore the critical role of hardware
and model co-design in meeting the escalating demands of AI workloads, offering
a practical blueprint for innovation in next-generation AI systems.

</details>


### [212] [Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach](https://arxiv.org/abs/2505.09576)
*Shannon Lodoen,Alexi Orchard*

Main category: cs.CY

Relevance: 85.0

TL;DR: 论文探讨了RLHF技术如何通过人类反馈优化LLM输出，分析了其对语言惯例、信息获取和社会关系的伦理影响。


<details>
  <summary>Details</summary>
Motivation: 研究RLHF技术对LLM输出的影响及其潜在的伦理和社会问题。

Method: 采用修辞分析方法，聚焦RLHF增强的LLM的底层机制。

Result: 揭示了RLHF可能强化霸权语言使用、偏见和去情境化学习。

Conclusion: 提出AI伦理研究的新方向，关注技术对社会关系的潜在影响。

Abstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude
have been trained using a specialized technique called Reinforcement Learning
from Human Feedback (RLHF) to fine-tune language model output using feedback
from human annotators. As a result, the integration of RLHF has greatly
enhanced the outputs of these large language models (LLMs) and made the
interactions and responses appear more "human-like" than those of previous
versions using only supervised learning. The increasing convergence of human
and machine-written text has potentially severe ethical, sociotechnical, and
pedagogical implications relating to transparency, trust, bias, and
interpersonal relations. To highlight these implications, this paper presents a
rhetorical analysis of some of the central procedures and processes currently
being reshaped by RLHF-enhanced generative AI chatbots: upholding language
conventions, information seeking practices, and expectations for social
relationships. Rhetorical investigations of generative AI and LLMs have, to
this point, focused largely on the persuasiveness of the content generated.
Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site
of rhetorical investigation from content analysis to the underlying mechanisms
of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical
investigation opens a new direction for further inquiry in AI ethics that
considers how procedures rerouted through AI-driven technologies might
reinforce hegemonic language use, perpetuate biases, decontextualize learning,
and encroach upon human relationships. It will therefore be of interest to
educators, researchers, scholars, and the growing number of users of generative
AI chatbots.

</details>


### [213] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
*Michael Majurski,Cynthia Matuszek*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了一种自动化构建基于事实的合成数据模型评估方法，利用语言模型自动生成领域特定知识的问题，减少人工构建基准测试的需求。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模和范围的扩大，人工构建评估基准变得不切实际，需要自动化方法来高效评估模型能力。

Method: 利用语言模型基于文档自动生成领域特定知识的问题，支持多选和开放式问题，用于评估模型性能。

Result: 合成数据基准与人工构建的问题相关性高（Spearman 0.96，Pearson 0.79），并在评估Gemma3模型时发现其表现优异。

Conclusion: 该方法为自动化评估语言模型提供了高效工具，减少了对人工基准的依赖。

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [214] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了一种元提示方法，通过联合优化人类心理预期与LLM神经处理的相似性，生成复杂任务的流畅文本。采用代理强化学习技术，让LLM作为裁判（LLMaaJ）教导另一LLM生成内容。实验显示，人类内容审核者的预期与AI对齐率为53.8%，迭代次数平均4.38次。


<details>
  <summary>Details</summary>
Motivation: 解决人类心理预期与LLM生成内容之间的对齐问题，提升文本生成质量。

Method: 使用代理强化学习（LLMaaJ）教导LLM生成内容，并通过Hilbert向量空间优化内容特征（如事实性、新颖性等）。

Result: 人类预期与AI对齐率为53.8%，内容质量提升，覆盖更多网球动作。

Conclusion: 该方法在US Open 2024中成功应用，并推广至其他体育和娱乐活动。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [215] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
*Adarsh Kumar,Hwiyoon Kim,Jawahar Sai Nathani,Neil Roy*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了如何通过结合Chain-of-Thought (CoT)提示、检索增强生成(RAG)、自一致性和自验证策略，减少大语言模型(LLMs)的幻觉问题，提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂开放任务中生成自信但错误或无关信息（幻觉）的问题限制了其应用。CoT提示虽能改进多步推理，但未能完全解决幻觉问题。

Method: 结合CoT与RAG，并应用自一致性和自验证策略，通过引入外部知识源和模型自我验证机制，生成更准确连贯的响应。

Result: 比较了基线LLMs与CoT、CoT+RAG、自一致性和自验证技术的效果，结果表明这些方法能有效减少幻觉，同时保持流畅性和推理深度。

Conclusion: 综合使用CoT、RAG、自一致性和自验证是最稳健的方法，能显著减少幻觉并提高事实准确性。

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [216] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/abs/2505.09114)
*Minh Hoang Nguyen,Linh Le Pham Van,Thommen George Karimpanal,Sunil Gupta,Hung Le*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种基于反事实推理的决策变换器（CRDT），通过生成和利用反事实经验，提升在数据有限或动态变化场景中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，高质量离线数据稀缺，次优数据影响决策变换器（DT）性能，需改进其泛化能力。

Method: 提出CRDT框架，通过反事实推理生成额外经验，增强DT在未知场景中的决策能力。

Result: 在Atari和D4RL基准测试中，CRDT优于传统DT，尤其在数据有限或动态变化时表现更优。

Conclusion: 反事实推理能显著提升强化学习代理的性能和泛化能力，无需修改架构。

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [217] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/abs/2505.09289)
*Pedro M. P. Curvo,Mara Dragomir,Salvador Torpes,Mohammadmahdi Rahimi*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究评估并扩展了Piatti等人的GovSim框架，验证了大型语言模型（如GPT-4-turbo）在资源分享场景中的合作能力，并探讨了其在不同模型、场景和语言中的适应性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证大型语言模型在复杂合作任务中的性能，并扩展框架以测试其通用性。

Method: 通过复制关键实验并引入新设置（如异构多智能体环境和日语指令场景），评估模型的合作行为。

Result: 结果表明，大型模型（如GPT-4-turbo）在合作任务中表现优异，且能影响低性能模型的行为。

Conclusion: 该框架适用于新模型和场景，为合作AI系统的开发提供了重要见解。

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [218] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/abs/2505.09396)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了LLM代理在博弈论设置中的表现，比较了三种代理设计，发现人类启发式认知结构能提升LLM代理与人类战略行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代理是否能够复制人类的战略推理能力，特别是在博弈论环境中。

Method: 评估了三种代理设计：简单博弈论模型、非结构化LLM代理模型以及整合传统代理框架的LLM。使用猜测游戏作为测试平台，并与人类参与者对比。

Result: 人类启发式认知结构能提升LLM代理与人类战略行为的一致性，但代理设计复杂性与人类相似性之间的关系是非线性的。

Conclusion: LLM代理的能力限制了简单架构增强的效果，强调了底层LLM能力的关键作用。

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [219] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
*Anthony GX-Chen,Dongyan Lin,Mandana Samiei,Doina Precup,Blake A. Richards,Rob Fergus,Kenneth Marino*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文研究了语言模型（LM）代理在探索和推断因果关系时的能力，发现其存在“析取偏见”，并提出了一种测试时采样方法以减少这种偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨LM是否具备高效探索和理解世界因果结构的能力，以及是否存在系统性偏见导致错误结论。

Method: 使用发展心理学中的“Blicket Test”范式，评估不同LM家族、规模和提示策略下的表现。

Result: LM在推断常见析取因果关系时表现可靠，但在罕见合取因果关系时存在系统性困难，且任务复杂度增加时表现下降。

Conclusion: LM表现出类似成人的推理模式，提出了一种测试时采样方法以减少偏见，提升因果推理的严谨性。

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


### [220] [Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation](https://arxiv.org/abs/2505.09027)
*Yi Cui*

Main category: cs.SE

Relevance: 85.0

TL;DR: WebApp1K是一个新颖的基准测试，用于评估大型语言模型（LLMs）在测试驱动开发（TDD）任务中的表现，强调通过测试用例直接生成代码的能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖自然语言提示，而WebApp1K更贴近实际软件开发实践，通过测试用例评估LLMs的功能实现能力。

Method: 该基准包含1000个多样化挑战，覆盖20个应用领域，评估LLMs在上下文长度和多特征复杂性约束下生成紧凑功能代码的能力。

Result: 研究发现指令遵循和上下文学习对TDD成功至关重要，超过一般编码能力或预训练知识。对19个前沿模型的评估揭示了性能瓶颈，如长提示中的指令丢失。

Conclusion: WebApp1K强调了TDD特定基准的实际价值，为提升LLMs在严格应用驱动编码场景中的能力奠定了基础。

Abstract: We introduce WebApp1K, a novel benchmark for evaluating large language models
(LLMs) in test-driven development (TDD) tasks, where test cases serve as both
prompt and verification for code generation. Unlike traditional approaches
relying on natural language prompts, our benchmark emphasizes the ability of
LLMs to interpret and implement functionality directly from test cases,
reflecting real-world software development practices. Comprising 1000 diverse
challenges across 20 application domains, the benchmark evaluates LLMs on their
ability to generate compact, functional code under the constraints of context
length and multi-feature complexity. Our findings highlight instruction
following and in-context learning as critical capabilities for TDD success,
surpassing the importance of general coding proficiency or pretraining
knowledge. Through comprehensive evaluation of 19 frontier models, we reveal
performance bottlenecks, such as instruction loss in long prompts, and provide
a detailed error analysis spanning multiple root causes. This work underscores
the practical value of TDD-specific benchmarks and lays the foundation for
advancing LLM capabilities in rigorous, application-driven coding scenarios.

</details>


### [221] [ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor](https://arxiv.org/abs/2505.09142)
*Seungbeom Choi,Jeonghoe Goo,Eunjoo Jeon,Mingyu Yang,Minsung Jang*

Main category: cs.DC

Relevance: 85.0

TL;DR: ELIS是一个用于大型语言模型（LLM）的服务系统，采用迭代最短剩余时间优先（ISRTF）调度器，通过预测剩余令牌数优化推理任务调度，减少平均任务完成时间达19.6%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统采用先到先服务调度策略，易导致“队头阻塞”问题，需预测推理时间并应用最短作业优先策略。

Method: 训练基于BGE模型的响应长度预测器，设计ISRTF调度策略，并在Kubernetes上实现云原生调度系统。

Result: 实验表明，ISRTF将平均任务完成时间减少高达19.6%。

Conclusion: ELIS通过优化调度策略显著提升LLM推理效率。

Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring
an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to
efficiently manage inference tasks with the shortest remaining tokens. Current
LLM serving systems often employ a first-come-first-served scheduling strategy,
which can lead to the "head-of-line blocking" problem. To overcome this
limitation, it is necessary to predict LLM inference times and apply a shortest
job first scheduling strategy. However, due to the auto-regressive nature of
LLMs, predicting the inference latency is challenging. ELIS addresses this
challenge by training a response length predictor for LLMs using the BGE model,
an encoder-based state-of-the-art model. Additionally, we have devised the
ISRTF scheduling strategy, an optimization of shortest remaining time first
tailored to existing LLM iteration batching. To evaluate our work in an
industrial setting, we simulate streams of requests based on our study of
real-world user LLM serving trace records. Furthermore, we implemented ELIS as
a cloud-native scheduler system on Kubernetes to evaluate its performance in
production environments. Our experimental results demonstrate that ISRTF
reduces the average job completion time by up to 19.6%.

</details>


### [222] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343)
*Chenggang Zhao,Chengqi Deng,Chong Ruan,Damai Dai,Huazuo Gao,Jiashi Li,Liyue Zhang,Panpan Huang,Shangyan Zhou,Shirong Ma,Wenfeng Liang,Ying He,Yuqing Wang,Yuxuan Liu,Y. X. Wei*

Main category: cs.DC

Relevance: 85.0

TL;DR: DeepSeek-V3通过硬件与模型协同设计解决了LLM训练和推理中的硬件瓶颈，提出了多项创新技术，并探讨了未来硬件发展方向。


<details>
  <summary>Details</summary>
Motivation: 当前硬件架构在LLM扩展中存在内存、计算效率和互联带宽等限制，需要硬件与模型协同设计以解决这些问题。

Method: 采用Multi-head Latent Attention (MLA)、Mixture of Experts (MoE)、FP8混合精度训练和Multi-Plane Network Topology等技术优化模型架构和基础设施。

Result: DeepSeek-V3在2048个NVIDIA H800 GPU上实现了高效、低成本的训练和推理。

Conclusion: 硬件与模型协同设计对满足AI工作负载需求至关重要，为下一代AI系统提供了创新蓝图。

Abstract: The rapid scaling of large language models (LLMs) has unveiled critical
limitations in current hardware architectures, including constraints in memory
capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,
trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model
co-design can effectively address these challenges, enabling cost-efficient
training and inference at scale. This paper presents an in-depth analysis of
the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting
key innovations such as Multi-head Latent Attention (MLA) for enhanced memory
efficiency, Mixture of Experts (MoE) architectures for optimized
computation-communication trade-offs, FP8 mixed-precision training to unlock
the full potential of hardware capabilities, and a Multi-Plane Network Topology
to minimize cluster-level network overhead. Building on the hardware
bottlenecks encountered during DeepSeek-V3's development, we engage in a
broader discussion with academic and industry peers on potential future
hardware directions, including precise low-precision computation units,
scale-up and scale-out convergence, and innovations in low-latency
communication fabrics. These insights underscore the critical role of hardware
and model co-design in meeting the escalating demands of AI workloads, offering
a practical blueprint for innovation in next-generation AI systems.

</details>


### [223] [Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach](https://arxiv.org/abs/2505.09576)
*Shannon Lodoen,Alexi Orchard*

Main category: cs.CY

Relevance: 85.0

TL;DR: 论文探讨了RLHF技术如何通过人类反馈优化LLM输出，并分析了其对社会伦理、透明度和人际关系的影响。


<details>
  <summary>Details</summary>
Motivation: 研究RLHF技术如何使LLM输出更“人性化”，并揭示其潜在的伦理和社会问题。

Method: 采用修辞分析方法，聚焦RLHF对语言惯例、信息获取和社交关系的重塑。

Result: 揭示了RLHF可能强化霸权语言使用、偏见和学习去语境化的问题。

Conclusion: 提出AI伦理研究的新方向，关注技术对社会关系的潜在影响。

Abstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude
have been trained using a specialized technique called Reinforcement Learning
from Human Feedback (RLHF) to fine-tune language model output using feedback
from human annotators. As a result, the integration of RLHF has greatly
enhanced the outputs of these large language models (LLMs) and made the
interactions and responses appear more "human-like" than those of previous
versions using only supervised learning. The increasing convergence of human
and machine-written text has potentially severe ethical, sociotechnical, and
pedagogical implications relating to transparency, trust, bias, and
interpersonal relations. To highlight these implications, this paper presents a
rhetorical analysis of some of the central procedures and processes currently
being reshaped by RLHF-enhanced generative AI chatbots: upholding language
conventions, information seeking practices, and expectations for social
relationships. Rhetorical investigations of generative AI and LLMs have, to
this point, focused largely on the persuasiveness of the content generated.
Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site
of rhetorical investigation from content analysis to the underlying mechanisms
of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical
investigation opens a new direction for further inquiry in AI ethics that
considers how procedures rerouted through AI-driven technologies might
reinforce hegemonic language use, perpetuate biases, decontextualize learning,
and encroach upon human relationships. It will therefore be of interest to
educators, researchers, scholars, and the growing number of users of generative
AI chatbots.

</details>


### [224] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/abs/2505.08988)
*Montaser Mohammedalamen,Michael Bowling*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文研究了在奖励不可观测的Mon-MDP中使用函数逼近（FA）的挑战，提出了一种结合奖励模型的方法，使智能体能够从未观测奖励的状态中泛化，并提出了缓解过度泛化的优化方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中奖励并非总是可观测，而现有的Mon-MDP研究局限于简单场景，无法应用于实际问题。本文旨在通过函数逼近和奖励模型扩展Mon-MDP的适用性。

Method: 结合函数逼近和奖励模型，使智能体从可观测奖励的状态泛化到不可观测奖励的状态，并提出基于奖励不确定性的谨慎策略优化方法。

Result: 该方法在理论上无法解决的环境中实现了接近最优的策略，但发现函数逼近可能导致过度泛化问题。

Conclusion: 通过奖励模型和谨慎优化方法，本文为Mon-MDP理论在现实中的应用提供了初步解决方案。

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [225] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/abs/2505.09029)
*Hazim Alzorgan,Abolfazl Razi*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出了一种结合光束搜索和蒙特卡洛模拟的新方法MCBS，用于改进TD3算法的探索和动作选择，在多个连续控制任务中表现出更高的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于噪声的探索方法（如TD3）可能导致策略收敛不理想，因此需要更高效的探索和动作选择方法。

Method: MCBS结合光束搜索和蒙特卡洛模拟，生成候选动作并通过短时域模拟评估，优化动作选择。

Result: 在多个基准任务中，MCBS比TD3、SAC、PPO和A2C表现更好，收敛速度更快（如200千步达到90%最大奖励）。

Conclusion: MCBS通过结构化前瞻搜索提升了策略学习效率，同时保持了计算效率，适用于复杂控制任务。

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [226] [Federated Large Language Models: Feasibility, Robustness, Security and Future Directions](https://arxiv.org/abs/2505.08830)
*Wenhao Jiang,Yuchuan Luo,Guilin Deng,Silong Chen,Xu Yang,Shihong Wu,Xinwen Gao,Lin Liu,Shaojing Fu*

Main category: cs.CR

Relevance: 75.0

TL;DR: 本文综述了联邦大语言模型（FLLM）的最新进展，探讨了可行性、鲁棒性、安全性及未来方向，强调需进一步研究以提升系统鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式数据联合训练中的隐私保护和数据孤岛问题，同时应对FLLM面临的通信、计算、异构性和安全挑战。

Method: 通过文献综述，从可行性、鲁棒性、安全性和未来方向四个角度分析FLLM的挑战与解决方案。

Result: 总结了现有FLLM可行性研究，提出了增强鲁棒性的方法，分析了隐私与安全风险，并探讨了防御机制和未来研究方向。

Conclusion: FLLM领域亟需进一步研究以应对FL与LLM整合带来的独特挑战，特别是在鲁棒性和安全性方面。

Abstract: The integration of Large Language Models (LLMs) and Federated Learning (FL)
presents a promising solution for joint training on distributed data while
preserving privacy and addressing data silo issues. However, this emerging
field, known as Federated Large Language Models (FLLM), faces significant
challenges, including communication and computation overheads, heterogeneity,
privacy and security concerns. Current research has primarily focused on the
feasibility of FLLM, but future trends are expected to emphasize enhancing
system robustness and security. This paper provides a comprehensive review of
the latest advancements in FLLM, examining challenges from four critical
perspectives: feasibility, robustness, security, and future directions. We
present an exhaustive survey of existing studies on FLLM feasibility, introduce
methods to enhance robustness in the face of resource, data, and task
heterogeneity, and analyze novel risks associated with this integration,
including privacy threats and security challenges. We also review the latest
developments in defense mechanisms and explore promising future research
directions, such as few-shot learning, machine unlearning, and IP protection.
This survey highlights the pressing need for further research to enhance system
robustness and security while addressing the unique challenges posed by the
integration of FL and LLM.

</details>


### [227] [Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation](https://arxiv.org/abs/2505.09027)
*Yi Cui*

Main category: cs.SE

Relevance: 75.0

TL;DR: WebApp1K是一个用于评估大语言模型（LLMs）在测试驱动开发（TDD）任务中的新基准，强调从测试用例直接生成代码的能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖自然语言提示，而WebApp1K更贴近实际软件开发实践，通过测试用例作为提示和验证，评估LLMs的能力。

Method: 该基准包含1000个跨20个应用领域的多样化挑战，评估LLMs在上下文长度和多特征复杂性约束下生成紧凑、功能代码的能力。

Result: 研究发现，指令遵循和上下文学习对TDD成功至关重要，超过一般编码能力或预训练知识。通过对19个前沿模型的评估，揭示了性能瓶颈（如长提示中的指令丢失）和详细错误分析。

Conclusion: WebApp1K强调了TDD特定基准的实用价值，为提升LLMs在严格、应用驱动的编码场景中的能力奠定了基础。

Abstract: We introduce WebApp1K, a novel benchmark for evaluating large language models
(LLMs) in test-driven development (TDD) tasks, where test cases serve as both
prompt and verification for code generation. Unlike traditional approaches
relying on natural language prompts, our benchmark emphasizes the ability of
LLMs to interpret and implement functionality directly from test cases,
reflecting real-world software development practices. Comprising 1000 diverse
challenges across 20 application domains, the benchmark evaluates LLMs on their
ability to generate compact, functional code under the constraints of context
length and multi-feature complexity. Our findings highlight instruction
following and in-context learning as critical capabilities for TDD success,
surpassing the importance of general coding proficiency or pretraining
knowledge. Through comprehensive evaluation of 19 frontier models, we reveal
performance bottlenecks, such as instruction loss in long prompts, and provide
a detailed error analysis spanning multiple root causes. This work underscores
the practical value of TDD-specific benchmarks and lays the foundation for
advancing LLM capabilities in rigorous, application-driven coding scenarios.

</details>


### [228] [SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation](https://arxiv.org/abs/2505.09081)
*Gaurav Koley*

Main category: cs.SI

Relevance: 75.0

TL;DR: SALM框架将语言模型（LMs）整合到社交网络模拟中，实现了多智能体场景下的时间稳定性，并通过分层提示架构、注意力记忆系统和人格稳定性界限提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的代理建模方法难以捕捉社交系统的细微动态，SALM通过LMs的上下文理解能力解决了这一问题。

Method: SALM采用分层提示架构减少token使用，注意力记忆系统优化内存效率，并引入人格稳定性界限。

Result: 在SNAP社交网络上验证，SALM实现了4000步以上的稳定模拟，token使用减少73%，内存增长仅9.5%。

Conclusion: SALM是首个能模拟长期社交现象并保持行为保真度的LLM框架。

Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have
traditionally emphasized rule-based behaviors, limiting their ability to
capture nuanced dynamics by moving beyond predefined rules and leveraging
contextual understanding from LMs of human social interaction. This paper
presents SALM (Social Agent LM Framework), a novel approach for integrating
language models (LMs) into social network simulation that achieves
unprecedented temporal stability in multi-agent scenarios. Our primary
contributions include: (1) a hierarchical prompting architecture enabling
stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)
an attention-based memory system achieving 80% cache hit rates (95% CI [78%,
82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on
personality stability. Through extensive validation against SNAP ego networks,
we demonstrate the first LLM-based framework capable of modeling long-term
social phenomena while maintaining empirically validated behavioral fidelity.

</details>


### [229] [Toward Fair Federated Learning under Demographic Disparities and Data Imbalance](https://arxiv.org/abs/2505.09295)
*Qiming Wu,Siqi Li,Doudou Zhou,Nan Liu*

Main category: cs.CY

Relevance: 75.0

TL;DR: FedIDA是一个联邦学习框架，专注于解决数据不平衡和算法偏见问题，通过公平感知正则化和组条件过采样，提升多敏感属性下的公平性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI模型可能因数据不平衡和人口统计偏差加剧现有不平等。联邦学习虽保护隐私，但仍面临算法偏见和子群不平衡的挑战。

Method: 提出FedIDA，结合公平感知正则化和组条件过采样，支持多敏感属性和异构数据分布，不影响联邦学习的收敛性。

Result: 理论分析证明FedIDA能改善公平性，实验结果显示其在基准和真实临床数据集上均提升公平性且保持预测性能。

Conclusion: FedIDA为医疗领域提供了公平且隐私保护的建模方法，代码已开源。

Abstract: Ensuring fairness is critical when applying artificial intelligence to
high-stakes domains such as healthcare, where predictive models trained on
imbalanced and demographically skewed data risk exacerbating existing
disparities. Federated learning (FL) enables privacy-preserving collaboration
across institutions, but remains vulnerable to both algorithmic bias and
subgroup imbalance - particularly when multiple sensitive attributes intersect.
We propose FedIDA (Fed erated Learning for Imbalance and D isparity A
wareness), a framework-agnostic method that combines fairness-aware
regularization with group-conditional oversampling. FedIDA supports multiple
sensitive attributes and heterogeneous data distributions without altering the
convergence behavior of the underlying FL algorithm. We provide theoretical
analysis establishing fairness improvement bounds using Lipschitz continuity
and concentration inequalities, and show that FedIDA reduces the variance of
fairness metrics across test sets. Empirical results on both benchmark and
real-world clinical datasets confirm that FedIDA consistently improves fairness
while maintaining competitive predictive performance, demonstrating its
effectiveness for equitable and privacy-preserving modeling in healthcare. The
source code is available on GitHub.

</details>


### [230] [Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment](https://arxiv.org/abs/2505.09438)
*Paul Tschisgale,Holger Maus,Fabian Kieser,Ben Kroehs,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

Relevance: 75.0

TL;DR: 研究比较了通用LLM（GPT-4o）和推理优化模型（o1-preview）在物理奥林匹克问题上的表现，发现它们平均优于人类参与者，并讨论了教育评估的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在物理问题解决中的能力，以指导其在教育和评估中的负责任应用。

Method: 通过德国物理奥林匹克问题测试GPT-4o和o1-preview的性能，分析其解决方案的正确性和特点。

Result: 两种LLM在物理问题上表现优于人类，o1-preview尤其突出，提示技术对GPT-4o影响较小。

Conclusion: 研究为物理教育评估设计提供了启示，强调维护评估完整性和学生批判性使用LLM的重要性。

Abstract: Large language models (LLMs) are now widely accessible, reaching learners at
all educational levels. This development has raised concerns that their use may
circumvent essential learning processes and compromise the integrity of
established assessment formats. In physics education, where problem solving
plays a central role in instruction and assessment, it is therefore essential
to understand the physics-specific problem-solving capabilities of LLMs. Such
understanding is key to informing responsible and pedagogically sound
approaches to integrating LLMs into instruction and assessment. This study
therefore compares the problem-solving performance of a general-purpose LLM
(GPT-4o, using varying prompting techniques) and a reasoning-optimized model
(o1-preview) with that of participants of the German Physics Olympiad, based on
a set of well-defined Olympiad problems. In addition to evaluating the
correctness of the generated solutions, the study analyzes characteristic
strengths and limitations of LLM-generated solutions. The findings of this
study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate
advanced problem-solving capabilities on Olympiad-type physics problems, on
average outperforming the human participants. Prompting techniques had little
effect on GPT-4o's performance, while o1-preview almost consistently
outperformed both GPT-4o and the human benchmark. Based on these findings, the
study discusses implications for the design of summative and formative
assessment in physics education, including how to uphold assessment integrity
and support students in critically engaging with LLMs.

</details>


### [231] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/abs/2505.09558)
*Shengpeng Ji,Tianle Liang,Yangzhuo Li,Jialong Zuo,Minghui Fang,Jinzheng He,Yifu Chen,Zhengqing Liu,Ziyue Jiang,Xize Cheng,Siqi Zheng,Jin Xu,Junyang Lin,Zhou Zhao*

Main category: eess.AS

Relevance: 75.0

TL;DR: WavReward是一个基于音频语言模型的奖励反馈模型，用于评估语音对话系统的IQ和EQ表现，结合强化学习训练，显著提升了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 语音对话模型（如GPT-4o-audio）的评估因非文本信息的复杂性而被忽视，WavReward填补了这一空白。

Method: 基于音频语言模型，结合深度推理和非线性奖励机制，通过强化学习训练专用评估器；引入ChatReward-30K偏好数据集。

Result: WavReward在多个语音对话场景中表现优异，客观准确率从55.1%提升至91.5%，主观A/B测试领先83%。

Conclusion: WavReward为语音对话模型提供了高效评估工具，其组件设计均被验证为必要。

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


### [232] [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)
*Nidhal Jegham,Marwen Abdelatti,Lassad Elmoubarki,Abdeltawab Hendawi*

Main category: cs.CY

Relevance: 75.0

TL;DR: 该论文提出了一种基础设施感知的基准测试框架，用于量化商业数据中心中30种最先进LLM推理的环境足迹，揭示了模型在能源消耗和生态效率上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各行业的广泛应用，理解其在推理阶段的环境影响变得至关重要。现有研究多忽略专有模型或基础设施差异，而该研究填补了这一空白。

Method: 结合公共API性能数据、区域环境乘数及硬件配置统计推断，采用交叉效率数据包络分析（DEA）对模型进行生态效率排名。

Result: 研究发现o3和DeepSeek-R1能耗最高（33 Wh/长提示），而Claude-3.7 Sonnet生态效率最佳。GPT-4o单次短查询能耗0.43 Wh，但全球规模下环境影响巨大。

Conclusion: 研究为LLM部署的可持续性提供了标准化基准方法，为未来AI开发中的环境责任和可持续标准奠定基础。

Abstract: As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.

</details>


### [233] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/abs/2505.08988)
*Montaser Mohammedalamen,Michael Bowling*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该论文研究了在奖励不完全可观测的监控马尔可夫决策过程（Mon-MDP）中，如何利用函数逼近（FA）和奖励模型实现策略泛化，并提出了一种基于奖励不确定性的谨慎策略优化方法以缓解过度泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中奖励往往不完全可观测，而现有Mon-MDP研究局限于简单表格化情况，限制了其实际应用。本文旨在探索Mon-MDP在函数逼近下的挑战与解决方案。

Method: 结合函数逼近与奖励模型，从可观测奖励的状态泛化到不可观测奖励的状态，并提出基于奖励不确定性的谨慎策略优化方法。

Result: 研究表明，奖励模型能帮助策略在理论上不可解的环境中实现接近最优的表现，但也揭示了函数逼近可能导致的过度泛化问题。

Conclusion: 本文为Mon-MDP理论与实际应用的结合提供了初步探索，并提出了缓解过度泛化的方法。

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [234] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/abs/2505.09029)
*Hazim Alzorgan,Abolfazl Razi*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出了一种结合蒙特卡洛波束搜索（MCBS）与TD3的新方法，以改进强化学习中的探索和动作选择，在多个连续控制任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于噪声的探索方法（如TD3）可能导致策略收敛不理想，因此需要更高效的探索和动作选择方法。

Method: MCBS结合波束搜索和蒙特卡洛展开，生成候选动作并通过短视界评估，提升动作选择质量。

Result: 在多个基准测试中，MCBS的样本效率和性能优于TD3、SAC、PPO和A2C，收敛速度更快。

Conclusion: MCBS通过结构化前瞻搜索提升策略学习，同时保持计算效率，适用于复杂控制任务。

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [235] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/abs/2505.09341)
*Evžen Wybitul*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出了一种基于用户凭证和风险分类的AI安全框架，通过小规模专家模块实现高效风险检测，解决AI的双重用途困境。


<details>
  <summary>Details</summary>
Motivation: AI安全系统面临双重用途困境，即同一请求可能无害或有害，仅基于内容决策会导致误判。

Method: 提出基于验证用户凭证和风险分类的访问控制框架，使用小规模专家模块进行风险检测。

Result: 框架实现了对AI能力的细粒度治理，平衡模型效用与安全性。

Conclusion: 该框架为解决AI双重用途困境提供了初步解决方案，但仍需进一步研究验证机制和技术实现。

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [236] [Federated Large Language Models: Feasibility, Robustness, Security and Future Directions](https://arxiv.org/abs/2505.08830)
*Wenhao Jiang,Yuchuan Luo,Guilin Deng,Silong Chen,Xu Yang,Shihong Wu,Xinwen Gao,Lin Liu,Shaojing Fu*

Main category: cs.CR

Relevance: 75.0

TL;DR: 该论文综述了联邦大语言模型（FLLM）的最新进展，探讨了可行性、鲁棒性、安全性及未来方向，强调需进一步研究以提升系统鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式数据联合训练中的隐私保护和数据孤岛问题，同时应对FLLM面临的通信、计算、异构性和安全挑战。

Method: 通过文献综述，从可行性、鲁棒性、安全性和未来方向四个角度分析FLLM的挑战与解决方案。

Result: 总结了现有FLLM研究的可行性方法，提出了增强鲁棒性和安全性的技术，并探讨了未来研究方向。

Conclusion: FLLM领域需更多研究以应对FL与LLM整合的独特挑战，尤其是鲁棒性和安全性。

Abstract: The integration of Large Language Models (LLMs) and Federated Learning (FL)
presents a promising solution for joint training on distributed data while
preserving privacy and addressing data silo issues. However, this emerging
field, known as Federated Large Language Models (FLLM), faces significant
challenges, including communication and computation overheads, heterogeneity,
privacy and security concerns. Current research has primarily focused on the
feasibility of FLLM, but future trends are expected to emphasize enhancing
system robustness and security. This paper provides a comprehensive review of
the latest advancements in FLLM, examining challenges from four critical
perspectives: feasibility, robustness, security, and future directions. We
present an exhaustive survey of existing studies on FLLM feasibility, introduce
methods to enhance robustness in the face of resource, data, and task
heterogeneity, and analyze novel risks associated with this integration,
including privacy threats and security challenges. We also review the latest
developments in defense mechanisms and explore promising future research
directions, such as few-shot learning, machine unlearning, and IP protection.
This survey highlights the pressing need for further research to enhance system
robustness and security while addressing the unique challenges posed by the
integration of FL and LLM.

</details>


### [237] [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Haim Permuter,Flavio P. Calmon*

Main category: cs.CR

Relevance: 75.0

TL;DR: 本文分析了在单次设置下的大语言模型（LLM）文本水印技术，探讨了水印检测能力与生成文本质量之间的权衡，并提出了一种最优耦合和随机化策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成的文本越来越接近人类水平，水印技术成为区分AI生成内容的重要手段。本文旨在优化水印设计，平衡检测能力与文本质量。

Method: 通过假设检验和侧信息分析，提出了一种基于LLM词汇表随机分区的最优耦合策略，并在最小熵约束下进行了理论分析。

Result: 给出了检测率的闭式表达式，并通过数值实验验证了所提方案在合成数据和实际LLM水印中的优越性。

Conclusion: 本文为LLM文本水印提供了一种理论最优的设计框架，并在实践中验证了其有效性。

Abstract: Large-language models (LLMs) are now able to produce text that is, in many
cases, seemingly indistinguishable from human-generated content. This has
fueled the development of watermarks that imprint a ``signal'' in LLM-generated
text with minimal perturbation of an LLM's output. This paper provides an
analysis of text watermarking in a one-shot setting. Through the lens of
hypothesis testing with side information, we formulate and analyze the
fundamental trade-off between watermark detection power and distortion in
generated textual quality. We argue that a key component in watermark design is
generating a coupling between the side information shared with the watermark
detector and a random partition of the LLM vocabulary. Our analysis identifies
the optimal coupling and randomization strategy under the worst-case LLM
next-token distribution that satisfies a min-entropy constraint. We provide a
closed-form expression of the resulting detection rate under the proposed
scheme and quantify the cost in a max-min sense. Finally, we provide an array
of numerical results, comparing the proposed scheme with the theoretical
optimum and existing schemes, in both synthetic data and LLM watermarking. Our
code is available at https://github.com/Carol-Long/CC_Watermark

</details>


### [238] [SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation](https://arxiv.org/abs/2505.09081)
*Gaurav Koley*

Main category: cs.SI

Relevance: 75.0

TL;DR: SALM框架将语言模型（LMs）整合到社交网络模拟中，实现了多智能体场景的时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的ABM方法难以捕捉社交互动的细微动态，SALM通过LMs提升模拟的真实性和稳定性。

Method: 采用分层提示架构、基于注意力的记忆系统，并定义了人格稳定性界限。

Result: 在SNAP ego网络上验证，SALM能稳定模拟超过4000时间步，减少73%的token使用，记忆系统缓存命中率达80%。

Conclusion: SALM是首个能模拟长期社交现象并保持行为保真度的LLM框架。

Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have
traditionally emphasized rule-based behaviors, limiting their ability to
capture nuanced dynamics by moving beyond predefined rules and leveraging
contextual understanding from LMs of human social interaction. This paper
presents SALM (Social Agent LM Framework), a novel approach for integrating
language models (LMs) into social network simulation that achieves
unprecedented temporal stability in multi-agent scenarios. Our primary
contributions include: (1) a hierarchical prompting architecture enabling
stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)
an attention-based memory system achieving 80% cache hit rates (95% CI [78%,
82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on
personality stability. Through extensive validation against SNAP ego networks,
we demonstrate the first LLM-based framework capable of modeling long-term
social phenomena while maintaining empirically validated behavioral fidelity.

</details>


### [239] [Toward Fair Federated Learning under Demographic Disparities and Data Imbalance](https://arxiv.org/abs/2505.09295)
*Qiming Wu,Siqi Li,Doudou Zhou,Nan Liu*

Main category: cs.CY

Relevance: 75.0

TL;DR: FedIDA是一种联邦学习方法，通过公平感知正则化和组条件过采样解决数据不平衡和偏见问题，适用于多敏感属性和异构数据分布，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI模型可能因数据不平衡和人口统计偏差加剧现有不公平。联邦学习虽保护隐私，但仍面临算法偏见和子群不平衡问题。

Method: 结合公平感知正则化和组条件过采样，支持多敏感属性和异构数据分布，不影响联邦学习算法的收敛性。

Result: 理论分析证明公平性改进界限，实验显示FedIDA在公平性指标上降低方差，同时保持预测性能。

Conclusion: FedIDA在医疗领域实现了公平且隐私保护的建模，代码已开源。

Abstract: Ensuring fairness is critical when applying artificial intelligence to
high-stakes domains such as healthcare, where predictive models trained on
imbalanced and demographically skewed data risk exacerbating existing
disparities. Federated learning (FL) enables privacy-preserving collaboration
across institutions, but remains vulnerable to both algorithmic bias and
subgroup imbalance - particularly when multiple sensitive attributes intersect.
We propose FedIDA (Fed erated Learning for Imbalance and D isparity A
wareness), a framework-agnostic method that combines fairness-aware
regularization with group-conditional oversampling. FedIDA supports multiple
sensitive attributes and heterogeneous data distributions without altering the
convergence behavior of the underlying FL algorithm. We provide theoretical
analysis establishing fairness improvement bounds using Lipschitz continuity
and concentration inequalities, and show that FedIDA reduces the variance of
fairness metrics across test sets. Empirical results on both benchmark and
real-world clinical datasets confirm that FedIDA consistently improves fairness
while maintaining competitive predictive performance, demonstrating its
effectiveness for equitable and privacy-preserving modeling in healthcare. The
source code is available on GitHub.

</details>


### [240] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/abs/2505.09558)
*Shengpeng Ji,Tianle Liang,Yangzhuo Li,Jialong Zuo,Minghui Fang,Jinzheng He,Yifu Chen,Zhengqing Liu,Ziyue Jiang,Xize Cheng,Siqi Zheng,Jin Xu,Junyang Lin,Zhou Zhao*

Main category: eess.AS

Relevance: 75.0

TL;DR: WavReward是一个基于音频语言模型的奖励反馈模型，用于评估语音对话系统的IQ和EQ表现，通过强化学习和多样本反馈提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 目前语音对话模型的评估主要依赖文本语言模型，忽略了非文本信息的复杂性，WavReward填补了这一空白。

Method: 1) 基于音频语言模型设计非线性奖励机制；2) 引入ChatReward-30K数据集训练模型。

Result: WavReward在多个场景下显著优于现有评估模型，客观准确率从55.1%提升至91.5%，主观A/B测试领先83%。

Conclusion: WavReward为语音对话模型提供了高效的评估工具，验证了其组件的必要性。

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


### [241] [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)
*Nidhal Jegham,Marwen Abdelatti,Lassad Elmoubarki,Abdeltawab Hendawi*

Main category: cs.CY

Relevance: 75.0

TL;DR: 该论文提出了一种基础设施感知的基准测试框架，用于量化商业数据中心中30种最先进LLM推理的环境足迹，揭示了不同模型在能源消耗和生态效率上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各行业的广泛应用，量化其推理阶段的环境影响变得至关重要，但现有研究多忽略专有模型或基础设施差异，本文旨在填补这一空白。

Method: 结合公共API性能数据、区域特定环境乘数、硬件配置统计推断，并采用交叉效率数据包络分析（DEA）对模型进行排名。

Result: 发现某些模型（如o3和DeepSeek-R1）能源消耗极高，而Claude-3.7 Sonnet生态效率最佳；大规模查询的环境影响显著。

Conclusion: 研究为LLM部署的可持续性提供了标准化基准方法，为未来AI环境责任和可持续发展标准奠定基础。

Abstract: As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.

</details>


### [242] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
*Lucas McCullum,Pelagie Ami Agassi,Leo Anthony Celi,Daniel K. Ebner,Chrystinne Oliveira Fernandes,Rachel S. Hicklen,Mkliwa Koumbia,Lisa Soleymani Lehmann,David Restrepo*

Main category: cs.HC

Relevance: 70.0

TL;DR: 论文呼吁停止将LLM与人类专家直接比较，转而研究如何安全地将LLM整合到临床环境中，促进人机协作。


<details>
  <summary>Details</summary>
Motivation: 当前研究倾向于将LLM与人类专家比较，但缺乏明确的专家定义，且LLM的快速更新可能威胁患者安全。因此，需要研究如何安全使用LLM。

Method: 提出应开发策略，使人类与LLM以近乎共生的方式高效协作，而非直接比较。

Result: 强调了在临床环境中安全使用LLM的重要性，并呼吁研究方向的转变。

Conclusion: 未来的研究应聚焦于LLM的安全整合策略，而非简单的性能比较。

Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.

</details>


### [243] [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
*Derian Boer,Stephen Roth,Stefan Kramer*

Main category: cs.IR

Relevance: 70.0

TL;DR: FocusedRetriever是一个基于半结构化知识库（SKB）的多跳问答框架，结合了LLM的能力和结构化知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型和交互系统在处理结构化与非结构化知识时的局限性，通过SKB桥接两者，提升知识访问和利用效率。

Method: 整合VSS实体搜索、LLM生成Cypher查询和成对重排序，利用LLM提取关系事实和实体属性，通过节点集连接和向量相似性搜索过滤候选答案。

Result: 在STaRK基准测试中，平均首次命中率比次优方法高25.7%。

Conclusion: FocusedRetriever展示了LLM与结构化知识结合的潜力，未来可通过微调进一步优化。

Abstract: In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .

</details>


### [244] [Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities](https://arxiv.org/abs/2505.09477)
*Zachary Ravichandran,Fernando Cladera,Jason Hughes,Varun Murali,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

Relevance: 70.0

TL;DR: 论文探讨了在大型无结构环境中部署基于基础模型（FM）的机器人，提出了SPINE框架，支持LLM驱动的机器人规划，并展示了在无人机上的应用。


<details>
  <summary>Details</summary>
Motivation: 现有FM机器人主要在封闭环境中运行，而实际任务需要机器人在大规模无结构环境中操作，因此需要解决探索、导航、计算限制等问题。

Method: 提出SPINE框架，支持LLM驱动的机器人规划，并通过模型蒸馏实现小型语言模型在资源受限平台上的运行。

Result: 展示了SPINE在数公里任务中的首次大规模LLM机器人规划应用，并实现了首个基于设备端语言模型的无人机规划器。

Conclusion: 提出了未来研究方向，强调了在无结构环境中LLM机器人规划的潜力。

Abstract: The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.

</details>


### [245] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
*Lucas McCullum,Pelagie Ami Agassi,Leo Anthony Celi,Daniel K. Ebner,Chrystinne Oliveira Fernandes,Rachel S. Hicklen,Mkliwa Koumbia,Lisa Soleymani Lehmann,David Restrepo*

Main category: cs.HC

Relevance: 70.0

TL;DR: 论文呼吁停止将LLM与人类专家直接比较，转而研究如何安全地将LLM整合到临床环境中，以实现人机协作。


<details>
  <summary>Details</summary>
Motivation: 当前研究倾向于将LLM与人类专家比较，但缺乏明确的专家定义，且快速迭代的LLM可能威胁患者安全。因此，需要研究如何安全使用LLM。

Method: 提出需要开发策略，使人类与LLM能够高效协作，而非直接比较。

Result: 强调人机协作的重要性，而非对抗性比较。

Conclusion: 未来研究应聚焦于LLM在临床环境中的安全整合，促进人机共生。

Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.

</details>


### [246] [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
*Derian Boer,Stephen Roth,Stefan Kramer*

Main category: cs.IR

Relevance: 70.0

TL;DR: FocusedRetriever是一个基于半结构化知识库（SKB）的多跳问答框架，结合了结构化与非结构化数据，利用LLMs提取关系事实和实体属性，通过节点集连接和向量相似性搜索优化答案候选，最终在STaRK基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型和交互系统在结构化与非结构化数据之间难以有效结合的问题，提升多跳问答的性能。

Method: 1. 利用LLMs从非结构化文本中提取关系事实和实体属性；2. 通过节点集连接过滤答案候选；3. 使用向量相似性搜索检索和排序相关内容；4. 利用LLMs的上下文能力对答案进行最终排序。

Result: 在STaRK基准测试的三个测试集上均优于现有方法，平均首次命中率比第二名高25.7%。

Conclusion: FocusedRetriever展示了结合结构化与非结构化数据的潜力，并为进一步优化（如微调）提供了机会。

Abstract: In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .

</details>


### [247] [AI-Mediated Code Comment Improvement](https://arxiv.org/abs/2505.09021)
*Maria Dhakal,Chia-Yi Su,Robert Wallace,Chris Fakhimi,Aakash Bansal,Toby Li,Yu Huang,Collin McMillan*

Main category: cs.SE

Relevance: 70.0

TL;DR: 本文提出了一种基于AI的工具，利用LLM（如GPT-4）改进代码注释质量，并通过蒸馏技术实现本地化运行。


<details>
  <summary>Details</summary>
Motivation: 代码注释质量对软件开发至关重要，但现有注释往往不足或不准确。本文旨在通过AI工具提升注释质量。

Method: 结合实证研究和定性分析确定质量改进方向，使用LLM（GPT-4）重写注释，并蒸馏为小模型供本地使用。

Result: 实验表明，该方法显著提升了代码注释质量，且蒸馏模型效果接近原始LLM。

Conclusion: AI工具可有效改进代码注释质量，蒸馏技术解决了数据隐私问题。

Abstract: This paper describes an approach to improve code comments along different
quality axes by rewriting those comments with customized Artificial
Intelligence (AI)-based tools. We conduct an empirical study followed by
grounded theory qualitative analysis to determine the quality axes to improve.
Then we propose a procedure using a Large Language Model (LLM) to rewrite
existing code comments along the quality axes. We implement our procedure using
GPT-4o, then distil the results into a smaller model capable of being run
in-house, so users can maintain data custody. We evaluate both our approach
using GPT-4o and the distilled model versions. We show in an evaluation how our
procedure improves code comments along the quality axes. We release all data
and source code in an online repository for reproducibility.

</details>


### [248] [Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models](https://arxiv.org/abs/2505.09062)
*Junda Zhao,Yuliang Song,Eldan Cohen*

Main category: cs.SE

Relevance: 70.0

TL;DR: 论文提出了一种名为Variational Prefix Tuning (VPT)的新方法，通过结合条件变分自编码器（CVAE）框架，增强预训练模型生成多样化且准确的代码摘要的能力，同时避免昂贵的模型重训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为给定源代码生成单一高质量摘要，忽略了生成摘要可能不足且需要替代选项的场景。

Method: VPT将CVAE框架作为模块化组件集成到预训练模型中，通过采样连续嵌入作为前缀来引导多样化输出生成，并使用双标准重排方法优化摘要的多样性和准确性。

Result: 实验证明VPT在广泛使用的数据集和当前最先进的预训练代码摘要模型上有效且具有适应性。

Conclusion: VPT提供了一种参数高效的方法，能够生成多样化且准确的代码摘要选项，适用于实际应用。

Abstract: Recent advancements in source code summarization have leveraged
transformer-based pre-trained models, including Large Language Models of Code
(LLMCs), to automate and improve the generation of code summaries. However,
existing methods often focus on generating a single high-quality summary for a
given source code, neglecting scenarios where the generated summary might be
inadequate and alternative options are needed. In this paper, we introduce
Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained
models' ability to generate diverse yet accurate sets of summaries, allowing
the user to choose the most suitable one for the given source code. Our method
integrates a Conditional Variational Autoencoder (CVAE) framework as a modular
component into pre-trained models, enabling us to model the distribution of
observed target summaries and sample continuous embeddings to be used as
prefixes to steer the generation of diverse outputs during decoding.
Importantly, we construct our method in a parameter-efficient manner,
eliminating the need for expensive model retraining, especially when using
LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset
of generated summaries, optimizing both the diversity and the accuracy of the
options presented to users. We present extensive experimental evaluations using
widely used datasets and current state-of-the-art pre-trained code
summarization models to demonstrate the effectiveness of our approach and its
adaptability across models.

</details>


### [249] [Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment](https://arxiv.org/abs/2505.09438)
*Paul Tschisgale,Holger Maus,Fabian Kieser,Ben Kroehs,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

Relevance: 70.0

TL;DR: 研究比较了通用LLM（GPT-4o）和推理优化模型（o1-preview）在解决物理奥林匹克问题上的表现，发现两者均优于人类参与者，并探讨了其对物理教育评估的影响。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在物理问题解决中的能力，以指导其在教学和评估中的负责任应用。

Method: 通过比较GPT-4o和o1-preview在德国物理奥林匹克问题上的表现，分析其解题能力和局限性。

Result: 两种LLM在物理问题上表现优于人类，o1-preview尤其突出。提示技术对GPT-4o影响不大。

Conclusion: 研究为物理教育评估设计提供了建议，强调维护评估完整性和引导学生批判性使用LLM。

Abstract: Large language models (LLMs) are now widely accessible, reaching learners at
all educational levels. This development has raised concerns that their use may
circumvent essential learning processes and compromise the integrity of
established assessment formats. In physics education, where problem solving
plays a central role in instruction and assessment, it is therefore essential
to understand the physics-specific problem-solving capabilities of LLMs. Such
understanding is key to informing responsible and pedagogically sound
approaches to integrating LLMs into instruction and assessment. This study
therefore compares the problem-solving performance of a general-purpose LLM
(GPT-4o, using varying prompting techniques) and a reasoning-optimized model
(o1-preview) with that of participants of the German Physics Olympiad, based on
a set of well-defined Olympiad problems. In addition to evaluating the
correctness of the generated solutions, the study analyzes characteristic
strengths and limitations of LLM-generated solutions. The findings of this
study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate
advanced problem-solving capabilities on Olympiad-type physics problems, on
average outperforming the human participants. Prompting techniques had little
effect on GPT-4o's performance, while o1-preview almost consistently
outperformed both GPT-4o and the human benchmark. Based on these findings, the
study discusses implications for the design of summative and formative
assessment in physics education, including how to uphold assessment integrity
and support students in critically engaging with LLMs.

</details>


### [250] [Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities](https://arxiv.org/abs/2505.09477)
*Zachary Ravichandran,Fernando Cladera,Jason Hughes,Varun Murali,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

Relevance: 70.0

TL;DR: 论文探讨了如何将基础模型（FMs）应用于机器人在大规模非结构化环境中的自主任务，提出了SPINE框架，并展示了首个基于LLM的无人机规划器。


<details>
  <summary>Details</summary>
Motivation: 现有FM机器人主要在封闭环境中运行，而实际任务需要机器人在非结构化环境中自主探索和规划。

Method: 提出了SPINE框架，支持LLM驱动的机器人规划，并通过模型蒸馏实现小型语言模型在资源受限平台上的运行。

Result: 成功展示了首个基于LLM的大规模机器人规划系统，并实现了首个语言驱动的无人机规划器。

Conclusion: 论文提出了未来研究方向，强调了在非结构化环境中部署FM机器人的潜力。

Abstract: The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.

</details>


### [251] [Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models](https://arxiv.org/abs/2505.09062)
*Junda Zhao,Yuliang Song,Eldan Cohen*

Main category: cs.SE

Relevance: 65.0

TL;DR: 论文提出了一种名为Variational Prefix Tuning (VPT)的新方法，通过结合条件变分自编码器（CVAE）框架，增强预训练模型生成多样化且准确的代码摘要的能力，同时避免昂贵的模型重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为给定源代码生成单一高质量摘要，忽略了生成摘要可能不足且需要替代方案的情况。

Method: VPT将CVAE框架作为模块化组件集成到预训练模型中，建模目标摘要的分布并采样连续嵌入作为前缀，以在解码时引导多样化输出生成。此外，采用双标准重排序方法选择摘要子集，优化多样性和准确性。

Result: 实验评估表明，VPT在广泛使用的数据集和当前最先进的预训练代码摘要模型上表现出有效性及跨模型适应性。

Conclusion: VPT为代码摘要任务提供了一种高效且灵活的方法，能够生成多样化且准确的摘要选项。

Abstract: Recent advancements in source code summarization have leveraged
transformer-based pre-trained models, including Large Language Models of Code
(LLMCs), to automate and improve the generation of code summaries. However,
existing methods often focus on generating a single high-quality summary for a
given source code, neglecting scenarios where the generated summary might be
inadequate and alternative options are needed. In this paper, we introduce
Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained
models' ability to generate diverse yet accurate sets of summaries, allowing
the user to choose the most suitable one for the given source code. Our method
integrates a Conditional Variational Autoencoder (CVAE) framework as a modular
component into pre-trained models, enabling us to model the distribution of
observed target summaries and sample continuous embeddings to be used as
prefixes to steer the generation of diverse outputs during decoding.
Importantly, we construct our method in a parameter-efficient manner,
eliminating the need for expensive model retraining, especially when using
LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset
of generated summaries, optimizing both the diversity and the accuracy of the
options presented to users. We present extensive experimental evaluations using
widely used datasets and current state-of-the-art pre-trained code
summarization models to demonstrate the effectiveness of our approach and its
adaptability across models.

</details>


### [252] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/abs/2505.09412)
*Paul Kobialka,Lina Gerlach,Francesco Leofante,Erika Ábrahám,Silvia Lizeth Tapia Tarifa,Einar Broch Johnsen*

Main category: cs.AI

Relevance: 60.0

TL;DR: 该论文提出了针对马尔可夫决策过程（MDP）的反事实策略方法，填补了现有反事实方法在序列决策任务中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有反事实方法主要关注单步决策，无法直接应用于序列决策任务，因此需要一种适用于MDP的反事实策略方法。

Method: 通过非线性优化问题编码反事实策略，并扩展以合成多样化的反事实策略。

Result: 在四个真实数据集上验证了方法的实用性，证明了其在复杂序列决策任务中的可行性。

Conclusion: 该方法为序列决策任务中的反事实分析提供了有效工具。

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [253] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)
*Hiba Eltigani,Rukhshan Haroon,Asli Kocak,Abdullah Bin Faisal,Noah Martin,Fahad Dogar*

Main category: cs.HC

Relevance: 60.0

TL;DR: WaLLM是一个基于WhatsApp的AI聊天机器人，旨在解决发展中国家的数字鸿沟问题，提供信息查询和用户互动功能。


<details>
  <summary>Details</summary>
Motivation: 解决发展中国家因数字鸿沟导致的AI访问问题，通过WhatsApp这一广泛使用的平台提供定制化AI服务。

Method: 开发WaLLM，分析6个月的用户日志数据，研究用户互动模式。

Result: 55%的查询为事实信息，健康与福祉是最热门话题（28%），使用排行榜功能的用户互动频率高3倍。

Conclusion: 讨论了文化定制、用户界面设计和用户对AI信任度校准的重要性。

Abstract: Recent advances in generative AI, such as ChatGPT, have transformed access to
information in education, knowledge-seeking, and everyday decision-making.
However, in many developing regions, access remains a challenge due to the
persistent digital divide. To help bridge this gap, we developed WaLLM - a
custom AI chatbot over WhatsApp, a widely used communication platform in
developing regions. Beyond answering queries, WaLLM offers several features to
enhance user engagement: a daily top question, suggested follow-up questions,
trending and recent queries, and a leaderboard-based reward system. Our service
has been operational for over 6 months, amassing over 14.7K queries from
approximately 100 users. In this paper, we present WaLLM's design and a
systematic analysis of logs to understand user interactions. Our results show
that 55% of user queries seek factual information. "Health and well-being" was
the most popular topic (28%), including queries about nutrition and disease,
suggesting users view WaLLM as a reliable source. Two-thirds of users' activity
occurred within 24 hours of the daily top question. Users who accessed the
"Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by
discussing implications for culture-based customization, user interface design,
and appropriate calibration of users' trust in AI systems for developing
regions.

</details>


### [254] [AI-Mediated Code Comment Improvement](https://arxiv.org/abs/2505.09021)
*Maria Dhakal,Chia-Yi Su,Robert Wallace,Chris Fakhimi,Aakash Bansal,Toby Li,Yu Huang,Collin McMillan*

Main category: cs.SE

Relevance: 60.0

TL;DR: 论文提出了一种利用LLM（如GPT-4）改进代码注释质量的方法，并通过蒸馏技术实现本地化运行。


<details>
  <summary>Details</summary>
Motivation: 改进代码注释质量，同时确保数据隐私和可控性。

Method: 结合实证研究和定性分析确定质量维度，使用LLM（GPT-4）重写注释，并通过蒸馏技术生成轻量级模型。

Result: 方法显著提升了代码注释质量，并实现了本地化运行。

Conclusion: 该方法有效且可复现，适用于实际开发环境。

Abstract: This paper describes an approach to improve code comments along different
quality axes by rewriting those comments with customized Artificial
Intelligence (AI)-based tools. We conduct an empirical study followed by
grounded theory qualitative analysis to determine the quality axes to improve.
Then we propose a procedure using a Large Language Model (LLM) to rewrite
existing code comments along the quality axes. We implement our procedure using
GPT-4o, then distil the results into a smaller model capable of being run
in-house, so users can maintain data custody. We evaluate both our approach
using GPT-4o and the distilled model versions. We show in an evaluation how our
procedure improves code comments along the quality axes. We release all data
and source code in an online repository for reproducibility.

</details>


### [255] [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](https://arxiv.org/abs/2505.09108)
*Fernando Cladera,Zachary Ravichandran,Jason Hughes,Varun Murali,Carlos Nieto-Granda,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

Relevance: 60.0

TL;DR: 论文提出了一种基于大型语言模型（LLM）的规划系统，使无人机（UAV）和无人地面车（UGV）能够通过自然语言协作完成任务，并在任务中动态响应变化。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人系统的成熟，用户希望通过高级意图而非低层细节指定任务。语言是一种直观的表达方式，但实现语言引导的机器人团队面临技术挑战。

Method: 利用LLM驱动的规划器，在语义-度量地图上进行推理，地图由空中和地面机器人在线构建并共享。系统通过语义映射推断任务相关语义并主动获取信息。

Result: 在城乡任务驱动导航实验中，系统成功处理了七种不同的自然语言任务，并实现了公里级导航。

Conclusion: 该系统展示了LLM在机器人任务规划中的潜力，尤其是在动态环境和异构机器人协作中。

Abstract: As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.

</details>


### [256] [Learning Long-Context Diffusion Policies via Past-Token Prediction](https://arxiv.org/abs/2505.09561)
*Marcel Torne,Andy Tang,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

Relevance: 60.0

TL;DR: 提出了一种名为Past-Token Prediction (PTP)的辅助任务，通过预测过去的动作标记来改进长上下文策略的性能，并结合多阶段训练策略和自验证机制，显著提升了策略效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 长上下文策略学习面临内存需求高和性能下降的挑战，现有方法通常截断上下文，丢弃关键历史信息。本文旨在通过正则化和优化策略设计解决这些问题。

Method: 引入PTP辅助任务，结合多阶段训练策略（预训练视觉编码器后微调策略头），并在推理时扩展为自验证机制。

Result: 在四个真实世界和六个模拟任务中，方法将长上下文扩散策略性能提升3倍，训练速度加快10倍以上。

Conclusion: PTP和多阶段训练策略有效解决了长上下文策略学习中的挑战，显著提升了性能和效率。

Abstract: Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.

</details>


### [257] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/abs/2505.09412)
*Paul Kobialka,Lina Gerlach,Francesco Leofante,Erika Ábrahám,Silvia Lizeth Tapia Tarifa,Einar Broch Johnsen*

Main category: cs.AI

Relevance: 60.0

TL;DR: 该论文提出了针对马尔可夫决策过程（MDPs）的反事实策略方法，填补了现有反事实方法在序列决策任务中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有反事实方法主要关注单步决策，无法直接应用于序列决策任务，因此需要针对MDPs的反事实策略方法。

Method: 通过将反事实策略编码为非线优化问题，并扩展以生成多样化的反事实策略。

Result: 在四个真实数据集上验证了方法的实用性，展示了其在复杂序列决策任务中的可行性。

Conclusion: 该方法为序列决策任务中的反事实分析提供了有效工具。

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [258] [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://arxiv.org/abs/2505.08844)
*Jiawen Chen,Jianghao Zhang,Huaxiu Yao,Yun Li*

Main category: q-bio.GN

Relevance: 60.0

TL;DR: CellTypeAgent是一个基于大型语言模型（LLM）的代理工具，用于单细胞RNA测序分析中的细胞类型注释，通过整合数据库验证提高了准确性并减少了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序分析中的细胞类型注释是一个关键但耗时的步骤，现有方法存在准确性和幻觉问题。

Method: 结合大型语言模型（LLM）与数据库验证，开发了CellTypeAgent工具。

Result: 在9个真实数据集（涉及36个组织的303种细胞类型）上评估，CellTypeAgent的准确性优于现有方法。

Conclusion: CellTypeAgent为细胞类型注释提供了更高效和可靠的方法。

Abstract: Cell type annotation is a critical yet laborious step in single-cell RNA
sequencing analysis. We present a trustworthy large language model (LLM)-agent,
CellTypeAgent, which integrates LLMs with verification from relevant databases.
CellTypeAgent achieves higher accuracy than existing methods while mitigating
hallucinations. We evaluated CellTypeAgent across nine real datasets involving
303 cell types from 36 tissues. This combined approach holds promise for more
efficient and reliable cell type annotation.

</details>


### [259] [Learning Long-Context Diffusion Policies via Past-Token Prediction](https://arxiv.org/abs/2505.09561)
*Marcel Torne,Andy Tang,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

Relevance: 60.0

TL;DR: 提出了一种名为Past-Token Prediction (PTP)的辅助任务，用于改善长上下文扩散策略的性能，并通过多阶段训练策略显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文策略学习中因截断上下文导致的历史信息丢失问题，以及扩散策略中过去与未来动作依赖关系不足的挑战。

Method: 引入PTP辅助任务，预测过去动作令牌；采用多阶段训练策略，先预训练视觉编码器，再微调策略头；在测试时扩展PTP为自验证机制。

Result: 在四个真实世界和六个模拟任务中，性能提升3倍，训练速度加快10倍以上。

Conclusion: PTP和多阶段训练策略有效提升了长上下文扩散策略的性能和效率。

Abstract: Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.

</details>


### [260] [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
*Nicolas Dupuis,Ravi Nair,Shyam Ramji,Sean McClintock,Nishant Chauhan,Priyanka Nagpal,Bart Blaner,Ken Valk,Leon Stok,Ruchir Puri*

Main category: cs.SE

Relevance: 50.0

TL;DR: 该论文探讨了开发专门用于解释VHDL代码的LLM，通过扩展预训练和专家评估提高了模型性能，并提出了进一步改进硬件设计LLM的方法。


<details>
  <summary>Details</summary>
Motivation: VHDL在硬件设计中广泛使用，但LLM在该领域的应用较少，尤其是针对高性能处理器设计的独特需求。

Method: 通过扩展预训练（EPT）和专家评估开发专用LLM，并引入LLM-as-a-judge方法评估模型。

Result: EPT模型的专家评估评分从43%提升至69%，指令调优版本预期可达71%，未来可能达到85%。

Conclusion: 论文展示了硬件设计LLM的潜力，并提出了利用生成式AI新进展进一步改进的方向。

Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.

</details>


### [261] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

Relevance: 40.0

TL;DR: 该论文提出了一种基于深度强化学习（DRL）的信号交叉口纵向车辆控制策略，通过DDPG和SAC算法优化效率、安全和舒适性。


<details>
  <summary>Details</summary>
Motivation: 信号交叉口的车辆控制决策复杂，传统方法难以兼顾效率与安全，因此需要一种更智能的控制策略。

Method: 结合真实和模拟轨迹数据，使用DDPG和SAC算法训练模型，并通过CDF图和关键场景测试性能。

Result: RL模型在保持安全的同时，实现了更高的效率和更低的急动度，DDPG表现更平滑。

Conclusion: DRL策略能有效提升信号交叉口的交通安全性、效率和舒适性。

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [262] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08995)
*Ardian Selmonaj,Oleg Szehr,Giacomo Del Rio,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger*

Main category: cs.AI

Relevance: 40.0

TL;DR: 提出了一种分层多智能体强化学习框架，用于分析模拟空战场景，通过分层决策解决复杂动态和状态空间大的问题。


<details>
  <summary>Details</summary>
Motivation: 探索低成本、安全的模拟防御场景，解决多智能体系统中复杂飞行动态和状态空间大的挑战。

Method: 将决策分为低层（控制个体单位）和高层（指挥官策略），利用课程学习和预训练策略。

Result: 实证验证了框架的有效性，能够实现任务目标。

Conclusion: 分层框架在多智能体强化学习中具有优势，适用于复杂任务。

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [263] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/abs/2505.09518)
*Maris F. L. Galesloot,Roman Andriushchenko,Milan Češka,Sebastian Junges,Nils Jansen*

Main category: cs.AI

Relevance: 40.0

TL;DR: 该论文提出了一种在部分可观察马尔可夫决策过程（POMDPs）中计算鲁棒策略的方法，结合形式化验证和次梯度上升技术，以应对环境不确定性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决POMDPs中策略对环境扰动不鲁棒的问题，通过隐藏模型POMDPs（HM-POMDPs）建模多环境模型，确保策略在未知环境中的性能。

Method: 方法结合了形式化验证（计算最坏情况POMDP）和次梯度上升（优化策略），以生成鲁棒策略。

Result: 实验表明，该方法生成的策略比基线更鲁棒，且能推广到未见过的POMDPs，同时可扩展到包含数十万环境的HM-POMDPs。

Conclusion: 结论表明，该方法在鲁棒性和可扩展性方面优于现有技术，适用于复杂环境下的决策问题。

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [264] [A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science](https://arxiv.org/abs/2412.15404)
*Ahmet Yasin Aytar,Kemal Kilic,Kamer Kaya*

Main category: cs.IR

Relevance: 40.0

TL;DR: 该论文提出了一种增强的检索增强生成（RAG）应用，通过结合GROBID技术、微调嵌入模型和语义分块等方法，显著提高了学术文献检索的准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 解决数据科学领域学术文献导航的挑战，减少信息过载并提升决策效率。

Method: 结合GROBID技术提取文献信息，使用微调嵌入模型和语义分块，采用摘要优先检索方法。

Result: 通过RAGAS框架评估，系统在上下文相关性等关键指标上表现显著提升。

Conclusion: 增强的RAG系统有望改变数据科学领域的学术探索方式，推动研究和创新流程。

Abstract: In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.

</details>


### [265] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/abs/2505.08798)
*Mobina Shrestha,Bishwas Mandal,Vishal Mandal,Asis Shrestha*

Main category: eess.IV

Relevance: 40.0

TL;DR: 论文研究了在肿瘤学中应用上下文学习（ICL）作为模型重新训练的替代方案，通过少量标注样本实现任务适应，并在多个视觉语言模型（VLMs）上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI在肿瘤学中依赖大型标注数据集和需要重新训练的问题，探索ICL的实用性。

Method: 使用四种VLMs（Paligemma、CLIP、ALIGN、GPT-4o）在三个肿瘤学数据集（MHIST、PatchCamelyon、HAM10000）上评估ICL性能。

Result: 所有模型在少量样本提示下表现显著提升，GPT-4o在二分类和多分类任务中分别达到F1分数0.81和0.60。开源模型（如Paligemma和CLIP）在计算受限环境中表现竞争性。

Conclusion: ICL在肿瘤学中具有潜力，特别是在资源有限或罕见癌症场景下。

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [266] [Security of Internet of Agents: Attacks and Countermeasures](https://arxiv.org/abs/2505.08807)
*Yuntao Wang,Yanghe Pan,Shaolong Guo,Zhou Su*

Main category: cs.CR

Relevance: 40.0

TL;DR: 该论文综述了物联网代理（IoA）系统的安全与隐私问题，分析了其架构、漏洞及防御机制，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言和视觉语言模型的兴起，AI代理成为自主交互系统，但安全与隐私问题亟待解决。

Method: 通过分析IoA架构及其与传统网络的差异，聚焦身份认证威胁、跨代理信任问题、实体安全和隐私风险。

Result: 总结了现有和新兴的防御机制，并指出了持续存在的挑战。

Conclusion: 提出了推动弹性与隐私保护IoA生态系统发展的开放研究方向。

Abstract: With the rise of large language and vision-language models, AI agents have
evolved into autonomous, interactive systems capable of perception, reasoning,
and decision-making. As they proliferate across virtual and physical domains,
the Internet of Agents (IoA) has emerged as a key infrastructure for enabling
scalable and secure coordination among heterogeneous agents. This survey offers
a comprehensive examination of the security and privacy landscape in IoA
systems. We begin by outlining the IoA architecture and its distinct
vulnerabilities compared to traditional networks, focusing on four critical
aspects: identity authentication threats, cross-agent trust issues, embodied
security, and privacy risks. We then review existing and emerging defense
mechanisms and highlight persistent challenges. Finally, we identify open
research directions to advance the development of resilient and
privacy-preserving IoA ecosystems.

</details>


### [267] [MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges](https://arxiv.org/abs/2505.08809)
*Shixi Qin,Zhiyong Yang,Shilong Bao,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.CR

Relevance: 40.0

TL;DR: 本文提出MixBridge，一种新型扩散Schrödinger桥（DSB）框架，用于处理任意输入分布的后门触发植入，并通过Divide-and-Merge策略解决多任务冲突。


<details>
  <summary>Details</summary>
Motivation: 现有后门方法主要针对单攻击场景和高斯噪声输入模型，无法满足复杂输入分布的需求。

Method: 提出MixBridge框架，通过Divide-and-Merge策略和Weight Reallocation Scheme（WRS）实现多后门触发植入。

Result: 实验证明MixBridge在多样化生成任务中有效。

Conclusion: MixBridge为桥模型后门行为研究提供了灵活工具，并解决了多任务冲突问题。

Abstract: This paper focuses on implanting multiple heterogeneous backdoor triggers in
bridge-based diffusion models designed for complex and arbitrary input
distributions. Existing backdoor formulations mainly address single-attack
scenarios and are limited to Gaussian noise input models. To fill this gap, we
propose MixBridge, a novel diffusion Schr\"odinger bridge (DSB) framework to
cater to arbitrary input distributions (taking I2I tasks as special cases).
Beyond this trait, we demonstrate that backdoor triggers can be injected into
MixBridge by directly training with poisoned image pairs. This eliminates the
need for the cumbersome modifications to stochastic differential equations
required in previous studies, providing a flexible tool to study backdoor
behavior for bridge models. However, a key question arises: can a single DSB
model train multiple backdoor triggers? Unfortunately, our theory shows that
when attempting this, the model ends up following the geometric mean of benign
and backdoored distributions, leading to performance conflict across backdoor
tasks. To overcome this, we propose a Divide-and-Merge strategy to mix
different bridges, where models are independently pre-trained for each specific
objective (Divide) and then integrated into a unified model (Merge). In
addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the
stealthiness of MixBridge. Empirical studies across diverse generation tasks
speak to the efficacy of MixBridge.

</details>


### [268] [Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare](https://arxiv.org/abs/2505.08818)
*Amara Tariq,Rimita Lahiri,Charles Kahn,Imon Banerjee*

Main category: cs.CY

Relevance: 40.0

TL;DR: 该立场论文主张为视觉语言模型（VLM）研究建立标准化的报告协议，并提出分类框架和检查表以确保一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 由于VLM研究的多样性和复杂性，特别是在医疗等高风险领域，需要明确的报告标准以确保可重复性和直观理解。

Method: 提出VLM研究的分类框架，并设计相应的报告标准，涵盖性能评估、数据报告和论文撰写建议。

Result: 开发了一个检查表，用于标准化VLM相关研究的报告，确保一致性和质量。

Conclusion: 传统机器学习报告标准需调整以适应VLM研究，提出的框架和检查表有助于社区采纳。

Abstract: The intricate and multifaceted nature of vision language model (VLM)
development, adaptation, and application necessitates the establishment of
clear and standardized reporting protocols, particularly within the high-stakes
context of healthcare. Defining these reporting standards is inherently
challenging due to the diverse nature of studies involving VLMs, which vary
significantly from the development of all new VLMs or finetuning for domain
alignment to off-the-shelf use of VLM for targeted diagnosis and prediction
tasks. In this position paper, we argue that traditional machine learning
reporting standards and evaluation guidelines must be restructured to
accommodate multiphase VLM studies; it also has to be organized for intuitive
understanding of developers while maintaining rigorous standards for
reproducibility. To facilitate community adoption, we propose a categorization
framework for VLM studies and outline corresponding reporting standards that
comprehensively address performance evaluation, data reporting protocols, and
recommendations for manuscript composition. These guidelines are organized
according to the proposed categorization scheme. Lastly, we present a checklist
that consolidates reporting standards, offering a standardized tool to ensure
consistency and quality in the publication of VLM-related research.

</details>


### [269] [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
*Nicolas Dupuis,Ravi Nair,Shyam Ramji,Sean McClintock,Nishant Chauhan,Priyanka Nagpal,Bart Blaner,Ken Valk,Leon Stok,Ruchir Puri*

Main category: cs.SE

Relevance: 40.0

TL;DR: 论文探讨了LLM在VHDL代码解释中的应用，针对高性能处理器设计需求，通过扩展预训练和指令调优提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是填补LLM在VHDL设计中的空白，并满足高性能处理器设计组织的独特需求。

Method: 方法包括开发特定测试集、扩展预训练（EPT）、指令调优模型，并使用LLM-as-a-judge评估。

Result: EPT模型专家评分从43%提升至69%，指令调优模型预期达71%，未来可能达85%。

Conclusion: 结论指出可通过生成式AI新技术进一步提升硬件设计LLM质量。

Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.

</details>


### [270] [A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction](https://arxiv.org/abs/2505.08821)
*Meryem Altin Karagoz,Marc D. Breton,Anas El Fathi*

Main category: q-bio.QM

Relevance: 40.0

TL;DR: 论文比较了多种基于Transformer的模型在血糖预测中的表现，发现PatchTST和Crossformer在短期和长期预测中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer架构在复杂多变量时间序列预测（如血糖预测）中的潜力，以支持糖尿病治疗的个性化干预。

Method: 使用公开数据集DCLP3和OhioT1DM，训练了多种嵌入方式的Transformer模型（点状、块状、序列状和混合），并比较其预测性能。

Result: Crossformer在30分钟预测中表现最佳（RMSE 15.6 mg/dL），PatchTST在长期预测（1h、2h、4h）中表现最优（RMSE 24.6 mg/dL、36.1 mg/dL、46.5 mg/dL）。

Conclusion: 基于块的Transformer模型在血糖预测中表现出色，能够捕捉季节性模式并提高准确性。

Abstract: Accurate blood glucose prediction can enable novel interventions for type 1
diabetes treatment, including personalized insulin and dietary adjustments.
Although recent advances in transformer-based architectures have demonstrated
the power of attention mechanisms in complex multivariate time series
prediction, their potential for blood glucose (BG) prediction remains
underexplored. We present a comparative analysis of transformer models for
multi-horizon BG prediction, examining forecasts up to 4 hours and input
history up to 1 week. The publicly available DCLP3 dataset (n=112) was split
(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset
(n=12) served as an external test set. We trained networks with point-wise,
patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal
data. For short-term blood glucose prediction, Crossformer, a patch-wise
transformer architecture, achieved a superior 30-minute prediction of RMSE
(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),
PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6
mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used
tokenization through patches demonstrated improved accuracy with larger input
sizes, with the best results obtained with a one-week history. These findings
highlight the promise of transformer-based architectures for BG prediction by
capturing and leveraging seasonal patterns in multivariate time-series data to
improve accuracy.

</details>


### [271] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/abs/2505.08838)
*Peixuan Ge,Tongkun Su,Faqin Lv,Baoliang Zhao,Peng Zhang,Chi Hong Wong,Liang Yao,Yu Sun,Zenan Wang,Pak Kin Wong,Ying Hu*

Main category: eess.IV

Relevance: 40.0

TL;DR: 提出了一种统一的多器官和多语言超声报告生成框架，通过模块化文本片段和多语言训练，结合双语数据集，显著提升了报告的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 超声报告生成因图像变异性、操作依赖性和标准化文本需求而具有挑战性，现有方法缺乏一致的数据集。

Method: 整合模块化文本片段与多样化成像数据，利用双语数据集，并通过选择性解冻视觉变换器（ViT）优化文本-图像对齐。

Result: 相比KMVE方法，BLEU提升2%，ROUGE-L提升3%，CIDEr提升15%，显著减少错误内容。

Conclusion: 该框架展示了在真实临床工作流中的潜力，实现了多器官和多语言报告生成的统一。

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [272] [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://arxiv.org/abs/2505.08844)
*Jiawen Chen,Jianghao Zhang,Huaxiu Yao,Yun Li*

Main category: q-bio.GN

Relevance: 40.0

TL;DR: CellTypeAgent是一个基于LLM的代理，结合数据库验证，用于单细胞RNA测序分析中的细胞类型注释，准确性更高且减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序分析中的细胞类型注释是一个关键但繁琐的步骤，需要更高效和可靠的方法。

Method: 结合大型语言模型（LLMs）和相关数据库验证，开发了CellTypeAgent。

Result: 在涉及303种细胞类型和36个组织的9个真实数据集中，CellTypeAgent表现出比现有方法更高的准确性。

Conclusion: CellTypeAgent为细胞类型注释提供了更高效和可靠的解决方案。

Abstract: Cell type annotation is a critical yet laborious step in single-cell RNA
sequencing analysis. We present a trustworthy large language model (LLM)-agent,
CellTypeAgent, which integrates LLMs with verification from relevant databases.
CellTypeAgent achieves higher accuracy than existing methods while mitigating
hallucinations. We evaluated CellTypeAgent across nine real datasets involving
303 cell types from 36 tissues. This combined approach holds promise for more
efficient and reliable cell type annotation.

</details>


### [273] [On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction](https://arxiv.org/abs/2505.08847)
*Fatima Ezzeddine,Rinad Akel,Ihab Sbeity,Silvia Giordano,Marc Langheinrich,Omran Ayoub*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文研究了在MLaaS平台中，如何平衡模型性能、隐私和可解释性，特别是针对通过反事实解释（CFs）进行的模型提取攻击（MEA）。作者评估了两种差分隐私（DP）策略：一种在分类模型训练时实施，另一种在CF生成时实施。


<details>
  <summary>Details</summary>
Motivation: 随着MLaaS平台的普及，安全和隐私问题日益突出，尤其是XAI的引入为攻击者提供了新的攻击途径（如利用CFs进行MEA）。因此，研究如何在保持模型性能和可解释性的同时增强隐私保护至关重要。

Method: 作者评估了两种DP策略：1）在分类模型训练阶段应用DP；2）在CF生成阶段应用DP。通过实验比较这两种策略对模型性能、隐私和可解释性的影响。

Result: 实验结果表明，两种DP策略在不同程度上影响了模型的性能、隐私和可解释性。具体而言，在CF生成阶段应用DP可能更有效地平衡这三者。

Conclusion: 论文强调了在MLaaS平台中应用DP技术的重要性，尤其是在XAI背景下。选择合适的DP策略（如CF生成阶段）可以更好地平衡性能、隐私和可解释性。

Abstract: Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.

</details>


### [274] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)
*Suchismita Naik,Prakash Shukla,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parsons*

Main category: cs.HC

Relevance: 40.0

TL;DR: 研究分析了学生在设计课程中使用生成式AI工具时的判断类型，发现了传统设计判断和新出现的判断类型（如责任分配和可靠性判断），揭示了AI如何增加设计推理的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI工具在设计工作流程中的整合如何影响学生的判断和协作方式。

Method: 分析33个学生团队在HCI设计课程中的反思内容。

Result: 发现了传统设计判断和新出现的判断类型（责任分配和可靠性判断），表明AI增加了设计推理的复杂性。

Conclusion: 生成式AI在设计情境中引入了新的复杂性，促使学生反思如何与AI协作。

Abstract: As generative AI tools become integrated into design workflows, students
increasingly engage with these tools not just as aids, but as collaborators.
This study analyzes reflections from 33 student teams in an HCI design course
to examine the kinds of judgments students make when using AI tools. We found
both established forms of design judgment (e.g., instrumental, appreciative,
quality) and emergent types: agency-distribution judgment and reliability
judgment. These new forms capture how students negotiate creative
responsibility with AI and assess the trustworthiness of its outputs. Our
findings suggest that generative AI introduces new layers of complexity into
design reasoning, prompting students to reflect not only on what AI produces,
but also on how and when to rely on it. By foregrounding these judgments, we
offer a conceptual lens for understanding how students engage in co-creative
sensemaking with AI in design contexts.

</details>


### [275] [RT-cache: Efficient Robot Trajectory Retrieval System](https://arxiv.org/abs/2505.09040)
*Owen Kwon,Abraham George,Alison Bartsch,Amir Barati Farimani*

Main category: cs.RO

Relevance: 40.0

TL;DR: RT-cache通过检索和学习历史轨迹加速机器人推理，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现代视觉-语言-动作模型在机器人任务中推理成本高，导致延迟问题。

Method: 结合大数据检索和轨迹记忆，设计Memory Builder和Trajectory Retrieval流程。

Result: 在Open-X数据集上，RT-cache比无检索基线更快且更成功。

Conclusion: RT-cache为实时机器人操作提供了一种实用的数据驱动解决方案。

Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.

</details>


### [276] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/abs/2505.09091)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.SD

Relevance: 40.0

TL;DR: 提出了一种基于可变形周期性网络的GAN（DPN-GAN），通过引入周期性激活函数和多分辨率生成模块，显著提升了音频生成的质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有GAN模型在音频生成中受限于带宽有限的mel-spectrograms，导致分辨率不足和模式崩溃问题。

Method: 设计了DPN-GAN，包含周期性ReLU激活函数和可变形卷积模块，支持多分辨率生成，并优化了判别器网络。

Result: 在多个数据集上，DPN-GAN在音频质量和鲁棒性上优于现有GAN模型。

Conclusion: DPN-GAN通过创新的架构设计，解决了音频生成中的关键问题，表现出卓越的性能。

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [277] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)
*Hannu Simonen,Atte Kiviniemi,Jonas Oppenlaender*

Main category: cs.HC

Relevance: 40.0

TL;DR: 该论文研究了文本到图像生成（TTI）中的“默认图像”现象，即在输入包含未知术语时模型生成的相似图像。通过系统实验和用户调查，探讨了默认图像对用户满意度的影响，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究默认图像现象有助于改进TTI模型和提示工程，提升生成质量和用户满意度。

Method: 通过Midjourney平台，设计输入提示触发默认图像，进行实验和小规模消融研究，并结合用户调查分析影响。

Result: 揭示了默认图像的存在及其对用户满意度的负面影响，为TTI模型优化提供了实证基础。

Conclusion: 默认图像是TTI中的重要问题，需进一步研究以改进模型设计和提示工程。

Abstract: In the creative practice of text-to-image generation (TTI), images are
generated from text prompts. However, TTI models are trained to always yield an
output, even if the prompt contains unknown terms. In this case, the model may
generate what we call "default images": images that closely resemble each other
across many unrelated prompts. We argue studying default images is valuable for
designing better solutions for TTI and prompt engineering. In this paper, we
provide the first investigation into default images on Midjourney, a popular
image generator. We describe our systematic approach to create input prompts
triggering default images, and present the results of our initial experiments
and several small-scale ablation studies. We also report on a survey study
investigating how default images affect user satisfaction. Our work lays the
foundation for understanding default images in TTI and highlights challenges
and future research directions.

</details>


### [278] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)
*Lei Fan,Kunyang Deng,Fangxue Liu*

Main category: cs.HC

Relevance: 40.0

TL;DR: 研究调查了中国148名工程学生使用生成式AI的情况，探讨了其对学习效率、创造力和独立思考的影响，同时指出了准确性和领域可靠性等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在工程教育中的应用及其对学生学习体验的影响。

Method: 基于问卷调查数据，分析生成式AI的使用频率、应用场景、学习影响及挑战。

Result: 超过半数学生认为生成式AI提升了学习效率和创造力，但对其准确性和学术表现的实际影响存疑。

Conclusion: 生成式AI在工程教育中具有潜力，但需解决准确性和可靠性问题。

Abstract: With the rapid advancement of generative artificial intelligence(AI), its
potential applications in higher education have attracted significant
attention. This study investigated how 148 students from diverse engineering
disciplines and regions across China used generative AI, focusing on its impact
on their learning experience and the opportunities and challenges it poses in
engineering education. Based on the surveyed data, we explored four key areas:
the frequency and application scenarios of AI use among engineering students,
its impact on students' learning and performance, commonly encountered
challenges in using generative AI, and future prospects for its adoption in
engineering education. The results showed that more than half of the
participants reported a positive impact of generative AI on their learning
efficiency, initiative, and creativity, with nearly half believing it also
enhanced their independent thinking. However, despite acknowledging improved
study efficiency, many felt their actual academic performance remained largely
unchanged and expressed concerns about the accuracy and domain-specific
reliability of generative AI. Our findings provide a first-hand insight into
the current benefits and challenges generative AI brings to students,
particularly Chinese engineering students, while offering several
recommendations, especially from the students' perspective, for effectively
integrating generative AI into engineering education.

</details>


### [279] [Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems](https://arxiv.org/abs/2505.09342)
*Mostafa Jafari,Alireza Shameli-Sendi*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出两种对抗性攻击方法（Prioritized Binary Rounding和sigma-binary攻击），用于在二进制特征空间中绕过Android恶意软件检测系统，并揭示了现有防御方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的Android恶意软件检测系统容易受到对抗性攻击，但缺乏针对二进制特征空间的全面评估框架，限制了对其鲁棒性的理解。

Method: 1. Prioritized Binary Rounding：将连续扰动转换为二进制特征空间；2. sigma-binary攻击：一种针对二进制域的新型对抗性攻击方法。

Result: sigma-binary攻击在Malscan数据集上表现优异，攻击成功率高达94.56%，远超现有防御方法（如PAD-SMA）。

Conclusion: 研究强调了精确对抗性攻击方法的重要性，以暴露现有防御的漏洞并推动更鲁棒的恶意软件检测系统的发展。

Abstract: Machine learning is a key tool for Android malware detection, effectively
identifying malicious patterns in apps. However, ML-based detectors are
vulnerable to evasion attacks, where small, crafted changes bypass detection.
Despite progress in adversarial defenses, the lack of comprehensive evaluation
frameworks in binary-constrained domains limits understanding of their
robustness. We introduce two key contributions. First, Prioritized Binary
Rounding, a technique to convert continuous perturbations into binary feature
spaces while preserving high attack success and low perturbation size. Second,
the sigma-binary attack, a novel adversarial method for binary domains,
designed to achieve attack goals with minimal feature changes. Experiments on
the Malscan dataset show that sigma-binary outperforms existing attacks and
exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped
with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant
brittleness, with attack success rates exceeding 90% using fewer than 10
feature modifications and reaching 100% with just 20. Adversarially trained
defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small
budgets but remains vulnerable to unrestricted perturbations, with attack
success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates
strong robustness against state-of-the-art gradient-based adversarial attacks
by maintaining an attack success rate below 16.55%, the sigma-binary attack
significantly outperforms these methods, achieving a 94.56% success rate under
unrestricted perturbations. These findings highlight the critical need for
precise method like sigma-binary to expose hidden vulnerabilities in existing
defenses and support the development of more resilient malware detection
systems.

</details>


### [280] [TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search](https://arxiv.org/abs/2505.09371)
*Akash Kundu,Stefano Mangini*

Main category: quant-ph

Relevance: 40.0

TL;DR: 论文提出了TensorRL-QAS，一种结合张量网络和强化学习的可扩展框架，用于设计量子电路，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决量子架构搜索（QAS）中强化学习方法面临的扩展性问题，如计算和训练成本高，性能受限。

Method: 结合张量网络（TN）和强化学习（RL），通过矩阵乘积状态近似目标解来预热搜索空间，加速收敛。

Result: 在12量子比特的量子化学问题中，CNOT数量和电路深度减少10倍，功能评估减少100倍，训练速度提升98%，10量子比特系统成功率50%。

Conclusion: TensorRL-QAS是一种高效、可扩展的量子电路设计方法，适用于近期量子硬件。

Abstract: Variational quantum algorithms hold the promise to address meaningful quantum
problems already on noisy intermediate-scale quantum hardware, but they face
the challenge of designing quantum circuits that both solve the target problem
and comply with device limitations. Quantum architecture search (QAS) automates
this design process, with reinforcement learning (RL) emerging as a promising
approach. Yet, RL-based QAS methods encounter significant scalability issues,
as computational and training costs grow rapidly with the number of qubits,
circuit depth, and noise, severely impacting performance. To address these
challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that
combines tensor network (TN) methods with RL for designing quantum circuits. By
warm-starting the architecture search with a matrix product state approximation
of the target solution, TensorRL-QAS effectively narrows the search space to
physically meaningful circuits, accelerating convergence to the desired
solution. Tested on several quantum chemistry problems of up to 12-qubit,
TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth
compared to baseline methods, while maintaining or surpassing chemical
accuracy. It reduces function evaluations by up to 100-fold, accelerates
training episodes by up to $98\%$, and achieves up to $50\%$ success
probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline
approaches. Robustness and versatility are demonstrated both in the noiseless
and noisy scenarios, where we report a simulation of up to 8-qubit. These
advancements establish TensorRL-QAS as a promising candidate for a scalable and
efficient quantum circuit discovery protocol on near-term quantum hardware.

</details>


### [281] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08995)
*Ardian Selmonaj,Oleg Szehr,Giacomo Del Rio,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger*

Main category: cs.AI

Relevance: 40.0

TL;DR: 提出了一个分层多智能体强化学习框架，用于分析模拟空战场景中的异构智能体，通过分层决策实现高效训练和任务成功。


<details>
  <summary>Details</summary>
Motivation: 研究旨在低成本、安全的环境中探索真实防御场景，解决多智能体系统中复杂动态、状态和动作空间爆炸等问题。

Method: 采用分层强化学习，低层策略控制个体单位，高层指挥官策略发布宏观命令，通过课程学习逐步训练低层策略。

Result: 实证验证表明，该框架能有效解决复杂多智能体任务，实现任务目标。

Conclusion: 分层方法在多智能体强化学习中具有优势，适用于复杂动态环境。

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [282] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/abs/2505.09012)
*Bo Meng,Chenghao Xu,Yongli Zhu*

Main category: cs.AI

Relevance: 40.0

TL;DR: 该论文将电力网络中的多级级联故障问题视为强化学习任务，开发了模拟环境，并通过确定性策略梯度算法训练强化学习代理以实现连续动作，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 电力网络中的级联故障可能导致电网崩溃，现有策略多为单阶段，忽略了多阶段的复杂性。

Method: 将多级级联故障问题建模为强化学习任务，开发模拟环境，使用确定性策略梯度算法训练代理。

Result: 在IEEE 14-bus和IEEE 118-bus系统上验证了方法的有效性。

Conclusion: 提出的强化学习方法能有效应对多级级联故障问题。

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [283] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/abs/2505.09518)
*Maris F. L. Galesloot,Roman Andriushchenko,Milan Češka,Sebastian Junges,Nils Jansen*

Main category: cs.AI

Relevance: 40.0

TL;DR: 该论文提出了一种针对部分可观测马尔可夫决策过程（POMDPs）的鲁棒策略计算方法，通过结合形式化验证技术和子梯度上升优化，生成对未知环境具有更强适应性的策略。


<details>
  <summary>Details</summary>
Motivation: 在不确定性环境中，POMDPs的最优策略可能对环境扰动不鲁棒。HM-POMDPs通过捕捉一组潜在环境模型来解决这一问题，但需要计算鲁棒策略以应对未知的真实环境。

Method: 结合形式化验证技术（计算最坏情况POMDP）和子梯度上升优化策略，生成对HM-POMDPs中所有潜在模型均有效的鲁棒策略。

Result: 实验表明，该方法生成的策略比基线更鲁棒，且能泛化到未见过的POMDPs，同时可扩展到包含数十万个环境的HM-POMDPs。

Conclusion: 该方法为HM-POMDPs提供了一种高效的鲁棒策略计算框架，适用于大规模不确定性环境。

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [284] [A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science](https://arxiv.org/abs/2412.15404)
*Ahmet Yasin Aytar,Kemal Kilic,Kamer Kaya*

Main category: cs.IR

Relevance: 40.0

TL;DR: 本文提出了一种改进的检索增强生成（RAG）应用，帮助数据科学家高效获取学术资源。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献导航的挑战，减少信息过载，提升决策效率。

Method: 结合GROBID技术、微调嵌入模型、语义分块和摘要优先检索方法。

Result: 在RAGAS框架下评估，上下文相关性显著提升。

Conclusion: 该系统有望改变数据科学领域的学术探索方式。

Abstract: In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.

</details>


### [285] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/abs/2505.08798)
*Mobina Shrestha,Bishwas Mandal,Vishal Mandal,Asis Shrestha*

Main category: eess.IV

Relevance: 40.0

TL;DR: 论文研究了在肿瘤学中应用上下文学习（ICL）作为模型重新训练的替代方案，通过少量标注样本实现任务适应，评估了四种视觉语言模型（VLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI在肿瘤学中依赖大型标注数据集和需重新训练的问题，探索ICL的实用性。

Method: 使用四种VLMs（Paligemma、CLIP、ALIGN、GPT-4o）在三个肿瘤学数据集（MHIST、PatchCamelyon、HAM10000）上评估ICL性能。

Result: 所有模型在少样本提示下表现显著提升，GPT-4o在二分类和多分类任务中分别达到F1分数0.81和0.60。开源模型表现竞争性。

Conclusion: ICL在肿瘤学中具有潜力，特别是在资源有限的情况下。

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [286] [Security of Internet of Agents: Attacks and Countermeasures](https://arxiv.org/abs/2505.08807)
*Yuntao Wang,Yanghe Pan,Shaolong Guo,Zhou Su*

Main category: cs.CR

Relevance: 40.0

TL;DR: 该论文综述了物联网代理（IoA）系统的安全与隐私问题，分析了其架构、漏洞及防御机制，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言和视觉语言模型的兴起，AI代理成为自主交互系统，但其在物联网代理（IoA）中的安全与隐私问题亟待解决。

Method: 论文首先概述了IoA架构及其与传统网络不同的漏洞，重点分析了身份认证威胁、跨代理信任问题、实体安全和隐私风险。随后回顾了现有和新兴的防御机制。

Result: 论文总结了当前防御机制的局限性，并指出了未解决的安全与隐私挑战。

Conclusion: 提出了未来研究方向，以推动构建更具弹性和隐私保护的IoA生态系统。

Abstract: With the rise of large language and vision-language models, AI agents have
evolved into autonomous, interactive systems capable of perception, reasoning,
and decision-making. As they proliferate across virtual and physical domains,
the Internet of Agents (IoA) has emerged as a key infrastructure for enabling
scalable and secure coordination among heterogeneous agents. This survey offers
a comprehensive examination of the security and privacy landscape in IoA
systems. We begin by outlining the IoA architecture and its distinct
vulnerabilities compared to traditional networks, focusing on four critical
aspects: identity authentication threats, cross-agent trust issues, embodied
security, and privacy risks. We then review existing and emerging defense
mechanisms and highlight persistent challenges. Finally, we identify open
research directions to advance the development of resilient and
privacy-preserving IoA ecosystems.

</details>


### [287] [MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges](https://arxiv.org/abs/2505.08809)
*Shixi Qin,Zhiyong Yang,Shilong Bao,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.CR

Relevance: 40.0

TL;DR: 本文提出MixBridge，一种新型扩散Schrödinger桥框架，支持任意输入分布，并研究多异质后门触发器的植入问题。通过Divide-and-Merge策略和权重重分配方案（WRS）解决性能冲突和隐蔽性问题。


<details>
  <summary>Details</summary>
Motivation: 现有后门方法主要针对单攻击场景和高斯噪声输入模型，无法满足复杂输入分布的需求。本文旨在填补这一空白。

Method: 提出MixBridge框架，通过Divide-and-Merge策略和WRS实现多后门触发器的植入与隐蔽性增强。

Result: 实验证明MixBridge在多样生成任务中有效。

Conclusion: MixBridge为桥模型后门行为研究提供了灵活工具，并通过策略优化解决了多任务冲突。

Abstract: This paper focuses on implanting multiple heterogeneous backdoor triggers in
bridge-based diffusion models designed for complex and arbitrary input
distributions. Existing backdoor formulations mainly address single-attack
scenarios and are limited to Gaussian noise input models. To fill this gap, we
propose MixBridge, a novel diffusion Schr\"odinger bridge (DSB) framework to
cater to arbitrary input distributions (taking I2I tasks as special cases).
Beyond this trait, we demonstrate that backdoor triggers can be injected into
MixBridge by directly training with poisoned image pairs. This eliminates the
need for the cumbersome modifications to stochastic differential equations
required in previous studies, providing a flexible tool to study backdoor
behavior for bridge models. However, a key question arises: can a single DSB
model train multiple backdoor triggers? Unfortunately, our theory shows that
when attempting this, the model ends up following the geometric mean of benign
and backdoored distributions, leading to performance conflict across backdoor
tasks. To overcome this, we propose a Divide-and-Merge strategy to mix
different bridges, where models are independently pre-trained for each specific
objective (Divide) and then integrated into a unified model (Merge). In
addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the
stealthiness of MixBridge. Empirical studies across diverse generation tasks
speak to the efficacy of MixBridge.

</details>


### [288] [Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare](https://arxiv.org/abs/2505.08818)
*Amara Tariq,Rimita Lahiri,Charles Kahn,Imon Banerjee*

Main category: cs.CY

Relevance: 40.0

TL;DR: 该立场论文主张为视觉语言模型（VLM）研究建立标准化报告协议，并提出分类框架和检查清单以确保一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 由于VLM研究的多样性，传统机器学习报告标准不足以满足需求，特别是在医疗等高风险领域。

Method: 提出VLM研究的分类框架，并制定相应的报告标准，涵盖性能评估、数据报告和论文撰写建议。

Result: 设计了一个检查清单，以标准化VLM相关研究的报告流程。

Conclusion: 标准化报告协议有助于提高VLM研究的可重复性和质量。

Abstract: The intricate and multifaceted nature of vision language model (VLM)
development, adaptation, and application necessitates the establishment of
clear and standardized reporting protocols, particularly within the high-stakes
context of healthcare. Defining these reporting standards is inherently
challenging due to the diverse nature of studies involving VLMs, which vary
significantly from the development of all new VLMs or finetuning for domain
alignment to off-the-shelf use of VLM for targeted diagnosis and prediction
tasks. In this position paper, we argue that traditional machine learning
reporting standards and evaluation guidelines must be restructured to
accommodate multiphase VLM studies; it also has to be organized for intuitive
understanding of developers while maintaining rigorous standards for
reproducibility. To facilitate community adoption, we propose a categorization
framework for VLM studies and outline corresponding reporting standards that
comprehensively address performance evaluation, data reporting protocols, and
recommendations for manuscript composition. These guidelines are organized
according to the proposed categorization scheme. Lastly, we present a checklist
that consolidates reporting standards, offering a standardized tool to ensure
consistency and quality in the publication of VLM-related research.

</details>


### [289] [A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction](https://arxiv.org/abs/2505.08821)
*Meryem Altin Karagoz,Marc D. Breton,Anas El Fathi*

Main category: q-bio.QM

Relevance: 40.0

TL;DR: 论文比较了多种基于Transformer的架构在血糖预测中的表现，发现Patch-wise方法在短期和长期预测中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer架构在复杂多变量时间序列预测中的潜力，尤其是血糖预测领域。

Method: 使用公开数据集DCLP3和OhioT1DM，比较了点状、块状、序列状和混合嵌入的Transformer模型。

Result: Crossformer在30分钟预测中表现最佳（RMSE 15.6 mg/dL），PatchTST在长期预测中表现最优（1h-4h RMSE 24.6-46.5 mg/dL）。

Conclusion: 块状Transformer架构能有效捕捉季节性模式，提升血糖预测准确性。

Abstract: Accurate blood glucose prediction can enable novel interventions for type 1
diabetes treatment, including personalized insulin and dietary adjustments.
Although recent advances in transformer-based architectures have demonstrated
the power of attention mechanisms in complex multivariate time series
prediction, their potential for blood glucose (BG) prediction remains
underexplored. We present a comparative analysis of transformer models for
multi-horizon BG prediction, examining forecasts up to 4 hours and input
history up to 1 week. The publicly available DCLP3 dataset (n=112) was split
(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset
(n=12) served as an external test set. We trained networks with point-wise,
patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal
data. For short-term blood glucose prediction, Crossformer, a patch-wise
transformer architecture, achieved a superior 30-minute prediction of RMSE
(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),
PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6
mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used
tokenization through patches demonstrated improved accuracy with larger input
sizes, with the best results obtained with a one-week history. These findings
highlight the promise of transformer-based architectures for BG prediction by
capturing and leveraging seasonal patterns in multivariate time-series data to
improve accuracy.

</details>


### [290] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08825)
*Pedro Antonio Alarcon Granadeno,Theodore Chambers,Jane Cleland-Huang*

Main category: cs.MA

Relevance: 40.0

TL;DR: 提出一种基于多智能体强化学习（MARL）的算法，用于定位多个空气污染源，采用LSTM-based ADDRQN模型，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业灾难（如博帕尔事件）显示需要快速可靠的羽流追踪算法以保护公共健康和环境，传统方法在湍流条件下效果不佳。

Method: 将问题建模为部分可观察马尔可夫博弈（POMG），使用LSTM-based ADDRQN模型，结合历史动作-观察序列，模拟三维环境和多源羽流。

Result: 算法仅需探索环境的1.29%即可成功定位污染源，显著优于传统方法。

Conclusion: MARL结合LSTM-based ADDRQN在复杂环境中表现出色，适用于多源污染定位。

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


### [291] [Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores](https://arxiv.org/abs/2505.08835)
*Hyunsik Na,Wonho Lee,Seungdeok Roh,Sohee Park,Daeseon Choi*

Main category: cs.CR

Relevance: 40.0

TL;DR: 研究探讨了无人商店中基于AI的自动结账系统面临的安全漏洞，特别是对抗性补丁攻击对物体检测模型的破坏性影响，并提出了新的攻击方法和评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着无人商店的普及，其依赖的AI系统面临安全威胁，尤其是对抗性补丁攻击可能导致盗窃和库存问题，亟需研究防御策略。

Method: 研究了三种对抗性补丁攻击（隐藏、创建和修改攻击），并引入新的颜色直方图相似性损失函数和边界框指标。实验包括数字环境和物理测试床。

Result: 对抗性补丁攻击在物理环境中显著有效，且黑盒场景下影子攻击可提高成功率。当前防御机制在实时检测中存在局限性。

Conclusion: 研究强调了开发鲁棒防御策略的必要性，以保护无人商店免受对抗性攻击。

Abstract: The advent of convenient and efficient fully unmanned stores equipped with
artificial intelligence-based automated checkout systems marks a new era in
retail. However, these systems have inherent artificial intelligence security
vulnerabilities, which are exploited via adversarial patch attacks,
particularly in physical environments. This study demonstrated that adversarial
patches can severely disrupt object detection models used in unmanned stores,
leading to issues such as theft, inventory discrepancies, and interference. We
investigated three types of adversarial patch attacks -- Hiding, Creating, and
Altering attacks -- and highlighted their effectiveness. We also introduce the
novel color histogram similarity loss function by leveraging attacker knowledge
of the color information of a target class object. Besides the traditional
confusion-matrix-based attack success rate, we introduce a new
bounding-boxes-based metric to analyze the practical impact of these attacks.
Starting with attacks on object detection models trained on snack and fruit
datasets in a digital environment, we evaluated the effectiveness of
adversarial patches in a physical testbed that mimicked a real unmanned store
with RGB cameras and realistic conditions. Furthermore, we assessed the
robustness of these attacks in black-box scenarios, demonstrating that shadow
attacks can enhance success rates of attacks even without direct access to
model parameters. Our study underscores the necessity for robust defense
strategies to protect unmanned stores from adversarial threats. Highlighting
the limitations of the current defense mechanisms in real-time detection
systems and discussing various proactive measures, we provide insights into
improving the robustness of object detection models and fortifying unmanned
retail environments against these attacks.

</details>


### [292] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/abs/2505.08838)
*Peixuan Ge,Tongkun Su,Faqin Lv,Baoliang Zhao,Peng Zhang,Chi Hong Wong,Liang Yao,Yu Sun,Zenan Wang,Pak Kin Wong,Ying Hu*

Main category: eess.IV

Relevance: 40.0

TL;DR: 提出了一种统一框架，用于多器官和多语言的超声报告生成，通过片段化多语言训练和视觉变换器（ViT）的微调，显著提升了生成报告的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 超声报告生成面临图像变异性、操作依赖性和数据集缺乏标准化的挑战，本研究旨在通过统一框架解决这些问题。

Method: 结合片段化多语言训练，利用标准化超声报告特性，并通过双语数据集（英-中）和ViT的选择性解冻微调优化文本-图像对齐。

Result: 相比KMVE方法，BLEU提升约2%，ROUGE-L提升约3%，CIDEr提升约15%，同时显著减少错误内容。

Conclusion: 该框架展示了在临床工作流中实现多器官和多语言报告生成的潜力。

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [293] [On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction](https://arxiv.org/abs/2505.08847)
*Fatima Ezzeddine,Rinad Akel,Ihab Sbeity,Silvia Giordano,Marc Langheinrich,Omran Ayoub*

Main category: cs.CR

Relevance: 40.0

TL;DR: 该论文研究了在MLaaS平台中，如何通过差分隐私（DP）技术平衡模型性能、隐私和可解释性，以应对模型提取攻击（MEA）。


<details>
  <summary>Details</summary>
Motivation: MLaaS平台的普及带来了安全和隐私问题，尤其是模型提取攻击（MEA）。随着可解释AI（XAI）的集成，攻击者可能利用反事实解释（CFs）进行MEA。因此，需要探索如何通过差分隐私技术保护隐私，同时保持模型性能和可解释性。

Method: 论文评估了两种差分隐私策略：1）在分类模型训练时实施DP；2）在解释器生成反事实解释时实施DP。

Result: 研究发现，两种DP策略在保护隐私和维持模型性能及可解释性方面各有优劣。

Conclusion: 差分隐私技术可以有效缓解CFs辅助的MEA，但需要在隐私、性能和可解释性之间权衡。

Abstract: Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.

</details>


### [294] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)
*Hiba Eltigani,Rukhshan Haroon,Asli Kocak,Abdullah Bin Faisal,Noah Martin,Fahad Dogar*

Main category: cs.HC

Relevance: 40.0

TL;DR: WaLLM是一个基于WhatsApp的定制AI聊天机器人，旨在解决发展中国家数字鸿沟问题，提供信息查询和用户互动功能。


<details>
  <summary>Details</summary>
Motivation: 解决发展中国家因数字鸿沟导致的信息获取困难问题。

Method: 开发WaLLM，一个基于WhatsApp的AI聊天机器人，分析用户交互日志。

Result: 55%的查询为事实信息，健康与福祉是最热门话题（28%），使用排行榜功能的用户互动频率提高3倍。

Conclusion: 讨论了文化定制、用户界面设计和用户对AI系统信任的适当校准。

Abstract: Recent advances in generative AI, such as ChatGPT, have transformed access to
information in education, knowledge-seeking, and everyday decision-making.
However, in many developing regions, access remains a challenge due to the
persistent digital divide. To help bridge this gap, we developed WaLLM - a
custom AI chatbot over WhatsApp, a widely used communication platform in
developing regions. Beyond answering queries, WaLLM offers several features to
enhance user engagement: a daily top question, suggested follow-up questions,
trending and recent queries, and a leaderboard-based reward system. Our service
has been operational for over 6 months, amassing over 14.7K queries from
approximately 100 users. In this paper, we present WaLLM's design and a
systematic analysis of logs to understand user interactions. Our results show
that 55% of user queries seek factual information. "Health and well-being" was
the most popular topic (28%), including queries about nutrition and disease,
suggesting users view WaLLM as a reliable source. Two-thirds of users' activity
occurred within 24 hours of the daily top question. Users who accessed the
"Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by
discussing implications for culture-based customization, user interface design,
and appropriate calibration of users' trust in AI systems for developing
regions.

</details>


### [295] [RT-cache: Efficient Robot Trajectory Retrieval System](https://arxiv.org/abs/2505.09040)
*Owen Kwon,Abraham George,Alison Bartsch,Amir Barati Farimani*

Main category: cs.RO

Relevance: 40.0

TL;DR: RT-cache是一种新型轨迹记忆管道，通过大数据检索和经验学习加速机器人推理，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现代视觉-语言-动作（VLA）模型处理多样机器人任务时推理成本高，导致延迟显著。RT-cache旨在通过检索成功轨迹减少开销。

Method: RT-cache结合Memory Builder和Trajectory Retrieval，高效检索大规模数据集中的多步运动片段。

Result: 在Open-X Embodiment数据集等实验中，RT-cache比无检索基线更快且更成功完成任务。

Conclusion: RT-cache为实时操作提供了一种实用的数据驱动解决方案。

Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.

</details>


### [296] [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](https://arxiv.org/abs/2505.09108)
*Fernando Cladera,Zachary Ravichandran,Jason Hughes,Varun Murali,Carlos Nieto-Granda,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

Relevance: 40.0

TL;DR: 论文提出了一种基于大型语言模型（LLM）的规划系统，使无人机（UAV）和无人地面车辆（UGV）能够通过自然语言协作完成任务，并实时响应任务变化。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人系统的成熟，用户希望通过高级意图而非低级细节指定任务，语言是一种直观的表达方式，但其实现面临语义推理、异构机器人协调和间歇通信等技术挑战。

Method: 利用LLM支持的规划器，结合在线构建的语义-度量地图，实现无人机和地面机器人的协作任务导航。系统通过语义映射推断任务相关语义并主动获取信息。

Result: 在城乡环境的地面和空地团队实验中，系统成功完成了七种不同自然语言指定的任务，导航范围可达千米级。

Conclusion: 该系统首次实现了基于自然语言的异构机器人协作任务规划，展示了LLM在复杂机器人任务中的潜力。

Abstract: As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.

</details>


### [297] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)
*Hannu Simonen,Atte Kiviniemi,Jonas Oppenlaender*

Main category: cs.HC

Relevance: 40.0

TL;DR: 该论文首次研究了Midjourney中的'默认图像'现象，即模型对未知提示生成的相似图像，并探讨其对用户满意度的影响。


<details>
  <summary>Details</summary>
Motivation: 研究默认图像有助于改进文本到图像生成（TTI）模型的设计和提示工程。

Method: 通过系统方法创建触发默认图像的提示，进行实验和小规模消融研究，并开展用户调查。

Result: 揭示了默认图像的存在及其对用户满意度的影响，为理解TTI中的默认图像奠定了基础。

Conclusion: 默认图像是TTI中的重要现象，研究为未来改进模型和提示工程提供了方向。

Abstract: In the creative practice of text-to-image generation (TTI), images are
generated from text prompts. However, TTI models are trained to always yield an
output, even if the prompt contains unknown terms. In this case, the model may
generate what we call "default images": images that closely resemble each other
across many unrelated prompts. We argue studying default images is valuable for
designing better solutions for TTI and prompt engineering. In this paper, we
provide the first investigation into default images on Midjourney, a popular
image generator. We describe our systematic approach to create input prompts
triggering default images, and present the results of our initial experiments
and several small-scale ablation studies. We also report on a survey study
investigating how default images affect user satisfaction. Our work lays the
foundation for understanding default images in TTI and highlights challenges
and future research directions.

</details>


### [298] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)
*Lei Fan,Kunyang Deng,Fangxue Liu*

Main category: cs.HC

Relevance: 40.0

TL;DR: 研究调查了中国148名工程学生使用生成式AI的情况，发现其对学习效率和创造力有积极影响，但也存在准确性和领域可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在高等教育中的应用及其对工程学生学习体验的影响。

Method: 基于问卷调查数据，分析AI使用频率、应用场景、学习影响、挑战及未来前景。

Result: 超过半数学生认为生成式AI提升了学习效率和创造力，但对学术表现影响有限，且存在准确性问题。

Conclusion: 生成式AI在工程教育中有潜力，但需解决准确性和可靠性问题。

Abstract: With the rapid advancement of generative artificial intelligence(AI), its
potential applications in higher education have attracted significant
attention. This study investigated how 148 students from diverse engineering
disciplines and regions across China used generative AI, focusing on its impact
on their learning experience and the opportunities and challenges it poses in
engineering education. Based on the surveyed data, we explored four key areas:
the frequency and application scenarios of AI use among engineering students,
its impact on students' learning and performance, commonly encountered
challenges in using generative AI, and future prospects for its adoption in
engineering education. The results showed that more than half of the
participants reported a positive impact of generative AI on their learning
efficiency, initiative, and creativity, with nearly half believing it also
enhanced their independent thinking. However, despite acknowledging improved
study efficiency, many felt their actual academic performance remained largely
unchanged and expressed concerns about the accuracy and domain-specific
reliability of generative AI. Our findings provide a first-hand insight into
the current benefits and challenges generative AI brings to students,
particularly Chinese engineering students, while offering several
recommendations, especially from the students' perspective, for effectively
integrating generative AI into engineering education.

</details>


### [299] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/abs/2505.09012)
*Bo Meng,Chenghao Xu,Yongli Zhu*

Main category: cs.AI

Relevance: 30.0

TL;DR: 该论文将多级级联故障问题视为强化学习任务，开发了一个仿真环境，并通过确定性策略梯度算法训练强化学习代理，以验证其在IEEE 14总线和118总线系统中的有效性。


<details>
  <summary>Details</summary>
Motivation: 电网中的级联故障可能导致电网崩溃，严重影响社会和经济活动。现有策略通常仅针对单级故障，忽略了多级故障的复杂性。

Method: 将多级级联故障建模为强化学习任务，开发仿真环境，并使用确定性策略梯度算法训练代理。

Result: 在IEEE 14总线和118总线系统中验证了方法的有效性。

Conclusion: 提出的强化学习方法能够有效应对多级级联故障问题。

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [300] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08825)
*Pedro Antonio Alarcon Granadeno,Theodore Chambers,Jane Cleland-Huang*

Main category: cs.MA

Relevance: 30.0

TL;DR: 该论文提出了一种基于多智能体强化学习（MARL）的算法，用于定位多个空气污染源，使用小型无人机群（sUAS）。该方法在部分可观察的马尔可夫博弈（POMG）框架下，结合LSTM和ADDRQN技术，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业灾难（如博帕尔事件和阿利索峡谷气体泄漏）表明需要快速可靠的羽流追踪算法以保护公共健康和环境。传统方法在复杂湍流条件下效果不佳。

Method: 采用多智能体强化学习（MARL）框架，结合LSTM和ADDRQN技术，利用历史动作-观察序列建模部分可观察环境。基于高斯羽流模型（GPM）的仿真环境模拟了三维空间、传感器噪声和多智能体交互。

Result: 仿真结果显示，该算法仅需探索环境的1.29%即可成功定位污染源，显著优于传统方法。

Conclusion: 该方法在复杂、部分可观察的环境中表现出高效性和适应性，为污染源定位提供了新思路。

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


### [301] [Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores](https://arxiv.org/abs/2505.08835)
*Hyunsik Na,Wonho Lee,Seungdeok Roh,Sohee Park,Daeseon Choi*

Main category: cs.CR

Relevance: 30.0

TL;DR: 论文研究了无人商店中基于AI的自动结账系统的对抗性补丁攻击，展示了其严重性，并提出了一种新的颜色直方图相似性损失函数和评估指标。


<details>
  <summary>Details</summary>
Motivation: 无人商店的AI系统存在安全漏洞，对抗性补丁攻击可能导致盗窃和库存问题，亟需研究防御策略。

Method: 研究了三种对抗性补丁攻击（隐藏、创建、修改），提出新损失函数和评估指标，并在数字和物理环境中测试攻击效果。

Result: 攻击在数字和物理环境中均有效，黑盒场景下影子攻击能提高成功率，现有防御机制存在局限性。

Conclusion: 需开发更鲁棒的防御策略以保护无人商店免受对抗性攻击。

Abstract: The advent of convenient and efficient fully unmanned stores equipped with
artificial intelligence-based automated checkout systems marks a new era in
retail. However, these systems have inherent artificial intelligence security
vulnerabilities, which are exploited via adversarial patch attacks,
particularly in physical environments. This study demonstrated that adversarial
patches can severely disrupt object detection models used in unmanned stores,
leading to issues such as theft, inventory discrepancies, and interference. We
investigated three types of adversarial patch attacks -- Hiding, Creating, and
Altering attacks -- and highlighted their effectiveness. We also introduce the
novel color histogram similarity loss function by leveraging attacker knowledge
of the color information of a target class object. Besides the traditional
confusion-matrix-based attack success rate, we introduce a new
bounding-boxes-based metric to analyze the practical impact of these attacks.
Starting with attacks on object detection models trained on snack and fruit
datasets in a digital environment, we evaluated the effectiveness of
adversarial patches in a physical testbed that mimicked a real unmanned store
with RGB cameras and realistic conditions. Furthermore, we assessed the
robustness of these attacks in black-box scenarios, demonstrating that shadow
attacks can enhance success rates of attacks even without direct access to
model parameters. Our study underscores the necessity for robust defense
strategies to protect unmanned stores from adversarial threats. Highlighting
the limitations of the current defense mechanisms in real-time detection
systems and discussing various proactive measures, we provide insights into
improving the robustness of object detection models and fortifying unmanned
retail environments against these attacks.

</details>


### [302] [Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America](https://arxiv.org/abs/2505.08841)
*Andrea Cremaschi,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.CY

Relevance: 30.0

TL;DR: 论文研究了拉丁美洲公众对人工智能和机器人技术导致失业的担忧，通过调查数据分析其演变及影响因素。


<details>
  <summary>Details</summary>
Motivation: 了解公众对AI和机器人技术的态度，特别是在新兴经济体中，填补全球北方以外地区的研究空白。

Method: 使用拉丁美洲16国的调查数据（2017-2023），采用统计建模和潜在类别分析。

Result: 发现教育水平和政治倾向是担忧的主要驱动因素，2018年担忧达到峰值，且存在显著的跨国差异。

Conclusion: 研究揭示了新兴经济体中AI焦虑的社会和结构性维度，为全球北方以外的自动化态度提供了新见解。

Abstract: As artificial intelligence and robotics increasingly reshape the global labor
market, understanding public perceptions of these technologies becomes
critical. We examine how these perceptions have evolved across Latin America,
using survey data from the 2017, 2018, 2020, and 2023 waves of the
Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16
countries, we analyze fear of job loss due to artificial intelligence and
robotics. Using statistical modeling and latent class analysis, we identify key
structural and ideological predictors of concern, with education level and
political orientation emerging as the most consistent drivers. Our findings
reveal substantial temporal and cross-country variation, with a notable peak in
fear during 2018 and distinct attitudinal profiles emerging from latent
segmentation. These results offer new insights into the social and structural
dimensions of AI anxiety in emerging economies and contribute to a broader
understanding of public attitudes toward automation beyond the Global North.

</details>


### [303] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/abs/2505.08845)
*Misgina Tsighe Hagos,Antti Suutala,Dmitrii Bychkov,Hakan Kücükel,Joar von Bahr,Milda Poceviciute,Johan Lundin,Nina Linder,Claes Lundström*

Main category: eess.IV

Relevance: 30.0

TL;DR: 该论文探讨了在宫颈癌分类中使用共形预测（conformal prediction）生成预测集的方法，以解决深度学习模型过度自信和不确定性表达不足的问题。研究发现，现有共形预测评估方法高估了性能，且预测集与人类标注不一致。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在宫颈癌分类中常过度自信且无法可靠表达诊断不确定性，共形预测提供了一种模型无关的框架来解决这一问题。

Method: 研究评估了三种共形预测方法在三种深度学习模型上的表现，并使用专家标注集进行验证。

Result: 研究发现传统基于覆盖率的评估高估了性能，且共形预测生成的预测集与人类标注不一致。

Conclusion: 共形预测方法在表达模型不确定性和处理模糊数据方面仍有改进空间。

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [304] [A New Tractable Description Logic under Categorical Semantics](https://arxiv.org/abs/2505.08916)
*Chan Le Duc,Ludovic Brieulle*

Main category: cs.LO

Relevance: 30.0

TL;DR: 提出了一种新的扩展EL逻辑的方法，通过弱化否定语义来保留可处理性，同时表达负知识。


<details>
  <summary>Details</summary>
Motivation: 生物医学本体中常涉及负知识（如“缺乏部分”），但现有逻辑（如EL）无法直接表示此类知识，且引入完全否定会导致可处理性丧失。

Method: 引入SH逻辑的范畴语义，通过识别并移除导致不可处理性的独立范畴属性，弱化析取和全称限制的语义。

Result: 新逻辑比带有底部概念、传递角色和角色包含的EL更具表达力，同时保持可处理性。

Conclusion: 该研究为生物医学本体中负知识的表示提供了一种可行的逻辑扩展方法。

Abstract: Biomedical ontologies contain numerous concept or role names involving
negative knowledge such as lacks_part, absence_of. Such a representation with
labels rather than logical constructors would not allow a reasoner to interpret
lacks_part as a kind of negation of has_part. It is known that adding negation
to the tractable Description Logic (DL) EL allowing for conjunction,
existential restriction and concept inclusion makes it intractable since the
obtained logic includes implicitly disjunction and universal restriction which
interact with other constructors. In this paper, we propose a new extension of
EL with a weakened negation allowing to represent negative knowledge while
retaining tractability. To this end, we introduce categorical semantics of all
logical constructors of the DL SH including EL with disjunction, negation,
universal restriction, role inclusion and transitive roles. The categorical
semantics of a logical constructor is usually described as a set of categorical
properties referring to several objects without using set membership. To
restore tractability, we have to weaken semantics of disjunction and universal
restriction by identifying \emph{independent} categorical properties that are
responsible for intractability, and dropping them from the set of categorical
properties. We show that the logic resulting from weakening semantics is more
expressive than EL with the bottom concept, transitive roles and role
inclusion.

</details>


### [305] [When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes](https://arxiv.org/abs/2505.08918)
*Marina Popova,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: q-bio.GN

Relevance: 30.0

TL;DR: 研究探讨了基因组序列的BPE分词策略，发现共享词汇随基因组比较增加而快速减少，且基于分词重叠的系统发育树未能重现已知灵长类关系。


<details>
  <summary>Details</summary>
Motivation: 探索基因组序列的有效分词策略，以支持大规模基因组语言模型的开发。

Method: 使用自定义工具dnaBPE对9个T2T灵长类基因组进行BPE分词，固定词汇量为512,000。

Result: 仅11,569个分词共享，991,854个为单基因组独有；系统发育树与已知关系不符。

Conclusion: BPE分词在压缩重复序列有效，但对高拷贝元素敏感，需结合混合策略和重复屏蔽。

Abstract: The emergence of telomere-to-telomere (T2T) genome assemblies has opened new
avenues for comparative genomics, yet effective tokenization strategies for
genomic sequences remain underexplored. In this pilot study, we apply Byte Pair
Encoding (BPE) to nine T2T primate genomes including three human assemblies by
training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens
using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are
shared across all assemblies, while nearly 991,854 tokens are unique to a
single genome, indicating a rapid decline in shared vocabulary with increasing
assembly comparisons. Moreover, phylogenetic trees derived from token overlap
failed to recapitulate established primate relationships, a discrepancy
attributed to the disproportionate influence of species-specific high-copy
repetitive elements. These findings underscore the dual nature of BPE
tokenization: while it effectively compresses repetitive sequences, its
sensitivity to high-copy elements limits its utility as a universal tool for
comparative genomics. We discuss potential hybrid strategies and repeat-masking
approaches to refine genomic tokenization, emphasizing the need for
domain-specific adaptations in the development of large-scale genomic language
models. The dnaBPE tool used in this study is open-source and available at
https://github.com/aglabx/dnaBPE.

</details>


### [306] [Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919)
*Kangxian Xie,Yufei Zhu,Kaiming Kuang,Li Zhang,Hongwei Bran Li,Mingchen Gao,Jiancheng Yang*

Main category: cs.GR

Relevance: 30.0

TL;DR: 提出了一种基于神经隐式函数的方法，用于高质量3D肺段重建，并通过变形可学习模板实现解剖学感知的精确重建。同时，引入了两个临床相关的评估指标，并开发了一个名为Lung3D的数据集用于算法基准测试。


<details>
  <summary>Details</summary>
Motivation: 高质量的3D肺段重建在肺段切除术和肺癌手术治疗规划中至关重要，但现有深度学习方法受限于计算资源或分辨率不足。

Method: 采用神经隐式函数方法，通过变形可学习模板实现3D表面重建，并开发了Lung3D数据集用于评估。

Result: 提出的方法在肺段重建上优于现有方法，为重建提供了新视角。

Conclusion: 该方法为肺段重建提供了高效且精确的解决方案，并公开了代码和数据以促进研究。

Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in
segmentectomy and surgical treatment planning for lung cancer. Due to the
resolution requirement of the target reconstruction, conventional deep
learning-based methods often suffer from computational resource constraints or
limited granularity. Conversely, implicit modeling is favored due to its
computational efficiency and continuous representation at any resolution. We
propose a neural implicit function-based method to learn a 3D surface to
achieve anatomy-aware, precise pulmonary segment reconstruction, represented as
a shape by deforming a learnable template. Additionally, we introduce two
clinically relevant evaluation metrics to assess the reconstruction
comprehensively. Further, due to the absence of publicly available shape
datasets to benchmark reconstruction algorithms, we developed a shape dataset
named Lung3D, including the 3D models of 800 labeled pulmonary segments and the
corresponding airways, arteries, veins, and intersegmental veins. We
demonstrate that the proposed approach outperforms existing methods, providing
a new perspective for pulmonary segment reconstruction. Code and data will be
available at https://github.com/M3DV/ImPulSe.

</details>


### [307] [InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials](https://arxiv.org/abs/2505.09203)
*Xiao-Qi Han,Peng-Jie Guo,Ze-Feng Gao,Hao Sun,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

Relevance: 30.0

TL;DR: 提出了一种基于主动学习策略的逆向材料设计生成框架InvDesFlow-AL，显著提升了晶体结构预测的准确性和材料生成的成功率。


<details>
  <summary>Details</summary>
Motivation: 开发逆向设计方法以加速功能材料的发现，解决现有生成模型成功率低的问题。

Method: 结合扩散原理的生成模型与主动学习策略，迭代优化材料生成过程。

Result: 在晶体结构预测中，RMSE为0.0423 Å，性能提升32.96%；成功设计出低形成能材料并发现高温超导体Li₂AuH₆。

Conclusion: InvDesFlow-AL在材料逆向设计中表现出高效性，为材料科学提供了新工具。

Abstract: Developing inverse design methods for functional materials with specific
properties is critical to advancing fields like renewable energy, catalysis,
energy storage, and carbon capture. Generative models based on diffusion
principles can directly produce new materials that meet performance
constraints, thereby significantly accelerating the material design process.
However, existing methods for generating and predicting crystal structures
often remain limited by low success rates. In this work, we propose a novel
inverse material design generative framework called InvDesFlow-AL, which is
based on active learning strategies. This framework can iteratively optimize
the material generation process to gradually guide it towards desired
performance characteristics. In terms of crystal structure prediction, the
InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96%
improvement in performance compared to exsisting generative models.
Additionally, InvDesFlow-AL has been successfully validated in the design of
low-formation-energy and low-Ehull materials. It can systematically generate
materials with progressively lower formation energies while continuously
expanding the exploration across diverse chemical spaces. These results fully
demonstrate the effectiveness of the proposed active learning-driven generative
model in accelerating material discovery and inverse design. To further prove
the effectiveness of this method, we took the search for BCS superconductors
under ambient pressure as an example explored by InvDesFlow-AL. As a result, we
successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor
with an ultra-high transition temperature of 140 K. This discovery provides
strong empirical support for the application of inverse design in materials
science.

</details>


### [308] [EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://arxiv.org/abs/2505.09262)
*Hongxin Xiang,Ke Li,Mingquan Liu,Zhixiang Cheng,Bin Yao,Wenjie Du,Jun Xia,Li Zeng,Xin Jin,Xiangxiang Zeng*

Main category: physics.chem-ph

Relevance: 30.0

TL;DR: 论文提出了EDBench，一个大规模、高质量的电子密度数据集，用于推动电子尺度的学习研究，并展示了基于学习的方法在计算电子密度上的高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有分子机器学习力场（MLFFs）通常忽略电子密度（ED）的重要性，而ED是理解分子力场的关键。由于传统DFT计算耗时，缺乏大规模ED数据，限制了其在MLFFs中的应用。

Method: 基于PCQM4Mv2构建了包含330万分子的大规模ED数据集EDBench，并设计了一套ED为中心的基准任务（预测、检索、生成）来评估模型能力。

Result: 实验表明，基于EDBench的学习方法不仅可行，还能高效计算ED，精度与传统DFT相当，但计算成本显著降低。

Conclusion: EDBench为电子密度驱动的药物发现和材料科学提供了坚实基础，所有数据和基准将免费开放。

Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on
the learning of atoms, molecules, and simple quantum chemical properties (such
as energy and force), but ignore the importance of electron density (ED)
$\rho(r)$ in accurately understanding molecular force fields (MFFs). ED
describes the probability of finding electrons at specific locations around
atoms or molecules, which uniquely determines all ground state properties (such
as energy, molecular structure, etc.) of interactive multi-particle systems
according to the Hohenberg-Kohn theorem. However, the calculation of ED relies
on the time-consuming first-principles density functional theory (DFT) which
leads to the lack of large-scale ED data and limits its application in MLFFs.
In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED
designed to advance learning-based research at the electronic scale. Built upon
the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million
molecules. To comprehensively evaluate the ability of models to understand and
utilize electronic information, we design a suite of ED-centric benchmark tasks
spanning prediction, retrieval, and generation. Our evaluation on several
state-of-the-art methods demonstrates that learning from EDBench is not only
feasible but also achieves high accuracy. Moreover, we show that learning-based
method can efficiently calculate ED with comparable precision while
significantly reducing the computational cost relative to traditional DFT
calculations. All data and benchmarks from EDBench will be freely available,
laying a robust foundation for ED-driven drug discovery and materials science.

</details>


### [309] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

Relevance: 30.0

TL;DR: 论文提出了一种混合量子-经典框架Quantum-Train (QT)，用于台风轨迹预测，通过量子神经网络(QNNs)生成可训练参数，减少计算需求。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对灾害准备至关重要，但传统方法计算资源需求高，因此需要一种更高效的方法。

Method: 结合Quantum Parameter Adaptation (QPA)和Attention-based Multi-ConvGRU模型，实现参数高效训练。

Result: QPA显著减少可训练参数数量，同时保持预测性能。

Conclusion: QT框架为大规模台风预测提供了一种可扩展且节能的解决方案。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


### [310] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/abs/2505.09565)
*Maik Dannecker,Thomas Sanchez,Meritxell Bach Cuadra,Özgün Turgut,Anthony N. Price,Lucilio Cordero-Grande,Vanessa Kyriakopoulou,Joseph V. Hajnal,Daniel Rueckert*

Main category: eess.IV

Relevance: 30.0

TL;DR: 提出了一种基于隐式神经表示的新型切片到体积重建（SVR）方法，用于处理运动伪影严重的MRI图像重建，显著提升了重建质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在运动伪影严重的MRI图像重建中表现不佳，需要切片预对齐或无法处理严重运动。本文旨在解决这些问题。

Method: 采用隐式神经表示进行运动校正、异常值处理和高分辨率重建，并通过自监督元学习初始化任务特定先验。

Result: 在模拟和临床MRI数据上的实验表明，该方法在严重运动情况下重建质量优于现有方法，且重建时间减少50%。

Conclusion: 该方法在运动伪影严重的MRI重建中具有显著优势，适用于实际临床应用。

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


### [311] [Quantum state-agnostic work extraction (almost) without dissipation](https://arxiv.org/abs/2505.09456)
*Josep Lumbreras,Ruo Cheng Huang,Yanglin Hu,Mile Gu,Marco Tomamichel*

Main category: quant-ph

Relevance: 30.0

TL;DR: 论文研究了通过强化学习设计的工作提取协议，以最大化从未知纯量子比特状态中提取能量到电池的效率。


<details>
  <summary>Details</summary>
Motivation: 核心挑战在于设计交互以平衡两个目标：利用当前量子比特最优充电，以及通过量子比特获取更多信息以改进后续能量收集。

Method: 利用强化学习中的探索-利用权衡，开发自适应策略。

Result: 实现了能量耗散仅随N的多对数增长，相比基于全状态层析的现有协议有指数级改进。

Conclusion: 强化学习策略在量子能量提取中表现出显著优势。

Abstract: We investigate work extraction protocols designed to transfer the maximum
possible energy to a battery using sequential access to $N$ copies of an
unknown pure qubit state. The core challenge is designing interactions to
optimally balance two competing goals: charging of the battery optimally using
the qubit in hand, and acquiring more information by qubit to improve energy
harvesting in subsequent rounds. Here, we leverage exploration-exploitation
trade-off in reinforcement learning to develop adaptive strategies achieving
energy dissipation that scales only poly-logarithmically in $N$. This
represents an exponential improvement over current protocols based on full
state tomography.

</details>


### [312] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

Relevance: 30.0

TL;DR: 论文提出了一种基于深度强化学习（DRL）的信号交叉口纵向车辆控制策略，结合DDPG和SAC算法，优化了效率、安全性和舒适性。


<details>
  <summary>Details</summary>
Motivation: 信号交叉口的车辆控制决策复杂，传统方法难以兼顾效率与安全，因此探索DRL在此场景的应用。

Method: 设计了综合奖励函数，结合DDPG和SAC算法，利用真实和模拟轨迹数据训练模型。

Result: RL模型在效率和舒适性上优于人类驾驶，且能处理安全关键场景，DDPG表现更平滑。

Conclusion: DRL策略可提升信号交叉口的交通安全性、效率和舒适性。

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [313] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/abs/2505.08845)
*Misgina Tsighe Hagos,Antti Suutala,Dmitrii Bychkov,Hakan Kücükel,Joar von Bahr,Milda Poceviciute,Johan Lundin,Nina Linder,Claes Lundström*

Main category: eess.IV

Relevance: 30.0

TL;DR: 该论文探讨了在宫颈癌分类中使用保形预测（conformal prediction）生成预测集的方法，以解决深度学习模型过度自信和不确定性表达不足的问题。研究发现传统基于覆盖率的评估高估了性能，且预测集与人类标注不一致。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在宫颈癌分类中常过度自信且无法可靠表达不确定性，保形预测可解决这一问题，但现有评估方法忽略了预测集的真实性和实用性。

Method: 研究使用专家标注集对三种保形预测方法在三种深度学习模型上进行全面验证，评估预测集与人类标注的一致性。

Result: 传统覆盖率评估高估性能，保形预测生成的预测集常与人类标注不一致。研究还探讨了保形预测在识别模糊和分布外数据方面的能力。

Conclusion: 保形预测需改进以生成更符合人类期望的预测集，未来工作应关注预测集的真实性和实用性。

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [314] [FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations](https://arxiv.org/abs/2505.08904)
*Varun Nagaraj Rao,Samantha Dalal,Andrew Schwartz,Amna Liaqat,Dana Calacci,Andrés Monroy-Hernández*

Main category: cs.CY

Relevance: 30.0

TL;DR: 论文介绍了FareShare工具，用于自动化计算被解雇的网约车司机的工资损失，显著提高了效率并减少了错误。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决网约车司机因AI和算法决策被突然解雇后缺乏有效工具支持的问题，尤其是工资损失计算和法律申诉流程。

Method: 通过与华盛顿州最大的网约车工会合作，设计了FareShare工具，自动化工资损失计算，并在实际部署中验证其效果。

Result: 工具将工资损失计算时间减少95%，消除了手动数据输入错误，并提高了法律团队生成仲裁报告的速度。

Conclusion: FareShare工具在提高效率的同时，也揭示了高压力劳动环境中信任、同意和工具采用的社会技术挑战。

Abstract: What happens when a rideshare driver is suddenly locked out of the platform
connecting them to riders, wages, and daily work? Deactivation-the abrupt
removal of gig workers' platform access-typically occurs through arbitrary AI
and algorithmic decisions with little explanation or recourse. This represents
one of the most severe forms of algorithmic control and often devastates
workers' financial stability. Recent U.S. state policies now mandate appeals
processes and recovering compensation during the period of wrongful
deactivation based on past earnings. Yet, labor organizers still lack effective
tools to support these complex, error-prone workflows. We designed FareShare, a
computational tool automating lost wage estimation for deactivated drivers,
through a 6 month partnership with the State of Washington's largest rideshare
labor union. Over the following 3 months, our field deployment of FareShare
registered 178 account signups. We observed that the tool could reduce lost
wage calculation time by over 95%, eliminate manual data entry errors, and
enable legal teams to generate arbitration-ready reports more efficiently.
Beyond these gains, the deployment also surfaced important socio-technical
challenges around trust, consent, and tool adoption in high-stakes labor
contexts.

</details>


### [315] [A New Tractable Description Logic under Categorical Semantics](https://arxiv.org/abs/2505.08916)
*Chan Le Duc,Ludovic Brieulle*

Main category: cs.LO

Relevance: 30.0

TL;DR: 论文提出了一种扩展EL描述逻辑的方法，通过引入弱化否定来保留可处理性，同时增强表达能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学本体中常包含否定知识（如“缺乏部分”），但现有表示方法无法让推理机将其解释为否定。扩展EL逻辑以支持否定会引入不可处理性，因此需要一种新方法。

Method: 引入SH描述逻辑的分类语义，识别并弱化导致不可处理性的独立分类属性，从而保留可处理性。

Result: 提出的弱化语义逻辑比带有底部概念、传递角色和角色包含的EL更具表达力。

Conclusion: 通过弱化语义，实现了在保留可处理性的同时增强逻辑表达力，适用于生物医学本体中的否定知识表示。

Abstract: Biomedical ontologies contain numerous concept or role names involving
negative knowledge such as lacks_part, absence_of. Such a representation with
labels rather than logical constructors would not allow a reasoner to interpret
lacks_part as a kind of negation of has_part. It is known that adding negation
to the tractable Description Logic (DL) EL allowing for conjunction,
existential restriction and concept inclusion makes it intractable since the
obtained logic includes implicitly disjunction and universal restriction which
interact with other constructors. In this paper, we propose a new extension of
EL with a weakened negation allowing to represent negative knowledge while
retaining tractability. To this end, we introduce categorical semantics of all
logical constructors of the DL SH including EL with disjunction, negation,
universal restriction, role inclusion and transitive roles. The categorical
semantics of a logical constructor is usually described as a set of categorical
properties referring to several objects without using set membership. To
restore tractability, we have to weaken semantics of disjunction and universal
restriction by identifying \emph{independent} categorical properties that are
responsible for intractability, and dropping them from the set of categorical
properties. We show that the logic resulting from weakening semantics is more
expressive than EL with the bottom concept, transitive roles and role
inclusion.

</details>


### [316] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)
*Suchismita Naik,Prakash Shukla,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parsons*

Main category: cs.HC

Relevance: 30.0

TL;DR: 该研究分析了33个学生团队在HCI设计课程中使用AI工具时的判断类型，发现了传统设计判断和新兴判断（如代理分配判断和可靠性判断），揭示了生成式AI如何增加设计推理的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨学生在设计工作流程中如何与生成式AI工具互动，并识别新兴的判断类型。

Method: 通过分析33个学生团队的反思内容，识别设计判断的类型。

Result: 发现了传统设计判断和两种新兴判断（代理分配判断和可靠性判断），揭示了AI工具在设计中的复杂影响。

Conclusion: 生成式AI为设计推理引入了新的复杂性，需要学生反思如何依赖AI工具。

Abstract: As generative AI tools become integrated into design workflows, students
increasingly engage with these tools not just as aids, but as collaborators.
This study analyzes reflections from 33 student teams in an HCI design course
to examine the kinds of judgments students make when using AI tools. We found
both established forms of design judgment (e.g., instrumental, appreciative,
quality) and emergent types: agency-distribution judgment and reliability
judgment. These new forms capture how students negotiate creative
responsibility with AI and assess the trustworthiness of its outputs. Our
findings suggest that generative AI introduces new layers of complexity into
design reasoning, prompting students to reflect not only on what AI produces,
but also on how and when to rely on it. By foregrounding these judgments, we
offer a conceptual lens for understanding how students engage in co-creative
sensemaking with AI in design contexts.

</details>


### [317] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/abs/2505.09091)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.SD

Relevance: 30.0

TL;DR: 论文提出了一种基于可变形周期网络的GAN（DPN-GAN），通过引入周期性激活函数和多分辨率生成模块，显著提升了音频生成的质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的音频生成模型受限于带宽受限的梅尔频谱图，导致生成分辨率低和模式崩溃问题。

Method: 提出DPN-GAN，结合周期性ReLU激活函数和可变形卷积操作，支持多分辨率生成和自适应感受野。

Result: 实验表明，DPN-GAN在多种数据集上优于现有GAN模型，表现出更高的生成质量和鲁棒性。

Conclusion: DPN-GAN为音频生成提供了一种高效且鲁棒的解决方案。

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [318] [InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials](https://arxiv.org/abs/2505.09203)
*Xiao-Qi Han,Peng-Jie Guo,Ze-Feng Gao,Hao Sun,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

Relevance: 30.0

TL;DR: 提出了一种基于主动学习的逆向材料设计生成框架InvDesFlow-AL，显著提升了晶体结构预测性能，并在材料发现中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 开发逆向设计方法以加速功能材料的发现，解决现有生成模型成功率低的问题。

Method: 基于主动学习策略的生成框架InvDesFlow-AL，迭代优化材料生成过程。

Result: 晶体结构预测RMSE为0.0423 Å，性能提升32.96%；成功设计出低形成能材料，并发现高温超导体Li₂AuH₆。

Conclusion: InvDesFlow-AL在材料逆向设计中表现出高效性，为材料科学提供了新工具。

Abstract: Developing inverse design methods for functional materials with specific
properties is critical to advancing fields like renewable energy, catalysis,
energy storage, and carbon capture. Generative models based on diffusion
principles can directly produce new materials that meet performance
constraints, thereby significantly accelerating the material design process.
However, existing methods for generating and predicting crystal structures
often remain limited by low success rates. In this work, we propose a novel
inverse material design generative framework called InvDesFlow-AL, which is
based on active learning strategies. This framework can iteratively optimize
the material generation process to gradually guide it towards desired
performance characteristics. In terms of crystal structure prediction, the
InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96%
improvement in performance compared to exsisting generative models.
Additionally, InvDesFlow-AL has been successfully validated in the design of
low-formation-energy and low-Ehull materials. It can systematically generate
materials with progressively lower formation energies while continuously
expanding the exploration across diverse chemical spaces. These results fully
demonstrate the effectiveness of the proposed active learning-driven generative
model in accelerating material discovery and inverse design. To further prove
the effectiveness of this method, we took the search for BCS superconductors
under ambient pressure as an example explored by InvDesFlow-AL. As a result, we
successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor
with an ultra-high transition temperature of 140 K. This discovery provides
strong empirical support for the application of inverse design in materials
science.

</details>


### [319] [EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://arxiv.org/abs/2505.09262)
*Hongxin Xiang,Ke Li,Mingquan Liu,Zhixiang Cheng,Bin Yao,Wenjie Du,Jun Xia,Li Zeng,Xin Jin,Xiangxiang Zeng*

Main category: physics.chem-ph

Relevance: 30.0

TL;DR: 论文介绍了EDBench，一个大规模、高质量的电子密度数据集，旨在推动电子尺度的学习研究。


<details>
  <summary>Details</summary>
Motivation: 现有分子机器学习力场（MLFFs）忽略了电子密度（ED）在准确理解分子力场中的重要性，且ED计算依赖耗时的一阶原理密度泛函理论（DFT），导致缺乏大规模ED数据。

Method: 基于PCQM4Mv2构建EDBench，提供330万分子的精确ED数据，并设计了一套ED为中心的基准任务（预测、检索、生成）。

Result: 实验表明，基于EDBench的学习方法不仅可行，还能高效计算ED，显著降低计算成本。

Conclusion: EDBench为电子驱动的药物发现和材料科学奠定了坚实基础。

Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on
the learning of atoms, molecules, and simple quantum chemical properties (such
as energy and force), but ignore the importance of electron density (ED)
$\rho(r)$ in accurately understanding molecular force fields (MFFs). ED
describes the probability of finding electrons at specific locations around
atoms or molecules, which uniquely determines all ground state properties (such
as energy, molecular structure, etc.) of interactive multi-particle systems
according to the Hohenberg-Kohn theorem. However, the calculation of ED relies
on the time-consuming first-principles density functional theory (DFT) which
leads to the lack of large-scale ED data and limits its application in MLFFs.
In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED
designed to advance learning-based research at the electronic scale. Built upon
the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million
molecules. To comprehensively evaluate the ability of models to understand and
utilize electronic information, we design a suite of ED-centric benchmark tasks
spanning prediction, retrieval, and generation. Our evaluation on several
state-of-the-art methods demonstrates that learning from EDBench is not only
feasible but also achieves high accuracy. Moreover, we show that learning-based
method can efficiently calculate ED with comparable precision while
significantly reducing the computational cost relative to traditional DFT
calculations. All data and benchmarks from EDBench will be freely available,
laying a robust foundation for ED-driven drug discovery and materials science.

</details>


### [320] [Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems](https://arxiv.org/abs/2505.09342)
*Mostafa Jafari,Alireza Shameli-Sendi*

Main category: cs.CR

Relevance: 30.0

TL;DR: 论文提出了一种针对二进制特征空间的对抗攻击方法（sigma-binary）和一种转换技术（Prioritized Binary Rounding），用于评估Android恶意软件检测防御的鲁棒性。实验显示现有防御方法存在显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在Android恶意软件检测中表现优异，但其防御方法在二进制特征空间中的鲁棒性缺乏全面评估，亟需更精确的攻击方法以暴露潜在漏洞。

Method: 1. Prioritized Binary Rounding：将连续扰动转换为二进制特征空间；2. sigma-binary攻击：针对二进制域设计，以最小特征修改实现攻击目标。

Result: sigma-binary攻击在Malscan数据集上优于现有方法，攻击成功率高达94.56%，暴露了现有防御的脆弱性（如KDE、DLA等）。

Conclusion: 研究强调了精确攻击方法（如sigma-binary）的重要性，以支持开发更鲁棒的恶意软件检测系统。

Abstract: Machine learning is a key tool for Android malware detection, effectively
identifying malicious patterns in apps. However, ML-based detectors are
vulnerable to evasion attacks, where small, crafted changes bypass detection.
Despite progress in adversarial defenses, the lack of comprehensive evaluation
frameworks in binary-constrained domains limits understanding of their
robustness. We introduce two key contributions. First, Prioritized Binary
Rounding, a technique to convert continuous perturbations into binary feature
spaces while preserving high attack success and low perturbation size. Second,
the sigma-binary attack, a novel adversarial method for binary domains,
designed to achieve attack goals with minimal feature changes. Experiments on
the Malscan dataset show that sigma-binary outperforms existing attacks and
exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped
with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant
brittleness, with attack success rates exceeding 90% using fewer than 10
feature modifications and reaching 100% with just 20. Adversarially trained
defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small
budgets but remains vulnerable to unrestricted perturbations, with attack
success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates
strong robustness against state-of-the-art gradient-based adversarial attacks
by maintaining an attack success rate below 16.55%, the sigma-binary attack
significantly outperforms these methods, achieving a 94.56% success rate under
unrestricted perturbations. These findings highlight the critical need for
precise method like sigma-binary to expose hidden vulnerabilities in existing
defenses and support the development of more resilient malware detection
systems.

</details>


### [321] [TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search](https://arxiv.org/abs/2505.09371)
*Akash Kundu,Stefano Mangini*

Main category: quant-ph

Relevance: 30.0

TL;DR: TensorRL-QAS结合张量网络和强化学习，提出了一种可扩展的量子电路设计框架，显著减少了计算成本和电路复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决量子架构搜索（QAS）中强化学习方法因计算和训练成本高而难以扩展的问题。

Method: 通过矩阵乘积状态近似目标解预热架构搜索，缩小搜索空间，结合张量网络和强化学习优化量子电路设计。

Result: 在12量子比特的量子化学问题中，CNOT数量和电路深度减少10倍，训练速度提升98%，成功率提高至50%。

Conclusion: TensorRL-QAS为近量子硬件提供了一种高效且可扩展的量子电路发现协议。

Abstract: Variational quantum algorithms hold the promise to address meaningful quantum
problems already on noisy intermediate-scale quantum hardware, but they face
the challenge of designing quantum circuits that both solve the target problem
and comply with device limitations. Quantum architecture search (QAS) automates
this design process, with reinforcement learning (RL) emerging as a promising
approach. Yet, RL-based QAS methods encounter significant scalability issues,
as computational and training costs grow rapidly with the number of qubits,
circuit depth, and noise, severely impacting performance. To address these
challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that
combines tensor network (TN) methods with RL for designing quantum circuits. By
warm-starting the architecture search with a matrix product state approximation
of the target solution, TensorRL-QAS effectively narrows the search space to
physically meaningful circuits, accelerating convergence to the desired
solution. Tested on several quantum chemistry problems of up to 12-qubit,
TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth
compared to baseline methods, while maintaining or surpassing chemical
accuracy. It reduces function evaluations by up to 100-fold, accelerates
training episodes by up to $98\%$, and achieves up to $50\%$ success
probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline
approaches. Robustness and versatility are demonstrated both in the noiseless
and noisy scenarios, where we report a simulation of up to 8-qubit. These
advancements establish TensorRL-QAS as a promising candidate for a scalable and
efficient quantum circuit discovery protocol on near-term quantum hardware.

</details>


### [322] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

Relevance: 30.0

TL;DR: 论文提出了一种混合量子-经典框架Quantum-Train（QT），并引入Quantum Parameter Adaptation（QPA）用于台风轨迹预测，减少了可训练参数数量，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对灾害准备至关重要，但现有方法计算资源需求高，QT框架通过量子神经网络（QNNs）在训练时生成参数，避免推理时依赖量子硬件，提供了一种高效解决方案。

Method: 结合QT框架和Attention-based Multi-ConvGRU模型，提出QPA方法，实现参数高效训练。

Result: QPA显著减少了可训练参数数量，同时保持了预测性能，为气候建模提供了可扩展且节能的方法。

Conclusion: QPA首次将量子机器学习应用于大规模台风轨迹预测，展示了混合量子-经典学习在高效气候建模中的潜力。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


### [323] [Quantum state-agnostic work extraction (almost) without dissipation](https://arxiv.org/abs/2505.09456)
*Josep Lumbreras,Ruo Cheng Huang,Yanglin Hu,Mile Gu,Marco Tomamichel*

Main category: quant-ph

Relevance: 30.0

TL;DR: 论文研究了通过强化学习中的探索-利用权衡，设计自适应策略以优化从未知纯量子比特状态中提取能量的协议，实现了能量耗散的多对数缩放。


<details>
  <summary>Details</summary>
Motivation: 当前基于完整状态层析的协议在能量提取效率上存在局限性，研究旨在通过强化学习改进这一过程。

Method: 利用强化学习中的探索-利用权衡，设计自适应策略来优化能量提取和信息获取的平衡。

Result: 提出的策略实现了能量耗散的多对数缩放，相比现有协议有指数级改进。

Conclusion: 强化学习方法显著提升了从量子比特状态中提取能量的效率。

Abstract: We investigate work extraction protocols designed to transfer the maximum
possible energy to a battery using sequential access to $N$ copies of an
unknown pure qubit state. The core challenge is designing interactions to
optimally balance two competing goals: charging of the battery optimally using
the qubit in hand, and acquiring more information by qubit to improve energy
harvesting in subsequent rounds. Here, we leverage exploration-exploitation
trade-off in reinforcement learning to develop adaptive strategies achieving
energy dissipation that scales only poly-logarithmically in $N$. This
represents an exponential improvement over current protocols based on full
state tomography.

</details>


### [324] [FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations](https://arxiv.org/abs/2505.08904)
*Varun Nagaraj Rao,Samantha Dalal,Andrew Schwartz,Amna Liaqat,Dana Calacci,Andrés Monroy-Hernández*

Main category: cs.CY

Relevance: 20.0

TL;DR: 论文介绍了FareShare工具，用于自动化计算被平台停用的网约车司机的工资损失，显著提高了效率并减少了错误。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决网约车司机因AI和算法决策被突然停用平台而导致的工资损失问题，缺乏有效工具支持复杂的申诉流程。

Method: 通过与华盛顿州最大的网约车工会合作，设计了FareShare工具，自动化工资损失计算，并在3个月的实地部署中测试其效果。

Result: 工具将工资损失计算时间减少了95%以上，消除了手动数据输入错误，并帮助法律团队更高效地生成仲裁报告。

Conclusion: FareShare在提高效率的同时，也揭示了在劳动场景中信任、同意和工具采用的社会技术挑战。

Abstract: What happens when a rideshare driver is suddenly locked out of the platform
connecting them to riders, wages, and daily work? Deactivation-the abrupt
removal of gig workers' platform access-typically occurs through arbitrary AI
and algorithmic decisions with little explanation or recourse. This represents
one of the most severe forms of algorithmic control and often devastates
workers' financial stability. Recent U.S. state policies now mandate appeals
processes and recovering compensation during the period of wrongful
deactivation based on past earnings. Yet, labor organizers still lack effective
tools to support these complex, error-prone workflows. We designed FareShare, a
computational tool automating lost wage estimation for deactivated drivers,
through a 6 month partnership with the State of Washington's largest rideshare
labor union. Over the following 3 months, our field deployment of FareShare
registered 178 account signups. We observed that the tool could reduce lost
wage calculation time by over 95%, eliminate manual data entry errors, and
enable legal teams to generate arbitration-ready reports more efficiently.
Beyond these gains, the deployment also surfaced important socio-technical
challenges around trust, consent, and tool adoption in high-stakes labor
contexts.

</details>


### [325] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)
*Yu Lun Hsu,Yun-Rung Chou,Chiao-Ju Chang,Yu-Cheng Chang,Zer-Wei Lee,Rokas Gipiškis,Rachel Li,Chih-Yuan Shih,Jen-Kuei Peng,Hsien-Liang Huang,Jaw-Shiun Tsai,Mike Y. Chen*

Main category: cs.HC

Relevance: 20.0

TL;DR: 论文介绍了一个名为PreCare的AI驱动网站，旨在通过三个AI助手帮助用户探索个人价值观、获取ACP知识并支持决策，弥补在线ACP的不足。


<details>
  <summary>Details</summary>
Motivation: 在线ACP虽然便捷，但缺乏临床咨询的个性化价值探索和决策后果澄清等关键优势，因此需要改进。

Method: 通过两项形成性研究（观察访谈ACP团队和网站用户）设计PreCare，并进行可用性和比较评估。

Result: PreCare在可用性测试中表现优秀（SUS评分高），AI助手显著提升用户价值探索、知识获取和决策信心，92%参与者更偏好PreCare。

Conclusion: PreCare通过AI助手有效弥补了在线ACP的不足，提升了用户体验和决策支持。

Abstract: Advance Care Planning (ACP) allows individuals to specify their preferred
end-of-life life-sustaining treatments before they become incapacitated by
injury or terminal illness (e.g., coma, cancer, dementia). While online ACP
offers high accessibility, it lacks key benefits of clinical consultations,
including personalized value exploration, immediate clarification of decision
consequences. To bridge this gap, we conducted two formative studies: 1)
shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and
social workers (18 patients total), and 2) interviewed 14 users of ACP
websites. Building on these insights, we designed PreCare in collaboration with
6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed
to guide users through exploring personal values, gaining ACP knowledge, and
supporting informed decision-making. A usability study (n=12) showed that
PreCare achieved a System Usability Scale (SUS) rating of excellent. A
comparative evaluation (n=12) showed that PreCare's AI assistants significantly
improved exploration of personal values, knowledge, and decisional confidence,
and was preferred by 92% of participants.

</details>


### [326] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/abs/2505.09382)
*Zhengyan Sheng,Jinghao He,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

Relevance: 20.0

TL;DR: VtaD 2025挑战赛旨在通过感官描述符（如明亮、粗糙、柔软等）以比较方式解释声音音色属性，并计划在2025年10月的NCMMSC2025会议上展示成果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过人类感知描述符来量化和解释声音音色的独特属性，以提升对声音质量的科学理解。

Method: 方法包括使用感官描述符对声音音色进行比较分析，并通过挑战赛形式收集和评估数据。

Result: 预期结果包括对声音音色属性的详细解释和比较分析。

Conclusion: 结论是通过VtaD挑战赛推动声音音色属性的科学研究和应用。

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [327] [Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America](https://arxiv.org/abs/2505.08841)
*Andrea Cremaschi,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.CY

Relevance: 20.0

TL;DR: 研究分析了拉丁美洲公众对人工智能和机器人导致失业的担忧，发现教育水平和政治倾向是主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 了解人工智能和机器人对全球劳动力市场的影响，特别是在新兴经济体中公众的态度演变。

Method: 使用拉丁美洲晴雨表2017-2023年的调查数据，通过统计建模和潜在类别分析。

Result: 发现2018年担忧达到峰值，教育水平和政治倾向是最一致的预测因素。

Conclusion: 研究揭示了新兴经济体中AI焦虑的社会和结构性维度。

Abstract: As artificial intelligence and robotics increasingly reshape the global labor
market, understanding public perceptions of these technologies becomes
critical. We examine how these perceptions have evolved across Latin America,
using survey data from the 2017, 2018, 2020, and 2023 waves of the
Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16
countries, we analyze fear of job loss due to artificial intelligence and
robotics. Using statistical modeling and latent class analysis, we identify key
structural and ideological predictors of concern, with education level and
political orientation emerging as the most consistent drivers. Our findings
reveal substantial temporal and cross-country variation, with a notable peak in
fear during 2018 and distinct attitudinal profiles emerging from latent
segmentation. These results offer new insights into the social and structural
dimensions of AI anxiety in emerging economies and contribute to a broader
understanding of public attitudes toward automation beyond the Global North.

</details>


### [328] [When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes](https://arxiv.org/abs/2505.08918)
*Marina Popova,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: q-bio.GN

Relevance: 20.0

TL;DR: 研究探讨了在基因组序列中使用Byte Pair Encoding（BPE）进行分词的效果，发现共享词汇量随基因组比较增加而快速下降，且基于分词重叠的系统发育树未能准确反映灵长类关系。


<details>
  <summary>Details</summary>
Motivation: 探索基因组序列的有效分词策略，以支持大规模基因组语言模型的开发。

Method: 使用自定义工具dnaBPE对9个T2T灵长类基因组（包括3个人类基因组）训练独立的BPE分词器，固定词汇量为512,000。

Result: 仅11,569个分词在所有基因组中共享，而991,854个分词为单一基因组特有；基于分词重叠的系统发育树未能准确反映灵长类关系。

Conclusion: BPE分词在压缩重复序列方面有效，但对高拷贝重复元素的敏感性限制了其在比较基因组学中的通用性。建议采用混合策略和重复屏蔽方法优化分词。

Abstract: The emergence of telomere-to-telomere (T2T) genome assemblies has opened new
avenues for comparative genomics, yet effective tokenization strategies for
genomic sequences remain underexplored. In this pilot study, we apply Byte Pair
Encoding (BPE) to nine T2T primate genomes including three human assemblies by
training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens
using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are
shared across all assemblies, while nearly 991,854 tokens are unique to a
single genome, indicating a rapid decline in shared vocabulary with increasing
assembly comparisons. Moreover, phylogenetic trees derived from token overlap
failed to recapitulate established primate relationships, a discrepancy
attributed to the disproportionate influence of species-specific high-copy
repetitive elements. These findings underscore the dual nature of BPE
tokenization: while it effectively compresses repetitive sequences, its
sensitivity to high-copy elements limits its utility as a universal tool for
comparative genomics. We discuss potential hybrid strategies and repeat-masking
approaches to refine genomic tokenization, emphasizing the need for
domain-specific adaptations in the development of large-scale genomic language
models. The dnaBPE tool used in this study is open-source and available at
https://github.com/aglabx/dnaBPE.

</details>


### [329] [Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919)
*Kangxian Xie,Yufei Zhu,Kaiming Kuang,Li Zhang,Hongwei Bran Li,Mingchen Gao,Jiancheng Yang*

Main category: cs.GR

Relevance: 20.0

TL;DR: 提出了一种基于神经隐式函数的方法，用于高质量3D肺段重建，并通过临床相关指标评估。同时发布了一个名为Lung3D的数据集。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习方法在肺段重建中因分辨率需求导致的资源限制和粒度不足问题。

Method: 使用神经隐式函数学习3D表面，通过变形可学习模板实现解剖感知的精确重建。

Result: 提出的方法优于现有方法，为肺段重建提供了新视角。

Conclusion: 该方法在计算效率和分辨率上表现优异，同时发布的Lung3D数据集填补了公开数据空白。

Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in
segmentectomy and surgical treatment planning for lung cancer. Due to the
resolution requirement of the target reconstruction, conventional deep
learning-based methods often suffer from computational resource constraints or
limited granularity. Conversely, implicit modeling is favored due to its
computational efficiency and continuous representation at any resolution. We
propose a neural implicit function-based method to learn a 3D surface to
achieve anatomy-aware, precise pulmonary segment reconstruction, represented as
a shape by deforming a learnable template. Additionally, we introduce two
clinically relevant evaluation metrics to assess the reconstruction
comprehensively. Further, due to the absence of publicly available shape
datasets to benchmark reconstruction algorithms, we developed a shape dataset
named Lung3D, including the 3D models of 800 labeled pulmonary segments and the
corresponding airways, arteries, veins, and intersegmental veins. We
demonstrate that the proposed approach outperforms existing methods, providing
a new perspective for pulmonary segment reconstruction. Code and data will be
available at https://github.com/M3DV/ImPulSe.

</details>


### [330] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)
*Yu Lun Hsu,Yun-Rung Chou,Chiao-Ju Chang,Yu-Cheng Chang,Zer-Wei Lee,Rokas Gipiškis,Rachel Li,Chih-Yuan Shih,Jen-Kuei Peng,Hsien-Liang Huang,Jaw-Shiun Tsai,Mike Y. Chen*

Main category: cs.HC

Relevance: 20.0

TL;DR: 论文介绍了PreCare，一个结合AI助手的网站，旨在弥补在线临终关怀计划（ACP）与临床咨询之间的差距，通过帮助用户探索个人价值观、获取知识并支持决策。


<details>
  <summary>Details</summary>
Motivation: 在线ACP虽然便捷，但缺乏临床咨询的个性化价值探索和即时澄清决策后果等优势。

Method: 通过两项形成性研究（观察ACP团队和访谈ACP网站用户）设计PreCare，并进行了可用性研究和比较评估。

Result: PreCare在可用性研究中获得优秀评分，AI助手显著提升了用户对个人价值观的探索、知识获取和决策信心，92%的参与者更偏好PreCare。

Conclusion: PreCare通过AI助手有效弥补了在线ACP的不足，提升了用户体验和决策支持。

Abstract: Advance Care Planning (ACP) allows individuals to specify their preferred
end-of-life life-sustaining treatments before they become incapacitated by
injury or terminal illness (e.g., coma, cancer, dementia). While online ACP
offers high accessibility, it lacks key benefits of clinical consultations,
including personalized value exploration, immediate clarification of decision
consequences. To bridge this gap, we conducted two formative studies: 1)
shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and
social workers (18 patients total), and 2) interviewed 14 users of ACP
websites. Building on these insights, we designed PreCare in collaboration with
6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed
to guide users through exploring personal values, gaining ACP knowledge, and
supporting informed decision-making. A usability study (n=12) showed that
PreCare achieved a System Usability Scale (SUS) rating of excellent. A
comparative evaluation (n=12) showed that PreCare's AI assistants significantly
improved exploration of personal values, knowledge, and decisional confidence,
and was preferred by 92% of participants.

</details>


### [331] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/abs/2505.09382)
*Zhengyan Sheng,Jinghao He,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

Relevance: 20.0

TL;DR: 论文介绍了2025年声音音色属性检测挑战（VtaD），旨在通过感官描述符（如明亮、粗糙、柔和等）对人类感知的声音音色进行比较性解释。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解和解释人类对声音音色的感知，通过比较两个声音在特定描述符维度上的强度来实现。

Method: 使用感官描述符对声音音色进行比较性分析，具体方法未在摘要中详细说明。

Result: 挑战赛将于2025年5月启动，最终成果将在2025年10月的NCMMSC2025会议上展示。

Conclusion: 该挑战旨在推动声音音色属性的解释和理解，可能为语音处理领域提供新的研究方向。

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [332] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/abs/2505.09565)
*Maik Dannecker,Thomas Sanchez,Meritxell Bach Cuadra,Özgün Turgut,Anthony N. Price,Lucilio Cordero-Grande,Vanessa Kyriakopoulou,Joseph V. Hajnal,Daniel Rueckert*

Main category: eess.IV

Relevance: 20.0

TL;DR: 提出了一种基于隐式神经表示的新型切片到体积重建方法，用于运动伪影严重的MRI图像重建，显著提升了重建质量和速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在严重运动伪影和图像损坏情况下的MRI重建性能不足问题。

Method: 采用隐式神经表示进行运动校正、异常值处理和高分辨率重建，并通过自监督元学习初始化任务特定先验。

Result: 在模拟和临床MRI数据上验证了方法有效性，重建质量提升且时间减少50%。

Conclusion: 该方法在严重运动伪影情况下优于现有技术，具有高效和准确的MRI重建潜力。

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


### [333] [Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication](https://arxiv.org/abs/2505.08810)
*Bappa Muktar,Vincent Fono,Adama Nouboukpo*

Main category: cs.CR

Relevance: 10.0

TL;DR: 本文提出了一种用于检测高速公路VANET中DDoS攻击的框架，通过合成数据集和多种分类器评估，XGBoost和CatBoost表现最佳，F1-score达96%。


<details>
  <summary>Details</summary>
Motivation: VANET在智能交通系统中至关重要，但DDoS攻击会威胁其可靠性，因此需要一种鲁棒且可扩展的检测方法。

Method: 使用NS-3和SUMO构建合成数据集，结合真实交通数据，通过预处理和特征工程后，评估11种分类器性能。

Result: XGBoost和CatBoost表现最佳，F1-score均为96%。

Conclusion: 该框架具有鲁棒性，适合实时部署以保护VANET中的关键通信。

Abstract: Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent
Transportation Systems (ITS), particularly in enabling real-time communication
for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,
which interfere with safety-critical communication channels, can severely
impair their reliability. This study introduces a robust and scalable framework
to detect DDoS attacks in highway-based VANET environments. A synthetic dataset
was constructed using Network Simulator 3 (NS-3) in conjunction with the
Simulation of Urban Mobility (SUMO) and further enriched with real-world
mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).
Three traffic categories were simulated: DDoS, VoIP, and TCP-based video
streaming (VideoTCP). The data preprocessing pipeline included normalization,
signal-to-noise ratio (SNR) feature engineering, missing value imputation, and
class balancing using the Synthetic Minority Over-sampling Technique (SMOTE).
Feature importance was assessed using SHapley Additive exPlanations (SHAP).
Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),
AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).
XGB and CB achieved the best performance, each attaining an F1-score of 96%.
These results highlight the robustness of the proposed framework and its
potential for real-time deployment in VANETs to secure critical emergency
communications.

</details>


### [334] [UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units](https://arxiv.org/abs/2505.09393)
*Huakun Liu,Hiroki Ota,Xin Wei,Yutaro Hirao,Monica Perusquia-Hernandez,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: cs.GR

Relevance: 10.0

TL;DR: UMotion是一个基于不确定性驱动的在线融合状态估计框架，结合IMU和UWB传感器，用于3D人体形状和姿态估计，解决了姿态模糊和数据漂移问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏可穿戴IMU在3D人体运动估计中存在姿态模糊、数据漂移和适应性差的问题，需要一种更鲁棒的解决方案。

Method: 提出UMotion框架，结合IMU和UWB传感器，通过UKF实时融合传感器数据和人体运动约束。

Result: 实验表明UMotion能稳定传感器数据，并在姿态估计精度上优于现有方法。

Conclusion: UMotion通过多传感器融合和不确定性建模，显著提升了3D人体姿态估计的鲁棒性和准确性。

Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for
estimating 3D human motion. However, challenges such as pose ambiguity, data
drift, and limited adaptability to diverse bodies persist. To address these
issues, we propose UMotion, an uncertainty-driven, online fusing-all state
estimation framework for 3D human shape and pose estimation, supported by six
integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB
sensors measure inter-node distances to infer spatial relationships, aiding in
resolving pose ambiguities and body shape variations when combined with
anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors
are affected by body occlusions. Consequently, we develop a tightly coupled
Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor
data and estimated human motion based on individual body shape. The UKF
iteratively refines IMU and UWB measurements by aligning them with uncertain
human motion constraints in real-time, producing optimal estimates for each.
Experiments on both synthetic and real-world datasets demonstrate the
effectiveness of UMotion in stabilizing sensor data and the improvement over
state of the art in pose accuracy.

</details>


### [335] [Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication](https://arxiv.org/abs/2505.08810)
*Bappa Muktar,Vincent Fono,Adama Nouboukpo*

Main category: cs.CR

Relevance: 10.0

TL;DR: 论文提出了一种用于检测高速公路VANET中DDoS攻击的框架，通过合成数据集和多种分类器评估，XGBoost和CatBoost表现最佳，F1分数达96%。


<details>
  <summary>Details</summary>
Motivation: VANET在智能交通系统中至关重要，但DDoS攻击会威胁其可靠性，因此需要一种鲁棒且可扩展的检测方法。

Method: 使用NS-3和SUMO构建合成数据集，结合真实交通数据，通过预处理和特征工程后，评估11种分类器性能。

Result: XGBoost和CatBoost表现最佳，F1分数为96%，证明了框架的鲁棒性。

Conclusion: 该框架具有实时部署潜力，可保障VANET中关键应急通信的安全。

Abstract: Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent
Transportation Systems (ITS), particularly in enabling real-time communication
for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,
which interfere with safety-critical communication channels, can severely
impair their reliability. This study introduces a robust and scalable framework
to detect DDoS attacks in highway-based VANET environments. A synthetic dataset
was constructed using Network Simulator 3 (NS-3) in conjunction with the
Simulation of Urban Mobility (SUMO) and further enriched with real-world
mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).
Three traffic categories were simulated: DDoS, VoIP, and TCP-based video
streaming (VideoTCP). The data preprocessing pipeline included normalization,
signal-to-noise ratio (SNR) feature engineering, missing value imputation, and
class balancing using the Synthetic Minority Over-sampling Technique (SMOTE).
Feature importance was assessed using SHapley Additive exPlanations (SHAP).
Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),
AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).
XGB and CB achieved the best performance, each attaining an F1-score of 96%.
These results highlight the robustness of the proposed framework and its
potential for real-time deployment in VANETs to secure critical emergency
communications.

</details>


### [336] [UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units](https://arxiv.org/abs/2505.09393)
*Huakun Liu,Hiroki Ota,Xin Wei,Yutaro Hirao,Monica Perusquia-Hernandez,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: cs.GR

Relevance: 10.0

TL;DR: UMotion是一个基于不确定性驱动的在线融合状态估计框架，用于3D人体形状和姿态估计，结合了IMU和UWB传感器数据，通过UKF框架优化测量结果。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏可穿戴IMU在3D人体运动估计中存在的姿态模糊、数据漂移和适应性不足等问题。

Method: 提出UMotion框架，结合IMU和UWB传感器数据，使用UKF实时融合不确定性并优化测量结果。

Result: 在合成和真实数据集上验证了UMotion在稳定传感器数据和姿态精度上的优越性。

Conclusion: UMotion通过融合多传感器数据和不确定性驱动的方法，显著提升了3D人体运动估计的准确性。

Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for
estimating 3D human motion. However, challenges such as pose ambiguity, data
drift, and limited adaptability to diverse bodies persist. To address these
issues, we propose UMotion, an uncertainty-driven, online fusing-all state
estimation framework for 3D human shape and pose estimation, supported by six
integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB
sensors measure inter-node distances to infer spatial relationships, aiding in
resolving pose ambiguities and body shape variations when combined with
anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors
are affected by body occlusions. Consequently, we develop a tightly coupled
Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor
data and estimated human motion based on individual body shape. The UKF
iteratively refines IMU and UWB measurements by aligning them with uncertain
human motion constraints in real-time, producing optimal estimates for each.
Experiments on both synthetic and real-world datasets demonstrate the
effectiveness of UMotion in stabilizing sensor data and the improvement over
state of the art in pose accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [337] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

Relevance: 90.0

TL;DR: 大型语言模型通过自我评判实现自我改进，无需参考解决方案，在Countdown谜题和MIT积分比赛中验证了其有效性。结合合成问题生成，实现了8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在缺乏参考解决方案或复杂评估需求的领域中的自我改进能力，以解决强化学习环境中的奖励信号生成难题。

Method: 利用生成与验证解决方案的不对称性，通过自我评判提供可靠的奖励信号，结合合成问题生成实现完整的自我改进循环。

Result: 在Qwen 2.5 7B上实现了8%的性能提升，并在积分任务中超越GPT-4o。

Conclusion: LLM的自我评判能力为强化学习提供了有效的奖励信号，可能推动AI系统向自我导向学习范式转变。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [338] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

Relevance: 90.0

TL;DR: 论文提出了一种通过自我评判实现大语言模型自我改进的方法，无需参考解决方案，实验证明在Countdown谜题和MIT积分问题上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型如何通过自我评判实现自我改进，解决传统强化学习中奖励信号难以获取的问题。

Method: 利用生成与验证解决方案的不对称性，模型自我生成问题、解决并评估性能，形成完整的自我改进循环。

Result: 实验显示，Qwen 2.5 7B模型性能提升8%，在积分任务上超越GPT-4o。

Conclusion: 自我评判为模型训练提供了有效的奖励信号，可能推动AI系统通过自主学习持续改进。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [339] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/abs/2505.08803)
*Zizhao Hu,Mohammad Rostami,Jesse Thomason*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究了多模态生成模型在自生成数据训练中的性能退化（模型崩溃）问题，发现其与单模态模型不同，并提出缓解方法。


<details>
  <summary>Details</summary>
Motivation: 探索多模态AI系统中模型崩溃的特性及其缓解策略，以应对现实场景中多模态数据的复杂性。

Method: 扩展研究至多模态视觉-语言生成系统（如VLMs和扩散模型），分析递归生成-训练循环中的模型崩溃现象。

Result: 多模态模型崩溃表现出独特特征（如视觉-语言对齐改善），并提出解码预算增加、模型多样性等方法有效缓解。

Conclusion: 为自改进多模态AI系统和合成数据集的鲁棒性提供初步见解和实用指南。

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [340] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
*Cody Steinmetz,Gavin Childress,Aaron Herbst,Gavin Jones,Jasdeep Singh,Eli Vang,Keagan Weinstock*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种通过RMS归一化和渐进式量化方法，将全精度LLM稳定微调为三元（2位）量化的方法，无需额外训练或增加模型复杂度。


<details>
  <summary>Details</summary>
Motivation: LLM的实际部署成本高，量化虽能减少内存和计算需求，但通常导致精度下降。现有方法如量化感知训练能恢复性能但需额外训练，而三元量化虽节省更多但稳定性差。

Method: 在全精度LLM中，每个线性投影前插入RMS归一化，并采用渐进式分层量化计划，实现稳定微调。

Result: 在标准语言建模基准测试中，该方法匹配或超越了更复杂的知识蒸馏流程，且未增加模型复杂度。

Conclusion: 研究表明，仅通过精心设计的归一化即可显著缩小三元与全精度LLM间的精度差距，使超低比特推理实用化。

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [341] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/abs/2505.08977)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种通用方法SaFARi，用于构建基于任意框架或基的SSM，突破了现有SSM仅依赖多项式基的限制。


<details>
  <summary>Details</summary>
Motivation: 当前SSM的实现仅基于少数多项式基，限制了其表达能力。本文旨在扩展SSM的适用范围，使其能够利用任意框架或基。

Method: 提出SaFARi框架，支持在SSM中使用任意框架或基，包括但不限于HiPPO方法。

Result: SaFARi框架为SSM提供了更广泛的基选择，增强了模型的表达能力。

Conclusion: SaFARi为SSM的设计提供了更大的灵活性，有望在长程依赖数据建模中发挥更大作用。

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [342] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/abs/2505.09022)
*Annan Yu,N. Benjamin Erichson*

Main category: cs.LG

Relevance: 85.0

TL;DR: Mamba扩展了状态空间模型（SSMs），但在长序列任务中表现不佳。本文通过理论分析提出改进方法B2S6，提升了性能。


<details>
  <summary>Details</summary>
Motivation: Mamba在长序列任务中的表现与其架构设计不符，影响了其通用性和多功能性。

Method: 从表达能力、归纳偏差和训练稳定性三个角度分析Mamba的局限性，并提出改进方法B2S6。

Result: B2S6在长序列任务中优于S4和S4D，同时保持语言建模性能。

Conclusion: B2S6解决了Mamba的局限性，提升了其在长序列任务中的表现。

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [343] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)
*Timothy Qian,Vinith Suriyakumar,Ashia Wilson,Dylan Hadfield-Menell*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文研究了后训练方法（如微调、对齐和遗忘）如何改变语言模型的行为和表示，并探讨了这些修改的脆弱性。提出了一种分层遗忘（LU）算法，通过抑制机制提高对抗性重新学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解后训练方法对语言模型的影响及其脆弱性，探索如何提高这些方法的鲁棒性。

Method: 设计了一种分层遗忘（LU）算法，通过分阶段抑制数据子集来限制重新学习的能力。

Result: 实验表明，LU提高了对抗性重新学习的鲁棒性，为机器遗忘领域提供了新见解。

Conclusion: LU为后训练更新提供了更鲁棒的方法，并揭示了其行为改变的机制。

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [344] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602)
*David Khachaturov,Robert Mullins*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为Adversarial Suffix Filtering (ASF)的轻量级防御方法，用于保护大语言模型（LLMs）免受对抗性后缀攻击。ASF作为输入预处理和净化器，能有效检测并过滤恶意后缀，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: LLMs在自主系统和公共环境中广泛应用，但其易受对抗性攻击（如jailbreak漏洞）影响，现有防御方法存在局限性。

Method: 提出ASF，一种模型无关的防御管道，通过预处理和过滤对抗性后缀来保护LLMs。

Result: ASF在黑白盒攻击场景下均有效，将攻击成功率降至4%以下，同时不影响模型在非对抗性场景的性能。

Conclusion: ASF是一种高效且轻量级的防御方案，适用于保护LLMs免受对抗性后缀攻击。

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


### [345] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/abs/2505.08803)
*Zizhao Hu,Mohammad Rostami,Jesse Thomason*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究了多模态生成模型在自生成数据持续训练中的性能退化（模型崩溃）问题，发现其与单模态模型不同，并提出缓解方法。


<details>
  <summary>Details</summary>
Motivation: 探索多模态生成模型在自生成数据训练中的模型崩溃现象，填补现有研究空白。

Method: 扩展研究到多模态视觉语言生成系统（如VLMs和扩散模型），分析递归生成-训练循环中的模型崩溃特性。

Result: 多模态模型崩溃表现出独特特征（如视觉语言对齐改善），并提出解码预算增加、模型多样性等方法缓解崩溃。

Conclusion: 为多模态自改进AI系统和合成数据集的稳健性提供初步见解和实用指南。

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [346] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
*Cody Steinmetz,Gavin Childress,Aaron Herbst,Gavin Jones,Jasdeep Singh,Eli Vang,Keagan Weinstock*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种通过RMS归一化和渐进式量化方法，将全精度LLM稳定地微调为三元（2位）量化的方法，显著降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的实际部署成本高昂，量化是降低内存和计算需求的有效方法，但传统量化方法在低比特（如三元）时不稳定且性能下降严重。

Method: 在每层线性投影前插入RMS归一化，并采用渐进式分层量化策略，直接从全精度检查点微调为三元LLM。

Result: 该方法在标准语言建模基准测试中表现优于或等同于复杂的知识蒸馏方法，且未增加模型复杂度。

Conclusion: 研究表明，仅通过精心设计的归一化即可显著缩小三元与全精度LLM之间的性能差距，使超低比特推理变得可行。

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [347] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/abs/2505.08977)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种通用方法SaFARi，用于构建基于任意框架或基的SSM，突破了现有方法仅限于多项式基的限制。


<details>
  <summary>Details</summary>
Motivation: 当前SSM的实现仅基于少数多项式基，限制了其灵活性和多样性。本文旨在扩展SSM的适用范围，使其能够利用任意框架或基进行建模。

Method: 提出SaFARi框架，允许使用任意框架或基构建SSM，而不仅限于多项式基。该方法扩展了HiPPO等现有方法。

Result: SaFARi框架能够生成无限多样的SSM架构，为长程依赖数据的建模提供了更灵活的工具。

Conclusion: SaFARi为SSM的设计提供了更大的自由度，有望推动其在机器学习中的应用。

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [348] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/abs/2505.09022)
*Annan Yu,N. Benjamin Erichson*

Main category: cs.LG

Relevance: 85.0

TL;DR: Mamba模型在长序列任务中表现不佳，作者通过理论分析提出改进方案B2S6，提升了性能。


<details>
  <summary>Details</summary>
Motivation: Mamba在长序列任务中的表现与其设计初衷不符，影响了其通用性。

Method: 从表达能力、归纳偏置和训练稳定性三个角度分析Mamba的不足，并提出B2S6改进方案。

Result: B2S6在长序列任务中优于S4和S4D，同时保持语言建模性能。

Conclusion: B2S6解决了Mamba的长序列任务问题，提升了其通用性。

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [349] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)
*Timothy Qian,Vinith Suriyakumar,Ashia Wilson,Dylan Hadfield-Menell*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文研究了后训练方法（如微调、对齐和遗忘）如何修改语言模型的行为和表示，并提出了分层遗忘（LU）算法以提高对抗性重新学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 后训练方法对语言模型的修改具有脆弱性，容易被绕过或重新学习。研究旨在探索这种脆弱性的原因并提出改进方法。

Method: 设计了分层遗忘（LU）算法，通过分阶段遗忘数据子集来抑制重新学习的能力，并在合成和LLM实验中评估其效果。

Result: LU提高了多种遗忘方法对抗对抗性重新学习的鲁棒性。

Conclusion: LU为机器遗忘提供了新方法，并揭示了后训练更新的影响。

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [350] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602)
*David Khachaturov,Robert Mullins*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为Adversarial Suffix Filtering (ASF)的轻量级防御方法，用于保护大型语言模型免受对抗性后缀攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自主系统和公共环境中广泛应用，但其易受越狱漏洞攻击，现有防御方法存在局限性。

Method: ASF作为输入预处理器和净化器，检测并过滤对抗性后缀，无需依赖模型内部架构。

Result: ASF将最先进对抗性后缀攻击的有效性降至4%以下，且对模型在非对抗场景下的性能影响极小。

Conclusion: ASF是一种模型无关的防御方案，适用于黑盒和白盒攻击场景，显著提升LLM的安全性和可信度。

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


### [351] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/abs/2505.08829)
*David Kinney*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出了一种通过线性组合准确性和公平性指标来评估预测算法的方法，以解决两者之间的权衡问题，并基于Harsanyi的偏好聚合理论进行了形式化论证。


<details>
  <summary>Details</summary>
Motivation: 随着AI预测算法的快速发展，如何在准确性和公平性之间找到平衡成为重要挑战。本文旨在提供一种方法来解决这一权衡问题。

Method: 使用线性组合准确性和公平性指标，基于Harsanyi的偏好聚合理论进行形式化论证，并在COMPAS数据集上验证。

Result: 提出了一种有效的方法来评估预测算法的综合价值，适用于同时关注准确性和公平性的场景。

Conclusion: 线性组合方法为解决准确性和公平性之间的权衡提供了理论支持，并在实际数据中验证了其有效性。

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [352] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/abs/2505.09085)
*Jiaxuan Chen,Yu Qi,Yueming Wang,Gang Pan*

Main category: cs.LG

Relevance: 75.0

TL;DR: 利用脑信号监督学习增强大型语言模型的认知能力，提升抽象概念理解和任务表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言模型在自然语言理解方面表现出色，但在抽象概念推理和适应新场景等复杂认知能力上仍有不足。研究旨在通过人类脑信号监督学习提升模型的认知能力。

Method: 采用脑信号监督学习（brain-in-the-loop supervised learning），利用少量脑信号将人类概念结构迁移到深度神经网络中。

Result: 实验表明，该方法显著提升了模型对抽象和未见概念的理解能力，在少样本/零样本学习和分布外识别任务中表现优异，同时生成可解释的概念表示。

Conclusion: 人类监督学习可有效增强大型模型的复杂认知能力，为开发更接近人类认知的人工系统提供了新途径。

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [353] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
*Raghav Garg,Kapil Sharma,Karan Gupta*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出CXMArena，一个用于评估LLM在客户体验管理（CXM）中实用性的合成基准数据集，填补了现有基准在真实性和任务多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估LLM在复杂操作环境中的实用性时存在数据稀缺和缺乏真实性的问题，CXMArena旨在解决这些问题。

Method: 通过LLM驱动的管道生成模拟品牌CXM实体的合成数据集，包括知识文章和对话，并通过噪声注入和自动化验证增强真实性。

Result: 基线实验显示，即使是先进模型在文章搜索和知识库细化等任务上表现不佳（68%准确率和F1 0.3），凸显了当前模型的挑战。

Conclusion: CXMArena为LLM在CXM中的评估提供了更真实的基准，揭示了当前模型在复杂任务中的局限性。

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [354] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/abs/2505.09486)
*Seyed Roozbeh Razavi Rohani,Khashayar Khajavi,Wesley Chung,Mo Chen,Sharan Vaswani*

Main category: cs.LG

Relevance: 75.0

TL;DR: AdaLin是一种动态调整神经元激活函数的方法，旨在缓解深度神经网络中的可塑性损失问题，适用于非稳态问题场景。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在非稳态问题中容易失去可塑性，而深度线性网络对此具有较强鲁棒性。AdaLin通过动态调整激活函数来维持梯度信号，避免可塑性损失。

Method: AdaLin为每个神经元引入可学习参数和门控机制，动态注入线性性，无需额外超参数或任务边界。

Result: 在多个标准基准测试（如MNIST、CIFAR-10/100）和复杂场景（如增量学习和强化学习）中，AdaLin显著提升了性能。

Conclusion: AdaLin通过神经元级适应性调整有效缓解可塑性损失，为持续学习提供了新思路。

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [355] [Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks](https://arxiv.org/abs/2505.09326)
*Vincent Abbott,Kotaro Kamiya,Gerard Glowacki,Yu Atsumi,Gioele Zardini,Yoshihiro Maruyama*

Main category: math.CT

Relevance: 75.0

TL;DR: 论文提出了一种基于神经电路图的系统性方法，用于开发高效的新型AI架构，并通过球形注意力算法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动编译方法效率低下，而高效算法需要多年人工开发。研究旨在通过神经电路图提供一种系统化方法，改进深度学习架构的推理和高效代码生成。

Method: 利用基于范畴论的神经电路图，提出球形注意力算法，替代SoftMax的$L^2$范数，并开发高效的FlashSign内核。

Result: FlashSign内核在A100上性能与FlashAttention相当，且比PyTorch快3.6倍。

Conclusion: 神经电路图适合作为自动化开发高效AI架构的高层框架。

Abstract: How do we enable artificial intelligence models to improve themselves? This
is central to exponentially improving generalized artificial intelligence
models, which can improve their own architecture to handle new problem domains
in an efficient manner that leverages the latest hardware. However, current
automated compilation methods are poor, and efficient algorithms require years
of human development. In this paper, we use neural circuit diagrams, based in
category theory, to prove a general theorem related to deep learning
algorithms, guide the development of a novel attention algorithm catered to the
domain of gene regulatory networks, and produce a corresponding efficient
kernel. The algorithm we propose, spherical attention, shows that neural
circuit diagrams enable a principled and systematic method for reasoning about
deep learning architectures and providing high-performance code. By replacing
SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special
function unit bottleneck of standard attention while retaining the streaming
property essential to high-performance. Our diagrammatically derived
\textit{FlashSign} kernel achieves comparable performance to the
state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and
$3.6\times$ the performance of PyTorch. Overall, this investigation shows
neural circuit diagrams' suitability as a high-level framework for the
automated development of efficient, novel artificial intelligence
architectures.

</details>


### [356] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/abs/2505.09003)
*Zeki Doruk Erden,Donia Gasmi,Boi Faltings*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种结合自动编码器和策略优化的持续学习方法，用于强化学习代理的任务和环境识别与知识保留。


<details>
  <summary>Details</summary>
Motivation: 持续学习在强化学习中是一个挑战，尤其是在没有外部信号指示任务或环境变化时，如何保留和利用现有信息。

Method: 采用自动编码器检测新任务并匹配已知环境，结合策略优化构建端到端持续学习系统。

Result: 初步结果显示，系统能在无外部信号的情况下成功实现持续学习，有效识别新任务并保留旧知识。

Conclusion: 该方法展示了在无外部信号情况下持续学习的潜力，为强化学习代理的适应性提供了新思路。

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [357] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/abs/2505.09085)
*Jiaxuan Chen,Yu Qi,Yueming Wang,Gang Pan*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种利用脑信号监督学习的方法，将人类概念结构迁移到DNN中，显著提升了模型对抽象和未见概念的理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言模型在图像和自然语言理解方面表现出色，但在复杂认知能力（如抽象概念理解、推理和适应新场景）方面仍存在挑战。

Method: 采用脑信号监督学习（brain-in-the-loop supervised learning），利用少量脑信号将人类概念结构迁移到DNN中。

Result: 实验表明，该方法显著提升了模型在少样本/零样本学习和分布外识别任务中的性能，并生成了高度可解释的概念表示。

Conclusion: 人类监督学习可以有效增强大模型的复杂认知能力，为开发更具人类认知能力的人工系统提供了新途径。

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [358] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
*Raghav Garg,Kapil Sharma,Karan Gupta*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了CXMArena，一个用于评估AI在客户体验管理（CXM）中实用性的合成基准数据集，解决了现有基准在真实性和任务多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和现有基准的局限性，评估大型语言模型（LLMs）在复杂操作环境中的实用性面临挑战。

Method: 开发了一个可扩展的LLM驱动的管道，模拟品牌CXM实体，并通过噪声注入和自动验证生成真实分布的数据集。

Result: 基线实验显示，即使是先进模型在文章搜索和知识库细化等任务上表现不佳，凸显了当前模型的挑战。

Conclusion: CXMArena为评估LLMs在CXM中的实用性提供了更真实的基准，揭示了现有技术的不足。

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [359] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/abs/2505.09427)
*Achref Doula,Max Mühläuser,Alejandro Sanchez Guinea*

Main category: cs.LG

Relevance: 75.0

TL;DR: SafePath是一个模块化框架，通过共形预测为基于LLM的路径规划提供形式化安全保证，显著降低不确定性和碰撞率。


<details>
  <summary>Details</summary>
Motivation: LLM在自动驾驶路径规划中表现出潜力，但其过度自信和幻觉行为引发安全隐患，需要一种方法确保安全性。

Method: SafePath分三阶段操作：生成多样候选路径，通过共形预测过滤高风险路径，最终选择最安全路径或委托人类控制。

Result: 实验显示，SafePath减少77%的不确定性和70%的碰撞率，显著提升安全性。

Conclusion: SafePath通过形式化安全保证和人类委托机制，有效平衡了LLM路径规划的自主性与安全性。

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [360] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/abs/2505.09486)
*Seyed Roozbeh Razavi Rohani,Khashayar Khajavi,Wesley Chung,Mo Chen,Sharan Vaswani*

Main category: cs.LG

Relevance: 75.0

TL;DR: AdaLin是一种动态调整神经元激活函数的方法，通过引入可学习参数和门控机制，有效缓解深度神经网络中的塑性损失问题，提升持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在非稳态问题中容易出现塑性损失，影响增量学习能力。AdaLin旨在通过自适应线性化解决这一问题。

Method: AdaLin为每个神经元引入可学习参数和门控机制，动态调整激活函数的线性化程度，无需额外超参数或任务边界。

Result: 在多个基准测试（如MNIST、CIFAR-10/100）和复杂场景（如增量学习和强化学习）中，AdaLin显著提升了性能。

Conclusion: AdaLin通过神经元级自适应有效缓解塑性损失，为持续学习提供了新思路。

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [361] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2505.09503)
*Patrik Kenfack,Samira Ebrahimi Kaho,Ulrich Aïvodji*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文研究了表格基础模型在上下文学习（ICL）中的公平性问题，提出了三种预处理策略，发现基于不确定性的演示选择能显著提升群体公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管表格基础模型在结构化数据上表现出强大的上下文学习能力，但其公平性影响尚不明确。本文旨在填补这一空白。

Method: 研究了三种预处理策略：相关性去除、组平衡演示选择和基于不确定性的演示选择。

Result: 实验表明，基于不确定性的演示选择能一致性地提升上下文预测的群体公平性。

Conclusion: 表格ICL中的公平性问题可通过预处理策略缓解，特别是基于不确定性的方法。

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [362] [Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks](https://arxiv.org/abs/2505.09326)
*Vincent Abbott,Kotaro Kamiya,Gerard Glowacki,Yu Atsumi,Gioele Zardini,Yoshihiro Maruyama*

Main category: math.CT

Relevance: 75.0

TL;DR: 论文提出了一种基于神经电路图的系统方法，用于开发高效的新型AI架构，特别是针对基因调控网络的球形注意力算法。


<details>
  <summary>Details</summary>
Motivation: 当前自动编译方法效率低下，需要大量人工开发时间，因此需要一种系统化方法来自动改进AI模型架构。

Method: 使用基于范畴论的神经电路图，提出球形注意力算法，用L2范数替代SoftMax，并开发了高效的FlashSign内核。

Result: FlashSign内核在A100上性能与FlashAttention相当，比PyTorch快3.6倍。

Conclusion: 神经电路图适合作为自动化开发高效AI架构的高层框架。

Abstract: How do we enable artificial intelligence models to improve themselves? This
is central to exponentially improving generalized artificial intelligence
models, which can improve their own architecture to handle new problem domains
in an efficient manner that leverages the latest hardware. However, current
automated compilation methods are poor, and efficient algorithms require years
of human development. In this paper, we use neural circuit diagrams, based in
category theory, to prove a general theorem related to deep learning
algorithms, guide the development of a novel attention algorithm catered to the
domain of gene regulatory networks, and produce a corresponding efficient
kernel. The algorithm we propose, spherical attention, shows that neural
circuit diagrams enable a principled and systematic method for reasoning about
deep learning architectures and providing high-performance code. By replacing
SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special
function unit bottleneck of standard attention while retaining the streaming
property essential to high-performance. Our diagrammatically derived
\textit{FlashSign} kernel achieves comparable performance to the
state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and
$3.6\times$ the performance of PyTorch. Overall, this investigation shows
neural circuit diagrams' suitability as a high-level framework for the
automated development of efficient, novel artificial intelligence
architectures.

</details>


### [363] [Distilling Realizable Students from Unrealizable Teachers](https://arxiv.org/abs/2505.09546)
*Yujin Kim,Nathaniel Chin,Arnav Vasudev,Sanjiban Choudhury*

Main category: cs.RO

Relevance: 75.0

TL;DR: 论文研究了在特权信息下的策略蒸馏，学生策略仅能部分观察，需从全状态访问的教师中学习。提出两种方法：模仿学习和强化学习，显著提升了训练效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 解决信息不对称导致的分布偏移和策略退化问题，现有方法效率低下。

Method: 1) 模仿学习方法，自适应决定何时查询教师；2) 强化学习方法，选择初始化训练点以高效探索。

Result: 在模拟和真实机器人任务中验证，显著优于基线方法。

Conclusion: 通过战略互动和高效探索，解决了信息不对称问题，提升了性能。

Abstract: We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/

</details>


### [364] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/abs/2505.09003)
*Zeki Doruk Erden,Donia Gasmi,Boi Faltings*

Main category: cs.LG

Relevance: 70.0

TL;DR: 论文研究了在无外部信号的情况下，利用自编码器实现强化学习智能体的持续学习，通过熟悉度自编码器识别新任务并匹配已知环境。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习智能体在持续学习中的挑战，特别是在无外部信号指示任务或环境变化时如何保留和利用已有信息。

Method: 结合策略优化和熟悉度自编码器，构建端到端的持续学习系统，识别新任务并匹配已知环境。

Result: 初步结果显示，系统能在无外部信号的情况下成功实现持续学习，并选择性检索相关知识。

Conclusion: 该方法在无外部信号指示任务变化或重现时表现出潜力。

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [365] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2505.09503)
*Patrik Kenfack,Samira Ebrahimi Kaho,Ulrich Aïvodji*

Main category: cs.LG

Relevance: 70.0

TL;DR: 该论文研究了表格基础模型在上下文学习（ICL）中的公平性问题，提出了三种预处理策略以减少偏见，实验表明基于不确定性的演示选择能显著提升公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管表格基础模型在结构化数据上表现出强大的上下文学习能力，但其潜在的公平性问题尚未明确。论文旨在探索表格ICL中的偏见表现及解决方法。

Method: 论文提出了三种预处理策略：相关性去除、组平衡演示选择和基于不确定性的演示选择，并通过实验验证其效果。

Result: 实验结果表明，基于不确定性的演示选择能显著提升上下文预测的组公平性。

Conclusion: 表格ICL的公平性问题可以通过预处理策略有效缓解，尤其是基于不确定性的方法。

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [366] [Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features](https://arxiv.org/abs/2505.09004)
*Monica Welfert,Nathan Stromberg,Mario Diaz,Lalitha Sankar*

Main category: stat.ML

Relevance: 70.0

TL;DR: 提出了一种基于最小均方误差（MMSE）估计的对抗性评估框架，用于敏感特征推断，并建立了理论下界。


<details>
  <summary>Details</summary>
Motivation: 研究敏感特征推断的对抗性评估，以平衡理论保证与实际效率。

Method: 使用有限样本和线性预测模型，推导出MMSE的理论下界，并通过闭式界限分析近似误差。

Result: 推导了线性映射、二元对称信道和多变量高斯分布等关系的闭式界限，并通过实验验证了框架的有效性。

Conclusion: 该框架为敏感特征推断的对抗性评估提供了理论支持与实践工具。

Abstract: We propose an adversarial evaluation framework for sensitive feature
inference based on minimum mean-squared error (MMSE) estimation with a finite
sample size and linear predictive models. Our approach establishes theoretical
lower bounds on the true MMSE of inferring sensitive features from noisy
observations of other correlated features. These bounds are expressed in terms
of the empirical MMSE under a restricted hypothesis class and a non-negative
error term. The error term captures both the estimation error due to finite
number of samples and the approximation error from using a restricted
hypothesis class. For linear predictive models, we derive closed-form bounds,
which are order optimal in terms of the noise variance, on the approximation
error for several classes of relationships between the sensitive and
non-sensitive features, including linear mappings, binary symmetric channels,
and class-conditional multi-variate Gaussian distributions. We also present a
new lower bound that relies on the MSE computed on a hold-out validation
dataset of the MMSE estimator learned on finite-samples and a restricted
hypothesis class. Through empirical evaluation, we demonstrate that our
framework serves as an effective tool for MMSE-based adversarial evaluation of
sensitive feature inference that balances theoretical guarantees with practical
efficiency.

</details>


### [367] [Distilling Realizable Students from Unrealizable Teachers](https://arxiv.org/abs/2505.09546)
*Yujin Kim,Nathaniel Chin,Arnav Vasudev,Sanjiban Choudhury*

Main category: cs.RO

Relevance: 70.0

TL;DR: 论文研究了在特权信息下的策略蒸馏，提出两种方法（模仿学习和强化学习）以解决信息不对称问题，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决学生策略因部分观测无法直接访问教师完整状态空间导致的信息不对称问题，避免现有方法的低效性。

Method: 1) 模仿学习方法，自适应决定学生何时查询教师；2) 强化学习方法，选择初始化训练点以高效探索。

Result: 在仿真和真实机器人任务中验证，显著优于基线方法。

Conclusion: 通过策略性交互和高效探索，解决了信息不对称问题，提升了性能。

Abstract: We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/

</details>


### [368] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/abs/2505.08829)
*David Kinney*

Main category: cs.LG

Relevance: 70.0

TL;DR: 本文提出了一种线性组合方法来衡量预测算法的综合价值，以解决准确性与公平性之间的权衡问题，并基于Harsanyi的偏好聚合理论进行了形式化论证。


<details>
  <summary>Details</summary>
Motivation: 随着AI预测算法的快速发展，准确性与公平性之间的潜在冲突成为一个重要问题。本文旨在提供一种解决这一冲突的规范性方法。

Method: 通过线性组合准确性和公平性指标，并结合Harsanyi的偏好聚合理论，提出了一种衡量预测算法综合价值的方法。

Result: 在COMPAS数据集上的分析表明，线性组合方法能够有效平衡准确性与公平性之间的权衡。

Conclusion: 本文为处理预测算法中准确性与公平性的冲突提供了一种理论框架，并展示了其实际应用价值。

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [369] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/abs/2505.09214)
*Zhonghao Lyu,Ming Xiao,Jie Xu,Mikael Skoglund,Marco Di Renzo*

Main category: cs.LG

Relevance: 70.0

TL;DR: 论文提出了一种基于剪枝的LAIM（大型人工智能模型）协同推理方案，通过优化剪枝比例、传输功率和计算频率，在边缘设备与服务器之间分配模型，以平衡推理性能、延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着对低延迟和隐私保护的需求增加，边缘设备与服务器的协同推理成为资源高效的LAIM执行策略。本文旨在通过剪枝和优化设计提升协同推理性能。

Method: 1. 证明LAIM输出失真与参数失真之间的上限关系；2. 通过率失真理论推导参数失真的下限；3. 联合优化剪枝比例、传输功率和计算频率，提出高效算法解决非凸问题。

Result: 仿真表明，参数失真能可靠地约束输出失真，联合优化设计在性能、延迟和能耗方面优于完全在设备或服务器上推理的基准方案。

Conclusion: 剪枝和资源管理的联合优化显著提升了边缘环境下的LAIM协同推理性能，分割点对系统优化至关重要。

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [370] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/abs/2505.09427)
*Achref Doula,Max Mühläuser,Alejandro Sanchez Guinea*

Main category: cs.LG

Relevance: 65.0

TL;DR: SafePath是一个模块化框架，通过使用共形预测为基于LLM的路径规划提供形式化安全保证，显著降低不确定性和碰撞率。


<details>
  <summary>Details</summary>
Motivation: LLM在自动驾驶路径规划中表现出潜力，但其过度自信和幻觉行为引发安全问题，需要一种方法来确保安全。

Method: SafePath分三阶段工作：生成多样候选路径，通过共形预测过滤高风险路径，最后选择最安全路径或委托人类控制。

Result: 实验显示SafePath减少77%的不确定性和70%的碰撞率。

Conclusion: SafePath有效提升LLM路径规划的安全性，平衡自主性与安全性。

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [371] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/abs/2505.08915)
*Jialin Mao,Itay Griniasty,Yan Sun,Mark K. Transtrum,James P. Sethna,Pratik Chaudhari*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文研究了深度神经网络训练轨迹在概率分布空间中形成的低维“超带状”流形，并通过线性网络的理论分析揭示了其几何特性。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络训练轨迹的共同特性，以理解其低维流形结构的形成机制。

Method: 使用动态系统理论工具，分析线性网络的训练轨迹，并扩展到核机器和随机梯度下降训练的线性模型。

Result: 揭示了低维流形的几何特性由输入相关矩阵的特征值衰减率、初始权重与真实输出的相对尺度以及梯度下降步数控制。

Conclusion: 研究为理解深度神经网络的训练动态提供了理论框架，并揭示了低维流形的形成条件。

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [372] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/abs/2505.09214)
*Zhonghao Lyu,Ming Xiao,Jie Xu,Mikael Skoglund,Marco Di Renzo*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种基于剪枝的大型人工智能模型（LAIM）协同推理方案，通过联合优化剪枝比例、传输功率和计算频率，在边缘设备与服务器之间分配模型，以降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着对低延迟和隐私保护的需求增加，边缘设备与服务器协同推理成为资源高效执行LAIM的有前景策略。

Method: 通过理论分析剪枝比例与性能关系，并联合优化剪枝比例、传输功率和计算频率，提出高效算法解决非凸问题。

Result: 实验证明剪枝比例和资源管理设计在性能、延迟和能耗之间取得平衡，优于完全在设备或服务器上推理的方案。

Conclusion: 剪枝和资源联合优化在边缘环境中显著提升系统性能，分裂点对优化至关重要。

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [373] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/abs/2505.09344)
*Gabriel Cortês,Nuno Lourenço,Paolo Romano,Penousal Machado*

Main category: cs.LG

Relevance: 60.0

TL;DR: GreenFactory是一种集成零成本代理的方法，通过随机森林回归器直接预测模型测试精度，解决了传统代理在多样场景中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索中训练和评估每个网络耗时且资源密集，现有零成本代理泛化能力差且仅提供相对排名。

Method: 提出GreenFactory，集成多个零成本代理，使用随机森林回归器直接预测模型测试精度。

Result: 在NATS-Bench上表现优异，Kendall相关系数高（如CIFAR-10为0.907），验证了其可靠性。

Conclusion: GreenFactory在多样场景中表现稳健，为神经架构搜索提供了高效替代方案。

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [374] [Variational Rank Reduction Autoencoder](https://arxiv.org/abs/2505.09458)
*Jad Mounayer,Alicia Tierz,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

Relevance: 60.0

TL;DR: 提出了一种结合确定性秩降自编码器（RRAEs）和变分自编码器（VAEs）优势的变分秩降自编码器（VRRAEs），通过KL散度正则化和潜在空间采样，在生成任务中优于RRAEs和VAEs，并减少后验崩塌。


<details>
  <summary>Details</summary>
Motivation: 结合RRAEs的确定性正则化和VAEs的生成能力，以提升生成模型的性能并避免后验崩塌。

Method: 提出VRRAEs模型，通过截断SVD正则化潜在空间，并引入KL散度进行进一步正则化，结合采样策略。

Result: 在合成数据集和MNIST、CelebA、CIFAR-10等真实数据集上，VRRAEs在生成和插值任务中表现优于RRAEs和VAEs，且减少了后验崩塌。

Conclusion: VRRAEs通过结合RRAEs和VAEs的优势，显著提升了生成模型的性能，并增强了鲁棒性。

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [375] [Fairness-aware Bayes optimal functional classification](https://arxiv.org/abs/2505.09471)
*Xiaoyu Hu,Gengyu Xue,Zhenhua Lin,Yi Yu*

Main category: stat.ML

Relevance: 60.0

TL;DR: 该论文提出了一种公平性约束下的功能数据分类框架，设计了Fair-FLDA算法，确保分类器的公平性，并提供了理论和实验支持。


<details>
  <summary>Details</summary>
Motivation: 研究功能数据分类中的公平性问题，解决现有方法在高维功能空间中的局限性。

Method: 提出Fair-FLDA算法，通过组间阈值调整实现公平性，并在高斯过程假设下进行理论分析。

Result: 理论证明了算法的公平性和风险控制，实验验证了其在实际数据中的有效性。

Conclusion: Fair-FLDA为功能数据分类提供了一种公平且高效的解决方案。

Abstract: Algorithmic fairness has become a central topic in machine learning, and
mitigating disparities across different subpopulations has emerged as a rapidly
growing research area. In this paper, we systematically study the
classification of functional data under fairness constraints, ensuring the
disparity level of the classifier is controlled below a pre-specified
threshold. We propose a unified framework for fairness-aware functional
classification, tackling an infinite-dimensional functional space, addressing
key challenges from the absence of density ratios and intractability of
posterior probabilities, and discussing unique phenomena in functional
classification. We further design a post-processing algorithm, Fair Functional
Linear Discriminant Analysis classifier (Fair-FLDA), which targets at
homoscedastic Gaussian processes and achieves fairness via group-wise
thresholding. Under weak structural assumptions on eigenspace, theoretical
guarantees on fairness and excess risk controls are established. As a
byproduct, our results cover the excess risk control of the standard FLDA as a
special case, which, to the best of our knowledge, is first time seen. Our
theoretical findings are complemented by extensive numerical experiments on
synthetic and real datasets, highlighting the practicality of our designed
algorithm.

</details>


### [376] [Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data](https://arxiv.org/abs/2505.09496)
*Rui Miao,Babak Shahbaba,Annie Qu*

Main category: stat.ML

Relevance: 60.0

TL;DR: 提出了一种针对异构离线强化学习的个性化策略优化框架（P4L），通过个体潜在变量估计Q函数，并在弱部分覆盖假设下保证快速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习方法在处理异构数据时可能导致次优策略，因此需要一种个性化优化框架。

Method: 提出P4L算法，基于异构MDP模型和个体潜在变量估计Q函数，采用惩罚悲观策略优化。

Result: 在模拟和实际数据中表现优于现有方法，且在弱部分覆盖假设下具有快速收敛性。

Conclusion: P4L为异构离线强化学习提供了一种高效且可靠的解决方案。

Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.

</details>


### [377] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/abs/2505.08915)
*Jialin Mao,Itay Griniasty,Yan Sun,Mark K. Transtrum,James P. Sethna,Pratik Chaudhari*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文分析了深度神经网络训练轨迹的低维流形现象，并通过线性网络的理论分析揭示了其几何特性。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络训练轨迹的低维流形现象，以理解其背后的几何和动力学机制。

Method: 使用动态系统理论工具，分析线性网络的训练轨迹，并扩展到核机器和随机梯度下降训练的线性模型。

Result: 揭示了低维流形的几何特性受输入相关矩阵特征值衰减率、初始权重与真实输出的相对尺度以及梯度下降步数的影响。

Conclusion: 研究为理解深度神经网络的训练行为提供了理论支持，并揭示了低维流形的相边界。

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [378] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/abs/2505.09239)
*Faruk Alpay*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种通过符号延续和熵正则化轨迹实现稳定且凸的信息瓶颈（IB）优化的新方法，解决了IB方法在优化过程中不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈（IB）方法在优化过程中常因表示突变而不稳定，尤其是在IB权衡参数β的临界点附近。本文旨在解决这一问题。

Method: 通过引入熵正则化项，作者证明了IB解路径的凸性和唯一性，并展示了该方法如何稳定表示学习。此外，还提供了围绕临界点β的敏感性分析和统计稳健的不确定性量化。

Result: 实验结果表明，该方法在广泛的β值范围内实现了稳定的表示学习，并提供了可复现的开源实现。

Conclusion: 本文提出的方法为信息瓶颈的稳定优化提供了理论和实践基础，并具有实际部署和未来扩展的潜力。

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [379] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/abs/2505.09308)
*George Andriopoulos,Soyuj Jung Basnet,Juan Guevara,Li Guo,Keith Ross*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种无约束特征模型（UFM），用于分析深度神经网络中的训练损失和性能指标，并探讨了多任务模型与单任务模型的性能差异以及目标归一化对训练的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过UFM框架为深度神经网络的设计和数据预处理提供理论支持，特别是在多任务学习和目标归一化方面。

Method: 利用UFM框架进行理论分析，并通过实验验证多任务模型与单任务模型的性能差异以及目标归一化的效果。

Result: 理论预测和实验结果表明，多任务模型在相同或更强的正则化条件下表现更好，目标归一化在特定条件下能降低训练误差。

Conclusion: UFM是一个强大的工具，能够为神经网络设计和数据预处理提供理论指导。

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [380] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/abs/2505.09344)
*Gabriel Cortês,Nuno Lourenço,Paolo Romano,Penousal Machado*

Main category: cs.LG

Relevance: 60.0

TL;DR: GreenFactory是一种基于随机森林回归器的零成本代理集成方法，用于直接预测模型测试准确率，解决了传统代理方法泛化性差和仅提供相对排名的问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索中训练和评估每个网络耗时且资源密集，现有零成本代理方法泛化性不足且无法预测准确率。

Method: 提出GreenFactory，集成多个零成本代理，利用随机森林回归器直接预测模型测试准确率。

Result: 在NATS-Bench上表现优异，Kendall相关系数高（如CIFAR-10为0.907），表明预测与实际性能高度一致。

Conclusion: GreenFactory在多样数据集和搜索空间中表现稳健，为神经架构搜索提供了高效可靠的性能预测工具。

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [381] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenche-Young Losses](https://arxiv.org/abs/2505.09432)
*Yuzhou Cao,Han Bao,Lei Feng,Bo An*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种克服凸平滑代理损失与线性遗憾边界之间权衡的方法，通过构造基于Fenchel-Young损失的代理损失，实现了优化和统计效率的提升。


<details>
  <summary>Details</summary>
Motivation: 解决凸平滑代理损失在优化和估计性能与线性遗憾边界之间的权衡问题，为离散目标损失提供高效解决方案。

Method: 使用卷积负熵生成的Fenchel-Young损失构造凸平滑代理损失，结合定制预测链接，保持线性遗憾边界。

Result: 成功构造了具有线性遗憾边界的凸平滑代理损失，并证明了其优化和统计效率。

Conclusion: 通过凸分析在风险最小化中的应用，展示了优化和统计效率的新方法。

Abstract: Surrogate regret bounds bridge the gap between the convergence rates of
surrogate and target losses, with linear bounds favorable for their lossless
regret transfer. While convex smooth surrogate losses are appealing in
particular due to the efficient estimation and optimization, the existence of a
trade-off between the smoothness and linear regret bound has been believed in
the community. That being said, the better optimization and estimation
properties of convex smooth surrogate losses may inevitably deteriorate after
undergoing the regret transfer onto a target loss. We overcome this dilemma for
arbitrary discrete target losses by constructing a convex smooth surrogate
loss, which entails a linear surrogate regret bound composed with a tailored
prediction link. The construction is based on Fenchel-Young losses generated by
the convolutional negentropy, which are equivalent to the infimal convolution
of a generalized negentropy and the target Bayes risk. Consequently, the
infimal convolution enables us to derive a smooth loss while maintaining the
surrogate regret bound linear. We additionally benefit from the infimal
convolution to have a consistent estimator of the underlying class probability.
Our results are overall a novel demonstration of how convex analysis penetrates
into optimization and statistical efficiency in risk minimization.

</details>


### [382] [Variational Rank Reduction Autoencoder](https://arxiv.org/abs/2505.09458)
*Jad Mounayer,Alicia Tierz,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

Relevance: 60.0

TL;DR: 提出了一种结合确定性降秩自编码器（RRAEs）和变分自编码器（VAEs）优势的模型VRRAEs，通过KL散度正则化和SVD正则化，提升了生成能力和抗后验坍缩能力。


<details>
  <summary>Details</summary>
Motivation: 结合RRAEs的确定性正则化和VAEs的生成能力，以提升自编码器的性能。

Method: 在RRAEs的隐空间中进行采样，并引入KL散度正则化，结合SVD的正则化效果。

Result: 在合成数据集和MNIST、CelebA、CIFAR-10等真实数据集上，VRRAEs在生成和插值任务中优于RRAEs和VAEs。

Conclusion: VRRAEs通过结合两种模型的优势，显著提升了生成能力和抗坍缩性能。

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [383] [Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features](https://arxiv.org/abs/2505.09004)
*Monica Welfert,Nathan Stromberg,Mario Diaz,Lalitha Sankar*

Main category: stat.ML

Relevance: 60.0

TL;DR: 提出了一种基于最小均方误差（MMSE）的对抗性评估框架，用于敏感特征推断，并建立了理论下界。


<details>
  <summary>Details</summary>
Motivation: 研究如何在有限样本和线性预测模型下，评估敏感特征的推断能力，以平衡理论保证与实际效率。

Method: 使用MMSE估计和有限样本，推导了线性预测模型的闭式下界，并提出了基于验证集的新下界。

Result: 理论下界在噪声方差方面是最优的，并通过实证验证了框架的有效性。

Conclusion: 该框架为敏感特征推断的对抗性评估提供了理论和实践支持。

Abstract: We propose an adversarial evaluation framework for sensitive feature
inference based on minimum mean-squared error (MMSE) estimation with a finite
sample size and linear predictive models. Our approach establishes theoretical
lower bounds on the true MMSE of inferring sensitive features from noisy
observations of other correlated features. These bounds are expressed in terms
of the empirical MMSE under a restricted hypothesis class and a non-negative
error term. The error term captures both the estimation error due to finite
number of samples and the approximation error from using a restricted
hypothesis class. For linear predictive models, we derive closed-form bounds,
which are order optimal in terms of the noise variance, on the approximation
error for several classes of relationships between the sensitive and
non-sensitive features, including linear mappings, binary symmetric channels,
and class-conditional multi-variate Gaussian distributions. We also present a
new lower bound that relies on the MSE computed on a hold-out validation
dataset of the MMSE estimator learned on finite-samples and a restricted
hypothesis class. Through empirical evaluation, we demonstrate that our
framework serves as an effective tool for MMSE-based adversarial evaluation of
sensitive feature inference that balances theoretical guarantees with practical
efficiency.

</details>


### [384] [Fairness-aware Bayes optimal functional classification](https://arxiv.org/abs/2505.09471)
*Xiaoyu Hu,Gengyu Xue,Zhenhua Lin,Yi Yu*

Main category: stat.ML

Relevance: 60.0

TL;DR: 论文提出了一种公平性约束下的功能数据分类统一框架，设计了Fair-FLDA算法，并在理论和实验中验证了其公平性和风险控制能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决功能数据分类中的公平性问题，确保分类器在不同子群体中的差异水平可控。

Method: 提出了一种后处理算法Fair-FLDA，针对同方差高斯过程，通过组间阈值调整实现公平性。

Result: 在理论和实验中验证了算法的公平性和风险控制能力，并首次覆盖了标准FLDA的过剩风险控制。

Conclusion: Fair-FLDA算法在功能数据分类中实现了公平性和性能的平衡，具有实际应用价值。

Abstract: Algorithmic fairness has become a central topic in machine learning, and
mitigating disparities across different subpopulations has emerged as a rapidly
growing research area. In this paper, we systematically study the
classification of functional data under fairness constraints, ensuring the
disparity level of the classifier is controlled below a pre-specified
threshold. We propose a unified framework for fairness-aware functional
classification, tackling an infinite-dimensional functional space, addressing
key challenges from the absence of density ratios and intractability of
posterior probabilities, and discussing unique phenomena in functional
classification. We further design a post-processing algorithm, Fair Functional
Linear Discriminant Analysis classifier (Fair-FLDA), which targets at
homoscedastic Gaussian processes and achieves fairness via group-wise
thresholding. Under weak structural assumptions on eigenspace, theoretical
guarantees on fairness and excess risk controls are established. As a
byproduct, our results cover the excess risk control of the standard FLDA as a
special case, which, to the best of our knowledge, is first time seen. Our
theoretical findings are complemented by extensive numerical experiments on
synthetic and real datasets, highlighting the practicality of our designed
algorithm.

</details>


### [385] [Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data](https://arxiv.org/abs/2505.09496)
*Rui Miao,Babak Shahbaba,Annie Qu*

Main category: stat.ML

Relevance: 60.0

TL;DR: 提出了一种针对异构时间平稳MDP的个性化离线策略优化框架（P4L），通过个体潜在变量高效估计个体Q函数，并在弱部分覆盖假设下保证快速平均遗憾率。


<details>
  <summary>Details</summary>
Motivation: 传统离线RL方法在处理异构数据时可能导致次优策略，因此需要一种能适应异构群体的个性化方法。

Method: 提出基于个体潜在变量的异构模型，设计P4L算法，结合惩罚悲观策略优化，实现高效个体Q函数估计。

Result: 仿真和实际数据应用表明，P4L在数值性能上优于现有方法。

Conclusion: P4L为异构离线RL提供了一种高效且理论保证的解决方案。

Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.

</details>


### [386] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/abs/2505.09572)
*Julian Kranz,Davide Gallon,Steffen Dereich,Arnulf Jentzen*

Main category: cs.LG

Relevance: 50.0

TL;DR: 论文研究了全连接前馈神经网络在常见可微激活函数下的梯度流行为，证明了梯度流要么收敛到临界点，要么发散到无穷大，同时损失收敛到渐近临界值。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络梯度流的收敛性和发散性，特别是在不同初始化条件下的行为，以理解损失函数的几何特性。

Method: 通过理论分析和几何方法（o-minimal结构）研究梯度流，并结合数值实验验证理论结果。

Result: 证明了梯度流在特定条件下会发散到无穷大，同时损失收敛到渐近临界值；在多项式目标函数和大架构下，最优损失值为零且只能渐近实现。

Conclusion: 梯度流的行为高度依赖于初始化和损失函数的几何特性，理论结果在现实场景中也有类似表现。

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [387] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/abs/2505.09284)
*Panqi Chen,Yifan Sun,Lei Cheng,Yang Yang,Weichang Li,Yang Liu,Weiqing Liu,Jiang Bian,Shikai Fang*

Main category: cs.LG

Relevance: 50.0

TL;DR: SDIFT是一种基于扩散模型的框架，用于从稀疏和不规则观测中重建多维物理动力学，利用功能Tucker空间和序列扩散模型提高重建精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏和不规则观测中重建多维物理动力学的挑战，填补现有扩散模型在处理连续性和稀疏性方面的不足。

Method: 提出SDIFT框架，结合功能Tucker模型和序列扩散模型，通过高斯过程噪声去噪生成核心张量序列，并引入消息传递后验采样机制。

Result: 在三个物理系统（天文、环境、分子）中验证，SDIFT在重建精度和计算效率上显著优于现有方法。

Conclusion: SDIFT为稀疏观测下的物理动力学建模提供了高效且准确的解决方案。

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [388] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文探讨了机器学习中如何通过交叉性框架提升社会公平性，强调了正确应用交叉性理论的重要性。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术在社会身份支持上存在不足，交叉性理论可作为工具提升公平性，但当前应用存在偏差。

Method: 基于Crenshaw、Combahee和Collins的交叉性理论，构建初步框架，评估机器学习文献中的应用情况。

Result: 揭示了机器学习文献中交叉性应用的偏差与不足。

Conclusion: 正确应用交叉性理论可提升机器学习的公平性，需更严格的理论遵循。

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [389] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
*Andres Anabalon,Hugo Garces,Julio Oliva,Jose Cifuentes*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种快速算法，将层次结构嵌入三维Minkowski时空，数据相关性完全由因果结构编码。方法仅依赖局部层次信号，无需全局符号结构，并在WordNet语料上验证了完美嵌入。


<details>
  <summary>Details</summary>
Motivation: 探索离散数据的几何表示，特别是层次结构在三维时空中的完美嵌入，揭示概念与几何的深层联系。

Method: 基于局部层次信号（定向令牌对），设计了一种因果驱动的检索机制，无需全局结构信息。

Result: 成功实现了WordNet中哺乳动物子树的完美嵌入，并扩展到最大无歧义子集（82,115名词令牌），验证了离散数据的几何表示可能性。

Conclusion: 层次结构及其关系本质上是几何的，结果暗示了与广义相对论和场论的潜在联系。

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [390] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/abs/2505.08846)
*Felix Marti-Perez,Brigt Håvardstun,Cèsar Ferri,Carlos Monserrat,Jan Arne Telle*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了评估时间序列分类器（TSC）可解释性的简化时间序列的指标，并验证了其优于原始时间序列的效果。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据对人类不直观，需要简化以提高可解释性。

Method: 定义了简化复杂度和忠诚度指标，评估了四种简化算法在不同TSC算法和数据集上的表现。

Result: 简化时间序列在季节性、非平稳或低熵数据上表现更优。

Conclusion: 简化时间序列可显著提升TSC的可解释性。

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [391] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
*Gavin Hull,Alex Bihlo*

Main category: cs.LG

Relevance: 40.0

TL;DR: ForeCite是一个基于预训练因果语言模型的框架，通过添加线性头预测论文的月度平均引用率，在生物医学领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动化研究评估和加速科学进步需要预测论文的未来引用率。

Method: 使用预训练的因果语言模型，添加线性头进行回归任务，预测引用率。

Result: 在90万+生物医学论文数据集上，测试相关性达到ρ=0.826，比之前最优方法提升27点。

Conclusion: ForeCite为学术研究的长期影响力预测设立了新标杆，并为自动化科学贡献评估奠定了基础。

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [392] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/abs/2505.09017)
*Bizhan Alipour Pijan,Serdar Bozdag*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种名为DyGSSM的新方法，结合GCN和GRU提取动态图的局部和全局特征，并利用HiPPO算法优化参数更新，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法无法同时提取局部和全局信息，且参数更新时未考虑当前快照的性能，导致时间依赖管理不足。

Method: 结合GCN提取局部特征和GRU提取全局特征，使用交叉注意力机制融合特征，并基于HiPPO算法的SSM优化参数更新。

Result: 在五个公开数据集上，DyGSSM在20个案例中的17个优于现有基线方法和SOTA方法。

Conclusion: DyGSSM通过同时提取局部和全局特征并优化时间依赖管理，显著提升了动态图表示学习的性能。

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [393] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/abs/2505.09089)
*Philipp Hess,Maximilian Gelbrecht,Christof Schötz,Michael Aich,Yu Huang,Shangshang Yang,Niklas Boers*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种时间一致性判别器，使预训练的图像扩散模型能够生成逼真的时空动态，无需扩展或微调。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）训练成本高，限制了广泛应用，因此需要一种更高效的方法。

Method: 使用时间一致性判别器指导采样推理过程，无需修改预训练的图像扩散模型。

Result: 在理想湍流模拟和真实全球降水数据集上表现与从头训练的VDM相当，但具有更好的不确定性校准和更低偏差。

Conclusion: 该方法在保持时间一致性的同时，降低了计算成本，适用于长期气候模拟。

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [394] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/abs/2505.09113)
*Yingrong Wang,Anpeng Wu,Baohong Li,Ziyang Xiao,Ruoxuan Xiong,Qing Han,Kun Kuang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种名为DSIV-CFR的新框架，用于解决序列治疗效应估计中的未测量混杂偏差问题，利用工具变量和负控制假设，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在序列决策场景中，未测量的混杂因素会影响治疗分配和结果，现有基于Transformer的方法无法完全解决这一问题，因此需要新的方法来调整这种偏差。

Method: 提出DSIV-CFR框架，利用工具变量和负控制假设，通过广义矩条件估计序列治疗效应。

Result: 在4个数据集上的实验表明，该方法在单步和多步预测中表现优异，并能识别动态系统的最优治疗方案。

Conclusion: DSIV-CFR框架有效解决了序列治疗效应估计中的未测量混杂问题，为动态系统的最优决策提供了支持。

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [395] [Fair Clustering via Alignment](https://arxiv.org/abs/2505.09131)
*Kunwoong Kim,Jihu Lee,Sangchul Park,Yongdai Kim*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种新的公平聚类算法FCA，通过数据对齐和中心优化实现高效用公平聚类，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类算法因复杂约束导致聚类效用低或数值不稳定，需改进。

Method: 基于公平K均值目标函数分解，交替进行数据对齐和中心优化。

Result: FCA在公平性和聚类效用间取得更好平衡，且数值稳定。

Conclusion: FCA无需复杂约束即可实现高效用公平聚类，优于现有方法。

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [396] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/abs/2505.09218)
*Alexander Tyurin,Danil Sivtsov*

Main category: cs.LG

Relevance: 40.0

TL;DR: Birch SGD是一个新的统一框架，用于分析和设计分布式SGD方法，通过计算树的几何分析优化动态，设计了八种新方法，其中六种具有最优计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 提出一个统一的理论框架，简化分布式SGD方法的收敛分析，并为方法设计提供直观的图论基础。

Method: 将每种方法表示为加权有向树（计算树），利用几何分析优化动态，设计新方法并分析其性能。

Result: 设计了八种新方法，其中六种具有最优计算复杂度，揭示了所有方法的迭代速率相同但存在不同权衡。

Conclusion: Birch SGD为理解和设计高效异步与并行优化方法提供了统一基础。

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [397] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/abs/2505.09239)
*Faruk Alpay*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种通过符号延续和熵正则化轨迹实现稳定且凸的信息瓶颈（IB）优化的新方法。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈方法在优化过程中常出现不稳定性，表现为在IB权衡参数β的临界点附近出现突然的表示变化。

Method: 引入熵正则化项，通过符号延续和熵正则化轨迹实现稳定优化，并证明了IB解路径的凸性和唯一性。

Result: 实验结果表明，该方法在广泛的β值范围内稳定了表示学习，并提供了统计上稳健的不确定性量化（95%置信区间）。

Conclusion: 该方法为信息瓶颈的稳定优化提供了理论保证和实用工具，开源实现和可复现框架支持未来扩展。

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [398] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/abs/2505.09284)
*Panqi Chen,Yifan Sun,Lei Cheng,Yang Yang,Weichang Li,Yang Liu,Weiqing Liu,Jiang Bian,Shikai Fang*

Main category: cs.LG

Relevance: 40.0

TL;DR: SDIFT是一种基于扩散模型的框架，用于从稀疏和不规则观测中重建多维物理动力学，利用功能Tucker空间和序列扩散模型提高重建精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏和不规则观测中重建多维物理动力学的挑战，填补现有扩散模型在处理连续和稀疏观测数据时的不足。

Method: 提出SDIFT框架，结合功能Tucker模型和序列扩散模型，通过高斯过程噪声去噪生成核心张量序列，并引入消息传递后验采样机制。

Result: 在多个物理系统（天文、环境、分子）中验证，SDIFT在重建精度和计算效率上显著优于现有方法。

Conclusion: SDIFT为稀疏观测下的物理动力学建模提供了高效且准确的解决方案。

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [399] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/abs/2505.09308)
*George Andriopoulos,Soyuj Jung Basnet,Juan Guevara,Li Guo,Keith Ross*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出Unconstrained Feature Model (UFM)框架，用于分析深度神经网络中的训练损失和性能，特别关注多任务模型与单任务模型的比较以及目标归一化的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过UFM框架为深度神经网络的设计和数据预处理提供理论支持，特别是在模仿学习、机器人和强化学习领域。

Method: 利用UFM框架进行理论分析，并通过实证验证多任务模型与单任务模型的性能差异以及目标归一化的影响。

Result: 理论预测和实证结果表明，多任务模型在相同或更强正则化条件下表现优于单任务模型，目标归一化在特定条件下能降低训练MSE。

Conclusion: UFM是一个强大的框架，可为深度神经网络设计和数据预处理提供可操作的见解。

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [400] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/abs/2505.09354)
*Guangtai Wang,Chi-Man Vong,Jintao Huang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种名为CleanSE的新校准策略，利用干净样本增强部分标签学习中的候选标签置信度。


<details>
  <summary>Details</summary>
Motivation: 现有部分标签学习的消歧策略主要关注单个实例特征，忽略了数据集中干净样本的强监督信息。

Method: 结合可微分计数损失和K近邻算法，利用干净样本为候选标签提供指导，并通过限制标签计数区间来表征样本分布。

Result: 在3个合成基准和5个真实PLL数据集上的实验表明，CleanSE能提升现有PLL方法的性能。

Conclusion: CleanSE通过利用干净样本的监督信息，有效提升了部分标签学习的消歧能力。

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [401] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/abs/2505.09361)
*Samir Moustafa,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出MixQ-GNN框架，通过混合精度量化提升图神经网络（GNN）的推理效率，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: GNN在大规模图应用中计算需求高，需要高效方法加速推理，混合精度量化是一种潜在解决方案。

Method: 提出定理保证整数消息传递的数值等价性，并设计MixQ-GNN框架，灵活选择GNN层中各组件的整数位宽。

Result: MixQ-GNN平均减少节点分类和图分类的比特操作分别为5.5倍和5.1倍。

Conclusion: MixQ-GNN通过混合精度量化显著提升GNN效率，且与现有量化方法兼容。

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [402] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.09366)
*SeyedMojtaba Mohasel,Alireza Afzal Aghaei,Corey Pew*

Main category: cs.LG

Relevance: 40.0

TL;DR: 研究了可学习激活函数在Kolmogorov-Arnold Networks（KANs）中的潜力，用于下肢假肢的个性化控制，并比较了用户特定数据与池化数据对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索可学习激活函数在复杂任务中的优势，以及个性化数据对假肢控制模型性能的影响。

Method: 收集了五名下肢截肢者的IMU数据，比较了MLP、KAN、CNN和FKAN在转弯意图预测中的表现，并评估了用户特定与池化数据的效果。

Result: 可学习激活函数未显著提升性能；用户特定数据在ML模型中表现更好，但在DL模型中与池化数据无显著差异。

Conclusion: 可学习激活函数可能在更复杂任务中表现更好；DL模型可使用池化数据训练。

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [403] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenche-Young Losses](https://arxiv.org/abs/2505.09432)
*Yuzhou Cao,Han Bao,Lei Feng,Bo An*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种克服凸平滑代理损失与线性遗憾界限之间权衡的方法，通过构造基于Fenchel-Young损失的代理损失，实现了优化和统计效率的双赢。


<details>
  <summary>Details</summary>
Motivation: 解决凸平滑代理损失在优化和估计效率与线性遗憾界限之间的权衡问题。

Method: 基于卷积负熵生成Fenchel-Young损失，构造凸平滑代理损失，并通过预测链接保持线性遗憾界限。

Result: 成功构造了一种凸平滑代理损失，同时保持线性遗憾界限，并提供了类别概率的一致估计。

Conclusion: 凸分析在风险最小化中的优化和统计效率方面具有重要作用。

Abstract: Surrogate regret bounds bridge the gap between the convergence rates of
surrogate and target losses, with linear bounds favorable for their lossless
regret transfer. While convex smooth surrogate losses are appealing in
particular due to the efficient estimation and optimization, the existence of a
trade-off between the smoothness and linear regret bound has been believed in
the community. That being said, the better optimization and estimation
properties of convex smooth surrogate losses may inevitably deteriorate after
undergoing the regret transfer onto a target loss. We overcome this dilemma for
arbitrary discrete target losses by constructing a convex smooth surrogate
loss, which entails a linear surrogate regret bound composed with a tailored
prediction link. The construction is based on Fenchel-Young losses generated by
the convolutional negentropy, which are equivalent to the infimal convolution
of a generalized negentropy and the target Bayes risk. Consequently, the
infimal convolution enables us to derive a smooth loss while maintaining the
surrogate regret bound linear. We additionally benefit from the infimal
convolution to have a consistent estimator of the underlying class probability.
Our results are overall a novel demonstration of how convex analysis penetrates
into optimization and statistical efficiency in risk minimization.

</details>


### [404] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/abs/2505.09586)
*Yipeng Zhang,Longlong Li,Kelin Xia*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于菱形平铺结构（RT）的聚类方法RTPool，用于图分类任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）的聚类方法依赖图的连通性结构，难以捕捉几何图的丰富几何特征。

Method: 提出RT聚类方法，利用数据的复杂几何信息进行聚类，并设计RTPool模型用于图分类。

Result: 在7个基准数据集上优于21种最先进方法。

Conclusion: RT聚类和RTPool模型能有效提取高阶几何结构，提升图分类性能。

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [405] [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
*Longtian Wang,Xiaofei Xie,Tianlin Li,Yuhan Zhi,Chao Shen*

Main category: cs.CR

Relevance: 40.0

TL;DR: TokenProber是一种用于评估T2I模型中拒绝机制鲁棒性的方法，通过生成对抗性提示来绕过安全过滤器。


<details>
  <summary>Details</summary>
Motivation: T2I模型可能生成NSFW内容，现有拒绝机制存在漏洞，需评估其鲁棒性。

Method: 通过敏感性感知的差异测试，分析提示中的关键词，生成对抗性提示。

Result: TokenProber在5个安全检查器和3个T2I模型上表现优于现有方法，绕过率提高54%以上。

Conclusion: TokenProber能有效揭示拒绝机制的鲁棒性问题。

Abstract: Text-to-image (T2I) models have significantly advanced in producing
high-quality images. However, such models have the ability to generate images
containing not-safe-for-work (NSFW) content, such as pornography, violence,
political content, and discrimination. To mitigate the risk of generating NSFW
content, refusal mechanisms, i.e., safety checkers, have been developed to
check potential NSFW content. Adversarial prompting techniques have been
developed to evaluate the robustness of the refusal mechanisms. The key
challenge remains to subtly modify the prompt in a way that preserves its
sensitive nature while bypassing the refusal mechanisms. In this paper, we
introduce TokenProber, a method designed for sensitivity-aware differential
testing, aimed at evaluating the robustness of the refusal mechanisms in T2I
models by generating adversarial prompts. Our approach is based on the key
observation that adversarial prompts often succeed by exploiting discrepancies
in how T2I models and safety checkers interpret sensitive content. Thus, we
conduct a fine-grained analysis of the impact of specific words within prompts,
distinguishing between dirty words that are essential for NSFW content
generation and discrepant words that highlight the different sensitivity
assessments between T2I models and safety checkers. Through the
sensitivity-aware mutation, TokenProber generates adversarial prompts, striking
a balance between maintaining NSFW content generation and evading detection.
Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I
models, using 324 NSFW prompts, demonstrates its superior effectiveness in
bypassing safety filters compared to existing methods (e.g., 54%+ increase on
average), highlighting TokenProber's ability to uncover robustness issues in
the existing refusal mechanisms.

</details>


### [406] [Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems](https://arxiv.org/abs/2505.08816)
*Ippokratis Koukoulis,Ilias Syrigos,Thanasis Korakis*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了一种基于Transformer的自监督对比学习方法，用于通用入侵检测，显著提升了异常检测和有限标记数据下的监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 随着零日攻击的增加，传统入侵检测系统（IDS）依赖标记数据且泛化能力有限，亟需创新的解决方案。

Method: 采用自监督对比学习结合Transformer编码器，通过数据增强和自动学习包序列表示，优化入侵检测。

Result: 在异常检测和监督学习中表现优异，AUC提升显著（最高20%），并在跨数据集适应中展现强鲁棒性。

Conclusion: 提出的方法为入侵检测提供了高效、通用的解决方案，尤其在数据有限或跨域场景下表现突出。

Abstract: As the digital landscape becomes more interconnected, the frequency and
severity of zero-day attacks, have significantly increased, leading to an
urgent need for innovative Intrusion Detection Systems (IDS). Machine
Learning-based IDS that learn from the network traffic characteristics and can
discern attack patterns from benign traffic offer an advanced solution to
traditional signature-based IDS. However, they heavily rely on labeled
datasets, and their ability to generalize when encountering unseen traffic
patterns remains a challenge. This paper proposes a novel self-supervised
contrastive learning approach based on transformer encoders, specifically
tailored for generalizable intrusion detection on raw packet sequences. Our
proposed learning scheme employs a packet-level data augmentation strategy
combined with a transformer-based architecture to extract and generate
meaningful representations of traffic flows. Unlike traditional methods reliant
on handcrafted statistical features (NetFlow), our approach automatically
learns comprehensive packet sequence representations, significantly enhancing
performance in anomaly identification tasks and supervised learning for
intrusion detection. Our transformer-based framework exhibits better
performance in comparison to existing NetFlow self-supervised methods.
Specifically, we achieve up to a 3% higher AUC in anomaly detection for
intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset
evaluation. Moreover, our model provides a strong baseline for supervised
intrusion detection with limited labeled data, exhibiting an improvement over
self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated
on the same dataset. Additionally, we show the adaptability of our pretrained
model when fine-tuned across different datasets, demonstrating strong
performance even when lacking benign data from the target domain.

</details>


### [407] [Statistical Decision Theory with Counterfactual Loss](https://arxiv.org/abs/2505.08908)
*Benedikt Koch,Kosuke Imai*

Main category: math.ST

Relevance: 40.0

TL;DR: 论文扩展了经典统计决策理论，引入反事实损失来评估决策质量，解决了仅依赖观察结果的局限性。


<details>
  <summary>Details</summary>
Motivation: 经典统计决策理论仅基于观察结果评估决策，无法考虑反事实结果，限制了决策质量的全面评估。

Method: 通过假设强可忽略性，证明反事实风险在反事实损失函数对潜在结果可加时可识别。

Result: 反事实损失函数在潜在结果可加时可识别，且在多治疗选项下可能产生与标准损失函数不同的推荐。

Conclusion: 扩展的决策理论为评估决策质量提供了更全面的框架，尤其在多选项决策中更具优势。

Abstract: Classical statistical decision theory evaluates treatment choices based
solely on observed outcomes. However, by ignoring counterfactual outcomes, it
cannot assess the quality of decisions relative to feasible alternatives. For
example, the quality of a physician's decision may depend not only on patient
survival, but also on whether a less invasive treatment could have produced a
similar result. To address this limitation, we extend standard decision theory
to incorporate counterfactual losses--criteria that evaluate decisions using
all potential outcomes. The central challenge in this generalization is
identification: because only one potential outcome is observed for each unit,
the associated risk under a counterfactual loss is generally not identifiable.
We show that under the assumption of strong ignorability, a counterfactual risk
is identifiable if and only if the counterfactual loss function is additive in
the potential outcomes. Moreover, we demonstrate that additive counterfactual
losses can yield treatment recommendations that differ from those based on
standard loss functions, provided that the decision problem involves more than
two treatment options.

</details>


### [408] [A Comparative Review of RNA Language Models](https://arxiv.org/abs/2505.09087)
*He Wang,Yikun Zhang,Jie Chen,Jian Zhan,Yaoqi Zhou*

Main category: q-bio.BM

Relevance: 40.0

TL;DR: 该论文比较了13种RNA语言模型在RNA二级结构和功能分类的零样本预测中的表现，发现模型在两项任务上的表现存在权衡，需要更平衡的无监督训练。


<details>
  <summary>Details</summary>
Motivation: RNA语言模型在结构和功能推断中具有潜力，但缺乏统一的标准进行比较。

Method: 将RNA语言模型分为三类（多RNA类型预训练、特定用途RNA、RNA与DNA/蛋白质统一模型），并与DNA和蛋白质模型作为对照进行比较。

Result: 模型在RNA二级结构预测和功能分类上的表现存在权衡，表现优异的模型往往在另一任务上表现较差。

Conclusion: 需要更平衡的无监督训练以提高RNA语言模型的多任务性能。

Abstract: Given usefulness of protein language models (LMs) in structure and functional
inference, RNA LMs have received increased attentions in the last few years.
However, these RNA models are often not compared against the same standard.
Here, we divided RNA LMs into three classes (pretrained on multiple RNA types
(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with
DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein
LMs as controls in zero-shot prediction of RNA secondary structure and
functional classification. Results shows that the models doing well on
secondary structure prediction often perform worse in function classification
or vice versa, suggesting that more balanced unsupervised training is needed.

</details>


### [409] [Imitation Learning for Adaptive Control of a Virtual Soft Exoglove](https://arxiv.org/abs/2505.09099)
*Shirui Lyu,Vittorio Caggiano,Matteo Leonetti,Dario Farina,Letizia Gionfrida*

Main category: cs.RO

Relevance: 40.0

TL;DR: 提出了一种基于强化学习和生物力学模型的定制化可穿戴机器人控制器，用于补偿手部运动障碍患者的肌肉缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有康复训练中可穿戴机器人常忽略患者肌肉损失的独特性，需要个性化解决方案。

Method: 结合强化学习和生物力学模型，通过仿真训练定制化控制器，并利用视频数据学习演示任务。

Result: 虚拟可穿戴机器人手套成功补偿了肌肉力量减弱，恢复了90.5%的原始操作能力。

Conclusion: 该方法为个性化康复训练提供了有效工具，尤其适用于手部运动障碍患者。

Abstract: The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.

</details>


### [410] [Toward Malicious Clients Detection in Federated Learning](https://arxiv.org/abs/2505.09110)
*Zhihao Dou,Jiaqi Wang,Wei Sun,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了一种名为SafeFL的新算法，用于在联邦学习中准确识别恶意客户端，通过生成合成数据集来区分恶意和良性模型，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其容易受到投毒攻击，现有的拜占庭鲁棒聚合规则和检测方法存在不足，需要更高效的解决方案。

Method: SafeFL算法通过服务器收集一系列全局模型生成合成数据集，基于模型行为区分恶意和良性客户端。

Result: 实验表明，SafeFL在检测恶意客户端方面效率和准确性均优于现有方法。

Conclusion: SafeFL为联邦学习中的安全问题提供了一种高效且准确的解决方案。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global machine learning model without sharing their raw data. However, the
decentralized nature of FL introduces vulnerabilities, particularly to
poisoning attacks, where malicious clients manipulate their local models to
disrupt the training process. While Byzantine-robust aggregation rules have
been developed to mitigate such attacks, they remain inadequate against more
advanced threats. In response, recent advancements have focused on FL detection
techniques to identify potentially malicious participants. Unfortunately, these
methods often misclassify numerous benign clients as threats or rely on
unrealistic assumptions about the server's capabilities. In this paper, we
propose a novel algorithm, SafeFL, specifically designed to accurately identify
malicious clients in FL. The SafeFL approach involves the server collecting a
series of global models to generate a synthetic dataset, which is then used to
distinguish between malicious and benign models based on their behavior.
Extensive testing demonstrates that SafeFL outperforms existing methods,
offering superior efficiency and accuracy in detecting malicious clients.

</details>


### [411] [Online Learning of Neural Networks](https://arxiv.org/abs/2505.09167)
*Amit Daniely,Idan Mehalel,Elchanan Mossel*

Main category: stat.ML

Relevance: 40.0

TL;DR: 该论文研究了具有符号激活函数的前馈神经网络的在线学习问题，提出了一个边界条件作为在线可学习性的充分（有时必要）条件，并给出了错误界限的定量分析。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在在线学习中的表现，特别是在有限标签集和单位球输入空间下的学习能力。

Method: 通过分析神经网络的边界条件和输入维度的影响，提出了两种限制条件（多索引模型和扩展边界假设）以减少对输入维度的依赖。

Result: 证明了在某些条件下，错误界限可以显著降低，例如在多索引模型中错误界限约为$(1.5/\gamma)^{k + 2}$，在扩展边界假设下约为$(\log Y)/ \gamma^{O(L)}$。

Conclusion: 输入维度对在线学习的错误界限有显著影响，但通过引入额外的限制条件可以缓解这一问题。

Abstract: We study online learning of feedforward neural networks with the sign
activation function that implement functions from the unit ball in
$\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some
cases necessary for online learnability of a neural network: Every neuron in
the first hidden layer classifies all instances with some margin $\gamma$
bounded away from zero. Quantitatively, we prove that for any net, the optimal
mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the
$(d,\gamma)$-totally-separable-packing number, a more restricted variation of
the standard $(d,\gamma)$-packing number. We complement this result by
constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many
mistakes. We also give a quantitative lower bound of approximately
$\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq
1/2$, implying that for some nets and input sequences every learner will err
for $\exp(d)$ many times, and that a dimension-free mistake bound is almost
always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional
natural restrictions to be placed on the network, so that the dependence on $d$
is removed. We study two such restrictions. The first is the multi-index model,
in which the function computed by the net depends only on $k \ll d$ orthonormal
directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in
this model. The second is the extended margin assumption. In this setting, we
assume that all neurons (in all layers) in the network classify every ingoing
input from previous layer with margin $\gamma$ bounded away from zero. In this
model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$,
where L is the depth of the network.

</details>


### [412] [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](https://arxiv.org/abs/2505.09229)
*Brian Britos,Mathias Bourel*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文探讨了在旋转偏移的监督域适应问题中，利用最优传输（OT）恢复潜在旋转的方法，并提出了一种结合K均值聚类、OT和SVD的算法。


<details>
  <summary>Details</summary>
Motivation: 研究旋转偏移下的域适应问题，特别是在传感器校准或图像方向等应用中，如何利用OT恢复旋转并改进模型迁移。

Method: 结合K均值聚类、OT和SVD，估计旋转角度并调整回归模型，尤其适用于目标域数据稀疏的情况。

Result: 在$\mathbb{R}^2$中，使用p范数成本（p≥2）时，OT能恢复潜在旋转，算法在稀疏目标域数据下表现良好。

Conclusion: 论文为基于OT的几何变换下模型适应提供了理论和实践见解。

Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is sparsely
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.

</details>


### [413] [Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques](https://arxiv.org/abs/2505.09266)
*Lirandë Pira,Airin Antony,Nayanthara Prathap,Daniel Peace,Jacquiline Romero*

Main category: physics.optics

Relevance: 40.0

TL;DR: 论文探讨了在光子芯片逆向设计中应用机器学习可解释性技术（如LIME）以提升设计透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 逆向设计的黑盒特性导致设计过程不透明，限制了对其输出的理解。

Method: 采用LIME（局部可解释模型无关解释）技术分析逆向设计优化过程，应用于双模复用器的设计。

Result: LIME提供的洞察改进了初始条件，直接提升了器件性能。

Conclusion: 可解释性技术不仅能解释模型，还能主动指导和优化逆向设计的光子器件。

Abstract: Photonic chip design has seen significant advancements with the adoption of
inverse design methodologies, offering flexibility and efficiency in optimizing
device performance. However, the black-box nature of the optimization
approaches, such as those used in inverse design in order to minimize a loss
function or maximize coupling efficiency, poses challenges in understanding the
outputs. This challenge is prevalent in machine learning-based optimization
methods, which can suffer from the same lack of transparency. To this end,
interpretability techniques address the opacity of optimization models. In this
work, we apply interpretability techniques from machine learning, with the aim
of gaining understanding of inverse design optimization used in designing
photonic components, specifically two-mode multiplexers. We base our
methodology on the widespread interpretability technique known as local
interpretable model-agnostic explanations, or LIME. As a result, LIME-informed
insights point us to more effective initial conditions, directly improving
device performance. This demonstrates that interpretability methods can do more
than explain models -- they can actively guide and enhance the inverse-designed
photonic components. Our results demonstrate the ability of interpretable
techniques to reveal underlying patterns in the inverse design process, leading
to the development of better-performing components.

</details>


### [414] [Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch](https://arxiv.org/abs/2505.09364)
*Michael Benigni,Maurizio Ferrari Dacrema,Dietmar Jannach*

Main category: cs.IR

Relevance: 40.0

TL;DR: 论文通过复现2023-2024年SIGIR会议上发表的四种基于去噪扩散概率模型的推荐系统模型，发现这些模型在方法论上仍存在问题，且性能不如简单模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证当前推荐系统研究中是否存在方法论问题，尤其是基于扩散模型的最新推荐技术是否真的优于传统方法。

Method: 复现了四种发表在SIGIR会议上的扩散模型推荐系统，并与简单模型进行对比实验。

Result: 扩散模型在推荐任务中表现不佳，且存在方法论问题，如与未调优基线模型比较。

Conclusion: 研究呼吁更高的科学严谨性和研究文化的变革。

Abstract: Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.

</details>


### [415] [Independent Component Analysis by Robust Distance Correlation](https://arxiv.org/abs/2505.09425)
*Sarah Leyder,Jakob Raymaekers,Peter J. Rousseeuw,Tom Van Deuren,Tim Verdonck*

Main category: stat.CO

Relevance: 40.0

TL;DR: 提出了一种名为RICA的鲁棒独立成分分析方法，通过最小化多元随机变量之间的鲁棒依赖度量（距离相关dCor）来估计独立成分，并引入bowl变换增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ICA方法对异常值不鲁棒，限制了其在实际应用中的效果。

Method: 使用距离相关（dCor）作为依赖度量，结合bowl变换处理异常值，并顺序估计独立成分。

Result: RICA在模拟研究中表现优于其他方法，具有强一致性和参数收敛速率。

Conclusion: RICA是一种鲁棒且有效的ICA方法，适用于含异常值的数据。

Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a
multivariate signal or distribution into fully independent sources, not just
uncorrelated ones. Unfortunately, most approaches to ICA are not robust against
outliers. Here we propose a robust ICA method called RICA, which estimates the
components by minimizing a robust measure of dependence between multivariate
random variables. The dependence measure used is the distance correlation
(dCor). In order to make it more robust we first apply a new transformation
called the bowl transform, which is bounded, one-to-one, continuous, and maps
far outliers to points close to the origin. This preserves the crucial property
that a zero dCor implies independence. RICA estimates the independent sources
sequentially, by looking for the component that has the smallest dCor with the
remainder. RICA is strongly consistent and has the usual parametric rate of
convergence. Its robustness is investigated by a simulation study, in which it
generally outperforms its competitors. The method is illustrated on three
applications, including the well-known cocktail party problem.

</details>


### [416] [Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU](https://arxiv.org/abs/2505.09430)
*Yutong Hu,Pinhao Song,Kehan Wen,Renaud Detry*

Main category: cs.RO

Relevance: 40.0

TL;DR: 提出了一种名为Mini-Diffuser的方法，通过Level-2 minibatching显著减少多任务视觉语言机器人扩散策略的训练时间和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练多任务视觉语言机器人扩散策略时存在高计算成本和内存需求的问题，Mini-Diffuser旨在解决这一问题。

Method: 利用动作扩散与图像扩散的维度差异，引入Level-2 minibatching，将多个噪声动作样本与每个视觉语言条件配对，并改进扩散Transformer架构以防止信息泄漏。

Result: 在RLBench仿真中，Mini-Diffuser达到SOTA性能的95%，同时仅需5%的训练时间和7%的内存。真实实验验证了其保留扩散策略优势的能力。

Conclusion: Mini-Diffuser是一种高效的多任务视觉语言机器人扩散策略训练方法，显著降低了计算资源需求。

Abstract: We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.

</details>


### [417] [DataMIL: Selecting Data for Robot Imitation Learning with Datamodels](https://arxiv.org/abs/2505.09603)
*Shivin Dass,Alaa Khaddaj,Logan Engstrom,Aleksander Madry,Andrew Ilyas,Roberto Martín-Martín*

Main category: cs.RO

Relevance: 40.0

TL;DR: DataMIL是一个基于策略的数据选择框架，通过端到端方式优化数据选择，提升机器人任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用机器人策略在特定任务上表现不佳的问题，避免盲目数据选择对性能的负面影响。

Method: 引入DataMIL框架，利用策略自身优化数据选择，避免昂贵的环境测试，采用替代损失函数。

Result: 在60多个模拟和真实任务中验证，性能优于基线方法。

Conclusion: 端到端、性能感知的数据选择能释放大型数据集在机器人领域的潜力。

Abstract: Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/

</details>


### [418] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出一个基于交叉性理论的框架，用于评估机器学习技术对社会身份的公平性支持。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术在社会身份支持方面存在不足，交叉性理论可以改善这一问题，但当前应用存在偏差。

Method: 基于Crenshaw、Combahee和Collins的交叉性理论，构建框架并评估机器学习文献中的（误）应用。

Result: 揭示了机器学习文献中交叉性理论应用的偏差，提出了改进方向。

Conclusion: 交叉性理论的正确应用有助于开发更公平的机器学习技术。

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [419] [Onboard Optimization and Learning: A Survey](https://arxiv.org/abs/2505.08793)
*Monirul Islam Pavel,Siyi Hu,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.LG

Relevance: 40.0

TL;DR: 这篇综述探讨了边缘AI中的板载学习方法，重点解决了资源受限设备上的实时数据处理、模型训练和安全性挑战。


<details>
  <summary>Details</summary>
Motivation: 板载学习在边缘AI中至关重要，因其支持低延迟、隐私保护和能效，但面临计算资源有限、推理成本高和安全性问题。

Method: 综述了优化模型效率、加速推理和支持分布式设备协作学习的方法，包括模型简化、推理速度提升和隐私保护计算。

Result: 通过硬件-软件协同设计、模型压缩和去中心化学习，展示了板载学习的当前进展。

Conclusion: 板载学习为边缘AI提供了高效、安全和可扩展的解决方案，但仍需进一步研究以应对动态环境中的挑战。

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [420] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
*Andres Anabalon,Hugo Garces,Julio Oliva,Jose Cifuentes*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种快速算法，将层次结构嵌入三维闵可夫斯基时空，数据相关性完全由因果结构编码。该方法仅依赖局部层次信号，无需全局符号结构，并在WordNet上验证了完美嵌入。


<details>
  <summary>Details</summary>
Motivation: 探索离散数据的几何表示，揭示层次结构与几何之间的深层联系，可能为概念和类别的几何本质提供新视角。

Method: 使用三维闵可夫斯基时空嵌入层次结构，仅依赖局部层次信号（定向令牌对），并通过因果结构编码数据相关性。

Result: 成功实现了WordNet中哺乳动物子树的完美嵌入，并扩展到82,115个名词令牌的最大无歧义子集，验证了离散数据的几何表示可能性。

Conclusion: 层次结构和概念关系本质上是几何的，且与广义相对论和场论存在深层联系。

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [421] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/abs/2505.08846)
*Felix Marti-Perez,Brigt Håvardstun,Cèsar Ferri,Carlos Monserrat,Jan Arne Telle*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文提出了评估时间序列分类器（TSC）可解释性的简化时间序列的指标，并验证了其在季节性、非平稳和低熵时间序列中的有效性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据对人类不直观，因此需要简化以提高可解释性。

Method: 提出了衡量简化复杂性和忠诚度的指标，并评估了四种简化算法在不同TSC算法和数据集上的表现。

Result: 简化时间序列在季节性、非平稳和低熵情况下显著优于原始时间序列的可解释性。

Conclusion: 简化时间序列是提升TSC可解释性的有效方法，尤其在特定数据特征下效果更佳。

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [422] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
*Gavin Hull,Alex Bihlo*

Main category: cs.LG

Relevance: 40.0

TL;DR: ForeCite是一个基于预训练因果语言模型的框架，用于预测学术论文的未来引用率，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化研究评估和加速科学进步需要预测论文的引用率。

Method: 在预训练的因果语言模型上添加线性头，用于回归任务预测平均每月引用率。

Result: 在90万+生物医学论文数据集上，测试相关性达到ρ=0.826，比之前最佳方法提升27点。

Conclusion: ForeCite为学术影响力预测设立了新标杆，并为自动化科学评估奠定了基础。

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [423] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/abs/2505.09017)
*Bizhan Alipour Pijan,Serdar Bozdag*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种结合局部和全局特征提取的动态图表示学习方法DyGSSM，利用HiPPO算法优化状态空间模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法未能同时提取局部和全局信息，且缺乏对时间依赖性的有效管理。

Method: 结合GCN提取局部特征和GRU提取全局特征，通过交叉注意力机制整合，并利用HiPPO算法优化SSM以管理长期依赖。

Result: 在五个公开数据集上，DyGSSM在20个案例中的17个优于现有方法。

Conclusion: DyGSSM通过多视角特征提取和HiPPO优化的SSM，显著提升了动态图表示学习的性能。

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [424] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/abs/2505.09089)
*Philipp Hess,Maximilian Gelbrecht,Christof Schötz,Michael Aich,Yu Huang,Shangshang Yang,Niklas Boers*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种时间一致性判别器，使预训练的图像扩散模型能够生成逼真的时空动态，无需扩展或微调模型。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）在生成逼真动态方面表现优异，但从头训练VDMs计算成本高，限制了其广泛应用。

Method: 引入时间一致性判别器，指导采样推理过程，无需修改或微调预训练的图像扩散模型。

Result: 在理想湍流模拟和真实全球降水数据集上，该方法与从头训练的VDMs表现相当，且具有更好的不确定性校准和更低偏差。

Conclusion: 该方法能够实现稳定的百年尺度气候模拟，为视频生成和处理提供了高效解决方案。

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [425] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/abs/2505.09113)
*Yingrong Wang,Anpeng Wu,Baohong Li,Ziyang Xiao,Ruoxuan Xiong,Qing Han,Kun Kuang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种名为DSIV-CFR的新框架，用于解决序列治疗效应估计中的潜在混杂偏差问题，通过分解序列工具变量和反事实回归实现。


<details>
  <summary>Details</summary>
Motivation: 在序列决策场景中，未测量的混杂因素会影响治疗分配和结果，现有基于Transformer的方法虽能捕捉时间依赖性，但仍无法解决这一问题。

Method: 提出DSIV-CFR框架，利用工具变量和负控制假设，通过广义矩条件估计序列治疗效应。

Result: 在4个数据集上的实验表明，该方法在单步和多步预测中表现优异，并能识别动态系统的最优治疗。

Conclusion: DSIV-CFR框架有效解决了序列治疗效应中的潜在混杂问题，为动态系统决策提供了新工具。

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [426] [Fair Clustering via Alignment](https://arxiv.org/abs/2505.09131)
*Kunwoong Kim,Jihu Lee,Sangchul Park,Yongdai Kim*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种新的公平聚类算法FCA，通过数据对齐和聚类中心优化，实现了高实用性和公平性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类算法因复杂性和近似性导致实用性不足或数值不稳定，FCA旨在解决这些问题。

Method: FCA通过交替进行数据对齐（联合概率分布）和聚类中心优化，避免复杂约束。

Result: 实验表明FCA在公平性和聚类实用性上优于现有方法，且数值稳定。

Conclusion: FCA为高实用性公平聚类提供了理论保证和实践优势。

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [427] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/abs/2505.09160)
*Berkay Guler,Giovanni Geraci,Hamid Jafarkhani*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了WiMAE和ContraWiMAE两种基于Transformer的无线信道表征自监督学习方法，后者通过对比学习进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法未充分考虑无线通信的独特性，因此提出针对性的模型。

Method: WiMAE是基于Transformer的编码器-解码器模型，ContraWiMAE在其基础上结合对比学习任务。

Result: 在多个下游任务中表现优异，ContraWiMAE在分离性和适应性上更优。

Conclusion: 模型为无线信道表征学习提供了高效基线。

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [428] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/abs/2505.09354)
*Guangtai Wang,Chi-Man Vong,Jintao Huang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种名为CleanSE的校准策略，通过利用数据集中的干净样本增强部分标签学习中的候选标签置信度。


<details>
  <summary>Details</summary>
Motivation: 现有部分标签学习的消歧策略主要关注单个实例特征，而忽略了数据集中随机存在的干净样本的强监督信息。

Method: 结合可微分计数损失策略和K近邻算法，CleanSE通过干净样本为候选标签提供指导，并假设干净样本的标签是其最近邻的候选标签之一时，更可能是其邻域的真实标签。

Result: 在3个合成基准和5个真实PLL数据集上的实验表明，CleanSE可应用于多数先进PLL方法并提升其性能。

Conclusion: CleanSE通过利用干净样本的监督信息，有效提升了部分标签学习的消歧能力。

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [429] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/abs/2505.09361)
*Samir Moustafa,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

Relevance: 40.0

TL;DR: MixQ-GNN框架通过混合精度量化提升图神经网络（GNN）推理效率，保证性能不下降。


<details>
  <summary>Details</summary>
Motivation: GNN计算需求高，需高效方法加速推理，混合精度量化是潜在解决方案。

Method: 提出定理保证整数消息传递的数值等价性，并设计MixQ-GNN框架灵活选择位宽。

Result: 平均减少5.5x（节点分类）和5.1x（图分类）的位操作。

Conclusion: MixQ-GNN高效且兼容现有方法，显著提升GNN推理效率。

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [430] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/abs/2505.09572)
*Julian Kranz,Davide Gallon,Steffen Dereich,Arnulf Jentzen*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文研究了全连接前馈神经网络的梯度流行为，证明了梯度流要么收敛到临界点，要么发散到无穷大。同时，存在一个阈值，使得初始损失足够接近最优值的梯度流会收敛到最优值。对于多项式目标函数，最优损失为零且只能渐近实现。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络梯度流的收敛性和发散性，以理解其优化行为及其在训练中的表现。

Method: 通过理论分析梯度流的动态行为，结合o-minimal结构的几何性质，并通过数值实验验证理论结果。

Result: 证明了梯度流在特定条件下会发散到无穷大，同时损失收敛到渐近临界值；数值实验支持了理论发现。

Conclusion: 梯度流在神经网络优化中表现出复杂的动态行为，初始化和架构设计对收敛性有重要影响。

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [431] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/abs/2505.09586)
*Yipeng Zhang,Longlong Li,Kelin Xia*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了基于菱形平铺结构（RT）的聚类方法，用于几何图的层次聚类，并设计了RTPool模型，在7个基准数据集上优于21种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络（GNNs）聚类方法主要依赖图的连通性结构，难以捕捉几何图中的丰富几何特征。

Method: 提出Rhomboid Tiling（RT）聚类方法，利用数据的复杂几何信息进行聚类，并设计RTPool模型用于图分类任务。

Result: RTPool在7个基准数据集上表现优异，优于21种现有方法。

Conclusion: RT聚类和RTPool模型能有效提取几何图的高阶几何结构，提升图分类性能。

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [432] [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
*Longtian Wang,Xiaofei Xie,Tianlin Li,Yuhan Zhi,Chao Shen*

Main category: cs.CR

Relevance: 40.0

TL;DR: TokenProber是一种用于评估T2I模型拒绝机制鲁棒性的方法，通过生成对抗性提示来揭示安全检查器的漏洞。


<details>
  <summary>Details</summary>
Motivation: T2I模型可能生成NSFW内容，现有拒绝机制存在漏洞，需要评估其鲁棒性。

Method: 通过敏感性感知差异测试，分析提示中的关键词语（脏词和差异词），生成对抗性提示。

Result: TokenProber在324个NSFW提示上测试，平均绕过率提升54%，优于现有方法。

Conclusion: TokenProber能有效揭示拒绝机制的鲁棒性问题，为改进安全机制提供依据。

Abstract: Text-to-image (T2I) models have significantly advanced in producing
high-quality images. However, such models have the ability to generate images
containing not-safe-for-work (NSFW) content, such as pornography, violence,
political content, and discrimination. To mitigate the risk of generating NSFW
content, refusal mechanisms, i.e., safety checkers, have been developed to
check potential NSFW content. Adversarial prompting techniques have been
developed to evaluate the robustness of the refusal mechanisms. The key
challenge remains to subtly modify the prompt in a way that preserves its
sensitive nature while bypassing the refusal mechanisms. In this paper, we
introduce TokenProber, a method designed for sensitivity-aware differential
testing, aimed at evaluating the robustness of the refusal mechanisms in T2I
models by generating adversarial prompts. Our approach is based on the key
observation that adversarial prompts often succeed by exploiting discrepancies
in how T2I models and safety checkers interpret sensitive content. Thus, we
conduct a fine-grained analysis of the impact of specific words within prompts,
distinguishing between dirty words that are essential for NSFW content
generation and discrepant words that highlight the different sensitivity
assessments between T2I models and safety checkers. Through the
sensitivity-aware mutation, TokenProber generates adversarial prompts, striking
a balance between maintaining NSFW content generation and evading detection.
Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I
models, using 324 NSFW prompts, demonstrates its superior effectiveness in
bypassing safety filters compared to existing methods (e.g., 54%+ increase on
average), highlighting TokenProber's ability to uncover robustness issues in
the existing refusal mechanisms.

</details>


### [433] [Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems](https://arxiv.org/abs/2505.08816)
*Ippokratis Koukoulis,Ilias Syrigos,Thanasis Korakis*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了一种基于Transformer编码器的自监督对比学习方法，用于通用的入侵检测，显著提升了异常检测和入侵识别的性能。


<details>
  <summary>Details</summary>
Motivation: 随着零日攻击的增加，传统入侵检测系统（IDS）的局限性凸显，尤其是对未见过流量模式的泛化能力不足。本文旨在通过自监督学习解决这一问题。

Method: 采用基于Transformer的自监督对比学习方法，结合数据增强策略，从原始数据包序列中学习流量表示。

Result: 在异常检测任务中，AUC分数比现有方法提高了3%（数据集内）和20%（跨数据集）；在有限标注数据的监督学习中，AUC提高了1.5%。

Conclusion: 该方法在入侵检测中表现出色，尤其在泛化能力和适应性方面优于传统方法。

Abstract: As the digital landscape becomes more interconnected, the frequency and
severity of zero-day attacks, have significantly increased, leading to an
urgent need for innovative Intrusion Detection Systems (IDS). Machine
Learning-based IDS that learn from the network traffic characteristics and can
discern attack patterns from benign traffic offer an advanced solution to
traditional signature-based IDS. However, they heavily rely on labeled
datasets, and their ability to generalize when encountering unseen traffic
patterns remains a challenge. This paper proposes a novel self-supervised
contrastive learning approach based on transformer encoders, specifically
tailored for generalizable intrusion detection on raw packet sequences. Our
proposed learning scheme employs a packet-level data augmentation strategy
combined with a transformer-based architecture to extract and generate
meaningful representations of traffic flows. Unlike traditional methods reliant
on handcrafted statistical features (NetFlow), our approach automatically
learns comprehensive packet sequence representations, significantly enhancing
performance in anomaly identification tasks and supervised learning for
intrusion detection. Our transformer-based framework exhibits better
performance in comparison to existing NetFlow self-supervised methods.
Specifically, we achieve up to a 3% higher AUC in anomaly detection for
intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset
evaluation. Moreover, our model provides a strong baseline for supervised
intrusion detection with limited labeled data, exhibiting an improvement over
self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated
on the same dataset. Additionally, we show the adaptability of our pretrained
model when fine-tuned across different datasets, demonstrating strong
performance even when lacking benign data from the target domain.

</details>


### [434] [A Comparative Review of RNA Language Models](https://arxiv.org/abs/2505.09087)
*He Wang,Yikun Zhang,Jie Chen,Jian Zhan,Yaoqi Zhou*

Main category: q-bio.BM

Relevance: 40.0

TL;DR: 论文比较了13种RNA语言模型在RNA二级结构和功能分类任务中的表现，发现模型在两项任务中的表现不平衡，需要更均衡的无监督训练。


<details>
  <summary>Details</summary>
Motivation: RNA语言模型在结构和功能推断中有用，但缺乏统一标准进行比较。

Method: 将RNA模型分为三类（多类型RNA预训练、特定用途RNA、RNA与DNA/蛋白质统一模型），并与DNA和蛋白质模型对比，评估其在零样本任务中的表现。

Result: 模型在二级结构预测和功能分类任务中表现不平衡。

Conclusion: 需要更均衡的无监督训练以提高模型性能。

Abstract: Given usefulness of protein language models (LMs) in structure and functional
inference, RNA LMs have received increased attentions in the last few years.
However, these RNA models are often not compared against the same standard.
Here, we divided RNA LMs into three classes (pretrained on multiple RNA types
(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with
DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein
LMs as controls in zero-shot prediction of RNA secondary structure and
functional classification. Results shows that the models doing well on
secondary structure prediction often perform worse in function classification
or vice versa, suggesting that more balanced unsupervised training is needed.

</details>


### [435] [Toward Malicious Clients Detection in Federated Learning](https://arxiv.org/abs/2505.09110)
*Zhihao Dou,Jiaqi Wang,Wei Sun,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

Relevance: 40.0

TL;DR: 提出了一种名为SafeFL的新算法，用于在联邦学习中准确识别恶意客户端，通过生成合成数据集来区分恶意和良性模型，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其容易受到投毒攻击，现有方法在应对高级威胁时表现不足，且常误判良性客户端。

Method: 服务器收集一系列全局模型生成合成数据集，基于行为区分恶意和良性模型。

Result: SafeFL在检测恶意客户端方面表现出更高的效率和准确性。

Conclusion: SafeFL为联邦学习中的安全问题提供了一种有效的解决方案。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global machine learning model without sharing their raw data. However, the
decentralized nature of FL introduces vulnerabilities, particularly to
poisoning attacks, where malicious clients manipulate their local models to
disrupt the training process. While Byzantine-robust aggregation rules have
been developed to mitigate such attacks, they remain inadequate against more
advanced threats. In response, recent advancements have focused on FL detection
techniques to identify potentially malicious participants. Unfortunately, these
methods often misclassify numerous benign clients as threats or rely on
unrealistic assumptions about the server's capabilities. In this paper, we
propose a novel algorithm, SafeFL, specifically designed to accurately identify
malicious clients in FL. The SafeFL approach involves the server collecting a
series of global models to generate a synthetic dataset, which is then used to
distinguish between malicious and benign models based on their behavior.
Extensive testing demonstrates that SafeFL outperforms existing methods,
offering superior efficiency and accuracy in detecting malicious clients.

</details>


### [436] [Online Learning of Neural Networks](https://arxiv.org/abs/2505.09167)
*Amit Daniely,Idan Mehalel,Elchanan Mossel*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文研究了带有符号激活函数的前馈神经网络的在线学习问题，重点分析了边缘条件对学习性能的影响，并提出了两种减少维度依赖性的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解神经网络在线学习中的边缘条件对错误界限的影响，并探索如何减少对输入维度的依赖。

Method: 通过理论分析，提出了两种限制条件：多索引模型和扩展边缘假设，以降低维度依赖性。

Result: 证明了在某些条件下，错误界限可以显著降低，例如在多索引模型中为$(1.5/\gamma)^{k + 2}$，在扩展边缘假设中为$(\log Y)/ \gamma^{O(L)}$。

Conclusion: 结论表明，通过引入额外的限制条件，可以显著减少在线学习中的错误界限，尤其是在高维输入情况下。

Abstract: We study online learning of feedforward neural networks with the sign
activation function that implement functions from the unit ball in
$\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some
cases necessary for online learnability of a neural network: Every neuron in
the first hidden layer classifies all instances with some margin $\gamma$
bounded away from zero. Quantitatively, we prove that for any net, the optimal
mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the
$(d,\gamma)$-totally-separable-packing number, a more restricted variation of
the standard $(d,\gamma)$-packing number. We complement this result by
constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many
mistakes. We also give a quantitative lower bound of approximately
$\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq
1/2$, implying that for some nets and input sequences every learner will err
for $\exp(d)$ many times, and that a dimension-free mistake bound is almost
always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional
natural restrictions to be placed on the network, so that the dependence on $d$
is removed. We study two such restrictions. The first is the multi-index model,
in which the function computed by the net depends only on $k \ll d$ orthonormal
directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in
this model. The second is the extended margin assumption. In this setting, we
assume that all neurons (in all layers) in the network classify every ingoing
input from previous layer with margin $\gamma$ bounded away from zero. In this
model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$,
where L is the depth of the network.

</details>


### [437] [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](https://arxiv.org/abs/2505.09229)
*Brian Britos,Mathias Bourel*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文提出了一种基于最优传输（OT）的方法，用于解决线性回归模型在旋转偏移下的监督域适应问题，结合K-means聚类和SVD估计旋转角度。


<details>
  <summary>Details</summary>
Motivation: 研究在传感器校准或图像定向等应用中，源域和目标域之间存在旋转关系时的模型适应问题。

Method: 结合K-means聚类、最优传输和奇异值分解（SVD）来估计旋转角度并调整回归模型。

Result: 在目标域采样稀疏时，该方法能有效利用丰富的源域数据提升泛化能力。

Conclusion: 研究为基于OT的几何变换下模型适应提供了理论和实践见解。

Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is sparsely
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.

</details>


### [438] [Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques](https://arxiv.org/abs/2505.09266)
*Lirandë Pira,Airin Antony,Nayanthara Prathap,Daniel Peace,Jacquiline Romero*

Main category: physics.optics

Relevance: 40.0

TL;DR: 论文探讨了在光子芯片设计中应用机器学习解释性技术（如LIME）以提升逆向设计优化的透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 逆向设计方法在光子芯片设计中具有高效性，但其黑箱特性导致输出难以理解。研究旨在通过解释性技术解决这一问题。

Method: 采用LIME（局部可解释模型无关解释）技术分析逆向设计优化过程，特别针对双模复用器。

Result: LIME提供的见解优化了初始条件，直接提升了器件性能，表明解释性技术可指导设计。

Conclusion: 解释性技术不仅能解释模型，还能主动优化光子组件的逆向设计过程。

Abstract: Photonic chip design has seen significant advancements with the adoption of
inverse design methodologies, offering flexibility and efficiency in optimizing
device performance. However, the black-box nature of the optimization
approaches, such as those used in inverse design in order to minimize a loss
function or maximize coupling efficiency, poses challenges in understanding the
outputs. This challenge is prevalent in machine learning-based optimization
methods, which can suffer from the same lack of transparency. To this end,
interpretability techniques address the opacity of optimization models. In this
work, we apply interpretability techniques from machine learning, with the aim
of gaining understanding of inverse design optimization used in designing
photonic components, specifically two-mode multiplexers. We base our
methodology on the widespread interpretability technique known as local
interpretable model-agnostic explanations, or LIME. As a result, LIME-informed
insights point us to more effective initial conditions, directly improving
device performance. This demonstrates that interpretability methods can do more
than explain models -- they can actively guide and enhance the inverse-designed
photonic components. Our results demonstrate the ability of interpretable
techniques to reveal underlying patterns in the inverse design process, leading
to the development of better-performing components.

</details>


### [439] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/abs/2505.09304)
*Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Yuxuan Zhang,Bengt Oelmann,Sebastian Bader*

Main category: cs.SD

Relevance: 40.0

TL;DR: 论文提出了一种低计算量的方法，用于关键词识别（KWS）中预训练神经网络的连续噪声适应，仅需单次学习和一个周期。该方法在资源受限设备上表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准KWS系统在嵌入式设备上性能下降，需要动态适应以应对噪声等挑战。

Method: 提出一种低计算量的连续噪声适应方法，适用于预训练神经网络，仅需1-shot学习和一个周期。

Result: 在多种噪声环境下，适应后的模型性能显著优于预训练模型，尤其在低信噪比（SNR ≤ 18 dB）时，准确率提升4.9%至46.0%。

Conclusion: 该方法高效且轻量，适合资源受限设备部署。

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


### [440] [Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch](https://arxiv.org/abs/2505.09364)
*Michael Benigni,Maurizio Ferrari Dacrema,Dietmar Jannach*

Main category: cs.IR

Relevance: 40.0

TL;DR: 论文研究了推荐系统中扩散模型的应用，发现其性能被夸大且方法存在问题，呼吁更严格的科学标准。


<details>
  <summary>Details</summary>
Motivation: 探讨推荐系统研究中扩散模型的真实进展，揭示方法学问题。

Method: 复现2023-2024年SIGIR会议上的四篇扩散模型推荐系统论文。

Result: 扩散模型在推荐任务中表现不如简单模型，且存在方法学问题。

Conclusion: 需提高科学严谨性，改变研究文化。

Abstract: Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.

</details>


### [441] [Independent Component Analysis by Robust Distance Correlation](https://arxiv.org/abs/2505.09425)
*Sarah Leyder,Jakob Raymaekers,Peter J. Rousseeuw,Tom Van Deuren,Tim Verdonck*

Main category: stat.CO

Relevance: 40.0

TL;DR: 提出了一种鲁棒的独立成分分析方法（RICA），通过最小化距离相关性（dCor）来估计独立成分，并引入bowl变换增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ICA方法对异常值不鲁棒，需要一种更稳健的方法来分解多变量信号。

Method: 使用距离相关性（dCor）作为依赖度量，结合bowl变换处理异常值，并顺序估计独立成分。

Result: RICA在模拟研究中表现优于竞争对手，并在实际应用中验证了其有效性。

Conclusion: RICA是一种鲁棒且一致的ICA方法，适用于包含异常值的数据。

Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a
multivariate signal or distribution into fully independent sources, not just
uncorrelated ones. Unfortunately, most approaches to ICA are not robust against
outliers. Here we propose a robust ICA method called RICA, which estimates the
components by minimizing a robust measure of dependence between multivariate
random variables. The dependence measure used is the distance correlation
(dCor). In order to make it more robust we first apply a new transformation
called the bowl transform, which is bounded, one-to-one, continuous, and maps
far outliers to points close to the origin. This preserves the crucial property
that a zero dCor implies independence. RICA estimates the independent sources
sequentially, by looking for the component that has the smallest dCor with the
remainder. RICA is strongly consistent and has the usual parametric rate of
convergence. Its robustness is investigated by a simulation study, in which it
generally outperforms its competitors. The method is illustrated on three
applications, including the well-known cocktail party problem.

</details>


### [442] [DataMIL: Selecting Data for Robot Imitation Learning with Datamodels](https://arxiv.org/abs/2505.09603)
*Shivin Dass,Alaa Khaddaj,Logan Engstrom,Aleksander Madry,Andrew Ilyas,Roberto Martín-Martín*

Main category: cs.RO

Relevance: 40.0

TL;DR: DataMIL是一个基于策略的数据选择框架，通过端到端优化数据选择来提升机器人策略性能，避免传统基于人工质量标准的低效方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略在多样化任务上表现良好，但在特定任务上表现不佳，且需要针对新任务数据进行微调。传统数据选择方法可能损害性能。

Method: DataMIL利用策略自身优化数据选择，通过替代损失函数避免昂贵的环境交互，直接从任务数据中选择提升性能的数据。

Result: 在60多个仿真和真实世界任务中，DataMIL显著提升了成功率，优于多个基线方法。

Conclusion: 端到端、性能感知的数据选择是释放大型数据集潜力的关键。

Abstract: Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/

</details>


### [443] [Onboard Optimization and Learning: A Survey](https://arxiv.org/abs/2505.08793)
*Monirul Islam Pavel,Siyi Hu,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文是一篇关于边缘AI中板载学习的综述，探讨了在资源受限设备上实现实时数据处理和自适应模型训练的方法，重点关注模型效率优化、推理加速和分布式协作学习。


<details>
  <summary>Details</summary>
Motivation: 板载学习对于低延迟、高隐私和能效的应用至关重要，但面临计算资源有限、推理成本高和安全漏洞等挑战。

Method: 综述了优化模型效率、加速推理、支持分布式协作学习的方法，包括模型复杂度降低、推理速度提升和隐私保护计算。

Result: 通过硬件-软件协同设计、模型压缩和去中心化学习，提供了当前板载学习的现状和解决方案。

Conclusion: 板载学习为边缘AI提供了高效、安全和可扩展的部署方案。

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [444] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/abs/2505.08940)
*Jeremie Blanchard,Lisa Casino,Jordan Gierschendorf*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文探讨了利用机器学习技术从模拟光谱数据中提取大气成分的方法，重点研究了不确定性估计对性能的影响，并指出了表格建模和特征工程的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过Ariel数据挑战赛，探索机器学习在分析系外行星大气光谱中的应用，特别关注泛化能力而非竞赛优化。

Method: 方法包括特征提取、信号转换和异方差不确定性建模，通过实验验证了不确定性估计对高斯对数似然分数的重要性。

Result: 结果显示不确定性估计显著提升了性能（GLL分数提高11%），但也揭示了表格建模和特征工程的局限性。

Conclusion: 结论强调了在复杂的天体物理数据分析中，模型简单性、可解释性和泛化能力之间的权衡。

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [445] [GPML: Graph Processing for Machine Learning](https://arxiv.org/abs/2505.08964)
*Majed Jaber,Julien Michel,Nicolas Boutry,Pierre Parrend*

Main category: cs.LG

Relevance: 30.0

TL;DR: GPML是一个图处理库，用于将网络流量数据转化为图表示，以检测动态网络中的异常和社区变化，支持实时检测和历史分析。


<details>
  <summary>Details</summary>
Motivation: 动态网络中复杂、多步且快速演变的攻击日益增多，需要先进的威胁检测工具。

Method: GPML通过将原始网络流量数据转换为图表示，提取社区和谱度量，支持实时和历史分析。

Result: GPML提供了一种基于图的强大方法，用于检测网络异常和社区变化。

Conclusion: GPML为现代网络安全挑战提供了一种有效的图处理解决方案。

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [446] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/abs/2505.08982)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种针对未知线性随机系统的在线预测方法，通过指数遗忘平衡回归模型，提高了预测准确性并减少了累积误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于递归最小二乘的方法在未知系统中可能因回归模型不平衡导致性能下降，本文旨在解决这一问题。

Method: 引入指数遗忘作为归纳偏置，平衡回归模型，优化回归与正则化误差的权衡。

Result: 实现了更优的预测准确性，减少了累积误差，并提供了更严格的O(log^3 N)遗憾界。

Conclusion: 通过指数遗忘平衡回归模型是一种有效的在线预测方法，适用于未知线性随机系统。

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [447] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/abs/2505.09063)
*Khalid Rafiq,Wenjing Liao,Aditya G. Nair*

Main category: cs.LG

Relevance: 30.0

TL;DR: Flexi-VAE是一个高效的单次预测非线性参数偏微分方程（PDEs）的数据驱动框架，无需迭代时间步进，同时保持高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统PDE预测方法中迭代时间步进的低效问题，提供一种更高效且稳定的替代方案。

Method: 结合神经传播器和变分自编码器（VAE），提出两种传播策略（DCP和PEP），并通过几何诊断分析优化潜在空间。

Result: 在1D和2D PDE基准测试中表现出色，比基线方法快50倍（CPU）和90倍（GPU）。

Conclusion: Flexi-VAE是一种可扩展且可解释的替代建模工具，适用于计算流体动力学（CFD）等应用。

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [448] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/abs/2505.09076)
*Berkay Guler,Hamid Jafarkhani*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种名为AdaFortiTran的新型模型，用于增强OFDM系统中的信道估计性能，结合卷积层和Transformer编码器，显著降低了MSE。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在快速衰落信道和低信噪比场景下性能下降的问题。

Method: 结合卷积层（捕捉局部相关性）和Transformer编码器（全局注意力机制），并整合信道统计信息作为先验。

Result: 在多种测试条件下，AdaFortiTran比现有模型降低了6 dB的MSE，表现出更强的鲁棒性。

Conclusion: AdaFortiTran在挑战性环境中显著提升了信道估计性能。

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [449] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/abs/2505.09106)
*Ya Liu,Kai Yang,Yu Zhu,Keying Yang,Haibo Zhao*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文提出了一种名为Argus的异步算法，用于解决SAGIN网络中的非凸非光滑分散联邦双层学习问题，避免了传统集中式同步优化算法的不足。


<details>
  <summary>Details</summary>
Motivation: 由于SAGIN网络的基础设施缺乏和环境时变性，传统集中式同步优化算法不适用，因此需要开发一种异步算法来解决这一问题。

Method: 提出Argus算法，支持网络代理（如自主飞行器）在时变网络中异步处理双层学习问题，避免拖慢整体训练速度。

Result: 通过理论分析和数值实验验证了Argus算法的迭代复杂度、通信复杂度和计算复杂度，并证明了其有效性。

Conclusion: Argus算法在SAGIN网络中表现出色，解决了传统算法的局限性，为异步联邦学习提供了新思路。

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [450] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/abs/2505.09134)
*Daniel Huang*

Main category: cs.LG

Relevance: 30.0

TL;DR: DSoftKI是一种可扩展的高斯过程方法，能够拟合和预测全导数观测，适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 扩展SoftKI方法以处理导数观测，提升核函数的近似能力。

Method: 通过改进SoftKI的插值方案，引入插值点方向性，构建可扩展的近似核函数及其导数。

Result: 在合成函数基准和高维分子力场预测中表现出高准确性和可扩展性。

Conclusion: DSoftKI能够处理更大规模的全导数观测数据集，优于现有方法。

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [451] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/abs/2505.09160)
*Berkay Guler,Giovanni Geraci,Hamid Jafarkhani*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了WiMAE和ContraWiMAE两种基于Transformer的无线信道表示学习模型，通过自监督学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法未充分考虑无线通信的独特性，需针对性设计模型。

Method: WiMAE为基于Transformer的编码器-解码器模型；ContraWiMAE在WiMAE基础上引入对比学习目标。

Result: 两种模型在未见场景中表现优异，ContraWiMAE在线性可分性和适应性上更优。

Conclusion: 模型为无线信道表示学习提供了高效基线。

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [452] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/abs/2505.09287)
*Shunsuke Yoneda,Valdemar Švábenský,Gen Li,Daisuke Deguchi,Atsushi Shimada*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种结合联邦学习和差分特征的方法，用于解决教育数据挖掘中的隐私问题，并在多课程数据上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决教育数据挖掘中因隐私问题导致的数据孤岛问题，提升模型的通用性和性能。

Method: 结合联邦学习和差分特征，前者避免数据集中化以保护隐私，后者通过相对值提升模型性能。

Result: 在多课程数据上验证，性能与集中学习相当，差分特征显著提升预测表现。

Conclusion: 该方法有效平衡隐私保护与模型性能，适用于早期风险学生预测。

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [453] [On the Learning with Augmented Class via Forests](https://arxiv.org/abs/2505.09294)
*Fan Xu,Wuyang Chen,Wei Gao*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文提出了一种名为LACForest的方法，通过引入增强Gini不纯度准则，利用测试数据中的未标记信息来学习增强类，并构建浅层森林和深度神经森林以提高性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决测试数据中可能出现的增强类（未在训练数据中出现）的问题，通过改进决策树和森林的学习方法。

Method: 方法包括引入增强Gini不纯度准则，构建浅层森林（LACForest）和深度神经森林，并利用伪标记增强实例进行分割。

Result: 实验验证了方法的有效性，理论分析提供了增强Gini不纯度的收敛性证明。

Conclusion: 结论表明LACForest能够有效处理测试数据中的增强类，并通过神经网络的表示能力进一步提升性能。

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [454] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/abs/2505.09331)
*Cunlai Pu,Fangrui Wu,Rajput Ramiz Sharafat,Guangzhao Dai,Xiangbo Shu*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种多尺度结构-时间链接预测模型（MUST），用于无人机自组网（UANETs）中的链接预测，解决了高动态性和稀疏性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 在对抗环境中，无人机路由信息不可用，预测未来链接需依赖历史拓扑信息。现有方法多关注单一结构尺度的时间动态，忽视了稀疏性影响，导致信息捕获不足。

Method: 结合图注意力网络（GATs）捕获多尺度结构特征（个体、社区、网络层面），使用LSTM学习时间动态，并通过优化的损失函数解决稀疏性问题。

Result: 在多个模拟UANET数据集上验证，MUST在高度动态和稀疏环境中实现了最先进的链接预测性能。

Conclusion: MUST通过多尺度结构-时间建模和稀疏性优化，显著提升了UANETs中的链接预测准确性。

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [455] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

Relevance: 30.0

TL;DR: Online-iForest是一种专为流数据设计的在线异常检测方法，无需重复访问数据或周期性重训练，性能与离线方法相当且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法多为离线设计，不适应流数据场景，且在线方法常依赖周期性重训练。

Method: 提出Online-iForest，直接适应流数据生成过程的动态变化。

Result: 在真实数据集上验证，性能与离线方法相当，效率优于所有竞争对手。

Conclusion: Online-iForest是高效在线异常检测的有力解决方案，适用于网络安全、欺诈检测等场景。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [456] [Equilibrium Propagation for Learning in Lagrangian Dynamical Systems](https://arxiv.org/abs/2505.07363)
*Serge Massar*

Main category: nlin.CD

Relevance: 30.0

TL;DR: 提出了一种基于拉格朗日力学和平衡传播的动态系统训练方法，适用于周期性边界条件或固定初始/终止状态的系统。


<details>
  <summary>Details</summary>
Motivation: 扩展平衡传播方法以训练动态系统，避免显式时间反向传播。

Method: 利用作用极值原理，通过微调轨迹并测量参数响应来训练系统。

Result: 适用于周期性边界条件或固定状态的系统，并可推广到耗散系统。

Conclusion: 该方法为动态系统提供了一种高效的训练方式，尤其在周期性边界条件下具有量子平衡传播的半经典极限。

Abstract: We propose a method for training dynamical systems governed by Lagrangian
mechanics using Equilibrium Propagation. Our approach extends Equilibrium
Propagation -- initially developed for energy-based models -- to dynamical
trajectories by leveraging the principle of action extremization. Training is
achieved by gently nudging trajectories toward desired targets and measuring
how the variables conjugate to the parameters to be trained respond. This
method is particularly suited to systems with periodic boundary conditions or
fixed initial and final states, enabling efficient parameter updates without
requiring explicit backpropagation through time. In the case of periodic
boundary conditions, this approach yields the semiclassical limit of Quantum
Equilibrium Propagation. Applications to systems with dissipation are also
discussed.

</details>


### [457] [The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics](https://arxiv.org/abs/2505.08822)
*Yuhao Wang,Kailai Wang,Songhua Hu,Yunpeng,Zhang,Gino Lim,Pengyu Zhu*

Main category: cs.CY

Relevance: 30.0

TL;DR: 论文提出了一种结合Transformer和GCN的BiTransGCN框架，用于预测和分析交通网络安全生态系统中的访客流模式及其社会经济影响。


<details>
  <summary>Details</summary>
Motivation: 研究交通网络安全生态系统的时空动态，以支持经济规划和网络韧性建设。

Method: 开发了BiTransGCN框架，结合注意力机制Transformer和GCN，用于建模和预测访客流模式。

Result: 框架提高了对行业集群和流动性趋势的跟踪与预测能力，为经济规划和投资提供数据支持。

Conclusion: BiTransGCN为交通网络安全生态系统的战略规划提供了数据驱动的解决方案。

Abstract: The rapid evolution of the transportation cybersecurity ecosystem,
encompassing cybersecurity, automotive, and transportation and logistics
sectors, will lead to the formation of distinct spatial clusters and visitor
flow patterns across the US. This study examines the spatiotemporal dynamics of
visitor flows, analyzing how socioeconomic factors shape industry clustering
and workforce distribution within these evolving sectors. To model and predict
visitor flow patterns, we develop a BiTransGCN framework, integrating an
attention-based Transformer architecture with a Graph Convolutional Network
backbone. By integrating AI-enabled forecasting techniques with spatial
analysis, this study improves our ability to track, interpret, and anticipate
changes in industry clustering and mobility trends, thereby supporting
strategic planning for a secure and resilient transportation network. It offers
a data-driven foundation for economic planning, workforce development, and
targeted investments in the transportation cybersecurity ecosystem.

</details>


### [458] [ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation](https://arxiv.org/abs/2505.08986)
*Amirreza Davar,Zhengtong Xu,Siavash Mahmoudi,Pouya Sohrabipour,Chaitanya Pallerla,Yu She,Wan Shou,Philip Crandall,Dongyi Wang*

Main category: cs.RO

Relevance: 30.0

TL;DR: ChicGrasp是一个硬件-软件协同设计的系统，用于自动化家禽加工线上的抓取任务，通过模仿学习实现了对易变形生物产品的可靠操作。


<details>
  <summary>Details</summary>
Motivation: 解决家禽加工线上人工操作效率低、易损坏的问题，同时满足严格的卫生要求。

Method: 采用双爪气动夹持器和条件扩散策略控制器，通过50次多视角远程操作演示训练，实现5自由度末端执行器运动规划。

Result: 系统在生鸡胴体上的抓取和提升成功率为40.6%，完成一个抓取-悬挂周期需38秒，优于现有基准方法。

Conclusion: 模仿学习可以弥补刚性硬件与可变生物产品之间的差距，为农业工程和机器人学习提供了可复现的基准和公开数据集。

Abstract: Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.

</details>


### [459] [Statistical Mean Estimation with Coded Relayed Observations](https://arxiv.org/abs/2505.09098)
*Yan Hao Ling,Zhouhao Yang,Jonathan Scarlett*

Main category: cs.IT

Relevance: 30.0

TL;DR: 论文研究了通过中继（“教师”）和信道传输的统计均值估计问题，在大偏差机制下建立了紧致的误差指数，并对比了两种基线方法的次优性。


<details>
  <summary>Details</summary>
Motivation: 探讨在信息通过信道传输时的统计均值估计问题，特别是在大偏差机制下的最优性能。

Method: 研究了伯努利源和二进制对称信道的情况，随后推广到亚高斯、重尾分布及任意离散无记忆信道。

Result: 建立了在大偏差机制下紧致的误差指数，并证明两种基线方法性能次优。

Conclusion: 通过信道传输的统计均值估计在大偏差机制下可以实现紧致的误差指数，优于基线方法。

Abstract: We consider a problem of statistical mean estimation in which the samples are
not observed directly, but are instead observed by a relay (``teacher'') that
transmits information through a memoryless channel to the decoder
(``student''), who then produces the final estimate. We consider the minimax
estimation error in the large deviations regime, and establish achievable error
exponents that are tight in broad regimes of the estimation accuracy and
channel quality. In contrast, two natural baseline methods are shown to yield
strictly suboptimal error exponents. We initially focus on Bernoulli sources
and binary symmetric channels, and then generalize to sub-Gaussian and
heavy-tailed settings along with arbitrary discrete memoryless channels.

</details>


### [460] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/abs/2505.09304)
*Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Yuxuan Zhang,Bengt Oelmann,Sebastian Bader*

Main category: cs.SD

Relevance: 30.0

TL;DR: 论文提出了一种低计算量的方法，用于预训练神经网络在关键词识别（KWS）中的连续噪声适应，仅需单次学习和一个周期。该方法在资源受限设备上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式设备上关键词识别系统在真实环境中性能下降的问题，尤其是噪声适应能力不足的挑战。

Method: 通过1-shot学习和单周期训练，实现预训练神经网络的连续噪声适应。

Result: 在多种噪声环境下，适应后的模型性能显著优于预训练模型，尤其在低信噪比（SNR ≤ 18 dB）时，准确率提升4.9%至46.0%。

Conclusion: 该方法轻量高效，适用于资源受限设备，显著提升了关键词识别系统的噪声适应能力。

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


### [461] [Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios](https://arxiv.org/abs/2505.09516)
*Siyi Wang,Alexandre Leblanc,Paul D. McNicholas*

Main category: stat.ME

Relevance: 30.0

TL;DR: 本文提出了一种基于数据深度的局部中心聚类方法（DLCC），通过局部数据深度识别多模态数据的中心和非凸簇，并引入新的内部评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通常针对特定场景设计且存在局限性，无法有效处理多模态和非凸簇数据。

Method: DLCC利用局部数据深度识别局部中心和不同形状的簇，并提出了基于密度的内部评估指标。

Result: DLCC能够克服传统聚类方法的局限性，提升多模态和非凸簇数据的分析能力。

Conclusion: DLCC是一种灵活的聚类方法，适用于广泛的应用场景。

Abstract: Cluster analysis, or clustering, plays a crucial role across numerous
scientific and engineering domains. Despite the wealth of clustering methods
proposed over the past decades, each method is typically designed for specific
scenarios and presents certain limitations in practical applications. In this
paper, we propose depth-based local center clustering (DLCC). This novel method
makes use of data depth, which is known to produce a center-outward ordering of
sample points in a multivariate space. However, data depth typically fails to
capture the multimodal characteristics of {data}, something of the utmost
importance in the context of clustering. To overcome this, DLCC makes use of a
local version of data depth that is based on subsets of {data}. From this,
local centers can be identified as well as clusters of varying shapes.
Furthermore, we propose a new internal metric based on density-based clustering
to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a
flexible clustering approach that seems to overcome some limitations of
traditional clustering methods, thereby enhancing data analysis capabilities
across a wide range of application scenarios.

</details>


### [462] [Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods](https://arxiv.org/abs/2505.09552)
*Pascal Kündig,Fabio Sigrist*

Main category: stat.ME

Relevance: 30.0

TL;DR: 提出了一种基于Krylov子空间的新方法，用于高效处理高维交叉随机效应的混合效应模型，显著提升了计算速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Cholesky分解的方法在处理高维交叉随机效应时计算效率低下，亟需更高效的解决方案。

Method: 采用Krylov子空间方法，结合共轭梯度法和随机Lanczos求积法，并优化预处理器，推导新的收敛结果。

Result: 实验表明，新方法在计算速度和稳定性上显著优于现有方法，如lme4和glmmTMB，运行时减少约两个数量级。

Conclusion: 新方法为高维交叉随机效应的混合效应模型提供了高效且稳定的计算解决方案，已集成到GPBoost软件库中。

Abstract: Mixed effects models are widely used for modeling data with hierarchically
grouped structures and high-cardinality categorical predictor variables.
However, for high-dimensional crossed random effects, current standard
computations relying on Cholesky decompositions can become prohibitively slow.
In this work, we present novel Krylov subspace-based methods that address
several existing computational bottlenecks. Among other things, we
theoretically analyze and empirically evaluate various preconditioners for the
conjugate gradient and stochastic Lanczos quadrature methods, derive new
convergence results, and develop computationally efficient methods for
calculating predictive variances. Extensive experiments using simulated and
real-world data sets show that our proposed methods scale much better than
Cholesky-based computations, for instance, achieving a runtime reduction of
approximately two orders of magnitudes for both estimation and prediction.
Moreover, our software implementation is up to 10'000 times faster and more
stable than state-of-the-art implementations such as lme4 and glmmTMB when
using default settings. Our methods are implemented in the free C++ software
library GPBoost with high-level Python and R packages.

</details>


### [463] [Adaptively-weighted Nearest Neighbors for Matrix Completion](https://arxiv.org/abs/2505.09612)
*Tathagata Sadhukhan,Manit Paul,Raaz Dwivedi*

Main category: stat.ML

Relevance: 30.0

TL;DR: AWNN是一种自适应加权最近邻方法，用于矩阵补全，通过平衡偏差-方差权衡优化权重选择，无需依赖交叉验证。


<details>
  <summary>Details</summary>
Motivation: 解决传统最近邻方法在权重和半径选择上缺乏系统性方法的问题。

Method: 提出自适应加权最近邻方法（AWNN），通过理论分析和实验验证其有效性。

Result: AWNN在合成实验中表现良好，并提供理论保证。

Conclusion: AWNN为矩阵补全提供了一种高效且理论支持的方法。

Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.

</details>


### [464] [GPML: Graph Processing for Machine Learning](https://arxiv.org/abs/2505.08964)
*Majed Jaber,Julien Michel,Nicolas Boutry,Pierre Parrend*

Main category: cs.LG

Relevance: 30.0

TL;DR: GPML库通过将网络流量数据转化为图表示，提供工具检测动态网络中的异常和社区变化，支持实时检测和历史取证分析。


<details>
  <summary>Details</summary>
Motivation: 动态网络中复杂、多步骤和快速演变的攻击增加，需要先进的网络威胁检测工具。

Method: GPML库将原始网络流量数据转化为图表示，支持社区和谱度量提取。

Result: 该库增强了实时检测和历史取证分析能力。

Conclusion: GPML为现代网络安全挑战提供了基于图的强大解决方案。

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [465] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/abs/2505.08982)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种针对未知线性随机系统的在线预测方法，通过指数遗忘平衡回归模型，优化了回归与正则化误差的权衡，并减少了累积误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于递归最小二乘的方法在未知系统中可能因回归模型不平衡导致性能下降，本文旨在解决这一问题。

Method: 通过指数遗忘技术注入归纳偏置，平衡回归模型，优化误差权衡。

Result: 提出了更优的对数遗憾界O(log^3 N)，显著提升了预测精度。

Conclusion: 该方法有效解决了未知系统中的预测问题，为在线学习提供了新思路。

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [466] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/abs/2505.09063)
*Khalid Rafiq,Wenjing Liao,Aditya G. Nair*

Main category: cs.LG

Relevance: 30.0

TL;DR: Flexi-VAE是一个高效的单次预测非线性参数偏微分方程（PDEs）的数据驱动框架，无需迭代时间步进，同时保持高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统PDE预测方法中迭代时间步进的高计算成本问题，提供高效且稳定的替代方案。

Method: 结合神经传播器和变分自编码器（VAE），提出两种传播策略（DCP和PEP），并通过表示理论分析优化潜在空间。

Result: 在1D和2D PDE基准测试中表现出色，实现了50x CPU和90x GPU加速，优于基线模型。

Conclusion: Flexi-VAE是一个可扩展且可解释的替代建模工具，适用于计算流体动力学等参数PDE驱动的应用。

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [467] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/abs/2505.09076)
*Berkay Guler,Hamid Jafarkhani*

Main category: cs.LG

Relevance: 30.0

TL;DR: AdaFortiTran是一种结合卷积和Transformer的模型，用于提升OFDM系统中信道估计的性能，尤其在快速衰落和低信噪比环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在快速衰落信道和低信噪比场景下性能下降的问题。

Method: 结合卷积层（捕捉局部相关性）和Transformer编码器（全局注意力机制），并整合信道统计信息作为先验。

Result: 在多种测试条件下，AdaFortiTran的均方误差比现有最优模型降低6 dB。

Conclusion: AdaFortiTran在复杂信道环境下表现出卓越的鲁棒性和性能。

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [468] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/abs/2505.09106)
*Ya Liu,Kai Yang,Yu Zhu,Keying Yang,Haibo Zhao*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文提出了一种名为Argus的异步算法，用于解决空间-空中-地面一体化网络（SAGIN）中的非凸非光滑分散联邦双层学习问题。


<details>
  <summary>Details</summary>
Motivation: 传统集中式和同步优化算法在SAGIN中不适用，因其基础设施不足且环境时变。

Method: 开发了异步算法Argus，允许网络代理（如自主飞行器）在时变网络中异步解决双层学习问题。

Result: 提供了Argus的迭代复杂度、通信复杂度和计算复杂度的理论分析，并通过数值实验验证了其有效性。

Conclusion: Argus在SAGIN中表现出高效性，避免了慢速节点拖慢整体训练速度。

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [469] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/abs/2505.09174)
*Xinyu You,Xiang Liu,Chuan-Shen Hu,Kelin Xia,Tze Chien Sum*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种基于商复形的新型表示方法和QCformer模型，用于预测混合有机-无机钙钛矿（HOIPs）的材料性质，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）难以捕捉周期性结构和高阶相互作用，限制了在材料性质预测中的应用。

Method: 使用商复形表示材料结构，结合基于单纯形的Transformer模块（QCformer），预训练后微调。

Result: QCformer在HOIP性质预测中优于现有模型。

Conclusion: 商复形表示和QCformer为钙钛矿材料的预测建模提供了新工具。

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [470] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/abs/2505.09218)
*Alexander Tyurin,Danil Sivtsov*

Main category: cs.LG

Relevance: 30.0

TL;DR: Birch SGD是一个新的统一框架，用于分析和设计分布式SGD方法，通过计算树的几何性质简化收敛分析，并设计了八种新方法。


<details>
  <summary>Details</summary>
Motivation: 提出一个统一的框架来理解和设计分布式SGD方法，以简化收敛分析并优化性能。

Method: 将每个方法表示为加权有向树（计算树），通过树的几何性质分析收敛性，并设计新方法。

Result: 设计了八种新方法，其中至少六种具有最优计算时间复杂度，揭示了所有方法的迭代速率相同但存在不同权衡。

Conclusion: Birch SGD为理解和设计高效异步和并行优化方法提供了统一基础。

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [471] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/abs/2505.09287)
*Shunsuke Yoneda,Valdemar Švábenský,Gen Li,Daisuke Deguchi,Atsushi Shimada*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该研究提出了一种结合联邦学习和差分特征的方法，用于解决教育数据挖掘中的隐私问题，并在预测高风险学生方面取得了与集中学习相当的性能。


<details>
  <summary>Details</summary>
Motivation: 教育数据挖掘中，隐私问题限制了跨学校数据的整合，导致模型泛化能力不足。

Method: 结合联邦学习和差分特征，前者保护隐私，后者提升模型性能和泛化能力。

Result: 在12门课程的数据上验证，性能与集中学习相当，差分特征显著提升了预测性能。

Conclusion: 该方法有效解决了隐私问题，同时提升了模型的泛化能力和早期预测性能。

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [472] [On the Learning with Augmented Class via Forests](https://arxiv.org/abs/2505.09294)
*Fan Xu,Wuyang Chen,Wei Gao*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种名为LACForest的方法，通过引入增强Gini不纯度作为新的分裂准则，处理测试数据中可能出现的增强类别。该方法结合浅层森林和深度神经森林，利用伪标记数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统决策树和森林方法假设训练和测试数据的类别相同，但实际应用中可能存在测试数据中出现训练数据中未见的增强类别。本文旨在解决这一问题。

Method: 提出增强Gini不纯度作为分裂准则，结合浅层森林和深度神经森林，利用伪标记数据优化性能。

Result: 理论分析了增强Gini不纯度的收敛性，实验验证了方法的有效性。

Conclusion: LACForest方法能有效处理测试数据中的增强类别，结合神经网络的表示能力进一步提升了性能。

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [473] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/abs/2505.09331)
*Cunlai Pu,Fangrui Wu,Rajput Ramiz Sharafat,Guangzhao Dai,Xiangbo Shu*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种多尺度结构-时间链接预测模型（MUST），用于无人机自组网（UANETs）中的链接预测，解决了高动态性和稀疏性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 在无人机自组网中，由于网络拓扑高度动态且稀疏，传统链接预测方法难以有效捕捉结构和时间模式。

Method: 结合图注意力网络（GATs）和多尺度结构特征（个体、社区、网络级别），并利用LSTM学习时间动态性，同时通过改进的损失函数应对稀疏性。

Result: 实验表明，MUST在高动态和稀疏的UANETs中实现了最先进的链接预测性能。

Conclusion: MUST通过多尺度结构和时间特征的结合，显著提升了UANETs中的链接预测准确性。

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [474] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.09366)
*SeyedMojtaba Mohasel,Alireza Afzal Aghaei,Corey Pew*

Main category: cs.LG

Relevance: 30.0

TL;DR: 研究了可学习激活函数在KANs中用于个性化控制的潜力，比较了用户特定数据和池化数据对ML和DL模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索可学习激活函数在KANs中的表现，以及数据来源对模型性能的影响，以改进假肢控制中的转向意图预测。

Method: 收集了五名截肢者的IMU数据，比较了MLP、KAN、CNN和FKAN的分类性能，并评估了用户特定和池化数据的效果。

Result: 可学习激活函数未显著提升性能；用户特定数据对ML模型更优，但对DL模型无显著差异。

Conclusion: 可学习激活函数可能在更复杂任务中表现更好；池化数据在DL模型中可行。

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [475] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

Relevance: 30.0

TL;DR: Online-iForest是一种专为流数据设计的在线异常检测方法，无需重复访问数据或周期性重训练，性能接近离线方法且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法多为离线或需周期性重训练，难以适应流数据场景。

Method: 提出Online-iForest，直接适应流数据生成过程的变化。

Result: 在真实数据集上表现接近在线方法，媲美离线方法，且效率显著优于所有竞争对手。

Conclusion: Online-iForest是高效流数据异常检测的有力解决方案，适用于网络安全、欺诈检测等场景。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [476] [Equilibrium Propagation for Learning in Lagrangian Dynamical Systems](https://arxiv.org/abs/2505.07363)
*Serge Massar*

Main category: nlin.CD

Relevance: 30.0

TL;DR: 提出了一种基于拉格朗日力学的动态系统训练方法，利用平衡传播原理，适用于周期性边界条件或固定初始/终止状态的系统。


<details>
  <summary>Details</summary>
Motivation: 扩展平衡传播方法至动态轨迹训练，解决传统时间反向传播的效率问题，并探索其在量子平衡传播中的应用。

Method: 通过轻微调整轨迹至目标状态，测量参数共轭变量的响应，实现参数更新，无需显式时间反向传播。

Result: 方法适用于周期性边界条件或固定状态系统，且在量子平衡传播中表现出半经典极限特性。

Conclusion: 该方法为动态系统训练提供了高效替代方案，尤其在周期性或固定边界条件下表现优越。

Abstract: We propose a method for training dynamical systems governed by Lagrangian
mechanics using Equilibrium Propagation. Our approach extends Equilibrium
Propagation -- initially developed for energy-based models -- to dynamical
trajectories by leveraging the principle of action extremization. Training is
achieved by gently nudging trajectories toward desired targets and measuring
how the variables conjugate to the parameters to be trained respond. This
method is particularly suited to systems with periodic boundary conditions or
fixed initial and final states, enabling efficient parameter updates without
requiring explicit backpropagation through time. In the case of periodic
boundary conditions, this approach yields the semiclassical limit of Quantum
Equilibrium Propagation. Applications to systems with dissipation are also
discussed.

</details>


### [477] [The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics](https://arxiv.org/abs/2505.08822)
*Yuhao Wang,Kailai Wang,Songhua Hu,Yunpeng,Zhang,Gino Lim,Pengyu Zhu*

Main category: cs.CY

Relevance: 30.0

TL;DR: 该研究提出了一种结合Transformer和GCN的BiTransGCN框架，用于预测交通网络安全生态系统中的访客流模式，支持经济规划和劳动力发展。


<details>
  <summary>Details</summary>
Motivation: 研究交通网络安全生态系统中访客流模式的时空动态，以支持战略规划和网络韧性建设。

Method: 开发了BiTransGCN框架，结合基于注意力的Transformer和图卷积网络（GCN）。

Result: 通过AI预测技术和空间分析，提高了对行业集群和流动性趋势的跟踪和预测能力。

Conclusion: 研究为经济规划、劳动力发展和交通网络安全生态系统的投资提供了数据驱动的基础。

Abstract: The rapid evolution of the transportation cybersecurity ecosystem,
encompassing cybersecurity, automotive, and transportation and logistics
sectors, will lead to the formation of distinct spatial clusters and visitor
flow patterns across the US. This study examines the spatiotemporal dynamics of
visitor flows, analyzing how socioeconomic factors shape industry clustering
and workforce distribution within these evolving sectors. To model and predict
visitor flow patterns, we develop a BiTransGCN framework, integrating an
attention-based Transformer architecture with a Graph Convolutional Network
backbone. By integrating AI-enabled forecasting techniques with spatial
analysis, this study improves our ability to track, interpret, and anticipate
changes in industry clustering and mobility trends, thereby supporting
strategic planning for a secure and resilient transportation network. It offers
a data-driven foundation for economic planning, workforce development, and
targeted investments in the transportation cybersecurity ecosystem.

</details>


### [478] [Statistical Decision Theory with Counterfactual Loss](https://arxiv.org/abs/2505.08908)
*Benedikt Koch,Kosuke Imai*

Main category: math.ST

Relevance: 30.0

TL;DR: 论文扩展了经典统计决策理论，通过引入反事实损失来评估决策质量，解决了仅依赖观察结果的局限性。


<details>
  <summary>Details</summary>
Motivation: 经典统计决策理论仅基于观察结果评估决策，无法考虑反事实结果，限制了决策质量的全面评估。

Method: 通过假设强可忽略性，论文证明了反事实风险的可识别性，并提出加性反事实损失函数。

Result: 在强可忽略性假设下，反事实风险可识别当且仅当损失函数在潜在结果中为加性。加性反事实损失可能导致不同于标准损失函数的治疗建议。

Conclusion: 论文为决策理论提供了更全面的评估框架，尤其在多治疗选项的场景下。

Abstract: Classical statistical decision theory evaluates treatment choices based
solely on observed outcomes. However, by ignoring counterfactual outcomes, it
cannot assess the quality of decisions relative to feasible alternatives. For
example, the quality of a physician's decision may depend not only on patient
survival, but also on whether a less invasive treatment could have produced a
similar result. To address this limitation, we extend standard decision theory
to incorporate counterfactual losses--criteria that evaluate decisions using
all potential outcomes. The central challenge in this generalization is
identification: because only one potential outcome is observed for each unit,
the associated risk under a counterfactual loss is generally not identifiable.
We show that under the assumption of strong ignorability, a counterfactual risk
is identifiable if and only if the counterfactual loss function is additive in
the potential outcomes. Moreover, we demonstrate that additive counterfactual
losses can yield treatment recommendations that differ from those based on
standard loss functions, provided that the decision problem involves more than
two treatment options.

</details>


### [479] [Risk Bounds For Distributional Regression](https://arxiv.org/abs/2505.09075)
*Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla,Sabyasachi Chatterjee*

Main category: stat.ML

Relevance: 30.0

TL;DR: 该论文研究了非参数分布回归估计器的风险界限，提出了针对CRPS和MSE的理论上限，并应用于等渗和趋势滤波分布回归，同时验证了神经网络估计器的实用性。


<details>
  <summary>Details</summary>
Motivation: 研究非参数分布回归估计器的风险界限，以提供理论支持并验证其在实际数据中的有效性。

Method: 通过理论分析建立CRPS和MSE的上限，并应用于等渗和趋势滤波分布回归及神经网络估计器。

Result: 理论结果与均值估计的收敛速率一致，实验验证了方法的有效性。

Conclusion: 论文为非参数分布回归提供了理论框架，并在实践中验证了其效果。

Abstract: This work examines risk bounds for nonparametric distributional regression
estimators. For convex-constrained distributional regression, general upper
bounds are established for the continuous ranked probability score (CRPS) and
the worst-case mean squared error (MSE) across the domain. These theoretical
results are applied to isotonic and trend filtering distributional regression,
yielding convergence rates consistent with those for mean estimation.
Furthermore, a general upper bound is derived for distributional regression
under non-convex constraints, with a specific application to neural
network-based estimators. Comprehensive experiments on both simulated and real
data validate the theoretical contributions, demonstrating their practical
effectiveness.

</details>


### [480] [Statistical Mean Estimation with Coded Relayed Observations](https://arxiv.org/abs/2505.09098)
*Yan Hao Ling,Zhouhao Yang,Jonathan Scarlett*

Main category: cs.IT

Relevance: 30.0

TL;DR: 论文研究了通过中继（教师）和信道（学生）传递样本的统计均值估计问题，提出了在大偏差机制下的最小最大估计误差，并证明了在某些情况下基线方法的次优性。


<details>
  <summary>Details</summary>
Motivation: 研究通过信道传递样本的统计均值估计问题，探索在大偏差机制下的最优估计误差。

Method: 采用最小最大估计误差框架，分析Bernoulli源和二进制对称信道，并推广到亚高斯、重尾分布及任意离散无记忆信道。

Result: 建立了在大偏差机制下紧致的误差指数，并证明基线方法的次优性。

Conclusion: 通过中继和信道传递样本的统计均值估计在大偏差机制下可以实现最优误差指数。

Abstract: We consider a problem of statistical mean estimation in which the samples are
not observed directly, but are instead observed by a relay (``teacher'') that
transmits information through a memoryless channel to the decoder
(``student''), who then produces the final estimate. We consider the minimax
estimation error in the large deviations regime, and establish achievable error
exponents that are tight in broad regimes of the estimation accuracy and
channel quality. In contrast, two natural baseline methods are shown to yield
strictly suboptimal error exponents. We initially focus on Bernoulli sources
and binary symmetric channels, and then generalize to sub-Gaussian and
heavy-tailed settings along with arbitrary discrete memoryless channels.

</details>


### [481] [Imitation Learning for Adaptive Control of a Virtual Soft Exoglove](https://arxiv.org/abs/2505.09099)
*Shirui Lyu,Vittorio Caggiano,Matteo Leonetti,Dario Farina,Letizia Gionfrida*

Main category: cs.RO

Relevance: 30.0

TL;DR: 提出了一种基于强化学习和生物力学模型的定制化可穿戴机器人控制器，用于补偿手部运动障碍患者的特定肌肉缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有康复训练中常忽视患者肌肉损失的独特性，需针对性解决方案。

Method: 结合强化学习和生物力学模型，通过演示学习训练操作模型，并模拟神经运动障碍以补偿。

Result: 虚拟可穿戴机器人手套辅助效果显著，恢复原操作能力的90.5%。

Conclusion: 该方法为个性化康复训练提供了有效工具。

Abstract: The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.

</details>


### [482] [Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures](https://arxiv.org/abs/2505.09161)
*Yu Xin,Peng Liu,Zhuohang Xie,Wenhui Mi,Pengyue Gao,Hong Jian Zhao,Jian Lv,Yanchao Wang,Yanming Ma*

Main category: cond-mat.mtrl-sci

Relevance: 30.0

TL;DR: 论文提出了一种基于可合成性的晶体结构预测框架，结合对称性引导和机器学习模型，有效筛选出可合成材料。


<details>
  <summary>Details</summary>
Motivation: 解决传统热力学能量驱动的晶体结构预测方法难以识别实验可合成的亚稳态材料的问题。

Method: 结合对称性引导的结构推导和基于Wyckoff编码的机器学习模型，筛选可合成结构子空间，并利用结构可合成性评估模型和从头计算。

Result: 成功预测了13种已知XSe结构，并从554,054个候选结构中筛选出92,310个潜在可合成结构。

Conclusion: 该框架为无机材料合成提供了数据驱动的机器学习辅助范式，有望缩小计算预测与实验实现的差距。

Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has
revolutionized materials discovery, the energy-driven CSP approaches often
struggle to identify experimentally realizable metastable materials synthesized
through kinetically controlled pathways, creating a critical gap between
theoretical predictions and experimental synthesis. Here, we propose a
synthesizability-driven CSP framework that integrates symmetry-guided structure
derivation with a Wyckoff encode-based machine-learning model, allowing for the
efficient localization of subspaces likely to yield highly synthesizable
structures. Within the identified promising subspaces, a structure-based
synthesizability evaluation model, fine-tuned using recently synthesized
structures to enhance predictive accuracy, is employed in conjunction with ab
initio calculations to systematically identify synthesizable candidates. The
framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,
Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting
synthesizable structures. Notably, 92,310 structures are filtered from the
554,054 candidates predicted by GNoME, exhibiting great potential for promising
synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =
Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$
candidates exhibit high synthesizability, presenting viable candidates for
experimental realization and potentially associated with experimentally
observed temperature-induced phase transitions. This work establishes a
data-driven paradigm for machine-learning-assisted inorganic materials
synthesis, highlighting its potential to bridge the gap between computational
predictions and experimental realization while unlocking new opportunities for
the targeted discovery of novel functional materials.

</details>


### [483] [Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach](https://arxiv.org/abs/2505.09313)
*Qiangqiang Liu,Qian Huang,Frank Fan,Haishan Wu,Xueyan Tang*

Main category: cs.CR

Relevance: 30.0

TL;DR: 提出了一种基于子图特征提取和lightGBM的新型Sybil地址识别方法，用于区块链安全。


<details>
  <summary>Details</summary>
Motivation: Sybil攻击对区块链生态系统构成重大安全威胁，尤其是在代币空投事件中。

Method: 构建双层交易子图，提取Sybil地址生命周期的关键事件操作特征（如首次交易时间、首次获取gas等），并结合金额和网络结构特征。

Result: 在包含193,701个地址的数据集上，所有指标（精确率、召回率、F1分数和AUC）均超过0.9。

Conclusion: 该方法可扩展到更广泛的区块链安全领域，如交易操纵识别和流动性风险评估。

Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems,
particularly in token airdrop events. This paper proposes a novel sybil address
identification method based on subgraph feature extraction lightGBM. The method
first constructs a two-layer deep transaction subgraph for each address, then
extracts key event operation features according to the lifecycle of sybil
addresses, including the time of first transaction, first gas acquisition,
participation in airdrop activities, and last transaction. These temporal
features effectively capture the consistency of sybil address behavior
operations. Additionally, the method extracts amount and network structure
features, comprehensively describing address behavior patterns and network
topology through feature propagation and fusion. Experiments conducted on a
dataset containing 193,701 addresses (including 23,240 sybil addresses) show
that this method outperforms existing approaches in terms of precision, recall,
F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of
this study can be further applied to broader blockchain security areas such as
transaction manipulation identification and token liquidity risk assessment,
contributing to the construction of a more secure and fair blockchain
ecosystem.

</details>


### [484] [Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU](https://arxiv.org/abs/2505.09430)
*Yutong Hu,Pinhao Song,Kehan Wen,Renaud Detry*

Main category: cs.RO

Relevance: 30.0

TL;DR: Mini-Diffuser是一种训练多任务视觉语言机器人扩散策略的方法，通过利用动作扩散与图像扩散之间的维度不对称性，显著减少训练时间和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略训练方法在计算资源和时间上消耗巨大，尤其是在处理高维视觉语言条件和低维动作空间时效率低下。

Method: 提出Level-2 minibatching策略，将多个噪声动作样本与每个视觉语言条件配对，并改进扩散Transformer架构以防止信息泄漏。

Result: 在RLBench模拟中，Mini-Diffuser达到SOTA性能的95%，同时仅需5%的训练时间和7%的内存。

Conclusion: Mini-Diffuser在保持扩散策略优势的同时，显著提升了训练效率，适用于实际机器人任务。

Abstract: We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.

</details>


### [485] [Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios](https://arxiv.org/abs/2505.09516)
*Siyi Wang,Alexandre Leblanc,Paul D. McNicholas*

Main category: stat.ME

Relevance: 30.0

TL;DR: 提出了一种基于数据深度的局部中心聚类方法（DLCC），解决了传统聚类方法在捕捉多模态数据特性上的不足，并引入了一种新的内部评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通常针对特定场景设计，存在局限性，尤其是在处理多模态和非凸数据时表现不佳。

Method: DLCC利用局部数据深度识别局部中心和不同形状的簇，并提出了基于密度的内部评估指标。

Result: DLCC能够灵活处理多模态和非凸数据，克服了传统方法的局限性。

Conclusion: DLCC为广泛的应用场景提供了更强大的数据分析能力。

Abstract: Cluster analysis, or clustering, plays a crucial role across numerous
scientific and engineering domains. Despite the wealth of clustering methods
proposed over the past decades, each method is typically designed for specific
scenarios and presents certain limitations in practical applications. In this
paper, we propose depth-based local center clustering (DLCC). This novel method
makes use of data depth, which is known to produce a center-outward ordering of
sample points in a multivariate space. However, data depth typically fails to
capture the multimodal characteristics of {data}, something of the utmost
importance in the context of clustering. To overcome this, DLCC makes use of a
local version of data depth that is based on subsets of {data}. From this,
local centers can be identified as well as clusters of varying shapes.
Furthermore, we propose a new internal metric based on density-based clustering
to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a
flexible clustering approach that seems to overcome some limitations of
traditional clustering methods, thereby enhancing data analysis capabilities
across a wide range of application scenarios.

</details>


### [486] [Adaptively-weighted Nearest Neighbors for Matrix Completion](https://arxiv.org/abs/2505.09612)
*Tathagata Sadhukhan,Manit Paul,Raaz Dwivedi*

Main category: stat.ML

Relevance: 30.0

TL;DR: AWNN是一种自适应加权最近邻方法，用于矩阵补全，解决了传统最近邻方法中半径和权重选择的问题。


<details>
  <summary>Details</summary>
Motivation: 传统最近邻方法在矩阵补全中表现良好，但缺乏系统化的半径和权重选择方法，AWNN旨在解决这一问题。

Method: AWNN通过平衡加权最近邻回归中的偏差-方差权衡，自适应选择半径和权重，无需依赖交叉验证。

Result: 理论分析和合成实验验证了AWNN的有效性和鲁棒性。

Conclusion: AWNN为矩阵补全提供了一种高效且理论支持的自适应最近邻方法。

Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.

</details>


### [487] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/abs/2505.09174)
*Xinyu You,Xiang Liu,Chuan-Shen Hu,Kelin Xia,Tze Chien Sum*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文提出了一种基于商复形（QC）的新表示方法，并设计了商复形Transformer（QCformer）用于预测材料性质，特别针对混合有机-无机钙钛矿（HOIPs）。QCformer通过捕捉高阶相互作用和周期性结构，在HOIP性质预测中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）难以捕捉材料中的周期性结构和高阶相互作用，限制了其在材料性质预测中的应用。

Method: 提出商复形表示方法，结合基于单纯形的Transformer模块，预训练于Materials Project和JARVIS数据集，并在HOIP数据集上微调。

Result: QCformer在HOIP性质预测中优于现有模型。

Conclusion: 商复形表示和QCformer为钙钛矿材料的预测建模提供了新工具。

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [488] [Bounding Neyman-Pearson Region with $f$-Divergences](https://arxiv.org/abs/2505.08899)
*Andrew Mullhaupt,Cheng Peng*

Main category: math.ST

Relevance: 20.0

TL;DR: 论文提出了关于二元假设检验中Neyman-Pearson边界的新下界和上界，改进了Pinsker不等式，并提供了构造分布对的方法。


<details>
  <summary>Details</summary>
Motivation: 研究二元假设检验中Neyman-Pearson边界的精确描述及其优化，以改进现有理论和方法。

Method: 利用$f$-散度建立下界，使用Chernoff系数推导上界，并设计构造分布对的算法。

Result: 得到了Neyman-Pearson边界的新下界和上界，改进了Pinsker不等式，并实现了边界近似或精确构造。

Conclusion: 论文为二元假设检验提供了更精确的边界描述和构造方法，对统计理论有重要贡献。

Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of
points whose coordinates represent the false positive rate and false negative
rate of some test. The lower boundary of this region is given by the
Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the
optimal ROC curve. We establish a novel lower bound for the boundary in terms
of any $f$-divergence. Since the bound generated by hockey-stick
$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best
possible. In the case of KL divergence, this bound improves Pinsker's
inequality. Furthermore, we obtain a closed-form refined upper bound for the
Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally,
we present methods for constructing pairs of distributions that can
approximately or exactly realize any given Neyman-Pearson boundary.

</details>


### [489] [Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes](https://arxiv.org/abs/2505.09026)
*Domniki Ladopoulou,Dat Minh Hong,Petros Dellaportas*

Main category: stat.AP

Relevance: 20.0

TL;DR: 论文提出了一种非平稳高斯过程框架，用于风功率的概率预测，通过广义谱混合核捕捉非平稳性和异方差性，在短期预测中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 风功率预测对电网稳定和可再生能源高效整合至关重要，传统高斯过程模型的平稳核无法有效建模风功率的非平稳特性。

Method: 采用广义谱混合（GSM）核的非平稳高斯过程框架，以捕捉风功率数据的时间变化模式和异方差行为。

Result: 在真实SCADA数据上，GSM核模型在短期预测中优于传统的径向基函数和谱混合核。

Conclusion: 非平稳高斯过程模型在风功率预测中具有实际应用价值，尤其是短期预测。

Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining
grid stability and enabling efficient integration of renewable energy sources.
Gaussian Process (GP) models offer a principled framework for quantifying
uncertainty; however, conventional approaches rely on stationary kernels, which
are inadequate for modeling the inherently non-stationary nature of wind speed
and power output. We propose a non-stationary GP framework that incorporates
the generalized spectral mixture (GSM) kernel, enabling the model to capture
time-varying patterns and heteroscedastic behaviors in wind speed and wind
power data. We evaluate the performance of the proposed model on real-world
SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons.
Compared to standard radial basis function and spectral mixture kernels, the
GSM-based model outperforms, particularly in short-term forecasts. These
results highlight the necessity of modeling non-stationarity in wind power
forecasting and demonstrate the practical value of non-stationary GP models in
operational settings.

</details>


### [490] [Risk Bounds For Distributional Regression](https://arxiv.org/abs/2505.09075)
*Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla,Sabyasachi Chatterjee*

Main category: stat.ML

Relevance: 20.0

TL;DR: 该论文研究了非参数分布回归估计器的风险界限，提出了CRPS和MSE的理论上界，并应用于等渗和趋势滤波回归，验证了理论的实用性。


<details>
  <summary>Details</summary>
Motivation: 探索非参数分布回归估计器的风险界限，为实际应用提供理论支持。

Method: 通过凸约束和非凸约束下的分布回归，推导CRPS和MSE的上界，并应用于等渗回归和神经网络估计器。

Result: 理论结果在模拟和真实数据实验中验证了其有效性。

Conclusion: 论文为非参数分布回归提供了理论框架，并展示了其实际应用价值。

Abstract: This work examines risk bounds for nonparametric distributional regression
estimators. For convex-constrained distributional regression, general upper
bounds are established for the continuous ranked probability score (CRPS) and
the worst-case mean squared error (MSE) across the domain. These theoretical
results are applied to isotonic and trend filtering distributional regression,
yielding convergence rates consistent with those for mean estimation.
Furthermore, a general upper bound is derived for distributional regression
under non-convex constraints, with a specific application to neural
network-based estimators. Comprehensive experiments on both simulated and real
data validate the theoretical contributions, demonstrating their practical
effectiveness.

</details>


### [491] [Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures](https://arxiv.org/abs/2505.09161)
*Yu Xin,Peng Liu,Zhuohang Xie,Wenhui Mi,Pengyue Gao,Hong Jian Zhao,Jian Lv,Yanchao Wang,Yanming Ma*

Main category: cond-mat.mtrl-sci

Relevance: 20.0

TL;DR: 论文提出了一种基于可合成性的晶体结构预测框架，结合对称性引导和机器学习模型，有效预测实验可合成的材料结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统热力学能量驱动的晶体结构预测方法难以预测实验可合成的亚稳态材料的问题。

Method: 结合对称性引导的结构推导和Wyckoff编码的机器学习模型，定位高可合成性结构子空间，并利用基于结构的可合成性评估模型筛选候选材料。

Result: 成功预测了13种已知XSe结构和92,310种潜在可合成结构，并识别出8种热力学有利的Hf-X-O结构。

Conclusion: 该框架为机器学习辅助的无机材料合成提供了数据驱动范式，有望缩小计算预测与实验合成之间的差距。

Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has
revolutionized materials discovery, the energy-driven CSP approaches often
struggle to identify experimentally realizable metastable materials synthesized
through kinetically controlled pathways, creating a critical gap between
theoretical predictions and experimental synthesis. Here, we propose a
synthesizability-driven CSP framework that integrates symmetry-guided structure
derivation with a Wyckoff encode-based machine-learning model, allowing for the
efficient localization of subspaces likely to yield highly synthesizable
structures. Within the identified promising subspaces, a structure-based
synthesizability evaluation model, fine-tuned using recently synthesized
structures to enhance predictive accuracy, is employed in conjunction with ab
initio calculations to systematically identify synthesizable candidates. The
framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,
Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting
synthesizable structures. Notably, 92,310 structures are filtered from the
554,054 candidates predicted by GNoME, exhibiting great potential for promising
synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =
Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$
candidates exhibit high synthesizability, presenting viable candidates for
experimental realization and potentially associated with experimentally
observed temperature-induced phase transitions. This work establishes a
data-driven paradigm for machine-learning-assisted inorganic materials
synthesis, highlighting its potential to bridge the gap between computational
predictions and experimental realization while unlocking new opportunities for
the targeted discovery of novel functional materials.

</details>


### [492] [Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach](https://arxiv.org/abs/2505.09313)
*Qiangqiang Liu,Qian Huang,Frank Fan,Haishan Wu,Xueyan Tang*

Main category: cs.CR

Relevance: 20.0

TL;DR: 该论文提出了一种基于子图特征提取和lightGBM的新型Sybil地址识别方法，用于区块链生态系统中的安全威胁检测。


<details>
  <summary>Details</summary>
Motivation: Sybil攻击对区块链生态系统构成重大安全威胁，尤其是在代币空投事件中。现有方法在识别Sybil地址时存在局限性，因此需要更高效的方法。

Method: 通过构建双层深度的交易子图，提取Sybil地址生命周期的关键事件操作特征（如首次交易时间、首次获取gas等），并结合金额和网络结构特征进行综合分析。

Result: 在包含193,701个地址的数据集上，该方法在精确率、召回率、F1分数和AUC上均超过0.9，优于现有方法。

Conclusion: 该方法可进一步应用于更广泛的区块链安全领域，如交易操纵识别和代币流动性风险评估。

Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems,
particularly in token airdrop events. This paper proposes a novel sybil address
identification method based on subgraph feature extraction lightGBM. The method
first constructs a two-layer deep transaction subgraph for each address, then
extracts key event operation features according to the lifecycle of sybil
addresses, including the time of first transaction, first gas acquisition,
participation in airdrop activities, and last transaction. These temporal
features effectively capture the consistency of sybil address behavior
operations. Additionally, the method extracts amount and network structure
features, comprehensively describing address behavior patterns and network
topology through feature propagation and fusion. Experiments conducted on a
dataset containing 193,701 addresses (including 23,240 sybil addresses) show
that this method outperforms existing approaches in terms of precision, recall,
F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of
this study can be further applied to broader blockchain security areas such as
transaction manipulation identification and token liquidity risk assessment,
contributing to the construction of a more secure and fair blockchain
ecosystem.

</details>


### [493] [Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders](https://arxiv.org/abs/2505.09506)
*María Alejandra Hernández,Oscar Rodriguez,Dae-Jin Lee*

Main category: stat.ML

Relevance: 20.0

TL;DR: 提出了一种基于自编码器的深度学习框架Deep-SITAR，用于预测人类生长轨迹，结合了深度学习的灵活性和传统混合效应模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统SITAR模型在预测个体生长轨迹时需要重新估计模型，效率较低。本文旨在通过深度学习提升预测效率。

Method: 使用自编码器架构，编码器估计个体随机效应，解码器基于B样条拟合生长曲线。

Result: Deep-SITAR能够高效预测新个体的生长轨迹，无需重新估计模型。

Conclusion: Deep-SITAR结合了深度学习的灵活性和传统模型的可解释性，为生长轨迹预测提供了新方法。

Abstract: Several approaches have been developed to capture the complexity and
nonlinearity of human growth. One widely used is the Super Imposition by
Translation and Rotation (SITAR) model, which has become popular in studies of
adolescent growth. SITAR is a shape-invariant mixed-effects model that
represents the shared growth pattern of a population using a natural cubic
spline mean curve while incorporating three subject-specific random effects --
timing, size, and growth intensity -- to account for variations among
individuals. In this work, we introduce a supervised deep learning framework
based on an autoencoder architecture that integrates a deep neural network
(neural network) with a B-spline model to estimate the SITAR model. In this
approach, the encoder estimates the random effects for each individual, while
the decoder performs a fitting based on B-splines similar to the classic SITAR
model. We refer to this method as the Deep-SITAR model. This innovative
approach enables the prediction of the random effects of new individuals
entering a population without requiring a full model re-estimation. As a
result, Deep-SITAR offers a powerful approach to predicting growth
trajectories, combining the flexibility and efficiency of deep learning with
the interpretability of traditional mixed-effects models.

</details>


### [494] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/abs/2505.08940)
*Jeremie Blanchard,Lisa Casino,Jordan Gierschendorf*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文探讨了机器学习在分析系外行星大气光谱数据中的应用，强调泛化性而非竞赛优化，并指出不确定性估计对性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过机器学习技术从模拟光谱数据中提取系外行星大气成分，探索数据为中心的方法，而非竞赛优化。

Method: 采用数据为中心的方法，包括特征提取、信号变换和异方差不确定性建模，评估其对高斯对数似然分数的影响。

Result: 不确定性估计显著提升性能（GLL分数提高11%），但表格建模和特征工程存在固有局限性。

Conclusion: 研究强调了模型简单性、可解释性和泛化性在天体物理数据分析中的权衡。

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [495] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/abs/2505.09134)
*Daniel Huang*

Main category: cs.LG

Relevance: 20.0

TL;DR: DSoftKI是一种可扩展的高斯过程方法，能够拟合和预测全导数观测数据，扩展了SoftKI方法以支持导数。


<details>
  <summary>Details</summary>
Motivation: 解决在高维数据（如分子力场预测）中处理全导数观测的可扩展性问题。

Method: 通过改进SoftKI的插值方案，结合插值点的方向性信息，构建可扩展的近似核函数及其导数。

Result: 在合成函数基准和高维分子力场预测中表现出高准确性，并能处理更大规模的数据集。

Conclusion: DSoftKI为处理全导数观测的高维数据提供了一种有效的可扩展方法。

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [496] [ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation](https://arxiv.org/abs/2505.08986)
*Amirreza Davar,Zhengtong Xu,Siavash Mahmoudi,Pouya Sohrabipour,Chaitanya Pallerla,Yu She,Wan Shou,Philip Crandall,Dongyi Wang*

Main category: cs.RO

Relevance: 20.0

TL;DR: ChicGrasp是一个硬件-软件协同设计的系统，用于自动化家禽加工线上的鸡腿抓取任务，通过模仿学习实现高效操作。


<details>
  <summary>Details</summary>
Motivation: 传统吸盘和脚本化动作在家禽加工中不可靠，需解决鸡腿抓取的高效性和适应性。

Method: 使用双爪气动夹持器和条件扩散策略控制器，基于50次多视角遥操作演示训练。

Result: 系统抓取成功率为40.6%，完成一次抓取到悬挂的循环时间为38秒，优于现有基线方法。

Conclusion: 模仿学习可弥补硬件与生物产品间的差异，为农业工程和机器人学习提供可复现基准。

Abstract: Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.

</details>


### [497] [Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders](https://arxiv.org/abs/2505.09506)
*María Alejandra Hernández,Oscar Rodriguez,Dae-Jin Lee*

Main category: stat.ML

Relevance: 20.0

TL;DR: 提出了一种基于自编码器架构的Deep-SITAR模型，结合深度神经网络和B样条模型，用于预测人类生长轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统SITAR模型需要重新估计新个体的随机效应，而Deep-SITAR通过深度学习框架实现了高效预测，同时保留了传统模型的解释性。

Method: 使用自编码器架构，编码器估计个体随机效应，解码器基于B样条拟合生长曲线。

Result: Deep-SITAR能够预测新个体的生长轨迹，无需重新估计模型，结合了深度学习的灵活性和传统模型的解释性。

Conclusion: Deep-SITAR为生长轨迹预测提供了高效且可解释的方法。

Abstract: Several approaches have been developed to capture the complexity and
nonlinearity of human growth. One widely used is the Super Imposition by
Translation and Rotation (SITAR) model, which has become popular in studies of
adolescent growth. SITAR is a shape-invariant mixed-effects model that
represents the shared growth pattern of a population using a natural cubic
spline mean curve while incorporating three subject-specific random effects --
timing, size, and growth intensity -- to account for variations among
individuals. In this work, we introduce a supervised deep learning framework
based on an autoencoder architecture that integrates a deep neural network
(neural network) with a B-spline model to estimate the SITAR model. In this
approach, the encoder estimates the random effects for each individual, while
the decoder performs a fitting based on B-splines similar to the classic SITAR
model. We refer to this method as the Deep-SITAR model. This innovative
approach enables the prediction of the random effects of new individuals
entering a population without requiring a full model re-estimation. As a
result, Deep-SITAR offers a powerful approach to predicting growth
trajectories, combining the flexibility and efficiency of deep learning with
the interpretability of traditional mixed-effects models.

</details>


### [498] [Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods](https://arxiv.org/abs/2505.09552)
*Pascal Kündig,Fabio Sigrist*

Main category: stat.ME

Relevance: 20.0

TL;DR: 论文提出了一种基于Krylov子空间的新方法，用于解决高维交叉随机效应模型中的计算瓶颈，显著提升了计算效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Cholesky分解的计算方法在处理高维交叉随机效应时效率低下，亟需更高效的替代方案。

Method: 采用Krylov子空间方法，结合共轭梯度法和随机Lanczos求积法，并分析不同预处理器的效果。

Result: 实验表明，新方法在计算时间和稳定性上显著优于现有方法，如lme4和glmmTMB，速度提升可达10000倍。

Conclusion: 新方法在高维交叉随机效应模型中具有显著优势，已通过GPBoost库实现。

Abstract: Mixed effects models are widely used for modeling data with hierarchically
grouped structures and high-cardinality categorical predictor variables.
However, for high-dimensional crossed random effects, current standard
computations relying on Cholesky decompositions can become prohibitively slow.
In this work, we present novel Krylov subspace-based methods that address
several existing computational bottlenecks. Among other things, we
theoretically analyze and empirically evaluate various preconditioners for the
conjugate gradient and stochastic Lanczos quadrature methods, derive new
convergence results, and develop computationally efficient methods for
calculating predictive variances. Extensive experiments using simulated and
real-world data sets show that our proposed methods scale much better than
Cholesky-based computations, for instance, achieving a runtime reduction of
approximately two orders of magnitudes for both estimation and prediction.
Moreover, our software implementation is up to 10'000 times faster and more
stable than state-of-the-art implementations such as lme4 and glmmTMB when
using default settings. Our methods are implemented in the free C++ software
library GPBoost with high-level Python and R packages.

</details>


### [499] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/abs/2505.09011)
*Antonio Candito,Matthew D Blackledge,Richard Holbrey,Nuria Porta,Ana Ribeiro,Fabio Zugni,Luca D'Erme,Francesca Castagnoli,Alina Dragan,Ricardo Donners,Christina Messiou,Nina Tunariu,Dow-Mu Koh*

Main category: cs.LG

Relevance: 10.0

TL;DR: 该论文提出了一种AI驱动的软件解决方案，用于从全身扩散加权成像（WB-DWI）扫描中量化转移性骨病。核心包括弱监督的Residual U-Net模型、统计框架和浅层卷积神经网络，用于生成骨骼概率图和病变掩膜。实验验证了其准确性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 开发一种自动化工具，用于从WB-DWI扫描中量化转移性骨病，以支持临床决策。

Method: 结合弱监督Residual U-Net、统计框架和浅层CNN，生成骨骼概率图和病变掩膜，并提取TDV和gADC统计数据。

Result: Dice分数为0.6，表面距离为2mm；log-TDV和gADC的重复性变异系数分别为4.57%和3.54%；软件评估治疗反应的准确率为80.5%。

Conclusion: 该软件能够从WB-DWI扫描中提供可重复的TDV和gADC量化，为临床决策提供支持。

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [500] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/abs/2505.09175)
*Mohammad Ganjirad,Mahmoud Reza Delavar,Hossein Bagheri,Mohammad Mehdi Azizi*

Main category: cs.LG

Relevance: 10.0

TL;DR: 论文提出了一种基于社会经济、环境和敏感性指标的框架，用于优先发展德黑兰的城市绿地。通过多种机器学习模型（如XGBoost、LightGBM、RF等）进行分类和评估，RF表现最佳。最终生成了绿地发展优先级地图，并通过微气候模拟验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决城市绿地规划中缺乏系统性和数据驱动方法的问题，为城市规划者提供科学依据。

Method: 结合多种数据源（如Google Earth Engine、WRF模型等），使用机器学习模型（如RF）进行分类和优先级评估，并通过微气候模拟验证效果。

Result: RF模型在植被覆盖分类中表现最佳（准确率>94%），绿地发展优先级地图显示夜间地表温度和敏感人口是关键指标，绿色屋顶技术可降低气温0.67°C。

Conclusion: 该框架为城市规划者提供了数据驱动的绿地发展工具，具有实际应用价值。

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [501] [ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections](https://arxiv.org/abs/2505.09365)
*H. T. Rüdisser,G. Nguyen,J. Le Louëdec,C. Möstl*

Main category: physics.space-ph

Relevance: 10.0

TL;DR: 论文提出了一种名为ARCANE的框架，用于实时检测太阳风数据中的ICMEs事件，比较了基于机器学习的ResUNet++模型与阈值基线方法，结果显示ResUNet++表现更优。


<details>
  <summary>Details</summary>
Motivation: ICMEs是空间天气扰动的主要驱动因素，对技术和人类活动构成风险，因此需要实时检测方法。

Method: 提出了ARCANE框架，使用ResUNet++模型和阈值基线方法进行ICMEs检测，并比较了实时太阳风数据与高分辨率科学数据的效果。

Result: ResUNet++显著优于基线方法，F1得分为0.53，检测延迟为事件持续时间的21.5%。

Conclusion: ARCANE框架在自动化空间天气监测方面取得了重要进展，为实时预报奠定了基础。

Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space
weather disturbances, posing risks to both technological infrastructure and
human activities. Automatic detection of ICMEs in solar wind in situ data is
essential for early warning systems. While several methods have been proposed
to identify these structures in time series data, robust real-time detection
remains a significant challenge. In this work, we present ARCANE - the first
framework explicitly designed for early ICME detection in streaming solar wind
data under realistic operational constraints, enabling event identification
without requiring observation of the full structure. Our approach evaluates the
strengths and limitations of detection models by comparing a machine
learning-based method to a threshold-based baseline. The ResUNet++ model,
previously validated on science data, significantly outperforms the baseline,
particularly in detecting high-impact events, while retaining solid performance
on lower-impact cases. Notably, we find that using real-time solar wind (RTSW)
data instead of high-resolution science data leads to only minimal performance
degradation. Despite the challenges of operational settings, our detection
pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%
of the event's duration while only seeing a minimal amount of data. As more
data becomes available, the performance increases significantly. These results
mark a substantial step forward in automated space weather monitoring and lay
the groundwork for enhanced real-time forecasting capabilities.

</details>


### [502] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/abs/2505.09011)
*Antonio Candito,Matthew D Blackledge,Richard Holbrey,Nuria Porta,Ana Ribeiro,Fabio Zugni,Luca D'Erme,Francesca Castagnoli,Alina Dragan,Ricardo Donners,Christina Messiou,Nina Tunariu,Dow-Mu Koh*

Main category: cs.LG

Relevance: 10.0

TL;DR: 开发了一种AI驱动的软件解决方案，用于从全身扩散加权成像（WB-DWI）扫描中量化转移性骨病。核心包括弱监督的Residual U-Net模型、统计框架和浅层卷积神经网络，用于生成骨骼概率图和病变掩膜。测试结果显示较高的准确性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 为临床决策提供可重复的转移性骨病监测工具，通过量化TDV和gADC来评估治疗效果。

Method: 结合弱监督Residual U-Net、统计框架和浅层CNN，生成骨骼概率图和病变掩膜，并提取TDV和gADC统计数据。

Result: Dice分数为0.6，表面距离2mm；log-TDV和gADC的差异分别低于9%和5%；重复性分析显示高一致性。软件在评估治疗反应时达到80.5%准确率。

Conclusion: 该软件为转移性骨病的监测提供了可重复的量化工具，具有临床决策支持潜力。

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [503] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/abs/2505.09175)
*Mohammad Ganjirad,Mahmoud Reza Delavar,Hossein Bagheri,Mohammad Mehdi Azizi*

Main category: cs.LG

Relevance: 10.0

TL;DR: 论文提出了一种基于多源数据的城市绿地开发优先级框架，结合机器学习模型（如随机森林）和微气候模拟，为城市规划提供工具。


<details>
  <summary>Details</summary>
Motivation: 解决城市绿地开发优先级问题，结合社会经济、环境敏感性和气象数据，优化城市环境。

Method: 使用WRF模型估计气温，结合机器学习模型（XGBoost、LightGBM、RF等）分类植被覆盖，并通过特征重要性分析确定关键指标。

Result: 随机森林模型表现最佳（准确率>94%），绿色屋顶技术可降低气温0.67°C。

Conclusion: 框架为城市规划者提供了有效的绿地开发优先级工具。

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [504] [Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes](https://arxiv.org/abs/2505.09026)
*Domniki Ladopoulou,Dat Minh Hong,Petros Dellaportas*

Main category: stat.AP

Relevance: 10.0

TL;DR: 论文提出了一种非平稳高斯过程（GP）框架，结合广义谱混合（GSM）核，用于风能和风速数据的概率预测，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 风能预测对电网稳定性和可再生能源整合至关重要，传统GP模型因使用平稳核而无法捕捉非平稳特性。

Method: 提出非平稳GP框架，采用GSM核以建模时变模式和异方差行为。

Result: 在真实SCADA数据上，GSM核模型在短期预测中表现优于传统核方法。

Conclusion: 非平稳GP模型在风能预测中具有实际价值，尤其在短期预测中。

Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining
grid stability and enabling efficient integration of renewable energy sources.
Gaussian Process (GP) models offer a principled framework for quantifying
uncertainty; however, conventional approaches rely on stationary kernels, which
are inadequate for modeling the inherently non-stationary nature of wind speed
and power output. We propose a non-stationary GP framework that incorporates
the generalized spectral mixture (GSM) kernel, enabling the model to capture
time-varying patterns and heteroscedastic behaviors in wind speed and wind
power data. We evaluate the performance of the proposed model on real-world
SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons.
Compared to standard radial basis function and spectral mixture kernels, the
GSM-based model outperforms, particularly in short-term forecasts. These
results highlight the necessity of modeling non-stationarity in wind power
forecasting and demonstrate the practical value of non-stationary GP models in
operational settings.

</details>


### [505] [ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections](https://arxiv.org/abs/2505.09365)
*H. T. Rüdisser,G. Nguyen,J. Le Louëdec,C. Möstl*

Main category: physics.space-ph

Relevance: 10.0

TL;DR: 论文提出了ARCANE框架，用于实时检测太阳风数据中的ICMEs，比较了机器学习方法和阈值基线方法，发现ResUNet++模型表现优异。


<details>
  <summary>Details</summary>
Motivation: ICMEs对空间天气有重大影响，实时检测对预警系统至关重要，但目前仍具挑战性。

Method: 提出ARCANE框架，比较机器学习（ResUNet++）和阈值基线方法，评估其在实时数据中的表现。

Result: ResUNet++显著优于基线，F1得分为0.53，检测延迟为事件持续时间的21.5%。

Conclusion: ARCANE框架在实时空间天气监测中迈出重要一步，为未来实时预报奠定基础。

Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space
weather disturbances, posing risks to both technological infrastructure and
human activities. Automatic detection of ICMEs in solar wind in situ data is
essential for early warning systems. While several methods have been proposed
to identify these structures in time series data, robust real-time detection
remains a significant challenge. In this work, we present ARCANE - the first
framework explicitly designed for early ICME detection in streaming solar wind
data under realistic operational constraints, enabling event identification
without requiring observation of the full structure. Our approach evaluates the
strengths and limitations of detection models by comparing a machine
learning-based method to a threshold-based baseline. The ResUNet++ model,
previously validated on science data, significantly outperforms the baseline,
particularly in detecting high-impact events, while retaining solid performance
on lower-impact cases. Notably, we find that using real-time solar wind (RTSW)
data instead of high-resolution science data leads to only minimal performance
degradation. Despite the challenges of operational settings, our detection
pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%
of the event's duration while only seeing a minimal amount of data. As more
data becomes available, the performance increases significantly. These results
mark a substantial step forward in automated space weather monitoring and lay
the groundwork for enhanced real-time forecasting capabilities.

</details>


### [506] [Bounding Neyman-Pearson Region with $f$-Divergences](https://arxiv.org/abs/2505.08899)
*Andrew Mullhaupt,Cheng Peng*

Main category: math.ST

Relevance: 1.0

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of
points whose coordinates represent the false positive rate and false negative
rate of some test. The lower boundary of this region is given by the
Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the
optimal ROC curve. We establish a novel lower bound for the boundary in terms
of any $f$-divergence. Since the bound generated by hockey-stick
$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best
possible. In the case of KL divergence, this bound improves Pinsker's
inequality. Furthermore, we obtain a closed-form refined upper bound for the
Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally,
we present methods for constructing pairs of distributions that can
approximately or exactly realize any given Neyman-Pearson boundary.

</details>
